<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JRSSSB_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jrsssb---104">JRSSSB - 104</h2>
<ul>
<li><details>
<summary>
(2022). Contents of volume 84, 2022. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(5), 2088–2089. (<a
href="https://doi.org/10.1111/rssb.12558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  doi     = {10.1111/rssb.12558},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {5},
  pages   = {2088-2089},
  title   = {Contents of volume 84, 2022},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Structure learning for extremal tree models. <em>Journal of
the Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(5), 2055–2087. (<a
href="https://doi.org/10.1111/rssb.12556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Extremal graphical models are sparse statistical models for multivariate extreme events. The underlying graph encodes conditional independencies and enables a visual interpretation of the complex extremal dependence structure. For the important case of tree models, we develop a data-driven methodology for learning the graphical structure. We show that sample versions of the extremal correlation and a new summary statistic, which we call the extremal variogram, can be used as weights for a minimum spanning tree to consistently recover the true underlying tree. Remarkably, this implies that extremal tree models can be learned in a completely non-parametric fashion by using simple summary statistics and without the need to assume discrete distributions, existence of densities or parametric models for bivariate distributions.},
  archive  = {J},
  author   = {Sebastian Engelke and Stanislav Volgushev},
  doi      = {10.1111/rssb.12556},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {2055-2087},
  title    = {Structure learning for extremal tree models},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A statistical test to reject the structural interpretation
of a latent factor model. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(5), 2032–2054. (<a
href="https://doi.org/10.1111/rssb.12555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Factor analysis is often used to assess whether a single univariate latent variable is sufficient to explain most of the covariance among a set of indicators for some underlying construct. When evidence suggests that a single factor is adequate, research often proceeds by using a univariate summary of the indicators in subsequent research. Implicit in such practices is the assumption that it is the underlying latent, rather than the indicators, that is causally efficacious. The assumption that the indicators do not have effects on anything subsequent, and that they are themselves only affected by antecedents through the underlying latent is a strong assumption, effectively imposing a structural interpretation on the latent factor model. In this paper, we show that this structural assumption has empirically testable implications, even though the latent variable itself is unobserved. We develop a statistical test to potentially reject the structural interpretation of a latent factor model. We apply this test to data concerning associations between the Satisfaction with Life Scale and subsequent all-cause mortality, which provides strong evidence against a structural interpretation for a univariate latent underlying the scale. Discussion is given to the implications of this result for the development, evaluation and use of measures, and for the use of factor analysis itself.},
  archive  = {J},
  author   = {Tyler J. VanderWeele and Stijn Vansteelandt},
  doi      = {10.1111/rssb.12555},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {2032-2054},
  title    = {A statistical test to reject the structural interpretation of a latent factor model},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-dimensional principal component analysis with
heterogeneous missingness. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(5), 2000–2031. (<a
href="https://doi.org/10.1111/rssb.12550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We study the problem of high-dimensional Principal Component Analysis (PCA) with missing observations. In a simple, homogeneous observation model, we show that an existing observed-proportion weighted (OPW) estimator of the leading principal components can (nearly) attain the minimax optimal rate of convergence, which exhibits an interesting phase transition. However, deeper investigation reveals that, particularly in more realistic settings where the observation probabilities are heterogeneous, the empirical performance of the OPW estimator can be unsatisfactory; moreover, in the noiseless case, it fails to provide exact recovery of the principal components. Our main contribution, then, is to introduce a new method, which we call primePCA , that is designed to cope with situations where observations may be missing in a heterogeneous manner. Starting from the OPW estimator, primePCA iteratively projects the observed entries of the data matrix onto the column space of our current estimate to impute the missing entries, and then updates our estimate by computing the leading right singular space of the imputed data matrix. We prove that the error of primePCA converges to zero at a geometric rate in the noiseless case, and when the signal strength is not too small. An important feature of our theoretical guarantees is that they depend on average, as opposed to worst-case, properties of the missingness mechanism. Our numerical studies on both simulated and real data reveal that primePCA exhibits very encouraging performance across a wide range of scenarios, including settings where the data are not Missing Completely At Random.},
  archive  = {J},
  author   = {Ziwei Zhu and Tengyao Wang and Richard J. Samworth},
  doi      = {10.1111/rssb.12550},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {2000-2031},
  title    = {High-dimensional principal component analysis with heterogeneous missingness},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Causal inference with spatio-temporal data: Estimating the
effects of airstrikes on insurgent violence in iraq. <em>Journal of the
Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(5), 1969–1999. (<a
href="https://doi.org/10.1111/rssb.12548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Many causal processes have spatial and temporal dimensions. Yet the classic causal inference framework is not directly applicable when the treatment and outcome variables are generated by spatio-temporal point processes. We extend the potential outcomes framework to these settings by formulating the treatment point process as a stochastic intervention. Our causal estimands include the expected number of outcome events in a specified area under a particular stochastic treatment assignment strategy. Our methodology allows for arbitrary patterns of spatial spillover and temporal carryover effects. Using martingale theory, we show that the proposed estimator is consistent and asymptotically normal as the number of time periods increases. We propose a sensitivity analysis for the possible existence of unmeasured confounders, and extend it to the Hájek estimator. Simulation studies are conducted to examine the estimators&#39; finite sample performance. Finally, we illustrate the proposed methods by estimating the effects of American airstrikes on insurgent violence in Iraq from February 2007 to July 2008. Our analysis suggests that increasing the average number of daily airstrikes for up to 1 month may result in more insurgent attacks. We also find some evidence that airstrikes can displace attacks from Baghdad to new locations up to 400 km away.},
  archive  = {J},
  author   = {Georgia Papadogeorgou and Kosuke Imai and Jason Lyall and Fan Li},
  doi      = {10.1111/rssb.12548},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1969-1999},
  title    = {Causal inference with spatio-temporal data: Estimating the effects of airstrikes on insurgent violence in iraq},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Empirical likelihood-based inference for functional means
with application to wearable device data. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(5), 1947–1968. (<a
href="https://doi.org/10.1111/rssb.12543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper develops a nonparametric inference framework that is applicable to occupation time curves derived from wearable device data. These curves consider all activity levels within the range of device readings, which is preferable to the practice of classifying activity into discrete categories. Motivated by certain features of these curves, we introduce a powerful likelihood ratio approach to construct confidence bands and compare functional means. Notably, our approach allows discontinuities in the functional covariances while accommodating discretization of the observed trajectories. A simulation study shows that the proposed procedures outperform competing functional data procedures. We illustrate the proposed methods using wearable device data from an NHANES study.},
  archive  = {J},
  author   = {Hsin-wen Chang and Ian W. McKeague},
  doi      = {10.1111/rssb.12543},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1947-1968},
  title    = {Empirical likelihood-based inference for functional means with application to wearable device data},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ZAP: Z<span class="math display"><em>z</em></span>-value
adaptive procedures for false discovery rate control with side
information. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>84</em>(5), 1886–1946. (<a
href="https://doi.org/10.1111/rssb.12557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Adaptive multiple testing with covariates is an important research direction that has gained major attention in recent years. It has been widely recognised that leveraging side information provided by auxiliary covariates can improve the power of false discovery rate (FDR) procedures. Currently, most such procedures are devised with p -values as their main statistics. However, for two-sided hypotheses, the usual data processing step that transforms the primary statistics, known as z -values, into p -values not only leads to a loss of information carried by the main statistics, but can also undermine the ability of the covariates to assist with the FDR inference. We develop a z -value based covariate-adaptive (ZAP) methodology that operates on the intact structural information encoded jointly by the z -values and covariates. It seeks to emulate the oracle z -value procedure via a working model, and its rejection regions significantly depart from those of the p -value adaptive testing approaches. The key strength of ZAP is that the FDR control is guaranteed with minimal assumptions, even when the working model is misspecified. We demonstrate the state-of-the-art performance of ZAP using both simulated and real data, which shows that the efficiency gain can be substantial in comparison with p -value-based methods. Our methodology is implemented in the R package zap .},
  archive  = {J},
  author   = {Dennis Leung and Wenguang Sun},
  doi      = {10.1111/rssb.12557},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1886-1946},
  title    = {ZAP: Z$$ z $$-value adaptive procedures for false discovery rate control with side information},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Linear regression and its inference on noisy network-linked
data. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>84</em>(5), 1851–1885. (<a
href="https://doi.org/10.1111/rssb.12554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Linear regression on network-linked observations has been an essential tool in modelling the relationship between response and covariates with additional network structures. Previous methods either lack inference tools or rely on restrictive assumptions on social effects and usually assume that networks are observed without errors. This paper proposes a regression model with non-parametric network effects. The model does not assume that the relational data or network structure is exactly observed and can be provably robust to network perturbations. Asymptotic inference framework is established under a general requirement of the network observational errors, and the robustness of this method is studied in the specific setting when the errors come from random network models. We discover a phase-transition phenomenon of the inference validity concerning the network density when no prior knowledge of the network model is available while also showing a significant improvement achieved by knowing the network model. Simulation studies are conducted to verify these theoretical results and demonstrate the advantage of the proposed method over existing work in terms of accuracy and computational efficiency under different data-generating models. The method is then applied to middle school students&#39; network data to study the effectiveness of educational workshops in reducing school conflicts.},
  archive  = {J},
  author   = {Can M. Le and Tianxi Li},
  doi      = {10.1111/rssb.12554},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1851-1885},
  title    = {Linear regression and its inference on noisy network-linked data},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional independence testing in hilbert spaces with
applications to functional data analysis. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(5), 1821–1850. (<a
href="https://doi.org/10.1111/rssb.12544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We study the problem of testing the null hypothesis that X and Y are conditionally independent given Z , where each of X , Y and Z may be functional random variables. This generalises testing the significance of X in a regression model of scalar response Y on functional regressors X and Z . We show, however, that even in the idealised setting where additionally ( X , Y , Z ) has a Gaussian distribution, the power of any test cannot exceed its size. Further modelling assumptions are needed and we argue that a convenient way of specifying these assumptions is based on choosing methods for regressing each of X and Y on Z . We propose a test statistic involving inner products of the resulting residuals that is simple to compute and calibrate: type I error is controlled uniformly when the in-sample prediction errors are sufficiently small. We show this requirement is met by ridge regression in functional linear model settings without requiring any eigen-spacing conditions or lower bounds on the eigenvalues of the covariance of the functional regressor. We apply our test in constructing confidence intervals for truncation points in truncated functional linear models and testing for edges in a functional graphical model for EEG data.},
  archive  = {J},
  author   = {Anton Rask Lundborg and Rajen D. Shah and Jonas Peters},
  doi      = {10.1111/rssb.12544},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1821-1850},
  title    = {Conditional independence testing in hilbert spaces with applications to functional data analysis},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CovNet: Covariance networks for functional data on
multidimensional domains. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(5), 1785–1820. (<a
href="https://doi.org/10.1111/rssb.12551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Covariance estimation is ubiquitous in functional data analysis. Yet, the case of functional observations over multidimensional domains introduces computational and statistical challenges, rendering the standard methods effectively inapplicable. To address this problem, we introduce Covariance Networks (CovNet) as a modelling and estimation tool. The CovNet model is universal —it can be used to approximate any covariance up to desired precision. Moreover, the model can be fitted efficiently to the data and its neural network architecture allows us to employ modern computational tools in the implementation. The CovNet model also admits a closed-form eigendecomposition, which can be computed efficiently, without constructing the covariance itself. This facilitates easy storage and subsequent manipulation of a covariance in the context of the CovNet. We establish consistency of the proposed estimator and derive its rate of convergence. The usefulness of the proposed method is demonstrated via an extensive simulation study and an application to resting state functional magnetic resonance imaging data.},
  archive  = {J},
  author   = {Soham Sarkar and Victor M. Panaretos},
  doi      = {10.1111/rssb.12551},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1785-1820},
  title    = {CovNet: Covariance networks for functional data on multidimensional domains},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dimension-free mixing for high-dimensional bayesian variable
selection. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>84</em>(5), 1751–1784. (<a
href="https://doi.org/10.1111/rssb.12546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Yang et al. proved that the symmetric random walk Metropolis–Hastings algorithm for Bayesian variable selection is rapidly mixing under mild high-dimensional assumptions. We propose a novel Markov chain Monte Carlo (MCMC) sampler using an informed proposal scheme, which we prove achieves a much faster mixing time that is independent of the number of covariates, under the assumptions of Yang et al. To the best of our knowledge, this is the first high-dimensional result which rigorously shows that the mixing rate of informed MCMC methods can be fast enough to offset the computational cost of local posterior evaluation. Motivated by the theoretical analysis of our sampler, we further propose a new approach called ‘two-stage drift condition’ to studying convergence rates of Markov chains on general state spaces, which can be useful for obtaining tight complexity bounds in high-dimensional settings. The practical advantages of our algorithm are illustrated by both simulation studies and real data analysis.},
  archive  = {J},
  author   = {Quan Zhou and Jun Yang and Dootika Vats and Gareth O. Roberts and Jeffrey S. Rosenthal},
  doi      = {10.1111/rssb.12546},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1751-1784},
  title    = {Dimension-free mixing for high-dimensional bayesian variable selection},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An approximation algorithm for blocking of an experimental
design. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>84</em>(5), 1726–1750. (<a
href="https://doi.org/10.1111/rssb.12545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Blocked randomized designs are used to improve the precision of treatment effect estimates compared to a completely randomized design. A block is a set of units that are relatively homogeneous and consequently would tend to produce relatively similar outcomes if the treatment had no effect. The problem of finding the optimal blocking of the units into equal sized blocks of any given size larger than two is known to be a difficult problem—there is no polynomial time method guaranteed to find the optimal blocking. All available methods to solve the problem are heuristic methods. We propose methods that run in polynomial time and guarantee a blocking that is provably close to the optimal blocking. In all our simulation studies, the proposed methods perform better, create better homogeneous blocks, compared with the existing methods. Our blocking method aims to minimize the maximum of all pairwise differences of units in the same block. We show that bounding this maximum difference ensures that the error in the average treatment effect estimate is similarly bounded for all treatment assignments. In contrast, if the blocking bounds the average or sum of these differences, the error in the average treatment effect estimate can still be large in several treatment assignments.},
  archive  = {J},
  author   = {Bikram Karmakar},
  doi      = {10.1111/rssb.12545},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1726-1750},
  title    = {An approximation algorithm for blocking of an experimental design},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Segmenting time series via self-normalisation. <em>Journal
of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>84</em>(5), 1699–1725. (<a
href="https://doi.org/10.1111/rssb.12552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a novel and unified framework for change-point estimation in multivariate time series. The proposed method is fully non-parametric, robust to temporal dependence and avoids the demanding consistent estimation of long-run variance. One salient and distinct feature of the proposed method is its versatility, where it allows change-point detection for a broad class of parameters (such as mean, variance, correlation and quantile) in a unified fashion. At the core of our method, we couple the self-normalisation- (SN) based tests with a novel nested local-window segmentation algorithm, which seems new in the growing literature of change-point analysis. Due to the presence of an inconsistent long-run variance estimator in the SN test, non-standard theoretical arguments are further developed to derive the consistency and convergence rate of the proposed SN-based change-point detection method. Extensive numerical experiments and relevant real data analysis are conducted to illustrate the effectiveness and broad applicability of our proposed method in comparison with state-of-the-art approaches in the literature.},
  archive  = {J},
  author   = {Zifeng Zhao and Feiyu Jiang and Xiaofeng Shao},
  doi      = {10.1111/rssb.12552},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1699-1725},
  title    = {Segmenting time series via self-normalisation},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exact clustering in tensor block model: Statistical
optimality and computational limit. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>84</em>(5),
1666–1698. (<a href="https://doi.org/10.1111/rssb.12547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {High-order clustering aims to identify heterogeneous substructures in multiway datasets that arise commonly in neuroimaging, genomics, social network studies, etc. The non-convex and discontinuous nature of this problem pose significant challenges in both statistics and computation. In this paper, we propose a tensor block model and the computationally efficient methods, high-order Lloyd algorithm (HLloyd), and high-order spectral clustering (HSC), for high-order clustering. The convergence guarantees and statistical optimality are established for the proposed procedure under a mild sub-Gaussian noise assumption. Under the Gaussian tensor block model, we completely characterise the statistical-computational trade-off for achieving high-order exact clustering based on three different signal-to-noise ratio regimes. The analysis relies on new techniques of high-order spectral perturbation analysis and a ‘singular-value-gap-free’ error bound in tensor estimation, which are substantially different from the matrix spectral analyses in the literature. Finally, we show the merits of the proposed procedures via extensive experiments on both synthetic and real datasets.},
  archive  = {J},
  author   = {Rungang Han and Yuetian Luo and Miaoyan Wang and Anru R. Zhang},
  doi      = {10.1111/rssb.12547},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1666-1698},
  title    = {Exact clustering in tensor block model: Statistical optimality and computational limit},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). General bayesian loss function selection and the use of
improper models. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>84</em>(5), 1640–1665. (<a
href="https://doi.org/10.1111/rssb.12553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Statisticians often face the choice between using probability models or a paradigm defined by minimising a loss function. Both approaches are useful and, if the loss can be re-cast into a proper probability model, there are many tools to decide which model or loss is more appropriate for the observed data, in the sense of explaining the data&#39;s nature. However, when the loss leads to an improper model, there are no principled ways to guide this choice. We address this task by combining the Hyvärinen score, which naturally targets infinitesimal relative probabilities, and general Bayesian updating, which provides a unifying framework for inference on losses and models. Specifically we propose the ℋ $$ \mathscr{H} $$ -score, a general Bayesian selection criterion and prove that it consistently selects the (possibly improper) model closest to the data-generating truth in Fisher&#39;s divergence. We also prove that an associated ℋ $$ \mathscr{H} $$ -posterior consistently learns optimal hyper-parameters featuring in loss functions, including a challenging tempering parameter in generalised Bayesian inference. As salient examples, we consider robust regression and non-parametric density estimation where popular loss functions define improper models for the data and hence cannot be dealt with using standard model selection tools. These examples illustrate advantages in robustness-efficiency trade-offs and enable Bayesian inference for kernel density estimation, opening a new avenue for Bayesian non-parametrics.},
  archive  = {J},
  author   = {Jack Jewson and David Rossell},
  doi      = {10.1111/rssb.12553},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1640-1665},
  title    = {General bayesian loss function selection and the use of improper models},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Calibrating the scan statistic: Finite sample performance
versus asymptotics. <em>Journal of the Royal Statistical Society: Series
B (Statistical Methodology)</em>, <em>84</em>(5), 1608–1639. (<a
href="https://doi.org/10.1111/rssb.12549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We consider the problem of detecting an elevated mean on an interval with unknown location and length in the univariate Gaussian sequence model. Recent results have shown that using scale-dependent critical values for the scan statistic allows to attain asymptotically optimal detection simultaneously for all signal lengths, thereby improving on the traditional scan, but this procedure has been criticised for losing too much power for short signals. We explain this discrepancy by showing that these asymptotic optimality results will necessarily be too imprecise to discern the performance of scan statistics in a practically relevant way, even in a large sample context. Instead, we propose to assess the performance with a new finite sample criterion. We then present three calibrations for scan statistics that perform well across a range of relevant signal lengths: The first calibration uses a particular adjustment to the critical values and is therefore tailored to the Gaussian case. The second calibration uses a scale-dependent adjustment to the significance levels rather than to the critical values and this adjustment is therefore applicable to arbitrary known null distributions. The third calibration restricts the scan to a particular sparse subset of the scan windows and then applies a weighted Bonferroni adjustment to the corresponding test statistics. This Bonferroni scan is also applicable to arbitrary null distributions and in addition is very simple to implement. We show how to apply these calibrations for scanning in a number of distributional settings: for normal observations with an unknown baseline and a known or unknown constant variance, for observations from a natural exponential family, for potentially heteroscadastic observations from a symmetric density by employing self-normalisation in a novel way, and for exchangeable observations using tests based on permutations, ranks or signs.},
  archive  = {J},
  author   = {Guenther Walther and Andrew Perry},
  doi      = {10.1111/rssb.12549},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1608-1639},
  title    = {Calibrating the scan statistic: Finite sample performance versus asymptotics},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modelling the COVID-19 infection trajectory: A piecewise
linear quantile trend model*. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>84</em>(5),
1589–1607. (<a href="https://doi.org/10.1111/rssb.12453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a piecewise linear quantile trend model to analyse the trajectory of the COVID-19 daily new cases (i.e. the infection curve) simultaneously across multiple quantiles. The model is intuitive, interpretable and naturally captures the phase transitions of the epidemic growth rate via change-points. Unlike the mean trend model and least squares estimation, our quantile-based approach is robust to outliers, captures heteroscedasticity (commonly exhibited by COVID-19 infection curves) and automatically delivers both point and interval forecasts with minimal assumptions. Building on a self-normalized (SN) test statistic, this paper proposes a novel segmentation algorithm for multiple change-point estimation. Theoretical guarantees such as segmentation consistency are established under mild and verifiable assumptions. Using the proposed method, we analyse the COVID-19 infection curves in 35 major countries and discover patterns with potentially relevant implications for effectiveness of the pandemic responses by different countries. A simple change-adaptive two-stage forecasting scheme is further designed to generate short-term prediction of COVID-19 cumulative new cases and is shown to deliver accurate forecast valuable to public health decision-making.},
  archive  = {J},
  author   = {Feiyu Jiang and Zifeng Zhao and Xiaofeng Shao},
  doi      = {10.1111/rssb.12453},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1589-1607},
  title    = {Modelling the COVID-19 infection trajectory: A piecewise linear quantile trend model*},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Corrigendum to “simulation of multivariate diffusion
bridges.” <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>84</em>(4), 1581–1585. (<a
href="https://doi.org/10.1111/rssb.12512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We correct an error in Theorem 1 in Bladt et al. (2016) Journal of the Royal Statistical Society: Series B , 78 , 343–369 by changing the initial distribution of an auxiliary diffusion process, which is used to describe the distribution of the proposed approximate diffusion bridges. As a consequence, we correct two algorithms for simulating exact diffusion bridges by changing the initial distribution of auxiliary diffusion processes in the same way. Simulation studies affected by the error are redone.},
  archive  = {J},
  author   = {Mogens Bladt and Samuel Finch and Michael Sørensen},
  doi      = {10.1111/rssb.12512},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1581-1585},
  title    = {Corrigendum to ‘Simulation of multivariate diffusion bridges’},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Universal prediction band via semi-definite programming.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>84</em>(4), 1558–1580. (<a
href="https://doi.org/10.1111/rssb.12542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a computationally efficient method to construct nonparametric, heteroscedastic prediction bands for uncertainty quantification, with or without any user-specified predictive model. Our approach provides an alternative to the now-standard conformal prediction for uncertainty quantification, with novel theoretical insights and computational advantages. The data-adaptive prediction band is universally applicable with minimal distributional assumptions, has strong non-asymptotic coverage properties, and is easy to implement using standard convex programs. Our approach can be viewed as a novel variance interpolation with confidence and further leverages techniques from semi-definite programming and sum-of-squares optimization. Theoretical and numerical performances for the proposed approach for uncertainty quantification are analysed.},
  archive  = {J},
  author   = {Tengyuan Liang},
  doi      = {10.1111/rssb.12542},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1558-1580},
  title    = {Universal prediction band via semi-definite programming},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The debiased spatial whittle likelihood. <em>Journal of the
Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(4), 1526–1557. (<a
href="https://doi.org/10.1111/rssb.12539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We provide a computationally and statistically efficient method for estimating the parameters of a stochastic covariance model observed on a regular spatial grid in any number of dimensions. Our proposed method, which we call the Debiased Spatial Whittle likelihood, makes important corrections to the well-known Whittle likelihood to account for large sources of bias caused by boundary effects and aliasing. We generalize the approach to flexibly allow for significant volumes of missing data including those with lower-dimensional substructure, and for irregular sampling boundaries. We build a theoretical framework under relatively weak assumptions which ensures consistency and asymptotic normality in numerous practical settings including missing data and non-Gaussian processes. We also extend our consistency results to multivariate processes. We provide detailed implementation guidelines which ensure the estimation procedure can be conducted in O ( n log n ) operations, where n is the number of points of the encapsulating rectangular grid, thus keeping the computational scalability of Fourier and Whittle-based methods for large data sets. We validate our procedure over a range of simulated and realworld settings, and compare with state-of-the-art alternatives, demonstrating the enduring practical appeal of Fourier-based methods, provided they are corrected by the procedures developed in this paper.},
  archive  = {J},
  author   = {Arthur P. Guillaumin and Adam M. Sykulski and Sofia C. Olhede and Frederik J. Simons},
  doi      = {10.1111/rssb.12539},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1526-1557},
  title    = {The debiased spatial whittle likelihood},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Paired or partially paired two-sample tests with unordered
samples. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>84</em>(4), 1503–1525. (<a
href="https://doi.org/10.1111/rssb.12541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In paired two-sample tests for mean equality, it is common to encounter unordered samples in which subject identities are not observed or unobservable, and it is impossible to link the measurements before and after treatment. The absence of subject identities masks the correspondence between the two samples, rendering existing methods inapplicable. In this paper, we propose two novel testing approaches. The first splits one of the two unordered samples into blocks and approximates the population mean using the average of the other sample. The second method is a variant of the first, in which subsampling is used to construct an incomplete U -statistic. Both methods are affine invariant and can readily be extended to partially paired two-sample tests with unordered samples. Asymptotic null distributions of the proposed test statistics are derived and the local powers of the tests are studied. Comprehensive simulations show that the proposed testing methods are able to maintain the correct size, and their powers are comparable to those of the oracle tests with perfect pair information. Four real examples are used to illustrate the proposed methods, in which we demonstrate that naive methods can yield misleading conclusions.},
  archive  = {J},
  author   = {Yudong Wang and Yanlin Tang and Zhi-Sheng Ye},
  doi      = {10.1111/rssb.12541},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1503-1525},
  title    = {Paired or partially paired two-sample tests with unordered samples},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the cross-validation bias due to unsupervised
preprocessing. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>84</em>(4), 1474–1502. (<a
href="https://doi.org/10.1111/rssb.12537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Cross-validation is the de facto standard for predictive model evaluation and selection. In proper use, it provides an unbiased estimate of a model&#39;s predictive performance. However, data sets often undergo various forms of data-dependent preprocessing, such as mean-centring, rescaling, dimensionality reduction and outlier removal. It is often believed that such preprocessing stages, if done in an unsupervised manner (that does not incorporate the class labels or response values) are generally safe to do prior to cross-validation. In this paper, we study three commonly practised preprocessing procedures prior to a regression analysis: (i) variance-based feature selection; (ii) grouping of rare categorical features; and (iii) feature rescaling. We demonstrate that unsupervised preprocessing can, in fact, introduce a substantial bias into cross-validation estimates and potentially hurt model selection. This bias may be either positive or negative and its exact magnitude depends on all the parameters of the problem in an intricate manner. Further research is needed to understand the real-world impact of this bias across different application domains, particularly when dealing with small sample sizes and high-dimensional data.},
  archive  = {J},
  author   = {Amit Moscovich and Saharon Rosset},
  doi      = {10.1111/rssb.12537},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1474-1502},
  title    = {On the cross-validation bias due to unsupervised preprocessing},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A statistical interpretation of spectral embedding: The
generalised random dot product graph. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(4), 1446–1473. (<a
href="https://doi.org/10.1111/rssb.12509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Spectral embedding is a procedure which can be used to obtain vector representations of the nodes of a graph. This paper proposes a generalisation of the latent position network model known as the random dot product graph, to allow interpretation of those vector representations as latent position estimates. The generalisation is needed to model heterophilic connectivity (e.g. ‘opposites attract’) and to cope with negative eigenvalues more generally. We show that, whether the adjacency or normalised Laplacian matrix is used, spectral embedding produces uniformly consistent latent position estimates with asymptotically Gaussian error (up to identifiability). The standard and mixed membership stochastic block models are special cases in which the latent positions take only K distinct vector values, representing communities, or live in the ( K − 1)-simplex with those vertices respectively. Under the stochastic block model, our theory suggests spectral clustering using a Gaussian mixture model (rather than K -means) and, under mixed membership, fitting the minimum volume enclosing simplex, existing recommendations previously only supported under non-negative-definite assumptions. Empirical improvements in link prediction (over the random dot product graph), and the potential to uncover richer latent structure (than posited under the standard or mixed membership stochastic block models) are demonstrated in a cyber-security example.},
  archive  = {J},
  author   = {Patrick Rubin-Delanchy and Joshua Cape and Minh Tang and Carey E. Priebe},
  doi      = {10.1111/rssb.12509},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1446-1473},
  title    = {A statistical interpretation of spectral embedding: The generalised random dot product graph},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiply robust estimation of causal effects under principal
ignorability. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>84</em>(4), 1423–1445. (<a
href="https://doi.org/10.1111/rssb.12538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Causal inference concerns not only the average effect of the treatment on the outcome but also the underlying mechanism through an intermediate variable of interest. Principal stratification characterizes such a mechanism by targeting subgroup causal effects within principal strata, which are defined by the joint potential values of an intermediate variable. Due to the fundamental problem of causal inference, principal strata are inherently latent, rendering it challenging to identify and estimate subgroup effects within them. A line of research leverages the principal ignorability assumption that the latent principal strata are mean independent of the potential outcomes conditioning on the observed covariates. Under principal ignorability, we derive various nonparametric identification formulas for causal effects within principal strata in observational studies, which motivate estimators relying on the correct specifications of different parts of the observed-data distribution. Appropriately combining these estimators yields triply robust estimators for the causal effects within principal strata. These triply robust estimators are consistent if two of the treatment, intermediate variable and outcome models are correctly specified, and moreover, they are locally efficient if all three models are correctly specified. We show that these estimators arise naturally from either the efficient influence functions in the semiparametric theory or the model-assisted estimators in the survey sampling theory. We evaluate different estimators based on their finite-sample performance through simulation and apply them to two observational studies.},
  archive  = {J},
  author   = {Zhichao Jiang and Shu Yang and Peng Ding},
  doi      = {10.1111/rssb.12538},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1423-1445},
  title    = {Multiply robust estimation of causal effects under principal ignorability},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Functional peaks-over-threshold analysis. <em>Journal of the
Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(4), 1392–1422. (<a
href="https://doi.org/10.1111/rssb.12498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Peaks-over-threshold analysis using the generalised Pareto distribution is widely applied in modelling tails of univariate random variables, but much information may be lost when complex extreme events are studied using univariate results. In this paper, we extend peaks-over-threshold analysis to extremes of functional data. Threshold exceedances defined using a functional r are modelled by the generalised r -Pareto process, a functional generalisation of the generalised Pareto distribution that covers the three classical regimes for the decay of tail probabilities, and that is the only possible continuous limit for r -exceedances of a properly rescaled process. We give construction rules, simulation algorithms and inference procedures for generalised r -Pareto processes, discuss model validation and apply the new methodology to extreme European windstorms and heavy spatial rainfall.},
  archive  = {J},
  author   = {Raphaël de Fondeville and Anthony C. Davison},
  doi      = {10.1111/rssb.12498},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1392-1422},
  title    = {Functional peaks-over-threshold analysis},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient evaluation of prediction rules in semi-supervised
settings under stratified sampling. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>84</em>(4),
1353–1391. (<a href="https://doi.org/10.1111/rssb.12502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In many contemporary applications, large amounts of unlabelled data are readily available while labelled examples are limited. There has been substantial interest in semi-supervised learning (SSL) which aims to leverage unlabelled data to improve estimation or prediction. However, current SSL literature focuses primarily on settings where labelled data are selected uniformly at random from the population of interest. Stratified sampling, while posing additional analytical challenges, is highly applicable to many real-world problems. Moreover, no SSL methods currently exist for estimating the prediction performance of a fitted model when the labelled data are not selected uniformly at random. In this paper, we propose a two-step SSL procedure for evaluating a prediction rule derived from a working binary regression model based on the Brier score and overall misclassification rate under stratified sampling. In step I, we impute the missing labels via weighted regression with nonlinear basis functions to account for stratified sampling and to improve efficiency. In step II, we augment the initial imputations to ensure the consistency of the resulting estimators regardless of the specification of the prediction model or the imputation model. The final estimator is then obtained with the augmented imputations. We provide asymptotic theory and numerical studies illustrating that our proposals outperform their supervised counterparts in terms of efficiency gain. Our methods are motivated by electronic health record (EHR) research and validated with a real data analysis of an EHR-based study of diabetic neuropathy.},
  archive  = {J},
  author   = {Jessica Gronsbell and Molei Liu and Lu Tian and Tianxi Cai},
  doi      = {10.1111/rssb.12502},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1353-1391},
  title    = {Efficient evaluation of prediction rules in semi-supervised settings under stratified sampling},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonparametric, tuning-free estimation of s-shaped functions.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>84</em>(4), 1324–1352. (<a
href="https://doi.org/10.1111/rssb.12481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We consider the nonparametric estimation of an S-shaped regression function. The least squares estimator provides a very natural, tuning-free approach, but results in a non-convex optimization problem, since the inflection point is unknown. We show that the estimator may nevertheless be regarded as a projection onto a finite union of convex cones, which allows us to propose a mixed primal-dual bases algorithm for its efficient, sequential computation. After developing a projection framework that demonstrates the consistency and robustness to misspecification of the estimator, our main theoretical results provide sharp oracle inequalities that yield worst-case and adaptive risk bounds for the estimation of the regression function, as well as a rate of convergence for the estimation of the inflection point. These results reveal not only that the estimator achieves the minimax optimal rate of convergence for both the estimation of the regression function and its inflection point (up to a logarithmic factor in the latter case), but also that it is able to achieve an almost-parametric rate when the true regression function is piecewise affine with not too many affine pieces. Simulations and a real data application to air pollution modelling also confirm the desirable finite-sample properties of the estimator, and our algorithm is implemented in the R package Sshaped .},
  archive  = {J},
  author   = {Oliver Y. Feng and Yining Chen and Qiyang Han and Raymond J. Carroll and Richard J. Samworth},
  doi      = {10.1111/rssb.12481},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1324-1352},
  title    = {Nonparametric, tuning-free estimation of S-shaped functions},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian context trees: Modelling and exact inference for
discrete time series. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(4), 1287–1323. (<a
href="https://doi.org/10.1111/rssb.12511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We develop a new Bayesian modelling framework for the class of higher-order, variable-memory Markov chains, and introduce an associated collection of methodological tools for exact inference with discrete time series. We show that a version of the context tree weighting alg-orithm can compute the prior predictive likelihood exa-ctly (averaged over both models and parameters), and two related algorithms are introduced, which identify the a posteriori most likely models and compute their exact posterior probabilities. All three algorithms are deterministic and have linear-time complexity. A family of variable-dimension Markov chain Monte Carlo samplers is also provided, facilitating further exploration of the posterior. The performance of the proposed methods in model selection, Markov order estimation and prediction is illustrated through simulation experiments and real-world applications with data from finance, genetics, neuroscience and animal communication. The associated algorithms are implemented in the R package BCT .},
  archive  = {J},
  author   = {Ioannis Kontoyiannis and Lambros Mertzanis and Athina Panotopoulou and Ioannis Papageorgiou and Maria Skoularidou},
  doi      = {10.1111/rssb.12511},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1287-1323},
  title    = {Bayesian context trees: Modelling and exact inference for discrete time series},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian inference for risk minimization via exponentially
tilted empirical likelihood. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>84</em>(4),
1257–1286. (<a href="https://doi.org/10.1111/rssb.12510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The celebrated Bernstein von-Mises theorem ensures credible regions from a Bayesian posterior to be well-calibrated when the model is correctly-specified, in the frequentist sense that their coverage probabilities tend to the nominal values as data accrue. However, this conventional Bayesian framework is known to lack robustness when the model is misspecified or partly specified, for example, in quantile regression, risk minimization based supervised/unsupervised learning and robust estimation. To alleviate this limitation, we propose a new Bayesian inferential approach that substitutes the (misspecified or partly specified) likelihoods with proper exponentially tilted empirical likelihoods plus a regularization term. Our surrogate empirical likelihood is carefully constructed by using the first-order optimality condition of empirical risk minimization as the moment condition. We show that the Bayesian posterior obtained by combining this surrogate empirical likelihood and a prior is asymptotically close to a normal distribution centering at the empirical risk minimizer with an appropriate sandwich-form covariance matrix. Consequently, the resulting Bayesian credible regions are automatically calibrated to deliver valid uncertainty quantification. Computationally, the proposed method can be easily implemented by Markov Chain Monte Carlo sampling algorithms. Our numerical results show that the proposed method tends to be more accurate than existing state-of-the-art competitors.},
  archive  = {J},
  author   = {Rong Tang and Yun Yang},
  doi      = {10.1111/rssb.12510},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1257-1286},
  title    = {Bayesian inference for risk minimization via exponentially tilted empirical likelihood},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Manifold markov chain monte carlo methods for bayesian
inference in diffusion models. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>84</em>(4),
1229–1256. (<a href="https://doi.org/10.1111/rssb.12497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Bayesian inference for nonlinear diffusions, observed at discrete times, is a challenging task that has prompted the development of a number of algorithms, mainly within the computational statistics community. We propose a new direction, and accompanying methodology—borrowing ideas from statistical physics and computational chemistry—for inferring the posterior distribution of latent diffusion paths and model parameters, given observations of the process. Joint configurations of the underlying process noise and of parameters, mapping onto diffusion paths consistent with observations, form an implicitly defined manifold. Then, by making use of a constrained Hamiltonian Monte Carlo algorithm on the embedded manifold, we are able to perform computationally efficient inference for a class of discretely observed diffusion models. Critically, in contrast with other approaches proposed in the literature, our methodology is highly automated , requiring minimal user intervention and applying alike in a range of settings, including: elliptic or hypo-elliptic systems; observations with or without noise; linear or non-linear observation operators. Exploiting Markovianity, we propose a variant of the method with complexity that scales linearly in the resolution of path discretisation and the number of observation times. Python code reproducing the results is available at http://doi.org/10.5281/zenodo.5796148 .},
  archive  = {J},
  author   = {Matthew M. Graham and Alexandre H. Thiery and Alexandros Beskos},
  doi      = {10.1111/rssb.12497},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1229-1256},
  title    = {Manifold markov chain monte carlo methods for bayesian inference in diffusion models},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast increased fidelity samplers for approximate bayesian
gaussian process regression. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>84</em>(4),
1198–1228. (<a href="https://doi.org/10.1111/rssb.12494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Gaussian processes (GPs) are common components in Bayesian non-parametric models having a rich methodological literature and strong theoretical grounding. The use of exact GPs in Bayesian models is limited to problems containing several thousand observations due to their prohibitive computational demands. We develop a posterior sampling algorithm using H -matrix approximations that scales at O ( n log 2 n ) . We show that this approximation&#39;s Kullback–Leibler divergence to the true posterior can be made arbitrarily small. Although multidimensional GPs could be used with our algorithm, d -dimensional surfaces are modelled as tensor products of univariate GPs to minimize the cost of matrix construction and maximize computational efficiency. We illustrate the performance of this fast increased fidelity approximate GP, FIFA-GP, using both simulated and non-synthetic data sets.},
  archive  = {J},
  author   = {Kelly R. Moran and Matthew W. Wheeler},
  doi      = {10.1111/rssb.12494},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1198-1228},
  title    = {Fast increased fidelity samplers for approximate bayesian gaussian process regression},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric latent class analysis of recurrent event
data. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>84</em>(4), 1175–1197. (<a
href="https://doi.org/10.1111/rssb.12499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recurrent event data frequently arise in chronic disease studies, providing rich information on disease progression. The concept of latent class offers a sensible perspective to characterize complex population heterogeneity in recurrent event trajectories that may not be adequately captured by a single regression model. However, the development of latent class methods for recurrent event data has been sparse, typically requiring strong parametric assumptions and involving algorithmic issues. In this work, we investigate latent class analysis of recurrent event data based on flexible semiparametric multiplicative modelling. We derive a robust estimation procedure through novelly adapting the conditional score technique and utilizing the special characteristics of multiplicative intensity modelling. The proposed estimation procedure can be stably and efficiently implemented based on existing computational routines. We provide solid theoretical underpinnings for the proposed method, and demonstrate its satisfactory finite sample performance via extensive simulation studies. An application to a dataset from research participants at Goizueta Alzheimer&#39;s Disease Research Center illustrates the practical utility of our proposals.},
  archive  = {J},
  author   = {Wei Zhao and Limin Peng and John Hanfelt},
  doi      = {10.1111/rssb.12499},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1175-1197},
  title    = {Semiparametric latent class analysis of recurrent event data},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bootstrap inference for the finite population mean under
complex sampling designs. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(4), 1150–1174. (<a
href="https://doi.org/10.1111/rssb.12506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Bootstrap is a useful computational tool for statistical inference, but it may lead to erroneous analysis under complex survey sampling. In this paper, we propose a unified bootstrap method for stratified multi-stage cluster sampling, Poisson sampling, simple random sampling without replacement and probability proportional to size sampling with replacement. In the proposed bootstrap method, we first generate bootstrap finite populations, apply the same sampling design to each bootstrap population to get a bootstrap sample, and then apply studentization. The second-order accuracy of the proposed bootstrap method is established by the Edgeworth expansion. Simulation studies confirm that the proposed bootstrap method outperforms the commonly used Wald-type method in terms of coverage, especially when the sample size is not large.},
  archive  = {J},
  author   = {Zhonglei Wang and Liuhua Peng and Jae Kwang Kim},
  doi      = {10.1111/rssb.12506},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1150-1174},
  title    = {Bootstrap inference for the finite population mean under complex sampling designs},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient manifold approximation with spherelets.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>84</em>(4), 1129–1149. (<a
href="https://doi.org/10.1111/rssb.12508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In statistical dimensionality reduction, it is common to rely on the assumption that high dimensional data tend to concentrate near a lower dimensional manifold. There is a rich literature on approximating the unknown manifold, and on exploiting such approximations in clustering, data compression, and prediction. Most of the literature relies on linear or locally linear approximations. In this article, we propose a simple and general alternative, which instead uses spheres, an approach we refer to as spherelets. We develop spherical principal components analysis (SPCA), and provide theory on the convergence rate for global and local SPCA, while showing that spherelets can provide lower covering numbers and mean squared errors for many manifolds. Results relative to state-of-the-art competitors show gains in ability to accurately approximate manifolds with fewer components. Unlike most competitors, which simply output lower-dimensional features, our approach projects data onto the estimated manifold to produce fitted values that can be used for model assessment and cross validation. The methods are illustrated with applications to multiple data sets.},
  archive  = {J},
  author   = {Didong Li and Minerva Mukhopadhyay and David B. Dunson},
  doi      = {10.1111/rssb.12508},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1129-1149},
  title    = {Efficient manifold approximation with spherelets},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal and maximin procedures for multiple testing
problems. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>84</em>(4), 1105–1128. (<a
href="https://doi.org/10.1111/rssb.12507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multiple testing problems (MTPs) are a staple of modern statistical analysis. The fundamental objective of MTPs is to reject as many false null hypotheses as possible (that is, maximize some notion of power), subject to controlling an overall measure of false discovery, like family-wise error rate (FWER) or false discovery rate (FDR). In this paper we provide generalizations to MTPs of the optimal Neyman-Pearson test for a single hypothesis. We show that for simple hypotheses, for both FWER and FDR and relevant notions of power, finding the optimal multiple testing procedure can be formulated as infinite dimensional binary programs and can in principle be solved for any number of hypotheses. We also characterize maximin rules for complex alternatives, and demonstrate that such rules can be found in practice, leading to improved practical procedures compared to existing alternatives that guarantee strong error control on the entire parameter space. We demonstrate the usefulness of these novel rules for identifying which studies contain signal in numerical experiments as well as in application to clinical trials with multiple studies. In various settings, the increase in power from using optimal and maximin procedures can range from 15\% to more than 100\%.},
  archive  = {J},
  author   = {Saharon Rosset and Ruth Heller and Amichai Painsky and Ehud Aharoni},
  doi      = {10.1111/rssb.12507},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1105-1128},
  title    = {Optimal and maximin procedures for multiple testing problems},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Testing for a change in mean after changepoint detection.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>84</em>(4), 1082–1104. (<a
href="https://doi.org/10.1111/rssb.12501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {While many methods are available to detect structural changes in a time series, few procedures are available to quantify the uncertainty of these estimates post-detection. In this work, we fill this gap by proposing a new framework to test the null hypothesis that there is no change in mean around an estimated changepoint. We further show that it is possible to efficiently carry out this framework in the case of changepoints estimated by binary segmentation and its variants, ℓ 0 segmentation, or the fused lasso. Our setup allows us to condition on much less information than existing approaches, which yields higher powered tests. We apply our proposals in a simulation study and on a dataset of chromosomal guanine-cytosine content. These approaches are freely available in the R package ChangepointInference at https://jewellsean.github.io/changepoint -inference/.},
  archive  = {J},
  author   = {Sean Jewell and Paul Fearnhead and Daniela Witten},
  doi      = {10.1111/rssb.12501},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1082-1104},
  title    = {Testing for a change in mean after changepoint detection},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal thinning of MCMC output. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(4), 1059–1081. (<a
href="https://doi.org/10.1111/rssb.12503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The use of heuristics to assess the convergence and compress the output of Markov chain Monte Carlo can be sub-optimal in terms of the empirical approximations that are produced. Typically a number of the initial states are attributed to ‘burn in’ and removed, while the remainder of the chain is ‘thinned’ if compression is also required. In this paper, we consider the problem of retrospectively selecting a subset of states, of fixed cardinality, from the sample path such that the approximation provided by their empirical distribution is close to optimal. A novel method is proposed, based on greedy minimisation of a kernel Stein discrepancy, that is suitable when the gradient of the log-target can be evaluated and approximation using a small number of states is required. Theoretical results guarantee consistency of the method and its effectiveness is demonstrated in the challenging context of parameter inference for ordinary differential equations. Software is available in the Stein Thinning package in Python , R and MATLAB .},
  archive  = {J},
  author   = {Marina Riabiz and Wilson Ye Chen and Jon Cockayne and Pawel Swietach and Steven A. Niederer and Lester Mackey and Chris. J. Oates},
  doi      = {10.1111/rssb.12503},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1059-1081},
  title    = {Optimal thinning of MCMC output},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-dimensional changepoint estimation with heterogeneous
missingness. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>84</em>(3), 1023–1055. (<a
href="https://doi.org/10.1111/rssb.12540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a new method for changepoint estimation in partially observed, high-dimensional time series that undergo a simultaneous change in mean in a sparse subset of coordinates. Our first methodological contribution is to introduce a ‘MissCUSUM’ transformation (a generalisation of the popular cumulative sum statistics), that captures the interaction between the signal strength and the level of missingness in each coordinate. In order to borrow strength across the coordinates, we propose to project these MissCUSUM statistics along a direction found as the solution to a penalised optimisation problem tailored to the specific sparsity structure. The changepoint can then be estimated as the location of the peak of the absolute value of the projected univariate series. In a model that allows different missingness probabilities in different component series, we identify that the key interaction between the missingness and the signal is a weighted sum of squares of the signal change in each coordinate, with weights given by the observation probabilities. More specifically, we prove that the angle between the estimated and oracle projection directions, as well as the changepoint location error, are controlled with high probability by the sum of two terms, both involving this weighted sum of squares, and representing the error incurred due to noise and the error due to missingness respectively. A lower bound confirms that our changepoint estimator, which we call MissInspect , is optimal up to a logarithmic factor. The striking effectiveness of the MissInspect methodology is further demonstrated both on simulated data, and on an oceanographic data set covering the Neogene period.},
  archive  = {J},
  author   = {Bertille Follain and Tengyao Wang and Richard J. Samworth},
  doi      = {10.1111/rssb.12540},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {1023-1055},
  title    = {High-dimensional changepoint estimation with heterogeneous missingness},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust generalised bayesian inference for intractable
likelihoods. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>84</em>(3), 997–1022. (<a
href="https://doi.org/10.1111/rssb.12500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Generalised Bayesian inference updates prior beliefs using a loss function, rather than a likelihood, and can therefore be used to confer robustness against possible mis-specification of the likelihood. Here we consider generalised Bayesian inference with a Stein discrepancy as a loss function, motivated by applications in which the likelihood contains an intractable normalisation constant. In this context, the Stein discrepancy circumvents evaluation of the normalisation constant and produces generalised posteriors that are either closed form or accessible using the standard Markov chain Monte Carlo. On a theoretical level, we show consistency, asymptotic normality, and bias-robustness of the generalised posterior, highlighting how these properties are impacted by the choice of Stein discrepancy. Then, we provide numerical experiments on a range of intractable distributions, including applications to kernel-based exponential family models and non-Gaussian graphical models.},
  archive  = {J},
  author   = {Takuo Matsubara and Jeremias Knoblauch and François-Xavier Briol and Chris J. Oates},
  doi      = {10.1111/rssb.12500},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {997-1022},
  title    = {Robust generalised bayesian inference for intractable likelihoods},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coupling-based convergence assessment of some gibbs samplers
for high-dimensional bayesian regression with shrinkage priors.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>84</em>(3), 973–996. (<a
href="https://doi.org/10.1111/rssb.12495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We consider Markov chain Monte Carlo (MCMC) algorithms for Bayesian high-dimensional regression with continuous shrinkage priors. A common challenge with these algorithms is the choice of the number of iterations to perform. This is critical when each iteration is expensive, as is the case when dealing with modern data sets, such as genome-wide association studies with thousands of rows and up to hundreds of thousands of columns. We develop coupling techniques tailored to the setting of high-dimensional regression with shrinkage priors, which enable practical, non-asymptotic diagnostics of convergence without relying on traceplots or long-run asymptotics. By establishing geometric drift and minorization conditions for the algorithm under consideration, we prove that the proposed couplings have finite expected meeting time. Focusing on a class of shrinkage priors which includes the ‘Horseshoe’, we empirically demonstrate the scalability of the proposed couplings. A highlight of our findings is that less than 1000 iterations can be enough for a Gibbs sampler to reach stationarity in a regression on 100,000 covariates. The numerical results also illustrate the impact of the prior on the computational efficiency of the coupling, and suggest the use of priors where the local precisions are Half- t distributed with degree of freedom larger than one.},
  archive  = {J},
  author   = {Niloy Biswas and Anirban Bhattacharya and Pierre E. Jacob and James E. Johndrow},
  doi      = {10.1111/rssb.12495},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {973-996},
  title    = {Coupling-based convergence assessment of some gibbs samplers for high-dimensional bayesian regression with shrinkage priors},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On functional processes with multiple discontinuities.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>84</em>(3), 933–972. (<a
href="https://doi.org/10.1111/rssb.12493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We consider the problem of estimating multiple change points for a functional data process. There are numerous examples in science and finance in which the process of interest may be subject to some sudden changes in the mean. The process data that are not in a close vicinity of any change point can be analysed by the usual nonparametric smoothing methods. However, the data close to change points and contain the most pertinent information of structural breaks need to be handled with special care. This paper considers a half-kernel approach that addresses the inference of the total number, locations and jump sizes of the changes. Convergence rates and asymptotic distributional results for the proposed procedures are thoroughly investigated. Simulations are conducted to examine the performance of the approach, and a number of real data sets are analysed to provide an illustration.},
  archive  = {J},
  author   = {Jialiang Li and Yaguang Li and Tailen Hsing},
  doi      = {10.1111/rssb.12493},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {933-972},
  title    = {On functional processes with multiple discontinuities},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Supervised multivariate learning with simultaneous feature
auto-grouping and dimension reduction. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(3), 912–932. (<a
href="https://doi.org/10.1111/rssb.12492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Modern high-dimensional methods often adopt the ‘bet on sparsity’ principle, while in supervised multivariate learning statisticians may face ‘dense’ problems with a large number of nonzero coefficients. This paper proposes a novel clustered reduced-rank learning (CRL) framework that imposes two joint matrix regularizations to automatically group the features in constructing predictive factors. CRL is more interpretable than low-rank modelling and relaxes the stringent sparsity assumption in variable selection. In this paper, new information-theoretical limits are presented to reveal the intrinsic cost of seeking for clusters, as well as the blessing from dimensionality in multivariate learning. Moreover, an efficient optimization algorithm is developed, which performs subspace learning and clustering with guaranteed convergence. The obtained fixed-point estimators, although not necessarily globally optimal, enjoy the desired statistical accuracy beyond the standard likelihood setup under some regularity conditions. Moreover, a new kind of information criterion, as well as its scale-free form, is proposed for cluster and rank selection, and has a rigorous theoretical support without assuming an infinite sample size. Extensive simulations and real-data experiments demonstrate the statistical accuracy and interpretability of the proposed method.},
  archive  = {J},
  author   = {Yiyuan She and Jiahui Shen and Chao Zhang},
  doi      = {10.1111/rssb.12492},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {912-932},
  title    = {Supervised multivariate learning with simultaneous feature auto-grouping and dimension reduction},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The sceptical bayes factor for the assessment of replication
success. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>84</em>(3), 879–911. (<a
href="https://doi.org/10.1111/rssb.12491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Replication studies are increasingly conducted but there is no established statistical criterion for replication success. We propose a novel approach combining reverse-Bayes analysis with Bayesian hypothesis testing: a sceptical prior is determined for the effect size such that the original finding is no longer convincing in terms of a Bayes factor. This prior is then contrasted to an advocacy prior (the reference posterior of the effect size based on the original study), and replication success is declared if the replication data favour the advocacy over the sceptical prior at a higher level than the original data favoured the sceptical prior over the null hypothesis. The sceptical Bayes factor is the highest level where replication success can be declared. A comparison to existing methods reveals that the sceptical Bayes factor combines several notions of replicability: it ensures that both studies show sufficient evidence against the null and penalises incompatibility of their effect estimates. Analysis of asymptotic properties and error rates, as well as case studies from the Social Sciences Replication Project show the advantages of the method for the assessment of replicability.},
  archive  = {J},
  author   = {Samuel Pawel and Leonhard Held},
  doi      = {10.1111/rssb.12491},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {879-911},
  title    = {The sceptical bayes factor for the assessment of replication success},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Empirical bayes PCA in high dimensions. <em>Journal of the
Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(3), 853–878. (<a
href="https://doi.org/10.1111/rssb.12490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {When the dimension of data is comparable to or larger than the number of data samples, principal components analysis (PCA) may exhibit problematic high-dimensional noise. In this work, we propose an empirical Bayes PCA method that reduces this noise by estimating a joint prior distribution for the principal components. EB-PCA is based on the classical Kiefer–Wolfowitz non-parametric maximum likelihood estimator for empirical Bayes estimation, distributional results derived from random matrix theory for the sample PCs and iterative refinement using an approximate message passing (AMP) algorithm. In theoretical ‘spiked’ models, EB-PCA achieves Bayes-optimal estimation accuracy in the same settings as an oracle Bayes AMP procedure that knows the true priors. Empirically, EB-PCA significantly improves over PCA when there is strong prior structure, both in simulation and on quantitative benchmarks constructed from the 1000 Genomes Project and the International HapMap Project. An illustration is presented for analysis of gene expression data obtained by single-cell RNA-seq.},
  archive  = {J},
  author   = {Xinyi Zhong and Chang Su and Zhou Fan},
  doi      = {10.1111/rssb.12490},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {853-878},
  title    = {Empirical bayes PCA in high dimensions},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). False discovery rate control with e-values. <em>Journal of
the Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(3), 822–852. (<a
href="https://doi.org/10.1111/rssb.12489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {E-values have gained attention as potential alternatives to p-values as measures of uncertainty, significance and evidence. In brief, e-values are realized by random variables with expectation at most one under the null; examples include betting scores, (point null) Bayes factors, likelihood ratios and stopped supermartingales. We design a natural analogue of the Benjamini-Hochberg (BH) procedure for false discovery rate (FDR) control that utilizes e-values, called the e-BH procedure, and compare it with the standard procedure for p-values. One of our central results is that, unlike the usual BH procedure, the e-BH procedure controls the FDR at the desired level—with no correction—for any dependence structure between the e-values. We illustrate that the new procedure is convenient in various settings of complicated dependence, structured and post-selection hypotheses, and multi-armed bandit problems. Moreover, the BH procedure is a special case of the e-BH procedure through calibration between p-values and e-values. Overall, the e-BH procedure is a novel, powerful and general tool for multiple testing under dependence, that is complementary to the BH procedure, each being an appropriate choice in different applications.},
  archive  = {J},
  author   = {Ruodu Wang and Aaditya Ramdas},
  doi      = {10.1111/rssb.12489},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {822-852},
  title    = {False discovery rate control with e-values},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric estimation for causal mediation analysis with
multiple causally ordered mediators. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(3), 794–821. (<a
href="https://doi.org/10.1111/rssb.12487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Causal mediation analysis concerns the pathways through which a treatment affects an outcome. While most of the mediation literature focuses on settings with a single mediator, a flourishing line of research has examined settings involving multiple mediators, under which path-specific effects (PSEs) are often of interest. We consider estimation of PSEs when the treatment effect operates through K (≥ 1) causally ordered, possibly multivariate mediators. In this setting, the PSEs for many causal paths are not nonparametrically identified, and we focus on a set of PSEs that are identified under Pearl&#39;s nonparametric structural equation model. These PSEs are defined as contrasts between the expectations of 2 K + 1 potential outcomes and identified via what we call the generalized mediation functional (GMF). We introduce an array of regression-imputation, weighting and ‘hybrid’ estimators, and, in particular, two K + 2-robust and locally semiparametric efficient estimators for the GMF. The latter estimators are well suited to the use of data-adaptive methods for estimating their nuisance functions. We establish the rate conditions required of the nuisance functions for semiparametric efficiency. We also discuss how our framework applies to several estimands that may be of particular interest in empirical applications. The proposed estimators are illustrated with a simulation study and an empirical example.},
  archive  = {J},
  author   = {Xiang Zhou},
  doi      = {10.1111/rssb.12487},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {794-821},
  title    = {Semiparametric estimation for causal mediation analysis with multiple causally ordered mediators},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical inference of the value function for
reinforcement learning in infinite-horizon settings. <em>Journal of the
Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(3), 765–793. (<a
href="https://doi.org/10.1111/rssb.12465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Reinforcement learning is a general technique that allows an agent to learn an optimal policy and interact with an environment in sequential decision-making problems. The goodness of a policy is measured by its value function starting from some initial state. The focus of this paper was to construct confidence intervals (CIs) for a policy’s value in infinite horizon settings where the number of decision points diverges to infinity. We propose to model the action-value state function (Q-function) associated with a policy based on series/sieve method to derive its confidence interval. When the target policy depends on the observed data as well, we propose a S equenti A l V alue E valuation (SAVE) method to recursively update the estimated policy and its value estimator. As long as either the number of trajectories or the number of decision points diverges to infinity, we show that the proposed CI achieves nominal coverage even in cases where the optimal policy is not unique. Simulation studies are conducted to back up our theoretical findings. We apply the proposed method to a dataset from mobile health studies and find that reinforcement learning algorithms could help improve patient’s health status. A Python implementation of the proposed procedure is available at https://github.com/shengzhang37/SAVE .},
  archive  = {J},
  author   = {Chengchun Shi and Sheng Zhang and Wenbin Lu and Rui Song},
  doi      = {10.1111/rssb.12465},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {765-793},
  title    = {Statistical inference of the value function for reinforcement learning in infinite-horizon settings},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian estimation and comparison of conditional moment
models. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>84</em>(3), 740–764. (<a
href="https://doi.org/10.1111/rssb.12484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We consider the Bayesian analysis of models in which the unknown distribution of the outcomes is specified up to a set of conditional moment restrictions. The non-parametric exponentially tilted empirical likelihood function is constructed to satisfy a sequence of unconditional moments based on an increasing (in sample size) vector of approximating functions (such as tensor splines based on the splines of each conditioning variable). For any given sample size, results are robust to the number of expanded moments. We derive Bernstein–von Mises theorems for the behaviour of the posterior distribution under both correct and incorrect specification of the conditional moments, subject to growth rate conditions (slower under misspecification) on the number of approximating functions. A large-sample theory for comparing different conditional moment models is also developed. The central result is that the marginal likelihood criterion selects the model that is less misspecified. We also introduce sparsity-based model search for high-dimensional conditioning variables, and provide efficient Markov chain Monte Carlo computations for high-dimensional parameters. Along with clarifying examples, the framework is illustrated with real data applications to risk-factor determination in finance, and causal inference under conditional ignorability.},
  archive  = {J},
  author   = {Siddhartha Chib and Minchul Shin and Anna Simoni},
  doi      = {10.1111/rssb.12484},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {740-764},
  title    = {Bayesian estimation and comparison of conditional moment models},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Authors’ reply to the discussion of “assumption-lean
inference for generalised linear model parameters” by vansteelandt and
dukes. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>84</em>(3), 729–739. (<a
href="https://doi.org/10.1111/rssb.12536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Stijn Vansteelandt and Oliver Dukes},
  doi     = {10.1111/rssb.12536},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {729-739},
  title   = {Authors&#39; reply to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Niwen zhou and xu guo’s contribution to the discussion of
“assumption-lean inference for generalised linear model parameters” by
vansteelandt and dukes. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(3), 727–729. (<a
href="https://doi.org/10.1111/rssb.12535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Niwen Zhou and Xu Guo},
  doi     = {10.1111/rssb.12535},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {727-729},
  title   = {Niwen zhou and xu guo’s contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Jiwei zhao’s contribution to the discussion of
“assumption-lean inference for generalised linear model parameters” by
vansteelandt and dukes. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(3), 725–726. (<a
href="https://doi.org/10.1111/rssb.12534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Jiwei Zhao},
  doi     = {10.1111/rssb.12534},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {725-726},
  title   = {Jiwei zhao’s contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Eric j tchetgen tchetgen’s contribution to the discussion of
“assumption-lean inference for generalised linear model parameters” by
vansteelandt and dukes. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(3), 723–725. (<a
href="https://doi.org/10.1111/rssb.12533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Eric J. Tchetgen Tchetgen},
  doi     = {10.1111/rssb.12533},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {723-725},
  title   = {Eric j tchetgen tchetgen’s contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Yanbo tang’s contribution to the discussion of
“assumption-lean inference for generalised linear model parameters” by
vansteelandt and dukes. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(3), 722–723. (<a
href="https://doi.org/10.1111/rssb.12532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Yanbo Tang},
  doi     = {10.1111/rssb.12532},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {722-723},
  title   = {Yanbo tang&#39;s contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ilya shpitser’s contribution to the discussion of
“assumption-lean inference for generalised linear model parameters” by
vansteelandt and dukes. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(3), 720–721. (<a
href="https://doi.org/10.1111/rssb.12531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Ilya Shpitser},
  doi     = {10.1111/rssb.12531},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {720-721},
  title   = {Ilya shpitser’s contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Thomas s. Richardson’s contribution to the discussion of
“assumption-lean inference for generalised linear model parameters” by
vansteelandt and dukes. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(3), 719–720. (<a
href="https://doi.org/10.1111/rssb.12530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Thomas S. Richardson},
  doi     = {10.1111/rssb.12530},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {719-720},
  title   = {Thomas s. richardson’s contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rachael v. Phillips and mark j. Van der laan’s contribution
to the discussion of “assumption-lean inference for generalised linear
model parameters” by vansteelandt and dukes. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(3), 717–718. (<a
href="https://doi.org/10.1111/rssb.12529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Rachael V. Phillips and Mark J. van der Laan},
  doi     = {10.1111/rssb.12529},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {717-718},
  title   = {Rachael v. phillips and mark j. van der laan’s contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Elizabeth l ogburn, junhui cai, arun k kuchibhotla, richard
a berk and andreas buja’s contribution to the discussion of
“assumption-lean inference for generalised linear model parameters” by
vansteelandt and dukes. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(3), 715–716. (<a
href="https://doi.org/10.1111/rssb.12528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Elizabeth L. Ogburn and Junhui Cai and Arun K. Kuchibhotla and Richard A. Berk and Andreas Buja},
  doi     = {10.1111/rssb.12528},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {715-716},
  title   = {Elizabeth l ogburn, junhui cai, arun k kuchibhotla, richard a berk and andreas buja’s contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Michael lavine and james hodges’ contribution to the
discussion of “assumption-lean inference for generalised linear model
parameters” by vansteelandt and dukes. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(3), 713–714. (<a
href="https://doi.org/10.1111/rssb.12527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Michael Lavine and James Hodges},
  doi     = {10.1111/rssb.12527},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {713-714},
  title   = {Michael lavine and james hodges’ contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kuldeep kumar’s contribution to the discussion of
“assumption-lean inference for generalised linear model parameters” by
vansteelandt and dukes. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(3), 712–713. (<a
href="https://doi.org/10.1111/rssb.12526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Kuldeep Kumar},
  doi     = {10.1111/rssb.12526},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {712-713},
  title   = {Kuldeep kumar’s contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ian hunt’s contribution to the discussion of
“assumption-lean inference for generalised linear model parameters” by
vansteelandt and dukes. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(3), 711–712. (<a
href="https://doi.org/10.1111/rssb.12525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Ian Hunt},
  doi     = {10.1111/rssb.12525},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {711-712},
  title   = {Ian hunt&#39;s contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Oliver hines and karla diaz-ordazʼs contribution to the
discussion of “assumption-lean inference for generalised linear model
parameters” by vansteelandt and dukes. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(3), 709–710. (<a
href="https://doi.org/10.1111/rssb.12524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Oliver Hines and Karla Diaz-Ordaz},
  doi     = {10.1111/rssb.12524},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {709-710},
  title   = {Oliver hines and karla diaz-ordazʼs contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Chaohua dong, jiti gao and oliver linton’s contribution to
the discussion of “assumption-lean inference for generalised linear
model parameters” by vansteelandt and dukes. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(3), 707–708. (<a
href="https://doi.org/10.1111/rssb.12523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Chaohua Dong and Jiti Gao and Oliver Linton},
  doi     = {10.1111/rssb.12523},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {707-708},
  title   = {Chaohua dong, jiti gao and oliver linton’s contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Anna choi and weng kee wong’s contribution to the discussion
of “assumption-lean inference for generalised linear model parameters”
by vansteelandt and dukes. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(3), 705–706. (<a
href="https://doi.org/10.1111/rssb.12522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Anna Choi and Weng Kee Wong},
  doi     = {10.1111/rssb.12522},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {705-706},
  title   = {Anna choi and weng kee wong’s contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Andreas buja, richard a. Berk, arun k. Kuchibhotla, linda
zhao and ed george’s contribution to the discussion of “assumption-lean
inference for generalised linear model parameters” by vansteelandt and
dukes. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>84</em>(3), 703–705. (<a
href="https://doi.org/10.1111/rssb.12521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Andreas Buja and Richard A. Berk and Arun K. Kuchibhotla and Linda Zhao and Ed George},
  doi     = {10.1111/rssb.12521},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {703-705},
  title   = {Andreas buja, richard a. berk, arun k. kuchibhotla, linda zhao and ed george’s contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blair bilodeau’s contribution to the discussion of
“assumption-lean inference for generalised linear model parameters” by
vansteelandt and dukes. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(3), 701–702. (<a
href="https://doi.org/10.1111/rssb.12520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Blair Bilodeau},
  doi     = {10.1111/rssb.12520},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {701-702},
  title   = {Blair bilodeau&#39;s contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pallavi basuʼs contribution to the discussion of
“assumption-lean inference for generalised linear model parameters” by
vansteelandt and dukes. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(3), 700–701. (<a
href="https://doi.org/10.1111/rssb.12519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Pallavi Basu},
  doi     = {10.1111/rssb.12519},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {700-701},
  title   = {Pallavi basuʼs contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Christian hennig’s contribution to the discussion of
“assumption-lean inference for generalised linear model parameters” by
vansteelandt and dukes. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(3), 698–699. (<a
href="https://doi.org/10.1111/rssb.12518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Christian Hennig},
  doi     = {10.1111/rssb.12518},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {698-699},
  title   = {Christian hennig&#39;s contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heather battey’s contribution to the discussion of
“assumption-lean inference for generalised linear model parameters” by
vansteelandt and dukes. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(3), 696–698. (<a
href="https://doi.org/10.1111/rssb.12517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Heather Battey},
  doi     = {10.1111/rssb.12517},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {696-698},
  title   = {Heather battey’s contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mats j stensrud and aaron l. Sarvet’s contribution to the
discussion of “assumption-lean inference for generalised linear model
parameters” by vansteelandt and dukes. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(3), 694–696. (<a
href="https://doi.org/10.1111/rssb.12516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Mats J. Stensrud and Aaron L. Sarvet},
  doi     = {10.1111/rssb.12516},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {694-696},
  title   = {Mats j stensrud and aaron l. sarvet’s contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Peng ding’s contribution to the discussion of
“assumption-lean inference for generalised linear model parameters” by
vansteelandt and dukes. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(3), 691–693. (<a
href="https://doi.org/10.1111/rssb.12515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Peng Ding},
  doi     = {10.1111/rssb.12515},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {691-693},
  title   = {Peng ding’s contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Seconder of the vote of thanks to vansteelandt and dukes and
contribution to the discussion of “assumption-lean inference for
generalised linear model parameters.” <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(3), 689–691. (<a
href="https://doi.org/10.1111/rssb.12514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Vanessa Didelez},
  doi     = {10.1111/rssb.12514},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {689-691},
  title   = {Seconder of the vote of thanks to vansteelandt and dukes and contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Proposer of the vote of thanks and contribution to the
discussion of “assumption-lean inference for generalised linear model
parameters” by vansteelandt and dukes. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(3), 686–689. (<a
href="https://doi.org/10.1111/rssb.12513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Rhian M. Daniel},
  doi     = {10.1111/rssb.12513},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {686-689},
  title   = {Proposer of the vote of thanks and contribution to the discussion of ‘Assumption-lean inference for generalised linear model parameters’ by vansteelandt and dukes},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Assumption-lean inference for generalised linear model
parameters. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>84</em>(3), 657–685. (<a
href="https://doi.org/10.1111/rssb.12504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Inference for the parameters indexing generalised linear models is routinely based on the assumption that the model is correct and a priori specified. This is unsatisfactory because the chosen model is usually the result of a data-adaptive model selection process, which may induce excess uncertainty that is not usually acknowledged. Moreover, the assumptions encoded in the chosen model rarely represent some a priori known, ground truth, making standard inferences prone to bias, but also failing to give a pure reflection of the information that is contained in the data. Inspired by developments on assumption-free inference for so-called projection parameters, we here propose novel nonparametric definitions of main effect estimands and effect modification estimands. These reduce to standard main effect and effect modification parameters in generalised linear models when these models are correctly specified, but have the advantage that they continue to capture respectively the (conditional) association between two variables, or the degree to which two variables interact in their association with outcome, even when these models are misspecified. We achieve an assumption-lean inference for these estimands on the basis of their efficient influence function under the nonparametric model while invoking flexible data-adaptive (e.g. machine learning) procedures.},
  archive  = {J},
  author   = {Stijn Vansteelandt and Oliver Dukes},
  doi      = {10.1111/rssb.12504},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {657-685},
  title    = {Assumption-lean inference for generalised linear model parameters},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SIMPLE: Statistical inference on membership profiles in
large networks. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>84</em>(2), 630–653. (<a
href="https://doi.org/10.1111/rssb.12505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Network data are prevalent in many contemporary big data applications in which a common interest is to unveil important latent links between different pairs of nodes. Yet a simple fundamental question of how to precisely quantify the statistical uncertainty associated with the identification of latent links still remains largely unexplored. In this paper, we propose the method of statistical inference on membership profiles in large networks (SIMPLE) in the setting of degree-corrected mixed membership model, where the null hypothesis assumes that the pair of nodes share the same profile of community memberships. In the simpler case of no degree heterogeneity, the model reduces to the mixed membership model for which an alternative more robust test is also proposed. Both tests are of the Hotelling-type statistics based on the rows of empirical eigenvectors or their ratios, whose asymptotic covariance matrices are very challenging to derive and estimate. Nevertheless, their analytical expressions are unveiled and the unknown covariance matrices are consistently estimated. Under some mild regularity conditions, we establish the exact limiting distributions of the two forms of SIMPLE test statistics under the null hypothesis and contiguous alternative hypothesis. They are the chi-square distributions and the noncentral chi-square distributions, respectively, with degrees of freedom depending on whether the degrees are corrected or not. We also address the important issue of estimating the unknown number of communities and establish the asymptotic properties of the associated test statistics. The advantages and practical utility of our new procedures in terms of both size and power are demonstrated through several simulation examples and real network applications.},
  archive  = {J},
  author   = {Jianqing Fan and Yingying Fan and Xiao Han and Jinchi Lv},
  doi      = {10.1111/rssb.12505},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {630-653},
  title    = {SIMPLE: Statistical inference on membership profiles in large networks},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Functional structural equation model. <em>Journal of the
Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(2), 600–629. (<a
href="https://doi.org/10.1111/rssb.12471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we introduce a functional structural equation model for estimating directional relations from multivariate functional data. We decouple the estimation into two major steps: directional order determination and selection through sparse functional regression. We first propose a score function at the linear operator level, and show that its minimization can recover the true directional order when the relation between each function and its parental functions is nonlinear. We then develop a sparse functional additive regression, where both the response and the multivariate predictors are functions and the regression relation is additive and nonlinear. We also propose strategies to speed up the computation and scale up our method. In theory, we establish the consistencies of order determination, sparse functional additive regression, and directed acyclic graph estimation, while allowing both the dimension of the Karhunen–Loéve expansion coefficients and the number of random functions to diverge with the sample size. We illustrate the efficacy of our method through simulations, and an application to brain effective connectivity analysis.},
  archive  = {J},
  author   = {Kuang-Yao Lee and Lexin Li},
  doi      = {10.1111/rssb.12471},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {600-629},
  title    = {Functional structural equation model},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graphical criteria for efficient total effect estimation via
adjustment in causal linear models. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>84</em>(2),
579–599. (<a href="https://doi.org/10.1111/rssb.12451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Covariate adjustment is a commonly used method for total causal effect estimation. In recent years, graphical criteria have been developed to identify all valid adjustment sets, that is, all covariate sets that can be used for this purpose. Different valid adjustment sets typically provide total causal effect estimates of varying accuracies. Restricting ourselves to causal linear models, we introduce a graphical criterion to compare the asymptotic variances provided by certain valid adjustment sets. We employ this result to develop two further graphical tools. First, we introduce a simple variance decreasing pruning procedure for any given valid adjustment set. Second, we give a graphical characterization of a valid adjustment set that provides the optimal asymptotic variance among all valid adjustment sets. Our results depend only on the graphical structure and not on the specific error variances or edge coefficients of the underlying causal linear model. They can be applied to directed acyclic graphs (DAGs), completed partially directed acyclic graphs (CPDAGs) and maximally oriented partially directed acyclic graphs (maximal PDAGs). We present simulations and a real data example to support our results and show their practical applicability.},
  archive  = {J},
  author   = {Leonard Henckel and Emilija Perković and Marloes H. Maathuis},
  doi      = {10.1111/rssb.12451},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {579-599},
  title    = {Graphical criteria for efficient total effect estimation via adjustment in causal linear models},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A kernel-expanded stochastic neural network. <em>Journal of
the Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(2), 547–578. (<a
href="https://doi.org/10.1111/rssb.12496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The deep neural network suffers from many fundamental issues in machine learning. For example, it often gets trapped into a local minimum in training, and its prediction uncertainty is hard to be assessed. To address these issues, we propose the so-called kernel-expanded stochastic neural network (K-StoNet) model, which incorporates support vector regression as the first hidden layer and reformulates the neural network as a latent variable model. The former maps the input vector into an infinite dimensional feature space via a radial basis function kernel, ensuring the absence of local minima on its training loss surface. The latter breaks the high-dimensional non-convex neural network training problem into a series of low-dimensional convex optimization problems, and enables its prediction uncertainty easily assessed. The K-StoNet can be easily trained using the imputation-regularized optimization algorithm. Compared to traditional deep neural networks, K-StoNet possesses a theoretical guarantee to asymptotically converge to the global optimum and enables the prediction uncertainty easily assessed. The performances of the new model in training, prediction and uncertainty quantification are illustrated by simulated and real data examples.},
  archive  = {J},
  author   = {Yan Sun and Faming Liang},
  doi      = {10.1111/rssb.12496},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {547-578},
  title    = {A kernel-expanded stochastic neural network},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prediction and outlier detection in classification problems.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>84</em>(2), 524–546. (<a
href="https://doi.org/10.1111/rssb.12443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We consider the multi-class classification problem when the training data and the out-of-sample test data may have different distributions and propose a method called BCOPS (balanced and conformal optimized prediction sets). BCOPS constructs a prediction set C ( x ) as a subset of class labels, possibly empty. It tries to optimize the out-of-sample performance, aiming to include the correct class and to detect outliers x as often as possible. BCOPS returns no prediction (corresponding to C ( x ) equal to the empty set) if it infers x to be an outlier. The proposed method combines supervised learning algorithms with conformal prediction to minimize a misclassification loss averaged over the out-of-sample distribution. The constructed prediction sets have a finite sample coverage guarantee without distributional assumptions. We also propose a method to estimate the outlier detection rate of a given procedure. We prove asymptotic consistency and optimality of our proposals under suitable assumptions and illustrate our methods on real data examples.},
  archive  = {J},
  author   = {Leying Guan and Robert Tibshirani},
  doi      = {10.1111/rssb.12443},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {524-546},
  title    = {Prediction and outlier detection in classification problems},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The barker proposal: Combining robustness and efficiency in
gradient-based MCMC. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(2), 496–523. (<a
href="https://doi.org/10.1111/rssb.12482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {There is a tension between robustness and efficiency when designing Markov chain Monte Carlo (MCMC) sampling algorithms. Here we focus on robustness with respect to tuning parameters, showing that more sophisticated algorithms tend to be more sensitive to the choice of step-size parameter and less robust to heterogeneity of the distribution of interest. We characterise this phenomenon by studying the behaviour of spectral gaps as an increasingly poor step-size is chosen for the algorithm. Motivated by these considerations, we propose a novel and simple gradient-based MCMC algorithm, inspired by the classical Barker accept-reject rule, with improved robustness properties. Extensive theoretical results, dealing with robustness to tuning, geometric ergodicity and scaling with dimension, suggest that the novel scheme combines the robustness of simple schemes with the efficiency of gradient-based ones. We show numerically that this type of robustness is particularly beneficial in the context of adaptive MCMC, giving examples where our proposed scheme significantly outperforms state-of-the-art alternatives.},
  archive  = {J},
  author   = {Samuel Livingstone and Giacomo Zanella},
  doi      = {10.1111/rssb.12482},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {496-523},
  title    = {The barker proposal: Combining robustness and efficiency in gradient-based MCMC},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model identification via total frobenius norm of
multivariate spectra. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(2), 473–495. (<a
href="https://doi.org/10.1111/rssb.12480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We study the integral of the Frobenius norm as a measure of the discrepancy between two multivariate spectra. Such a measure can be used to fit time series models, and ensures proximity between model and process at all frequencies of the spectral density—this is more demanding than Kullback–Leibler discrepancy, which is instead related to one-step ahead forecasting performance. We develop new asymptotic results for linear and quadratic functionals of the periodogram, and make two applications of the integrated Frobenius norm: (i) fitting time series models, and (ii) testing whether model residuals are white noise. Model fitting results are further specialized to the case of structural time series models, wherein co-integration rank testing is formally developed. Both applications are studied through simulation studies, as well as illustrations on inflation and construction data. The numerical results show that the proposed estimator can fit moderate- to large-dimensional structural time series in real time, an option that is lacking in current literature.},
  archive  = {J},
  author   = {Tucker S. McElroy and Anindya Roy},
  doi      = {10.1111/rssb.12480},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {473-495},
  title    = {Model identification via total frobenius norm of multivariate spectra},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient learning of optimal individualized treatment rules
for heteroscedastic or misspecified treatment-free effect models.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>84</em>(2), 440–472. (<a
href="https://doi.org/10.1111/rssb.12474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent development in data-driven decision science has seen great advances in individualized decision making. Given data with individual covariates, treatment assignments and outcomes, researchers can search for the optimal individualized treatment rule (ITR) that maximizes the expected outcome. Existing methods typically require initial estimation of some nuisance models. The double robustness property that can protect from misspecification of either the treatment-free effect or the propensity score has been widely advocated. However, when model misspecification exists, a doubly robust estimate can be consistent but may suffer from downgraded efficiency. Other than potential misspecified nuisance models, most existing methods do not account for the potential problem when the variance of outcome is heterogeneous among covariates and treatment. We observe that such heteroscedasticity can greatly affect the estimation efficiency of the optimal ITR. In this paper, we demonstrate that the consequences of misspecified treatment-free effect and heteroscedasticity can be unified as a covariate-treatment dependent variance of residuals. To improve efficiency of the estimated ITR, we propose an Efficient Learning (E-Learning) framework for finding an optimal ITR in the multi-armed treatment setting. We show that the proposed E-Learning is optimal among a regular class of semiparametric estimates that can allow treatment-free effect misspecification. In our simulation study, E-Learning demonstrates its effectiveness if one of or both misspecified treatment-free effect and heteroscedasticity exist. Our analysis of a type 2 diabetes mellitus (T2DM) observational study also suggests the improved efficiency of E-Learning.},
  archive  = {J},
  author   = {Weibin Mo and Yufeng Liu},
  doi      = {10.1111/rssb.12474},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {440-472},
  title    = {Efficient learning of optimal individualized treatment rules for heteroscedastic or misspecified treatment-free effect models},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph based gaussian processes on restricted domains.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>84</em>(2), 414–439. (<a
href="https://doi.org/10.1111/rssb.12486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In nonparametric regression, it is common for the inputs to fall in a restricted subset of Euclidean space. Typical kernel-based methods that do not take into account the intrinsic geometry of the domain across which observations are collected may produce sub-optimal results. In this article, we focus on solving this problem in the context of Gaussian process (GP) models, proposing a new class of Graph Laplacian based GPs (GL-GPs), which learn a covariance that respects the geometry of the input domain. As the heat kernel is intractable computationally, we approximate the covariance using finitely-many eigenpairs of the Graph Laplacian (GL). The GL is constructed from a kernel which depends only on the Euclidean coordinates of the inputs. Hence, we can benefit from the full knowledge about the kernel to extend the covariance structure to newly arriving samples by a Nyström type extension. We provide substantial theoretical support for the GL-GP methodology, and illustrate performance gains in various applications.},
  archive  = {J},
  author   = {David B. Dunson and Hau-Tieng Wu and Nan Wu},
  doi      = {10.1111/rssb.12486},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {414-439},
  title    = {Graph based gaussian processes on restricted domains},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Selective inference for effect modification via the lasso.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>84</em>(2), 382–413. (<a
href="https://doi.org/10.1111/rssb.12483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Effect modification occurs when the effect of the treatment on an outcome varies according to the level of other covariates and often has important implications in decision-making. When there are tens or hundreds of covariates, it becomes necessary to use the observed data to select a simpler model for effect modification and then make valid statistical inference. We propose a two-stage procedure to solve this problem. First, we use Robinson&#39;s transformation to decouple the nuisance parameters from the treatment effect of interest and use machine learning algorithms to estimate the nuisance parameters. Next, after plugging in the estimates of the nuisance parameters, we use the lasso to choose a low-complexity model for effect modification. Compared to a full model consisting of all the covariates, the selected model is much more interpretable. Compared to the univariate subgroup analyses, the selected model greatly reduces the number of false discoveries. We show that the conditional selective inference for the selected model is asymptotically valid given the rate assumptions in classical semiparametric regression. Extensive simulation studies are conducted to verify the asymptotic results and an epidemiological application is used to demonstrate the method.},
  archive  = {J},
  author   = {Qingyuan Zhao and Dylan S. Small and Ashkan Ertefaie},
  doi      = {10.1111/rssb.12483},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {382-413},
  title    = {Selective inference for effect modification via the lasso},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synthetic controls with staggered adoption. <em>Journal of
the Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(2), 351–381. (<a
href="https://doi.org/10.1111/rssb.12448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Staggered adoption of policies by different units at different times creates promising opportunities for observational causal inference. Estimation remains challenging, however, and common regression methods can give misleading results. A promising alternative is the synthetic control method (SCM), which finds a weighted average of control units that closely balances the treated unit’s pre-treatment outcomes. In this paper, we generalize SCM, originally designed to study a single treated unit, to the staggered adoption setting. We first bound the error for the average effect and show that it depends on both the imbalance for each treated unit separately and the imbalance for the average of the treated units. We then propose ‘partially pooled’ SCM weights to minimize a weighted combination of these measures; approaches that focus only on balancing one of the two components can lead to bias. We extend this approach to incorporate unit-level intercept shifts and auxiliary covariates. We assess the performance of the proposed method via extensive simulations and apply our results to the question of whether teacher collective bargaining leads to higher school spending, finding minimal impacts. We implement the proposed method in the augsynth R package.},
  archive  = {J},
  author   = {Eli Ben-Michael and Avi Feller and Jesse Rothstein},
  doi      = {10.1111/rssb.12448},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {351-381},
  title    = {Synthetic controls with staggered adoption},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-reversible parallel tempering: A scalable highly
parallel MCMC scheme. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(2), 321–350. (<a
href="https://doi.org/10.1111/rssb.12464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Parallel tempering (PT) methods are a popular class of Markov chain Monte Carlo schemes used to sample complex high-dimensional probability distributions. They rely on a collection of N interacting auxiliary chains targeting tempered versions of the target distribution to improve the exploration of the state space. We provide here a new perspective on these highly parallel algorithms and their tuning by identifying and formalizing a sharp divide in the behaviour and performance of reversible versus non-reversible PT schemes. We show theoretically and empirically that a class of non-reversible PT methods dominates its reversible counterparts and identify distinct scaling limits for the non-reversible and reversible schemes, the former being a piecewise-deterministic Markov process and the latter a diffusion. These results are exploited to identify the optimal annealing schedule for non-reversible PT and to develop an iterative scheme approximating this schedule. We provide a wide range of numerical examples supporting our theoretical and methodological contributions. The proposed methodology is applicable to sample from a distribution π with a density L with respect to a reference distribution π 0 and compute the normalizing constant ∫ L d π 0 . A typical use case is when π 0 is a prior distribution, L a likelihood function and π the corresponding posterior distribution.},
  archive  = {J},
  author   = {Saifuddin Syed and Alexandre Bouchard-Côté and George Deligiannidis and Arnaud Doucet},
  doi      = {10.1111/rssb.12464},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {321-350},
  title    = {Non-reversible parallel tempering: A scalable highly parallel MCMC scheme},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gaussian prepivoting for finite population causal inference.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>84</em>(2), 295–320. (<a
href="https://doi.org/10.1111/rssb.12439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In finite population causal inference exact randomization tests can be constructed for sharp null hypotheses, hypotheses which impute the missing potential outcomes. Oftentimes inference is instead desired for the weak null that the sample average of the treatment effects takes on a particular value while leaving the subject-specific treatment effects unspecified. Tests valid for sharp null hypotheses can be anti-conservative should only the weak null hold. We develop a general framework for unifying modes of inference for sharp and weak nulls, wherein a single procedure simultaneously delivers exact inference for sharp nulls and asymptotically valid inference for weak nulls. We employ randomization tests based upon prepivoted test statistics, wherein a test statistic is first transformed by a suitably constructed cumulative distribution function and its randomization distribution assuming the sharp null is then enumerated. For a large class of test statistics, we show that prepivoting may be accomplished by employing the push-forward of a sample-based Gaussian measure based upon a suitable covariance estimator. The approach enumerates the randomization distribution (assuming the sharp null) of a p -value for a large-sample test known to be valid under the weak null, and uses the resulting randomization distribution for inference. The versatility of the method is demonstrated through many examples, including rerandomized designs and regression-adjusted estimators in completely randomized designs.},
  archive  = {J},
  author   = {Peter L. Cohen and Colin B. Fogarty},
  doi      = {10.1111/rssb.12439},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {295-320},
  title    = {Gaussian prepivoting for finite population causal inference},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On efficient dimension reduction with respect to the
interaction between two response variables. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(2), 269–294. (<a
href="https://doi.org/10.1111/rssb.12477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we propose the novel theory and methodologies for dimension reduction with respect to the interaction between two response variables, which is a new research problem that has wide applications in missing data analysis, causal inference, graphical models, etc. We formulate the parameters of interest to be the locally and the globally efficient dimension reduction subspaces, and justify the generality of the corresponding low-dimensional assumption. We then construct estimating equations that characterize these parameters, using which we develop a generic family of consistent, model-free and easily implementable dimension reduction methods called the dual inverse regression methods. We also build the theory regarding the existence of the globally efficient dimension reduction subspace, and provide a handy way to check this in practice. The proposed work differs fundamentally from the literature of sufficient dimension reduction in terms of the research interest, the assumption adopted, the estimation methods and the corresponding applications, and it potentially creates a new paradigm of dimension reduction research. Its usefulness is illustrated by simulation studies and a real data example at the end.},
  archive  = {J},
  author   = {Wei Luo},
  doi      = {10.1111/rssb.12477},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {269-294},
  title    = {On efficient dimension reduction with respect to the interaction between two response variables},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-dimensional, multiscale online changepoint detection.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>84</em>(1), 234–266. (<a
href="https://doi.org/10.1111/rssb.12447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We introduce a new method for high-dimensional, online changepoint detection in settings where a p -variate Gaussian data stream may undergo a change in mean. The procedure works by performing likelihood ratio tests against simple alternatives of different scales in each coordinate, and then aggregating test statistics across scales and coordinates. The algorithm is online in the sense that both its storage requirements and worst-case computational complexity per new observation are independent of the number of previous observations; in practice, it may even be significantly faster than this. We prove that the patience, or average run length under the null, of our procedure is at least at the desired nominal level, and provide guarantees on its response delay under the alternative that depend on the sparsity of the vector of mean change. Simulations confirm the practical effectiveness of our proposal, which is implemented in the R package ocd , and we also demonstrate its utility on a seismology data set.},
  archive  = {J},
  author   = {Yudong Chen and Tengyao Wang and Richard J. Samworth},
  doi      = {10.1111/rssb.12447},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {234-266},
  title    = {High-dimensional, multiscale online changepoint detection},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-dimensional quantile regression: Convolution smoothing
and concave regularization. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>84</em>(1),
205–233. (<a href="https://doi.org/10.1111/rssb.12485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {ℓ 1 -penalized quantile regression (QR) is widely used for analysing high-dimensional data with heterogeneity. It is now recognized that the ℓ 1 -penalty introduces non-negligible estimation bias, while a proper use of concave regularization may lead to estimators with refined convergence rates and oracle properties as the signal strengthens. Although folded concave penalized M -estimation with strongly convex loss functions have been well studied, the extant literature on QR is relatively silent. The main difficulty is that the quantile loss is piecewise linear: it is non-smooth and has curvature concentrated at a single point. To overcome the lack of smoothness and strong convexity, we propose and study a convolution-type smoothed QR with iteratively reweighted ℓ 1 -regularization. The resulting smoothed empirical loss is twice continuously differentiable and (provably) locally strongly convex with high probability. We show that the iteratively reweighted ℓ 1 -penalized smoothed QR estimator, after a few iterations, achieves the optimal rate of convergence, and moreover, the oracle rate and the strong oracle property under an almost necessary and sufficient minimum signal strength condition. Extensive numerical studies corroborate our theoretical results.},
  archive  = {J},
  author   = {Kean Ming Tan and Lan Wang and Wen-Xin Zhou},
  doi      = {10.1111/rssb.12485},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {205-233},
  title    = {High-dimensional quantile regression: Convolution smoothing and concave regularization},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A graph-theoretic approach to randomization tests of causal
effects under general interference. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>84</em>(1),
174–204. (<a href="https://doi.org/10.1111/rssb.12478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Interference exists when a unit&#39;s outcome depends on another unit&#39;s treatment assignment. For example, intensive policing on one street could have a spillover effect on neighbouring streets. Classical randomization tests typically break down in this setting because many null hypotheses of interest are no longer sharp under interference. A promising alternative is to instead construct a conditional randomization test on a subset of units and assignments for which a given null hypothesis is sharp. Finding these subsets is challenging, however, and existing methods are limited to special cases or have limited power. In this paper, we propose valid and easy-to-implement randomization tests for a general class of null hypotheses under arbitrary interference between units. Our key idea is to represent the hypothesis of interest as a bipartite graph between units and assignments, and to find an appropriate biclique of this graph. Importantly, the null hypothesis is sharp within this biclique, enabling conditional randomization-based tests. We also connect the size of the biclique to statistical power. Moreover, we can apply off-the-shelf graph clustering methods to find such bicliques efficiently and at scale. We illustrate our approach in settings with clustered interference and show advantages over methods designed specifically for that setting. We then apply our method to a large-scale policing experiment in Medellín, Colombia, where interference has a spatial structure.},
  archive  = {J},
  author   = {David Puelz and Guillaume Basse and Avi Feller and Panos Toulis},
  doi      = {10.1111/rssb.12478},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {174-204},
  title    = {A graph-theoretic approach to randomization tests of causal effects under general interference},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transfer learning for high-dimensional linear regression:
Prediction, estimation and minimax optimality. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(1), 149–173. (<a
href="https://doi.org/10.1111/rssb.12479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper considers estimation and prediction of a high-dimensional linear regression in the setting of transfer learning where, in addition to observations from the target model, auxiliary samples from different but possibly related regression models are available. When the set of informative auxiliary studies is known, an estimator and a predictor are proposed and their optimality is established. The optimal rates of convergence for prediction and estimation are faster than the corresponding rates without using the auxiliary samples. This implies that knowledge from the informative auxiliary samples can be transferred to improve the learning performance of the target problem. When the set of informative auxiliary samples is unknown, we propose a data-driven procedure for transfer learning, called Trans-Lasso, and show its robustness to non-informative auxiliary samples and its efficiency in knowledge transfer. The proposed procedures are demonstrated in numerical studies and are applied to a dataset concerning the associations among gene expressions. It is shown that Trans-Lasso leads to improved performance in gene expression prediction in a target tissue by incorporating data from multiple different tissues as auxiliary samples.},
  archive  = {J},
  author   = {Sai Li and T. Tony Cai and Hongzhe Li},
  doi      = {10.1111/rssb.12479},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {149-173},
  title    = {Transfer learning for high-dimensional linear regression: Prediction, estimation and minimax optimality},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Waste-free sequential monte carlo. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(1), 114–148. (<a
href="https://doi.org/10.1111/rssb.12475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A standard way to move particles in a sequential Monte Carlo (SMC) sampler is to apply several steps of a Markov chain Monte Carlo (MCMC) kernel. Unfortunately, it is not clear how many steps need to be performed for optimal performance. In addition, the output of the intermediate steps are discarded and thus wasted somehow. We propose a new, waste-free SMC algorithm which uses the outputs of all these intermediate MCMC steps as particles. We establish that its output is consistent and asymptotically normal. We use the expression of the asymptotic variance to develop various insights on how to implement the algorithm in practice. We develop in particular a method to estimate, from a single run of the algorithm, the asymptotic variance of any particle estimate. We show empirically, through a range of numerical examples, that waste-free SMC tends to outperform standard SMC samplers, and especially so in situations where the mixing of the considered MCMC kernels decreases across iterations (as in tempering or rare event problems).},
  archive  = {J},
  author   = {Hai-Dang Dau and Nicolas Chopin},
  doi      = {10.1111/rssb.12475},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {114-148},
  title    = {Waste-free sequential monte carlo},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inferential wasserstein generative adversarial networks.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>84</em>(1), 83–113. (<a
href="https://doi.org/10.1111/rssb.12476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Generative adversarial networks (GANs) have been impactful on many problems and applications but suffer from unstable training. The Wasserstein GAN (WGAN) leverages the Wasserstein distance to avoid the caveats in the minmax two-player training of GANs but has other defects such as mode collapse and lack of metric to detect the convergence. We introduce a novel inferential Wasserstein GAN (iWGAN) model, which is a principled framework to fuse autoencoders and WGANs. The iWGAN model jointly learns an encoder network and a generator network motivated by the iterative primal-dual optimization process. The encoder network maps the observed samples to the latent space and the generator network maps the samples from the latent space to the data space. We establish the generalization error bound of the iWGAN to theoretically justify its performance. We further provide a rigorous probabilistic interpretation of our model under the framework of maximum likelihood estimation. The iWGAN, with a clear stopping criteria, has many advantages over other autoencoder GANs. The empirical experiments show that the iWGAN greatly mitigates the symptom of mode collapse, speeds up the convergence, and is able to provide a measurement of quality check for each individual sample. We illustrate the ability of the iWGAN by obtaining competitive and stable performances for benchmark datasets.},
  archive  = {J},
  author   = {Yao Chen and Qingyi Gao and Xiao Wang},
  doi      = {10.1111/rssb.12476},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {83-113},
  title    = {Inferential wasserstein generative adversarial networks},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Usable and precise asymptotics for generalized linear mixed
model analysis and design. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(1), 55–82. (<a
href="https://doi.org/10.1111/rssb.12473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We derive precise asymptotic results that are directly usable for confidence intervals and Wald hypothesis tests for likelihood-based generalized linear mixed model analysis. The essence of our approach is to derive the exact leading term behaviour of the Fisher information matrix when both the number of groups and number of observations within each group diverge. This leads to asymptotic normality results with simple studentizable forms. Similar analyses result in tractable leading term forms for the determination of approximate locally D-optimal designs.},
  archive  = {J},
  author   = {Jiming Jiang and Matt P. Wand and Aishwarya Bhaskaran},
  doi      = {10.1111/rssb.12473},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {55-82},
  title    = {Usable and precise asymptotics for generalized linear mixed model analysis and design},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Authors’ reply to the discussion of “gaussian differential
privacy” by dong et al. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>84</em>(1), 50–54. (<a
href="https://doi.org/10.1111/rssb.12463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Jinshuo Dong and Aaron Roth and Weijie J. Su},
  doi     = {10.1111/rssb.12463},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {1},
  pages   = {50-54},
  title   = {Authors’ reply to the discussion of ‘Gaussian differential privacy’ by dong et al.},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Priyantha wijayatunga’s contribution to the discussion of
“gaussian differential privacy” by dong et al. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(1), 49–50. (<a
href="https://doi.org/10.1111/rssb.12462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Priyantha Wijayatunga},
  doi     = {10.1111/rssb.12462},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {1},
  pages   = {49-50},
  title   = {Priyantha wijayatunga’s contribution to the discussion of ‘Gaussian differential privacy’ by dong et al.},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Jorge mateu’s contribution to the discussion of “gaussian
differential privacy” by dong et al. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(1), 47–48. (<a
href="https://doi.org/10.1111/rssb.12461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Jorge Mateu},
  doi     = {10.1111/rssb.12461},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {1},
  pages   = {47-48},
  title   = {Jorge mateu’s contribution to the discussion of ‘Gaussian differential privacy’ by dong et al.},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). J. Goseling and m.n.m. Van lieshout’s contribution to the
discussion of “gaussian differential privacy” by dong et al. <em>Journal
of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>84</em>(1), 46–47. (<a
href="https://doi.org/10.1111/rssb.12460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {J. Goseling and M.N.M. van Lieshout},
  doi     = {10.1111/rssb.12460},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {1},
  pages   = {46-47},
  title   = {J. goseling and M.N.M. van lieshout&#39;s contribution to the discussion of ‘Gaussian differential privacy’ by dong et al.},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sebastian dietz’s contribution to the discussion of
“gaussian differential privacy” by dong et al. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(1), 44–45. (<a
href="https://doi.org/10.1111/rssb.12459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Sebastian Dietz},
  doi     = {10.1111/rssb.12459},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {1},
  pages   = {44-45},
  title   = {Sebastian dietz’s contribution to the discussion of ‘Gaussian differential privacy’ by dong et al.},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Christine p. Chai’s contribution to the discussion of
“gaussian differential privacy” by dong et al. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(1), 43–44. (<a
href="https://doi.org/10.1111/rssb.12458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Christine P. Chai},
  doi     = {10.1111/rssb.12458},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {1},
  pages   = {43-44},
  title   = {Christine p. chai&#39;s contribution to the discussion of ‘Gaussian differential privacy’ by dong et al.},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Peter krusche and frank bretz’s contribution to the
discussion of “gaussian differential privacy” by dong et al. <em>Journal
of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>84</em>(1), 42–43. (<a
href="https://doi.org/10.1111/rssb.12457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Peter Krusche and Frank Bretz},
  doi     = {10.1111/rssb.12457},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {1},
  pages   = {42-43},
  title   = {Peter krusche and frank bretz&#39;s contribution to the discussion of ‘Gaussian differential privacy’ by dong et al.},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Seconder of the vote of thanks to dong et al. And
contribution to the discussion of “gaussian differential privacy.”
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>84</em>(1), 39–41. (<a
href="https://doi.org/10.1111/rssb.12456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Marco Avella-Medina},
  doi     = {10.1111/rssb.12456},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {1},
  pages   = {39-41},
  title   = {Seconder of the vote of thanks to dong et al. and contribution to the discussion of ‘Gaussian differential privacy’},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Proposer of the vote of thanks to dong et al. And
contribution to the discussion of “gaussian differential privacy.”
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>84</em>(1), 37–38. (<a
href="https://doi.org/10.1111/rssb.12455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Borja Balle},
  doi     = {10.1111/rssb.12455},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {1},
  pages   = {37-38},
  title   = {Proposer of the vote of thanks to dong et al. and contribution to the discussion of ‘Gaussian differential privacy’},
  volume  = {84},
  year    = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Gaussian differential privacy. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>84</em>(1), 3–37. (<a
href="https://doi.org/10.1111/rssb.12454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the past decade, differential privacy has seen remarkable success as a rigorous and practical formalization of data privacy. This privacy definition and its divergence based relaxations, however, have several acknowledged weaknesses, either in handling composition of private algorithms or in analysing important primitives like privacy amplification by subsampling. Inspired by the hypothesis testing formulation of privacy, this paper proposes a new relaxation of differential privacy, which we term ‘ f -differential privacy’ ( f -DP). This notion of privacy has a number of appealing properties and, in particular, avoids difficulties associated with divergence based relaxations. First, f -DP faithfully preserves the hypothesis testing interpretation of differential privacy, thereby making the privacy guarantees easily interpretable. In addition, f -DP allows for lossless reasoning about composition in an algebraic fashion. Moreover, we provide a powerful technique to import existing results proven for the original differential privacy definition to f -DP and, as an application of this technique, obtain a simple and easy-to-interpret theorem of privacy amplification by subsampling for f -DP. In addition to the above findings, we introduce a canonical single-parameter family of privacy notions within the f -DP class that is referred to as ‘Gaussian differential privacy’ (GDP), defined based on hypothesis testing of two shifted Gaussian distributions. GDP is the focal privacy definition among the family of f -DP guarantees due to a central limit theorem for differential privacy that we prove. More precisely, the privacy guarantees of any hypothesis testing based definition of privacy (including the original differential privacy definition) converges to GDP in the limit under composition. We also prove a Berry–Esseen style version of the central limit theorem, which gives a computationally inexpensive tool for tractably analysing the exact composition of private algorithms. Taken together, this collection of attractive properties render f -DP a mathematically coherent, analytically tractable and versatile framework for private data analysis. Finally, we demonstrate the use of the tools we develop by giving an improved analysis of the privacy guarantees of noisy stochastic gradient descent.},
  archive  = {J},
  author   = {Jinshuo Dong and Aaron Roth and Weijie J. Su},
  doi      = {10.1111/rssb.12454},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {3-37},
  title    = {Gaussian differential privacy},
  volume   = {84},
  year     = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
