<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIOMTC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="biomtc---163">BIOMTC - 163</h2>
<ul>
<li><details>
<summary>
(2022). Acknowledgments referees 2022. <em>BIOMTC</em>,
<em>78</em>(4), 1721–1724. (<a
href="https://doi.org/10.1111/biom.13806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  doi          = {10.1111/biom.13806},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1721-1724},
  shortjournal = {Biometrics},
  title        = {Acknowledgments referees 2022},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical issues in drug development, third edition.
Stephen s. Senn new jersey: John wiley and sons, ltd., 2021. ISBN:
978-1-119-23857-7. <em>BIOMTC</em>, <em>78</em>(4), 1716–1717. (<a
href="https://doi.org/10.1111/biom.13751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Jason C. Hsu},
  doi          = {10.1111/biom.13751},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1716-1717},
  shortjournal = {Biometrics},
  title        = {Statistical issues in drug development, third edition. stephen s. senn new jersey: john wiley and sons, ltd., 2021. ISBN: 978-1-119-23857-7.},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Assessing COVID-19 and other pandemics and epidemics using
computational modelling and data analysis subhendu kumar pani, sujata
dash, wellington p. Dos santos, syed ahmad chan bukhari, and francesco
flammini, switzerland: Springer nature switzerland AG. 2022.
Https://doi.org/10.1007/978-3-030-79753-9. <em>BIOMTC</em>,
<em>78</em>(4), 1715–1716. (<a
href="https://doi.org/10.1111/biom.13802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Yen-Chen Anne Feng},
  doi          = {10.1111/biom.13802},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1715-1716},
  shortjournal = {Biometrics},
  title        = {Assessing COVID-19 and other pandemics and epidemics using computational modelling and data analysis subhendu kumar pani, sujata dash, wellington p. dos santos, syed ahmad chan bukhari, and francesco flammini, switzerland: springer nature switzerland AG. 2022. https://doi.org/10.1007/978-3-030-79753-9},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fundamentals of causal inference with r. Babette a.
Brumback. 2021. New york: Chapman and hall/CRC press. 2021.
Https://doi.org/10.1201/9781003146674. <em>BIOMTC</em>, <em>78</em>(4),
1714–1715. (<a href="https://doi.org/10.1111/biom.13733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {An-Shun Tai and Sheng-Hsuan Lin},
  doi          = {10.1111/biom.13733},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1714-1715},
  shortjournal = {Biometrics},
  title        = {Fundamentals of causal inference with r. babette a. brumback. 2021. new york: chapman and Hall/CRC press. 2021. https://doi.org/10.1201/9781003146674},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weak-instrument robust tests in two-sample summary-data
mendelian randomization. <em>BIOMTC</em>, <em>78</em>(4), 1699–1713. (<a
href="https://doi.org/10.1111/biom.13524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization (MR) has been a popular method in genetic epidemiology to estimate the effect of an exposure on an outcome using genetic variants as instrumental variables (IV), with two-sample summary-data MR being the most popular. Unfortunately, instruments in MR studies are often weakly associated with the exposure, which can bias effect estimates and inflate Type I errors. In this work, we propose test statistics that are robust under weak-instrument asymptotics by extending the Anderson–Rubin, Kleibergen, and the conditional likelihood ratio test in econometrics to two-sample summary-data MR. We also use the proposed Anderson–Rubin test to develop a point estimator and to detect invalid instruments. We conclude with a simulation and an empirical study and show that the proposed tests control size and have better power than existing methods with weak instruments.},
  archive      = {J_BIOMTC},
  author       = {Sheng Wang and Hyunseung Kang},
  doi          = {10.1111/biom.13524},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1699-1713},
  shortjournal = {Biometrics},
  title        = {Weak-instrument robust tests in two-sample summary-data mendelian randomization},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint modeling of zero-inflated longitudinal proportions and
time-to-event data with application to a gut microbiome study.
<em>BIOMTC</em>, <em>78</em>(4), 1686–1698. (<a
href="https://doi.org/10.1111/biom.13515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have suggested that the temporal dynamics of the human microbiome may have associations with human health and disease. An increasing number of longitudinal microbiome studies, which record time to disease onset, aim to identify candidate microbes as biomarkers for prognosis. Owing to the ultra-skewness and sparsity of microbiome proportion (relative abundance) data, directly applying traditional statistical methods may result in substantial power loss or spurious inferences. We propose a novel joint modeling framework [JointMM], which is comprised of two sub-models: a longitudinal sub-model called zero-inflated scaled-beta generalized linear mixed-effects regression to depict the temporal structure of microbial proportions among subjects; and a survival sub-model to characterize the occurrence of an event and its relationship with the longitudinal microbiome proportions. JointMM is specifically designed to handle the zero-inflated and highly skewed longitudinal microbial proportion data and examine whether the temporal pattern of microbial presence and/or the nonzero microbial proportions are associated with differences in the time to an event. The longitudinal sub-model of JointMM also provides the capacity to investigate how the (time-varying) covariates are related to the temporal microbial presence/absence patterns and/or the changing trend in nonzero proportions. Comprehensive simulations and real data analyses are used to assess the statistical efficiency and interpretability of JointMM.},
  archive      = {J_BIOMTC},
  author       = {Jiyuan Hu and Chan Wang and Martin J. Blaser and Huilin Li},
  doi          = {10.1111/biom.13515},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1686-1698},
  shortjournal = {Biometrics},
  title        = {Joint modeling of zero-inflated longitudinal proportions and time-to-event data with application to a gut microbiome study},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient odds ratio estimation under two-phase sampling
using error-prone data from a multi-national HIV research cohort.
<em>BIOMTC</em>, <em>78</em>(4), 1674–1685. (<a
href="https://doi.org/10.1111/biom.13512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persons living with HIV engage in routine clinical care, generating large amounts of data in observational HIV cohorts. These data are often error-prone, and directly using them in biomedical research could bias estimation and give misleading results. A cost-effective solution is the two-phase design, under which the error-prone variables are observed for all patients during Phase I, and that information is used to select patients for data auditing during Phase II. For example, the Caribbean, Central, and South America network for HIV epidemiology (CCASAnet) selected a random sample from each site for data auditing. Herein, we consider efficient odds ratio estimation with partially audited, error-prone data. We propose a semiparametric approach that uses all information from both phases and accommodates a number of error mechanisms. We allow both the outcome and covariates to be error-prone and these errors to be correlated, and selection of the Phase II sample can depend on Phase I data in an arbitrary manner. We devise a computationally efficient, numerically stable EM algorithm to obtain estimators that are consistent, asymptotically normal, and asymptotically efficient. We demonstrate the advantages of the proposed methods over existing ones through extensive simulations. Finally, we provide applications to the CCASAnet cohort.},
  archive      = {J_BIOMTC},
  author       = {Sarah C. Lotspeich and Bryan E. Shepherd and Gustavo G. C. Amorim and Pamela A. Shaw and Ran Tao},
  doi          = {10.1111/biom.13512},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1674-1685},
  shortjournal = {Biometrics},
  title        = {Efficient odds ratio estimation under two-phase sampling using error-prone data from a multi-national HIV research cohort},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sample size estimation for cancer randomized trials in the
presence of heterogeneous populations. <em>BIOMTC</em>, <em>78</em>(4),
1662–1673. (<a href="https://doi.org/10.1111/biom.13527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key issue when designing clinical trials is the estimation of the number of subjects required. Assuming for multicenter trials or biomarker-stratified designs that the effect size between treatment arms is the same among the whole study population might be inappropriate. Limited work is available for properly determining the sample size for such trials. However, we need to account for both, the heterogeneity of the baseline hazards over clusters or strata but also the heterogeneity of the treatment effects, otherwise sample size estimates might be biased. Most existing methods account for either heterogeneous baseline hazards or treatment effects but they dot not allow to simultaneously account for both sources of variations. This article proposes an approach to calculate sample size formula for clustered or stratified survival data relying on frailty models. Both theoretical derivations and simulation results show the proposed approach can guarantee the desired power in worst case scenarios and is often much more efficient than existing approaches. Application to a real clinical trial designs is also illustrated.},
  archive      = {J_BIOMTC},
  author       = {Derek Dinart and Carine Bellera and Virginie Rondeau},
  doi          = {10.1111/biom.13527},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1662-1673},
  shortjournal = {Biometrics},
  title        = {Sample size estimation for cancer randomized trials in the presence of heterogeneous populations},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Flexible use of copula-type model for dose-finding in drug
combination clinical trials. <em>BIOMTC</em>, <em>78</em>(4), 1651–1661.
(<a href="https://doi.org/10.1111/biom.13510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification of the maximum tolerated dose combination (MTDC) of cancer drugs is an important objective in phase I oncology trials. Numerous dose-finding designs for drug combination have been proposed over the years. Copula-type models exhibit distinctive advantages in this task over other models used in existing competitive designs. For example, their application enables the consideration of dose-limiting toxicities attributable to one of two agents. However, if a particular combination therapy demonstrates extremely synergistic toxicity, copula-type models are liable to induce biases in toxicity probability estimators due to the associated Fréchet–Hoeffding bounds. Consequently, the dose-finding performance may be worse than those of other competitive designs. The objective of this study is to improve the performance of dose-finding designs based on copula-type models while maintaining their advantageous properties. We propose an extension of the parameter space of the interaction term in copula-type models. This releases the Fréchet–Hoeffding bounds, making the estimation of toxicity probabilities more flexible. Numerical examples in various scenarios demonstrate that the performance (e.g., the percentage of correct MTDC selection) of the proposed method is better than those exhibited by existing copula-type models and comparable with those of other competitive designs, irrespective of the existence of extreme synergistic toxicity. The results obtained in this study could motivate the real-world application of the proposed method in cases requiring the utilization of the properties of copula-type models.},
  archive      = {J_BIOMTC},
  author       = {Koichi Hashizume and Jun Tshuchida and Takashi Sozu},
  doi          = {10.1111/biom.13510},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1651-1661},
  shortjournal = {Biometrics},
  title        = {Flexible use of copula-type model for dose-finding in drug combination clinical trials},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bridging preference-based instrumental variable studies and
cluster-randomized encouragement experiments: Study design,
noncompliance, and average cluster effect ratio. <em>BIOMTC</em>,
<em>78</em>(4), 1639–1650. (<a
href="https://doi.org/10.1111/biom.13500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instrumental variable (IV) methods are widely used in medical research to draw causal conclusions when the treatment and outcome are confounded by unmeasured confounding variables. One important feature of such studies is that the IV is often applied at the cluster level, for example, hospitals&#39; or physicians&#39; preference for a certain treatment where each hospital or physician naturally defines a cluster. This paper proposes to embed such observational IV data into a cluster-randomized encouragement experiment using nonbipartite matching. Potential outcomes and causal assumptions underpinning the design are formalized and examined. Testing procedures for two commonly used estimands, Fisher&#39;s sharp null hypothesis and the pooled effect ratio (PER), are extended to the current setting. We then introduce a novel cluster-heterogeneous proportional treatment effect model and the relevant estimand: the average cluster effect ratio. This new estimand is advantageous over the structural parameter in a constant proportional treatment effect model in that it allows treatment heterogeneity, and is advantageous over the PER estimand in that it does not suffer from Simpson&#39;s paradox. We develop an asymptotically valid randomization-based testing procedure for this new estimand based on solving a mixed-integer quadratically constrained optimization problem. The proposed design and inferential methods are applied to a study of the effect of using transesophageal echocardiography during coronary artery bypass graft surgery on patients&#39; 30-day mortality rate. R package ivdesign implements the proposed method.},
  archive      = {J_BIOMTC},
  author       = {Bo Zhang and Siyu Heng and Emily J. MacKay and Ting Ye},
  doi          = {10.1111/biom.13500},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1639-1650},
  shortjournal = {Biometrics},
  title        = {Bridging preference-based instrumental variable studies and cluster-randomized encouragement experiments: Study design, noncompliance, and average cluster effect ratio},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sufficient dimension reduction for populations with
structured heterogeneity. <em>BIOMTC</em>, <em>78</em>(4), 1626–1638.
(<a href="https://doi.org/10.1111/biom.13546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key challenge in building effective regression models for large and diverse populations is accounting for patient heterogeneity. An example of such heterogeneity is in health system risk modeling efforts where different combinations of comorbidities fundamentally alter the relationship between covariates and health outcomes. Accounting for heterogeneity arising combinations of factors can yield more accurate and interpretable regression models. Yet, in the presence of high-dimensional covariates, accounting for this type of heterogeneity can exacerbate estimation difficulties even with large sample sizes. To handle these issues, we propose a flexible and interpretable risk modeling approach based on semiparametric sufficient dimension reduction. The approach accounts for patient heterogeneity, borrows strength in estimation across related subpopulations to improve both estimation efficiency and interpretability, and can serve as a useful exploratory tool or as a powerful predictive model. In simulated examples, we show that our approach often improves estimation performance in the presence of heterogeneity and is quite robust to deviations from its key underlying assumptions. We demonstrate our approach in an analysis of hospital admission risk for a large health system and demonstrate its predictive power when tested on further follow-up data.},
  archive      = {J_BIOMTC},
  author       = {Jared D. Huling and Menggang Yu},
  doi          = {10.1111/biom.13546},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1626-1638},
  shortjournal = {Biometrics},
  title        = {Sufficient dimension reduction for populations with structured heterogeneity},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint association and classification analysis of multi-view
data. <em>BIOMTC</em>, <em>78</em>(4), 1614–1625. (<a
href="https://doi.org/10.1111/biom.13536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view data, which is matched sets of measurements on the same subjects, have become increasingly common with advances in multi-omics technology. Often, it is of interest to find associations between the views that are related to the intrinsic class memberships. Existing association methods cannot directly incorporate class information, while existing classification methods do not take into account between-views associations. In this work, we propose a framework for Joint Association and Classification Analysis of multi-view data (JACA). Our goal is not to merely improve the misclassification rates, but to provide a latent representation of high-dimensional data that is both relevant for the subtype discrimination and coherent across the views. We motivate the methodology by establishing a connection between canonical correlation analysis and discriminant analysis. We also establish the estimation consistency of JACA in high-dimensional settings. A distinct advantage of JACA is that it can be applied to the multi-view data with block-missing structure, that is to cases where a subset of views or class labels is missing for some subjects. The application of JACA to quantify the associations between RNAseq and miRNA views with respect to consensus molecular subtypes in colorectal cancer data from The Cancer Genome Atlas project leads to improved misclassification rates and stronger found associations compared to existing methods.},
  archive      = {J_BIOMTC},
  author       = {Yunfeng Zhang and Irina Gaynanova},
  doi          = {10.1111/biom.13536},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1614-1625},
  shortjournal = {Biometrics},
  title        = {Joint association and classification analysis of multi-view data},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reducing subspace models for large-scale covariance
regression. <em>BIOMTC</em>, <em>78</em>(4), 1604–1613. (<a
href="https://doi.org/10.1111/biom.13531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop an envelope model for joint mean and covariance regression in the large p , small n setting. In contrast to existing envelope methods, which improve mean estimates by incorporating estimates of the covariance structure, we focus on identifying covariance heterogeneity by incorporating information about mean-level differences. We use a Monte Carlo EM algorithm to identify a low-dimensional subspace that explains differences in both means and covariances as a function of covariates, and then use MCMC to estimate the posterior uncertainty conditional on the inferred low-dimensional subspace. We demonstrate the utility of our model on a motivating application on the metabolomics of aging. We also provide R code that can be used to develop and test other generalizations of the response envelope model.},
  archive      = {J_BIOMTC},
  author       = {Alexander M. Franks},
  doi          = {10.1111/biom.13531},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1604-1613},
  shortjournal = {Biometrics},
  title        = {Reducing subspace models for large-scale covariance regression},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simultaneous feature selection and outlier detection with
optimality guarantees. <em>BIOMTC</em>, <em>78</em>(4), 1592–1603. (<a
href="https://doi.org/10.1111/biom.13553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomedical research is increasingly data rich, with studies comprising ever growing numbers of features. The larger a study, the higher the likelihood that a substantial portion of the features may be redundant and/or contain contamination (outlying values). This poses serious challenges, which are exacerbated in cases where the sample sizes are relatively small. Effective and efficient approaches to perform sparse estimation in the presence of outliers are critical for these studies, and have received considerable attention in the last decade. We contribute to this area considering high-dimensional regressions contaminated by multiple mean-shift outliers affecting both the response and the design matrix. We develop a general framework and use mixed-integer programming to simultaneously perform feature selection and outlier detection with provably optimal guarantees. We prove theoretical properties for our approach, that is, a necessary and sufficient condition for the robustly strong oracle property , where the number of features can increase exponentially with the sample size; the optimal estimation of parameters; and the breakdown point of the resulting estimates. Moreover, we provide computationally efficient procedures to tune integer constraints and warm-start the algorithm. We show the superior performance of our proposal compared to existing heuristic methods through simulations and use it to study the relationships between childhood obesity and the human microbiome.},
  archive      = {J_BIOMTC},
  author       = {Luca Insolia and Ana Kenney and Francesca Chiaromonte and Giovanni Felici},
  doi          = {10.1111/biom.13553},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1592-1603},
  shortjournal = {Biometrics},
  title        = {Simultaneous feature selection and outlier detection with optimality guarantees},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical cancer heterogeneity analysis based on
histopathological imaging features. <em>BIOMTC</em>, <em>78</em>(4),
1579–1591. (<a href="https://doi.org/10.1111/biom.13544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cancer research, supervised heterogeneity analysis has important implications. Such analysis has been traditionally based on clinical/demographic/molecular variables. Recently, histopathological imaging features, which are generated as a byproduct of biopsy, have been shown as effective for modeling cancer outcomes, and a handful of supervised heterogeneity analysis has been conducted based on such features. There are two types of histopathological imaging features, which are extracted based on specific biological knowledge and using automated imaging processing software, respectively. Using both types of histopathological imaging features, our goal is to conduct the first supervised cancer heterogeneity analysis that satisfies a hierarchical structure . That is, the first type of imaging features defines a rough structure, and the second type defines a nested and more refined structure. A penalization approach is developed, which has been motivated by but differs significantly from penalized fusion and sparse group penalization. It has satisfactory statistical and numerical properties. In the analysis of lung adenocarcinoma data, it identifies a heterogeneity structure significantly different from the alternatives and has satisfactory prediction and stability performance.},
  archive      = {J_BIOMTC},
  author       = {Mingyang Ren and Qingzhao Zhang and Sanguo Zhang and Tingyan Zhong and Jian Huang and Shuangge Ma},
  doi          = {10.1111/biom.13544},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1579-1591},
  shortjournal = {Biometrics},
  title        = {Hierarchical cancer heterogeneity analysis based on histopathological imaging features},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extracting brain disease-related connectome subgraphs by
adaptive dense subgraph discovery. <em>BIOMTC</em>, <em>78</em>(4),
1566–1578. (<a href="https://doi.org/10.1111/biom.13537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group-level brain connectome analysis has attracted increasing interest in neuropsychiatric research with the goal of identifying connectomic subnetworks (subgraphs) that are systematically associated with brain disorders. However, extracting disease-related subnetworks from the whole brain connectome has been challenging, because no prior knowledge is available regarding the sizes and locations of the subnetworks. In addition, neuroimaging data are often mixed with substantial noise that can further obscure informative subnetwork detection. We propose a likelihood-based adaptive dense subgraph discovery (ADSD) model to extract disease-related subgraphs from the group-level whole brain connectome data. Our method is robust to both false positive and false negative errors of edge-wise inference and thus can lead to a more accurate discovery of latent disease-related connectomic subnetworks. We develop computationally efficient algorithms to implement the novel ADSD objective function and derive theoretical results to guarantee the convergence properties. We apply the proposed approach to a brain fMRI study for schizophrenia research and identify well-organized and biologically meaningful subnetworks that exhibit schizophrenia-related salience network centered connectivity abnormality. Analysis of synthetic data also demonstrates the superior performance of the ADSD method for latent subnetwork detection in comparison with existing methods in various settings.},
  archive      = {J_BIOMTC},
  author       = {Qiong Wu and Xiaoqi Huang and Adam J. Culbreth and James A. Waltz and L. Elliot Hong and Shuo Chen},
  doi          = {10.1111/biom.13537},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1566-1578},
  shortjournal = {Biometrics},
  title        = {Extracting brain disease-related connectome subgraphs by adaptive dense subgraph discovery},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bivariate small-area estimation for binary and gaussian
variables based on a conditionally specified model. <em>BIOMTC</em>,
<em>78</em>(4), 1555–1565. (<a
href="https://doi.org/10.1111/biom.13552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many large-scale surveys collect both discrete and continuous variables. Small-area estimates may be desired for means of continuous variables, proportions in each level of a categorical variable, or for domain means defined as the mean of the continuous variable for each level of the categorical variable. In this paper, we introduce a conditionally specified bivariate mixed-effects model for small-area estimation, and provide a necessary and sufficient condition under which the conditional distributions render a valid joint distribution. The conditional specification allows better model interpretation. We use the valid joint distribution to calculate empirical Bayes predictors and use the parametric bootstrap to estimate the mean squared error. Simulation studies demonstrate the superior performance of the bivariate mixed-effects model relative to univariate model estimators. We apply the bivariate mixed-effects model to construct estimates for small watersheds using data from the Conservation Effects Assessment Project, a survey developed to quantify the environmental impacts of conservation efforts. We construct predictors of mean sediment loss, the proportion of land where the soil loss tolerance is exceeded, and the average sediment loss on land where the soil loss tolerance is exceeded. In the data analysis, the bivariate mixed-effects model leads to more scientifically interpretable estimates of domain means than those based on two independent univariate models.},
  archive      = {J_BIOMTC},
  author       = {Hao Sun and Emily Berg and Zhengyuan Zhu},
  doi          = {10.1111/biom.13552},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1555-1565},
  shortjournal = {Biometrics},
  title        = {Bivariate small-area estimation for binary and gaussian variables based on a conditionally specified model},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multidimensional molecular measurements–environment
interaction analysis for disease outcomes. <em>BIOMTC</em>,
<em>78</em>(4), 1542–1554. (<a
href="https://doi.org/10.1111/biom.13526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple types of molecular (genetic, genomic, epigenetic, etc.) measurements, environmental risk factors, and their interactions have been found to contribute to the outcomes and phenotypes of complex diseases. In each of the previous studies, only the interactions between one type of molecular measurement and environmental risk factors have been analyzed. In recent biomedical studies, multidimensional profiling, in which data from multiple types of molecular measurements are collected from the same subjects, is becoming popular. A myriad of recent studies have shown that collectively analyzing multiple types of molecular measurements is not only biologically sensible but also leads to improved estimation and prediction. In this study, we conduct an M–E interaction analysis, with M standing for multidimensional molecular measurements and E standing for environmental risk factors. This can accommodate multiple types of molecular measurements and sufficiently account for their overlapping as well as independent information. Extensive simulation shows that it outperforms several closely related alternatives. In the analysis of TCGA (The Cancer Genome Atlas) data on lung adenocarcinoma and cutaneous melanoma, we make some stable biological findings and achieve stable prediction.},
  archive      = {J_BIOMTC},
  author       = {Yaqing Xu and Mengyun Wu and Shuangge Ma},
  doi          = {10.1111/biom.13526},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1542-1554},
  shortjournal = {Biometrics},
  title        = {Multidimensional molecular measurements–environment interaction analysis for disease outcomes},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A linear noise approximation for stochastic epidemic models
fit to partially observed incidence counts. <em>BIOMTC</em>,
<em>78</em>(4), 1530–1541. (<a
href="https://doi.org/10.1111/biom.13538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic epidemic models (SEMs) fit to incidence data are critical to elucidating outbreak dynamics, shaping response strategies, and preparing for future epidemics. SEMs typically represent counts of individuals in discrete infection states using Markov jump processes (MJPs), but are computationally challenging as imperfect surveillance, lack of subject-level information, and temporal coarseness of the data obscure the true epidemic. Analytic integration over the latent epidemic process is impossible, and integration via Markov chain Monte Carlo (MCMC) is cumbersome due to the dimensionality and discreteness of the latent state space. Simulation-based computational approaches can address the intractability of the MJP likelihood, but are numerically fragile and prohibitively expensive for complex models. A linear noise approximation (LNA) that approximates the MJP transition density with a Gaussian density has been explored for analyzing prevalence data in large-population settings, but requires modification for analyzing incidence counts without assuming that the data are normally distributed. We demonstrate how to reparameterize SEMs to appropriately analyze incidence data, and fold the LNA into a data augmentation MCMC framework that outperforms deterministic methods, statistically, and simulation-based methods, computationally. Our framework is computationally robust when the model dynamics are complex and applies to a broad class of SEMs. We evaluate our method in simulations that reflect Ebola, influenza, and SARS-CoV-2 dynamics, and apply our method to national surveillance counts from the 2013–2015 West Africa Ebola outbreak.},
  archive      = {J_BIOMTC},
  author       = {Jonathan Fintzi and Jon Wakefield and Vladimir N. Minin},
  doi          = {10.1111/biom.13538},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1530-1541},
  shortjournal = {Biometrics},
  title        = {A linear noise approximation for stochastic epidemic models fit to partially observed incidence counts},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Re-calibrating pure risk integrating individual data from
two-phase studies with external summary statistics. <em>BIOMTC</em>,
<em>78</em>(4), 1515–1529. (<a
href="https://doi.org/10.1111/biom.13543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate risk assessment is critical in clinical decision-making. It entails the projected risk based on a risk prediction model agreeing with the observed risk in the target cohort. However, the model often over- or under-estimates the risk. Building a new model for the target cohort would be ideal but costly. It is therefore of great interest to recalibrate an existing model for the target cohort. Existing methods have been proposed to recalibrate the model by leveraging the disease incidence rates from the target cohort. However, they assume the same covariate distribution across cohorts and when the assumption is violated, the recalibrated model can be substantially biased. Further, recalibration is also complicated by the two-phase sampling design that is commonly used for developing risk prediction models. In this paper, we develop a weighted estimating-equation approach accounting for the two-phase design and combine it with a weighted empirical likelihood that leverages the summary information on both disease incidence rates and covariates from the target cohort. We provide a resampling-based inference procedure. Our extensive simulation results show that using the summary information from the target population, the proposed recalibration method yields nearly unbiased risk estimates under a wide range of scenarios. An application to a colorectal cancer study also illustrates that the proposed method yields a well-calibrated model in the target cohort.},
  archive      = {J_BIOMTC},
  author       = {Jiayin Zheng and Yingye Zheng and Li Hsu},
  doi          = {10.1111/biom.13543},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1515-1529},
  shortjournal = {Biometrics},
  title        = {Re-calibrating pure risk integrating individual data from two-phase studies with external summary statistics},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating mean potential outcome under adaptive treatment
length strategies in continuous time. <em>BIOMTC</em>, <em>78</em>(4),
1503–1514. (<a href="https://doi.org/10.1111/biom.13504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An adaptive treatment length strategy is a sequential stage-wise treatment strategy where a subject&#39;s treatment begins at baseline and one chooses to stop or continue treatment at each stage provided the subject has been continuously treated. The effects of treatment are assumed to be cumulative and, therefore, the effect of treatment length on clinical endpoint, measured at the end of the study, is of primary scientific interest. At the same time, adverse treatment-terminating events may occur during the course of treatment that require treatment be stopped immediately. Because the presence of a treatment-terminating event may be strongly associated with the study outcome, the treatment-terminating event is informative. In observational studies, decisions to stop or continue treatment depend on covariate history that confounds the relationship between treatment length on outcome. We propose a new risk-set weighted estimator of the mean potential outcome under the condition that time-dependent covariates update at a set of common landmarks. We show that our proposed estimator is asymptotically linear given mild assumptions and correctly specified working models. Specifically, we study the theoretical properties of our estimator when the nuisance parameters are modeled using either parametric or semiparametric methods. The finite sample performance and theoretical results of the proposed estimator are evaluated through simulation studies and demonstrated by application to the Enhanced Suppression of the Platelet Receptor IIb/IIIa with Integrilin Therapy (ESPRIT) infusion trial data.},
  archive      = {J_BIOMTC},
  author       = {Hao Sun and Ashkan Ertefaie and Brent A. Johnson},
  doi          = {10.1111/biom.13504},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1503-1514},
  shortjournal = {Biometrics},
  title        = {Estimating mean potential outcome under adaptive treatment length strategies in continuous time},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Another look at regression analysis using ranked set samples
with application to an osteoporosis study. <em>BIOMTC</em>,
<em>78</em>(4), 1489–1502. (<a
href="https://doi.org/10.1111/biom.13513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical learning with ranked set samples has shown promising results in estimating various population parameters. Despite the vast literature on rank-based statistical learning methodologies, very little effort has been devoted to studying regression analysis with such samples. A pressing issue is how to incorporate the rank information of ranked set samples into the analysis. We propose two methodologies based on a weighted least squares approach and multilevel modeling to better incorporate the rank information of such samples into the estimation and prediction processes of regression-type models. Our approaches reveal significant improvements in both estimation and prediction problems over already existing methods in the literature and the corresponding ones with simple random samples. We study the robustness of our methods with respect to the misspecification of the distribution of the error terms. Also, we show that rank-based regression models can effectively predict simple random test data by assigning ranks to them a posteriori using judgment poststratification. Theoretical results are augmented with simulations and an osteoporosis study based on a real data set from the Bone Mineral Density (BMD) program of Manitoba to estimate the BMD level of patients using easy to obtain covariates.},
  archive      = {J_BIOMTC},
  author       = {Nasrin Faraji and Mohammad Jafari Jozani and Nader Nematollahi},
  doi          = {10.1111/biom.13513},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1489-1502},
  shortjournal = {Biometrics},
  title        = {Another look at regression analysis using ranked set samples with application to an osteoporosis study},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pool adjacent violators algorithm–assisted learning with
application on estimating optimal individualized treatment regimes.
<em>BIOMTC</em>, <em>78</em>(4), 1475–1488. (<a
href="https://doi.org/10.1111/biom.13511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized medicine allows individuals to choose the best fit of their treatments based on their characteristics through an individualized treatment regime. In this paper, we develop a pool adjacent violators algorithm–assisted learning method to find the optimal individualized treatment regime under the monotone single-index outcome gain model. The proposed estimator is more efficient than peers, and it is robust to the misspecification of the propensity score model or the baseline regression model. The optimal treatment regime is also robust to the misspecification of the functional form of the expected outcome gain model. Simulation studies verified our theoretical results. We also provide an estimate of the expected outcome gain model. Plotting the expected outcome gain versus an individual&#39;s characteristics index can visualize how significant the treatment effect is over the control. We apply the proposed method to an AIDS study.},
  archive      = {J_BIOMTC},
  author       = {Baojiang Chen and Ao Yuan and Jing Qin},
  doi          = {10.1111/biom.13511},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1475-1488},
  shortjournal = {Biometrics},
  title        = {Pool adjacent violators algorithm–assisted learning with application on estimating optimal individualized treatment regimes},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A semiparametric isotonic regression model for skewed
distributions with application to DNA–RNA–protein analysis.
<em>BIOMTC</em>, <em>78</em>(4), 1464–1474. (<a
href="https://doi.org/10.1111/biom.13528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a semiparametric regression model that is built upon an isotonic regression model with the assumption that the random error follows a skewed distribution. We develop an expectation-maximization algorithm for obtaining the maximum likelihood estimates of the model parameters, examine the asymptotic properties of the estimators, conduct simulation studies to explore the performance of the proposed model, and apply the method to evaluate the DNA–RNA–protein relationship and identify genes that are key factors in tumor progression.},
  archive      = {J_BIOMTC},
  author       = {Chenguang Wang and Ao Yuan and Leslie Cope and Jing Qin},
  doi          = {10.1111/biom.13528},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1464-1474},
  shortjournal = {Biometrics},
  title        = {A semiparametric isotonic regression model for skewed distributions with application to DNA–RNA–protein analysis},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design-based properties of the nearest neighbor spatial
interpolator and its bootstrap mean squared error estimator.
<em>BIOMTC</em>, <em>78</em>(4), 1454–1463. (<a
href="https://doi.org/10.1111/biom.13505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nearest neighbor spatial interpolation for mapping continuous populations and finite populations of areas or units is approached from a design-based perspective, that is, populations are fixed, and uncertainty stems from the sampling scheme adopted to select locations. We derive conditions for design-based pointwise and uniform consistency of the nearest neighbor interpolators. We prove that consistency holds under certain schemes that are widely applied in environmental and forest surveys. Furthermore, we propose a pseudopopulation bootstrap estimator of the root mean squared errors of the interpolated values. Finally, a simulation study is performed to assess the theoretical results.},
  archive      = {J_BIOMTC},
  author       = {Lorenzo Fattorini and Marzia Marcheselli and Caterina Pisani and Luca Pratelli},
  doi          = {10.1111/biom.13505},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1454-1463},
  shortjournal = {Biometrics},
  title        = {Design-based properties of the nearest neighbor spatial interpolator and its bootstrap mean squared error estimator},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian adaptive trial design for a continuous biomarker
with possibly nonlinear or nonmonotone prognostic or predictive effects.
<em>BIOMTC</em>, <em>78</em>(4), 1441–1453. (<a
href="https://doi.org/10.1111/biom.13550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As diseases like cancer are increasingly understood on a molecular level, clinical trials are being designed to reveal or validate subpopulations in which an experimental therapy has enhanced benefit. Such biomarker-driven designs, particularly “adaptive enrichment” designs that initially enroll an unselected population and then allow for later restriction of accrual to “marker-positive” patients based on interim results, are increasingly popular. Many biomarkers of interest are naturally continuous, however, and most existing design approaches either require upfront dichotomization or force monotonicity through algorithmic searches for a single marker threshold, thereby excluding the possibility that the continuous biomarker has a nondisjoint and truly nonlinear or nonmonotone prognostic relationship with outcome or predictive relationship with treatment effect. To address this, we propose a novel trial design that leverages both the actual shapes of any continuous marker effects (both prognostic and predictive) and their corresponding posterior uncertainty in an adaptive decision-making framework. At interim analyses, this marker knowledge is updated and overall or marker-driven decisions are reached such as continuing enrollment to the next interim analysis or terminating early for efficacy or futility. Using simulations and patient-level data from a multi-center Children&#39;s Oncology Group trial in Acute Lymphoblastic Leukemia, we derive the operating characteristics of our design and compare its performance to a traditional approach that identifies and applies a dichotomizing marker threshold.},
  archive      = {J_BIOMTC},
  author       = {Yusha Liu and John A. Kairalla and Lindsay A. Renfro},
  doi          = {10.1111/biom.13550},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1441-1453},
  shortjournal = {Biometrics},
  title        = {Bayesian adaptive trial design for a continuous biomarker with possibly nonlinear or nonmonotone prognostic or predictive effects},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multivariate bayesian clustering using covariate-informed
components with application to boreal vegetation sensitivity.
<em>BIOMTC</em>, <em>78</em>(4), 1427–1440. (<a
href="https://doi.org/10.1111/biom.13507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Climate change is impacting both the distribution and abundance of vegetation, especially in far northern latitudes. The effects of climate change are different for every plant assemblage and vary heterogeneously in both space and time. Small changes in climate could result in large vegetation responses in sensitive assemblages but weak responses in robust assemblages. But, patterns and mechanisms of sensitivity and robustness are not yet well understood, largely due to a lack of long-term measurements of climate and vegetation. Fortunately, observations are sometimes available across a broad spatial extent. We develop a novel statistical model for a multivariate response based on unknown cluster-specific effects and covariances, where cluster labels correspond to sensitivity and robustness. Our approach utilizes a prototype model for cluster membership that offers flexibility while enforcing smoothness in cluster probabilities across sites with similar characteristics. We demonstrate our approach with an application to vegetation abundance in Alaska, USA, in which we leverage the broad spatial extent of the study area as a proxy for unrecorded historical observations. In the context of the application, our approach yields interpretable site-level cluster labels associated with assemblage-level sensitivity and robustness without requiring strong a priori assumptions about the drivers of climate sensitivity.},
  archive      = {J_BIOMTC},
  author       = {Henry R. Scharf and Ann M. Raiho and Sierra Pugh and Carl A. Roland and David K. Swanson and Sarah E. Stehn and Mevin B. Hooten},
  doi          = {10.1111/biom.13507},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1427-1440},
  shortjournal = {Biometrics},
  title        = {Multivariate bayesian clustering using covariate-informed components with application to boreal vegetation sensitivity},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Binacox: Automatic cut-point detection in high-dimensional
cox model with applications in genetics. <em>BIOMTC</em>,
<em>78</em>(4), 1414–1426. (<a
href="https://doi.org/10.1111/biom.13547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce binacox , a prognostic method to deal with the problem of detecting multiple cut-points per feature in a multivariate setting where a large number of continuous features are available. The method is based on the Cox model and combines one-hot encoding with the binarsity penalty, which uses total-variation regularization together with an extra linear constraint, and enables feature selection. Original nonasymptotic oracle inequalities for prediction (in terms of Kullback–Leibler divergence) and estimation with a fast rate of convergence are established. The statistical performance of the method is examined in an extensive Monte Carlo simulation study, and then illustrated on three publicly available genetic cancer data sets. On these high-dimensional data sets, our proposed method outperforms state-of-the-art survival models regarding risk prediction in terms of the C-index, with a computing time orders of magnitude faster. In addition, it provides powerful interpretability from a clinical perspective by automatically pinpointing significant cut-points in relevant variables.},
  archive      = {J_BIOMTC},
  author       = {Simon Bussy and Mokhtar Z. Alaya and Anne-Sophie Jannot and Agathe Guilloux},
  doi          = {10.1111/biom.13547},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1414-1426},
  shortjournal = {Biometrics},
  title        = {Binacox: Automatic cut-point detection in high-dimensional cox model with applications in genetics},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simultaneous variable selection in regression analysis of
multivariate interval-censored data. <em>BIOMTC</em>, <em>78</em>(4),
1402–1413. (<a href="https://doi.org/10.1111/biom.13548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate interval-censored data arise when each subject under study can potentially experience multiple events and the onset time of each event is not observed exactly but is known to lie in a certain time interval formed by adjacent examination times with changed statuses of the event. This type of incomplete and complex data structure poses a substantial challenge in practical data analysis. In addition, many potential risk factors exist in numerous studies. Thus, conducting variable selection for event-specific covariates simultaneously becomes useful in identifying important variables and assessing their effects on the events of interest. In this paper, we develop a variable selection technique for multivariate interval-censored data under a general class of semiparametric transformation frailty models. The minimum information criterion (MIC) method is embedded in the optimization step of the proposed expectation-maximization (EM) algorithm to obtain the parameter estimator. The proposed EM algorithm greatly reduces the computational burden in maximizing the observed likelihood function, and the MIC naturally avoids selecting the optimal tuning parameter as needed in many other popular penalties, making the proposed algorithm promising and reliable. The proposed method is evaluated through extensive simulation studies and illustrated by an analysis of patient data from the Aerobics Center Longitudinal Study.},
  archive      = {J_BIOMTC},
  author       = {Liuquan Sun and Shuwei Li and Lianming Wang and Xinyuan Song and Xuemei Sui},
  doi          = {10.1111/biom.13548},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1402-1413},
  shortjournal = {Biometrics},
  title        = {Simultaneous variable selection in regression analysis of multivariate interval-censored data},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonparametric estimation of the survival distribution under
covariate-induced dependent truncation. <em>BIOMTC</em>, <em>78</em>(4),
1390–1401. (<a href="https://doi.org/10.1111/biom.13545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is often delayed entry into observational studies, which results in left truncation. In the estimation of the distribution of time-to-event from left-truncated data, standard survival analysis methods require quasi-independence between the truncation time and event time. Incorrectly assuming quasi-independence may lead to biased estimation. We address the problem of estimation of the survival distribution when dependence between the event time and its left truncation time is induced by shared covariates. We introduce propensity scores for truncated data and propose two inverse probability weighting methods that adjust for both truncation and dependence, if all of the shared covariates are measured. The proposed methods additionally allow for right censoring. We evaluate the proposed methods in simulations, conduct sensitivity analyses, and provide guidelines for use in practice. We illustrate our approach in application to data from a central nervous system lymphoma study. The proposed methods are implemented in the R package, depLT .},
  archive      = {J_BIOMTC},
  author       = {Bella Vakulenko-Lagun and Jing Qian and Sy Han Chiou and Nancy Wang and Rebecca A. Betensky},
  doi          = {10.1111/biom.13545},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1390-1401},
  shortjournal = {Biometrics},
  title        = {Nonparametric estimation of the survival distribution under covariate-induced dependent truncation},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On estimating optimal regime for treatment initiation time
based on restricted mean residual lifetime. <em>BIOMTC</em>,
<em>78</em>(4), 1377–1389. (<a
href="https://doi.org/10.1111/biom.13530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When to initiate treatment on patients is an important problem in many medical studies such as AIDS and cancer. In this article, we formulate the treatment initiation time problem for time-to-event data and propose an optimal individualized regime that determines the best treatment initiation time for individual patients based on their characteristics. Different from existing optimal treatment regimes where treatments are undertaken at a pre-specified time, here new challenges arise from the complicated missing mechanisms in treatment initiation time data and the continuous treatment rule in terms of initiation time. To tackle these challenges, we propose to use restricted mean residual lifetime as a value function to evaluate the performance of different treatment initiation regimes, and develop a nonparametric estimator for the value function, which is consistent even when treatment initiation times are not completely observable and their distribution is unknown. We also establish the asymptotic properties of the resulting estimator in the decision rule and its associated value function estimator. In particular, the asymptotic distribution of the estimated value function is nonstandard, which follows a weighted chi-squared distribution. The finite-sample performance of the proposed method is evaluated by simulation studies and is further illustrated with an application to a breast cancer data.},
  archive      = {J_BIOMTC},
  author       = {Xin Chen and Rui Song and Jiajia Zhang and Swann Arp Adams and Liuquan Sun and Wenbin Lu},
  doi          = {10.1111/biom.13530},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1377-1389},
  shortjournal = {Biometrics},
  title        = {On estimating optimal regime for treatment initiation time based on restricted mean residual lifetime},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integration of survival data from multiple studies.
<em>BIOMTC</em>, <em>78</em>(4), 1365–1376. (<a
href="https://doi.org/10.1111/biom.13517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a statistical procedure that integrates datasets from multiple biomedical studies to predict patients&#39; survival, based on individual clinical and genomic profiles. The proposed procedure accounts for potential differences in the relation between predictors and outcomes across studies, due to distinct patient populations, treatments and technologies to measure outcomes and biomarkers. These differences are modeled explicitly with study-specific parameters. We use hierarchical regularization to shrink the study-specific parameters towards each other and to borrow information across studies. The estimation of the study-specific parameters utilizes a similarity matrix, which summarizes differences and similarities of the relations between covariates and outcomes across studies. We illustrate the method in a simulation study and using a collection of gene expression datasets in ovarian cancer. We show that the proposed model increases the accuracy of survival predictions compared to alternative meta-analytic methods.},
  archive      = {J_BIOMTC},
  author       = {Steffen Ventz and Rahul Mazumder and Lorenzo Trippa},
  doi          = {10.1111/biom.13517},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1365-1376},
  shortjournal = {Biometrics},
  title        = {Integration of survival data from multiple studies},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Group sequential testing for cluster randomized trials with
time-to-event endpoint. <em>BIOMTC</em>, <em>78</em>(4), 1353–1364. (<a
href="https://doi.org/10.1111/biom.13498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose group sequential methods for cluster randomized trials (CRTs) with time-to-event endpoint. The alpha spending function approach is used for sequential data monitoring. The key to this approach is determining the joint distribution of test statistics and the information fraction at the time of interim analysis. We prove that the sequentially computed log-rank statistics in CRTs do not have independent increment property. We also propose an information fraction for group sequential trials with clustered survival data and a corresponding sample size determination approach. Extensive simulation studies are conducted to evaluate the performance of our proposed testing procedure using some existing alpha spending functions in terms of expected sample size and maximal sample size. Real study examples are taken to demonstrate our method.},
  archive      = {J_BIOMTC},
  author       = {Jianghao Li and Sin-Ho Jung},
  doi          = {10.1111/biom.13498},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1353-1364},
  shortjournal = {Biometrics},
  title        = {Group sequential testing for cluster randomized trials with time-to-event endpoint},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of local sensitivity to nonignorability with
missing outcomes and predictors. <em>BIOMTC</em>, <em>78</em>(4),
1342–1352. (<a href="https://doi.org/10.1111/biom.13532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ISNI (index of sensitivity to local nonignorability) method quantifies local sensitivity of parametric inferences to nonignorable missingness in an outcome variable. Here we extend ISNI to the situations where both outcomes and predictors can be missing and where the missingness mechanism can be either parametric or semi-parametric. We define the quantity MinNI (minimum nonignorability) to be an approximation to the norm of the smallest value of the transformed nonignorability that gives a nonnegligible displacement of the estimate of the parameter of interest. We illustrate our method in a complete data set from which we synthetically delete observations according to various patterns. We then apply the method to real-data examples involving the normal linear model and conditional logistic regression.},
  archive      = {J_BIOMTC},
  author       = {Heng Chen and Daniel F. Heitjan},
  doi          = {10.1111/biom.13532},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1342-1352},
  shortjournal = {Biometrics},
  title        = {Analysis of local sensitivity to nonignorability with missing outcomes and predictors},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Varying-coefficient regression analysis for pooled
biomonitoring. <em>BIOMTC</em>, <em>78</em>(4), 1328–1341. (<a
href="https://doi.org/10.1111/biom.13516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human biomonitoring involves measuring the accumulation of contaminants in biological specimens (such as blood or urine) to assess individuals&#39; exposure to environmental contamination. Due to the expensive cost of a single assay, the method of pooling has become increasingly common in environmental studies. The implementation of pooling starts by physically mixing specimens into pools, and then measures pooled specimens for the concentration of contaminants. An important task is to reconstruct individual-level statistical characteristics based on pooled measurements. In this article, we propose to use the varying-coefficient regression model for individual-level biomonitoring and provide methods to estimate the varying coefficients based on different types of pooled data. Asymptotic properties of the estimators are presented. We illustrate our methodology via simulation and with application to pooled biomonitoring of a brominated flame retardant provided by the National Health and Nutrition Examination Survey (NHANES).},
  archive      = {J_BIOMTC},
  author       = {Dewei Wang and Xichen Mou and Yan Liu},
  doi          = {10.1111/biom.13516},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1328-1341},
  shortjournal = {Biometrics},
  title        = {Varying-coefficient regression analysis for pooled biomonitoring},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). WiSER: Robust and scalable estimation and inference of
within-subject variances from intensive longitudinal data.
<em>BIOMTC</em>, <em>78</em>(4), 1313–1327. (<a
href="https://doi.org/10.1111/biom.13506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The availability of vast amounts of longitudinal data from electronic health records (EHRs) and personal wearable devices opens the door to numerous new research questions. In many studies, individual variability of a longitudinal outcome is as important as the mean. Blood pressure fluctuations, glycemic variations, and mood swings are prime examples where it is critical to identify factors that affect the within-individual variability. We propose a scalable method, within-subject variance estimator by robust regression (WiSER), for the estimation and inference of the effects of both time-varying and time-invariant predictors on within-subject variance. It is robust against the misspecification of the conditional distribution of responses or the distribution of random effects. It shows similar performance as the correctly specified likelihood methods but is 10 3 ∼ 10 5 times faster. The estimation algorithm scales linearly in the total number of observations, making it applicable to massive longitudinal data sets. The effectiveness of WiSER is evaluated in extensive simulation studies. Its broad applicability is illustrated using the accelerometry data from the Women&#39;s Health Study and a clinical trial for longitudinal diabetes care.},
  archive      = {J_BIOMTC},
  author       = {Christopher A. German and Janet S. Sinsheimer and Jin Zhou and Hua Zhou},
  doi          = {10.1111/biom.13506},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1313-1327},
  shortjournal = {Biometrics},
  title        = {WiSER: Robust and scalable estimation and inference of within-subject variances from intensive longitudinal data},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Rejoinder to the discussions of “spatial+: A novel approach
to spatial confounding.” <em>BIOMTC</em>, <em>78</em>(4), 1309–1312. (<a
href="https://doi.org/10.1111/biom.13653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this rejoinder, we set out some of the main points that we took from the discussions of our paper “Spatial+: A novel approach to spatial confounding.” The comments provided by the discussants include excellent questions and suggestions for extensions and improvements to spatial+. The discussions also highlight the growing interest in understanding spatial confounding, underpinned by the many recent contributions to the literature on this topic.},
  archive      = {J_BIOMTC},
  author       = {Emiko Dupont and Simon N. Wood and Nicole H. Augustin},
  doi          = {10.1111/biom.13653},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1309-1312},
  shortjournal = {Biometrics},
  title        = {Rejoinder to the discussions of “Spatial+: A novel approach to spatial confounding”},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discussion on “spatial+: A novel approach to spatial
confounding” by emiko dupont, simon n. Wood, and nicole h. augustin.
<em>BIOMTC</em>, <em>78</em>(4), 1305–1308. (<a
href="https://doi.org/10.1111/biom.13655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I congratulate Dupont, Wood, and Augustin (DWA hereon) for providing an easy-to-implement method for estimation in the presence of spatial confounding, and for addressing some of the complicated aspects on the topic. I discuss conceptual and operational issues that are fundamental to inference in spatial settings: (i) the target quantity and its interpretability, (ii) the nonspatial aspect of covariates and their relative spatial scales, and (iii) the impact of spatial smoothing. While DWA provide some insights on these issues, I believe that the audience might benefit from a deeper discussion.},
  archive      = {J_BIOMTC},
  author       = {Georgia Papadogeorgou},
  doi          = {10.1111/biom.13655},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1305-1308},
  shortjournal = {Biometrics},
  title        = {Discussion on “Spatial+: A novel approach to spatial confounding” by emiko dupont, simon n. wood, and nicole h. augustin},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discussion on “spatial+: A novel approach to spatial
confounding” by emiko dupont, simon n. Wood, and nicole h. augustin.
<em>BIOMTC</em>, <em>78</em>(4), 1300–1304. (<a
href="https://doi.org/10.1111/biom.13654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Alexandra M. Schmidt},
  doi          = {10.1111/biom.13654},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1300-1304},
  shortjournal = {Biometrics},
  title        = {Discussion on “Spatial+: A novel approach to spatial confounding” by emiko dupont, simon n. wood, and nicole h. augustin},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discussion on “spatial+: A novel approach to spatial
confounding” by emiko dupont, simon n. Wood, and nicole h. augustin.
<em>BIOMTC</em>, <em>78</em>(4), 1295–1299. (<a
href="https://doi.org/10.1111/biom.13650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Isa Marques and Thomas Kneib},
  doi          = {10.1111/biom.13650},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1295-1299},
  shortjournal = {Biometrics},
  title        = {Discussion on “Spatial+: A novel approach to spatial confounding” by emiko dupont, simon n. wood, and nicole h. augustin},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discussion on “spatial+: A novel approach to spatial
confounding” by dupont, wood, and augustin. <em>BIOMTC</em>,
<em>78</em>(4), 1291–1294. (<a
href="https://doi.org/10.1111/biom.13651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Brian J. Reich and Shu Yang and Yawen Guan},
  doi          = {10.1111/biom.13651},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1291-1294},
  shortjournal = {Biometrics},
  title        = {Discussion on “Spatial+: A novel approach to spatial confounding” by dupont, wood, and augustin},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Spatial+: A novel approach to spatial confounding.
<em>BIOMTC</em>, <em>78</em>(4), 1279–1290. (<a
href="https://doi.org/10.1111/biom.13656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In spatial regression models, collinearity between covariates and spatial effects can lead to significant bias in effect estimates. This problem, known as spatial confounding, is encountered modeling forestry data to assess the effect of temperature on tree health. Reliable inference is difficult as results depend on whether or not spatial effects are included in the model. We propose a novel approach, spatial+, for dealing with spatial confounding when the covariate of interest is spatially dependent but not fully determined by spatial location. Using a thin plate spline model formulation we see that, in this case, the bias in covariate effect estimates is a direct result of spatial smoothing. Spatial+ reduces the sensitivity of the estimates to smoothing by replacing the covariates by their residuals after spatial dependence has been regressed away. Through asymptotic analysis we show that spatial+ avoids the bias problems of the spatial model. This is also demonstrated in a simulation study. Spatial+ is straightforward to implement using existing software and, as the response variable is the same as that of the spatial model, standard model selection criteria can be used for comparisons. A major advantage of the method is also that it extends to models with non-Gaussian response distributions. Finally, while our results are derived in a thin plate spline setting, the spatial+ methodology transfers easily to other spatial model formulations.},
  archive      = {J_BIOMTC},
  author       = {Emiko Dupont and Simon N. Wood and Nicole H. Augustin},
  doi          = {10.1111/biom.13656},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1279-1290},
  shortjournal = {Biometrics},
  title        = {Spatial+: A novel approach to spatial confounding},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Behavior analysis with machine learning using r. Enrique
garcia ceja. (2022). London; boca raton: CRC press. DOI:
10.1201/9781003203469. <em>BIOMTC</em>, <em>78</em>(3), 1274. (<a
href="https://doi.org/10.1111/biom.13737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Charlotte Wang},
  doi          = {10.1111/biom.13737},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1274},
  shortjournal = {Biometrics},
  title        = {Behavior analysis with machine learning using r. enrique garcia ceja. (2022). london; boca raton: CRC press. DOI: 10.1201/9781003203469},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multivariate data integration using r: Methods and
applications with the mixOmics package. Kim-anh lê cao and zoe marie
welham, new york: CRC press, 2021.
Https://doi.org/10.1201/9781003026860. <em>BIOMTC</em>, <em>78</em>(3),
1272–1273. (<a href="https://doi.org/10.1111/biom.13730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Yuehua Cui},
  doi          = {10.1111/biom.13730},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1272-1273},
  shortjournal = {Biometrics},
  title        = {Multivariate data integration using r: methods and applications with the mixOmics package. kim-anh lê cao and zoe marie welham, new york: CRC press, 2021. https://doi.org/10.1201/9781003026860},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Book review: Supervised machine learning for text analysis
in r by emil hvitfeldt and julia silge. <em>BIOMTC</em>, <em>78</em>(3),
1270–1272. (<a href="https://doi.org/10.1111/biom.13674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Han-Ming Wu},
  doi          = {10.1111/biom.13674},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1270-1272},
  shortjournal = {Biometrics},
  title        = {Book review: Supervised machine learning for text analysis in r by emil hvitfeldt and julia silge},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Handbook of measurement error models, edited by grace y.
Yi, aurore delaigle, and paul gustafson, boca raton, FL: Chapman and
hall/CRC, 2021. <em>BIOMTC</em>, <em>78</em>(3), 1269–1270. (<a
href="https://doi.org/10.1111/biom.13675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Li-Pang Chen},
  doi          = {10.1111/biom.13675},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1269-1270},
  shortjournal = {Biometrics},
  title        = {Handbook of measurement error models, edited by grace y. yi, aurore delaigle, and paul gustafson, boca raton, FL: Chapman and Hall/CRC, 2021.},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sample size formula for general win ratio analysis.
<em>BIOMTC</em>, <em>78</em>(3), 1257–1268. (<a
href="https://doi.org/10.1111/biom.13501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Originally proposed for the analysis of prioritized composite endpoints, the win ratio has now expanded into a broad class of methodology based on general pairwise comparisons. Complicated by the non-i.i.d. structure of the test statistic, however, sample size estimation for the win ratio has lagged behind. In this article, we develop general and easy-to-use formulas to calculate sample size for win ratio analysis of different outcome types. In a nonparametric setting, the null variance of the test statistic is derived using U -statistic theory in terms of a dispersion parameter called the standard rank deviation, an intrinsic characteristic of the null outcome distribution and the user-defined rule of comparison. The effect size can be hypothesized either on the original scale of the population win ratio, or on the scale of a “usual” effect size suited to the outcome type. The latter approach allows one to measure the effect size by, for example, odds/continuation ratio for totally/partially ordered outcomes and hazard ratios for composite time-to-event outcomes. Simulation studies show that the derived formulas provide accurate estimates for the required sample size across different settings. As illustration, real data from two clinical studies of hepatic and cardiovascular diseases are used as pilot data to calculate sample sizes for future trials.},
  archive      = {J_BIOMTC},
  author       = {Lu Mao and KyungMann Kim and Xinran Miao},
  doi          = {10.1111/biom.13501},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1257-1268},
  shortjournal = {Biometrics},
  title        = {Sample size formula for general win ratio analysis},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonhomogeneous markov chain for estimating the cumulative
risk of multiple false positive screening tests. <em>BIOMTC</em>,
<em>78</em>(3), 1244–1256. (<a
href="https://doi.org/10.1111/biom.13484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Screening tests are widely recommended for the early detection of disease among asymptomatic individuals. While detecting disease at an earlier stage has the potential to improve outcomes, screening also has negative consequences, including false positive results which may lead to anxiety, unnecessary diagnostic procedures, and increased healthcare costs. In addition, multiple false positive results could discourage participating in subsequent screening rounds. Screening guidelines typically recommend repeated screening over a period of many years, but little prior research has investigated how often individuals receive multiple false positive test results. Estimating the cumulative risk of multiple false positive results over the course of multiple rounds of screening is challenging due to the presence of censoring and competing risks, which may depend on the false positive risk, screening round, and number of prior false positive results. To address the general challenge of estimating the cumulative risk of multiple false positive test results, we propose a nonhomogeneous multistate model to describe the screening process including competing events. We developed alternative approaches for estimating the cumulative risk of multiple false positive results using this multistate model based on existing estimators for the cumulative risk of a single false positive. We compared the performance of the newly proposed models through simulation studies and illustrate model performance using data on screening mammography from the Breast Cancer Surveillance Consortium. Across most simulation scenarios, the multistate extension of a censoring bias model demonstrated lower bias compared to other approaches. In the context of screening mammography, we found that the cumulative risk of multiple false positive results is high. For instance, based on the censoring bias model, for a high-risk individual, the cumulative probability of at least two false positive mammography results after 10 rounds of annual screening is 40.4.},
  archive      = {J_BIOMTC},
  author       = {Marzieh K Golmakani and Rebecca A Hubbard and Diana L Miglioretti},
  doi          = {10.1111/biom.13484},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1244-1256},
  shortjournal = {Biometrics},
  title        = {Nonhomogeneous markov chain for estimating the cumulative risk of multiple false positive screening tests},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantifying direct and indirect effect for longitudinal
mediator and survival outcome using joint modeling approach.
<em>BIOMTC</em>, <em>78</em>(3), 1233–1243. (<a
href="https://doi.org/10.1111/biom.13475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal biomarkers are widely used in biomedical and translational researches to monitor the progressions of diseases. Methods have been proposed to jointly model longitudinal data and survival data, but its causal mechanism is yet to be investigated rigorously. Understanding how much of the total treatment effect is through the biomarker is important in understanding the treatment mechanism and evaluating the biomarker. In this work, we propose a causal mediation analysis method to compute the direct and indirect effects, when a joint modeling approach is used to take the longitudinal biomarker as the mediator and the survival endpoint as the outcome. Such a joint modeling approach allows us to relax the commonly used “sequential ignorability” assumption. We demonstrate how to evaluate longitudinally measured biomarkers using our method with two case studies, an AIDS study and a liver cirrhosis study.},
  archive      = {J_BIOMTC},
  author       = {Cheng Zheng and Lei Liu},
  doi          = {10.1111/biom.13475},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1233-1243},
  shortjournal = {Biometrics},
  title        = {Quantifying direct and indirect effect for longitudinal mediator and survival outcome using joint modeling approach},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stratified cox models with time-varying effects for national
kidney transplant patients: A new blockwise steepest ascent method.
<em>BIOMTC</em>, <em>78</em>(3), 1221–1232. (<a
href="https://doi.org/10.1111/biom.13473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing the national transplant database, which contains about 300,000 kidney transplant patients treated in over 290 transplant centers, may guide the disease management and inform the policy of kidney transplantation. Cox models stratified by centers provide a convenient means to account for the clustered data structure, while studying more than 160 predictors with effects that may vary over time. As fitting a time-varying effect model with such a large sample size may defy any existing software, we propose a blockwise steepest ascent procedure by leveraging the block structure of parameters inherent from the basis expansions for each coefficient function. The algorithm iteratively updates the optimal blockwise search direction, along which the increment of the partial likelihood is maximized. The proposed method can be interpreted from the perspective of the minorization-maximization algorithm and increases the partial likelihood until convergence. We further propose a Wald statistic to test whether the effects are indeed time varying. We evaluate the utility of the proposed method via simulations. Finally, we apply the method to analyze the national kidney transplant data and detect the time-varying nature of the effects of various risk factors.},
  archive      = {J_BIOMTC},
  author       = {Kevin He and Ji Zhu and Jian Kang and Yi Li},
  doi          = {10.1111/biom.13473},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1221-1232},
  shortjournal = {Biometrics},
  title        = {Stratified cox models with time-varying effects for national kidney transplant patients: A new blockwise steepest ascent method},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian analysis of coupled cellular and nuclear
trajectories for cell migration. <em>BIOMTC</em>, <em>78</em>(3),
1209–1220. (<a href="https://doi.org/10.1111/biom.13468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell migration, the process by which cells move from one location to another, plays crucial roles in many biological events. While much research has been devoted to understand the process, most statistical cell migration models rely on using time-lapse microscopy data from cell trajectories alone. However, the cell and its associated nucleus work together to orchestrate cell movement, which motivates a joint analysis of coupled cell–nucleus trajectories. In this paper, we propose a Bayesian hierarchical model for analyzing cell migration. We incorporate a bivariate angular distribution to handle the coupled cell–nucleus trajectories and introduce latent motility status indicators to model a cell&#39;s motility as a time-dependent characteristic. A Markov chain Monte Carlo algorithm is provided for practical implementation of our model, which is used on real experimental data from MDA-MB-231 and NIH 3T3 cells. Through the fitted models, deeper insights into the migratory patterns of these experimental cell populations are gained and their differences are quantified.},
  archive      = {J_BIOMTC},
  author       = {Saptarshi Chakraborty and Tian Lan and Yiider Tseng and Samuel W.K. Wong},
  doi          = {10.1111/biom.13468},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1209-1220},
  shortjournal = {Biometrics},
  title        = {Bayesian analysis of coupled cellular and nuclear trajectories for cell migration},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parameter inference for a stochastic kinetic model of
expanded polyglutamine proteins. <em>BIOMTC</em>, <em>78</em>(3),
1195–1208. (<a href="https://doi.org/10.1111/biom.13467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of protein aggregates in cells is a known feature of many human age-related diseases, such as Huntington&#39;s disease. Simulations using fixed parameter values in a model of the dynamic evolution of expanded polyglutaime (PolyQ) proteins in cells have been used to gain a better understanding of the biological system. However, there is considerable uncertainty about the values of some of the parameters governing the system. Currently, appropriate values are chosen by ad hoc attempts to tune the parameters so that the model output matches experimental data. The problem is further complicated by the fact that the data only offer a partial insight into the underlying biological process: the data consist only of the proportions of cell death and of cells with inclusion bodies at a few time points, corrupted by measurement error. Developing inference procedures to estimate the model parameters in this scenario is a significant task. The model probabilities corresponding to the observed proportions cannot be evaluated exactly, and so they are estimated within the inference algorithm by repeatedly simulating realizations from the model. In general such an approach is computationally very expensive, and we therefore construct Gaussian process emulators for the key quantities and reformulate our algorithm around these fast stochastic approximations. We conclude by highlighting appropriate values of the model parameters leading to new insights into the underlying biological processes.},
  archive      = {J_BIOMTC},
  author       = {H. F. Fisher and R. J. Boys and C. S. Gillespie and C. J. Proctor and A. Golightly},
  doi          = {10.1111/biom.13467},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1195-1208},
  shortjournal = {Biometrics},
  title        = {Parameter inference for a stochastic kinetic model of expanded polyglutamine proteins},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multiview model for relative and absolute microbial
abundances. <em>BIOMTC</em>, <em>78</em>(3), 1181–1194. (<a
href="https://doi.org/10.1111/biom.13503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The absolute abundance of bacterial taxa in human host-associated environments plays a critical role in reproductive and gastrointestinal health. However, obtaining the absolute abundance of many bacterial species is typically prohibitively expensive. In contrast, relative abundance data for many species are comparatively cheap and easy to collect (e.g., with universal primers for the 16S rRNA gene). In this paper, we propose a method to jointly model relative abundance data for many taxa and absolute abundance data for a subset of taxa. Our method provides point and interval estimates for the absolute abundance of all taxa. Crucially, our proposal accounts for differences in the efficiency of taxon detection in the relative and absolute abundance data. We show that modeling taxon-specific efficiencies substantially reduces the estimation error for absolute abundance, and controls the coverage of interval estimators. We demonstrate the performance of our proposed method via a simulation study, a study of the effect of HIV acquisition on microbial abundances, and a sensitivity study where we jackknife the taxa with observed absolute abundances.},
  archive      = {J_BIOMTC},
  author       = {Brian D. Williamson and James P. Hughes and Amy D. Willis},
  doi          = {10.1111/biom.13503},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1181-1194},
  shortjournal = {Biometrics},
  title        = {A multiview model for relative and absolute microbial abundances},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonparametric estimation in an illness-death model with
component-wise censoring. <em>BIOMTC</em>, <em>78</em>(3), 1168–1180.
(<a href="https://doi.org/10.1111/biom.13482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In disease settings where study participants are at risk for death and a serious nonfatal event, composite endpoints defined as the time until the earliest of death or the nonfatal event are often used as the primary endpoint in clinical trials. In practice, if the nonfatal event can only be detected at clinic visits and the death time is known exactly, the resulting composite endpoint exhibits “component-wise censoring.” The standard method used to estimate event-free survival in this setting fails to account for component-wise censoring. We apply a kernel smoothing method previously proposed for a marker process in a novel way to produce a nonparametric estimator for event-free survival that accounts for component-wise censoring. The key insight that allows us to apply this kernel method is thinking of nonfatal event status as an intermittently observed binary time-dependent variable rather than thinking of time to the nonfatal event as interval-censored. We also propose estimators for the probability in state and restricted mean time in state for reversible or irreversible illness-death models, under component-wise censoring, and derive their large-sample properties. We perform a simulation study to compare our method to existing multistate survival methods and apply the methods on data from a large randomized trial studying a multifactor intervention for reducing morbidity and mortality among men at above average risk of coronary heart disease.},
  archive      = {J_BIOMTC},
  author       = {Anne Eaton and Yifei Sun and James Neaton and Xianghua Luo},
  doi          = {10.1111/biom.13482},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1168-1180},
  shortjournal = {Biometrics},
  title        = {Nonparametric estimation in an illness-death model with component-wise censoring},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Utilizing stability criteria in choosing feature selection
methods yields reproducible results in microbiome data. <em>BIOMTC</em>,
<em>78</em>(3), 1155–1167. (<a
href="https://doi.org/10.1111/biom.13481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is indispensable in microbiome data analysis, but it can be particularly challenging as microbiome data sets are high dimensional, underdetermined, sparse and compositional. Great efforts have recently been made on developing new methods for feature selection that handle the above data characteristics, but almost all methods were evaluated based on performance of model predictions. However, little attention has been paid to address a fundamental question: how appropriate are those evaluation criteria? Most feature selection methods often control the model fit, but the ability to identify meaningful subsets of features cannot be evaluated simply based on the prediction accuracy. If tiny changes to the data would lead to large changes in the chosen feature subset, then many selected features are likely to be a data artifact rather than real biological signal. This crucial need of identifying relevant and reproducible features motivated the reproducibility evaluation criterion such as Stability, which quantifies how robust a method is to perturbations in the data. In our paper, we compare the performance of popular model prediction metrics (MSE or AUC) with proposed reproducibility criterion Stability in evaluating four widely used feature selection methods in both simulations and experimental microbiome applications with continuous or binary outcomes. We conclude that Stability is a preferred feature selection criterion over model prediction metrics because it better quantifies the reproducibility of the feature selection method.},
  archive      = {J_BIOMTC},
  author       = {Lingjing Jiang and Niina Haiminen and Anna-Paola Carrieri and Shi Huang and Yoshiki Vázquez-Baeza and Laxmi Parida and Ho-Cheol Kim and Austin D. Swafford and Rob Knight and Loki Natarajan},
  doi          = {10.1111/biom.13481},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1155-1167},
  shortjournal = {Biometrics},
  title        = {Utilizing stability criteria in choosing feature selection methods yields reproducible results in microbiome data},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient detection and classification of epigenomic changes
under multiple conditions. <em>BIOMTC</em>, <em>78</em>(3), 1141–1154.
(<a href="https://doi.org/10.1111/biom.13477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epigenomics, the study of the human genome and its interactions with proteins and other cellular elements, has become of significant interest in recent years. Such interactions have been shown to regulate essential cellular functions and are associated with multiple complex diseases. Therefore, understanding how these interactions may change across conditions is central in biomedical research. Chromatin immunoprecipitation followed by massively parallel sequencing (ChIP-seq) is one of several techniques to detect local changes in epigenomic activity (peaks). However, existing methods for differential peak calling are not optimized for the diversity in ChIP-seq signal profiles, are limited to the analysis of two conditions, or cannot classify specific patterns of differential change when multiple patterns exist. To address these limitations, we present a flexible and efficient method for the detection of differential epigenomic activity across multiple conditions. We utilize data from the ENCODE Consortium and show that the presented method, epigraHMM, exhibits superior performance to current tools and it is among the fastest algorithms available, while allowing the classification of combinatorial patterns of differential epigenomic activity and the characterization of chromatin regulatory states.},
  archive      = {J_BIOMTC},
  author       = {Pedro L. Baldoni and Naim U. Rashid and Joseph G. Ibrahim},
  doi          = {10.1111/biom.13477},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1141-1154},
  shortjournal = {Biometrics},
  title        = {Efficient detection and classification of epigenomic changes under multiple conditions},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inferring UK COVID-19 fatal infection trajectories from
daily mortality data: Were infections already in decline before the UK
lockdowns? <em>BIOMTC</em>, <em>78</em>(3), 1127–1140. (<a
href="https://doi.org/10.1111/biom.13462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of new infections per day is a key quantity for effective epidemic management. It can be estimated relatively directly by testing of random population samples. Without such direct epidemiological measurement, other approaches are required to infer whether the number of new cases is likely to be increasing or decreasing: for example, estimating the pathogen-effective reproduction number, R , using data gathered from the clinical response to the disease. For coronavirus disease 2019 (Covid-19/SARS-Cov-2), such R estimation is heavily dependent on modelling assumptions, because the available clinical case data are opportunistic observational data subject to severe temporal confounding. Given this difficulty, it is useful to retrospectively reconstruct the time course of infections from the least compromised available data, using minimal prior assumptions. A Bayesian inverse problem approach applied to UK data on first-wave Covid-19 deaths and the disease duration distribution suggests that fatal infections were in decline before full UK lockdown (24 March 2020), and that fatal infections in Sweden started to decline only a day or two later. An analysis of UK data using the model of Flaxman et al. gives the same result under relaxation of its prior assumptions on R , suggesting an enhanced role for non-pharmaceutical interventions short of full lockdown in the UK context. Similar patterns appear to have occurred in the subsequent two lockdowns.},
  archive      = {J_BIOMTC},
  author       = {Simon N. Wood},
  doi          = {10.1111/biom.13462},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1127-1140},
  shortjournal = {Biometrics},
  title        = {Inferring UK COVID-19 fatal infection trajectories from daily mortality data: Were infections already in decline before the UK lockdowns?},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Rejoinder to discussions of “distributional independent
component analysis for diverse neuroimaging modalities.”
<em>BIOMTC</em>, <em>78</em>(3), 1122–1126. (<a
href="https://doi.org/10.1111/biom.13588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We thank the editors for organizing the discussions and the discussants for insightful comments. Our rejoinder provides results and comments to address the questions raised in the discussions. Specifically, we present results showing DICA largely demonstrates better or comparable stability as compared with standard ICA. We also validate the DICA in real fMRI application by showing DICA generally shows higher reliability in reproducibly recovering major brain functional networks as compared with the standard ICA. We provide details on the computational complexity of the method. The computational cost of DICA is very reasonable with the analysis of the fMRI and DTI data easily implementable on a PC or laptop. Finally, we include discussions on several directions for extending the DICA framework in the future.},
  archive      = {J_BIOMTC},
  author       = {Ben Wu and Subhadip Pal and Jian Kang and Ying Guo},
  doi          = {10.1111/biom.13588},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1122-1126},
  shortjournal = {Biometrics},
  title        = {Rejoinder to discussions of “distributional independent component analysis for diverse neuroimaging modalities”},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discussion on “distributional independent component analysis
for diverse neuroimaging modalities” by ben wu, subhadip pal, jian kang,
and ying guo. <em>BIOMTC</em>, <em>78</em>(3), 1118–1121. (<a
href="https://doi.org/10.1111/biom.13590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are grateful for the opportunity to provide a discussion on this paper. We will first focus on the general context. Next, we will emphasize the novel key ideas proposed by the authors before formulating some open questions.},
  archive      = {J_BIOMTC},
  author       = {Beatrijs Moerkerke and Ruth Seurinck},
  doi          = {10.1111/biom.13590},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1118-1121},
  shortjournal = {Biometrics},
  title        = {Discussion on “distributional independent component analysis for diverse neuroimaging modalities” by ben wu, subhadip pal, jian kang, and ying guo},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discussion on “distributional independent component analysis
for diverse neuroimaging modalities” by ben wu, subhadip pal, jian kang,
and ying guo. <em>BIOMTC</em>, <em>78</em>(3), 1113–1117. (<a
href="https://doi.org/10.1111/biom.13591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wu et al. have made an important contribution to the methodology for data-driven analysis of MRI data. However, we wish to challenge the authors on new potential applications of their approach beyond diffusion tensor imaging data, and to think carefully about the impact of random initialization implicit in their method. We illustrate the variability found from re-analyzing the supplied demonstration data multiple times, finding that the discovered independent components have a wide range of reliability, from nearly perfect overlap to no overlap at all.},
  archive      = {J_BIOMTC},
  author       = {Kan Keeratimahat and Thomas E. Nichols},
  doi          = {10.1111/biom.13591},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1113-1117},
  shortjournal = {Biometrics},
  title        = {Discussion on “distributional independent component analysis for diverse neuroimaging modalities” by ben wu, subhadip pal, jian kang, and ying guo},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discussion on “distributional independent component analysis
for diverse neuroimaging modalities” by ben wu, subhadip pal, jian kang,
and ying guo. <em>BIOMTC</em>, <em>78</em>(3), 1109–1112. (<a
href="https://doi.org/10.1111/biom.13592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I applaud the authors on their innovative generalized independent component analysis (ICA) framework for neuroimaging data. Although ICA has enjoyed great popularity for the analysis of functional magnetic resonance imaging (fMRI) data, its applicability to other modalities has been limited because standard ICA algorithms may not be directly applicable to a diversity of data representations. This is particularly true for single-subject structural neuroimaging, where only a single measurement is collected at each location in the brain. The ingenious idea of Wu et al. (2021) is to transform the data to a vector of probabilities via a mixture distribution with K components, which (following a simple transformation to R K − 1 $\mathbb {R}^{K-1}$ ) can be directly analyzed with standard ICA algorithms, such as infomax (Bell and Sejnowski, 1995) or fastICA (Hyvarinen, 1999). The underlying distribution forming the basis of the mixture is customized to the particular modality being analyzed. This framework, termed distributional ICA (DICA), is applicable in theory to nearly any neuroimaging modality. This has substantial implications for ICA as a general tool for neuroimaging analysis, with particular promise for structural modalities and multimodal studies. This invited commentary focuses on the applicability and potential of DICA for different neuroimaging modalities, questions around details of implementation and performance, and limitations of the validation study presented in the paper.},
  archive      = {J_BIOMTC},
  author       = {Amanda F. Mejia},
  doi          = {10.1111/biom.13592},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1109-1112},
  shortjournal = {Biometrics},
  title        = {Discussion on “distributional independent component analysis for diverse neuroimaging modalities” by ben wu, subhadip pal, jian kang, and ying guo},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discussion on “distributional independent component analysis
for diverse neuroimaging modalities” by ben wu, subhadip pal, jian kang,
and ying guo. <em>BIOMTC</em>, <em>78</em>(3), 1106–1108. (<a
href="https://doi.org/10.1111/biom.13589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Heather Shappell and Sean L. Simpson},
  doi          = {10.1111/biom.13589},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1106-1108},
  shortjournal = {Biometrics},
  title        = {Discussion on “Distributional independent component analysis for diverse neuroimaging modalities” by ben wu, subhadip pal, jian kang, and ying guo},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Distributional independent component analysis for diverse
neuroimaging modalities. <em>BIOMTC</em>, <em>78</em>(3), 1092–1105. (<a
href="https://doi.org/10.1111/biom.13594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in neuroimaging technologies have provided opportunities to acquire brain images of different modalities for studying human brain organization from both functional and structural perspectives. Analysis of images derived from various modalities involves some common goals such as dimension reduction, denoising, and feature extraction. However, since these modalities have vastly different data characteristics, the current analysis is usually performed using distinct analytical tools that are only suitable for a specific imaging modality. In this paper, we present a Distributional Independent Component Analysis (DICA) that represents a new approach that performs decomposition on the distribution level, providing a unified framework for extracting features across imaging modalities with different scales and representations. When applying DICA to fMRI images, we successfully recover well-established brain functional networks in neuroscience literature, providing empirical validation that DICA delivers neurologically relevant findings. More importantly, we discover several structural network components when applying DICA to DTI images. Through fiber tracking, we find these DICA-derived structural components correspond to several major white fiber bundles. To the best of our knowledge, this is the first time these fiber bundles are successfully identified via blind source separation on single subject DTI images. We also evaluate the performance of DICA as compared with existing ICA methods through extensive simulation studies.},
  archive      = {J_BIOMTC},
  author       = {Ben Wu and Subhadip Pal and Jian Kang and Ying Guo},
  doi          = {10.1111/biom.13594},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1092-1105},
  shortjournal = {Biometrics},
  title        = {Distributional independent component analysis for diverse neuroimaging modalities},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integrative analysis of multiple case-control studies.
<em>BIOMTC</em>, <em>78</em>(3), 1080–1091. (<a
href="https://doi.org/10.1111/biom.13461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is often challenging to share detailed individual-level data among studies due to various informatics and privacy constraints. However, it is relatively easy to pool together aggregated summary level data, such as the ones required for standard meta-analyses. Focusing on data generated from case-control studies, we present a flexible inference procedure that integrates individual-level data collected from an “internal” study with summary data borrowed from “external” studies. This procedure is built on a retrospective empirical likelihood framework to account for the sampling bias in case-control studies. It can incorporate summary statistics extracted from various working models adopted by multiple independent or overlapping external studies. It also allows for external studies to be conducted in a population that is different from the internal study population. We show both theoretically and numerically its efficiency advantage over several competing alternatives.},
  archive      = {J_BIOMTC},
  author       = {Han Zhang and Lu Deng and William Wheeler and Jing Qin and Kai Yu},
  doi          = {10.1111/biom.13461},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1080-1091},
  shortjournal = {Biometrics},
  title        = {Integrative analysis of multiple case-control studies},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tensor envelope mixture model for simultaneous clustering
and multiway dimension reduction. <em>BIOMTC</em>, <em>78</em>(3),
1067–1079. (<a href="https://doi.org/10.1111/biom.13486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the form of multidimensional arrays, tensor data have become increasingly prevalent in modern scientific studies and biomedical applications such as computational biology, brain imaging analysis, and process monitoring system. These data are intrinsically heterogeneous with complex dependencies and structure. Therefore, ad-hoc dimension reduction methods on tensor data may lack statistical efficiency and can obscure essential findings. Model-based clustering is a cornerstone of multivariate statistics and unsupervised learning; however, existing methods and algorithms are not designed for tensor-variate samples. In this article, we propose a tensor envelope mixture model (TEMM) for simultaneous clustering and multiway dimension reduction of tensor data. TEMM incorporates tensor-structure-preserving dimension reduction into mixture modeling and drastically reduces the number of free parameters and estimative variability. An expectation-maximization-type algorithm is developed to obtain likelihood-based estimators of the cluster means and covariances, which are jointly parameterized and constrained onto a series of lower dimensional subspaces known as the tensor envelopes. We demonstrate the encouraging empirical performance of the proposed method in extensive simulation studies and a real data application in comparison with existing vector and tensor clustering methods.},
  archive      = {J_BIOMTC},
  author       = {Kai Deng and Xin Zhang},
  doi          = {10.1111/biom.13486},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1067-1079},
  shortjournal = {Biometrics},
  title        = {Tensor envelope mixture model for simultaneous clustering and multiway dimension reduction},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimization of sampling designs for pedigrees and
association studies. <em>BIOMTC</em>, <em>78</em>(3), 1056–1066. (<a
href="https://doi.org/10.1111/biom.13476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many studies, related individuals are phenotyped in order to infer how their genotype contributes to their phenotype, through the estimation of parameters such as breeding values or locus effects. When it is not possible to phenotype all the individuals, it is important to properly sample the population to improve the precision of the statistical analysis. This article studies how to optimize such sampling designs for pedigrees and association studies. Two sampling methods are developed, stratified sampling and D optimality. It is found that it is important to take account of mutation when sampling pedigrees with many generations: as the size of mutation effects increases, optimized designs sample more individuals in late generations. Optimized designs for association studies tend to improve the joint estimation of breeding values and locus effects, all the more as sample size is low and the genetic architecture of the trait is simple. When the trait is determined by few loci, they are reminiscent of classical experimental designs for regression models and tend to select homozygous individuals. When the trait is determined by many loci, locus effects may be difficult to estimate, even if an optimized design is used.},
  archive      = {J_BIOMTC},
  author       = {Olivier David and Arnaud Le Rouzic and Christine Dillmann},
  doi          = {10.1111/biom.13476},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1056-1066},
  shortjournal = {Biometrics},
  title        = {Optimization of sampling designs for pedigrees and association studies},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Addressing patient heterogeneity in disease predictive model
development. <em>BIOMTC</em>, <em>78</em>(3), 1045–1055. (<a
href="https://doi.org/10.1111/biom.13514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses patient heterogeneity associated with prediction problems in biomedical applications. We propose a systematic hypothesis testing approach to determine the existence of patient subgroup structure and the number of subgroups in patient population if subgroups exist. A mixture of generalized linear models is considered to model the relationship between the disease outcome and patient characteristics and clinical factors, including targeted biomarker profiles. We construct a test statistic based on expectation maximization (EM) algorithm and derive its asymptotic distribution under the null hypothesis. An important computational advantage of the test is that the involved parameter estimates under the complex alternative hypothesis can be obtained through a small number of EM iterations, rather than optimizing the objective function. We demonstrate the finite sample performance of the proposed test in terms of type-I error rate and power, using extensive simulation studies. The applicability of the proposed method is illustrated through an application to a multicenter prostate cancer study.},
  archive      = {J_BIOMTC},
  author       = {Xu Gao and Weining Shen and Jing Ning and Ziding Feng and Jianhua Hu},
  doi          = {10.1111/biom.13514},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1045-1055},
  shortjournal = {Biometrics},
  title        = {Addressing patient heterogeneity in disease predictive model development},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Restricted function-on-function linear regression model.
<em>BIOMTC</em>, <em>78</em>(3), 1031–1044. (<a
href="https://doi.org/10.1111/biom.13463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The usual function-on-function linear regression model depicts the association between functional variables in the whole rectangular region and the value of response curve at any point is influenced by the entire trajectory of the predictor curve. But in addition to this, there are cases where the value of the response curve at a point is only influenced by the value of the predictor curve in a subregion, such as the historical relationship and the short-term association. We will consider the restricted function-on-function regression model, where the value of response curve at any point is influenced by a subtrajectory of the predictor. We have two major purposes. First, we propose a novel estimation procedure that is more accurate and computational efficient for the restricted function-on-function model with a given subregion. Second, as the subregion is seldom specified in practice, we propose a subregion selection procedure that can lead to models with better interpretation and predictive performance. Algorithms are developed for both model estimation and subregion selection.},
  archive      = {J_BIOMTC},
  author       = {Ruiyan Luo and Xin Qi},
  doi          = {10.1111/biom.13463},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1031-1044},
  shortjournal = {Biometrics},
  title        = {Restricted function-on-function linear regression model},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Testing for association in multiview network data.
<em>BIOMTC</em>, <em>78</em>(3), 1018–1030. (<a
href="https://doi.org/10.1111/biom.13464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider data consisting of multiple networks, each composed of a different edge set on a common set of nodes. Many models have been proposed for the analysis of such multiview network data under the assumption that the data views are closely related. In this paper, we provide tools for evaluating this assumption. In particular, we ask: given two networks that each follow a stochastic block model, is there an association between the latent community memberships of the nodes in the two networks? To answer this question, we extend the stochastic block model for a single network view to the two-view setting, and develop a new hypothesis test for the null hypothesis that the latent community memberships in the two data views are independent. We apply our test to protein–protein interaction data from the HINT database. We find evidence of a weak association between the latent community memberships of proteins defined with respect to binary interaction data and the latent community memberships of proteins defined with respect to cocomplex association data. We also extend this proposal to the setting of a network with node covariates. The proposed methods extend readily to three or more network/multivariate data views.},
  archive      = {J_BIOMTC},
  author       = {Lucy L. Gao and Daniela Witten and Jacob Bien},
  doi          = {10.1111/biom.13464},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1018-1030},
  shortjournal = {Biometrics},
  title        = {Testing for association in multiview network data},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simulation-based estimators of analytically intractable
causal effects. <em>BIOMTC</em>, <em>78</em>(3), 1001–1017. (<a
href="https://doi.org/10.1111/biom.13499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In causal inference problems, one is often tasked with estimating causal effects which are analytically intractable functionals of the data-generating mechanism. Relevant settings include estimating intention-to-treat effects in longitudinal problems with missing data or computing direct and indirect effects in mediation analysis. One approach to computing these effects is to use the g -formula implemented via Monte Carlo integration; when simulation-based methods such as the nonparametric bootstrap or Markov chain Monte Carlo are used for inference, Monte Carlo integration must be nested within an already computationally intensive algorithm. We develop a widely-applicable approach to accelerating this Monte Carlo integration step which greatly reduces the computational burden of existing g -computation algorithms. We refer to our method as accelerated g -computation (AGC). The algorithms we present are similar in spirit to multiple imputation, but require removing within-imputation variance from the standard error rather than adding it. We illustrate the use of AGC on a mediation analysis problem using a beta regression model and in a longitudinal clinical trial subject to nonignorable missingness using a Bayesian additive regression trees model.},
  archive      = {J_BIOMTC},
  author       = {Antonio R. Linero},
  doi          = {10.1111/biom.13499},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1001-1017},
  shortjournal = {Biometrics},
  title        = {Simulation-based estimators of analytically intractable causal effects},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bayesian nonparametric approach for inferring drug
combination effects on mental health in people with HIV.
<em>BIOMTC</em>, <em>78</em>(3), 988–1000. (<a
href="https://doi.org/10.1111/biom.13508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although combination antiretroviral therapy (ART) with three or more drugs is highly effective in suppressing viral load for people with HIV (human immunodeficiency virus), many ART agents may exacerbate mental health-related adverse effects including depression. Therefore, understanding the effects of combination ART on mental health can help clinicians personalize medicine with less adverse effects to avoid undesirable health outcomes. The emergence of electronic health records offers researchers&#39; unprecedented access to HIV data including individuals&#39; mental health records, drug prescriptions, and clinical information over time. However, modeling such data is challenging due to high dimensionality of the drug combination space, the individual heterogeneity, and sparseness of the observed drug combinations. To address these challenges, we develop a Bayesian nonparametric approach to learn drug combination effect on mental health in people with HIV adjusting for sociodemographic, behavioral, and clinical factors. The proposed method is built upon the subset-tree kernel that represents drug combinations in a way that synthesizes known regimen structure into a single mathematical representation. It also utilizes a distance-dependent Chinese restaurant process to cluster heterogeneous populations while considering individuals&#39; treatment histories. We evaluate the proposed approach through simulation studies, and apply the method to a dataset from the Women&#39;s Interagency HIV Study, showing the clinical utility of our model in guiding clinicians to prescribe informed and effective personalized treatment based on individuals&#39; treatment histories and clinical characteristics.},
  archive      = {J_BIOMTC},
  author       = {Wei Jin and Yang Ni and Leah H. Rubin and Amanda B. Spence and Yanxun Xu},
  doi          = {10.1111/biom.13508},
  journal      = {Biometrics},
  number       = {3},
  pages        = {988-1000},
  shortjournal = {Biometrics},
  title        = {A bayesian nonparametric approach for inferring drug combination effects on mental health in people with HIV},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A transformation-free linear regression for compositional
outcomes and predictors. <em>BIOMTC</em>, <em>78</em>(3), 974–987. (<a
href="https://doi.org/10.1111/biom.13465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional data are common in many fields, both as outcomes and predictor variables. The inventory of models for the case when both the outcome and predictor variables are compositional is limited, and the existing models are often difficult to interpret in the compositional space, due to their use of complex log-ratio transformations. We develop a transformation-free linear regression model where the expected value of the compositional outcome is expressed as a single Markov transition from the compositional predictor. Our approach is based on estimating equations thereby not requiring complete specification of data likelihood and is robust to different data-generating mechanisms. Our model is simple to interpret, allows for 0s and 1s in both the compositional outcome and covariates, and subsumes several interesting subcases of interest. We also develop permutation tests for linear independence and equality of effect sizes of two components of the predictor. Finally, we show that despite its simplicity, our model accurately captures the relationship between compositional data using two datasets from education and medical research.},
  archive      = {J_BIOMTC},
  author       = {Jacob Fiksel and Scott Zeger and Abhirup Datta},
  doi          = {10.1111/biom.13465},
  journal      = {Biometrics},
  number       = {3},
  pages        = {974-987},
  shortjournal = {Biometrics},
  title        = {A transformation-free linear regression for compositional outcomes and predictors},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial correlation structures for detections of individuals
in spatial capture–recapture models. <em>BIOMTC</em>, <em>78</em>(3),
963–973. (<a href="https://doi.org/10.1111/biom.13502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial capture–recapture (SCR) models are commonly used to estimate animal density from surveys on which detectors passively detect animals without physical capture, for example, using camera traps, hair snares, or microphones. An individual is more likely to be recorded by detectors close to its activity center, the centroid of its movement throughout the survey. Existing models to account for this spatial heterogeneity in detection probabilities rely on an assumption of independence between detection records at different detectors conditional on the animals&#39; activity centers, which are treated as latent variables. In this paper, we show that this conditional independence assumption may be violated due to the way animals move around the survey region and encounter detectors, such that additional spatial correlation is almost inevitable. We highlight the links between the well-studied issue of unmodeled temporal heterogeneity in nonspatial capture–recapture and this variety of unmodeled spatial heterogeneity in SCR, showing that the latter causes predictable bias in the same way as the former. We address this by introducing a latent detection field into the model, and illustrate the resulting approach with a simulation study and an application to a camera-trap survey of snow leopards Panthera uncia . Our method is a unifying model for several existing SCR approaches, with special cases including standard SCR, models that account for nonspatial individual heterogeneity, and models with overdispersed detection counts.},
  archive      = {J_BIOMTC},
  author       = {Ben C. Stevenson and Rachel M. Fewster and Koustubh Sharma},
  doi          = {10.1111/biom.13502},
  journal      = {Biometrics},
  number       = {3},
  pages        = {963-973},
  shortjournal = {Biometrics},
  title        = {Spatial correlation structures for detections of individuals in spatial capture–recapture models},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A semiparametric model for between-subject attributes:
Applications to beta-diversity of microbiome data. <em>BIOMTC</em>,
<em>78</em>(3), 950–962. (<a
href="https://doi.org/10.1111/biom.13487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human microbiome plays an important role in our health and identifying factors associated with microbiome composition provides insights into inherent disease mechanisms. By amplifying and sequencing the marker genes in high-throughput sequencing, with highly similar sequences binned together, we obtain operational taxonomic units (OTUs) profiles for each subject. Due to the high-dimensionality and nonnormality features of the OTUs, the measure of diversity is introduced as a summarization at the microbial community level, including the distance-based beta-diversity between individuals. Analyses of such between-subject attributes are not amenable to the predominant within-subject-based statistical paradigm, such as t -tests and linear regression. In this paper, we propose a new approach to model beta-diversity as a response within a regression setting by utilizing the functional response models (FRMs), a class of semiparametric models for between- as well as within-subject attributes. The new approach not only addresses limitations of current methods for beta-diversity with cross-sectional data, but also provides a premise for extending the approach to longitudinal and other clustered data in the future. The proposed approach is illustrated with both real and simulated data.},
  archive      = {J_BIOMTC},
  author       = {J. Liu and Xinlian Zhang and T. Chen and T. Wu and T. Lin and L. Jiang and S. Lang and L. Liu and L. Natarajan and J.X. Tu and T. Kosciolek and J. Morton and T.T. Nguyen and B. Schnabl and R. Knight and C. Feng and Y. Zhong and X.M. Tu},
  doi          = {10.1111/biom.13487},
  journal      = {Biometrics},
  number       = {3},
  pages        = {950-962},
  shortjournal = {Biometrics},
  title        = {A semiparametric model for between-subject attributes: Applications to beta-diversity of microbiome data},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric estimation of structural nested mean models
with irregularly spaced longitudinal observations. <em>BIOMTC</em>,
<em>78</em>(3), 937–949. (<a
href="https://doi.org/10.1111/biom.13471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural nested mean models (SNMMs) are useful for causal inference of treatment effects in longitudinal observational studies. Most existing works assume that the data are collected at prefixed time points for all subjects, which, however, may be restrictive in practice. To deal with irregularly spaced observations, we assume a class of continuous-time SNMMs and a martingale condition of no unmeasured confounding (NUC) to identify the causal parameters. We develop the semiparametric efficiency theory and locally efficient estimators for continuous-time SNMMs. This task is nontrivial due to the restrictions from the NUC assumption imposed on the SNMM parameter. In the presence of ignorable censoring, we show that the complete-case estimator is optimal among a class of weighting estimators including the inverse probability of censoring weighting estimator, and it achieves a double robustness feature in that it is consistent if at least one of the models for the potential outcome mean function and the treatment process is correctly specified. The new framework allows us to conduct causal analysis respecting the underlying continuous-time nature of data processes. The simulation study shows that the proposed estimator outperforms existing approaches. We estimate the effect of time to initiate highly active antiretroviral therapy on the CD4 count at year 2 from the observational Acute Infection and Early Disease Research Program database.},
  archive      = {J_BIOMTC},
  author       = {Shu Yang},
  doi          = {10.1111/biom.13471},
  journal      = {Biometrics},
  number       = {3},
  pages        = {937-949},
  shortjournal = {Biometrics},
  title        = {Semiparametric estimation of structural nested mean models with irregularly spaced longitudinal observations},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling semi-competing risks data as a longitudinal
bivariate process. <em>BIOMTC</em>, <em>78</em>(3), 922–936. (<a
href="https://doi.org/10.1111/biom.13480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As individuals age, death is a competing risk for Alzheimer&#39;s disease (AD) but the reverse is not the case. As such, studies of AD can be placed within the semi-competing risks framework. Central to semi-competing risks, and in contrast to standard competing risks , is that one can learn about the dependence structure between the two events. To-date, however, most methods for semi-competing risks treat dependence as a nuisance and not a potential source of new clinical knowledge. We propose a novel regression-based framework that views the two time-to-event outcomes through the lens of a longitudinal bivariate process on a partition of the time scales of the two events. A key innovation of the framework is that dependence is represented in two distinct forms, local and global dependence, both of which have intuitive clinical interpretations. Estimation and inference are performed via penalized maximum likelihood, and can accommodate right censoring, left truncation, and time-varying covariates. An important consequence of the partitioning of the time scale is that an ambiguity regarding the specific form of the likelihood contribution may arise; a strategy for sensitivity analyses regarding this issue is described. The framework is then used to investigate the role of gender and having ≥1 apolipoprotein E (APOE) ε4 allele on the joint risk of AD and death using data from the Adult Changes in Thought study.},
  archive      = {J_BIOMTC},
  author       = {Daniel Nevo and Deborah Blacker and Eric B. Larson and Sebastien Haneuse},
  doi          = {10.1111/biom.13480},
  journal      = {Biometrics},
  number       = {3},
  pages        = {922-936},
  shortjournal = {Biometrics},
  title        = {Modeling semi-competing risks data as a longitudinal bivariate process},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regression with interval-censored covariates: Application to
cross-sectional incidence estimation. <em>BIOMTC</em>, <em>78</em>(3),
908–921. (<a href="https://doi.org/10.1111/biom.13472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A method for generalized linear regression with interval-censored covariates is described, extending previous approaches. A scenario is considered in which an interval-censored covariate of interest is defined as a function of other variables. Instead of directly modeling the distribution of the interval-censored covariate of interest, the distributions of the variables which determine that covariate are modeled, and the distribution of the covariate of interest is inferred indirectly. This approach leads to an estimation procedure using the Expectation-Maximization (EM) algorithm. The performance of this approach is compared to two alternative approaches, one in which the censoring interval midpoints are used as estimates of the censored covariate values, and another in which the censored values are multiply imputed using uniform distributions over the censoring intervals. A simulation framework is constructed to assess these methods’ accuracies across a range of scenarios. The proposed approach is found to have less bias than midpoint analysis and uniform imputation, at the cost of small increases in standard error.},
  archive      = {J_BIOMTC},
  author       = {Doug Morrison and Oliver Laeyendecker and Ron Brookmeyer},
  doi          = {10.1111/biom.13472},
  journal      = {Biometrics},
  number       = {3},
  pages        = {908-921},
  shortjournal = {Biometrics},
  title        = {Regression with interval-censored covariates: Application to cross-sectional incidence estimation},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature screening with large-scale and high-dimensional
survival data. <em>BIOMTC</em>, <em>78</em>(3), 894–907. (<a
href="https://doi.org/10.1111/biom.13479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data with a huge size present great challenges in modeling, inferences, and computation. In handling big data, much attention has been directed to settings with “large p small n ”, and relatively less work has been done to address problems with p and n being both large, though data with such a feature have now become more accessible than before, where p represents the number of variables and n stands for the sample size. The big volume of data does not automatically ensure good quality of inferences because a large number of unimportant variables may be collected in the process of gathering informative variables. To carry out valid statistical analysis, it is imperative to screen out noisy variables that have no predictive value for explaining the outcome variable. In this paper, we develop a screening method for handling large-sized survival data, where the sample size n is large and the dimension p of covariates is of non-polynomial order of the sample size n , or the so-called NP-dimension. We rigorously establish theoretical results for the proposed method and conduct numerical studies to assess its performance. Our research offers multiple extensions of existing work and enlarges the scope of high-dimensional data analysis. The proposed method capitalizes on the connections among useful regression settings and offers a computationally efficient screening procedure. Our method can be applied to different situations with large-scale data including genomic data.},
  archive      = {J_BIOMTC},
  author       = {Grace Y. Yi and Wenqing He and Raymond. J. Carroll},
  doi          = {10.1111/biom.13479},
  journal      = {Biometrics},
  number       = {3},
  pages        = {894-907},
  shortjournal = {Biometrics},
  title        = {Feature screening with large-scale and high-dimensional survival data},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric analysis of clustered interval-censored
survival data using soft bayesian additive regression trees (SBART).
<em>BIOMTC</em>, <em>78</em>(3), 880–893. (<a
href="https://doi.org/10.1111/biom.13478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Popular parametric and semiparametric hazards regression models for clustered survival data are inappropriate and inadequate when the unknown effects of different covariates and clustering are complex. This calls for a flexible modeling framework to yield efficient survival prediction. Moreover, for some survival studies involving time to occurrence of some asymptomatic events, survival times are typically interval censored between consecutive clinical inspections. In this article, we propose a robust semiparametric model for clustered interval-censored survival data under a paradigm of Bayesian ensemble learning, called soft Bayesian additive regression trees or SBART (Linero and Yang, 2018), which combines multiple sparse (soft) decision trees to attain excellent predictive accuracy. We develop a novel semiparametric hazards regression model by modeling the hazard function as a product of a parametric baseline hazard function and a nonparametric component that uses SBART to incorporate clustering, unknown functional forms of the main effects, and interaction effects of various covariates. In addition to being applicable for left-censored, right-censored, and interval-censored survival data, our methodology is implemented using a data augmentation scheme which allows for existing Bayesian backfitting algorithms to be used. We illustrate the practical implementation and advantages of our method via simulation studies and an analysis of a prostate cancer surgery study where dependence on the experience and skill level of the physicians leads to clustering of survival times. We conclude by discussing our method&#39;s applicability in studies involving high-dimensional data with complex underlying associations.},
  archive      = {J_BIOMTC},
  author       = {Piyali Basak and Antonio Linero and Debajyoti Sinha and Stuart Lipsitz},
  doi          = {10.1111/biom.13478},
  journal      = {Biometrics},
  number       = {3},
  pages        = {880-893},
  shortjournal = {Biometrics},
  title        = {Semiparametric analysis of clustered interval-censored survival data using soft bayesian additive regression trees (SBART)},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A consistent version of distance covariance for
right-censored survival data and its application in hypothesis testing.
<em>BIOMTC</em>, <em>78</em>(3), 867–879. (<a
href="https://doi.org/10.1111/biom.13470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance covariance is a powerful new dependence measure that was recently introduced by Székely et al. and Székely and Rizzo. In this work, the concept of distance covariance is extended to measuring dependence between a covariate vector and a right-censored survival endpoint by establishing an estimator based on an inverse-probability-of-censoring weighted U-statistic. The consistency of the novel estimator is derived. In a large simulation study, it is shown that induced distance covariance permutation tests show a good performance in detecting various complex associations. Applying the distance covariance permutation tests on a gene expression dataset from breast cancer patients outlines its potential for biostatistical practice.},
  archive      = {J_BIOMTC},
  author       = {Dominic Edelmann and Thomas Welchowski and Axel Benner},
  doi          = {10.1111/biom.13470},
  journal      = {Biometrics},
  number       = {3},
  pages        = {867-879},
  shortjournal = {Biometrics},
  title        = {A consistent version of distance covariance for right-censored survival data and its application in hypothesis testing},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multivariate survival analysis in big data: A
divide-and-combine approach. <em>BIOMTC</em>, <em>78</em>(3), 852–866.
(<a href="https://doi.org/10.1111/biom.13469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate failure time data are frequently analyzed using the marginal proportional hazards models and the frailty models. When the sample size is extraordinarily large, using either approach could face computational challenges. In this paper, we focus on the marginal model approach and propose a divide-and-combine method to analyze large-scale multivariate failure time data. Our method is motivated by the Myocardial Infarction Data Acquisition System (MIDAS), a New Jersey statewide database that includes 73,725,160 admissions to nonfederal hospitals and emergency rooms (ERs) from 1995 to 2017. We propose to randomly divide the full data into multiple subsets and propose a weighted method to combine these estimators obtained from individual subsets using three weights. Under mild conditions, we show that the combined estimator is asymptotically equivalent to the estimator obtained from the full data as if the data were analyzed all at once. In addition, to screen out risk factors with weak signals, we propose to perform the regularized estimation on the combined estimator using its combined confidence distribution. Theoretical properties, such as consistency, oracle properties, and asymptotic equivalence between the divide-and-combine approach and the full data approach are studied. Performance of the proposed method is investigated using simulation studies. Our method is applied to the MIDAS data to identify risk factors related to multivariate cardiovascular-related health outcomes.},
  archive      = {J_BIOMTC},
  author       = {Wei Wang and Shou-En Lu and Jerry Q. Cheng and Minge Xie and John B. Kostis},
  doi          = {10.1111/biom.13469},
  journal      = {Biometrics},
  number       = {3},
  pages        = {852-866},
  shortjournal = {Biometrics},
  title        = {Multivariate survival analysis in big data: A divide-and-combine approach},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Rejoinder: Estimating vaccine efficacy over time after a
randomized study is unblinded. <em>BIOMTC</em>, <em>78</em>(3), 848–851.
(<a href="https://doi.org/10.1111/biom.13539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Anastasios A. Tsiatis and Marie Davidian},
  doi          = {10.1111/biom.13539},
  journal      = {Biometrics},
  number       = {3},
  pages        = {848-851},
  shortjournal = {Biometrics},
  title        = {Rejoinder: Estimating vaccine efficacy over time after a randomized study is unblinded},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discussion on “estimating vaccine efficacy over time after a
randomized study is unblinded” by anastasios a. Tsiatis and marie
davidian. <em>BIOMTC</em>, <em>78</em>(3), 844–847. (<a
href="https://doi.org/10.1111/biom.13541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Dean Follmann},
  doi          = {10.1111/biom.13541},
  journal      = {Biometrics},
  number       = {3},
  pages        = {844-847},
  shortjournal = {Biometrics},
  title        = {Discussion on “estimating vaccine efficacy over time after a randomized study is unblinded” by anastasios a. tsiatis and marie davidian},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discussion on “estimating vaccine efficacy over time after a
randomized study is unblinded” by anastasios a. Tsiatis and marie
davidian. <em>BIOMTC</em>, <em>78</em>(3), 841–843. (<a
href="https://doi.org/10.1111/biom.13542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Holly Janes and Fei Gao and Alex Luedtke},
  doi          = {10.1111/biom.13542},
  journal      = {Biometrics},
  number       = {3},
  pages        = {841-843},
  shortjournal = {Biometrics},
  title        = {Discussion on “Estimating vaccine efficacy over time after a randomized study is unblinded” by anastasios a. tsiatis and marie davidian},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discussion on “estimating vaccine efficacy over time after a
randomized study is unblinded” by anastasios a. Tsiatis and marie
davidian. <em>BIOMTC</em>, <em>78</em>(3), 839–840. (<a
href="https://doi.org/10.1111/biom.13540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {M. Elizabeth Halloran},
  doi          = {10.1111/biom.13540},
  journal      = {Biometrics},
  number       = {3},
  pages        = {839-840},
  shortjournal = {Biometrics},
  title        = {Discussion on “Estimating vaccine efficacy over time after a randomized study is unblinded” by anastasios a. tsiatis and marie davidian},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Estimating vaccine efficacy over time after a randomized
study is unblinded. <em>BIOMTC</em>, <em>78</em>(3), 825–838. (<a
href="https://doi.org/10.1111/biom.13509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic due to the novel coronavirus SARS CoV-2 has inspired remarkable breakthroughs in the development of vaccines against the virus and the launch of several phase 3 vaccine trials in Summer 2020 to evaluate vaccine efficacy (VE). Trials of vaccine candidates using mRNA delivery systems developed by Pfizer-BioNTech and Moderna have shown substantial VEs of 94–95\%, leading the US Food and Drug Administration to issue Emergency Use Authorizations and subsequent widespread administration of the vaccines. As the trials continue, a key issue is the possibility that VE may wane over time. Ethical considerations dictate that trial participants be unblinded and those randomized to placebo be offered study vaccine, leading to trial protocol amendments specifying unblinding strategies. Crossover of placebo subjects to vaccine complicates inference on waning of VE. We focus on the particular features of the Moderna trial and propose a statistical framework based on a potential outcomes formulation within which we develop methods for inference on potential waning of VE over time and estimation of VE at any postvaccination time. The framework clarifies assumptions made regarding individual- and population-level phenomena and acknowledges the possibility that subjects who are more or less likely to become infected may be crossed over to vaccine differentially over time. The principles of the framework can be adapted straightforwardly to other trials.},
  archive      = {J_BIOMTC},
  author       = {Anastasios A. Tsiatis and Marie Davidian},
  doi          = {10.1111/biom.13509},
  journal      = {Biometrics},
  number       = {3},
  pages        = {825-838},
  shortjournal = {Biometrics},
  title        = {Estimating vaccine efficacy over time after a randomized study is unblinded},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Handbook of meta-analysis christopher h. Schmid, theo
stijnen, and ian r. White boca raton, FL: Chapman and hall/CRC. 2021.
ISBN 9781138106406, pp. 592. <em>BIOMTC</em>, <em>78</em>(2), 819–820.
(<a href="https://doi.org/10.1111/biom.13604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Li-Pang Chen},
  doi          = {10.1111/biom.13604},
  journal      = {Biometrics},
  number       = {2},
  pages        = {819-820},
  shortjournal = {Biometrics},
  title        = {Handbook of meta-analysis christopher h. schmid, theo stijnen, and ian r. white boca raton, FL: Chapman and Hall/CRC. 2021. ISBN 9781138106406, pp. 592.},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cure models: Methods, applications, and implementation
yingwei peng binbing yu boca raton, FL: Chapman and hall/CRC, 2021. Pp.
268. <em>BIOMTC</em>, <em>78</em>(2), 817. (<a
href="https://doi.org/10.1111/biom.13671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Satrajit Roychoudhury},
  doi          = {10.1111/biom.13671},
  journal      = {Biometrics},
  number       = {2},
  pages        = {817},
  shortjournal = {Biometrics},
  title        = {Cure models: methods, applications, and implementation yingwei peng binbing yu boca raton, FL: chapman and Hall/CRC, 2021. pp. 268.},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predictive analytics: Parametric models for regression and
classification using r ajit c. Tamhane, john wiley &amp; sons. 2020.
ISBN: 978-1-118-94889-7; 978-1-118-94890-3 (ebook). <em>BIOMTC</em>,
<em>78</em>(2), 816–817. (<a
href="https://doi.org/10.1111/biom.13627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Yu-Jyun Huang},
  doi          = {10.1111/biom.13627},
  journal      = {Biometrics},
  number       = {2},
  pages        = {816-817},
  shortjournal = {Biometrics},
  title        = {Predictive analytics: parametric models for regression and classification using r ajit c. tamhane, john wiley &amp; sons. 2020. ISBN: 978-1-118-94889-7; 978-1-118-94890-3 (ebook)},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using r for biostatistics. T. W. MacFarland and j. M. Yates.
Springer international publishing. 2021. ISBN: 978-3-030-62403-3;
978-3-030-62404-0 (eBook). <em>BIOMTC</em>, <em>78</em>(2), 815–816. (<a
href="https://doi.org/10.1111/biom.13638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Shu-Hui Wen},
  doi          = {10.1111/biom.13638},
  journal      = {Biometrics},
  number       = {2},
  pages        = {815-816},
  shortjournal = {Biometrics},
  title        = {Using r for biostatistics. t. w. MacFarland and j. m. yates. springer international publishing. 2021. ISBN: 978-3-030-62403-3; 978-3-030-62404-0 (eBook)},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian compendium, marcel van oijen, cham: Springer nature
switzerland AG. 2020. Xiv, 204 p. <em>BIOMTC</em>, <em>78</em>(2),
813–815. (<a href="https://doi.org/10.1111/biom.13676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Sayan Dasgupta},
  doi          = {10.1111/biom.13676},
  journal      = {Biometrics},
  number       = {2},
  pages        = {813-815},
  shortjournal = {Biometrics},
  title        = {Bayesian compendium, marcel van oijen, cham: Springer nature switzerland AG. 2020. xiv, 204 p.},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correction to “a penalized framework for distributed lag
non-linear models” by antonio gasparrini, fabian scheipl, ben armstrong,
and michael g. Kenward; 73, 938–948, september 2017. <em>BIOMTC</em>,
<em>78</em>(2), 812. (<a
href="https://doi.org/10.1111/biom.13682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Antonio Gasparrini and Fabian Scheipl and Ben Armstrong and Michael G. Kenward},
  doi          = {10.1111/biom.13682},
  journal      = {Biometrics},
  number       = {2},
  pages        = {812},
  shortjournal = {Biometrics},
  title        = {Correction to &quot;A penalized framework for distributed lag non-linear models&quot; by antonio gasparrini, fabian scheipl, ben armstrong, and michael g. kenward; 73, 938–948, september 2017},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identifying regions of inhomogeneities in spatial processes
via an m-RA and mixture priors. <em>BIOMTC</em>, <em>78</em>(2),
798–811. (<a href="https://doi.org/10.1111/biom.13446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soils have been heralded as a hidden resource that can be leveraged to mitigate and address some of the major global environmental challenges. Specifically, the organic carbon stored in soils, called soil organic carbon (SOC), can, through proper soil management, help offset fuel emissions, increase food productivity, and improve water quality. As collecting data on SOC are costly and time-consuming, not much data on SOC are available, although understanding the spatial variability in SOC is of fundamental importance for effective soil management. In this manuscript, we propose a modeling framework that can be used to gain a better understanding of the dependence structure of a spatial process by identifying regions within a spatial domain where the process displays the same spatial correlation range. To achieve this goal, we propose a generalization of the multiresolution approximation (M-RA) modeling framework of Katzfuss originally introduced as a strategy to reduce the computational burden encountered when analyzing massive spatial datasets. To allow for the possibility that the correlation of a spatial process might be characterized by a different range in different subregions of a spatial domain, we provide the M-RA basis functions weights with a two-component mixture prior with one of the mixture components a shrinking prior. We call our approach the mixture M-RA . Application of the mixture M-RA model to both stationary and nonstationary data show that the mixture M-RA model can handle both types of data, can correctly establish the type of spatial dependence structure in the data (e.g., stationary versus not), and can identify regions of local stationarity.},
  archive      = {J_BIOMTC},
  author       = {Marco H. Benedetti and Veronica J. Berrocal and Naveen N. Narisetty},
  doi          = {10.1111/biom.13446},
  journal      = {Biometrics},
  number       = {2},
  pages        = {798-811},
  shortjournal = {Biometrics},
  title        = {Identifying regions of inhomogeneities in spatial processes via an M-RA and mixture priors},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The tukey trend test: Multiplicity adjustment using multiple
marginal models. <em>BIOMTC</em>, <em>78</em>(2), 789–797. (<a
href="https://doi.org/10.1111/biom.13442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In dose–response analysis, it is a challenge to choose appropriate linear or curvilinear shapes when considering multiple, differently scaled endpoints. It has been proposed to fit several marginal regression models that try sets of different transformations of the dose levels as explanatory variables for each endpoint. However, the multiple testing problem underlying this approach, involving correlated parameter estimates for the dose effect between and within endpoints, could only be adjusted heuristically. An asymptotic correction for multiple testing can be derived from the score functions of the marginal regression models. Based on a multivariate t -distribution, the correction provides a one-step adjustment of p -values that accounts for the correlation between estimates from different marginal models. The advantages of the proposed methodology are demonstrated through three example datasets, involving generalized linear models with differently scaled endpoints, differing covariates, and a mixed effect model and through simulation results. The methodology is implemented in an R package.},
  archive      = {J_BIOMTC},
  author       = {Frank Schaarschmidt and Christian Ritz and Ludwig A. Hothorn},
  doi          = {10.1111/biom.13442},
  journal      = {Biometrics},
  number       = {2},
  pages        = {789-797},
  shortjournal = {Biometrics},
  title        = {The tukey trend test: Multiplicity adjustment using multiple marginal models},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inverse probability weighted estimators of vaccine effects
accommodating partial interference and censoring. <em>BIOMTC</em>,
<em>78</em>(2), 777–788. (<a
href="https://doi.org/10.1111/biom.13459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating population-level effects of a vaccine is challenging because there may be interference, that is, the outcome of one individual may depend on the vaccination status of another individual. Partial interference occurs when individuals can be partitioned into groups such that interference occurs only within groups. In the absence of interference, inverse probability weighted (IPW) estimators are commonly used to draw inference about causal effects of an exposure or treatment. Tchetgen Tchetgen and VanderWeele proposed a modified IPW estimator for causal effects in the presence of partial interference. Motivated by a cholera vaccine study in Bangladesh, this paper considers an extension of the Tchetgen Tchetgen and VanderWeele IPW estimator to the setting where the outcome is subject to right censoring using inverse probability of censoring weights (IPCW). Censoring weights are estimated using proportional hazards frailty models. The large sample properties of the IPCW estimators are derived, and simulation studies are presented demonstrating the estimators&#39; performance in finite samples. The methods are then used to analyze data from the cholera vaccine study.},
  archive      = {J_BIOMTC},
  author       = {Sujatro Chakladar and Samuel Rosin and Michael G. Hudgens and M. Elizabeth Halloran and John D. Clemens and Mohammad Ali and Michael E. Emch},
  doi          = {10.1111/biom.13459},
  journal      = {Biometrics},
  number       = {2},
  pages        = {777-788},
  shortjournal = {Biometrics},
  title        = {Inverse probability weighted estimators of vaccine effects accommodating partial interference and censoring},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling dynamic correlation in zero-inflated bivariate
count data with applications to single-cell RNA sequencing data.
<em>BIOMTC</em>, <em>78</em>(2), 766–776. (<a
href="https://doi.org/10.1111/biom.13457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactions between biological molecules in a cell are tightly coordinated and often highly dynamic. As a result of these varying signaling activities, changes in gene coexpression patterns could often be observed. The advancements in next-generation sequencing technologies bring new statistical challenges for studying these dynamic changes of gene coexpression. In recent years, methods have been developed to examine genomic information from individual cells. Single-cell RNA sequencing (scRNA-seq) data are count-based, and often exhibit characteristics such as overdispersion and zero inflation. To explore the dynamic dependence structure in scRNA-seq data and other zero-inflated count data, new approaches are needed. In this paper, we consider overdispersion and zero inflation in count outcomes and propose a ZEro-inflated negative binomial dynamic COrrelation model (ZENCO). The observed count data are modeled as a mixture of two components: success amplifications and dropout events in ZENCO. A latent variable is incorporated into ZENCO to model the covariate-dependent correlation structure. We conduct simulation studies to evaluate the performance of our proposed method and to compare it with existing approaches. We also illustrate the implementation of our proposed approach using scRNA-seq data from a study of minimal residual disease in melanoma.},
  archive      = {J_BIOMTC},
  author       = {Zhen Yang and Yen-Yi Ho},
  doi          = {10.1111/biom.13457},
  journal      = {Biometrics},
  number       = {2},
  pages        = {766-776},
  shortjournal = {Biometrics},
  title        = {Modeling dynamic correlation in zero-inflated bivariate count data with applications to single-cell RNA sequencing data},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EMBRACE: An EM-based bias reduction approach through
copas-model estimation for quantifying the evidence of selective
publishing in network meta-analysis. <em>BIOMTC</em>, <em>78</em>(2),
754–765. (<a href="https://doi.org/10.1111/biom.13441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Systematic reviews and meta-analyses synthesize results from well-conducted studies to optimize healthcare decision-making. Network meta-analysis (NMA) is particularly useful for improving precision, drawing new comparisons, and ranking multiple interventions. However, recommendations can be misled if published results are a selective sample of what has been collected by trialists, particularly when publication status is related to the significance of the findings. Unfortunately, the missing-not-at-random nature of this problem and the numerous parameters involved in modeling NMAs pose unique computational challenges to quantifying and correcting for publication bias, such that sensitivity analysis is used in practice. Motivated by this important methodological gap, we developed a novel and stable expectation-maximization (EM) algorithm to correct for publication bias in the network setting. We validate the method through simulation studies and show that it achieves substantial bias reduction in small to moderately sized NMAs. We also calibrate the method against a Bayesian analysis of a published NMA on antiplatlet therapies for maintaining vascular patency.},
  archive      = {J_BIOMTC},
  author       = {Arielle Marks-Anglin and Chongliang Luo and Jin Piao and Mary Beth Connolly Gibbons and Christopher H. Schmid and Jing Ning and Yong Chen},
  doi          = {10.1111/biom.13441},
  journal      = {Biometrics},
  number       = {2},
  pages        = {754-765},
  shortjournal = {Biometrics},
  title        = {EMBRACE: An EM-based bias reduction approach through copas-model estimation for quantifying the evidence of selective publishing in network meta-analysis},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bayesian spatial model for imaging genetics.
<em>BIOMTC</em>, <em>78</em>(2), 742–753. (<a
href="https://doi.org/10.1111/biom.13460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a Bayesian bivariate spatial model for multivariate regression analysis applicable to studies examining the influence of genetic variation on brain structure. Our model is motivated by an imaging genetics study of the Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI), where the objective is to examine the association between images of volumetric and cortical thickness values summarizing the structure of the brain as measured by magnetic resonance imaging (MRI) and a set of 486 single nucleotide polymorphism (SNPs) from 33 Alzheimer&#39;s disease (AD) candidate genes obtained from 632 subjects. A bivariate spatial process model is developed to accommodate the correlation structures typically seen in structural brain imaging data. First, we allow for spatial correlation on a graph structure in the imaging phenotypes obtained from a neighborhood matrix for measures on the same hemisphere of the brain. Second, we allow for correlation in the same measures obtained from different hemispheres (left/right) of the brain. We develop a mean-field variational Bayes algorithm and a Gibbs sampling algorithm to fit the model. We also incorporate Bayesian false discovery rate (FDR) procedures to select SNPs. We implement the methodology in a new release of the R package bgsmtr . We show that the new spatial model demonstrates superior performance over a standard model in our application. Data used in the preparation of this article were obtained from the ADNI database ( https://adni.loni.usc.edu ).},
  archive      = {J_BIOMTC},
  author       = {Yin Song and Shufei Ge and Jiguo Cao and Liangliang Wang and Farouk S. Nathoo},
  doi          = {10.1111/biom.13460},
  journal      = {Biometrics},
  number       = {2},
  pages        = {742-753},
  shortjournal = {Biometrics},
  title        = {A bayesian spatial model for imaging genetics},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian data fusion: Probabilistic sensitivity analysis for
unmeasured confounding using informative priors based on secondary data.
<em>BIOMTC</em>, <em>78</em>(2), 730–741. (<a
href="https://doi.org/10.1111/biom.13436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian causal inference offers a principled approach to policy evaluation of proposed interventions on mediators or time-varying exposures. Building on the Bayesian g-formula method introduced by Keil et al ., we outline a general approach for the estimation of population-level causal quantities involving dynamic and stochastic treatment regimes, including regimes related to mediation estimands such as natural direct and indirect effects. We further extend this approach to propose a Bayesian data fusion (BDF), an algorithm for performing probabilistic sensitivity analysis when a confounder unmeasured in a primary data set is available in an external data source. When the relevant relationships are causally transportable between the two source populations, BDF corrects confounding bias and supports causal inference and decision-making within the main study population without sharing of the individual-level external data set. We present results from a simulation study comparing BDF to two common frequentist correction methods for unmeasured mediator-outcome confounding bias in the mediation setting. We use these methods to analyze data on the role of stage at cancer diagnosis in contributing to Black–White colorectal cancer survival disparities.},
  archive      = {J_BIOMTC},
  author       = {Leah Comment and Brent A. Coull and Corwin Zigler and Linda Valeri},
  doi          = {10.1111/biom.13436},
  journal      = {Biometrics},
  number       = {2},
  pages        = {730-741},
  shortjournal = {Biometrics},
  title        = {Bayesian data fusion: Probabilistic sensitivity analysis for unmeasured confounding using informative priors based on secondary data},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pursuing sources of heterogeneity in modeling clustered
population. <em>BIOMTC</em>, <em>78</em>(2), 716–729. (<a
href="https://doi.org/10.1111/biom.13434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers often have to deal with heterogeneous population with mixed regression relationships, increasingly so in the era of data explosion. In such problems, when there are many candidate predictors, it is not only of interest to identify the predictors that are associated with the outcome, but also to distinguish the true sources of heterogeneity , that is, to identify the predictors that have different effects among the clusters and thus are the true contributors to the formation of the clusters. We clarify the concepts of the source of heterogeneity that account for potential scale differences of the clusters and propose a regularized finite mixture effects regression to achieve heterogeneity pursuit and feature selection simultaneously. We develop an efficient algorithm and show that our approach can achieve both estimation and selection consistency. Simulation studies further demonstrate the effectiveness of our method under various practical scenarios. Three applications are presented, namely, an imaging genetics study for linking genetic factors and brain neuroimaging traits in Alzheimer&#39;s disease, a public health study for exploring the association between suicide risk among adolescents and their school district characteristics, and a sport analytics study for understanding how the salary levels of baseball players are associated with their performance and contractual status.},
  archive      = {J_BIOMTC},
  author       = {Yan Li and Chun Yu and Yize Zhao and Weixin Yao and Robert H. Aseltine and Kun Chen},
  doi          = {10.1111/biom.13434},
  journal      = {Biometrics},
  number       = {2},
  pages        = {716-729},
  shortjournal = {Biometrics},
  title        = {Pursuing sources of heterogeneity in modeling clustered population},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Small-sample inference for cluster-based outcome-dependent
sampling schemes in resource-limited settings: Investigating low
birthweight in rwanda. <em>BIOMTC</em>, <em>78</em>(2), 701–715. (<a
href="https://doi.org/10.1111/biom.13423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The neonatal mortality rate in Rwanda remains above the United Nations Sustainable Development Goal 3 target of 12 deaths per 1000 live births. As part of a larger effort to reduce preventable neonatal deaths in the country, we conducted a study to examine risk factors for low birthweight. The data were collected via a cost-efficient cluster-based outcome-dependent sampling (ODS) scheme wherein clusters of individuals (health centers) were selected on the basis of, in part, the outcome rate of the individuals. For a given data set collected via a cluster-based ODS scheme, estimation for a marginal model may proceed via inverse-probability-weighted generalized estimating equations, where the cluster-specific weights are the inverse probability of the health center&#39;s inclusion in the sample. In this paper, we provide a detailed treatment of the asymptotic properties of this estimator, together with an explicit expression for the asymptotic variance and a corresponding estimator. Furthermore, motivated by the study we conducted in Rwanda, we propose a number of small-sample bias corrections to both the point estimates and the standard error estimates. Through simulation, we show that applying these corrections when the number of clusters is small generally reduces the bias in the point estimates, and results in closer to nominal coverage. The proposed methods are applied to data from 18 health centers and 1 district hospital in Rwanda.},
  archive      = {J_BIOMTC},
  author       = {Sara Sauer and Bethany Hedt-Gauthier and Claudia Rivera-Rodriguez and Sebastien Haneuse},
  doi          = {10.1111/biom.13423},
  journal      = {Biometrics},
  number       = {2},
  pages        = {701-715},
  shortjournal = {Biometrics},
  title        = {Small-sample inference for cluster-based outcome-dependent sampling schemes in resource-limited settings: Investigating low birthweight in rwanda},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Determination and estimation of optimal quarantine duration
for infectious diseases with application to data analysis of COVID-19.
<em>BIOMTC</em>, <em>78</em>(2), 691–700. (<a
href="https://doi.org/10.1111/biom.13444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quarantine measure is a commonly used nonpharmaceutical intervention during the outbreak of infectious diseases. A key problem for implementing quarantine measure is to determine the duration of the quarantine. Different from the existing methods that determine a constant quarantine duration for everyone, we develop an individualized quarantine rule that suggests different quarantine durations for individuals with different characteristics. The proposed quarantine rule is optimal in the sense that it minimizes the average quarantine duration of uninfected people with the constraint that the probability of symptom presentation for infected people attains the given value closing to 1. The optimal solution for the quarantine duration is obtained and estimated by some statistical methods with application to analyzing COVID-19 data.},
  archive      = {J_BIOMTC},
  author       = {Ruoyu Wang and Qihua Wang},
  doi          = {10.1111/biom.13444},
  journal      = {Biometrics},
  number       = {2},
  pages        = {691-700},
  shortjournal = {Biometrics},
  title        = {Determination and estimation of optimal quarantine duration for infectious diseases with application to data analysis of COVID-19},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synthesizing external aggregated information in the presence
of population heterogeneity: A penalized empirical likelihood approach.
<em>BIOMTC</em>, <em>78</em>(2), 679–690. (<a
href="https://doi.org/10.1111/biom.13429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing availability of data in the public domain, there has been a growing interest in exploiting information from external sources to improve the analysis of smaller scale studies. An emerging challenge in the era of big data is that the subject-level data are high dimensional, but the external information is at an aggregate level and of a lower dimension. Moreover, heterogeneity and uncertainty in the auxiliary information are often not accounted for in information synthesis. In this paper, we propose a unified framework to summarize various forms of aggregated information via estimating equations and develop a penalized empirical likelihood approach to incorporate such information in logistic regression. When the homogeneity assumption is violated, we extend the method to account for population heterogeneity among different sources of information. When the uncertainty in the external information is not negligible, we propose a variance estimator adjusting for the uncertainty. The proposed estimators are asymptotically more efficient than the conventional penalized maximum likelihood estimator and enjoy the oracle property even with a diverging number of predictors. Simulation studies show that the proposed approaches yield higher accuracy in variable selection compared with competitors. We illustrate the proposed methodologies with a pediatric kidney transplant study.},
  archive      = {J_BIOMTC},
  author       = {Ying Sheng and Yifei Sun and Chiung-Yu Huang and Mi-Ok Kim},
  doi          = {10.1111/biom.13429},
  journal      = {Biometrics},
  number       = {2},
  pages        = {679-690},
  shortjournal = {Biometrics},
  title        = {Synthesizing external aggregated information in the presence of population heterogeneity: A penalized empirical likelihood approach},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regression-based negative control of homophily in dyadic
peer effect analysis. <em>BIOMTC</em>, <em>78</em>(2), 668–678. (<a
href="https://doi.org/10.1111/biom.13483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A prominent threat to causal inference about peer effects in social science studies is the presence of homophily bias , that is, social influence between friends and families is entangled with common characteristics or underlying similarities that form close connections. Analysis of social study data has suggested that certain health conditions such as obesity and psychological states including happiness and loneliness can spread between friends and relatives. However, such analyses of peer effects or contagion effects have come under criticism because homophily bias may compromise the causal statement. We develop a regression-based approach which leverages a negative control exposure for identification and estimation of contagion effects on additive or multiplicative scales, in the presence of homophily bias. We apply our methods to evaluate the peer effect of obesity in Framingham Offspring Study.},
  archive      = {J_BIOMTC},
  author       = {Lan Liu and Eric Tchetgen Tchetgen},
  doi          = {10.1111/biom.13483},
  journal      = {Biometrics},
  number       = {2},
  pages        = {668-678},
  shortjournal = {Biometrics},
  title        = {Regression-based negative control of homophily in dyadic peer effect analysis},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variance reduction in the inverse probability weighted
estimators for the average treatment effect using the propensity score.
<em>BIOMTC</em>, <em>78</em>(2), 660–667. (<a
href="https://doi.org/10.1111/biom.13454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The propensity methodology is widely used in medical research to compare different treatments in designs with a nonrandomized treatment allocation. The inverse probability weighted (IPW) estimators are a primary tool for estimating the average treatment effect but the large variance of these estimators is often a significant concern for their reliable use in practice. Inspired by Rao-Blackwellization, this paper proposes a method to smooth an IPW estimator by replacing the weights in the original estimator by their mean over a distribution of the potential treatment assignment. In our simulation study, the smoothed IPW estimator achieves a substantial variance reduction over its original version with only a small increased bias, for example two-to-sevenfold variance reduction for the three IPW estimators in Lunceford and Davidian [Statistics in Medicine, 23(19), 2937–2960]. In addition, our proposed smoothing can also be applied to the locally efficient and doubly robust estimator for added protection against model misspecification. An implementation in R is provided.},
  archive      = {J_BIOMTC},
  author       = {Jiangang Liao and Charles Rohde},
  doi          = {10.1111/biom.13454},
  journal      = {Biometrics},
  number       = {2},
  pages        = {660-667},
  shortjournal = {Biometrics},
  title        = {Variance reduction in the inverse probability weighted estimators for the average treatment effect using the propensity score},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Global sensitivity analysis of randomized trials with
nonmonotone missing binary outcomes: Application to studies of substance
use disorders. <em>BIOMTC</em>, <em>78</em>(2), 649–659. (<a
href="https://doi.org/10.1111/biom.13455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a method for conducting global sensitivity analysis of randomized trials in which binary outcomes are scheduled to be collected on participants at prespecified points in time after randomization and these outcomes may be missing in a nonmonotone fashion. We introduce a class of missing data assumptions, indexed by sensitivity parameters, which are anchored around the missing not at random assumption introduced by Robins ( Statistics in Medicine , 1997). For each assumption in the class, we establish that the joint distribution of the outcomes is identifiable from the distribution of the observed data. Our estimation procedure uses the plug-in principle, where the distribution of the observed data is estimated using random forests. We establish n $\sqrt {n}$ asymptotic properties for our estimation procedure. We illustrate our methodology in the context of a randomized trial designed to evaluate a new approach to reducing substance use, assessed by testing urine samples twice weekly, among patients entering outpatient addiction treatment. We evaluate the finite sample properties of our method in a realistic simulation study. Our methods have been implemented in an R package entitled s labm.},
  archive      = {J_BIOMTC},
  author       = {Daniel O. Scharfstein and Jon Steingrimsson and Aidan McDermott and Chenguang Wang and Souvik Ray and Aimee Campbell and Edward Nunes and Abigail Matthews},
  doi          = {10.1111/biom.13455},
  journal      = {Biometrics},
  number       = {2},
  pages        = {649-659},
  shortjournal = {Biometrics},
  title        = {Global sensitivity analysis of randomized trials with nonmonotone missing binary outcomes: Application to studies of substance use disorders},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recruitment prediction for multicenter clinical trials based
on a hierarchical poisson–gamma model: Asymptotic analysis and improved
intervals. <em>BIOMTC</em>, <em>78</em>(2), 636–648. (<a
href="https://doi.org/10.1111/biom.13447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze predictions of future recruitment to a multicenter clinical trial based on a maximum-likelihood fitting of a commonly used hierarchical Poisson–gamma model for recruitments at individual centers. We consider the asymptotic accuracy of quantile predictions in the limit as the number of recruitment centers grows large and find that, in an important sense, the accuracy of the quantiles does not improve as the number of centers increases. When predicting the number of further recruits in an additional time period, the accuracy degrades as the ratio of the additional time to the census time increases, whereas when predicting the amount of additional time to recruit a further n • + $n^+_\bullet$ patients, the accuracy degrades as the ratio of n • + $n^+_\bullet$ to the number recruited up to the census period increases. Our analysis suggests an improved quantile predictor. Simulation studies verify that the predicted pattern holds for typical recruitment scenarios in clinical trials and verify the much improved coverage properties of prediction intervals obtained from our quantile predictor. In the process of extending the applicability of our methodology, we show that in terms of the accuracy of all integer moments it is always better to approximate the sum of independent gamma random variables by a single gamma random variable matched on the first two moments than by the moment-matched Gaussian available from the central limit theorem.},
  archive      = {J_BIOMTC},
  author       = {Rachael Mountain and Chris Sherlock},
  doi          = {10.1111/biom.13447},
  journal      = {Biometrics},
  number       = {2},
  pages        = {636-648},
  shortjournal = {Biometrics},
  title        = {Recruitment prediction for multicenter clinical trials based on a hierarchical poisson–gamma model: Asymptotic analysis and improved intervals},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Causal interaction trees: Finding subgroups with
heterogeneous treatment effects in observational data. <em>BIOMTC</em>,
<em>78</em>(2), 624–635. (<a
href="https://doi.org/10.1111/biom.13432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce causal interaction tree (CIT) algorithms for finding subgroups of individuals with heterogeneous treatment effects in observational data. The CIT algorithms are extensions of the classification and regression tree algorithm that use splitting criteria based on subgroup-specific treatment effect estimators appropriate for observational data. We describe inverse probability weighting, g-formula, and doubly robust estimators of subgroup-specific treatment effects, derive their asymptotic properties, and use them to construct splitting criteria for the CIT algorithms. We study the performance of the algorithms in simulations and implement them to analyze data from an observational study that evaluated the effectiveness of right heart catheterization for critically ill patients.},
  archive      = {J_BIOMTC},
  author       = {Jiabei Yang and Issa J. Dahabreh and Jon A. Steingrimsson},
  doi          = {10.1111/biom.13432},
  journal      = {Biometrics},
  number       = {2},
  pages        = {624-635},
  shortjournal = {Biometrics},
  title        = {Causal interaction trees: Finding subgroups with heterogeneous treatment effects in observational data},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse linear discriminant analysis for multiview structured
data. <em>BIOMTC</em>, <em>78</em>(2), 612–623. (<a
href="https://doi.org/10.1111/biom.13458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification methods that leverage the strengths of data from multiple sources (multiview data) simultaneously have enormous potential to yield more powerful findings than two-step methods: association followed by classification. We propose two methods, sparse integrative discriminant analysis (SIDA), and SIDA with incorporation of network information (SIDANet), for joint association and classification studies. The methods consider the overall association between multiview data, and the separation within each view in choosing discriminant vectors that are associated and optimally separate subjects into different classes. SIDANet is among the first methods to incorporate prior structural information in joint association and classification studies. It uses the normalized Laplacian of a graph to smooth coefficients of predictor variables, thus encouraging selection of predictors that are connected. We demonstrate the effectiveness of our methods on a set of synthetic datasets and explore their use in identifying potential nontraditional risk factors that discriminate healthy patients at low versus high risk for developing atherosclerosis cardiovascular disease in 10 years. Our findings underscore the benefit of joint association and classification methods if the goal is to correlate multiview data and to perform classification.},
  archive      = {J_BIOMTC},
  author       = {Sandra E. Safo and Eun Jeong Min and Lillian Haine},
  doi          = {10.1111/biom.13458},
  journal      = {Biometrics},
  number       = {2},
  pages        = {612-623},
  shortjournal = {Biometrics},
  title        = {Sparse linear discriminant analysis for multiview structured data},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A model for analyzing clustered occurrence data.
<em>BIOMTC</em>, <em>78</em>(2), 598–611. (<a
href="https://doi.org/10.1111/biom.13435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial or temporal clustering commonly arises in various biological and ecological applications, for example, species or communities may cluster in groups. In this paper, we develop a new clustered occurrence data model where presence–absence data are modeled under a multivariate negative binomial framework. We account for spatial or temporal clustering by introducing a community parameter in the model that controls the strength of dependence between observations thereby enhancing the estimation of the mean and dispersion parameters. We provide conditions to show the existence of maximum likelihood estimates when cluster sizes are homogeneous and equal to 2 or 3 and consider a composite likelihood approach that allows for additional robustness and flexibility in fitting for clustered occurrence data. The proposed method is evaluated in a simulation study and demonstrated using forest plot data from the Center for Tropical Forest Science. Finally, we present several examples using multiple visit occupancy data to illustrate the difference between the proposed model and those of N -mixture models.},
  archive      = {J_BIOMTC},
  author       = {Wen-Han Hwang and Richard Huggins and Jakub Stoklosa},
  doi          = {10.1111/biom.13435},
  journal      = {Biometrics},
  number       = {2},
  pages        = {598-611},
  shortjournal = {Biometrics},
  title        = {A model for analyzing clustered occurrence data},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A kernel regression model for panel count data with
nonparametric covariate functions. <em>BIOMTC</em>, <em>78</em>(2),
586–597. (<a href="https://doi.org/10.1111/biom.13440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The local kernel pseudo-partial likelihood is employed for estimation in a panel count model with nonparametric covariate functions. An estimator of the derivative of the nonparametric covariate function is derived first, and the nonparametric function estimator is then obtained by integrating the derivative estimator. Uniform consistency rates and pointwise asymptotic normality are obtained for the local derivative estimator under some regularity conditions. Moreover, the baseline function estimator is shown to be uniformly consistent. Demonstration of the asymptotic results strongly relies on the modern empirical theory, which generally does not require the Poisson assumption. Simulation studies also illustrate that the local derivative estimator performs well in a finite-sample regardless of whether the Poisson assumption holds. We also implement the proposed methodology to analyze a clinical study on childhood wheezing.},
  archive      = {J_BIOMTC},
  author       = {Yang Wang and Zhangsheng Yu},
  doi          = {10.1111/biom.13440},
  journal      = {Biometrics},
  number       = {2},
  pages        = {586-597},
  shortjournal = {Biometrics},
  title        = {A kernel regression model for panel count data with nonparametric covariate functions},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simultaneous estimation of cluster number and feature
sparsity in high-dimensional cluster analysis. <em>BIOMTC</em>,
<em>78</em>(2), 574–585. (<a
href="https://doi.org/10.1111/biom.13449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the number of clusters ( K ) is a critical and often difficult task in cluster analysis. Many methods have been proposed to estimate K , including some top performers using resampling approach. When performing cluster analysis in high-dimensional data, simultaneous clustering and feature selection is needed for improved interpretation and performance. To our knowledge, little has been studied for simultaneous estimation of K and feature sparsity parameter in a high-dimensional exploratory cluster analysis. In this paper, we propose a resampling method to bridge this gap and evaluate its performance under the sparse K -means clustering framework. The proposed target function balances between sensitivity and specificity of clustering evaluation of pairwise subjects from clustering of full and subsampled data. Through extensive simulations, the method performs among the best over classical methods in estimating K in low-dimensional data. For high-dimensional simulation data, it also shows superior performance to simultaneously estimate K and feature sparsity parameter. Finally, we evaluated the methods in four microarray, two RNA-seq, one SNP, and two nonomics datasets. The proposed method achieves better clustering accuracy with fewer selected predictive genes in almost all real applications.},
  archive      = {J_BIOMTC},
  author       = {Yujia Li and Xiangrui Zeng and Chien-Wei Lin and George C. Tseng},
  doi          = {10.1111/biom.13449},
  journal      = {Biometrics},
  number       = {2},
  pages        = {574-585},
  shortjournal = {Biometrics},
  title        = {Simultaneous estimation of cluster number and feature sparsity in high-dimensional cluster analysis},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial factor modeling: A bayesian matrix-normal approach
for misaligned data. <em>BIOMTC</em>, <em>78</em>(2), 560–573. (<a
href="https://doi.org/10.1111/biom.13452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate spatially oriented data sets are prevalent in the environmental and physical sciences. Scientists seek to jointly model multiple variables, each indexed by a spatial location, to capture any underlying spatial association for each variable and associations among the different dependent variables. Multivariate latent spatial process models have proved effective in driving statistical inference and rendering better predictive inference at arbitrary locations for the spatial process. High-dimensional multivariate spatial data, which are the theme of this article, refer to data sets where the number of spatial locations and the number of spatially dependent variables is very large. The field has witnessed substantial developments in scalable models for univariate spatial processes, but such methods for multivariate spatial processes, especially when the number of outcomes are moderately large, are limited in comparison. Here, we extend scalable modeling strategies for a single process to multivariate processes. We pursue Bayesian inference, which is attractive for full uncertainty quantification of the latent spatial process. Our approach exploits distribution theory for the matrix-normal distribution, which we use to construct scalable versions of a hierarchical linear model of coregionalization (LMC) and spatial factor models that deliver inference over a high-dimensional parameter space including the latent spatial process. We illustrate the computational and inferential benefits of our algorithms over competing methods using simulation studies and an analysis of a massive vegetation index data set.},
  archive      = {J_BIOMTC},
  author       = {Lu Zhang and Sudipto Banerjee},
  doi          = {10.1111/biom.13452},
  journal      = {Biometrics},
  number       = {2},
  pages        = {560-573},
  shortjournal = {Biometrics},
  title        = {Spatial factor modeling: A bayesian matrix-normal approach for misaligned data},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geostatistical modeling of positive-definite matrices: An
application to diffusion tensor imaging. <em>BIOMTC</em>,
<em>78</em>(2), 548–559. (<a
href="https://doi.org/10.1111/biom.13445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geostatistical modeling for continuous point-referenced data has extensively been applied to neuroimaging because it produces efficient and valid statistical inference. However, diffusion tensor imaging (DTI), a neuroimaging technique characterizing the brain&#39;s anatomical structure, produces a positive-definite (p.d.) matrix for each voxel. Currently, only a few geostatistical models for p.d. matrices have been proposed because introducing spatial dependence among p.d. matrices properly is challenging. In this paper, we use the spatial Wishart process, a spatial stochastic process (random field), where each p.d. matrix-variate random variable marginally follows a Wishart distribution, and spatial dependence between random matrices is induced by latent Gaussian processes. This process is valid on an uncountable collection of spatial locations and is almost-surely continuous, leading to a reasonable way of modeling spatial dependence. Motivated by a DTI data set of cocaine users, we propose a spatial matrix-variate regression model based on the spatial Wishart process. A problematic issue is that the spatial Wishart process has no closed-form density function. Hence, we propose an approximation method to obtain a feasible Cholesky decomposition model, which we show to be asymptotically equivalent to the spatial Wishart process model. A local likelihood approximation method is also applied to achieve fast computation. The simulation studies and real data application demonstrate that the Cholesky decomposition process model produces reliable inference and improved performance, compared to other methods.},
  archive      = {J_BIOMTC},
  author       = {Zhou Lan and Brian J. Reich and Joseph Guinness and Dipankar Bandyopadhyay and Liangsuo Ma and F. Gerard Moeller},
  doi          = {10.1111/biom.13445},
  journal      = {Biometrics},
  number       = {2},
  pages        = {548-559},
  shortjournal = {Biometrics},
  title        = {Geostatistical modeling of positive-definite matrices: An application to diffusion tensor imaging},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian spatial homogeneity pursuit for survival data with
an application to the SEER respiratory cancer data. <em>BIOMTC</em>,
<em>78</em>(2), 536–547. (<a
href="https://doi.org/10.1111/biom.13439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a new Bayesian spatial homogeneity pursuit method for survival data under the proportional hazards model to detect spatially clustered patterns in baseline hazard and regression coefficients. Specially, regression coefficients and baseline hazard are assumed to have spatial homogeneity pattern over space. To capture such homogeneity, we develop a geographically weighted Chinese restaurant process prior to simultaneously estimating coefficients and baseline hazards and their uncertainty measures. An efficient Markov chain Monte Carlo (MCMC) algorithm is designed for our proposed methods. Performance is evaluated using simulated data, and further applied to a real data analysis of respiratory cancer in the state of Louisiana.},
  archive      = {J_BIOMTC},
  author       = {Lijiang Geng and Guanyu Hu},
  doi          = {10.1111/biom.13439},
  journal      = {Biometrics},
  number       = {2},
  pages        = {536-547},
  shortjournal = {Biometrics},
  title        = {Bayesian spatial homogeneity pursuit for survival data with an application to the SEER respiratory cancer data},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gaussian graphical model-based heterogeneity analysis via
penalized fusion. <em>BIOMTC</em>, <em>78</em>(2), 524–535. (<a
href="https://doi.org/10.1111/biom.13426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneity is a hallmark of cancer, diabetes, cardiovascular diseases, and many other complex diseases. This study has been partly motivated by the unsupervised heterogeneity analysis for complex diseases based on molecular and imaging data, for which, network-based analysis, by accommodating the interconnections among variables, can be more informative than that limited to mean, variance, and other simple distributional properties. In the literature, there has been very limited research on network-based heterogeneity analysis, and a common limitation shared by the existing techniques is that the number of subgroups needs to be specified a priori or in an ad hoc manner. In this article, we develop a penalized fusion approach for heterogeneity analysis based on the Gaussian graphical model. It applies penalization to the mean and precision matrix parameters to generate regularized and interpretable estimates. More importantly, a fusion penalty is imposed to “automatedly” determine the number of subgroups and generate more concise, reliable, and interpretable estimation. Consistency properties are rigorously established, and an effective computational algorithm is developed. The heterogeneity analysis of non-small-cell lung cancer based on single-cell gene expression data of the Wnt pathway and that of lung adenocarcinoma based on histopathological imaging data not only demonstrate the practical applicability of the proposed approach but also lead to interesting new findings.},
  archive      = {J_BIOMTC},
  author       = {Mingyang Ren and Sanguo Zhang and Qingzhao Zhang and Shuangge Ma},
  doi          = {10.1111/biom.13426},
  journal      = {Biometrics},
  number       = {2},
  pages        = {524-535},
  shortjournal = {Biometrics},
  title        = {Gaussian graphical model-based heterogeneity analysis via penalized fusion},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Information-incorporated gaussian graphical model for gene
expression data. <em>BIOMTC</em>, <em>78</em>(2), 512–523. (<a
href="https://doi.org/10.1111/biom.13428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the analysis of gene expression data, network approaches take a system perspective and have played an irreplaceably important role. Gaussian graphical models (GGMs) have been popular in the network analysis of gene expression data. They investigate the conditional dependence between genes and “transform” the problem of estimating network structures into a sparse estimation of precision matrices. When there is a moderate to large number of genes, the number of parameters to be estimated may overwhelm the limited sample size, leading to unreliable estimation and selection. In this article, we propose incorporating information from previous studies (for example, those deposited at PubMed) to assist estimating the network structure in the present data. It is recognized that such information can be partial, biased, or even wrong. A penalization-based estimation approach is developed, shown to have consistency properties, and realized using an effective computational algorithm. Simulation demonstrates its competitive performance under various information accuracy scenarios. The analysis of TCGA lung cancer prognostic genes leads to network structures different from the alternatives.},
  archive      = {J_BIOMTC},
  author       = {Huangdi Yi and Qingzhao Zhang and Cunjie Lin and Shuangge Ma},
  doi          = {10.1111/biom.13428},
  journal      = {Biometrics},
  number       = {2},
  pages        = {512-523},
  shortjournal = {Biometrics},
  title        = {Information-incorporated gaussian graphical model for gene expression data},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On polygenic risk scores for complex traits prediction.
<em>BIOMTC</em>, <em>78</em>(2), 499–511. (<a
href="https://doi.org/10.1111/biom.13466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polygenic risk scores (PRS) have gained substantial attention for complex traits prediction in genome-wide association studies (GWAS). Motivated by the polygenic model of complex traits, we study the statistical properties of PRS under the high-dimensional but sparsity free setting where the triplet ( n , p , m ) → ( ∞ , ∞ , ∞ ) $(n,p,m) \rightarrow (\infty , \infty , \infty )$ with n , p , m $n, p, m$ being the sample size, the number of assayed single-nucleotide polymorphisms (SNPs), and the number of assayed causal SNPs, respectively. First, we derive asymptotic results on the out-of-sample (prediction) R -squared for PRS. These results help understand the widespread observed gap between the in-sample heritability (or partial R -squared due to the genetic features) estimate and the out-of-sample R -squared for most complex traits. Next, we investigate how features should be selected (e.g., by a p -value threshold) for constructing optimal PRS. We reveal that the optimal threshold depends largely on the genetic architecture underlying the complex trait and the sample size of the training GWAS, or the m / n $m/n$ ratio. For highly polygenic traits with a large m / n $m/n$ ratio, it is difficult to separate causal and null SNPs and stringent feature selection in principle often leads to poor PRS prediction. We numerically illustrate the theoretical results with intensive simulation studies and real data analysis on 33 complex traits with a wide range of genetic architectures in the UK Biobank database.},
  archive      = {J_BIOMTC},
  author       = {Bingxin Zhao and Fei Zou},
  doi          = {10.1111/biom.13466},
  journal      = {Biometrics},
  number       = {2},
  pages        = {499-511},
  shortjournal = {Biometrics},
  title        = {On polygenic risk scores for complex traits prediction},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A generalized robust allele-based genetic association test.
<em>BIOMTC</em>, <em>78</em>(2), 487–498. (<a
href="https://doi.org/10.1111/biom.13456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The allele-based association test, comparing allele frequency difference between case and control groups, is locally most powerful. However, application of the classical allelic test is limited in practice, because the method is sensitive to the Hardy–Weinberg equilibrium (HWE) assumption, not applicable to continuous traits, and not easy to account for covariate effect or sample correlation. To develop a generalized robust allelic test, we propose a new allele-based regression model with individual allele as the response variable. We show that the score test statistic derived from this robust and unifying regression framework contains a correction factor that explicitly adjusts for potential departure from HWE and encompasses the classical allelic test as a special case. When the trait of interest is continuous, the corresponding allelic test evaluates a weighted difference between individual-level allele frequency estimate and sample estimate where the weight is proportional to an individual&#39;s trait value, and the test remains valid under Y -dependent sampling. Finally, the proposed allele-based method can analyze multiple (continuous or binary) phenotypes simultaneously and multiallelic genetic markers, while accounting for covariate effect, sample correlation, and population heterogeneity. To support our analytical findings, we provide empirical evidence from both simulation and application studies.},
  archive      = {J_BIOMTC},
  author       = {Lin Zhang and Lei Sun},
  doi          = {10.1111/biom.13456},
  journal      = {Biometrics},
  number       = {2},
  pages        = {487-498},
  shortjournal = {Biometrics},
  title        = {A generalized robust allele-based genetic association test},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Varying coefficient frailty models with applications in
single molecular experiments. <em>BIOMTC</em>, <em>78</em>(2), 474–486.
(<a href="https://doi.org/10.1111/biom.13448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by an analysis of single molecular experiments in the study of T-cell signaling, a new model called varying coefficient frailty model with local linear estimation is proposed. Frailty models have been extensively studied, but extensions to nonconstant coefficients are limited to spline-based methods that tend to produce estimation bias near the boundary. To address this problem, we introduce a local polynomial kernel smoothing technique with a modified expectation-maximization algorithm to estimate the unknown parameters. Theoretical properties of the estimators, including their unbiased property near the boundary, are derived along with discussions on the asymptotic bias-variance trade-off. The finite sample performance is examined by simulation studies, and comparisons with existing spline-based approaches are conducted to show the potential advantages of the proposed approach. The proposed method is implemented for the analysis of T-cell signaling. The fitted varying coefficient model provides a rigorous quantification of an early and rapid impact on T-cell signaling from the accumulation of bond lifetime, which can shed new light on the fundamental understanding of how T cells initiate immune responses.},
  archive      = {J_BIOMTC},
  author       = {Ying Hung and Li-Hsiang Lin and C. F. Jeff Wu},
  doi          = {10.1111/biom.13448},
  journal      = {Biometrics},
  number       = {2},
  pages        = {474-486},
  shortjournal = {Biometrics},
  title        = {Varying coefficient frailty models with applications in single molecular experiments},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cox regression model under dependent truncation.
<em>BIOMTC</em>, <em>78</em>(2), 460–473. (<a
href="https://doi.org/10.1111/biom.13451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Truncation is a statistical phenomenon that occurs in many time-to-event studies. For example, autopsy-confirmed studies of neurodegenerative diseases are subject to an inherent left and right truncation, also known as double truncation. When the goal is to study the effect of risk factors on survival, the standard Cox regression model cannot be used when the survival time is subject to truncation. Existing methods that adjust for both left and right truncation in the Cox regression model require independence between the survival times and truncation times, which may not be a reasonable assumption in practice. We propose an expectation-maximization algorithm to relax the independence assumption in the Cox regression model under left, right, or double truncation to an assumption of conditional independence on the observed covariates. The resulting regression coefficient estimators are consistent and asymptotically normal. We demonstrate through extensive simulations that the proposed estimator has little bias and has a similar or lower mean-squared error compared to existing estimators. We implement our approach to assess the effect of occupation on survival in subjects with autopsy-confirmed Alzheimer&#39;s disease.},
  archive      = {J_BIOMTC},
  author       = {Lior Rennert and Sharon X. Xie},
  doi          = {10.1111/biom.13451},
  journal      = {Biometrics},
  number       = {2},
  pages        = {460-473},
  shortjournal = {Biometrics},
  title        = {Cox regression model under dependent truncation},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric estimation of the nonmixture cure model with
auxiliary survival information. <em>BIOMTC</em>, <em>78</em>(2),
448–459. (<a href="https://doi.org/10.1111/biom.13450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With rapidly increasing data sources, statistical methods that make use of external information are gradually becoming popular tools in medical research. In this article, we efficiently synthesize the auxiliary survival information and propose a semiparametric estimation method for the combined empirical likelihood in the framework of the nonmixture cure model, to enhance inference about the associations between exposures and disease outcomes. The auxiliary survival probabilities from external sources are first summarized as unbiased estimation equations, which help produce more efficient estimates of the effects of interest and improve the prediction accuracy for the risk of the event. Then we develop a Bernstein-based sieve empirical likelihood method to estimate the parametric and nonparametric components simultaneously. Such an estimation procedure allows us to reduce the computation burden while preserving the shape constraint on the baseline distribution function. The resulting estimators for the true associations are strongly consistent and asymptotically normal. Instead of collecting substantial exposure data, the auxiliary survival information at multiple time points is incorporated, which further reduces the mean squared error of the estimators. This contributes to biomarker evaluation and treatment effect analysis within smaller studies. We show how to choose the number of auxiliary survival probabilities appropriately and provide a guideline for practical applications. Simulation studies demonstrate that the estimators enjoy large gains in efficiency. A melanoma dataset is analyzed for illustrating the methodology.},
  archive      = {J_BIOMTC},
  author       = {Bo Han and Ingrid Van Keilegom and Xiaoguang Wang},
  doi          = {10.1111/biom.13450},
  journal      = {Biometrics},
  number       = {2},
  pages        = {448-459},
  shortjournal = {Biometrics},
  title        = {Semiparametric estimation of the nonmixture cure model with auxiliary survival information},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint model for survival and multivariate sparse functional
data with application to a study of alzheimer’s disease.
<em>BIOMTC</em>, <em>78</em>(2), 435–447. (<a
href="https://doi.org/10.1111/biom.13427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies of Alzheimer&#39;s disease (AD) often collect multiple longitudinal clinical outcomes, which are correlated and predictive of AD progression. It is of great scientific interest to investigate the association between the outcomes and time to AD onset. We model the multiple longitudinal outcomes as multivariate sparse functional data and propose a functional joint model linking multivariate functional data to event time data. In particular, we propose a multivariate functional mixed model to identify the shared progression pattern and outcome-specific progression patterns of the outcomes, which enables more interpretable modeling of associations between outcomes and AD onset. The proposed method is applied to the Alzheimer&#39;s Disease Neuroimaging Initiative study (ADNI) and the functional joint model sheds new light on inference of five longitudinal outcomes and their associations with AD onset. Simulation studies also confirm the validity of the proposed model. Data used in preparation of this article were obtained from the ADNI database.},
  archive      = {J_BIOMTC},
  author       = {Cai Li and Luo Xiao and Sheng Luo},
  doi          = {10.1111/biom.13427},
  journal      = {Biometrics},
  number       = {2},
  pages        = {435-447},
  shortjournal = {Biometrics},
  title        = {Joint model for survival and multivariate sparse functional data with application to a study of alzheimer&#39;s disease},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonparametric estimation of spearman’s rank correlation with
bivariate survival data. <em>BIOMTC</em>, <em>78</em>(2), 421–434. (<a
href="https://doi.org/10.1111/biom.13453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study rank-based approaches to estimate the correlation between two right-censored variables. With end-of-study censoring, it is often impossible to nonparametrically identify the complete bivariate survival distribution, and therefore it is impossible to nonparametrically compute Spearman&#39;s rank correlation. As a solution, we propose two measures that can be nonparametrically estimated. The first measure is Spearman&#39;s correlation in a restricted region. The second measure is Spearman&#39;s correlation for an altered but estimable joint distribution. We describe population parameters for these measures and illustrate how they are similar to and different from the overall Spearman&#39;s correlation. We propose consistent estimators of these measures and study their performance through simulations. We illustrate our methods with a study assessing the correlation between the time to viral failure and the time to regimen change among persons living with HIV in Latin America who start antiretroviral therapy.},
  archive      = {J_BIOMTC},
  author       = {Svetlana K. Eden and Chun Li and Bryan E. Shepherd},
  doi          = {10.1111/biom.13453},
  journal      = {Biometrics},
  number       = {2},
  pages        = {421-434},
  shortjournal = {Biometrics},
  title        = {Nonparametric estimation of spearman&#39;s rank correlation with bivariate survival data},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Omic association studies with r and bioconductor: Juan r.
Gonzalez and alejandro caceres boca raton, FL: Chapman and hall/CRC,
2020. Pp. 390. <em>BIOMTC</em>, <em>78</em>(1), 415–416. (<a
href="https://doi.org/10.1111/biom.13572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Amber W. Wang},
  doi          = {10.1111/biom.13572},
  journal      = {Biometrics},
  number       = {1},
  pages        = {415-416},
  shortjournal = {Biometrics},
  title        = {Omic association studies with r and bioconductor: juan r. gonzalez and alejandro caceres boca raton, FL: chapman and Hall/CRC, 2020. pp. 390},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Meta-analysis: Methods for health and experimental studies:
Shahjahan khan springer nature singapore pte. Ltd. 2020. Pp. 293.
<em>BIOMTC</em>, <em>78</em>(1), 414. (<a
href="https://doi.org/10.1111/biom.13605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Tugba Akkaya-Hocagil},
  doi          = {10.1111/biom.13605},
  journal      = {Biometrics},
  number       = {1},
  pages        = {414},
  shortjournal = {Biometrics},
  title        = {Meta-analysis: methods for health and experimental studies: shahjahan khan springer nature singapore pte. ltd. 2020. pp. 293},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian analysis of infectious diseases - COVID-19 and
beyond, by lyle broemeling lyle d. Broemeling boca raton, FL: Chapman
and hall/CRC, 2021. Pp. 330. <em>BIOMTC</em>, <em>78</em>(1), 412–413.
(<a href="https://doi.org/10.1111/biom.13607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Marie V. Ozanne},
  doi          = {10.1111/biom.13607},
  journal      = {Biometrics},
  number       = {1},
  pages        = {412-413},
  shortjournal = {Biometrics},
  title        = {Bayesian analysis of infectious diseases - COVID-19 and beyond, by lyle broemeling lyle d. broemeling boca raton, FL: Chapman and Hall/CRC, 2021. pp. 330},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical inference via data science: A modern dive into r
and the tidyverse, chester ismay and albert y. Kim boca raton, FL:
Chapman and hall/CRC, 2019. Pp. 460. <em>BIOMTC</em>, <em>78</em>(1),
410–412. (<a href="https://doi.org/10.1111/biom.13620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Zachary Fusfeld},
  doi          = {10.1111/biom.13620},
  journal      = {Biometrics},
  number       = {1},
  pages        = {410-412},
  shortjournal = {Biometrics},
  title        = {Statistical inference via data science: a modern dive into r and the tidyverse, chester ismay and albert y. kim boca raton, FL: chapman and Hall/CRC, 2019. pp. 460},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modelling spatial and spatial-temporal data: A bayesian
approach robert p. Haining and guangquan li boca raton, FL: Chapman and
hall/CRC, 2020. Pp. 608. <em>BIOMTC</em>, <em>78</em>(1), 409–410. (<a
href="https://doi.org/10.1111/biom.13619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Howard H. Chang},
  doi          = {10.1111/biom.13619},
  journal      = {Biometrics},
  number       = {1},
  pages        = {409-410},
  shortjournal = {Biometrics},
  title        = {Modelling spatial and spatial-temporal data: a bayesian approach robert p. haining and guangquan li boca raton, FL: chapman and Hall/CRC, 2020. pp. 608},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rejoinder to “biased estimation with shared parameter models
in the presence of competing dropout mechanisms.” <em>BIOMTC</em>,
<em>78</em>(1), 407–408. (<a
href="https://doi.org/10.1111/biom.13437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Christos Thomadakis and Loukia Meligkotsidou and Nikos Pantazis and Giota Touloumi},
  doi          = {10.1111/biom.13437},
  journal      = {Biometrics},
  number       = {1},
  pages        = {407-408},
  shortjournal = {Biometrics},
  title        = {Rejoinder to “Biased estimation with shared parameter models in the presence of competing dropout mechanisms”},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Biased estimation with shared parameter models in the
presence of competing dropout mechanisms. <em>BIOMTC</em>,
<em>78</em>(1), 399–406. (<a
href="https://doi.org/10.1111/biom.13438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Thomadakis et al . quantified potential sources of bias that can occur when shared parameter (SP) models are used to jointly model longitudinal trends of a biomarker over time (e.g., a slope) and time-to-dropout in an effort to address concerns over possible informative censoring. Although SP models induce no bias under a missingness completely at random dropout mechanism, the authors demonstrate that bias can occur under a missingness at random (MAR) dropout mechanism wherein dropout depends on the observed biomarker data. To address this, the authors propose including the most recent observed marker value within the hazard function for the time-to-dropout portion of an SP model. They demonstrate via a limited simulation that the proposed model minimizes bias under a specific MAR dropout mechanism and a specific missingness not-at-random dropout mechanism. In the present article, we compare and contrast their work with that of previous authors by illustrating via simulation and an example the degree of bias or lack thereof that can occur when applying SP models, particularly, in the presence of competing dropout mechanisms. We propose the use of a competing risk SP model as a means to minimize bias whenever competing dropout mechanisms are suspected assuming the competing mechanisms result from distinct observable causes of dropout.},
  archive      = {J_BIOMTC},
  author       = {Edward F. Vonesh and Tom Greene},
  doi          = {10.1111/biom.13438},
  journal      = {Biometrics},
  number       = {1},
  pages        = {399-406},
  shortjournal = {Biometrics},
  title        = {Biased estimation with shared parameter models in the presence of competing dropout mechanisms},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Power and sample size for observational studies of point
exposure effects. <em>BIOMTC</em>, <em>78</em>(1), 388–398. (<a
href="https://doi.org/10.1111/biom.13405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse probability of treatment weights (IPTWs) are commonly used to control for confounding when estimating causal effects of point exposures from observational data. When planning a study that will be analyzed with IPTWs, determining the required sample size for a given level of statistical power is challenging because of the effect of weighting on the variance of the estimated causal means. This paper considers the utility of the design effect to quantify the effect of weighting on the precision of causal estimates. The design effect is defined as the ratio of the variance of the causal mean estimator divided by the variance of a naïve estimator if, counter to fact, no confounding had been present and weights were not needed. A simple, closed-form approximation of the design effect is derived that is outcome invariant and can be estimated during the study design phase. Once the design effect is approximated for each treatment group, sample size calculations are conducted as for a randomized trial, but with variances inflated by the design effects to account for weighting. Simulations demonstrate the accuracy of the design effect approximation, and practical considerations are discussed.},
  archive      = {J_BIOMTC},
  author       = {Bonnie E. Shook-Sa and Michael G. Hudgens},
  doi          = {10.1111/biom.13405},
  journal      = {Biometrics},
  number       = {1},
  pages        = {388-398},
  shortjournal = {Biometrics},
  title        = {Power and sample size for observational studies of point exposure effects},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Random effects models of lymph node metastases in breast
cancer: Quantifying the roles of covariates and screening using a
continuous growth model. <em>BIOMTC</em>, <em>78</em>(1), 376–387. (<a
href="https://doi.org/10.1111/biom.13430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We recently described a joint model of breast cancer tumor size and number of affected lymph nodes, which conditions on screening history, mammographic density, and mode of detection, and can be used to infer growth rates, time to symptomatic detection, screening sensitivity, and rates of lymph node spread. The model of lymph node spread can be estimated in isolation from measurements of tumor volume and number of affected lymph nodes, giving inference identical to the joint model. Here, we extend our model to include covariate effects. We also derive theoretical results in order to study the role of screening on lymph node metastases at diagnosis. We analyze the association between hormone replacement therapy (HRT) and breast cancer lymph node spread, using data from a case-control study designed specifically to study the effects of HRT on breast cancer. Using our method, we estimate that women using HRT at time of diagnosis have a 36\% lower rate of lymph node spread than nonusers (95\% confidence interval [CI] =(8\%,58\%)). This can be contrasted with the effect of HRT on the tumor growth rate, estimated here to be 15\% slower in HRT users (95\% CI = (−34\%,+7\%)). For screen-detected cancers, we illustrate how lead time can relate to lymph node spread; and using symptomatic cancers, we illustrate the potential consequences of false negative screens in terms of lymph node spread.},
  archive      = {J_BIOMTC},
  author       = {Gabriel Isheden and Kamila Czene and Keith Humphreys},
  doi          = {10.1111/biom.13430},
  journal      = {Biometrics},
  number       = {1},
  pages        = {376-387},
  shortjournal = {Biometrics},
  title        = {Random effects models of lymph node metastases in breast cancer: Quantifying the roles of covariates and screening using a continuous growth model},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized multi-SNP mediation intersection–union test.
<em>BIOMTC</em>, <em>78</em>(1), 364–375. (<a
href="https://doi.org/10.1111/biom.13418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To elucidate the molecular mechanisms underlying genetic variants identified from genome-wide association studies (GWAS) for a variety of phenotypic traits encompassing binary, continuous, count, and survival outcomes, we propose a novel and flexible method to test for mediation that can simultaneously accommodate multiple genetic variants and different types of outcome variables. Specifically, we employ the intersection–union test approach combined with the likelihood ratio test to detect mediation effect of multiple genetic variants via some mediator (e.g., the expression of a neighboring gene) on outcome. We fit high-dimensional generalized linear mixed models under the mediation framework, separately under the null and alternative hypothesis. We leverage Laplace approximation to compute the marginal likelihood of outcome and use coordinate descent algorithm to estimate corresponding parameters. Our extensive simulations demonstrate the validity of our proposed methods and substantial, up to 97\%, power gains over alternative methods. Applications to real data for the study of Chlamydia trachomatis infection further showcase advantages of our methods. We believe our proposed methods will be of value and general interest in this post-GWAS era to disentangle the potential causal mechanism from DNA to phenotype for new drug discovery and personalized medicine.},
  archive      = {J_BIOMTC},
  author       = {Wujuan Zhong and Toni Darville and Xiaojing Zheng and Jason Fine and Yun Li},
  doi          = {10.1111/biom.13418},
  journal      = {Biometrics},
  number       = {1},
  pages        = {364-375},
  shortjournal = {Biometrics},
  title        = {Generalized multi-SNP mediation intersection–union test},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A closed max-t test for multiple comparisons of areas under
the ROC curve. <em>BIOMTC</em>, <em>78</em>(1), 352–363. (<a
href="https://doi.org/10.1111/biom.13401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparing areas under the ROC curve (AUCs) is a popular approach to compare prognostic biomarkers. The aim of this paper is to present an efficient method to control the family-wise error rate when multiple comparisons are performed. We suggest to combine the max-t test and the closed testing procedures. We build on previous work on asymptotic results for ROC curves and on general multiple testing methods to efficiently take into account both the correlations between the test statistics and the logical constraints between the null hypotheses. The proposed method results in an uniformly more powerful procedure than both the single-step max-t test procedure and popular stepwise extensions of the Bonferroni procedure, such as Bonferroni–Holm. As demonstrated in this paper, the method can be applied in most usual contexts, including the time-dependent context with right censored data. We show how the method works in practice through a motivating example where we compare several psychometric scores to predict the t-year risk of Alzheimer&#39;s disease. The example illustrates several multiple testing settings and demonstrates the advantage of using the proposed methods over common alternatives. R code has been made available to facilitate the use of the methods by others.},
  archive      = {J_BIOMTC},
  author       = {Paul Blanche and Jean-François Dartigues and Jérémie Riou},
  doi          = {10.1111/biom.13401},
  journal      = {Biometrics},
  number       = {1},
  pages        = {352-363},
  shortjournal = {Biometrics},
  title        = {A closed max-t test for multiple comparisons of areas under the ROC curve},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating the optimal individualized treatment rule from a
cost-effectiveness perspective. <em>BIOMTC</em>, <em>78</em>(1),
337–351. (<a href="https://doi.org/10.1111/biom.13406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal individualized treatment rules (ITRs) provide customized treatment recommendations based on subject characteristics to maximize clinical benefit in accordance with the objectives in precision medicine. As a result, there is growing interest in developing statistical tools for estimating optimal ITRs in evidence-based research. In health economic perspectives, policy makers consider the tradeoff between health gains and incremental costs of interventions to set priorities and allocate resources. However, most work on ITRs has focused on maximizing the effectiveness of treatment without considering costs. In this paper, we jointly consider the impact of effectiveness and cost on treatment decisions and define ITRs under a composite-outcome setting, so that we identify the most cost-effective ITR that accounts for individual-level heterogeneity through direct optimization. In particular, we propose a decision-tree–based statistical learning algorithm that uses a net-monetary-benefit–based reward to provide nonparametric estimations of the optimal ITR. We provide several approaches to estimating the reward underlying the ITR as a function of subject characteristics. We present the strengths and weaknesses of each approach and provide practical guidelines by comparing their performance in simulation studies. We illustrate the top-performing approach from our simulations by evaluating the projected 15-year personalized cost-effectiveness of the intensive blood pressure control of the Systolic Blood Pressure Intervention Trial (SPRINT) study.},
  archive      = {J_BIOMTC},
  author       = {Yizhe Xu and Tom H. Greene and Adam P. Bress and Brian C. Sauer and Brandon K. Bellows and Yue Zhang and William S. Weintraub and Andrew E. Moran and Jincheng Shen},
  doi          = {10.1111/biom.13406},
  journal      = {Biometrics},
  number       = {1},
  pages        = {337-351},
  shortjournal = {Biometrics},
  title        = {Estimating the optimal individualized treatment rule from a cost-effectiveness perspective},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simultaneous spatial smoothing and outlier detection using
penalized regression, with application to childhood obesity surveillance
from electronic health records. <em>BIOMTC</em>, <em>78</em>(1),
324–336. (<a href="https://doi.org/10.1111/biom.13404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records (EHRs) have become a platform for data-driven granular-level surveillance in recent years. In this paper, we make use of EHRs for early prevention of childhood obesity. The proposed method simultaneously provides smooth disease mapping and outlier information for obesity prevalence that are useful for raising public awareness and facilitating targeted intervention. More precisely, we consider a penalized multilevel generalized linear model. We decompose regional contribution into smooth and sparse signals, which are automatically identified by a combination of fusion and sparse penalties imposed on the likelihood function. In addition, we weigh the proposed likelihood to account for the missingness and potential nonrepresentativeness arising from the EHR data. We develop a novel alternating minimization algorithm, which is computationally efficient, easy to implement, and guarantees convergence. Simulation studies demonstrate superior performance of the proposed method. Finally, we apply our method to the University of Wisconsin Population Health Information Exchange database.},
  archive      = {J_BIOMTC},
  author       = {Young-Geun Choi and Lawrence P. Hanrahan and Derek Norton and Ying-Qi Zhao},
  doi          = {10.1111/biom.13404},
  journal      = {Biometrics},
  number       = {1},
  pages        = {324-336},
  shortjournal = {Biometrics},
  title        = {Simultaneous spatial smoothing and outlier detection using penalized regression, with application to childhood obesity surveillance from electronic health records},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A nonparametric bayesian model for estimating spectral
densities of resting-state EEG twin data. <em>BIOMTC</em>,
<em>78</em>(1), 313–323. (<a
href="https://doi.org/10.1111/biom.13393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) is a noninvasive neuroimaging modality that captures electrical brain activity many times per second. We seek to estimate power spectra from EEG data that ware gathered for 557 adolescent twin pairs through the Minnesota Twin Family Study (MTFS). Typically, spectral analysis methods treat time series from each subject separately, and independent spectral densities are fit to each time series. Since the EEG data were collected on twins, it is reasonable to assume that the time series have similar underlying characteristics, so borrowing information across subjects can significantly improve estimation. We propose a Nested Bernstein Dirichlet prior model to estimate the power spectrum of the EEG signal for each subject by smoothing periodograms within and across subjects while requiring minimal user input to tuning parameters. Furthermore, we leverage the MTFS twin study design to estimate the heritability of EEG power spectra with the hopes of establishing new endophenotypes. Through simulation studies designed to mimic the MTFS, we show our method out-performs a set of other popular methods.},
  archive      = {J_BIOMTC},
  author       = {Brian Hart and Michele Guindani and Stephen Malone and Mark Fiecas},
  doi          = {10.1111/biom.13393},
  journal      = {Biometrics},
  number       = {1},
  pages        = {313-323},
  shortjournal = {Biometrics},
  title        = {A nonparametric bayesian model for estimating spectral densities of resting-state EEG twin data},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian dose regimen assessment in early phase oncology
incorporating pharmacokinetics and pharmacodynamics. <em>BIOMTC</em>,
<em>78</em>(1), 300–312. (<a
href="https://doi.org/10.1111/biom.13433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase I dose-finding trials in oncology seek to find the maximum tolerated dose of a drug under a specific schedule. Evaluating drug schedules aims at improving treatment safety while maintaining efficacy. However, while we can reasonably assume that toxicity increases with the dose for cytotoxic drugs, the relationship between toxicity and multiple schedules remains elusive. We proposed a Bayesian dose regimen assessment method (DRtox) using pharmacokinetics/pharmacodynamics (PK/PD) to estimate the maximum tolerated dose regimen (MTD-regimen) at the end of the dose-escalation stage of a trial. We modeled the binary toxicity via a PD endpoint and estimated the dose regimen toxicity relationship through the integration of a dose regimen PD model and a PD toxicity model. For the first model, we considered nonlinear mixed-effects models, and for the second one, we proposed the following two Bayesian approaches: a logistic model and a hierarchical model. In an extensive simulation study, the DRtox outperformed traditional designs in terms of proportion of correctly selecting the MTD-regimen. Moreover, the inclusion of PK/PD information helped provide more precise estimates for the entire dose regimen toxicity curve; therefore the DRtox may recommend alternative untested regimens for expansion cohorts. The DRtox was developed to be applied at the end of the dose-escalation stage of an ongoing trial for patients with relapsed or refractory acute myeloid leukemia (NCT03594955) once all toxicity and PK/PD data are collected.},
  archive      = {J_BIOMTC},
  author       = {Emma Gerard and Sarah Zohar and Hoai-Thu Thai and Christelle Lorenzato and Marie-Karelle Riviere and Moreno Ursino},
  doi          = {10.1111/biom.13433},
  journal      = {Biometrics},
  number       = {1},
  pages        = {300-312},
  shortjournal = {Biometrics},
  title        = {Bayesian dose regimen assessment in early phase oncology incorporating pharmacokinetics and pharmacodynamics},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modelling group movement with behaviour switching in
continuous time. <em>BIOMTC</em>, <em>78</em>(1), 286–299. (<a
href="https://doi.org/10.1111/biom.13412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a new method for modelling collective movement in continuous time with behavioural switching, motivated by simultaneous tracking of wild or semi-domesticated animals. Each individual in the group is at times attracted to a unobserved leading point. However, the behavioural state of each individual can switch between ‘following’ and ‘independent’. The ‘following’ movement is modelled through a linear stochastic differential equation, while the ‘independent’ movement is modelled as Brownian motion. The movement of the leading point is modelled either as an Ornstein-Uhlenbeck (OU) process or as Brownian motion (BM), which makes the whole system a higher-dimensional Ornstein-Uhlenbeck process, possibly an intrinsic non-stationary version. An inhomogeneous Kalman filter Markov chain Monte Carlo algorithm is developed to estimate the diffusion and switching parameters and the behaviour states of each individual at a given time point. The method successfully recovers the true behavioural states in simulated data sets , and is also applied to model a group of simultaneously tracked reindeer ( Rangifer tarandus ).},
  archive      = {J_BIOMTC},
  author       = {Mu Niu and Fay Frost and Jordan E. Milner and Anna Skarin and Paul G. Blackwell},
  doi          = {10.1111/biom.13412},
  journal      = {Biometrics},
  number       = {1},
  pages        = {286-299},
  shortjournal = {Biometrics},
  title        = {Modelling group movement with behaviour switching in continuous time},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A latent capture history model for digital aerial surveys.
<em>BIOMTC</em>, <em>78</em>(1), 274–285. (<a
href="https://doi.org/10.1111/biom.13403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We anticipate that unmanned aerial vehicles will become popular wildlife survey platforms. Because detecting animals from the air is imperfect, we develop a mark-recapture line transect method using two digital cameras, possibly mounted on one aircraft, which cover the same area with a short time delay between them. Animal movement between the passage of the cameras introduces uncertainty in individual identity, so individual capture histories are unobservable and are treated as latent variables. We obtain the likelihood for mark-recapture line transects without capture histories by automatically enumerating all possibilities within segments of the transect that contain ambiguous identities, instead of attempting to decide identities in a prior step. We call this method “Latent Capture-history Enumeration” (LCE). We include an availability model for species that are periodically unavailable for detection, such as cetaceans that are undetectable while diving. External data are needed to estimate the availability cycle length, but not the mean availability rate, if the full availability model is employed. We compare the LCE method with the recently developed cluster capture-recapture method (CCR), which uses a Palm likelihood approximation, providing the first comparison of CCR with maximum likelihood. The LCE estimator has slightly lower variance, more so as sample size increases, and close to nominal coverage probabilities. Both methods are approximately unbiased. We illustrate with semisynthetic data from a harbor porpoise survey.},
  archive      = {J_BIOMTC},
  author       = {David L. Borchers and Peter Nightingale and Ben C. Stevenson and Rachel M. Fewster},
  doi          = {10.1111/biom.13403},
  journal      = {Biometrics},
  number       = {1},
  pages        = {274-285},
  shortjournal = {Biometrics},
  title        = {A latent capture history model for digital aerial surveys},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Speeding up monte carlo simulations for the adaptive sum of
powered score test with importance sampling. <em>BIOMTC</em>,
<em>78</em>(1), 261–273. (<a
href="https://doi.org/10.1111/biom.13407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A central but challenging problem in genetic studies is to test for (usually weak) associations between a complex trait (e.g., a disease status) and sets of multiple genetic variants. Due to the lack of a uniformly most powerful test, data-adaptive tests, such as the adaptive sum of powered score (aSPU) test, are advantageous in maintaining high power against a wide range of alternatives. However, there is often no closed-form to accurately and analytically calculate the p -values of many adaptive tests like aSPU, thus Monte Carlo (MC) simulations are often used, which can be time consuming to achieve a stringent significance level (e.g., 5e-8) used in genome-wide association studies (GWAS). To estimate such a small p -value, we need a huge number of MC simulations (e.g., 1e+10). As an alternative, we propose using importance sampling to speed up such calculations. We develop some theory to motivate a proposed algorithm for the aSPU test, and show that the proposed method is computationally more efficient than the standard MC simulations. Using both simulated and real data, we demonstrate the superior performance of the new method over the standard MC simulations.},
  archive      = {J_BIOMTC},
  author       = {Yangqing Deng and Yinqiu He and Gongjun Xu and Wei Pan},
  doi          = {10.1111/biom.13407},
  journal      = {Biometrics},
  number       = {1},
  pages        = {261-273},
  shortjournal = {Biometrics},
  title        = {Speeding up monte carlo simulations for the adaptive sum of powered score test with importance sampling},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Density estimation for circular data observed with errors.
<em>BIOMTC</em>, <em>78</em>(1), 248–260. (<a
href="https://doi.org/10.1111/biom.13431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Until now the problem of estimating circular densities when data are observed with errors has been mainly treated by Fourier series methods. We propose kernel-based estimators exhibiting simple construction and easy implementation. Specifically, we consider three different approaches: the first one is based on the equivalence between kernel estimators using data corrupted with different levels of error. This proposal appears to be totally unexplored, despite its potential for application also in the Euclidean setting. The second approach relies on estimators whose weight functions are circular deconvolution kernels. Due to the periodicity of the involved densities, it requires ad hoc mathematical tools. Finally, the third one is based on the idea of correcting extra bias of kernel estimators which use contaminated data and is essentially an adaptation of the standard theory to the circular case. For all the proposed estimators, we derive asymptotic properties, provide some simulation results, and also discuss some possible generalizations and extensions. Real data case studies are also included.},
  archive      = {J_BIOMTC},
  author       = {Marco Di Marzio and Stefania Fensore and Agnese Panzera and Charles C. Taylor},
  doi          = {10.1111/biom.13431},
  journal      = {Biometrics},
  number       = {1},
  pages        = {248-260},
  shortjournal = {Biometrics},
  title        = {Density estimation for circular data observed with errors},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simultaneous confidence intervals for ranks with application
to ranking institutions. <em>BIOMTC</em>, <em>78</em>(1), 238–247. (<a
href="https://doi.org/10.1111/biom.13419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a ranking of institutions such as medical centers or universities is based on a numerical measure of performance provided with a standard error, confidence intervals (CIs) should be calculated to assess the uncertainty of these ranks. We present a novel method based on Tukey&#39;s honest significant difference test to construct simultaneous CIs for the true ranks. When all the true performances are equal, the probability of coverage of our method attains the nominal level. In case the true performance measures have no exact ties, our method is conservative. For this situation, we propose a rescaling method to the nominal level that results in shorter CIs while keeping control of the simultaneous coverage. We also show that a similar rescaling can be applied to correct a recently proposed Monte-Carlo based method, which is anticonservative. After rescaling, the two methods perform very similarly. However, the rescaling of the Monte-Carlo based method is computationally much more demanding and becomes infeasible when the number of institutions is larger than 30–50. We discuss another recently proposed method similar to ours based on simultaneous CIs for the true performance. We show that our method provides uniformly shorter CIs for the same confidence level. We illustrate the superiority of our new methods with a data analysis for travel time to work in the United States and on rankings of 64 hospitals in the Netherlands.},
  archive      = {J_BIOMTC},
  author       = {Diaa Al Mohamad and Jelle J. Goeman and Erik W. van Zwet},
  doi          = {10.1111/biom.13419},
  journal      = {Biometrics},
  number       = {1},
  pages        = {238-247},
  shortjournal = {Biometrics},
  title        = {Simultaneous confidence intervals for ranks with application to ranking institutions},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric imputation using conditional gaussian mixture
models under item nonresponse. <em>BIOMTC</em>, <em>78</em>(1), 227–237.
(<a href="https://doi.org/10.1111/biom.13410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imputation is a popular technique for handling item nonresponse. Parametric imputation is based on a parametric model for imputation and is not robust against the failure of the imputation model. Nonparametric imputation is fully robust but is not applicable when the dimension of covariates is large due to the curse of dimensionality. Semiparametric imputation is another robust imputation based on a flexible model where the number of model parameters can increase with the sample size. In this paper, we propose a new semiparametric imputation based on a more flexible model assumption than the Gaussian mixture model. In the proposed mixture model, we assume a conditional Gaussian model for the study variable given the auxiliary variables, but the marginal distribution of the auxiliary variables is not necessarily Gaussian. The proposed mixture model is more flexible and achieves a better approximation than the Gaussian mixture models. The proposed method is applicable to high-dimensional covariate problem by including a penalty function in the conditional log-likelihood function. The proposed method is applied to the 2017 Korean Household Income and Expenditure Survey conducted by Statistics Korea.},
  archive      = {J_BIOMTC},
  author       = {Danhyang Lee and Jae Kwang Kim},
  doi          = {10.1111/biom.13410},
  journal      = {Biometrics},
  number       = {1},
  pages        = {227-237},
  shortjournal = {Biometrics},
  title        = {Semiparametric imputation using conditional gaussian mixture models under item nonresponse},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical inference for association studies using
electronic health records: Handling both selection bias and outcome
misclassification. <em>BIOMTC</em>, <em>78</em>(1), 214–226. (<a
href="https://doi.org/10.1111/biom.13400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health research using electronic health records (EHR) has gained popularity, but misclassification of EHR-derived disease status and lack of representativeness of the study sample can result in substantial bias in effect estimates and can impact power and type I error. In this paper, we develop new strategies for handling disease status misclassification and selection bias in EHR-based association studies. We first focus on each type of bias separately. For misclassification, we propose three novel likelihood-based bias correction strategies. A distinguishing feature of the EHR setting is that misclassification may be related to patient-varying factors , and the proposed methods leverage data in the EHR to estimate misclassification rates without gold standard labels . For addressing selection bias, we describe how calibration and inverse probability weighting methods from the survey sampling literature can be extended and applied to the EHR setting. Addressing misclassification and selection biases simultaneously is a more challenging problem than dealing with each on its own, and we propose several new strategies. For all methods proposed, we derive valid standard error estimators and provide software for implementation. We provide a new suite of statistical estimation and inference strategies for addressing misclassification and selection bias simultaneously that is tailored to problems arising in EHR data analysis. We apply these methods to data from The Michigan Genomics Initiative, a longitudinal EHR-linked biorepository.},
  archive      = {J_BIOMTC},
  author       = {Lauren J. Beesley and Bhramar Mukherjee},
  doi          = {10.1111/biom.13400},
  journal      = {Biometrics},
  number       = {1},
  pages        = {214-226},
  shortjournal = {Biometrics},
  title        = {Statistical inference for association studies using electronic health records: Handling both selection bias and outcome misclassification},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). New multivariate tests for assessing covariate balance in
matched observational studies. <em>BIOMTC</em>, <em>78</em>(1), 202–213.
(<a href="https://doi.org/10.1111/biom.13395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose new tests for assessing whether covariates in a treatment group and matched control group are balanced in observational studies. The tests exhibit high power under a wide range of multivariate alternatives, some of which existing tests have little power for. The asymptotic permutation null distributions of the proposed tests are studied and the P -values calculated through the asymptotic results work well in simulation studies, facilitating the application of the test to large data sets. The tests are illustrated in a study of the effect of smoking on blood lead levels. The proposed tests are implemented in an R package BalanceCheck .},
  archive      = {J_BIOMTC},
  author       = {Hao Chen and Dylan S. Small},
  doi          = {10.1111/biom.13395},
  journal      = {Biometrics},
  number       = {1},
  pages        = {202-213},
  shortjournal = {Biometrics},
  title        = {New multivariate tests for assessing covariate balance in matched observational studies},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Restricted mean survival time as a function of restriction
time. <em>BIOMTC</em>, <em>78</em>(1), 192–201. (<a
href="https://doi.org/10.1111/biom.13414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restricted mean survival time (RMST) is a clinically interpretable and meaningful survival metric that has gained popularity in recent years. Several methods are available for regression modeling of RMST, most based on pseudo-observations or what is essentially an inverse-weighted complete-case analysis. No existing RMST regression method allows for the covariate effects to be expressed as functions over time. This is a considerable limitation, in light of the many hazard regression methods that do accommodate such effects. To address this void in the literature, we propose RMST methods that permit estimating time-varying effects. In particular, we propose an inference framework for directly modeling RMST as a continuous function of L . Large-sample properties are derived. Simulation studies are performed to evaluate the performance of the methods in finite sample sizes. The proposed framework is applied to kidney transplant data obtained from the Scientific Registry of Transplant Recipients.},
  archive      = {J_BIOMTC},
  author       = {Yingchao Zhong and Douglas E. Schaubel},
  doi          = {10.1111/biom.13414},
  journal      = {Biometrics},
  number       = {1},
  pages        = {192-201},
  shortjournal = {Biometrics},
  title        = {Restricted mean survival time as a function of restriction time},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weight calibration to improve efficiency for estimating pure
risks from the additive hazards model with the nested case-control
design. <em>BIOMTC</em>, <em>78</em>(1), 179–191. (<a
href="https://doi.org/10.1111/biom.13413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the efficiency of covariate-specific estimates of pure risk (one minus the survival function) when some covariates are only available for case-control samples nested in a cohort. We focus on the semiparametric additive hazards model in which the hazard function equals a baseline hazard plus a linear combination of covariates with either time-varying or time-invariant coefficients. A published approach uses the design-based inclusion probabilities to reweight the nested case-control data. We obtain more efficient estimates of pure risks by calibrating the design weights to data available in the entire cohort, for both time-varying and time-invariant covariate coefficients. We develop explicit variance formulas for the weight-calibrated estimates based on influence functions. Simulations show the improvement in precision by using weight calibration and confirm the consistency of variance estimators and the validity of inference based on asymptotic normality. Examples are provided using data from the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial Study (PLCO).},
  archive      = {J_BIOMTC},
  author       = {Yei Eun Shin and Ruth M. Pfeiffer and Barry I. Graubard and Mitchell H. Gail},
  doi          = {10.1111/biom.13413},
  journal      = {Biometrics},
  number       = {1},
  pages        = {179-191},
  shortjournal = {Biometrics},
  title        = {Weight calibration to improve efficiency for estimating pure risks from the additive hazards model with the nested case-control design},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of clustered interval-censored data using a class
of semiparametric partly linear frailty transformation models.
<em>BIOMTC</em>, <em>78</em>(1), 165–178. (<a
href="https://doi.org/10.1111/biom.13399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A flexible class of semiparametric partly linear frailty transformation models is considered for analyzing clustered interval-censored data, which arise naturally in complex diseases and dental research. This class of models features two nonparametric components, resulting in a nonparametric baseline survival function and a potential nonlinear effect of a continuous covariate. The dependence among failure times within a cluster is induced by a shared, unobserved frailty term. A sieve maximum likelihood estimation method based on piecewise linear functions is proposed. The proposed estimators of the regression, dependence, and transformation parameters are shown to be strongly consistent and asymptotically normal, whereas the estimators of the two nonparametric functions are strongly consistent with optimal rates of convergence. An extensive simulation study is conducted to study the finite-sample performance of the proposed estimators. We provide an application to a dental study for illustration.},
  archive      = {J_BIOMTC},
  author       = {Chun Yin Lee and Kin Yau Wong and K. F. Lam and Jinfeng Xu},
  doi          = {10.1111/biom.13399},
  journal      = {Biometrics},
  number       = {1},
  pages        = {165-178},
  shortjournal = {Biometrics},
  title        = {Analysis of clustered interval-censored data using a class of semiparametric partly linear frailty transformation models},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simultaneous variable selection and estimation for joint
models of longitudinal and failure time data with interval censoring.
<em>BIOMTC</em>, <em>78</em>(1), 151–164. (<a
href="https://doi.org/10.1111/biom.13387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses variable selection in the context of joint analysis of longitudinal data and failure time data. A large literature has been developed for either variable selection or the joint analysis but there exists only limited literature for variable selection in the context of the joint analysis when failure time data are right censored. Corresponding to this, we will consider the situation where instead of right-censored data, one observes interval-censored failure time data, a more general and commonly occurring form of failure time data. For the problem, a class of penalized likelihood-based procedures will be developed for simultaneous variable selection and estimation of relevant covariate effects for both longitudinal and failure time variables of interest. In particular, a Monte Carlo EM (MCEM) algorithm is presented for the implementation of the proposed approach. The proposed method allows for the number of covariates to be diverging with the sample size and is shown to have the oracle property. An extensive simulation study is conducted to assess the finite sample performance of the proposed approach and indicates that it works well in practical situations. An application is also provided.},
  archive      = {J_BIOMTC},
  author       = {Fengting Yi and Niansheng Tang and Jianguo Sun},
  doi          = {10.1111/biom.13387},
  journal      = {Biometrics},
  number       = {1},
  pages        = {151-164},
  shortjournal = {Biometrics},
  title        = {Simultaneous variable selection and estimation for joint models of longitudinal and failure time data with interval censoring},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-stage penalized regression screening to detect
biomarker–treatment interactions in randomized clinical trials.
<em>BIOMTC</em>, <em>78</em>(1), 141–150. (<a
href="https://doi.org/10.1111/biom.13424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional biomarkers such as genomics are increasingly being measured in randomized clinical trials. Consequently, there is a growing interest in developing methods that improve the power to detect biomarker–treatment interactions. We adapt recently proposed two-stage interaction detecting procedures in the setting of randomized clinical trials. We also propose a new stage 1 multivariate screening strategy using ridge regression to account for correlations among biomarkers. For this multivariate screening, we prove the asymptotic between-stage independence, required for familywise error rate control, under biomarker–treatment independence. Simulation results show that in various scenarios, the ridge regression screening procedure can provide substantially greater power than the traditional one-biomarker-at-a-time screening procedure in highly correlated data. We also exemplify our approach in two real clinical trial data applications.},
  archive      = {J_BIOMTC},
  author       = {Jixiong Wang and Ashish Patel and James M.S. Wason and Paul J. Newcombe},
  doi          = {10.1111/biom.13424},
  journal      = {Biometrics},
  number       = {1},
  pages        = {141-150},
  shortjournal = {Biometrics},
  title        = {Two-stage penalized regression screening to detect biomarker–treatment interactions in randomized clinical trials},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Obtaining optimal cutoff values for tree classifiers using
multiple biomarkers. <em>BIOMTC</em>, <em>78</em>(1), 128–140. (<a
href="https://doi.org/10.1111/biom.13409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical practices, multiple biomarkers are often combined using a prespecified classification rule with tree structure for diagnostic decisions. The classification structure and cutoff point at each node of a tree are usually chosen on an ad hoc basis, depending on decision makers&#39; experience. There is a lack of analytical approaches that lead to optimal prediction performance, and that guide the choice of optimal cutoff points in a pre-specified classification tree. In this paper, we propose to search for and estimate the optimal decision rule through an approach of rank correlation maximization. The proposed method is flexible, theoretically sound, and computationally feasible when many biomarkers are available for classification or prediction. Using the proposed approach, for a prespecified tree-structured classification rule, we can guide the choice of optimal cutoff points at tree nodes and estimate optimal prediction performance from multiple biomarkers combined.},
  archive      = {J_BIOMTC},
  author       = {Yuxin Zhu and Mei-Cheng Wang},
  doi          = {10.1111/biom.13409},
  journal      = {Biometrics},
  number       = {1},
  pages        = {128-140},
  shortjournal = {Biometrics},
  title        = {Obtaining optimal cutoff values for tree classifiers using multiple biomarkers},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint calibrated estimation of inverse probability of
treatment and censoring weights for marginal structural models.
<em>BIOMTC</em>, <em>78</em>(1), 115–127. (<a
href="https://doi.org/10.1111/biom.13411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Marginal structural models (MSMs) with inverse probability weighted estimators (IPWEs) are widely used to estimate causal effects of treatment sequences on longitudinal outcomes in the presence of time-varying confounding and dependent censoring. However, IPWEs for MSMs can be inefficient and unstable if weights are estimated by maximum likelihood. To improve the performance of IPWEs, covariate balancing weight (CBW) methods have been proposed and recently extended to MSMs. However, existing CBW methods for MSMs are inflexible for practical use because they often do not handle dependent censoring, nonbinary treatments, and longitudinal outcomes (instead of eventual outcomes at a study end). In this paper, we propose a joint calibration approach to CBW estimation for MSMs that can accommodate (1) both time-varying confounding and dependent censoring, (2) binary and nonbinary treatments, (3) eventual outcomes and longitudinal outcomes. We develop novel calibration restrictions by jointly eliminating covariate associations with both treatment assignment and censoring processes after weighting the observed data sample (i.e., to optimize covariate balance in finite samples). Two different methods are proposed to implement the calibration. Simulations show that IPWEs with calibrated weights perform better than IPWEs with weights from maximum likelihood and the “Covariate Balancing Propensity Score” method. We apply our method to a natural history study of HIV for estimating the effects of highly active antiretroviral therapy on CD4 cell counts over time.},
  archive      = {J_BIOMTC},
  author       = {Sean Yiu and Li Su},
  doi          = {10.1111/biom.13411},
  journal      = {Biometrics},
  number       = {1},
  pages        = {115-127},
  shortjournal = {Biometrics},
  title        = {Joint calibrated estimation of inverse probability of treatment and censoring weights for marginal structural models},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Causal inference in high dimensions: A marriage between
bayesian modeling and good frequentist properties. <em>BIOMTC</em>,
<em>78</em>(1), 100–114. (<a
href="https://doi.org/10.1111/biom.13417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a framework for estimating causal effects of binary and continuous treatments in high dimensions. We show how posterior distributions of treatment and outcome models can be used together with doubly robust estimators. We propose an approach to uncertainty quantification for the doubly robust estimator, which utilizes posterior distributions of model parameters and (1) results in good frequentist properties in small samples, (2) is based on a single run of a Markov chain Monte Carlo (MCMC) algorithm, and (3) improves over frequentist measures of uncertainty which rely on asymptotic properties. We consider a flexible framework for modeling the treatment and outcome processes within the Bayesian paradigm that reduces model dependence, accommodates nonlinearity, and achieves dimension reduction of the covariate space. We illustrate the ability of the proposed approach to flexibly estimate causal effects in high dimensions and appropriately quantify uncertainty. We show that our proposed variance estimation strategy is consistent when both models are correctly specified, and we see empirically that it performs well in finite samples and under model misspecification. Finally, we estimate the effect of continuous environmental exposures on cholesterol and triglyceride levels.},
  archive      = {J_BIOMTC},
  author       = {Joseph Antonelli and Georgia Papadogeorgou and Francesca Dominici},
  doi          = {10.1111/biom.13417},
  journal      = {Biometrics},
  number       = {1},
  pages        = {100-114},
  shortjournal = {Biometrics},
  title        = {Causal inference in high dimensions: A marriage between bayesian modeling and good frequentist properties},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Assuming independence in spatial latent variable models:
Consequences and implications of misspecification. <em>BIOMTC</em>,
<em>78</em>(1), 85–99. (<a
href="https://doi.org/10.1111/biom.13416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate spatial data, where multiple responses are simultaneously recorded across spatially indexed observational units, are routinely collected in a wide variety of disciplines. For example, the Southern Ocean Continuous Plankton Recorder survey collects records of zooplankton communities in the Indian sector of the Southern Ocean, with the aim of identifying and quantifying spatial patterns in biodiversity in response to environmental change. One increasingly popular method for modeling such data is spatial generalized linear latent variable models (GLLVMs), where the correlation across sites is captured by a spatial covariance function in the latent variables. However, little is known about the impact of misspecifying the latent variable correlation structure on inference of various parameters in such models. To address this gap in the literature, we investigate how misspecifying and assuming independence for the latent variables&#39; correlation structure impacts estimation and inference in spatial GLLVMs. Through both theory and numerical studies, we show that performance of maximum likelihood estimation and inference on regression coefficients under misspecification depends on a combination of the response type, the magnitude of true regression coefficient, and the corresponding loadings, and, most importantly, whether the corresponding covariate is (also) spatially correlated. On the other hand, estimation and inference of truly nonzero loadings and prediction of latent variables is consistently not robust to misspecification of the latent variable correlation structure.},
  archive      = {J_BIOMTC},
  author       = {Francis K.C. Hui and Nicole A. Hill and A.H. Welsh},
  doi          = {10.1111/biom.13416},
  journal      = {Biometrics},
  number       = {1},
  pages        = {85-99},
  shortjournal = {Biometrics},
  title        = {Assuming independence in spatial latent variable models: Consequences and implications of misspecification},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A spatial bayesian latent factor model for image-on-image
regression. <em>BIOMTC</em>, <em>78</em>(1), 72–84. (<a
href="https://doi.org/10.1111/biom.13420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-on-image regression analysis, using images to predict images, is a challenging task, due to (1) the high dimensionality and (2) the complex spatial dependence structures in image predictors and image outcomes. In this work, we propose a novel image-on-image regression model, by extending a spatial Bayesian latent factor model to image data, where low-dimensional latent factors are adopted to make connections between high-dimensional image outcomes and image predictors. We assign Gaussian process priors to the spatially varying regression coefficients in the model, which can well capture the complex spatial dependence among image outcomes as well as that among the image predictors. We perform simulation studies to evaluate the out-of-sample prediction performance of our method compared with linear regression and voxel-wise regression methods for different scenarios. The proposed method achieves better prediction accuracy by effectively accounting for the spatial dependence and efficiently reduces image dimensions with latent factors. We apply the proposed method to analysis of multimodal image data in the Human Connectome Project where we predict task-related contrast maps using subcortical volumetric seed maps.},
  archive      = {J_BIOMTC},
  author       = {Cui Guo and Jian Kang and Timothy D. Johnson},
  doi          = {10.1111/biom.13420},
  journal      = {Biometrics},
  number       = {1},
  pages        = {72-84},
  shortjournal = {Biometrics},
  title        = {A spatial bayesian latent factor model for image-on-image regression},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian group sequential enrichment designs based on
adaptive regression of response and survival time on baseline
biomarkers. <em>BIOMTC</em>, <em>78</em>(1), 60–71. (<a
href="https://doi.org/10.1111/biom.13421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision medicine relies on the idea that, for a particular targeted agent, only a subpopulation of patients is sensitive to it and thus may benefit from it therapeutically. In practice, it is often assumed based on preclinical data that a treatment-sensitive subpopulation is known, and moreover that the agent is substantively efficacious in that subpopulation. Due to important differences between preclinical settings and human biology, however, data from patients treated with a new targeted agent often show that one or both of these assumptions are false. This paper provides a Bayesian randomized group sequential enrichment design that compares an experimental treatment to a control based on survival time and uses early response as an ancillary outcome to assist with adaptive variable selection and enrichment. Initially, the design enrolls patients under broad eligibility criteria. At each interim decision, submodels for regression of response and survival time on a baseline covariate vector and treatment are fit; variable selection is used to identify a covariate subvector that characterizes treatment-sensitive patients and determines a personalized benefit index, and comparative superiority and futility decisions are made. Enrollment of each cohort is restricted to the most recent adaptively identified treatment-sensitive patients. Group sequential decision cutoffs are calibrated to control overall type I error and account for the adaptive enrollment restriction. The design provides a basis for precision medicine by identifying a treatment-sensitive subpopulation, if it exists, and determining whether the experimental treatment is superior to the control in that subpopulation. A simulation study shows that the proposed design reliably identifies a sensitive subpopulation, yields much higher generalized power compared to several existing enrichment designs and a conventional all-comers group sequential design, and is robust.},
  archive      = {J_BIOMTC},
  author       = {Yeonhee Park and Suyu Liu and Peter F. Thall and Ying Yuan},
  doi          = {10.1111/biom.13421},
  journal      = {Biometrics},
  number       = {1},
  pages        = {60-71},
  shortjournal = {Biometrics},
  title        = {Bayesian group sequential enrichment designs based on adaptive regression of response and survival time on baseline biomarkers},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonlinear mediation analysis with high-dimensional mediators
whose causal structure is unknown. <em>BIOMTC</em>, <em>78</em>(1),
46–59. (<a href="https://doi.org/10.1111/biom.13402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With multiple possible mediators on the causal pathway from a treatment to an outcome, we consider the problem of decomposing the effects along multiple possible causal path(s) through each distinct mediator. Under a path-specific effects framework, such fine-grained decompositions necessitate stringent assumptions, such as correctly specifying the causal structure among the mediators, and no unobserved confounding among the mediators. In contrast, interventional direct and indirect effects for multiple mediators can be identified under much weaker conditions, while providing scientifically relevant causal interpretations. Nonetheless, current estimation approaches require (correctly) specifying a model for the joint mediator distribution, which can be difficult when there is a high-dimensional set of possibly continuous and noncontinuous mediators. In this article, we avoid the need to model this distribution, by developing a definition of interventional effects previously suggested for longitudinal mediation. We propose a novel estimation strategy that uses nonparametric estimates of the (counterfactual) mediator distributions. Noncontinuous outcomes can be accommodated using nonlinear outcome models. Estimation proceeds via Monte Carlo integration. The procedure is illustrated using publicly available genomic data to assess the causal effect of a microRNA expression on the 3-month mortality of brain cancer patients that is potentially mediated by expression values of multiple genes.},
  archive      = {J_BIOMTC},
  author       = {Wen Wei Loh and Beatrijs Moerkerke and Tom Loeys and Stijn Vansteelandt},
  doi          = {10.1111/biom.13402},
  journal      = {Biometrics},
  number       = {1},
  pages        = {46-59},
  shortjournal = {Biometrics},
  title        = {Nonlinear mediation analysis with high-dimensional mediators whose causal structure is unknown},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identifying individual predictive factors for treatment
efficacy. <em>BIOMTC</em>, <em>78</em>(1), 35–45. (<a
href="https://doi.org/10.1111/biom.13398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the heterogeneous responses to therapy and the high cost of treatments, there is an increasing interest in identifying pretreatment predictors of therapeutic effect. Clearly, the success of such an endeavor will depend on the amount of information that the patient-specific variables convey about the individual causal treatment effect on the response of interest. In the present work, using causal inference and information theory, a strategy is proposed to evaluate individual predictive factors for cancer immunotherapy efficacy. In a first step, the methodology proposes a causal inference model to describe the joint distribution of the pretreatment predictors and the individual causal treatment effect. Further, in a second step, the so-called predictive causal information (PCI), a metric that quantifies the amount of information the pretreatment predictors convey on the individual causal treatment effects, is introduced and its properties are studied. The methodology is applied to identify predictors of therapeutic success for a therapeutic vaccine in advanced lung cancer. A user-friendly R library EffectTreat is provided to carry out the necessary calculations.},
  archive      = {J_BIOMTC},
  author       = {Ariel Alonso and Wim Van der Elst and Lizet Sanchez and Patricia Luaces and Geert Molenberghs},
  doi          = {10.1111/biom.13398},
  journal      = {Biometrics},
  number       = {1},
  pages        = {35-45},
  shortjournal = {Biometrics},
  title        = {Identifying individual predictive factors for treatment efficacy},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two robust tools for inference about causal effects with
invalid instruments. <em>BIOMTC</em>, <em>78</em>(1), 24–34. (<a
href="https://doi.org/10.1111/biom.13415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instrumental variables have been widely used to estimate the causal effect of a treatment on an outcome. Existing confidence intervals for causal effects based on instrumental variables assume that all of the putative instrumental variables are valid; a valid instrumental variable is a variable that affects the outcome only by affecting the treatment and is not related to unmeasured confounders. However, in practice, some of the putative instrumental variables are likely to be invalid. This paper presents two tools to conduct valid inference and tests in the presence of invalid instruments. First, we propose a simple and general approach to construct confidence intervals based on taking unions of well-known confidence intervals. Second, we propose a novel test for the null causal effect based on a collider bias. Our two proposals outperform traditional instrumental variable confidence intervals when invalid instruments are present and can also be used as a sensitivity analysis when there is concern that instrumental variables assumptions are violated. The new approach is applied to a Mendelian randomization study on the causal effect of low-density lipoprotein on globulin levels.},
  archive      = {J_BIOMTC},
  author       = {Hyunseung Kang and Youjin Lee and T. Tony Cai and Dylan S. Small},
  doi          = {10.1111/biom.13415},
  journal      = {Biometrics},
  number       = {1},
  pages        = {24-34},
  shortjournal = {Biometrics},
  title        = {Two robust tools for inference about causal effects with invalid instruments},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust methods to correct for measurement error when
evaluating a surrogate marker. <em>BIOMTC</em>, <em>78</em>(1), 9–23.
(<a href="https://doi.org/10.1111/biom.13386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of valid surrogate markers of disease or disease progression has the potential to decrease the length and costs of future studies. Most available methods that assess the value of a surrogate marker ignore the fact that surrogates are often measured with error. Failing to adjust for measurement error can erroneously identify a useful surrogate marker as not useful or vice versa. We investigate and propose robust methods to correct for the effect of measurement error when evaluating a surrogate marker using multiple estimators developed for parametric and nonparametric estimates of the proportion of treatment effect explained by the surrogate marker. In addition, we quantify the attenuation bias induced by measurement error and develop inference procedures to allow for variance and confidence interval estimation. Through a simulation study, we show that our proposed estimators correct for measurement error in the surrogate marker and that our inference procedures perform well in finite samples. We illustrate these methods by examining a potential surrogate marker that is measured with error, hemoglobin A1c, using data from the Diabetes Prevention Program clinical trial.},
  archive      = {J_BIOMTC},
  author       = {Layla Parast and Tanya P. Garcia and Ross L. Prentice and Raymond J. Carroll},
  doi          = {10.1111/biom.13386},
  journal      = {Biometrics},
  number       = {1},
  pages        = {9-23},
  shortjournal = {Biometrics},
  title        = {Robust methods to correct for measurement error when evaluating a surrogate marker},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Report of the editors—2021. <em>BIOMTC</em>, <em>78</em>(1),
5–8. (<a href="https://doi.org/10.1111/biom.13635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  doi          = {10.1111/biom.13635},
  journal      = {Biometrics},
  number       = {1},
  pages        = {5-8},
  shortjournal = {Biometrics},
  title        = {Report of the editors—2021},
  volume       = {78},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
