<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COMJNL_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="comjnl---228">COMJNL - 228</h2>
<ul>
<li><details>
<summary>
(2022). A modified firefly deep ensemble for microarray data
classification. <em>COMJNL</em>, <em>65</em>(12), 3265–3274. (<a
href="https://doi.org/10.1093/comjnl/bxac143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many researchers are using microarray technology to examine and investigate the levels of gene expression in a specific organism, which is an emerging trend in the field of genetic research. Microarray studies have a wide range of applications in the health sector, including disease prediction and diagnostics, as well as cancer research. Due to the existence of irrelevant or duplicated data in microarray datasets, it is difficult to correctly and immediately capture possible patterns using existing algorithms. Feature selection (FS) has evolved into a critical approach for identifying and eliminating the most pertinent qualities. The enormous dimensionality of microarray datasets, on the other hand, presents a significant barrier to the majority of available FS techniques. In this research, we propose a Modified Firefly Feature Selection (MFFS) algorithm that will reduce the irrelevant attributes needed for classification and a Deep Learning Model for classifying the microarray data. The experimental outcomes show that the proposed MFFS algorithm combined with a Hybrid Deep Learning Algorithm outperforms the existing methods in terms of feature set size, accuracy, precision, recall, F-measure and AUC for a dataset with larger number of features.},
  archive      = {J_COMJNL},
  author       = {S, Arul Antran Vijay and V, Jothi Prakash},
  doi          = {10.1093/comjnl/bxac143},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3265-3274},
  shortjournal = {Comput. J.},
  title        = {A modified firefly deep ensemble for microarray data classification},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intelligent forecast of stock markets to handle COVID-19
economic crisis by modified generative adversarial networks.
<em>COMJNL</em>, <em>65</em>(12), 3250–3264. (<a
href="https://doi.org/10.1093/comjnl/bxac056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock markets have voluminous data and are subjected to uncertainty. The coronavirus disease of 2019 (COVID-19) pandemic has hit the stock markets and the trends of stock markets have accelerated share prices of few companies and has also brought freefall to certain companies. This factor highlights the importance of technical analysis of the stock markets over fundamental analysis. So, the proposed robust model for financial forecasting is built based on the technical indicators and the fake price data generated over a period of time from the stock dataset by a novel architecture of modified generative adversarial network, which uses a dense recurrent neural network as the generator and a dense spectrally normalized convolutional neural network as the discriminator. The hyperparameters used in the network model follow the two-time-scale-update rule and they are tuned by using the Bayesian optimization technique. The feature importance of the technical indicators in predicting the performance by the stock market is enhanced by the XGBoost algorithm. The generative adversarial networks (GAN) used for forecasting in the previous works suffer from problems like mode collapse and non-convergence. So, the proposed work concentrates on building a GAN model, which is stable, robust and converges to Nash equilibrium. The generated GAN model is applied on stock data from the major 100 companies of the S&amp;P 500 stock for a period of 20 years. The modified GAN model predicts prices precise ~99 percentage, which maximizes the stock returns. The proposed modified GAN model outperforms the baseline GAN model and other state of the art approaches of forecasting on comparison.},
  archive      = {J_COMJNL},
  author       = {Sornavalli, G and Angelin, Gladston and Khanna, Nehemiah H},
  doi          = {10.1093/comjnl/bxac056},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3250-3264},
  shortjournal = {Comput. J.},
  title        = {Intelligent forecast of stock markets to handle COVID-19 economic crisis by modified generative adversarial networks},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modelling and predictive analytics of COVID-19 transmission
using gustafson–kessel fuzzy clustering approach. <em>COMJNL</em>,
<em>65</em>(12), 3240–3249. (<a
href="https://doi.org/10.1093/comjnl/bxac022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel Corona virus disease 2019 (COVID-19) outbreak is declared as the international public health emergency concern by the World Health Organization in the month of March 2020. This viral disease invented from China in the month of December 2019 has previously caused havoc around the world, including India. In this paper, efficient mathematical models using Gustafson–Kessel fuzzy clustering approach for the transmission of the COVID-19 are developed by considering the actual reported cases in the state of Tamil Nadu, India. The results proved a good concord between the reported data and the estimated data given by the proposed models. Moreover, the developed models are also capable to predict the requirements of beds in hospitals on the month of August 2020 in Tamil Nadu, India. Also, this work suggests strictly implementing/extending the complete lockdown for at least 21 days in the month of August 2020 and immediate separation of infected cases are the positive steps to reduce the spread of novel corona virus in Tamil Nadu state, India.},
  archive      = {J_COMJNL},
  author       = {Vijayakarthick, M and Sivaraman, E and Sathishbabu, S and Vinoth, N and Sivaraj, S N},
  doi          = {10.1093/comjnl/bxac022},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3240-3249},
  shortjournal = {Comput. J.},
  title        = {Modelling and predictive analytics of COVID-19 transmission using Gustafson–Kessel fuzzy clustering approach},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Provable data possession schemes from standard lattices for
cloud computing. <em>COMJNL</em>, <em>65</em>(12), 3223–3239. (<a
href="https://doi.org/10.1093/comjnl/bxab190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provable Data Possession (PDP) is of crucial importance in public cloud storage since it allows users to check the integrity of their outsourced data without downloading it. However, the existing PDP schemes, which are based on classical number-theoretic assumptions, are insecure under quantum attacks. In this paper, we propose the first PDP scheme from standard lattices, using a specific leveled fully homomorphic signature (FHS) scheme. To remove the complex key management of PDP cryptosystem on the public key infrastructure (PKI) setting, we employ a specific leveled identity-based (ID-based) FHS scheme to construct the first ID-based PDP scheme from standard lattices. Our two PDP schemes are secure under the standard small integer solution (SIS) assumption, which is conjectured to withstand quantum attacks. Furthermore, we conduct experimental evaluations to validate the feasibility of the proposed PDP schemes in practice.},
  archive      = {J_COMJNL},
  author       = {Luo, Fucai and Al-Kuwari, Saif and Lin, Changlu and Wang, Fuqun and Chen, Kefei},
  doi          = {10.1093/comjnl/bxab190},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3223-3239},
  shortjournal = {Comput. J.},
  title        = {Provable data possession schemes from standard lattices for cloud computing},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quality of service estimation enabled with trust-based
resource allocation in collaborative cloud using improved grey wolf
optimization. <em>COMJNL</em>, <em>65</em>(12), 3209–3222. (<a
href="https://doi.org/10.1093/comjnl/bxab140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative cloud computing utilizes information technology to successfully provide service over the network and serve the end users with tremendously stronger computational capability and enormous memory space at lower costs. Moreover, providing highly trustworthy service is the most fundamental task even on this platform. So far, only some contributions are there that meet the requirements of trust computing in this scenario. This proposal estimates the Quality of Service and trust by analyzing the system behavior using a new trust computing model. This is handled using Neural Network model. Further, a parallel resource matching framework is introduced using the concept of MapReduce concept, thereby the resource allocation is performed without any conflicts. Particularly, the resource allocation is performed precisely by optimization logic, where an Improved Grey Wolf Optimizer is introduced to do the same. In fact, the proposed algorithm is the enhanced version of traditional Grey Wolf Optimizer. Finally, the performance of the projected model is compared over other state-of-the-art models concerning different performance measures.},
  archive      = {J_COMJNL},
  author       = {Pol, Pooja Shashank and Pachghare, Vinod K},
  doi          = {10.1093/comjnl/bxab140},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3209-3222},
  shortjournal = {Comput. J.},
  title        = {Quality of service estimation enabled with trust-based resource allocation in collaborative cloud using improved grey wolf optimization},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The 3-extra connectivity of the data center network BCube.
<em>COMJNL</em>, <em>65</em>(12), 3199–3208. (<a
href="https://doi.org/10.1093/comjnl/bxab137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connectivity is a significant metric to assess the fault tolerance of a network. For a faulty vertex set ; , the ; -extra connectivity is defined under the assumption that every component of the network removing ; has at least ; fault-free vertices. Compared to the traditional connectivity, which is defined under the assumption that the network removing ; is disconnected or trivial, the ; -extra connectivity can better reflect the true fault tolerance of the network. The ; is an important server-centric data center network; it has good fault tolerance and scalability. In this paper, our research focuses on the logical structure of ; , named ; , which is actually a specific type of generalized hypercubes. We prove that the 3-extra connectivity of ; is ; for ; and ; .},
  archive      = {J_COMJNL},
  author       = {Yi, Yi and Fan, Jianxi and Cheng, Baolei and Wang, Yan and Yu, Jia},
  doi          = {10.1093/comjnl/bxab137},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3199-3208},
  shortjournal = {Comput. J.},
  title        = {The 3-extra connectivity of the data center network BCube},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Border collie cat optimization for intrusion detection
system in healthcare IoT network using deep recurrent neural network.
<em>COMJNL</em>, <em>65</em>(12), 3181–3198. (<a
href="https://doi.org/10.1093/comjnl/bxab136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attacks are the major problems in the Internet of Things (IoT) applications and communication networks. The undetected intruders affect the availability of the system for end-users, increase identity theft and data breaches. Hence, it is required to detect the attacks in the IoT systems to ensure effective defense and security. In this research, the Border Collie Cat Optimization-based Deep Recurrent Neural Network is proposed to detect intrusion in the IoT networks. Here, the proposed Border Collie Cat Optimization algorithm is derived by the integration of Border Collie Optimization and Cat Swarm Optimization. At first, the messages are authenticated at the authentication phase using the hashing and encryption function. After authenticating the device, the communication between the server and user is carried out at the communication phase to make the IoT device eligible for data transfer within the network. Then, the Deep Recurrent Neural Network classifier is employed to detect the intruders in the IoT network in such a way that the training process is carried out using the proposed Border Collie Optimization algorithm. The proposed approach obtained higher performance with the metrics, like detection rate, sensitivity, specificity and accuracy with the values of 0.9375, 0.9539, 0.8791 and 0.9263, respectively.},
  archive      = {J_COMJNL},
  author       = {Chandol, Mohan Kumar and Rao, M Kameswara},
  doi          = {10.1093/comjnl/bxab136},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3181-3198},
  shortjournal = {Comput. J.},
  title        = {Border collie cat optimization for intrusion detection system in healthcare IoT network using deep recurrent neural network},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ant cat swarm optimization-enabled deep recurrent neural
network for big data classification based on map reduce framework.
<em>COMJNL</em>, <em>65</em>(12), 3167–3180. (<a
href="https://doi.org/10.1093/comjnl/bxab135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The amount of data generated is increasing day by day due to the development in remote sensors, and thus it needs concern to increase the accuracy in the classification of the big data. Many classification methods are in practice; however, they limit due to many reasons like its nature for data loss, time complexity, efficiency and accuracy. This paper proposes an effective and optimal data classification approach using the proposed Ant Cat Swarm Optimization-enabled Deep Recurrent Neural Network (ACSO-enabled Deep RNN) by Map Reduce framework, which is the incorporation of Ant Lion Optimization approach and the Cat Swarm Optimization technique. To process feature selection and big data classification, Map Reduce framework is used. The feature selection is performed using Pearson correlation-based Black hole entropy fuzzy clustering. The classification in reducer part is performed using Deep RNN that is trained using a developed ACSO scheme. It classifies the big data based on the reduced dimension features to produce a satisfactory result. The proposed ACSO-based Deep RNN showed improved results with maximal specificity of 0.884, highest accuracy of 0.893, maximal sensitivity of 0.900 and the maximum threat score of 0.827 based on the Cleveland dataset.},
  archive      = {J_COMJNL},
  author       = {Narayana, Satyala and Chandanapalli, Suresh Babu and Rao, Mekala Srinivasa and Srinivas, Kalyanapu},
  doi          = {10.1093/comjnl/bxab135},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3167-3180},
  shortjournal = {Comput. J.},
  title        = {Ant cat swarm optimization-enabled deep recurrent neural network for big data classification based on map reduce framework},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The star-structure connectivity and star-substructure
connectivity of hypercubes and folded hypercubes. <em>COMJNL</em>,
<em>65</em>(12), 3156–3166. (<a
href="https://doi.org/10.1093/comjnl/bxab133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a generalization of vertex connectivity, for connected graphs ; and ; , the ; -structure connectivity ; (resp. ; -substructure connectivity ; ) of ; is the minimum cardinality of a set of subgraphs ; of ; that each is isomorphic to ; (resp. to a connected subgraph of ; ) so that ; is disconnected. For ; -dimensional hypercube ; , Lin ; showed ; and ; for ; and ; (Lin, C.-K., Zhang, L.-L., Fan, J.-X. and Wang, D.-J. (2016) Structure connectivity and substructure connectivity of hypercubes. ; , 634, 97–107). Sabir ; obtained that ; for ; and for ; -dimensional folded hypercube ; , ; , ; with ; and ; (Sabir, E. and Meng, J.(2018) Structure fault tolerance of hypercubes and folded hypercubes. ; , 711, 44–55). They proposed an open problem of determining ; -structure connectivity of ; and ; for general ; . In this paper, we obtain that for each integer ; , ;  ;  ; and ; for all integers ; larger than ; in quare scale. For ; , we separately confirm the above result holds for ; in the remaining cases.},
  archive      = {J_COMJNL},
  author       = {Ba, Lina and Zhang, Heping},
  doi          = {10.1093/comjnl/bxab133},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3156-3166},
  shortjournal = {Comput. J.},
  title        = {The star-structure connectivity and star-substructure connectivity of hypercubes and folded hypercubes},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Benchmark for discriminating power of edge centrality
metrics. <em>COMJNL</em>, <em>65</em>(12), 3141–3155. (<a
href="https://doi.org/10.1093/comjnl/bxab132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge centrality has found wide applications in various aspects. Many edge centrality metrics have been proposed, but the crucial issue that how good the discriminating power of a metric is, with respect to other measures, is still open. In this paper, we address the question about the benchmark of the discriminating power of edge centrality metrics. We first use the automorphism concept to define equivalent edges, based on which we introduce a benchmark for the discriminating power of edge centrality measures and develop a fast approach to compare the discriminating power of different measures. According to the benchmark, for a desirable measure, equivalent edges have identical metric scores, while inequivalent edges possess different scores. However, we show that even in a toy graph, inequivalent edges cannot be discriminated by three existing edge centrality metrics. We then present a novel edge centrality metric called forest centrality (FC). Extensive experiments on real-world networks and model networks indicate that FC has better discriminating power than three existing edge centrality metrics.},
  archive      = {J_COMJNL},
  author       = {Bao, Qi and Xu, Wanyue and Zhang, Zhongzhi},
  doi          = {10.1093/comjnl/bxab132},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3141-3155},
  shortjournal = {Comput. J.},
  title        = {Benchmark for discriminating power of edge centrality metrics},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid machine learning approach for performance modeling
of cloud-based big data applications. <em>COMJNL</em>, <em>65</em>(12),
3123–3140. (<a href="https://doi.org/10.1093/comjnl/bxab131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, Apache Hadoop and Apache Spark are two of the most prominent distributed solutions for processing big data applications on the market. Since in many cases these frameworks are adopted to support business critical activities, it is often important to predict with fair confidence the execution time of submitted applications, for instance when service-level agreements are established with end-users. In this work, we propose and validate a hybrid approach for the performance prediction of big data applications running on clouds, which exploits both analytical modeling and machine learning (ML) techniques and it is able to achieve a good accuracy without too many time consuming and costly experiments on a real setup. The experimental results show how the proposed approach attains improvement in accuracy, number of experiments to be run on the operational system and cost over applying ML techniques without any support from analytical models. Moreover, we compare our approach with ; , an ML-based technique proposed in the literature by the Spark inventors. Experiments show that Ernest can accurately estimate the performance in interpolating scenarios while it fails to predict the performance when configurations with increasing number of cores are considered. Finally, a comparison with a similar hybrid approach proposed in the literature demonstrates how our approach significantly reduce prediction errors especially when few experiments on the real system are performed.},
  archive      = {J_COMJNL},
  author       = {Ataie, Ehsan and Evangelinou, Athanasia and Gianniti, Eugenio and Ardagna, Danilo},
  doi          = {10.1093/comjnl/bxab131},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3123-3140},
  shortjournal = {Comput. J.},
  title        = {A hybrid machine learning approach for performance modeling of cloud-based big data applications},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-hop relay deployment based on user trajectory in
wireless networks. <em>COMJNL</em>, <em>65</em>(12), 3106–3122. (<a
href="https://doi.org/10.1093/comjnl/bxab130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional relay deployment problem typically assumes that the locations of users are known and stationary, which is not realistic in practice. The prevalence of mobile devices has made it possible to collect user trajectory and account for user movement while deploying relays. Under this background, a novel problem ; (TBRD) is put forward. This problem considers communication-related metrics and is aimed at maximizing user connection time as users roam through the target area under relay resource constraints, which is more reasonable than the goal of expanding the relay coverage. To figure out the TBRD, we first propose the concept ; (DNs), which are virtual weighted nodes representing the locations where users frequently pass or stay for a long period. Next, we design a ; algorithm that can transform the continuous historical user trajectory into a number of discrete DNs. By generating DNs, we convert the TBRD problem into a demand node coverage (DNC) problem, which is proved to be NP-complete. Followed by that, we introduce an approximation algorithm, named ; , which solves the DNC problem with the approximation factor ; , where ; is the mathematical constant, and ; is the relay number constraint. Finally, five real trajectory datasets are used to evaluate our proposed algorithm, and the simulation results demonstrate that our algorithm can obtain high coverage for users in motion, which can lead to better user experience. In addition, we also analyze the impact of different parameters on the coverage performance, and under this circumstance, we may safely come to the conclusion that our work is at the leading edge to utilize user trajectories for relay deployment in wireless networks.},
  archive      = {J_COMJNL},
  author       = {Li, Zhiyao and Ouyang, Siru and Gao, Xiaofeng and Chen, Guihai},
  doi          = {10.1093/comjnl/bxab130},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3106-3122},
  shortjournal = {Comput. J.},
  title        = {Two-hop relay deployment based on user trajectory in wireless networks},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximating closest vector problem in ℓ∞-norm revisited.
<em>COMJNL</em>, <em>65</em>(12), 3100–3105. (<a
href="https://doi.org/10.1093/comjnl/bxab129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The security of most lattice-based cryptography schemes are based on two computational hard problems which are the short integer solution (SIS) and learning with errors (LWE) problems. The computational complexity of SIS and LWE problems are related to approximating shortest vector problem and bounded distance decoding (BDD) problem. Approximating BDD is a special case of approximating closest vector problem (CVP). In this paper, we revisit the study for approximating CVP. We give a proof that approximating the CVP over ; -norm (CVP; ) within any constant factor is NP-hard. The result is obtained by the gap-preserving reduction from Min Total Label Cover problem in ; -norm to to CVP; . This proof is simpler than known proofs [ ; ].},
  archive      = {J_COMJNL},
  author       = {Chen, Wenbin and Chen, Jianer},
  doi          = {10.1093/comjnl/bxab129},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3100-3105},
  shortjournal = {Comput. J.},
  title        = {Approximating closest vector problem in ℓ∞-norm revisited},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the security of the stream ciphers RCR-64 and RCR-32.
<em>COMJNL</em>, <em>65</em>(12), 3091–3099. (<a
href="https://doi.org/10.1093/comjnl/bxab128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stream ciphers RCR-64 and RCR-32 designed by Sekar ; are the most recent additions to the Py-family of stream ciphers, originally designed by Biham ; The ciphers are among the fastest stream ciphers on software. To the best of our knowledge, the only reported attacks on the ciphers are due to Ding ; , published in the ; . In this paper, we review these alleged attacks on the RCR ciphers and show that they are based on non-existent keystream biases stemming from flawed probability calculations.},
  archive      = {J_COMJNL},
  author       = {Joseph, Mabin and Sekar, Gautham and Balasubramanian, R and Venkiteswaran, G},
  doi          = {10.1093/comjnl/bxab128},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3091-3099},
  shortjournal = {Comput. J.},
  title        = {On the security of the stream ciphers RCR-64 and RCR-32},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). White matter, gray matter and cerebrospinal fluid
segmentation from brain magnetic resonance imaging using adaptive u-net
and local convolutional neural network. <em>COMJNL</em>,
<em>65</em>(12), 3081–3090. (<a
href="https://doi.org/10.1093/comjnl/bxab127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to the World Alzheimer Report 2015, 46 million people are living with dementia in the world. The diagnosis of diseases helps doctors treating patients better. One of the signs of diseases is related to white matter, grey matter and cerebrospinal fluid. Therefore, the automatic segmentation of three tissues in brain imaging especially from magnetic resonance imaging (MRI) plays an important role in medical analysis. In this research, we proposed an effective approach to segment automatically these tissues in three-dimensional (3D) brain MRI. First, a deep learning model is used to segment the sure and unsure regions. In the unsure region, another deep learning model is used to classify each pixel. In the experiments, an adaptive U-net model is used to segment the sure and unsure regions, and the Local Convolutional Neural Network (CNN) model with multiple inputs is used to classify each pixel only in the unsure region. Our method was evaluated with a real image database, Internet Brain Segmentation Repository database, with 18 persons (IBSR 18) (; ) and compared with state of art methods being the results very promising.},
  archive      = {J_COMJNL},
  author       = {Bao, Pham The and Tuan, Tran Anh and Thuy, Le Nhi Lam and Kim, Jin Young and Tavares, João Manuel R S},
  doi          = {10.1093/comjnl/bxab127},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3081-3090},
  shortjournal = {Comput. J.},
  title        = {White matter, gray matter and cerebrospinal fluid segmentation from brain magnetic resonance imaging using adaptive U-net and local convolutional neural network},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the probability and automatic search of rotational-XOR
cryptanalysis on ARX ciphers. <em>COMJNL</em>, <em>65</em>(12),
3062–3080. (<a href="https://doi.org/10.1093/comjnl/bxab126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rotational-XOR cryptanalysis is a very recent technique for ARX ciphers. In this paper, the probability propagation formula of RX-cryptanalysis in modular addition is extended, and the calculation of RX-difference probability for any rotation parameter (; ) can be realized. By proposing a concept of ; and constructing the corresponding distribution table, the propagation of RX-difference in modular addition can be derived from the propagation of XOR-difference. Combined with the improvement of the automatic search tool for XOR-differential characteristics of ARX ciphers, we only need to add one more operation in each round, i.e. traverse the possible value of ; and XOR it with the output XOR-difference of modular addition, thus it can achieve the search for RX-differential characteristics. With this method, the RX-differential distinguisher of ARX-C primitives without or with linear key schedule can be searched. For the applications, we have obtained the third-party RX-cryptanalysis results for Alzette and CHAM for the first time as far as we know.},
  archive      = {J_COMJNL},
  author       = {Huang, Mingjiang and Xu, Zhen and Wang, Liming},
  doi          = {10.1093/comjnl/bxab126},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3062-3080},
  shortjournal = {Comput. J.},
  title        = {On the probability and automatic search of rotational-XOR cryptanalysis on ARX ciphers},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On random-oracle-free top-level secure certificateless
signature schemes. <em>COMJNL</em>, <em>65</em>(12), 3049–3061. (<a
href="https://doi.org/10.1093/comjnl/bxab125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Certificateless public key cryptography (CL-PKC) overcomes the difficulties of the certificate managements in traditional public key infrastructure (PKI) and the key escrow problem in ID-Based public key cryptography (ID-PKC), concurrently. In 2018, Tseng et al. proposed a certificateless signature (CLS) scheme and claimed that their proposal is the first scheme which satisfies the security against the level-3 KGC (according to Girault’s three categorizations of the honesty level of a trusted third party (TTP) which is proposed in 1991), in the standard model. However, we will show that unfortunately their scheme is even vulnerable against a malicious KGC. Afterwards, we will improve their scheme to be robust against the proposed attack. Finally, we will propose a CLS scheme secure against the level-3 KGC in the standard model, based on Yuan and Wang’s CLS scheme. We will show that our proposal not only satisfies the level-3 security as well as the basic security requirements of a CLS scheme in the standard model, but also is more efficient than the previous works in the sense of computation and communication costs.},
  archive      = {J_COMJNL},
  author       = {Rastegari, Parvin and Susilo, Willy},
  doi          = {10.1093/comjnl/bxab125},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3049-3061},
  shortjournal = {Comput. J.},
  title        = {On random-oracle-free top-level secure certificateless signature schemes},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A method for building the quantitative and qualitative part
of bayesian networks for intelligent tutoring systems. <em>COMJNL</em>,
<em>65</em>(12), 3035–3048. (<a
href="https://doi.org/10.1093/comjnl/bxab124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bayesian network (BN) is an important technique to represent and infer knowledge in an Intelligent Tutoring System (ITS); however, ITSs are complex to build. Diverse authors have built BNs based on ontologies to accelerate the building process; nonetheless, they did not fully automate the process, and did not follow the ontologies standard Web Ontology Language, or simplified the final domain. This work proposes a method to build BNs based on ontologies and Wikipedia information to be employed on ITSs. The proposed method constructs the qualitative part of the BN through classes and relations of ontologies; the quantitative part is created based on frequencies, hops, and a measure of similarity between concepts of the ontology represented by Wikipedia articles. This study carried out an experiment to determine the correlation of our method against domain experts opinions; the method obtained a positive correlation of ; according to the Spearman test. The method constructs a BN to represent the knowledge in ITSs, in a similar way as experts would, supporting the automatic build of these systems.},
  archive      = {J_COMJNL},
  author       = {Ramírez-Noriega, Alan and Juárez-Ramírez, Reyes and Leyva-López, Juan Carlos and Jiménez, Samantha and Figueroa-Pérez, J Francisco},
  doi          = {10.1093/comjnl/bxab124},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3035-3048},
  shortjournal = {Comput. J.},
  title        = {A method for building the quantitative and qualitative part of bayesian networks for intelligent tutoring systems},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A self-tallying electronic voting based on blockchain.
<em>COMJNL</em>, <em>65</em>(12), 3020–3034. (<a
href="https://doi.org/10.1093/comjnl/bxab123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic voting (e-voting) has been studied for many years. Recently, researchers find that blockchain can provide an alternative secure platform for e-voting systems, because of its properties of tamper resistance and transparency. However, existing blockchain-based schemes either require central authorities to tally ballots or can only handle a limited number of voters. This paper tries to propose a self-tallying e-voting system, i.e. the public can verify the validity of all ballots and tally the ballots without a centralized authority. To achieve this goal, we solve two challenges, namely how to cancel out all random numbers used for ballots and to prove the validity of ballots using a non-interactive zero knowledge proof. Our scheme is proved to be secure and shown to be practical by experiments.},
  archive      = {J_COMJNL},
  author       = {Zeng, Gongxian and He, Meiqi and Yiu, Siu Ming and Huang, Zhengan},
  doi          = {10.1093/comjnl/bxab123},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3020-3034},
  shortjournal = {Comput. J.},
  title        = {A self-tallying electronic voting based on blockchain},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Specifying and model checking distributed control algorithms
at meta-level. <em>COMJNL</em>, <em>65</em>(12), 2998–3019. (<a
href="https://doi.org/10.1093/comjnl/bxab122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an approach to the specification and model checking of a large, important class of distributed algorithms called control algorithms (CAs), which are superimposed on underlying distributed systems (UDSs). The approach is based on rewriting logic by moving from its object level to the meta-level. We introduce the idea of specifying CAs as meta-programs that take the specifications of UDSs and automatically generate the specifications of the UDSs on which the CAs are superimposed (UDS-CAs). Due to many options, such as network topologies, even fixing the number of each kind of entities, such as mobile support stations (MSSs) and mobile hosts (MHs) in a mobile checkpointing algorithm, there are many instances of a UDS. To address the problem, we generate all possible initial states of a UDS for a fixed number of each kind of entities such that some constraints, such as MSSs strongly connected with a wired network, are fulfilled and conduct model checking for each of the initial states. We demonstrate the usefulness by reporting on a case study where a counterexample is found for some specific initial states but not for the other initial states, detecting a subtle flaw lurking in a mobile checkpointing algorithm.},
  archive      = {J_COMJNL},
  author       = {Doan, Ha Thi Thu and Ogata, Kazuhiro},
  doi          = {10.1093/comjnl/bxab122},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {2998-3019},
  shortjournal = {Comput. J.},
  title        = {Specifying and model checking distributed control algorithms at meta-level},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the modulus in matching vector codes. <em>COMJNL</em>,
<em>65</em>(12), 2991–2997. (<a
href="https://doi.org/10.1093/comjnl/bxab121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A ; -query locally decodable code (LDC) ; allows one to encode any ; -symbol message ; as a codeword ; of ; symbols such that each symbol of ; can be recovered by looking at ; symbols of ; , even if a constant fraction of ; has been corrupted. Currently, the best known LDCs are matching vector codes (MVCs). A modulus ; may result in an MVC with ; and ; . The ; is ; if it is possible to have ; . The good numbers yield more efficient MVCs. Prior to this work, there are only ; good numbers. All of them were obtained via computer search and have the form ; . In this paper, we study good numbers of the form ; . We show that if ; is good, then any multiple of ; of the form ; must be good as well. Given a good number ; , we show an explicit method of obtaining smaller good numbers that have the same prime divisors. Our approach yields ; new good numbers.},
  archive      = {J_COMJNL},
  author       = {Zhu, Lin and Li, Wen Ming and Zhang, Liang Feng},
  doi          = {10.1093/comjnl/bxab121},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {2991-2997},
  shortjournal = {Comput. J.},
  title        = {On the modulus in matching vector codes},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An algorithm to construct completely independent spanning
trees in line graphs. <em>COMJNL</em>, <em>65</em>(12), 2979–2990. (<a
href="https://doi.org/10.1093/comjnl/bxab120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past few years, much importance and attention have been attached to completely independent spanning trees (CISTs). Many results, such as edge-disjoint Hamilton cycles, traceability, number of spanning trees, structural properties, topological indices, etc., have been obtained on line graphs, and researchers have applied the line graphs of some interconnection networks such as generalized hypercubes, augmented cubes, crossed cubes, etc., into data center networks, such as SWCube, AQLCube, BCDC, etc. At the meanwhile, few results of CISTs are reported on the line graphs. In this paper, we establish the relation of edge-disjoint spanning trees in an interconnection network ; ’ with its line graph ; by proposing a general algorithm for the first time. By this method, more CISTs can be obtained comparing with results in the literature. Then, the decrease of diameter is discussed and simulation experiments are shown on the line graphs of hypercubes.},
  archive      = {J_COMJNL},
  author       = {Wang, Yifeng and Cheng, Baolei and Fan, Jianxi and Qian, Yu and Jiang, Ruofan},
  doi          = {10.1093/comjnl/bxab120},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {2979-2990},
  shortjournal = {Comput. J.},
  title        = {An algorithm to construct completely independent spanning trees in line graphs},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluating membership inference through adversarial
robustness. <em>COMJNL</em>, <em>65</em>(11), 2969–2978. (<a
href="https://doi.org/10.1093/comjnl/bxac080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The usage of deep learning is being escalated in many applications. Due to its outstanding performance, it is being used in a variety of security and privacy-sensitive areas in addition to conventional applications. One of the key aspects of deep learning efficacy is to have abundant data. This trait leads to the usage of data which can be highly sensitive and private, which in turn causes wariness with regard to deep learning in the general public. Membership inference attacks are considered lethal as they can be used to figure out whether a piece of data belongs to the training dataset or not. This can be problematic with regard to leakage of training data information and its characteristics. To highlight the significance of these types of attacks, we propose an enhanced methodology for membership inference attacks based on adversarial robustness, by adjusting the directions of adversarial perturbations through label smoothing under a white-box setting. We evaluate our proposed method on three datasets: Fashion-MNIST, CIFAR-10 and CIFAR-100. Our experimental results reveal that the performance of our method surpasses that of the existing adversarial robustness-based method when attacking normally trained models. Additionally, through comparing our technique with the state-of-the-art metric-based membership inference methods, our proposed method also shows better performance when attacking adversarially trained models. The code for reproducing the results of this work is available at ; .},
  archive      = {J_COMJNL},
  author       = {Zhang, Zhaoxi and Yu Zhang, Leo and Zheng, Xufei and Hussain Abbasi, Bilal and Hu, Shengshan},
  doi          = {10.1093/comjnl/bxac080},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2969-2978},
  shortjournal = {Comput. J.},
  title        = {Evaluating membership inference through adversarial robustness},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). A semantic embedding enhanced topic model for
user-generated textual content modeling in social ecosystems.
<em>COMJNL</em>, <em>65</em>(11), 2953–2968. (<a
href="https://doi.org/10.1093/comjnl/bxac091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of Information and Communication Technologies (ICT) and Web 2.0 promotes the emergence of diverse social ecosystems like social Internet of Things (IoT), social media and online communities. User-generated textual content (UGTC), which consists of unstructured texts, is the most important and common type of user-generated content in social ecosystems. UGTC in social ecosystems is generated according to two types of context information—global context (topics) and local context (semantic regularities). For UGTC modeling, topic models just consider global context but ignore semantic regularities, while semantic embedding models are on the opposite. So only utilizing topic models or semantic embedding models to model UGTC suffers from some drawbacks. For this problem, we propose a semantic embedding enhanced topic model named SEE-Twitter-LDA for accurately modeling UGTC in social ecosystems. The core of SEE-Twitter-LDA is that words are generated according to mutual semantic information of topics and semantic regularities. So global context and local context are jointly considered for UGTC modeling. By utilizing 553 098 tweets sampled from Twitter and 211 233 posts sampled from Weibo, we validate SEE-Twitter-LDA’s better performance on perplexity, topic divergence and topic coherence versus existing related models.},
  archive      = {J_COMJNL},
  author       = {Zhang, Peng and Liu, Baoxi and Lu, Tun and Gu, Hansu and Ding, Xianghua and Gu, Ning},
  doi          = {10.1093/comjnl/bxac091},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2953-2968},
  shortjournal = {Comput. J.},
  title        = {A semantic embedding enhanced topic model for user-generated textual content modeling in social ecosystems},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ASCUE: An adversarial network-based semantical conformance
checking method for unsupervised event extraction in social internet of
things. <em>COMJNL</em>, <em>65</em>(11), 2939–2952. (<a
href="https://doi.org/10.1093/comjnl/bxac108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event extraction (EE) methods are widely used in the Social Internet of Things (SIoT) to help objects obtain the key information from messages shared by other objects. Existing supervised EE methods can only extract predefined events and can hardly extract events with unseen event types. To address this issue, we propose an unsupervised EE method based on the idea of semantic role labeling. However, after extracting all possible events from a given message text, some of these events face the semantically inconsistent issue that will destroy the information credibility in SIoT. To solve this issue, we present ; , an adversarial network-based semantic conformance checking method for unsupervised EE in SIoT. Briefly, ; first mines all the event candidates from a given text unsupervisedly. Next, ; introduces two independent Bidirectional Encoder Representations from Transformers models to capture the semantics of the event candidate and text, respectively. Moreover, to enable the model to achieve competitive performance as label embedding for semantic conformance checking, an adversarial network is added into ; ’s training stage. Finally, ; extracts all the events that are semantically consistent with the original text by classifying them into two categories: semantically consistent and semantically inconsistent. In addition, since the existing datasets do not label whether the events are semantically consistent with the original text, we re-annotate an existing dataset to fit our task. Experimental results on the re-annotated dataset show that our model outperforms the state-of-the-art baselines in terms of accuracy and F1 scores.},
  archive      = {J_COMJNL},
  author       = {Wu, Jiawei and Cheng, Huangfei and Cao, Bin and Wang, Jiaxing and Fan, Jing},
  doi          = {10.1093/comjnl/bxac108},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2939-2952},
  shortjournal = {Comput. J.},
  title        = {ASCUE: An adversarial network-based semantical conformance checking method for unsupervised event extraction in social internet of things},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Practical blockchain-based steganographic communication via
adversarial AI: A case study in bitcoin. <em>COMJNL</em>,
<em>65</em>(11), 2926–2938. (<a
href="https://doi.org/10.1093/comjnl/bxac090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of 5G, the wireless Internet of Things (IoT) has become possible; how to provide privacy protections for the communication of IoT devices in a more vulnerable wireless transmission environment is a huge challenge. Thus, steganography is introduced as a safe and effective technology. Blockchain systems have been widely used in the area of steganography. Several works attempted to embed covert data into transactions in public blockchain systems such as Bitcoin, Ethereum and Monero. However, most of them merely focus on putting covert data into certain fields in transactions based on cryptographic algorithms. In this paper, a Covert Transaction Recognition (CTR) model is proposed by the Text Convolutional Neural Networks and Back Propagation Neural Networks. When utilizing the covert data-embedded field for recognizing, our CTR model can attain 0.79 precision and 0.83 recall on average for seven covert transaction construction schemes. The precision and recall can increase by at most 43 and 47\%, respectively, if other unembedded fields were additionally exploited for recognition. We further propose a Practical Covert Transaction Construction (PCTC) model. This model fixes the contents in the embedded fields of the constructed transactions, and generates the contents in other fields using Generative Adversarial Networks. Experimental results demonstrated that the precision and recall are greatly decreased when identifying the covert transactions generated by our PCTC model. The data underlying this article are available in ‘covert-transaction-model’, at ; .},
  archive      = {J_COMJNL},
  author       = {Wang, Minxian and Zhang, Zijian and He, Jialing and Gao, Feng and Li, Meng and Xu, Shubin and Zhu, Liehuang},
  doi          = {10.1093/comjnl/bxac090},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2926-2938},
  shortjournal = {Comput. J.},
  title        = {Practical blockchain-based steganographic communication via adversarial AI: A case study in bitcoin},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effectively detecting operational anomalies in large-scale
IoT data infrastructures by using a GAN-based predictive model.
<em>COMJNL</em>, <em>65</em>(11), 2909–2925. (<a
href="https://doi.org/10.1093/comjnl/bxac085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality of data services is crucial for operational large-scale internet-of-things (IoT) research data infrastructure, in particular when serving large amounts of distributed users. Effectively detecting runtime anomalies and diagnosing their root cause helps to defend against adversarial attacks, thereby essentially boosting system security and robustness of the IoT infrastructure services. However, conventional anomaly detection methods are inadequate when facing the dynamic complexities of these systems. In contrast, supervised machine learning methods are unable to exploit large amounts of data due to the unavailability of labeled data. This paper leverages popular GAN-based generative models and end-to-end one-class classification to improve unsupervised anomaly detection. A novel heterogeneous BiGAN-based anomaly detection model Heterogeneous Temporal Anomaly-reconstruction GAN (HTA-GAN) is proposed to make better use of a one-class classifier and a novel anomaly scoring function. The Generator-Encoder-Discriminator BiGAN structure can lead to practical anomaly score computation and temporal feature capturing. We empirically compare the proposed approach with several state-of-the-art anomaly detection methods on real-world datasets, anomaly benchmarks and synthetic datasets. The results show that HTA-GAN outperforms its competitors and demonstrates better robustness.},
  archive      = {J_COMJNL},
  author       = {Chen, Peng and Liu, Hongyun and Xin, Ruyue and Carval, Thierry and Zhao, Jiale and Xia, Yunni and Zhao, Zhiming},
  doi          = {10.1093/comjnl/bxac085},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2909-2925},
  shortjournal = {Comput. J.},
  title        = {Effectively detecting operational anomalies in large-scale IoT data infrastructures by using a GAN-based predictive model},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Efficient robustness verification of the deep neural
networks for smart IoT devices. <em>COMJNL</em>, <em>65</em>(11),
2894–2908. (<a href="https://doi.org/10.1093/comjnl/bxac094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Internet of Things, smart devices are expected to correctly capture and process data from environments, regardless of perturbation and adversarial attacks. Therefore, it is important to guarantee the robustness of their intelligent components, e.g. neural networks, to protect the system from environment perturbation and adversarial attacks. In this paper, we propose a formal verification technique for rigorously proving the robustness of neural networks. Our approach leverages a tight liner approximation technique and constraint substitution, by which we transform the robustness verification problem into an efficiently solvable linear programming problem. Unlike existing approaches, our approach can automatically generate adversarial examples when a neural network fails to verify. Besides, it is general and applicable to more complex neural network architectures such as CNN, LeNet and ResNet. We implement the approach in a prototype tool called ; and evaluate it on extensive benchmarks, including Fashion MNIST, CIFAR10 and GTSRB. Experimental results show that ; can verify neural networks that contain over 10 000 neurons on one input image in a minute with a 6.28\% probability of false positive on average.},
  archive      = {J_COMJNL},
  author       = {Zhang, Zhaodi and Liu, Jing and Zhang, Min and Sun, Haiying},
  doi          = {10.1093/comjnl/bxac094},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2894-2908},
  shortjournal = {Comput. J.},
  title        = {Efficient robustness verification of the deep neural networks for smart IoT devices},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robust blockchain-based distribution master for
distributing root zone data in DNS. <em>COMJNL</em>, <em>65</em>(11),
2880–2893. (<a href="https://doi.org/10.1093/comjnl/bxac131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain Name System (DNS) is a key infrastructure on the Internet. The Distribution Master (DM) system is used to transmit root zone data from Internet Assigned Numbers Authority (IANA) to the root server. DM is a centralized system that will introduce single points of failure and abuse of authority. To solve the problem of a single point of failure, we adopt the decentralization of blockchain in architecture, and propose a blockchain-based distributed DM architecture, which allows nodes to join and exit at any time. At the technical level, the 3R-PBFT (Replicated, Redundant Practical Byzantine Fault Tolerance) algorithm is proposed to reach a consensus, which increases the security of the system. We use flexible mechanisms to reduce the number of signatures and improve the performance of the system. A threshold signature algorithm is used to ensure the uniqueness of the signature key information, and the increase of DM nodes will not bring about an increase in the number of keys. The advantages of the system structure in stability and scalability were verified by experiments.},
  archive      = {J_COMJNL},
  author       = {Liu, Yan and Yu, Haisheng and Wang, Wenyong and Zou, Sai and Liu, Dong and Gong, Daobiao and Li, Zhen},
  doi          = {10.1093/comjnl/bxac131},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2880-2893},
  shortjournal = {Comput. J.},
  title        = {A robust blockchain-based distribution master for distributing root zone data in DNS},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient deep learning approach to IoT intrusion
detection. <em>COMJNL</em>, <em>65</em>(11), 2870–2879. (<a
href="https://doi.org/10.1093/comjnl/bxac119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the Internet of Things (IoT), network security challenges are becoming more and more complex, and the scale of intrusion attacks against the network is gradually increasing. Therefore, researchers have proposed Intrusion Detection Systems and constantly designed more effective systems to defend against attacks. One issue to consider is using limited computing power to process complex network data efficiently. In this paper, we take the AWID dataset as an example, propose an efficient data processing method to mitigate the interference caused by redundant data and design a lightweight deep learning-based model to analyze and predict the data category. Finally, we achieve an overall accuracy of 99.77\% and an accuracy of 97.95\% for attacks on the AWID dataset, with a detection rate of 99.98\% for the injection attack. Our model has low computational overhead and a fast response time after training, ensuring the feasibility of applying to edge nodes with weak computational power in the IoT.},
  archive      = {J_COMJNL},
  author       = {Cao, Jin and Lin, Liwei and Ma, Ruhui and Guan, Haibing and Tian, Mengke and Wang, Yong},
  doi          = {10.1093/comjnl/bxac119},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2870-2879},
  shortjournal = {Comput. J.},
  title        = {An efficient deep learning approach to IoT intrusion detection},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Privacy-enhanced federated generative adversarial networks
for internet of things. <em>COMJNL</em>, <em>65</em>(11), 2860–2869. (<a
href="https://doi.org/10.1093/comjnl/bxac060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated generative adversarial networks are designed to collaborate across the communication and privacy-constrained edge servers participating in training. However, in the Internet of Things scenario, local updates uploaded by edge servers can lead to the risk of privacy breaches. Gradient-sanitized-based approaches can transmit sanitized sensitive data with strict privacy guarantees, but gradient clipping and perturbation severely degrade convergence performance. In this paper, our proposed algorithm enhances the privacy of terminated raw data through differential privacy before it is transmitted to the edge server. The edge server trains the local generator and discriminator using the perturbed data, which provides privacy guarantees for the gradient attack on the FedGAN without compromising the gradient accuracy. The results of the experimental evaluation show that the algorithm generates images with slightly better quality than that generated by the gradient-sanitized-based approaches while maintaining privacy.},
  archive      = {J_COMJNL},
  author       = {Zeng, Qingkui and Zhou, Liwen and Lian, Zhuotao and Huang, Huakun and Kim, Jung Yoon},
  doi          = {10.1093/comjnl/bxac060},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2860-2869},
  shortjournal = {Comput. J.},
  title        = {Privacy-enhanced federated generative adversarial networks for internet of things},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AWFC: Preventing label flipping attacks towards federated
learning for intelligent IoT. <em>COMJNL</em>, <em>65</em>(11),
2849–2859. (<a href="https://doi.org/10.1093/comjnl/bxac124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Centralized machine learning methods require the aggregation of data collected from clients. Due to the awareness of data privacy, however, the aggregation of raw data collected by Internet of Things (IoT) devices is not feasible in many scenarios. Federated learning (FL), a kind of distributed learning framework, can be running on multiple IoT devices. It aims to resolve the issues of privacy leakage by training a model locally on the client-side, other than on the server-side that aggregates all the raw data. However, there are still threats of poisoning attacks in FL. Label flipping attacks, typical data poisoning attacks in FL, aim to poison the global model by sending model updates trained by the data with mismatched labels. The central parameter aggregation server is hard to detect the label flipping attacks due to its inaccessibility to the client in a typical FL system. In this work, we are motivated to prevent label flipping poisoning attacks by observing the changes in model parameters that were trained by different single labels. We propose a novel detection method called average weight of each class in its associated fully connected layer. In this method, we detect label flipping attacks by identifying the differences of classes in the data based on the weight assignments in a fully connected layer of the neural network model and use the statistical algorithm to recognize the malicious clients. We conduct extensive experiments on benchmark data like Fashion-MNIST and Intrusion Detection Evaluation Dataset (CIC-IDS2017). Comprehensive experimental results demonstrated that our method has the detection accuracy over 90\% for the identification of the attackers flipping labels.},
  archive      = {J_COMJNL},
  author       = {Lv, Zhuo and Cao, Hongbo and Zhang, Feng and Ren, Yuange and Wang, Bin and Chen, Cen and Li, Nuannuan and Chang, Hao and Wang, Wei},
  doi          = {10.1093/comjnl/bxac124},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2849-2859},
  shortjournal = {Comput. J.},
  title        = {AWFC: Preventing label flipping attacks towards federated learning for intelligent IoT},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on adversarial AI to IoT security and privacy
protection: Attacks and defenses. <em>COMJNL</em>, <em>65</em>(11),
2847–2848. (<a href="https://doi.org/10.1093/comjnl/bxac128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Gao, Honghao and Tan, Zhiyuan},
  doi          = {10.1093/comjnl/bxac128},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2847-2848},
  shortjournal = {Comput. J.},
  title        = {Special issue on adversarial AI to IoT security and privacy protection: Attacks and defenses},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Erratum to: MoG: Behavior-obfuscation resistance malware
detection. <em>COMJNL</em>, <em>65</em>(10), 2846. (<a
href="https://doi.org/10.1093/comjnl/bxab010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Cheng, Binlin and Liu, Jinjun and Jiejie, Chen and Shudong, Shi and Xufu, Peng and Xingwen, Zhang and Hai, Haiqing},
  doi          = {10.1093/comjnl/bxab010},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2846},
  shortjournal = {Comput. J.},
  title        = {Erratum to: MoG: behavior-obfuscation resistance malware detection},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Corrigendum to: Similarity of sentences with contradiction
using semantic similarity measures. <em>COMJNL</em>, <em>65</em>(10),
2845. (<a href="https://doi.org/10.1093/comjnl/bxaa129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Krishna Siva Prasad, M and Sharma, Poonam},
  doi          = {10.1093/comjnl/bxaa129},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2845},
  shortjournal = {Comput. J.},
  title        = {Corrigendum to: Similarity of sentences with contradiction using semantic similarity measures},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Public-key authenticated encryption with keyword search: A
generic construction and its quantum-resistant instantiation.
<em>COMJNL</em>, <em>65</em>(10), 2828–2844. (<a
href="https://doi.org/10.1093/comjnl/bxab119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The industrial Internet of Things (IIoT) integrates sensors, instruments, equipment and industrial applications, enabling traditional industries to automate and intelligently process data. To reduce the cost and demand of required service equipment, IIoT relies on cloud computing to further process and store data. Public-key encryption with keyword search (PEKS) plays an important role, due to its search functionality, to ensure the privacy and confidentiality of the outsourced data and the maintenance of flexibility in the use of the data. Recently, Huang and Li proposed the ‘public-key authenticated encryption with keyword search’ (PAEKS) to avoid the insider keyword guessing attacks (IKGAs) in the previous PEKS schemes. However, all current PAEKS schemes are based on the discrete logarithm assumption and are therefore vulnerable to quantum attacks. In this study, we first introduce a generic PAEKS construction, with the assistance of a trusted authority, that enjoys the security against IKGA in the standard model, if all building blocks are secure under standard model. Based on the framework, we further propose a novel instantiation of quantum-resistant PAEKS that is based on NTRU assumption under random oracle. Compared with its state-of-the-art counterparts, the experiment result indicates that our instantiation is more efficient and secure.},
  archive      = {J_COMJNL},
  author       = {Liu, Zi-Yuan and Tseng, Yi-Fan and Tso, Raylin and Mambo, Masahiro and Chen, Yu-Chi},
  doi          = {10.1093/comjnl/bxab119},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2828-2844},
  shortjournal = {Comput. J.},
  title        = {Public-key authenticated encryption with keyword search: A generic construction and its quantum-resistant instantiation},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Long multi-digit number recognition from images empowered by
deep convolutional neural networks. <em>COMJNL</em>, <em>65</em>(10),
2815–2827. (<a href="https://doi.org/10.1093/comjnl/bxab117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scanning images and converting the scanned information into digital format is an active research area. Scanning is an automated, fast and efficient process as compared to the traditional data entry, and the resultant converted data is more accurate. Recognizing digits from the scanned images is a challenging task. To address this issue, most of the existing techniques perform multiple individual steps that are localization, segmentation and recognition. Some researchers also focused on adopting a unified approach that combined these three steps for multi-digit recognition of up to five digits. To cope with the modern requirements, a unified multi-digit recognition technique capable of recognizing more than five digits is the need of the hour. Considering this necessity, a unified multi-digit recognition approach is presented in the current study that can recognize sequences up to 18 digits long. The proposed technique is based on a deep convolutional neural network algorithm that performs two basic functions. First, it localizes and extracts the region of interest in the image, and then it performs multi-digit recognition. The proposed algorithm recognizes sequences of up to 18 characters that makes it one of the preferred recognition techniques among the existing algorithms. The proposed technique is compared with state-of-the-art techniques and is proved to be superior and robust. The experiments are performed on two datasets, and overall accuracy up to 98\% is achieved.},
  archive      = {J_COMJNL},
  author       = {Asif, Muhammad and Bin Ahmad, Maaz and Mushtaq, Shiza and Masood, Khalid and Mahmood, Toqeer and Ali Nagra, Arfan},
  doi          = {10.1093/comjnl/bxab117},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2815-2827},
  shortjournal = {Comput. J.},
  title        = {Long multi-digit number recognition from images empowered by deep convolutional neural networks},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient realizability checking by modularization of LTL
specifications. <em>COMJNL</em>, <em>65</em>(10), 2801–2814. (<a
href="https://doi.org/10.1093/comjnl/bxab116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Realizability is an important requirement for reactive system specifications. A reactive system interacts with an environment and responds to input events. Realizability ensures the reactive system behaves as specified, no matter how its environment provides input. Typically, reactive system specifications are given using linear temporal logic (LTL), and they can be tested to ascertain whether the specification is realizable. However, the LTL specification realizability problem is 2EXPTIME-complete, which hinders its application to large-scale systems. In this article, we present a modularization method to mitigate this computational challenge.},
  archive      = {J_COMJNL},
  author       = {Ito, Sohei and Osari, Kenji and Shimakawa, Masaya and Hagihara, Shigeki and Yonezaki, Naoki},
  doi          = {10.1093/comjnl/bxab116},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2801-2814},
  shortjournal = {Comput. J.},
  title        = {Efficient realizability checking by modularization of LTL specifications},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An event-b-based approach to model and verify behaviors for
component-based applications. <em>COMJNL</em>, <em>65</em>(10),
2780–2800. (<a href="https://doi.org/10.1093/comjnl/bxab115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many disciplines have adopted component-based principles to avail themselves of the many advantages they bring, especially component reusability. In a short time, the component-based architecture became a renown branch in the IT world and the center of interest of many researchers. Much work has been conducted in this context for the verification of component-based applications (CBAs). However, the main focus has been on the structural aspect of such compositions, while the behavioral aspect has seldom been dealt with. In this paper, our goal is to close this gap and propose a formal approach to verify the behavioral correctness of CBAs. We first define a set of requirements to be satisfied by the structure and the behavior of a CBA, represented by a set of interactions that may occur between their components. Then, we build a formal ; model to represent these requirements in a rigorous and non-ambiguous way. The use of the ; refinement technique allows us to master the complexity of CBAs by introducing their elements in an incremental manner. The correctness of the development is ensured by establishing a set of proof obligations, under the Rodin platform, and also by animating it with the ; animator/model checker. The approach is illustrated by a running example.},
  archive      = {J_COMJNL},
  author       = {Mammar, Amel and Hamel, Lazhar and Graiet, Mohamed},
  doi          = {10.1093/comjnl/bxab115},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2780-2800},
  shortjournal = {Comput. J.},
  title        = {An event-B-based approach to model and verify behaviors for component-based applications},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Flush+reload attacks on SEED. <em>COMJNL</em>,
<em>65</em>(10), 2769–2779. (<a
href="https://doi.org/10.1093/comjnl/bxab113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flush+Reload is a powerful access-driven cache attack in which the attacker leverages a security weakness in the X86 processor architecture to extract the private data of the victim. This attack can be mounted in a cross-core setting, where the memory deduplication is enabled and several users are sharing the same physical machine. In this paper, for the first time, we demonstrate that SEED implementation of OpenSSL 1.1.0 running inside the victim VM is vulnerable against the Flush+Reload attacks and the attacker can recover the keys of this encryption. SEED is a standard encryption algorithm that was developed by the Korea Information Security Agency (KISA) and has been used for confidential services in the Republic of Korea. Our work demonstrates that the attacker can retrieve the secret keys of SEED in 3 min in the native setup and 4 min in the cross-VM setup by performing the Flush+Reload technique. Our experimental results show that common implementation of this standard cipher is vulnerable to Flush+Reload attack in both native and cross-VM settings. To the best of our knowledge, this paper presents the first cache-based attack on a SEED block cipher.},
  archive      = {J_COMJNL},
  author       = {Seddigh, Milad and Soleimany, Hadi},
  doi          = {10.1093/comjnl/bxab113},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2769-2779},
  shortjournal = {Comput. J.},
  title        = {Flush+Reload attacks on SEED},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blockchain-based confidential multiparty contract signing
protocol without TTP using elliptic curve cryptography. <em>COMJNL</em>,
<em>65</em>(10), 2755–2768. (<a
href="https://doi.org/10.1093/comjnl/bxab112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Signed information is difficult to manage and this difficulty is even bigger when pieces of signed digital information have to be exchanged in an atomic procedure. Multiparty contract signing is an application of fair exchange of values, where the information to be exchanged are signed versions of a contract. Traditionally, this trust service requires the involvement of a trusted third party (TTP). This is a point of weakness for the generalization of the use of electronic contract signing protocols. Another usual problem for multiparty contract signing is that the protocols for multiparty exchanges are far more complex that those for two-party exchanges. Additionally, it is common that the signers demand protocols that assure the confidentiality of the contract. In this proposal blockchain-based technologies and smart contracts are used to obtain a protocol for electronic contract signing that does not need a TTP in any stage of the procedure. The protocol satisfies the ideal requirements for the contract signing service, even allowing the signature of confidential contracts. Moreover, the proposed solution is far more efficient than previous solutions for fair exchange protocols using blockchain presented up to the date thanks to the use of elliptic curve cryptography.},
  archive      = {J_COMJNL},
  author       = {Payeras-Capellà, M Magdalena and Mut-Puigserver, Macià and Cabot-Nadal, Miquel À and Huguet-Rotger, Llorenç},
  doi          = {10.1093/comjnl/bxab112},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2755-2768},
  shortjournal = {Comput. J.},
  title        = {Blockchain-based confidential multiparty contract signing protocol without TTP using elliptic curve cryptography},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fingerprint template protection using irreversible minutiae
tetrahedrons. <em>COMJNL</em>, <em>65</em>(10), 2741–2754. (<a
href="https://doi.org/10.1093/comjnl/bxab111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of fingerprint continues to increase constantly with the propagation of biometric authentication technologies. Fingerprint is today the most widely used biometric modality for human verification and identification, due to its practical use and its discriminative structure that allows to different identities to be easily distinguished. Unfortunately, this emergence has been accompanied by a number of problems and challenges related to identity theft and to security issues in general. To address these concerns, many approaches have been proposed in which only few of them were able to reach an acceptable level of both security and performance. In this paper, we propose a new fingerprint template protection scheme that enhances the security of protected system while preserving performance. The proposed approach is a minutiae-based technique that performs fingerprint matching in a transformed space using irreversible minutiae tetrahedrons. Using the original Fingerprint Verification Competition (FVC) protocol, the provided experimental results on FVC2002 DB1, DB2 and DB3 fingerprint databases have shown satisfactory recognition rates. Our results are compared to some existing techniques that use the same protocol of test. We have proved as well that the proposed scheme meets the requirements of revocability, unlinkability and irreversibility.},
  archive      = {J_COMJNL},
  author       = {Lahmidi, Ayoub and Minaoui, Khalid and Moujahdi, Chouaib and Rziza, Mohammed},
  doi          = {10.1093/comjnl/bxab111},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2741-2754},
  shortjournal = {Comput. J.},
  title        = {Fingerprint template protection using irreversible minutiae tetrahedrons},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ensemble technique to predict breast cancer on multiple
datasets. <em>COMJNL</em>, <em>65</em>(10), 2730–2740. (<a
href="https://doi.org/10.1093/comjnl/bxab110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Chaurasia, Vikas and Pal, Saurabh},
  doi          = {10.1093/comjnl/bxab110},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2730-2740},
  shortjournal = {Comput. J.},
  title        = {Ensemble technique to predict breast cancer on multiple datasets},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved collision detection of MD5 using sufficient
condition combination. <em>COMJNL</em>, <em>65</em>(10), 2720–2729. (<a
href="https://doi.org/10.1093/comjnl/bxab109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counter-cryptanalysis uses cryptanalytic techniques to detect cryptanalytic attacks. It was introduced by Stevens with a collision detection algorithm that detects whether a message is one of a colliding message pair constructed using a collision attack. Later, Stevens and Shumow improved the collision detection against SHA-1 by using unavoidable conditions. However, there are no results improving collision detection against MD5 due to its weak diffusion properties. In this paper, an improved collision detection algorithm against MD5 is proposed by using the 14-bit sufficient condition combinations. This leads to the dividing the 223 classes into four sets. Each element, belonging to the first two sets, holds the same sufficient condition combination. Our new algorithm can classify 126 classes efficiently. The runtime is 28.6\% of the previous collision detection method.},
  archive      = {J_COMJNL},
  author       = {Shen, Yanzhao and Wu, Ting and Wang, Gaoli and Dong, Xinfeng and Qian, Haifeng},
  doi          = {10.1093/comjnl/bxab109},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2720-2729},
  shortjournal = {Comput. J.},
  title        = {Improved collision detection of MD5 using sufficient condition combination},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parallel divide-and-conquer algorithms for bubble sort,
selection sort and insertion sort. <em>COMJNL</em>, <em>65</em>(10),
2709–2719. (<a href="https://doi.org/10.1093/comjnl/bxab107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present efficient parallel recursive divide-and-conquer algorithms for bubble sort, selection sort, and insertion sort. Our algorithms have excellent data locality and are highly parallel. The computational complexity of our insertion sort is ; in contrast to ; of standard insertion sort.},
  archive      = {J_COMJNL},
  author       = {Ganapathi, Pramod and Chowdhury, Rezaul},
  doi          = {10.1093/comjnl/bxab107},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2709-2719},
  shortjournal = {Comput. J.},
  title        = {Parallel divide-and-conquer algorithms for bubble sort, selection sort and insertion sort},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A social network link prediction method based on stacked
generalization. <em>COMJNL</em>, <em>65</em>(10), 2693–2708. (<a
href="https://doi.org/10.1093/comjnl/bxab102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional link prediction methods of social network are vulnerable to the influence of network structure and have poor generalization, and only on a small number of networks and evaluation indicators. To improve the stability and accuracy of link prediction, this paper assembles 15 similarity indexes, introduces the idea of stacking into the link prediction of complex networks, and presents a link prediction method (Logistic-regression LightGBM Stacking Link Prediction, LLSLP). Firstly, social network link prediction is regarded as a binary classification problem. Secondly, the hyper parameters of the basic model are determined by using cross-validation and grid searching; thirdly, Logistic-regression and LightGBM are integrated by stacked generalization; Finally, take 10 different networks as practical examples. The feasibility and effectiveness of the proposed method are verified by comparing 7 evaluation indicators. The experimental results show that: the proposed method is not only more than 98.71\% higher than the traditional CN (Common Neighbor) and other models are 10.52\% higher on average. In addition, compared with the traditional 15 link prediction algorithms, ; value and ; (Matthews Correlation Coefficient) value is increased by 3.2\% ~ 9.7\% and 5.9\% ~ 14\% respectively. The proposed method has good accuracy and generalization. It can also be applied to recommendation system.},
  archive      = {J_COMJNL},
  author       = {Liu, Xiaoyang and Li, Xiang},
  doi          = {10.1093/comjnl/bxab102},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2693-2708},
  shortjournal = {Comput. J.},
  title        = {A social network link prediction method based on stacked generalization},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic detection of autism in young children using
weighted logarithmic transformed data with optimized deep learning.
<em>COMJNL</em>, <em>65</em>(10), 2678–2692. (<a
href="https://doi.org/10.1093/comjnl/bxab101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autism is difficult to recognize in young ones before twenty-four months of age. The symptoms of autism usually occur between twelve to eighteen months. If autism is detected before eighteen months, suitable treatment could be offered to the children, by which normal functioning of the brain could be attained. However, till now, there were no advanced facilities for detecting autism in children on time. That leads to many complex situations while diagnosing the disease. Hence, this paper intends to present an automatic autism detection system, in which the datasets are gathered from the UCI repository. Initially, the weighted transformation of input data is taken, which is carried out to correctly distinguish the interclass labels and reduce the dimension of data. Further, Principal Component Analysis (PCA) helps to perform the role of the dimension reduction process, from which the resultant data is considered as features. Moreover, the dimensionally reduced data as features are classified using the Deep Belief Network (DBN) that recognizes the absence or presence of autism in children. As the main contribution, this paper plans to optimize or tune the decision on the number of hidden neurons in the hidden layer, training algorithm, and activation function of DBN. This optimization process is done in such a way that the error between the predicted and actual output of DBN should be minimum. Accordingly, the optimized DBN is accomplished with the aid of a hybrid algorithm that links both the Crow Search Algorithm (CSA) and the Whale Optimization Algorithm (WOA). The adopted algorithm is known as the Whale Assisted Crow Search Algorithm (W-CSA), because the CSA principle is based on WOA. To the next of implementation, the proposed autism detection model is compared with the conventional approaches, and the results are analysed in an effective manner. Accordingly, from the analysis, it is evident that the accuracy of the proposed algorithm was 4.35\%, 2.86\%, 1.41\%, 1.41\%, and 71.43\% better than Particle Swarm Optimization (PSO), FireFly (FF), Grey Wolf Optimizer (GWO), WOA, and CS (Crow Search) algorithms, respectively.},
  archive      = {J_COMJNL},
  author       = {Guruvammal, S and Chellatamilan, T and Deborah, L Jegatha},
  doi          = {10.1093/comjnl/bxab101},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2678-2692},
  shortjournal = {Comput. J.},
  title        = {Automatic detection of autism in young children using weighted logarithmic transformed data with optimized deep learning},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tunicate swarm magnetic optimization with deep convolution
neural network for collaborative filter recommendation. <em>COMJNL</em>,
<em>65</em>(10), 2664–2677. (<a
href="https://doi.org/10.1093/comjnl/bxab098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative filtering (CF) is a well-known and eminent recommendation technique to predict the preference of new users by revealing the structures of historical records of the examined users. Even though CF is effectively adapted in several commercial areas, many limitations still exist, particularly in the sparsity of rating data that raises many issues. This paper devises a novel deep learning strategy for CF to recognize user preferences. Here, black hole entropic fuzzy clustering (BHEFC) is devised for clustering item sequences to form groups with similar item sequences. Moreover, cluster centroids are optimized using the tunicate swarm magnetic optimization algorithm (TSMOA), which is devised by combining tunicate swarm algorithm and magnetic optimization algorithm. After grouping similar items together, the group matching is performed based on a deep convolutional neural network (Deep CNN). Subsequently, the visitor sequence and query sequence are compared using Jaro–Winkler distance, which contributes to the best visitor sequence. From this best visitor sequence, the recommended product is acquired. The proposed TSMOA–BHEFC and Deep CNN outperformed other methods with minimal mean absolute error of 0.200, mean absolute percentage error of 0.198 and root mean square error of 0.447, respectively.},
  archive      = {J_COMJNL},
  author       = {Gupta, Shefali and Goel, Ankit and Dave, Dr Meenu},
  doi          = {10.1093/comjnl/bxab098},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2664-2677},
  shortjournal = {Comput. J.},
  title        = {Tunicate swarm magnetic optimization with deep convolution neural network for collaborative filter recommendation},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimization of climatic conditions affecting determination
of the amount of water needed by plants in relation to their life cycle
with particle swarm optimization, and determining the optimum irrigation
schedule. <em>COMJNL</em>, <em>65</em>(10), 2654–2663. (<a
href="https://doi.org/10.1093/comjnl/bxab097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plants’ need for water has become a topic of research for the agriculture industry. The fact that plant species are very diverse and each plant’s need for water varies makes it difficult to plan programs with conventional irrigation methods. Plants exhibit different stages from their seed time to harvest season. Each stage is defined within as days, and the amount of water needed by the plant throughout these stages varies. In this study, optimization of the irrigation schedule for each stage of a plant is provided. The amount of water needed by the plant was first figured out by using climatic data, and then, these values were recalculated in relation to the plant type. The amount of water needed at each stage was related to the plant type by using particle swarm optimization. The obtained results revealed the optimal irrigation schedule for each stage with the obtained data.},
  archive      = {J_COMJNL},
  author       = {Bülbül, Mehmet Akif and Öztürk, Celal and Işık, Mehmet Fatih},
  doi          = {10.1093/comjnl/bxab097},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2654-2663},
  shortjournal = {Comput. J.},
  title        = {Optimization of climatic conditions affecting determination of the amount of water needed by plants in relation to their life cycle with particle swarm optimization, and determining the optimum irrigation schedule},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A one-dimensional probabilistic convolutional neural network
for prediction of breast cancer survivability. <em>COMJNL</em>,
<em>65</em>(10), 2641–2653. (<a
href="https://doi.org/10.1093/comjnl/bxab096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, machine learning plays a major role in different branches of the healthcare industry, from prognosis and diagnosis to drug development providing a significant perspective on the medical landscape for disease prevention or treatment and the improvement of human life. Recently, the use of deep neural networks in different machine learning applications has shown a great contribution to the improvement of the accuracy of predictions. In this paper, a novel application of convolutional neural networks on medical prognosis is presented. The proposed method employs a one-dimensional convolutional neural network (1D-CNN) to predict the survivability of breast cancer patients. After further examining the network architecture, a number of 8, 14 and 24 convolutional filters were considered within three layers, respectively, followed by a max-pooling layer after the second and third layers. In addition, regarding the probabilistic nature of the survivability prediction problem, an extra layer was added to the network in order to calculate the probability of the patient survivability. To train the developed 1D-CNN machine, the SEER database as the most reliable repository of cancer survivability was used to retrieve the required training set. After a pre-processing to remove unusable records, a set of 50 000 breast cancer cases including 35 features was prepared for training the machine. Based on the results obtained in this study, the developed machine could reach an accuracy of 85.84\%. This accuracy is the highest level of accuracy compared to the previous prediction methods. Furthermore, the mean squared error of the calculated probability was 0.112, which is an acceptable value of error for a probability calculation machine. The output of the developed machine can be used reliably by physicians to make decision about the most appropriate treatment strategy.},
  archive      = {J_COMJNL},
  author       = {Salehi, Mohsen and Razmara, Jafar and Lotfi, Shahriar and Mahan, Farnaz},
  doi          = {10.1093/comjnl/bxab096},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2641-2653},
  shortjournal = {Comput. J.},
  title        = {A one-dimensional probabilistic convolutional neural network for prediction of breast cancer survivability},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context-aware authentication with dynamic credentials using
electricity consumption data. <em>COMJNL</em>, <em>65</em>(10),
2631–2640. (<a href="https://doi.org/10.1093/comjnl/bxab094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial IoT (IIoT) era is evolving rapidly in parallel to the progress in Industry 4.0, which leads factories to increase the engagement with external parties through different communication infrastructures. This brings a larger attack surface and requires the development of new security solutions suitable for IIoT systems. Authentication is a key enabler to prevent the attacks that come from the interactions with the outside world components. Though there are proposed authentication schemes for IoT or machine-to-machine (M2M) applications, they cannot be readily applied for IIoT since manufacturing machines have different features and requirements than generic IT and IoT devices. In this paper, context information is proposed to be used for enhancing the authentication process in the IIoT, where instantaneous electricity consumption measured by smart meters is the main context information used as a dynamic authentication credential. Besides, in our method, existing smart meter infrastructure is utilized for both exchanging credentials over an industrial network and verification of the credentials by trusted components like Supervisory Control and Data Acquisition (SCADA) or Energy Management System (EMS). The proposed method also allows the use of other context information as authentication credentials such as temperature and humidity collected by sensors in the manufacturing environment.},
  archive      = {J_COMJNL},
  author       = {Ustundag Soykan, Elif and KaraÇay, Leyli and Bilgin, Zeki and Tomur, Emrah and Akif Ersoy, Mehmet and KarakoÇ, Ferhat and Çomak, Pinar},
  doi          = {10.1093/comjnl/bxab094},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2631-2640},
  shortjournal = {Comput. J.},
  title        = {Context-aware authentication with dynamic credentials using electricity consumption data},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Friends-based crowdsourcing: Algorithms for task
dissemination over social groups. <em>COMJNL</em>, <em>65</em>(10),
2615–2630. (<a href="https://doi.org/10.1093/comjnl/bxab093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing has become increasingly popular in recent years. In order to achieve the optimal task allocation, one of the most important issues is to select more suitable crowdworkers. By leveraging its pervasiveness, social network can be employed as a novel worker recruitment platform. A robust task allocation scheme over the social network could also consider the word-of-mouth (WoM) mode, in which tasks are delivered from workers to workers. In this paper, we discuss an Non-deterministic Polynomial-Hard (NP-Hard) problem, cost-effective and budget-balanced task allocation (CBTA) problem under the WoM mode in social groups. We propose two heuristic algorithms: CB-greedy and CB-local based on greedy strategy and local search technique, respectively. We also prove that the running time of CB-greedy is ; , whereas CB-local utilizing disjoint-set achieves ; , where ; is the number of edges indicating interactions of social groups, ; is the number of social groups and ; is the inverse Ackerman function. Extensive experiments validate the efficiency and performance of our proposed algorithms.},
  archive      = {J_COMJNL},
  author       = {Li, Zhiyao and Liu, Wei and Gao, Xiaofeng and Chen, Guihai},
  doi          = {10.1093/comjnl/bxab093},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2615-2630},
  shortjournal = {Comput. J.},
  title        = {Friends-based crowdsourcing: Algorithms for task dissemination over social groups},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How many pop-stacks does it take to sort a permutation?
<em>COMJNL</em>, <em>65</em>(10), 2610–2614. (<a
href="https://doi.org/10.1093/comjnl/bxab092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pop-stacks are variants of stacks that were introduced by Avis and Newborn in 1981. Coincidentally, a 1982 result of Unger implies that every permutation of length ; can be sorted by ; passes through a deterministic pop-stack. We give a new proof of this result inspired by Knuth’s zero-one principle.},
  archive      = {J_COMJNL},
  author       = {Albert, Michael and Vatter, Vincent},
  doi          = {10.1093/comjnl/bxab092},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2610-2614},
  shortjournal = {Comput. J.},
  title        = {How many pop-stacks does it take to sort a permutation?},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analyzing the research trends of IoT using topic modeling.
<em>COMJNL</em>, <em>65</em>(10), 2589–2609. (<a
href="https://doi.org/10.1093/comjnl/bxab091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The internet of things (IoT) is one of the most rapidly growing technologies. Therefore, the interest in industry and academia has been increasing. The published research data have evolved in IoT because of scientific advances in this field. Since science plays a vital role in decision-making, this study examines the thematic landscape of research on IoT, which may contribute to understanding the research field’s structure allows for critical reflections and the identification of blind spots for advancing this field. The current study applies a text mining approach on 25966 Scopus-indexed abstracts and titles published from 2008 to 2020 on a latent Dirichlet allocation-based topic model. In this study, various models in the range of 1–100 topics were created. Examination of coherence scores was combined with manual analysis; the 25-topic model was chosen as an optimal one. The statistical methods employed highlight the timely trends of the extracted topics, intellectual topic structure and resulting communities in the topic network. The study carpingly depicts the quantitative results from an IoT perspective. The statistical analysis depicts that IoT publications has exponential growth rate. The hotspot of the IoT research can be concluded as ‘intrusion attack detection’, ‘cloud and edge computing’, ‘energy consumption’, ‘access channels’, ‘algorithm optimization’ and ‘healthcare and medical’. The topics that reflect the wireless sensor networks, security and privacy, high-range signal, devices and context aware computing and sensor control and monitoring have stable trends. This study identifies research focus on the development of low-energy consumption systems (Green IoT), application of high-range signals and their performance in tracking and identification, and data analytics (Big data IoT). Furthermore, the research focuses on industrial solutions towards diseases diagnosis and its treatment in health sector. Finally, in agriculture sector for intelligent manufacturing, research focuses on the application of image recognition for plant and food analysis.},
  archive      = {J_COMJNL},
  author       = {Inaam ul Haq, Muhammad and Li, Qianmu and Hou, Jun},
  doi          = {10.1093/comjnl/bxab091},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2589-2609},
  shortjournal = {Comput. J.},
  title        = {Analyzing the research trends of IoT using topic modeling},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Policy-based editing-enabled signatures: Authenticating
fine-grained and restricted data modification. <em>COMJNL</em>,
<em>65</em>(10), 2570–2588. (<a
href="https://doi.org/10.1093/comjnl/bxab090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data owners often encrypt their bulk data and upload it to cloud in order to save storage while protecting privacy of their data at the same time. A data owner can allow a third-party entity to decrypt and access her data. However, if that entity wants to modify the data and publish the same in an authenticated way, she has to ask the owner for a signature on the modified data. This incurs substantial communication overhead if the data is modified often. In this work, we introduce the notion of ; , where the data owner specifies a policy for her data such that ; an entity satisfying this policy can decrypt the data. Moreover, the entity is permitted to produce a valid signature for the modified data (on behalf of the owner) ; interacting with the owner every time the data is modified. On the other hand, a policy-based editing-enabled signature (PB-EES) scheme allows the data owner to choose ; set of modification operations applicable to her data and still restricts a (possibly untrusted) entity to authenticate the data modified using operations from that set ; . We provide two PB-EES constructions, a generic construction and a concrete instantiation. We formalize the security model for PB-EESs and analyze the security of our constructions. Finally, we evaluate the performance of the concrete PB-EES instantiation.},
  archive      = {J_COMJNL},
  author       = {Sengupta, Binanda and Li, Yingjiu and Tian, Yangguang and Deng, Robert H and Yang, Zheng},
  doi          = {10.1093/comjnl/bxab090},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2570-2588},
  shortjournal = {Comput. J.},
  title        = {Policy-based editing-enabled signatures: Authenticating fine-grained and restricted data modification},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerating analytics using improved binary particle swarm
optimization for discrete feature selection. <em>COMJNL</em>,
<em>65</em>(10), 2547–2569. (<a
href="https://doi.org/10.1093/comjnl/bxab089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection, a combinatorial optimization problem, remains broadly applied in the area of Computational Learning with the aim to construct a model with reduced features so as to improve the performance of the model. Feature selection algorithm aims to identify admissible subgroup of features without sacrificing the accuracy of the model. This research works uses Improved Binary Particle Swarm Optimization (IBPSO) to optimally identify subset of features. The problem of stagnation, trapping in local optima and premature convergence of Binary Particle Swarm Optimization (BPSO) for solving discrete feature selection dispute has been tackled using IBPSO. IBPSO prevents the model from overfitting and also takes less computational time for constructing the model because of reduced feature subset. The sine function, cosine function, position of the random particle and linear decrement of inertial weight are integrated in IBPSO, which balances between exploration and exploitation to identify optimal subset of features. The linear decrement of inertial weight tends to do good level of exploration at the starting phase, whereas at the end it tends to exploit solution space to find the optimal subset of features that are more informative and thereby discarding redundant and irrelevant features. Experimentation is carried out on seven benchmarking datasets obtained from University of California, Irvine repository, which includes various real-world datasets for processing with machine learning algorithms. The proposed IBPSO is compared with conventional metaheuristic algorithms such as BPSO, Simulated Annealing, Ant Colony Optimization, Genetic Algorithm and other hybrid metaheuristic feature selection algorithms. The result proves that IBPSO maximizes the accuracy of the classifier together with maximum dimensionality reduction ratio. Also, statistical tests such as T-test, Wilcoxon signed-pair test are also carried out to demonstrate IBPSO is better than other algorithms taken for experimentation with confidence level of 0.05.},
  archive      = {J_COMJNL},
  author       = {Moorthy, Rajalakshmi Shenbaga and Pabitha, P},
  doi          = {10.1093/comjnl/bxab089},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2547-2569},
  shortjournal = {Comput. J.},
  title        = {Accelerating analytics using improved binary particle swarm optimization for discrete feature selection},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new ensemble approach for congestive heart failure and
arrhythmia classification using shifted one-dimensional local binary
patterns with long short-term memory. <em>COMJNL</em>, <em>65</em>(9),
2535–2546. (<a href="https://doi.org/10.1093/comjnl/bxac087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electrocardiogram (ECG) is a vital diagnostic tool for identifying a variety of cardiac disorders, including cardiac arrhythmia (ARR), sinus rhythms and heart failure. However, rapid interpretation of ECG recordings is quite important in the diagnosis of heart-related diseases. Many patients can be saved using the systems developed for the rapid and accurate analysis of ECG signals. A novel ensemble method based on shifted one-dimensional local binary patterns (S-1D-LBP) and long short-term memory (LSTM) is presented for the prognosis of ARR, normal sinus rhythm (NSR) and congestive heart failure (CHF) in this study. The ECG signals were first subjected to the S-1D-LBP method. Depending on the R and L parameters of this method, nine different signals are generated. Each of the histograms of these signals is given to LSTM models with the same hyperparameters. ECG signals are classified according to the common decisions of LSTM models with nine different input signals. The suggested method was tested using ECG signals (ARR, NSR and CHF) from the MIT-BIH and BIDMC datasets. Considering the results obtained in the applications carried out with various scenarios, it was observed that a high (99.6\%) success rate was attained by the proposed approach. The suggested S-1D-LBP + ELSTM (Ensemble LSTM) model is expected to be safe to employ in the classification of various signals.},
  archive      = {J_COMJNL},
  author       = {Çalışkan, Abidin},
  doi          = {10.1093/comjnl/bxac087},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2535-2546},
  shortjournal = {Comput. J.},
  title        = {A new ensemble approach for congestive heart failure and arrhythmia classification using shifted one-dimensional local binary patterns with long short-term memory},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The h-restricted connectivity of a class of hypercube-based
compound networks. <em>COMJNL</em>, <em>65</em>(9), 2528–2534. (<a
href="https://doi.org/10.1093/comjnl/bxab105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the multiprocessor systems modeled by interconnection networks, one of the important properties is the characterization of fault tolerability. Connectivity, as an important parameter to evaluate fault tolerability, has witnessed research achievements. To make the evaluation more practical, conditional connectivity has been promisingly proposed. As one kind of conditional connectivity, ; -restricted connectivity of a connected graph ; , denoted by ; , is defined as the cardinality of the minimum vertex cut set ; such that ; . In this paper, we establish a universally ; -restricted connectivity for a class of hypercube-based compound networks, in which the well-known networks, such as hierarchical cubic network ; and its generalization complete cubic network ; , are involved.},
  archive      = {J_COMJNL},
  author       = {Li, Xiaowang and Zhou, Shuming and Ma, Tianlong and Guo, Xia and Ren, Xiangyu},
  doi          = {10.1093/comjnl/bxab105},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2528-2534},
  shortjournal = {Comput. J.},
  title        = {The h-restricted connectivity of a class of hypercube-based compound networks},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimized scale-invariant feature transform with local
tri-directional patterns for facial expression recognition with deep
learning model. <em>COMJNL</em>, <em>65</em>(9), 2506–2527. (<a
href="https://doi.org/10.1093/comjnl/bxab088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) is the process of identifying human expressions. People vary in their accuracy at recognizing the emotions of others. Use of technology to help people with emotion recognition is a relatively important research area. Various works have been conducted on automating the recognition of facial expressions. The main intent of this paper is to plan for the FER model with the aid of intelligent techniques. The proposed models consist of steps like data collection, face detection, optimized feature extraction and emotion recognition. Initially, the standard benchmark facial emotion dataset is collected, and it is subjected to face detection. The optimized scale-invariant feature transform (OSIFT) is adopted for feature extraction, in which the key points that are giving unique information are optimized by the hybrid meta-heuristic algorithm. Two meta-heuristic algorithms like spotted hyena optimization and beetle swarm optimization (BSO) are merged to form the proposed spotted hyena-based BSO (SH-BSO). Also, the local tri-directional pattern is extracted, which is further combined with optimized SIFT. Here, the proposed SH-BSO is utilized for optimizing the number of hidden neurons of both deep neural network and convolutional neural network in such a way that the recognition accuracy could attain maximum.},
  archive      = {J_COMJNL},
  author       = {Lokku, Gurukumar and Reddy, G Harinatha and Prasad, M N Giri},
  doi          = {10.1093/comjnl/bxab088},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2506-2527},
  shortjournal = {Comput. J.},
  title        = {Optimized scale-invariant feature transform with local tri-directional patterns for facial expression recognition with deep learning model},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Social network analysis of the panama papers concentrating
on the MENA region. <em>COMJNL</em>, <em>65</em>(9), 2493–2505. (<a
href="https://doi.org/10.1093/comjnl/bxab086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The release of millions of financial documents, which has been known as the ‘WikiLeaks’ of the financial world (a.k.a. ‘Panama Papers’), has dragged global attention in how highly structured means applied by some of the elite to conceal their financial assets. Consequently, significant financial corruption allegations were raised. We concentrate on a somewhat overlooked region, the Middle East and North Africa (MENA) region. This study aims to use social network analytics to study the information contained in these documents. We are checking the major players in the MENA’s trends and patterns to determine if it matches the known economic powers. The analysis reveals that while the constructed network enjoys some typical characteristics, many interesting observations and properties are worth discussing. Specifically, using the extracted network consisting of 62 987 nodes and 84 692 edges, our social network analysis finding shows that, perhaps surprisingly, the nodes or the social network are not necessarily directly correlated with perceived economic influence.},
  archive      = {J_COMJNL},
  author       = {Al shboul, Bashar and Rabab’ah, Abdullateef and Al-Ayyoub, Mahmoud and Jararweh, Yaser and Baker, Thar},
  doi          = {10.1093/comjnl/bxab086},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2493-2505},
  shortjournal = {Comput. J.},
  title        = {Social network analysis of the panama papers concentrating on the MENA region},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joining formal concept analysis to feature extraction for
data pruning in cloud of things. <em>COMJNL</em>, <em>65</em>(9),
2484–2492. (<a href="https://doi.org/10.1093/comjnl/bxab085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The enormous increase in the number of Internet of Things data sources (sensors, personal devices and embedded systems) has given rise to a huge amount of unnecessary and redundant data being sent to the cloud. This makes the task of processing and storing this volume of information a very hard one. Therefore, data pre-processing and filtering closer to data sources, such as in fog computing, is necessary. In particular, data reduction in the fog nodes may play a significant role in preventing the dramatic decrease in the Cloud of Things performance, especially in energy consumption, storage space, bandwidth and throughput. However, existing solutions are still lacking, considering they do not achieve the optimal data reduction performance especially in terms of delay and accuracy. In this article, we introduce an approach aiming to eliminate useless and redundant data captured by things basing on some intelligent information extraction techniques. We also evaluate the performance of the proposed solution on a real data set sample to demonstrate that it achieves a good features reduction.},
  archive      = {J_COMJNL},
  author       = {Moulahi, Tarek},
  doi          = {10.1093/comjnl/bxab085},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2484-2492},
  shortjournal = {Comput. J.},
  title        = {Joining formal concept analysis to feature extraction for data pruning in cloud of things},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of machine learning and deep learning frameworks
for opinion mining on drug reviews. <em>COMJNL</em>, <em>65</em>(9),
2470–2483. (<a href="https://doi.org/10.1093/comjnl/bxab084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Opinion mining from medical forums such as health check-ups is sparking growing interest and a stimulating area for natural language processing. This allows for a better understanding of patient health status and drug reactions while generating new knowledge for health care professionals and drug manufacturers, which helps improve the quality of service and produce more effective treatments. In this paper, the researchers present a framework of opinions classification of drug reviews. The objective of this work is to find the best model for analyzing patients’ emotions about drugs. In this sense, the researchers oppose classical text vectorization methods (bag of words, term frequency-inverse document frequency) and word embedding methods (Word2vec, GloVe) for classical opinion mining face to modern machine learning tools with the Convolutional Neural Network (CNN), the Recurrent Neural Networks (Long Short-term Memory and Bidirectional Long Short-Term Memory). Experiments results show that the best model for drug reviews was achieved by CNN based on the Skip-gram model (85\% accuracy). Experiments have led to conclude that the performance of a given model will depend on the type of dataset used, on feature representation and better collaboration between classifiers and feature extraction methods.},
  archive      = {J_COMJNL},
  author       = {Youbi, Fatiha and Settouti, Nesma},
  doi          = {10.1093/comjnl/bxab084},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2470-2483},
  shortjournal = {Comput. J.},
  title        = {Analysis of machine learning and deep learning frameworks for opinion mining on drug reviews},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the effect of the key-expansion algorithm in simon-like
ciphers. <em>COMJNL</em>, <em>65</em>(9), 2454–2469. (<a
href="https://doi.org/10.1093/comjnl/bxab082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we investigate how the choice of the key-expansion algorithm and its interaction with the round function affect the resistance of Simon-like ciphers against rotational-XOR cryptanalysis. We observe that, among the key-expansion algorithms we consider, ; is most resistant, while ; is much less so. Implications on lightweight ciphers design are discussed and open questions are proposed.},
  archive      = {J_COMJNL},
  author       = {Lu, Jinyu and Liu, Yunwen and Ashur, Tomer and Li, Chao},
  doi          = {10.1093/comjnl/bxab082},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2454-2469},
  shortjournal = {Comput. J.},
  title        = {On the effect of the key-expansion algorithm in simon-like ciphers},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bat ant lion optimization-based generative adversarial
network for structural heath monitoring in IoT. <em>COMJNL</em>,
<em>65</em>(9), 2439–2453. (<a
href="https://doi.org/10.1093/comjnl/bxab081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributable to the rapid growth of information technology, the Internet of Things (IoT) having strong permeability characteristics, huge usage of action and better comprehensive benefits. However, it encourages the development of IoT technology in the detection of structural engineering. Structural health monitoring (SHM) is responsible for identifying techniques and for prototyping systems performing a state diagnosis of structures. Its aim is to prevent sudden civil infrastructure failure as a result of several invisible sources of damage. This paper devises a novel method, namely bat-antlion Optimization dependent generative adversarial network (BALO-based GAN) for monitoring the states of structural health. Here, IoT nodes sense the signals of each channels and sensed data are transmitted to base station (BS) using Monarch-Earthworm (Monarch-EWA)-enabled secure routing protocol that selects the optimal path for the data transmission. After performing the IoT routing, the state of the structural health is monitored at the BS. For SHM, the input signal acquired from the IoT routing phase is fed to the pre-processing step for improving the signal quality for further processing. Then, the feature extraction is performed using fractional-amplitude modulation spectrogram (fractional AMS) for extracting the best features for improving the classification accuracy. The extracted features are adapted by the GAN, which is trained by BALO. The proposed BALO is newly designed by integrating the Bat algorithm and antlion optimizer. The proposed BALO-based GAN showed improved performance with maximal accuracy of 0.912, maximal sensitivity of 0.911, maximal throughput of 0.972 and maximal specificity of 0.913, respectively.},
  archive      = {J_COMJNL},
  author       = {S, Yoganand and S, Chithra},
  doi          = {10.1093/comjnl/bxab081},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2439-2453},
  shortjournal = {Comput. J.},
  title        = {Bat ant lion optimization-based generative adversarial network for structural heath monitoring in IoT},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Security analysis and improvement of a redactable consortium
blockchain for industrial internet-of-things. <em>COMJNL</em>,
<em>65</em>(9), 2430–2438. (<a
href="https://doi.org/10.1093/comjnl/bxab080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A redactable consortium blockchain (RCB) can build a trust layer for industrial internet of things (IIoT) so as to enable IIoT to resist certain powerful attacks resulting in improper block content. The redactability is particularly important for blockchains applied in IIoT with valuable or sensitive activities such as financial IoT or energy-trading IoT. Huang ; proposed a threshold chameleon hash (TCH) scheme and then constructed an accountable-and-sanitizable chameleon signature scheme based on TCH. These two primitives are further used as fundamental modules to build an RCB, which empowers IIoT devices to operate the blockchain in a controllable way. However, our paper shows that Huang ; ’s RCB suffers from a security problem that weakens the crucial redactability. Specifically, we find out that if a transaction in a given block is legally redacted by all authorized sensors who collectively hold the private redacting key, anyone (without any private information) can further redact this redacted transaction and delete any transaction within this redacted block and, meanwhile, any sensor user with a private signing (not redacting) key can insert a forged transaction into this redacted block. We further address this threat by replacing the TCH module in Huang ; ’s RCB with our designed TCH.},
  archive      = {J_COMJNL},
  author       = {Gao, Wei and Chen, Liqun and Rong, Chunming and Liang, Kaitai and Zheng, Xianghan and Yu, Jiangshan},
  doi          = {10.1093/comjnl/bxab080},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2430-2438},
  shortjournal = {Comput. J.},
  title        = {Security analysis and improvement of a redactable consortium blockchain for industrial internet-of-things},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient multi-signature scheme using lattice.
<em>COMJNL</em>, <em>65</em>(9), 2421–2429. (<a
href="https://doi.org/10.1093/comjnl/bxab077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the help of a multi-signature scheme, we can reduce the cost of storage and bandwidth in case of many signers signing the same message. Therefore, multi-signature schemes can be used in bitcoin to reduce the size of a blockchain. In this paper, we propose a lattice-based multi-signature scheme with the following highlighted features. Our lattice-based multi-signature scheme supports signature compression and public key aggregation. The only existing lattice-based multi-signature scheme by Kansal and Dutta (Africacrypt, 2020) that supports both signature compression and public key aggregation has communication and storage cost ; , whereas our communication and storage cost is ; . Our multi-signature scheme is in the plain public key model where the special registration of the public key is not necessary and it is secure under the rogue key attack. Our multi-signature scheme is secure under the hardness of ring short integer solution problem in the random oracle model.},
  archive      = {J_COMJNL},
  author       = {Kansal, Meenakshi and Singh, Amit Kumar and Dutta, Ratna},
  doi          = {10.1093/comjnl/bxab077},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2421-2429},
  shortjournal = {Comput. J.},
  title        = {Efficient multi-signature scheme using lattice},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved meet-in-the-middle attacks on reduced-round
tweakable block cipher deoxys-BC. <em>COMJNL</em>, <em>65</em>(9),
2411–2420. (<a href="https://doi.org/10.1093/comjnl/bxab076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deoxys-BC is an internal tweakable block cipher of the authenticated encryption algorithm Deoxys, which is a third-round finalist in the CAESAR competition. In this paper, we study the property of Deoxys-BC, such as the subtweakey difference cancelation and the freedom of the tweak. Combining the differential enumeration technique with these properties, the authors achieve the key-recovery attacks on Deoxys-BC under the meet-in-the-middle attack. As a result, we get an attack on 9-round Deoxys-BC-128-128 by constructing a 6-round meet-in-the-middle distinguisher with ; plaintext–tweak combinations, ; Deoxys-BC blocks and ; 9-round Deoxys-BC-128-128 encryptions. We also present an attack on 11-round Deoxys-BC-256-128 for the first time by constructing a 7-round meet-in-the-middle distinguisher with ; plaintext-tweak combinations, ; Deoxys-BC blocks and ; 11-round Deoxys-BC-256-128 encryptions.},
  archive      = {J_COMJNL},
  author       = {Li, Manman and Chen, Shaozhen},
  doi          = {10.1093/comjnl/bxab076},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2411-2420},
  shortjournal = {Comput. J.},
  title        = {Improved meet-in-the-middle attacks on reduced-round tweakable block cipher deoxys-BC},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new and efficient lattice-based online/offline signature
from perspective of abort. <em>COMJNL</em>, <em>65</em>(9), 2400–2410.
(<a href="https://doi.org/10.1093/comjnl/bxab074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lattice-based online/offline signature is attractive for the merit of resisting quantum attacks besides the short online response time. Prior to this work, the hash-sign-switch paradigm lattice-based online/offline signatures usually increase the length of each signature, and the Fiat–Shamir candidates are highly inefficient due to multiple aborts in online signing phase. In this work we mainly address its efficient issue and propose a new paradigm of its construction in the perspective of abort. In this paradigm, one tries to remove one or more aborts from online to offline signing phase by ; -transformation. Specifically, this work proposes an efficient lattice-based online/offline signature scheme with fewer online aborts and thus allows the signer to obtain a valid signature by fewer online repetitions. Through this way, the resulting scheme can reduce much online signing time with the same signature size. The performance evaluation shows that our scheme is efficient and practical.},
  archive      = {J_COMJNL},
  author       = {Zhang, Pingyuan and Jiang, Han and Zheng, Zhihua and Wang, Hao and Xu, Qiuliang},
  doi          = {10.1093/comjnl/bxab074},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2400-2410},
  shortjournal = {Comput. J.},
  title        = {A new and efficient lattice-based Online/Offline signature from perspective of abort},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An experimental approach to exact and random boolean-widths
and their comparison with other width parameters. <em>COMJNL</em>,
<em>65</em>(9), 2392–2399. (<a
href="https://doi.org/10.1093/comjnl/bxab073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameterized complexity is an exemplary approach that extracts and exploits the power of the hidden structures of input instances to solve hard problems. The tree-width (; ), path-width (; ), branch-width (; ), clique-width (; ), rank-width (; ) and boolean-width (; ) are some width measures of graphs that are used as parameters. Applications of these width parameters show that dynamic programming algorithms based on a path, tree or branch decomposition can be an alternative to other existing techniques for solving hard combinatorial problems on graphs. A large number of the linear- or polynomial-time fixed parameter tractability algorithms for problems on graphs start by computing a decomposition tree of the graph with a small width. The focus of this paper is to study the exact and random boolean-widths for special graphs, real-world graphs and random graphs, as well as to check their competency compared with several other existing width parameters. In our experiments, we use graphs from TreewidthLIB, which is a set of named graphs and random graphs generated by the ; model. Until now, only very limited experimental work has been carried out to determine the exact and random boolean-widths of graphs. Moreover, there are no approximation algorithms for computing the near-optimal boolean-width of a given graph. The results of this paper demonstrate that the boolean-width can be used not only in theory but also in practice and is competitive with other width parameters for real graphs.},
  archive      = {J_COMJNL},
  author       = {Sharmin, Sadia},
  doi          = {10.1093/comjnl/bxab073},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2392-2399},
  shortjournal = {Comput. J.},
  title        = {An experimental approach to exact and random boolean-widths and their comparison with other width parameters},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LraSched: Admitting more long-running applications via
auto-estimating container size and affinity. <em>COMJNL</em>,
<em>65</em>(9), 2377–2391. (<a
href="https://doi.org/10.1093/comjnl/bxab072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many long-running applications (LRAs) are increasingly using containerization in shared production clusters. To achieve high resource efficiency and LRA performance, one of the key decisions made by existing cluster schedulers is the placement of LRA containers within a cluster. However, they fail to account for estimating the size and affinity of LRA containers before executing placement. We present LraSched, a cluster scheduler that places LRA containers onto machines based on their sizes and affinities while providing consistently high performance. LraSched introduces an automated method that leverages historical data and collects new information to estimate container size and affinity for an LRA. Specifically, it uses an online machine learning method to map a new incoming LRA to previous workloads from which we can transfer experience and recommends the amount of resources (size) and the degree of collocation (affinity) for the containers of the new incoming LRA. By means of recommendations, LraSched adapts the heuristic for vector bin packing to LRA scheduling and places LRA containers in a manner that both maximizes the number of LRAs deployed and minimizes the resource fragmentation, but without affecting LRA performance. Testbed and simulation experiments show that LraSched can improve the resource utilization by up to 6.2\% while meeting performance constraints for LRAs.},
  archive      = {J_COMJNL},
  author       = {Cai, Binlei and Guo, Qin and Yu, Junfeng},
  doi          = {10.1093/comjnl/bxab072},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2377-2391},
  shortjournal = {Comput. J.},
  title        = {LraSched: Admitting more long-running applications via auto-estimating container size and affinity},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel decision-making process for COVID-19 fighting based
on association rules and bayesian methods. <em>COMJNL</em>,
<em>65</em>(9), 2360–2376. (<a
href="https://doi.org/10.1093/comjnl/bxab071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since recording the first case in Wuhan in November 2020, COVID-19 is still spreading widely and rapidly affecting the health of millions all over the globe. For fighting against this pandemic, numerous strategies have been made, where the early isolation is considered among the most effective ones. Proposing useful methods to screen and diagnose the patient’s situation for the purpose of specifying the adequate clinical management represents a significant challenge in diminishing the rates of mortality. Inspired from this current global health situation, we introduce a new autonomous process of decision-making that consists of two modules. The first module is the data analysis based on Bayesian network that is employed to indicate the coronavirus symptoms severity and then classify COVID-19 cases as severe, moderate or mild. The second module represents the decision-making based on association rules method that generates autonomously the adequate decision. To construct the model of Bayesian network, we used an effective method-oriented data for the sake of learning its structure. As a result, the algorithm accuracy in making the correct decision is 30\% and in making the adequate decision is 70\%. These experimental results demonstrate the importance of the suggested methods for decision-making.},
  archive      = {J_COMJNL},
  author       = {El Khediri, Salim and Thaljaoui, Adel and Alfayez, Fayez},
  doi          = {10.1093/comjnl/bxab071},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2360-2376},
  shortjournal = {Comput. J.},
  title        = {A novel decision-making process for COVID-19 fighting based on association rules and bayesian methods},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling higher-order interactions in complex networks by
edge product of graphs. <em>COMJNL</em>, <em>65</em>(9), 2347–2359. (<a
href="https://doi.org/10.1093/comjnl/bxab070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many graph products have been applied to generate complex networks with striking properties observed in real-world systems. In this paper, we propose a simple generative model for simplicial networks by iteratively using edge corona product. We present a comprehensive analysis of the structural properties of the network model, including degree distribution, diameter, clustering coefficient, as well as distribution of clique sizes, obtaining explicit expressions for these relevant quantities, which agree with the behaviors found in diverse real networks. Moreover, we obtain exact expressions for all the eigenvalues and their associated multiplicities of the normalized Laplacian matrix, based on which we derive explicit formulas for mixing time, mean hitting time and the number of spanning trees. Thus, as previous models generated by other graph products, our model is also an exactly solvable one, whose structural properties can be analytically treated. More interestingly, the expressions for the spectra of our model are also exactly determined, which is sharp contrast to previous models whose spectra can only be given recursively at most. This advantage makes our model a good test bed and an ideal substrate network for studying dynamical processes, especially those closely related to the spectra of normalized Laplacian matrix, in order to uncover the influences of simplicial structure on these processes.},
  archive      = {J_COMJNL},
  author       = {Wang, Yucheng and Yi, Yuhao and Xu, Wanyue and Zhang, Zhongzhi},
  doi          = {10.1093/comjnl/bxab070},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2347-2359},
  shortjournal = {Comput. J.},
  title        = {Modeling higher-order interactions in complex networks by edge product of graphs},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the g-extra connectivity of the enhanced hypercubes.
<em>COMJNL</em>, <em>65</em>(9), 2339–2346. (<a
href="https://doi.org/10.1093/comjnl/bxab069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliability evaluation of interconnection networks is of significant importance to the design and maintenance of interconnection networks. The extra connectivity is an important parameter for the reliability evaluation of interconnection networks and is a generalization of the traditional connectivity. Let ; be an integer and ; be a connected graph; the ; -extra connectivity of ; is the minimum cardinality of a set of vertices in ; , if it exists, whose removal disconnects ; and leaves every component with more than ; vertices. Determining the ; -extra connectivity is still an unsolved problem in many interconnection networks. Let ; , ; be positive integers. Let ; denote the ; -enhanced hypercube. In this paper, we determine the ; -extra connectivity of ; is ; for ; , ; . Some previous results in [Zhang, M. and Zhou, J. (2015) On g-extra connectivity of folded hypercubes. ; , 593, 146–153.] and [Sabir, E., Mamut, A. and Vumar, E. (2019) The extra connectivity of the enhanced hypercubes. ; , 799, 22–31.] are extended.},
  archive      = {J_COMJNL},
  author       = {Yin, Shanshan and Xu, Liqiong},
  doi          = {10.1093/comjnl/bxab069},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2339-2346},
  shortjournal = {Comput. J.},
  title        = {On the g-extra connectivity of the enhanced hypercubes},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybridized cuckoo search with multi-verse optimization-based
patch matching and deep learning concept for enhancing video inpainting.
<em>COMJNL</em>, <em>65</em>(9), 2315–2338. (<a
href="https://doi.org/10.1093/comjnl/bxab067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to develop a novel deep learning concept to deal with video inpainting. Initially, motion tracking is performed, which is the process of determining motion vectors that describe the transformation from adjacent frames in a video sequence. Further, the regions or patches of each frame are categorized using the optimized recurrent neural network (RNN), in which the region is split into a smooth and structure region. It is performed using the texture feature called grey-level co-occurrence matrix. The filling of the smooth region is accomplished by replacing with the mean pixels of unmasked region, and the structure region is done by optimized patch matching approach based on scale-invariant feature transform (SIFT). The main objective optimized patch matching is based on the minimized Euclidean distance between the extracted SIFT features of the original patch and reference patch, and the certain patch is inpainted by the optimized patch. Here, the hybridization of two meta-heuristic algorithms like cuckoo search algorithm and multi-verse optimization (MVO) called Cuckoo Search-based MVO is used to optimize the RNN and patch matching. Finally, the experimental results verify the reliability of the proposed algorithm over existing algorithms.},
  archive      = {J_COMJNL},
  author       = {Janardhana Rao, B and Chakrapani, Y and Srinivas Kumar, S},
  doi          = {10.1093/comjnl/bxab067},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2315-2338},
  shortjournal = {Comput. J.},
  title        = {Hybridized cuckoo search with multi-verse optimization-based patch matching and deep learning concept for enhancing video inpainting},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cryptanalysis of modular exponentiation outsourcing
protocols. <em>COMJNL</em>, <em>65</em>(9), 2299–2314. (<a
href="https://doi.org/10.1093/comjnl/bxab066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public-key cryptographic primitives are time consuming for resource-constrained devices. A classical problem is to securely offload group exponentiations from a (comparatively) weak device—the client—to an untrusted more powerful device—the server. A delegation protocol must usually meet two security objectives: ; —the exponent or the base should not be revealed to a passive adversary—and ; —a malicious server should not be able to make the client accept an invalid value as the result of the delegated computation. Most proposed protocols relies on a secret splitting of the exponent and the base, and a considerable amount of literature has been devoted to their analysis. Recently, Su ; . (Su, Q., Zhang, R. and Xue, R. (2020) Secure outsourcing algorithms for composite modular exponentiation based on single untrusted cloud. ; , 63, 1271.) and Rangasamy and Kuppusamy (Rangasamy, J. and Kuppusamy, L. (2018) Revisiting Single-Server Algorithms for Outsourcing Modular Exponentiation. In Chakraborty, D. and Iwata, T. (eds), ; , New Delhi, India, December 912, Vol. 11356, Lecture Notes in Computer Science. Springer, Heidelberg, Germany, pp. 320. proposed outsourcing protocols for modular exponentiations. They claim that their protocols achieve security (privacy and verifiability). We show that these claims are flawed and that their schemes are broken beyond repair. They remain insecure even if one increases significantly the proposed parameters (and consequently the protocols computational and communication complexities). Our attacks rely on standard lattice-based cryptanalytic techniques, namely the ; to find small integer zeroes of modular multivariate polynomials and simultaneous Diophantine approximation methods for the so-called ; .},
  archive      = {J_COMJNL},
  author       = {Bouillaguet, Charles and Martinez, Florette and Vergnaud, Damien},
  doi          = {10.1093/comjnl/bxab066},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2299-2314},
  shortjournal = {Comput. J.},
  title        = {Cryptanalysis of modular exponentiation outsourcing protocols},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Research on unexpected DNS response from open DNS resolvers.
<em>COMJNL</em>, <em>65</em>(9), 2276–2298. (<a
href="https://doi.org/10.1093/comjnl/bxab063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the backbone of the domain name system (DNS), DNS resolvers are essential to the Internet. Nowadays, the measurement of DNS resolvers, especially open DNS resolvers, has become a research focus. Previous research works show that DNS responses returned from some open DNS resolvers are not expected for clients and the Internet. We call these DNS responses ‘unexpected DNS responses’. Research on unexpected DNS responses is beneficial to the research, usage and management of open DNS resolvers. This paper explores unexpected DNS responses returned from open DNS resolvers in terms of identification and classification to better understand the behaviours of open DNS resolvers. First, an identification method is proposed to identify all kinds of DNS responses from each section of DNS messages. Second, a classification method is proposed to classify unexpected DNS responses by their influences on clients and the Internet. Furthermore, an efficient identification and classification method is proposed to simplify the above process. Among about 9 million responding open DNS resolvers in the experiments on the IPv4 address space, about 40\% return unexpected DNS responses. Experimental results show that the proposed methods can identify and classify all kinds of DNS responses returned from open DNS resolvers.},
  archive      = {J_COMJNL},
  author       = {Lu, Keyu and Chai, Tingting and Xu, Haiyan and Prasad, Shitala and Yan, Jianen and Zhang, Zhaoxin},
  doi          = {10.1093/comjnl/bxab063},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2276-2298},
  shortjournal = {Comput. J.},
  title        = {Research on unexpected DNS response from open DNS resolvers},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting malicious domain names with abnormal WHOIS records
using feature-based rules. <em>COMJNL</em>, <em>65</em>(9), 2262–2275.
(<a href="https://doi.org/10.1093/comjnl/bxab062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millions of new domain names are registered every day, but a large proportion of them are malicious and usually discovered and blacklisted after the crime has been committed. In order to improve the security of domain name registration, this paper proposes a lightweight detection method based on the AdaBoost to identify malicious domain names, which focuses on proactively detecting malicious domain names by exploring the abnormal WHOIS records. The domain name registries and registrars can adopt the proposed method as the first layer of defense to identify malicious domains on the domain registration stage. Extensive experiments on a large-scale database demonstrate that the proposed approach achieves satisfactory results on various malicious domain names.},
  archive      = {J_COMJNL},
  author       = {Cheng, Yanan and Chai, Tingting and Zhang, Zhaoxin and Lu, Keyu and Du, Yuejin},
  doi          = {10.1093/comjnl/bxab062},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2262-2275},
  shortjournal = {Comput. J.},
  title        = {Detecting malicious domain names with abnormal WHOIS records using feature-based rules},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Observations on the security of COMET. <em>COMJNL</em>,
<em>65</em>(9), 2247–2261. (<a
href="https://doi.org/10.1093/comjnl/bxab061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the security of counter mode encryption with authentication tag (COMET), one of the 32 second-round candidates in National Institute of Standards and Technology’s lightweight cryptography standardization process, against differential cryptanalysis. CHAM-64/128 is a block cipher chosen as one of the underlying block ciphers in COMET for hardware-oriented applications, and a differential characteristic with a high probability for CHAM-64/128 is useful for forgery attacks on COMET. However, we find that the optimal ; -round differential characteristic for CHAM-64/128 proposed by Roh ; , which is the longest differential characteristic of CHAM-64/128, is invalid. Then, we propose a new method of distinguishing an ; -bit block cipher from an ; -bit random permutation using a differential characteristic with a probability not higher than ; . Using our method, we use two ; -round differential characteristics with a probability of ; for CHAM-64/128 to distinguish ; -round-reduced CHAM-64/128 from a ; -bit random permutation, respectively. Furthermore, we refine the probabilities of two differentials with the same input and output differential masks as the two ; -round differential characteristics, respectively. Finally, we present the first forgery attacks on COMET with the two differentials without using weak keys. Our forgery attacks follow the nonce-misuse scenario. It should be noticed that this attack does not invalidate the security claims of the designers.},
  archive      = {J_COMJNL},
  author       = {Xu, Zheng and Li, Yongqiang and Wang, Mingsheng},
  doi          = {10.1093/comjnl/bxab061},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2247-2261},
  shortjournal = {Comput. J.},
  title        = {Observations on the security of COMET},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Corrigendum to: Deep learning in age-invariant face
recognition: A comparative study. <em>COMJNL</em>, <em>65</em>(8), 2245.
(<a href="https://doi.org/10.1093/comjnl/bxab033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Sajid, Muhammad and Ali, Nouman and Ratyal, Naeem Iqbal and Usman, Muhammad and Butt, Faisal Mehmood and Riaz, Imran and Musaddiq, Usman and Baig, Mirza Jabbar Aziz and Baig, Shahbaz and Salaria, Umair Ahmad},
  doi          = {10.1093/comjnl/bxab033},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2245},
  shortjournal = {Comput. J.},
  title        = {Corrigendum to: deep learning in age-invariant face recognition: a comparative study},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel image thresholding method combining entropy with
parzen window estimation. <em>COMJNL</em>, <em>65</em>(8), 2231–2244.
(<a href="https://doi.org/10.1093/comjnl/bxab182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image thresholding is an important and efficient image segmentation technique, which is crucial and essential for image analysis and computer vision. In this paper, we proposed a new image thresholding method based on entropy and parzen window (PW) estimation. First, the probability of each gray-level distribution is approximate by using the PW estimation. Second, by combining the obtained probability information with entropic information of the foreground and background, a new objective function is created. At last, the ideal threshold value is obtained by optimizing the objective function. By comparing with some classical thresholding methods, such as inter class variance method (OTSU), minimum error thresholding method (MET), Kapur’s entropy based method (KAPUR) and the recent methods that take spatial information into consideration (2D-D histogram method, GLLV histogram method and Gabor histogram method), the proposed method, experiment on 10 images (one synthetic image, four nondestructive testing images and five real-world images), presents a better performance on the accuracy, robustness and visual effect of segmentation.},
  archive      = {J_COMJNL},
  author       = {Xiong, Fusong and Zhang, Jian and Ling, Yun and Zhang, Zhiqiang},
  doi          = {10.1093/comjnl/bxab182},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2231-2244},
  shortjournal = {Comput. J.},
  title        = {A novel image thresholding method combining entropy with parzen window estimation},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guess-and-determine attacks on AEGIS. <em>COMJNL</em>,
<em>65</em>(8), 2221–2230. (<a
href="https://doi.org/10.1093/comjnl/bxab059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AEGIS is one of the authenticated encryption with associated data designs selected for the final portfolio of the CAESAR competition. It combines the AES round function and simple Boolean operations to update its large state and extract a keystream to achieve an excellent software performance. The AEGIS family consists of AEGIS-128, AEGIS-256 and AEGIS-128L, which use 5, 6 and 8 parallel AES round functions to process 128, 128 and 256 bits message block per step with slightly different output functions separately. Surprisingly, very few cryptanalytic results on AEGIS have been published so far. This paper presents the first guess-and-determine attacks on AEGIS family. Firstly, we propose a new observation on the structure of AEGIS that the relations of fixed variables remain in the outputs at consecutive steps under some conditions on the AND operations, and the vectorial bitwise AND operation is biased, which is able to derive the additional variables added directly. Secondly, we add several techniques, such as divide and conquer on byte-based columns, reduction by meet in the middle and simplification through constraints on variables, for each AEGIS member. Finally, we conduct guess-and-determine attacks on AEGIS-128, AEGIS-256 and AEGIS-128L and result in a complexity of ; , ; and ; to ; , respectively. Although neither attack threatens the practical security of AEGIS, it has great significance to evaluate the resistance of such structure compared with their large internal state exploited of 640, 768 and 1024 bits. It is also the first internal state recovery attack on AEGIS without nonce reusing, while only distinguishing attacks on AEGIS exist up to now.},
  archive      = {J_COMJNL},
  author       = {Jiao, Lin and Li, Yongqiang and Du, Shaoyu},
  doi          = {10.1093/comjnl/bxab059},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2221-2230},
  shortjournal = {Comput. J.},
  title        = {Guess-and-determine attacks on AEGIS},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A refinement of key mismatch attack on NewHope.
<em>COMJNL</em>, <em>65</em>(8), 2209–2220. (<a
href="https://doi.org/10.1093/comjnl/bxab058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NewHope cryptosystem is one of the second-round submissions of the National Institute of Standards and Technology post-quantum cryptography standardization process, which is a suite of two key encapsulation mechanisms based on the ring-learning with errors (LWE) problem. It has received much attention from the research community due to its small key size and high efficiency. Recently, three key mismatch attacks are proposed against NewHope under the condition of key reuse. They do not solve the ring-LWE instance directly but exploit the leakage of secret information. As far as we know, the best result is given by Okada ; ((2020) Improving Key Mismatch Attack on NewHope with Fewer Queries. In ; , Perth, WA, Australia, November 30–December 2, pp. 505–524. Springer Cham, Switzerland), which recovers the whole secret with a success probability of ; and ; average queries. In this paper, we further improve the key mismatch attack of NewHope by reducing the average queries to ; and raising the success probability to ; . Moreover, we analyze the key mismatch attack without key reuse for the first time and we propose a combinatorial attack against NewHope1024. The total complexity of the combinatorial attack is ; , which is lower than the complexity of primal attack and the claimed security strength of NewHope1024.},
  archive      = {J_COMJNL},
  author       = {Zhang, Xue and Zheng, Zhongxiang and Wang, Anyu},
  doi          = {10.1093/comjnl/bxab058},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2209-2220},
  shortjournal = {Comput. J.},
  title        = {A refinement of key mismatch attack on NewHope},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The reliability of k-ary n-cube based on component
connectivity. <em>COMJNL</em>, <em>65</em>(8), 2197–2208. (<a
href="https://doi.org/10.1093/comjnl/bxab054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connectivity and diagnosability are two crucial subjects for a network’s ability to tolerate and diagnose faulty processors. The ; -component connectivity ; of a network ; is the minimum number of vertices whose deletion results in a graph with at least ; components. The ; -component diagnosability ; of a network ; is the maximum number of faulty vertices that the system can guarantee to identify under the condition that there exist at least ; fault-free components. This paper first establishes that the ; -component connectivity of ; -ary ; -cube ; is ; for ; , ; and ; . In view of ; , we prove that the ; -component diagnosabilities of ; -ary ; -cube ; under the PMC model and MM* model are ; for ; , ; and ; .},
  archive      = {J_COMJNL},
  author       = {Lv, Mengjie and Fan, Jianxi and Zhou, Jingya and Yu, Jia and Jia, Xiaohua},
  doi          = {10.1093/comjnl/bxab054},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2197-2208},
  shortjournal = {Comput. J.},
  title        = {The reliability of k-ary n-cube based on component connectivity},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rider chicken optimization algorithm-based recurrent neural
network for big data classification in spark architecture.
<em>COMJNL</em>, <em>65</em>(8), 2183–2196. (<a
href="https://doi.org/10.1093/comjnl/bxab053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an effective classification method named Rider Chicken Optimization Algorithm-based Recurrent Neural Network (RCOA-based RNN) to perform big data classification in spark architecture. Initially, the input data are collected from the network by the master node and then forwarded to the slave node. These nodes are responsible for storing the data and performing computations. The features are effectively selected in the slave node using the proposed RCOA. The selected features are forwarded to the master node. The big data classification is achieved in the master node by using the RNN classifier, and the training of the classifier is done using the proposed RCOA algorithm, which is the integration of the Rider optimization algorithm (ROA) with the standard Chicken Swarm Optimization (CSO). The experimentation is done by using the Switzerland dataset, Cleveland dataset, Hungarian dataset and Skin disease dataset, in which the proposed RCOA-based RNN attained better performance based on the quantitative properties, such as sensitivity, accuracy and specificity with the values of 9.3E+01\%, 9.4E+01\% and 9.3E+01\% using Hungarian dataset. The existing learning methods failed to address the complex classification problems at a reasonable time, which is overcome by the proposed method.},
  archive      = {J_COMJNL},
  author       = {R, Vinoth and J P, Ananth},
  doi          = {10.1093/comjnl/bxab053},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2183-2196},
  shortjournal = {Comput. J.},
  title        = {Rider chicken optimization algorithm-based recurrent neural network for big data classification in spark architecture},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DCAF: Dynamic cross-chain anchoring framework using smart
contracts. <em>COMJNL</em>, <em>65</em>(8), 2164–2182. (<a
href="https://doi.org/10.1093/comjnl/bxab052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain has attracted tremendous attention from both the academia and the capital market. The insertion of data in a blockchain known as anchoring makes data verifiable and independently auditable and has many potential applications. The objective of this paper is to introduce a dynamic cross-chain anchoring framework for snapshotting users’ data into multiple public blockchains with the help of smart contracts. This paper starts by proposing the framework and depicting the components and workflow. Second, we design a target chain selection algorithm that combines the states of different blockchains. Using the algorithm, users can dynamically select a suitable public blockchain for anchoring tasks. Third, the virtual chain protocol is presented and achieves data anchoring orderly and integrally. Analyzation and evaluation show that the framework can enhance user experience, robustness and performance.},
  archive      = {J_COMJNL},
  author       = {Wang, Weidong and Li, Xiaofeng and Zhao, He},
  doi          = {10.1093/comjnl/bxab052},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2164-2182},
  shortjournal = {Comput. J.},
  title        = {DCAF: Dynamic cross-chain anchoring framework using smart contracts},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). COVID-19 diagnostic system using medical image
classification and retrieval: A novel method for image analysis.
<em>COMJNL</em>, <em>65</em>(8), 2146–2163. (<a
href="https://doi.org/10.1093/comjnl/bxab051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid increase in the number of people infected with COVID-19 disease in the entire world, and with the limited medical equipment used to detect it (testing kit), it becomes necessary to provide another detection method that mainly relies on Artificial Intelligence and radiographic Image Analysis to determine the disease infection. In this study, we proposed a diagnosis system that detects the COVID-19 using chest X-ray or computed tomography (CT) scan images knowing that this system does not eliminate the reverse transcription-polymerase chain reaction test but rather complements it. The proposed system consists of the following steps, starting with extracting the image’s features using Visual Words Fusion of ResNet-50 (deep neural network) and Histogram of Oriented Gradient descriptors based on Bag of Visual Word methodology. Then training the Adaptive Boosting classifier to classify the image to COVID-19 or NOTCOVID-19 and finally retrieving the most similar images. We implemented our work on X-ray and CT scan databases, and the experimental results demonstrate the effectiveness of the proposed system. The performance of the classification task in terms of accuracy was as follows: 100\% for classifying the input image to X-ray or CT scan, 99.18\% for classifying X-ray image to COVID-19 or NOTCOVID-19 and 97.84\% for classifying CT scan to COVID-19 or NOTCOVID-19.},
  archive      = {J_COMJNL},
  author       = {Alrahhal, Maher and K P, Supreethi},
  doi          = {10.1093/comjnl/bxab051},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2146-2163},
  shortjournal = {Comput. J.},
  title        = {COVID-19 diagnostic system using medical image classification and retrieval: A novel method for image analysis},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blockchain-based fair and decentralized data trading model.
<em>COMJNL</em>, <em>65</em>(8), 2133–2145. (<a
href="https://doi.org/10.1093/comjnl/bxab050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data is a kind of important asset in the digital economy and is driving the rise of data markets. Meanwhile, data markets promote data trading efficiently and improve the utilization of data. However, several challenges about data trading need to be addressed. Here, we resolve these challenges via our blockchain-based fair and decentralized data trading model. Disputes about data correctness is settled by the decentralized arbitration mechanism in our model. To ensure the fairness of data trading, we integrate a sale contract and a deterministic public-key encryption algorithm. The decentralization feature of blockchain cuts off the single-point failure for the data trading platform. In addition, we prove that the proposed protocol achieves the desirable security properties that a secure data trading protocol should have. Moreover, utilizing the smart contract in Solidity and program in Java, we implement our model and then evaluate its performance.},
  archive      = {J_COMJNL},
  author       = {Li, Taotao and Li, Dequan and Wang, Mingsheng},
  doi          = {10.1093/comjnl/bxab050},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2133-2145},
  shortjournal = {Comput. J.},
  title        = {Blockchain-based fair and decentralized data trading model},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on recommender systems for internet of things:
Techniques, applications and future directions. <em>COMJNL</em>,
<em>65</em>(8), 2098–2132. (<a
href="https://doi.org/10.1093/comjnl/bxab049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation is a critical tool for developing and promoting the benefits of the Internet of Things (IoT). In recent years, recommender systems have attracted considerable attention in many IoT-related fields such as smart health, smart home, smart tourism and smart marketing. However, traditional recommender system approaches fail to exploit ever-growing, dynamic and heterogeneous IoT data in building recommender systems for the IoT (RSIoT). This article aims to provide a comprehensive review of state-of-the-art RSIoT, including the related techniques, applications and a discussion on the limitations of applying recommendation systems to IoT. Finally, we propose a reference framework for comparing existing studies to guide future research and practices.},
  archive      = {J_COMJNL},
  author       = {Altulyan, May and Yao, Lina and Wang, Xianzhi and Huang, Chaoran and Kanhere, Salil S and Sheng, Quan Z},
  doi          = {10.1093/comjnl/bxab049},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2098-2132},
  shortjournal = {Comput. J.},
  title        = {A survey on recommender systems for internet of things: Techniques, applications and future directions},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal real-time scheduling algorithm for wireless sensors
with regenerative energy. <em>COMJNL</em>, <em>65</em>(8), 2087–2097.
(<a href="https://doi.org/10.1093/comjnl/bxab048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic voltage and frequency scaling (DVFS) is a promising and broadly used energy-efficient technique to overcome the main problems arising from using a finite energy reservoir capacity and uncertain energy source in real-time embedded systems. This work investigates an energy management scheme for real-time task scheduling in variable voltage processors located in sensor nodes and powered by ambient energy sources. We use DVFS technique to decrease the energy consumption of sensors at the time when the energy sources are limited. In particular, we develop and prove an optimal real-time scheduling framework with speed stretching, namely energy guarantee DVFS (EG-DVFS), that jointly accounts not only for the timing constraints, but also for the energy state incurred by the properties of the system components. EG-DVFS relies on the well-known earliest deadline-harvesting scheduling algorithm combined with DVFS technique where the sensor processing frequency is fine tuned to further minimize energy consumption and to achieve an energy autonomy of the system. Further, an exact feasibility test for a set of periodic, aperiodic or even sporadic tasks is presented.},
  archive      = {J_COMJNL},
  author       = {El Ghor, Hussein and Chetto, Maryline},
  doi          = {10.1093/comjnl/bxab048},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2087-2097},
  shortjournal = {Comput. J.},
  title        = {Optimal real-time scheduling algorithm for wireless sensors with regenerative energy},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performance evaluation of machine learning techniques for
fault diagnosis in vehicle fleet tracking modules. <em>COMJNL</em>,
<em>65</em>(8), 2073–2086. (<a
href="https://doi.org/10.1093/comjnl/bxab047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With industry 4.0, data-based approaches are in vogue. However, extracting the essential features is not a trivial task and greatly influences the final result. There is also a need for specialized system knowledge to monitor the environment and diagnose faults. In this context, the diagnosis of faults is significant, for example, in a vehicle fleet monitoring system, since it is possible to diagnose faults even before the customer is aware of the fault, minimizing the maintenance costs of the modules. In this paper, several models using machine learning (ML) techniques were applied and analyzed during the fault diagnosis process in vehicle fleet tracking modules. Two approaches were proposed: ‘With Knowledge’ and ‘Without Knowledge’, to explore the dataset using ML techniques to generate classifiers that can assist in the fault diagnosis process. The approach ‘With Knowledge’ performs the feature extraction manually, using the ML techniques: random forest, naive Bayes, support vector machine and Multi Layer Perceptron; on the other hand, the approach ‘Without Knowledge’ performs an automatic feature extraction, through a convolutional neural network. The results showed that the proposed approaches are promising. The best models with manual feature extraction obtained a precision of 99.76\% and 99.68\% for detection and detection and isolation of faults, respectively, in the provided dataset. The best models performing an automatic feature extraction obtained, respectively, 88.43\% and 54.98\% for detection and detection and isolation of failures.},
  archive      = {J_COMJNL},
  author       = {Sepulvene, Luis and Drummond, Isabela and Kuehne, Bruno and Frinhani, Rafael and Leite Filho, Dionisio and Peixoto, Maycon and Reiff-Marganiec, Stephan and Batista, Bruno},
  doi          = {10.1093/comjnl/bxab047},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2073-2086},
  shortjournal = {Comput. J.},
  title        = {Performance evaluation of machine learning techniques for fault diagnosis in vehicle fleet tracking modules},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ECK-secure authenticated key exchange against auxiliary
input leakage. <em>COMJNL</em>, <em>65</em>(8), 2063–2072. (<a
href="https://doi.org/10.1093/comjnl/bxab046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Authenticated key exchange protocols are quite important primitives for practical applications, since they enable two parties to generate a shared high entropy secret key. In this paper we mainly focus on the authenticated key exchange (AKE) against auxiliary input leakage. As the major contribution of this work, we present a generic framework for the construction of AKE protocols that are secure against auxiliary input leakage. An instantiation based on the generalized decisional Diffie-Hellman (GDDH) assumption in the standard model is also given to demonstrate the feasibility of our proposed framework. We also give a comparison among the existing leakage resilient AKE protocols with auxiliary inputs.},
  archive      = {J_COMJNL},
  author       = {Li, Sujuan and Zhang, Futai},
  doi          = {10.1093/comjnl/bxab046},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2063-2072},
  shortjournal = {Comput. J.},
  title        = {ECK-secure authenticated key exchange against auxiliary input leakage},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A decision tree ensemble model for predicting bus bunching.
<em>COMJNL</em>, <em>65</em>(8), 2044–2062. (<a
href="https://doi.org/10.1093/comjnl/bxab045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Travel delays and bus overcrowding are some of the daily dissatisfactions of public transportation users. These problems may be caused by bus bunching, an event that occurs when two or more buses are running the same route together, i.e. out of schedule. Due to the stochastic nature of the traffic, a static schedule is not effective to avoid the occurrence of these events; thus, preventive actions are necessary to improve the reliability of the public transportation system. In this context, we propose a decision tree ensemble model to predict bus bunching. We use an ensemble of Random Forest, eXtreme Gradient Boosting and Categorical Boosting models applied to Global Positioning System, General Transit Feed Specification, weather and traffic situation data. The efficacy of the proposed model has been demonstrated using real data sets and has been compared with four baselines: Linear Regression, Logistic Regression, Support Vector Machine and Relevance Vector Machine. According to the results, the proposed model can achieve an efficacy between 74 and 80\% and can be used to predict bus bunching in real time up to 10 stops before its occurrence.},
  archive      = {J_COMJNL},
  author       = {Borges Santos, Veruska and S Pires, Carlos Eduardo and Cassimiro Nascimento, Dimas and de Queiroz, Andreza Raquel M},
  doi          = {10.1093/comjnl/bxab045},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2044-2062},
  shortjournal = {Comput. J.},
  title        = {A decision tree ensemble model for predicting bus bunching},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gaze estimation using neural network and logistic
regression. <em>COMJNL</em>, <em>65</em>(8), 2034–2043. (<a
href="https://doi.org/10.1093/comjnl/bxab043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, a large number of mature methods are available for gaze estimation. However, most regular gaze estimation approaches require additional hardware or platforms with professional equipment for data collection or computing that typically involve high costs and are relatively tedious. Besides, the implementation is particularly complex. Traditional gaze estimation approaches usually require systematic prior knowledge or expertise for practical operations. Moreover, they are primarily based on the characteristics of pupil and iris, which uses pupil shapes or infrared light and iris glint to estimate gaze, requiring high-quality images shot in special environments and other light source or professional equipment. We herein propose a two-stage gaze estimation method that relies on deep learning methods and logistic regression, which can be applied to various mobile platforms without additional hardware devices or systematic prior knowledge. A set of automatic and fast data collection mechanism is designed for collecting gaze images through a mobile platform camera. Additionally, we propose a new annotation method that improves the prediction accuracy and outperforms the traditional gridding annotation method. Our method achieves good results and can be adapted to different applications.},
  archive      = {J_COMJNL},
  author       = {Xia, Yifan and Liang, Baosheng and Li, Zhaotong and Gao, Song},
  doi          = {10.1093/comjnl/bxab043},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2034-2043},
  shortjournal = {Comput. J.},
  title        = {Gaze estimation using neural network and logistic regression},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-view dynamic heterogeneous information network
embedding. <em>COMJNL</em>, <em>65</em>(8), 2016–2033. (<a
href="https://doi.org/10.1093/comjnl/bxab041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing heterogeneous information network (HIN) embedding methods focus on static environments while neglecting the evolving characteristic of real-world networks. Although several dynamic embedding methods have been proposed, they are merely designed for homogeneous networks and cannot be directly applied in heterogeneous environments. To tackle above challenges, we propose a novel framework for incorporating temporal information into HIN embedding, named multi-view dynamic HIN embedding (MDHNE), which can efficiently preserve evolution patterns of implicit relationships from different views in updating node vectors over time. We first transform HIN to a series of homogeneous networks corresponding to different views. Then our proposed MDHNE applies recurrent neural network (RNN) to incorporate evolving pattern of complex network structure and semantic relationships between nodes into latent embedding spaces, and thus the node vectors from multiple views can be learned and updated when HIN evolves over time. Moreover, we come up with an attention-based fusion mechanism, which can automatically infer weights of latent vectors corresponding to different views by minimizing the objective function specific for different mining tasks. Extensive experiments clearly demonstrate that our MDHNE model outperforms state-of-the-art baselines on three real-world dynamic datasets for different network mining tasks.},
  archive      = {J_COMJNL},
  author       = {Zhang, Zhenghao and Huang, Jianbin and Tan, Qinglin},
  doi          = {10.1093/comjnl/bxab041},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2016-2033},
  shortjournal = {Comput. J.},
  title        = {Multi-view dynamic heterogeneous information network embedding},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An ensemble-based credit card fraud detection algorithm
using an efficient voting strategy. <em>COMJNL</em>, <em>65</em>(8),
1998–2015. (<a href="https://doi.org/10.1093/comjnl/bxab038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existence of fraud in credit card transactions causes many financial losses leading to customers’ loss of trust. Fraud detection methods based on machine learning techniques prevent such losses. Despite the literature on fraud detection, there is a lack of algorithms that detect fraud with acceptable performance in the credit card fraud detection field. Therefore, this study proposed a comprehensive ensemble-based method using an efficient weighted voting strategy for credit card fraud detection that can address the previous algorithms’ weaknesses. First, since the dataset is imbalanced, the proposed method balanced the dataset by stratifying it into three different proportions of normal and fraudulent transactions (1 to 1, 1 to 4 and 1 to 9 ratios). The features in each dataset are ranked by four feature-ranking methods, and the Random Forest classifier is applied to each of them for selecting the essential features. Afterward, using the seven base classifiers and the chosen features, 12 ensembles have been developed. Besides, a weighted voting strategy is proposed, and the fraudulent transactions are detected through voting based on the base classifiers’ and ensembles’ weights, which are calculated by their performance. The computational results indicated that the suggested Eclf10 is the best ensemble and its Logistic Regression classifier also has the best performance among other base classifiers. The Eclf10 leads to 99.97\% accuracy, 87.78\% precision, 97.70\% recall, 92.21\% F1-score and 95.634\% F2-score, which has a superiority over the previous ensemble-based methods (e.g. majority voting ensemble, stacking classifier, Adaboost, Gradient Boosting).},
  archive      = {J_COMJNL},
  author       = {Rakhshaninejad, Morteza and Fathian, Mohammad and Amiri, Babak and Yazdanjue, Navid},
  doi          = {10.1093/comjnl/bxab038},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1998-2015},
  shortjournal = {Comput. J.},
  title        = {An ensemble-based credit card fraud detection algorithm using an efficient voting strategy},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The collision avoidance and situation-aware media access
scheme using the registered-backoff-time method for the IEEE
802.11ah-based IoT wireless networks. <em>COMJNL</em>, <em>65</em>(8),
1977–1997. (<a href="https://doi.org/10.1093/comjnl/bxab036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IEEE 802.11ah is a new protocol designed for Internet of Things (IoT). IEEE 802.11ah uses a hierarchical ID assignment schema and designs the corresponding channel access method such that stations can access channel alternatively through the four-level’s hierarchical channel access structure. Nevertheless, collisions still cannot be effectively avoided in a dense IOT networking environment using the legacy IEEE 802.11ah. The work proposed the registration-based situation-aware access extension (RSAE) control scheme to avoid collisions, decrease backoff’s waiting time and improve slot utilization through the following two ways: (i) in contrast to the traditional CSMA/CA protocol that generates the backoff time for the next channel access after the collision happened, registering the backoff time to AP for scheduling the next channel access before the collision happens; (ii) extending the channel accessing’s privilege of those stations that did not complete the data transmission/receiving in the current slot to the available time of the next slot. Comparing with the legacy IEEE 802.11ah, the performance evaluation of the proposed RSAE scheme shown that it still can have a better throughput and a lower collision rate when there is a bigger number of stations in a slot.},
  archive      = {J_COMJNL},
  author       = {Cheng, Rung-Shiang and Li, Yin-Ming and Huang, Chung-Ming},
  doi          = {10.1093/comjnl/bxab036},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1977-1997},
  shortjournal = {Comput. J.},
  title        = {The collision avoidance and situation-aware media access scheme using the registered-backoff-time method for the IEEE 802.11ah-based IoT wireless networks},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Measuring and evaluating the visual complexity of chinese
ink paintings. <em>COMJNL</em>, <em>65</em>(8), 1964–1976. (<a
href="https://doi.org/10.1093/comjnl/bxab035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Painters arrange white space in contrast with chromatic space composed of strokes. This research measures white space, color complexity and stroke density in Chinese ink paintings and examines how these attributes influence the paintings’ perceived complexity. Empirical evidence from 21 well-known modern Chinese artists’ ink paintings shows that white space decreases paintings’ complexity, while chromatic space and stroke density increase complexity. We also reveal that a large rate of white space guides the viewers’ attention on chromatic space and enhances the impacts of color complexity and stroke density on perceived complexity. An eye-tracker measures viewers’ elaboration duration on each painting, which provides consistent evidence to validate our conclusion based on subjective reported visual complexity. Our research provides insights into the rhetorical role of white space in sensory information processing.},
  archive      = {J_COMJNL},
  author       = {Fan, Zhen-Bao and Li, Yi-Na and Zhang, Kang and Yu, Jinhui and Huang, Mao Lin},
  doi          = {10.1093/comjnl/bxab035},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1964-1976},
  shortjournal = {Comput. J.},
  title        = {Measuring and evaluating the visual complexity of chinese ink paintings},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Secure and differentiated fog-assisted data access for
internet of things. <em>COMJNL</em>, <em>65</em>(8), 1948–1963. (<a
href="https://doi.org/10.1093/comjnl/bxab031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability of Fog computing to admit and process huge volumes of heterogeneous data is the catalyst for the fast expansion of Internet of things (IoT). The critical challenge is secure and differentiated access to the data, given limited computation capability and trustworthiness in typical IoT devices and Fog servers, respectively. This paper designs and develops a new approach for secure, efficient and differentiated data access. Secret sharing is decoupled to allow the Fog servers to assist the IoT devices with attribute-based encryption of data while preventing the Fog servers from tampering with the data and the access structure. The proposed encryption supports direct revocation and can be decoupled among multiple Fog servers for acceleration. Based on the decisional ; -parallel bilinear Diffie–Hellman exponent assumption, we propose a new extended ; -parallel bilinear Diffie–Hellman exponent (E; -PBDHE) assumption and prove that the proposed approach provides ‘indistinguishably chosen-plaintext attacks secure’ data access for legitimate data subscribers. As numerically and experimentally verified, the proposed approach is able to reduce the encryption time by 20\% at the IoT devices and by 50\% at the Fog network using parallel computing as compared to the state of the art .},
  archive      = {J_COMJNL},
  author       = {Yu, Ping and Ni, Wei and Zhang, Hua and Ping Liu, Ren and Wen, Qiaoyan and Li, Wenmin and Gao, Fei},
  doi          = {10.1093/comjnl/bxab031},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1948-1963},
  shortjournal = {Comput. J.},
  title        = {Secure and differentiated fog-assisted data access for internet of things},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the hardness of sparsely learning parity with noise.
<em>COMJNL</em>, <em>65</em>(8), 1939–1947. (<a
href="https://doi.org/10.1093/comjnl/bxab027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Learning Parity with Noise (LPN) problem represents the average-case analogue of the NP-Complete problem “decoding linear codes”, and it has been extensively studied in learning theory, coding theory and cryptography with applications to quantum-resistant cryptographic schemes. However, LPN also suffers from large public key size which is the common drawback that hinders code-based cryptography from being practical. In this paper, we study a sparse variant of LPN whose public matrix consists of sparse vectors instead of following uniform distribution. We show a win–win argument that at least one of the following assumption is true: (i) either the hardness of sparse LPN is implied by that of the standard LPN under the same noise rate; (ii) or there exists new black-box constructions of public-key encryption schemes and oblivious transfer protocols from standard LPN. Since the second assumption relies on the infeasible noise regimes for LPN-based public-key cryptography, we believe that the first assumption is more likely to hold, i.e. sparse LPN is as hard as standard LPN. Finally, we give a (heuristic) method to further compress the sparse public matrix by evaluating pseudorandom functions with keys made public, whose security again resorts to the aforementioned win–win technique.},
  archive      = {J_COMJNL},
  author       = {Yan, Di and Liu, Hanlin and Zhao, Shuoyao and Yu, Yu},
  doi          = {10.1093/comjnl/bxab027},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1939-1947},
  shortjournal = {Comput. J.},
  title        = {On the hardness of sparsely learning parity with noise},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Corrigendum to: Multi-swarm cuckoo search algorithm with
q-learning model. <em>COMJNL</em>, <em>65</em>(7), 1938. (<a
href="https://doi.org/10.1093/comjnl/bxaa128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Li, Juan and Xiao, Dan-dan and Zhang, Ting and Liu, Chun and Li, Yuan-xiang and Wang, Gai-ge},
  doi          = {10.1093/comjnl/bxaa128},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1938},
  shortjournal = {Comput. J.},
  title        = {Corrigendum to: Multi-swarm cuckoo search algorithm with Q-learning model},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Corrigendum: User-experience-oriented fuzzy logic controller
for adaptive streaming. <em>COMJNL</em>, <em>65</em>(7), 1937. (<a
href="https://doi.org/10.1093/comjnl/bxz053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Hou, Yonghong and Xue, Lin and Li, Shuo and Xing, Jiaming},
  doi          = {10.1093/comjnl/bxz053},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1937},
  shortjournal = {Comput. J.},
  title        = {Corrigendum: User-experience-oriented fuzzy logic controller for adaptive streaming},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Aging facial recognition for feature extraction using
adaptive fully recurrent deep neural learning. <em>COMJNL</em>,
<em>65</em>(7), 1923–1936. (<a
href="https://doi.org/10.1093/comjnl/bxab212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The face recognition (FR) process identifies or confirms a person with the help of images and videos. The traditional aging FR (AFR) method encounters numerous issues as a result of age-related biological transformation. An adaptive fully recurrent deep neural learning (AFRDNL) technique is presented in this research to improve FR accuracy (FRA) with minimal time complexity (TC). Feature extraction and classification are two processes included in the proposed technique. The AFRDNL technique gathers facial images from the database and trains the face images with three layers. To learn the facial features, three hidden layers are employed in the AFRDNL technique. Finally, the AFRDNL technique utilizes the Gaussian activation function for matching extracted testing features with earlier saved training features. This process is repeated until a minimal training error with a higher FRA is achieved. Experimental evaluation is carried out with three image datasets like Face and Gesture Recognition Research Network, MORPH and Cross-Age Labeled faces in the Wild (CALFW) datasets using different metrics, such as FRA, false-positive rate (FPR), TC and F1score. The experimental results confirm that the AFRDNL technique effectively increases the FRA and F1score and minimizes the FPR and TC more effectively than the state-of-the-art methods.},
  archive      = {J_COMJNL},
  author       = {Shoba, Betcy Thanga and Shatheesh Sam, I},
  doi          = {10.1093/comjnl/bxab212},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1923-1936},
  shortjournal = {Comput. J.},
  title        = {Aging facial recognition for feature extraction using adaptive fully recurrent deep neural learning},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic diagnosis of diabetic retinopathy from retinal
abnormalities: Improved jaya-based feature selection and recurrent
neural network. <em>COMJNL</em>, <em>65</em>(7), 1904–1922. (<a
href="https://doi.org/10.1093/comjnl/bxab068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate diagnosis of lesions bears the highest significance in the early detection of diabetic retinopathy (DR). In this paper, the combination of intelligent methods is developed for segmenting the abnormalities like ‘hard exudates, hemorrhages, microaneurysm and soft exudates’ to detect the DR. The proposed model involves seven main steps: (a) image pre-processing, (b) optic disk removal (c) blood vessel removal, (d) segmentation of abnormalities, (e) feature extraction, (f) optimal feature selection and (f) classification. The pre-processing of the input retinal fundus image is performed by two operations like contrast enhancement by histogram equalization and filtering by average filtering. For the segmentation of abnormalities, the same Circular Hough Transform followed by Top-hat filtering and Gabor filtering is used. Next, the entropy-scale-invariant feature transform (SIFT), grey level co-occurrence matrices and color morphological features are extracted in feature extraction. The optimally selected features are subjected to the classification part, which uses a modified deep learning algorithm called optimized recurrent neural network (RNN). As the main novelty, the optimal feature selection and optimized RNN depends on an improved meta-heuristic algorithm called fitness oriented improved Jaya algorithm. Hence, the beneficial part of the optimization algorithm improves the feature selection and classification.},
  archive      = {J_COMJNL},
  author       = {Ravala, Lavanya and G K, Rajini},
  doi          = {10.1093/comjnl/bxab068},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1904-1922},
  shortjournal = {Comput. J.},
  title        = {Automatic diagnosis of diabetic retinopathy from retinal abnormalities: Improved jaya-based feature selection and recurrent neural network},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Proof automation in the theory of finite sets and finite set
relation algebra. <em>COMJNL</em>, <em>65</em>(7), 1891–1903. (<a
href="https://doi.org/10.1093/comjnl/bxab030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {(‘setlog’) is a satisfiability solver for formulas of the theory of finite sets and finite set relation algebra (FS&amp;RA). As such, it can be used as an automated theorem prover for this theory. ; is able to automatically prove a number of FS&amp;RA theorems, but not all of them. Nevertheless, we have observed that many theorems that ; cannot automatically prove can be divided into a few subgoals automatically dischargeable by ; . The purpose of this work is to present a prototype interactive theorem prover (ITP), called ; -ITP, providing evidence that a proper integration of ; into world-class ITP’s can deliver a great deal of proof automation concerning FS&amp;RA. An empirical evaluation based on 210 theorems from the TPTP and Coq’s SSReflect libraries shows a noticeable reduction in the size and complexity of the proofs with respect to Coq.},
  archive      = {J_COMJNL},
  author       = {Cristiá, Maximiliano and Katz, Ricardo D and Rossi, Gianfranco},
  doi          = {10.1093/comjnl/bxab030},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1891-1903},
  shortjournal = {Comput. J.},
  title        = {Proof automation in the theory of finite sets and finite set relation algebra},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The matching preclusion of enhanced hypercubes.
<em>COMJNL</em>, <em>65</em>(7), 1874–1890. (<a
href="https://doi.org/10.1093/comjnl/bxab029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The (conditional) matching preclusion number of a graph is the minimum number of edges whose deletion leaves the resulting graph (with no isolated vertices) that has neither perfect matchings nor almost perfect matchings. The (conditional) strong matching preclusion number of a graph is the minimum number of vertices and edges whose deletion makes the resulting graph (with no isolated vertices) without perfect matching or almost perfect matching. The enhanced hypercube ;  ; is an extension of hypercube. In this paper, we prove that the matching preclusion number of ; is ;  ; , the strong matching preclusion number of ; is ;  ; , the conditional matching preclusion number of ; is ; , the conditional matching preclusion number of ; is ;  ; and the conditional strong matching preclusion number of ; is ;  ; .},
  archive      = {J_COMJNL},
  author       = {Wang, Shiying and Ma, Xiaolei},
  doi          = {10.1093/comjnl/bxab029},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1874-1890},
  shortjournal = {Comput. J.},
  title        = {The matching preclusion of enhanced hypercubes},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CWOA: Hybrid approach for task scheduling in cloud
environment. <em>COMJNL</em>, <em>65</em>(7), 1860–1873. (<a
href="https://doi.org/10.1093/comjnl/bxab028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A cloud computing system typically comprises of a huge number of interconnected servers that are organized in a datacentre. Such servers dynamically cater to the on-demand requests put forward by the clients seeking solutions to their applications through an interface. The scheduling activity concerned with scientific applications is designated under the NP hard problem category since they make use of heterogeneous resources of dynamic capabilities. Recently cloud computing researchers had developed numerous meta-heuristic approaches for providing solutions to the challenges arising in the task scheduling activities. Scheduling of tasks poses a major concern in cloud computing environment. This decreases the efficiency of the system considerably, if not handled properly. Hence, an improvised task scheduling algorithm that enhances the performance of the cloud is needed. There are two factors that affect the cloud environment: service quality and energy usage. To increase the performance in above suggested factors (memory, makespan and energy efficiency), an efficient hybridized algorithm, obtained by integrating the Cuckoo Search Algorithm (CSA) and Whale Optimization Algorithm (WOA), called the CWOA had been proposed in this work. The performance of our proposed CWOA algorithm had been compared with Ant Colony Optimization, CSA and WOA and it was found to produce an improvement of 5.62\%, 4.36\% and 2.27\% with respect to makespan, 16.36\%, 19.19\% and 13.13\% with respect to memory utilization and 19.08\%, 19.34\% and 16.75\% with respect to energy consumption parameters, respectively. Comprehensive results have been tabulated in the result section of this article.},
  archive      = {J_COMJNL},
  author       = {Pradeep, K and Ali, L Javid and Gobalakrishnan, N and Raman, C J and Manikandan, N},
  doi          = {10.1093/comjnl/bxab028},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1860-1873},
  shortjournal = {Comput. J.},
  title        = {CWOA: Hybrid approach for task scheduling in cloud environment},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning saliency-aware correlation filters for visual
tracking. <em>COMJNL</em>, <em>65</em>(7), 1846–1859. (<a
href="https://doi.org/10.1093/comjnl/bxab026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, visual object tracking has become a hot topic in computer vision community owing to its extensive applications and fundamental research significance. Many researchers have paid attention to the discriminative correlation filter (DCF) due to its excellent tracking performance. The background-aware CF (BACF) is developed to handle the inevitable boundary effects of DCFs that shows superior performance compared with other CF-based trackers. However, BACF has a poor occlusion handling ability. To overcome this defect, we propose a saliency-aware CF tracking (SACF) model. In SACF, a saliency map of the target is introduced on CFs to strengthen the ability to extract the target from a complex background. Meanwhile, to adjust to the rapid variation of the target appearance, we adaptively modify the learning rate of the filter template using adaptive learning rate. By incorporating the saliency map into the BACF objective function and adaptive learning rate adjustment, the robustness and accuracy of the tracker are boosted significantly. Extensive experiments validate that our method can efficaciously solve the occlusion problem and achieves state-of-the-art performance in terms of accuracy and speed on standard benchmarks, i.e. OTB-2013, OTB-2015 and LaSOT. Compared with other state-of-the-art CF tracking methods, the proposed method is competitive considering accuracy and success rate.},
  archive      = {J_COMJNL},
  author       = {Wang, Yanbo and Wang, Fasheng and Wang, Chang and Sun, Fuming and He, Jianjun},
  doi          = {10.1093/comjnl/bxab026},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1846-1859},
  shortjournal = {Comput. J.},
  title        = {Learning saliency-aware correlation filters for visual tracking},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The (im)possibility on constructing verifiable random
functions. <em>COMJNL</em>, <em>65</em>(7), 1826–1845. (<a
href="https://doi.org/10.1093/comjnl/bxab023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we further explore the properties of the verifiable random functions in both a black-box and a non-black-box manner. The results are mainly following two parts:;  ; It is set up for an impossibility result of black-box reduction from verifiable random functions to injective one-way functions and indistinguishability obfuscators, where the verifiable random functions are suggested to be domain-invariant (i.e. the support of the distribution of keys and the domain of the evaluation space are independent of the underlying building blocks). Our result illustrates how the non-domain-invariant constructions circumvent the black-box barriers for constructing verifiable random functions and sheds light on why it is so difficult to give a domain-invariant instantiation.;  ; On the other hand, the verifiable unpredictable functions are constructed from a given primitive by a non-black-box technique called the hitting-set generator. To show it&#39;s a somewhat useful technique for constructing the verifiable unpredictable functions, we further derive a limitation of the black-box barrier by proving the barrier still holds between the given primitive and verifiable unpredictable functions.; Our results not only analyse the properties of verifiable random functions theoretically, but also reveal the limitation of indistinguishability obfuscators in a black-box manner, and show the advantages by adopting non-black-box techniques.},
  archive      = {J_COMJNL},
  author       = {Cao, Shujiao and Xue, Rui},
  doi          = {10.1093/comjnl/bxab023},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1826-1845},
  shortjournal = {Comput. J.},
  title        = {The (Im)Possibility on constructing verifiable random functions},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep neural network for disease detection in rice plant
using the texture and deep features. <em>COMJNL</em>, <em>65</em>(7),
1812–1825. (<a href="https://doi.org/10.1093/comjnl/bxab022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diseases in plants pose a devastating impact on initiating safety in the production of food and they can lead to a reduction in the quantity and quality of agricultural products. In most cases, plant diseases lead to no grain harvest. Thus, an automatic diagnosis of plant disease is highly recommended for determining agricultural information. Several techniques are devised for plant disease detection wherein deep learning is preferred due to its effective performance. Novel deep learning is presented to spot disease from rice crop images. Here, the rice plant image undergoes pre-processing to remove noise and artifacts contained in the image. Then, the segmentation is performed with Segmentation Network (SegNet) to produce segments. The segments are further adapted for extracting statistical features, convolution neural network (CNN) features and texture features. These features are employed for plant disease detection wherein the deep recurrent neural network (Deep RNN) is utilized. The Deep RNN is trained with the proposed RideSpider Water Wave (RSW) algorithm. The proposed RSW is devised by integrating RWW in Spider monkey optimization. The proposed RWS-based Deep RNN provides superior performance with the highest accuracy of 90.5\%, maximal sensitivity of 84.9\% and maximal specificity of 95.2\%.},
  archive      = {J_COMJNL},
  author       = {Daniya, T and Vigneshwari, S},
  doi          = {10.1093/comjnl/bxab022},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1812-1825},
  shortjournal = {Comput. J.},
  title        = {Deep neural network for disease detection in rice plant using the texture and deep features},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimization enabled black hole entropic fuzzy clustering
approach for medical data. <em>COMJNL</em>, <em>65</em>(7), 1795–1811.
(<a href="https://doi.org/10.1093/comjnl/bxab021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical data clustering is an important part of medical decision systems as it refines highly sensitive information from the huge medical datasets. Medical data clustering includes processes, like determine random clusters, set data into specified clusters and handle data clusters dynamically. Hence, handling of medical data streams and clustering remains a challenging issue. This paper proposes a technique, namely Rider-based sunflower optimization (RSFO) for medical data clustering. Initially, the significant features are selected using the Tversky index with holoentropy that is established from the input data. The holo-entropy is utilized to analyze the relationship between the attributes and features. Here, the clustering is done by a Black Hole Entropic Fuzzy Clustering (BHEFC) algorithm, where the optimal cluster centroids are selected by the proposed RSFO algorithm. The proposed RSFO is designed by incorporating the Rider optimization algorithm (ROA) and sunflower optimization (SFO). The effectiveness of the proposed BHEFC+RSFO algorithm is analyzed by the Dermatology Data Set, and the proposed method has the maximal accuracy of 94.480\%, Jaccard coefficient of 94.224\% and Rand coefficient of 91.307\%, respectively.},
  archive      = {J_COMJNL},
  author       = {Rani, A Jaya Mabel and Pravin, A},
  doi          = {10.1093/comjnl/bxab021},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1795-1811},
  shortjournal = {Comput. J.},
  title        = {Optimization enabled black hole entropic fuzzy clustering approach for medical data},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting cyberbullying across social media platforms in
saudi arabia using sentiment analysis: A case study. <em>COMJNL</em>,
<em>65</em>(7), 1787–1794. (<a
href="https://doi.org/10.1093/comjnl/bxab019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twitter has become an open space for the users’ interactions and discussions on diverse trending topics. One of the issues raised on social media platforms is the misunderstanding of ‘freedom of speech’, which in turn, leads us to a new social and behavioral attack: cyberbullying. Cyberbully affects both individuals and societies. Despite tough sanctions globally and locally, cyberbullying is still a serious issue, which needs further consideration. Thus, this research aims to address this issue by proposing a framework, based on sentiment analysis, to detect cyberbullying in the tweets stream. The proposed framework in this paper extracts the tweets from Twitter. Then, the preprocessing through tweets tokenization was applied to remove noise from tweets and also symbols and phrases such as http, emoji faces, hash tag symbols, mention symbols and retweet. After data tokenization, the proposed system classifies the tweets, based on extracted keywords from both experts and potential victims, using deep-learning classification algorithm with 70\% of dataset samples used for the training purpose, and 30\% of the dataset samples used for the testing purpose. The experimental results show the ability of proposed methodology to detect cyberbullying effectively with accuracy 81\%.},
  archive      = {J_COMJNL},
  author       = {Fati, Suliman Mohamed},
  doi          = {10.1093/comjnl/bxab019},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1787-1794},
  shortjournal = {Comput. J.},
  title        = {Detecting cyberbullying across social media platforms in saudi arabia using sentiment analysis: A case study},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new code-based blind signature scheme. <em>COMJNL</em>,
<em>65</em>(7), 1776–1786. (<a
href="https://doi.org/10.1093/comjnl/bxab018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind signature is an important cryptographic primitive with widespread applications in secure e-commerce, for example to guarantee participants’ anonymity. Existing blind signature schemes are mostly based on number-theoretic hard problems, which have been shown to be solvable with quantum computers. The National Institute of Standards and Technology (NIST) began in 2017 to specify a new standard for digital signatures by selecting one or more additional signature algorithms, designed to be secure against attacks carried out using quantum computers. However, none of the third-round candidate algorithms are code-based, despite the potential of code-based signature algorithms in resisting quantum computing attacks. In this paper, we construct a new code-based blind signature (CBBS) scheme as an alternative to traditional number-theoretic based schemes. Specifically, we first extend Santoso and Yamaguchi’s three pass identification scheme to a concatenated version (abbreviated as the CSY scheme). Then, we construct our CBBS scheme from the CSY scheme. The security of our CBBS scheme relies on hardness of the syndrome decoding problem in coding theory, which has been shown to be NP-complete and secure against quantum attacks. Unlike Blazy et al.’s CBBS scheme which is based on a zero-knowledge protocol with cheating probability ; , our CBBS scheme is based on a zero-knowledge protocol with cheating probability ; . The lower cheating probability would reduce the interaction rounds under the same security level and thus leads to a higher efficiency. For example, to achieve security level ; , the signature size in our CBBS scheme is ; MB compared to ; MB in Blazy et al.’s scheme.},
  archive      = {J_COMJNL},
  author       = {Chen, Siyuan and Zeng, Peng and Choo, Kim-Kwang Raymond},
  doi          = {10.1093/comjnl/bxab018},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1776-1786},
  shortjournal = {Comput. J.},
  title        = {A new code-based blind signature scheme},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continuous leakage-amplified public-key encryption with CCA
security. <em>COMJNL</em>, <em>65</em>(7), 1760–1775. (<a
href="https://doi.org/10.1093/comjnl/bxab017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Secret key leakage has become a security threat in computer systems, and it is crucial that cryptographic schemes should resist various leakage attacks, including the continuous leakage attacks. In the literature, some research progresses have been made in designing leakage resistant cryptographic primitives, but there are still some remaining issues unsolved, e.g. the upper bound of the permitted leakage is fixed. In actual applications, the leakage requirements may vary; thus, the leakage parameter with fixed size is not sufficient against various leakage attacks. In this paper, we introduce some novel idea of designing a continuous leakage-amplified public-key encryption scheme with security against chosen-ciphertext attacks. In our construction, the leakage parameter can have an arbitrary length, i.e. the length of the permitted leakage can be flexibly adjusted according to the specific leakage requirements. The security of our proposed scheme is formally proved based on the classic decisional Diffie–Hellman assumption.},
  archive      = {J_COMJNL},
  author       = {Zhang, Wenzheng and Qiao, Zirui and Yang, Bo and Zhou, Yanwei and Zhang, Mingwu},
  doi          = {10.1093/comjnl/bxab017},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1760-1775},
  shortjournal = {Comput. J.},
  title        = {Continuous leakage-amplified public-key encryption with CCA security},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A statistical-based light-weight anomaly detection framework
for wireless body area networks. <em>COMJNL</em>, <em>65</em>(7),
1752–1759. (<a href="https://doi.org/10.1093/comjnl/bxab016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In healthcare scenario, the major challenge in anomaly detection for remote patient monitoring is to classify true medical conditions and false alarms. This paper proposes a light-weight anomaly detection (LWAD) framework for detecting anomalies in remote patient monitoring based on wireless body area networks. The proposed framework uses distance correlation for finding correlated (both linear and non-linear) physiological parameters. It also uses a statistical-based improvised dynamic sliding window algorithm for efficient short-range prediction of physiological parameters. Finally, the proposed LWAD framework detects anomalies using anomaly detection framework based on robust statistical techniques. The validation of LWAD framework is performed using three real time datasets with various statistical measures. The proposed LWAD framework outperforms existing methods.},
  archive      = {J_COMJNL},
  author       = {G S, Smrithy and Balakrishnan, Ramadoss},
  doi          = {10.1093/comjnl/bxab016},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1752-1759},
  shortjournal = {Comput. J.},
  title        = {A statistical-based light-weight anomaly detection framework for wireless body area networks},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A machine learning approach for identification of malignant
mesothelioma etiological factors in an imbalanced dataset.
<em>COMJNL</em>, <em>65</em>(7), 1740–1751. (<a
href="https://doi.org/10.1093/comjnl/bxab015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s world, lung cancer is a significant health burden, and it is one of the most leading causes of death. A leading type of lung cancer is malignant mesothelioma (MM). Most of the MM patients do not show any symptoms. Etiology plays a vital factor in the diagnosis of any disease. Positron emission tomography (PET), magnetic resonance imaging (MRI), biopsies, X-rays and blood tests are essential but costly and invasive MM risk factor identification methods. In this work, we mainly focused on the exploration of the MM risk factors. The identification of mesothelioma symptoms was carried out by utilizing the data of mesothelioma patients. However, the dataset was comprised of both healthy and mesothelioma patients. The dataset is prone to a class imbalance problem in which the number of MM patients significantly less than healthy individuals. To overcome the class imbalance problem, the synthetic minority oversampling technique has been utilized. The association rule mining-based Apriori algorithm has been applied to a preprocessed dataset. Before using the Apriori algorithm, both duplicate and irrelevant attributes were removed. Moreover, the numerical attributes were also classified into nominal attributes and the association rules were generated in the dataset. Our results show that erythrocyte sedimentation rate, asbestos exposure and its duration time, and pleural and serum lactic dehydrogenase ratio are major risk factors of MM. The severe stages of MM can be avoided by earlier identification of risk factors of the disease. The failure of identification of risk factors can lead to increased risk of multiple medical conditions, including cardiovascular diseases, mental distress, diabetes and anemia.},
  archive      = {J_COMJNL},
  author       = {Alam, Talha Mahboob and Shaukat, Kamran and Mahboob, Haris and Sarwar, Muhammad Umer and Iqbal, Farhat and Nasir, Adeel and Hameed, Ibrahim A and Luo, Suhuai},
  doi          = {10.1093/comjnl/bxab015},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1740-1751},
  shortjournal = {Comput. J.},
  title        = {A machine learning approach for identification of malignant mesothelioma etiological factors in an imbalanced dataset},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Differentially private publication for correlated
non-numerical data. <em>COMJNL</em>, <em>65</em>(7), 1726–1739. (<a
href="https://doi.org/10.1093/comjnl/bxab014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential privacy has made a significant progress in numerical data preserving. Compared with numerical data, non-numerical data (e.g. entity object) are also widely applied in intelligent processing tasks. But non-numerical data may reveal more user’s privacy. Recently, researchers attempt to take advantage of the exponential mechanism of differential privacy to solve this challenge. Nonetheless, exponential mechanism has a drawback in correlated data protection, which can not achieve expected privacy degree. To remedy this issue, in this paper, an effective correlated non-numerical data release mechanism is proposed by defining the notion of Correlation-Indistinguishability and designing a correlated exponential mechanism to realize Correlation-Indistinguishability in practice. Inspired by the concept of indistinguishability, Correlation-Indistinguishability can guarantee the correlations of the probability distributions between the output distribution and original data the same to an adversary. In addition, we would rather let two Gaussian white samples pass through a designed filter, to realize the definition of Correlation-Indistinguishability, than using independent exponential variables. Experimental evaluation demonstrates that our mechanism outperforms current schemes in terms of security and utility for frequent items mining.},
  archive      = {J_COMJNL},
  author       = {Wang, Hao and Wang, Huan},
  doi          = {10.1093/comjnl/bxab014},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1726-1739},
  shortjournal = {Comput. J.},
  title        = {Differentially private publication for correlated non-numerical data},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MBSO algorithm for handling energy-throughput trade-off in
cognitive radio networks. <em>COMJNL</em>, <em>65</em>(7), 1717–1725.
(<a href="https://doi.org/10.1093/comjnl/bxab012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive Radio network (CRN) depends on opportunistic spectrum access and spectrum sensing for improving the wireless networks’ spectrum efficiency. Since throughput maximization can result in high-energy consumption, the spectrum sensing technique should address the energy-throughput trade-off. The spectrum sensing time has to be determined by the considering the residual battery energies of each secondary user (SU). The primary user (PU) interference degrades the throughput of the entire network. Hence, the transmit power level should be determined by considering the PU interference and SU battery energy. This paper proposes the multi-objective brain storm optimization (MBSO) algorithm for handling energy-throughput trade-off in CRN. In this work, the sensing time is adaptively determined based on the residual battery energy of SUs, and the transmit power is determined based on the energy level of the PU signal and the residual battery energy of the SUs. A multi-objective optimization problem is formulated in order to maximize the throughput and minimize the packet error rate (PER) and is solved by applying the MBSO algorithm. Experimental results show that MBSO attains improved throughput, higher residual energy with lesser PER. The spectrum sensing performance is enhanced with higher probability of detection.},
  archive      = {J_COMJNL},
  author       = {Ramchandran, M and Ganesh, E N},
  doi          = {10.1093/comjnl/bxab012},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1717-1725},
  shortjournal = {Comput. J.},
  title        = {MBSO algorithm for handling energy-throughput trade-off in cognitive radio networks},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep multi-stage approach for emotional body gesture
recognition in job interview. <em>COMJNL</em>, <em>65</em>(7),
1702–1716. (<a href="https://doi.org/10.1093/comjnl/bxab011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Affective computing is a key research topic in artificial intelligence which is applied to psychology and machines. It consists of the estimation and measurement of human emotions. A person’s body language is one of the most significant sources of information during job interview, and it reflects a deep psychological state that is often missing from other data sources. In our work, we combine two tasks of pose estimation and emotion classification for emotional body gesture recognition to propose a deep multi-stage architecture that is able to deal with both tasks. Our deep pose decoding method detects and tracks the candidate’s skeleton in a video using a combination of depthwise convolutional network and detection-based method for 2D pose reconstruction. Moreover, we propose a representation technique based on the superposition of skeletons to generate for each video sequence a single image synthesizing the different poses of the subject. We call this image: ‘history pose image’, and it is used as input to the convolutional neural network model based on the Visual Geometry Group architecture. We demonstrate the effectiveness of our method in comparison with other methods in the state of the art on the standard Common Object in Context keypoint dataset and Face and Body gesture video database.},
  archive      = {J_COMJNL},
  author       = {Khalifa, Intissar and Ejbali, Ridha and Schettini, Raimondo and Zaied, Mourad},
  doi          = {10.1093/comjnl/bxab011},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1702-1716},
  shortjournal = {Comput. J.},
  title        = {Deep multi-stage approach for emotional body gesture recognition in job interview},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Provably secure online/offline identity-based signature
scheme based on SM9. <em>COMJNL</em>, <em>65</em>(7), 1692–1701. (<a
href="https://doi.org/10.1093/comjnl/bxab009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SM9 is a Chinese cryptography standard, which includes a set of identity-based cryptographic schemes over pairings. SM9 identity-based signature scheme (SM9-IBS) was standardized by ISO/IEC and has been widely used in many real-world applications such as blockchain. Nevertheless, the signing algorithm of SM9-IBS suffers from several heavy calculations (e.g. pairings, scalar multiplications in groups), which might be a bottleneck for lightweight devices such as sensors. In this paper, we modify the SM9-IBS scheme slightly to support fast signing. In order to achieve this, we make the use of online/offline methodology and propose a new online/offline IBS scheme based on SM9. The proposed scheme is proved to be EUF-sID-CMA secure and is about 99\% faster than SM9-IBS in terms of signature generation. Precisely, the time cost of online signing is &lt;1 ms. Our scheme is appropriate for the Internet of Things. The theoretical analysis and demonstration show that the proposed scheme is comparable to existing efficient online/offline IBS schemes.},
  archive      = {J_COMJNL},
  author       = {Lai, Jianchang and Huang, Xinyi and He, Debiao and Wu, Wei},
  doi          = {10.1093/comjnl/bxab009},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1692-1701},
  shortjournal = {Comput. J.},
  title        = {Provably secure Online/Offline identity-based signature scheme based on SM9},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Some combinatorial problems in power-law graphs.
<em>COMJNL</em>, <em>65</em>(7), 1679–1691. (<a
href="https://doi.org/10.1093/comjnl/bxab007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The power-law behavior is ubiquitous in a majority of real-world networks, and it was shown to have a strong effect on various combinatorial, structural and dynamical properties of graphs. For example, it has been shown that in real-life power-law networks, both the matching number and the domination number are relatively smaller, compared with homogeneous graphs. In this paper, we study analytically several combinatorial problems for two power-law graphs with the same number of vertices, edges and the same power exponent. For both graphs, we determine exactly or recursively their matching number, independence number, domination number, the number of maximum matchings, the number of maximum independent sets and the number of minimum dominating sets. We show that power-law behavior itself cannot characterize the combinatorial properties of a heterogenous graph. Since the combinatorial properties studied here have found wide applications in different fields, such as structural controllability of complex networks, our work offers insight in the applications of these combinatorial problems in power-law graphs.},
  archive      = {J_COMJNL},
  author       = {Jiang, Che and Xu, Wanyue and Zhou, Xiaotian and Zhang, Zhongzhi and Kan, Haibin},
  doi          = {10.1093/comjnl/bxab007},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1679-1691},
  shortjournal = {Comput. J.},
  title        = {Some combinatorial problems in power-law graphs},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Battering review spam through ensemble learning in
imbalanced datasets. <em>COMJNL</em>, <em>65</em>(7), 1666–1678. (<a
href="https://doi.org/10.1093/comjnl/bxab006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, people’s buying or availing services decisions are subject to online available reviews/opinions. The authenticity of these reviews/opinions is dubious, as there exist many fake reviews posted to attain monetary benefits by promoting their own or demoting the competitor’s products or services known as review spam. Although the number of spam is relatively less than that of normal reviews in real-life, this class imbalance is a critical concern in review spam detection. The performance degrades when the classifier skew towards the majority class. Moreover, efficient feature selection is essentially needed for this issue. The purpose of this study is to develop a framework based on different effective feature selection along with data balancing techniques. Validation results show that our proposed framework commendably copes up with the review spam issue and a higher precision on the real-life dataset. Further, we tested the sensitivity of our proposed framework using both parametric and non-parametric tests and found it significant.},
  archive      = {J_COMJNL},
  author       = {Khurshid, Faisal and Zhu, Yan and Hu, Jie and Ahmad, Muqeet and Ahmad, Mushtaq},
  doi          = {10.1093/comjnl/bxab006},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1666-1678},
  shortjournal = {Comput. J.},
  title        = {Battering review spam through ensemble learning in imbalanced datasets},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Forward-secure edge authentication for graphs.
<em>COMJNL</em>, <em>65</em>(7), 1653–1665. (<a
href="https://doi.org/10.1093/comjnl/bxab004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The edge authentication of graphs has been studied in the literature because graphs are one of the most widely used data organization structures. The majority of such schemes cannot be used to authenticate general directed graphs (GDGs); other schemes cannot be used for addressing either the issue of dynamic update or the issue of information leakage (such as the existence of nodes/edges and structural relationship of the graph). Also, all the existing schemes do not consider the forward security: if the signer’s secret key has been compromised, all previously generated signatures remain valid. This property provides high-level security protection for authentication schemes. To address these issues, in this work, we propose a forward-secure edge authentication scheme for GDGs. Observe that existing such schemes can only give a proof such that ‘there is an edge between nodes ; and ; ’. Our scheme, however, can directly give a proof such that ‘there is no edge between nodes ; and ; ’, which makes the function of edge authentication schemes more diverse. Moreover, our proposed scheme is proven to be secure against an adaptive chosen-message adversary in the random oracle model. To show its desirable performance, we analyze the computational costs of our scheme and compare it with other related schemes in terms of features.},
  archive      = {J_COMJNL},
  author       = {Zhu, Fei and Yi, Xun and Abuadbba, Alsharif and Khalil, Ibrahim and Nepal, Surya and Huang, Xinyi},
  doi          = {10.1093/comjnl/bxab004},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1653-1665},
  shortjournal = {Comput. J.},
  title        = {Forward-secure edge authentication for graphs},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fidelity homogenous genesis recommendation model for user
trust with item ratings. <em>COMJNL</em>, <em>65</em>(6), 1639–1652. (<a
href="https://doi.org/10.1093/comjnl/bxac045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ever-increasing volume of cloud services has created a service targeting issue. The mechanisms of recommenders address the issue by allowing consumers to easily access services that match their preferences. A recommendation is a regularly utilized function in recommender systems to assist users in swiftly narrowing their choices and making sensible judgments from a large amount of knowledge. In this document, design a “Fidelity Homogenous Genesis Recommendation Model” for user trust along with item ratings. The key for addressing data sparsity is how accurately the likely values of unoccupied cells are estimated. For sparsity reduction of the user-item matrix, we employ a similar prior case rationale technique mixed with average filling. This phase will aid in the later computation of user and item similarity. Genesis: the autonomous map technique was used to clustering the user-item matrix for similar users, followed by an optimization process to generate sub-optimal clusters with a more balanced number of users in each. Based on actual grid computing, the User-Item Privacy Marmalade Technique considers all trustworthy neighbors to be available after optimization. Based on the filtered item set, the trust weighting approach is intended to compute trust similarity among users. To locate trustworthy users, the filtered item set traverses all users in trust networks. In particular, a user trust neighbor set that has comparable preferences and matches with a target user and can be derived through user trust dispersion features in a trusted network. As a result, the proposed algorithm was able to give a novel recommendation model that was guided by user trust as well as item ratings.},
  archive      = {J_COMJNL},
  author       = {Albert, I Edwin and Deepa, A J and Fred, A Lenin},
  doi          = {10.1093/comjnl/bxac045},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1639-1652},
  shortjournal = {Comput. J.},
  title        = {Fidelity homogenous genesis recommendation model for user trust with item ratings},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tourists’ digital footprint: Prediction method of tourism
consumption decision preference. <em>COMJNL</em>, <em>65</em>(6),
1631–1638. (<a href="https://doi.org/10.1093/comjnl/bxab210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital footprints converge into a complex individual and group behavior picture, which truly reflects the user’s choice preference and deep-seated behavior law. However, its application in tourism needs to be further explored. Based on the core characteristics of consumption footprint, this paper applies it to tourism field to analyze tourists’ consumption behavior based on the theory of digital footprint and consumer behavior. This study aims at mining text data, analyzing its characteristics, creating a digital footprint integrated learning model and developing the mining and analysis technology of tourism digital footprints. This method can improve the accuracy of consumer decision-making tendency prediction, and the prediction results can be used to formulate targeted consumption strategies, so as to effectively stimulate consumption vitality and improve consumer satisfaction.},
  archive      = {J_COMJNL},
  author       = {Sun, Qiong and Huang, Xiankai and Liu, Zheng},
  doi          = {10.1093/comjnl/bxab210},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1631-1638},
  shortjournal = {Comput. J.},
  title        = {Tourists’ digital footprint: Prediction method of tourism consumption decision preference},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lung cancer detection and severity level classification
using sine cosine sail fish optimization based generative adversarial
network with CT images. <em>COMJNL</em>, <em>65</em>(6), 1611–1630. (<a
href="https://doi.org/10.1093/comjnl/bxab141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a lung nodule detection mechanism using the proposed sine cosine Sail Fish (SCSF) based generative adversarial network (GAN). However, the proposed SCSF-based GAN is designed by integrating the sine cosine algorithm with the SailFish optimizer, respectively. By using pre-processing, lung nodule segmentation, feature extraction, lung cancer detection, and severity level classification methods detection and classification are performed. The pre-processed computed tomography (CT) image is fed to the lung nodule segmentation phase, where the CT image is segmented into different sub-images to exactly detect the abnormal region. The segmented result after segmentation is fed to the feature extraction phase, where the features like mean, variance, entropy and hole entropy, are extracted from the nodule region. The affected regions are accurately detected using the loss function of the discriminator component. Finally, the lung nodules are detected and classified using the proposed SCSF-based GAN. The proposed approach obtained better performance with the accuracy of 96.925\%, sensitivity of 96.900\% and specificity of 97.920\% for the first-level classification, and the accuracy of 94.987\%, the sensitivity of 94.962\% and specificity of 95.962\% for second-level classification, respectively.},
  archive      = {J_COMJNL},
  author       = {Selvapandian A, Selvapandian A and Prabhu S, Nagendra and Sivakumar P, Sivakumar P and Rao D B, Jagannadha},
  doi          = {10.1093/comjnl/bxab141},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1611-1630},
  shortjournal = {Comput. J.},
  title        = {Lung cancer detection and severity level classification using sine cosine sail fish optimization based generative adversarial network with CT images},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NP-LFA: Non-profiled leakage fingerprint attacks against
improved rotating s-box masking scheme. <em>COMJNL</em>, <em>65</em>(6),
1598–1610. (<a href="https://doi.org/10.1093/comjnl/bxab003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DPA Contest is a world-famous side-channel competition aiming at analyzing and evaluating the implementing security of some latest countermeasures. Improved Rotating S-box Masking Scheme (RSM2.0) is one of the most popular countermeasures designed during DPA Contest V4.2, which arms with both Low Entropy Masking Schemes and shuffling strategy to ensure the software security of AES-128, particularly the non-profiled security. Up to now, conducting high efficient non-profiled attacking scheme with low resource costs is still a challenge. In this paper, we first propose general and non-profiled leakage fingerprint attacks (named NP-LFA) for secret cracking and make use of it to crack RSM2.0 random masks with almost 100\% accuracy. Further, we analyze the hidden vulnerabilities embedded in RSM2.0 implementation, and utilize them to bypass the shuffling defense and perform the master key recovery. Official evaluation results show that NP-LFA is capable of compromising RSM2.0 within 14 traces, each of which only costs 60 ms processing time. Such result validates the high efficiency and light-weighted characteristics of our attacking scheme, which has ranked the first in the official website till now. In addition, we discuss and put forward some possible strategies to mitigate our NP-LFA threats.},
  archive      = {J_COMJNL},
  author       = {Liu, Zeyi and Zhang, Weijuan and Xiang, Ji and Zha, Daren and Wang, Lei},
  doi          = {10.1093/comjnl/bxab003},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1598-1610},
  shortjournal = {Comput. J.},
  title        = {NP-LFA: Non-profiled leakage fingerprint attacks against improved rotating S-box masking scheme},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Metaheuristic-enabled artificial neural network framework
for multimodal biometric recognition with local fusion visual features.
<em>COMJNL</em>, <em>65</em>(6), 1586–1597. (<a
href="https://doi.org/10.1093/comjnl/bxab001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometric systems depending on ‘one-modal biometrics’ do not meet up with the required performance necessities for huge user appliances, owing to certain issues like ‘noisy data, intra-class variations, restricted degrees of freedom, spoof attacks and unacceptable error rates’. This work tends to discover a multimodal biometric recognition (MBR) model that includes three main phases like ‘(i) pre-processing, (ii) segmentation, (iii) feature extraction and (iv) classification’. Initially, the images are pre-processed and those pre-processed images are subjected to segmentation. In this context, segmentation is carried out using the Otsu thresholding model. The segmented images are then subjected to a feature extraction process. This work exploits local feature extraction, where ‘Gabor filter features, Zernibe moment features and proposed local binary pattern features’ are extracted. Subsequently, the fusion framework is developed, which has enhanced classification abilities with minimal dimension for MBR. As the next process, recognition takes place by the optimized neural network (NN) model. As a novelty, the training of NN is carried out using a new modified dragonfly algorithm by selecting the optimal weight. Finally, analysis is carried out for validating the betterment of the presented model in terms of different measures.},
  archive      = {J_COMJNL},
  author       = {Gokulkumari, G},
  doi          = {10.1093/comjnl/bxab001},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1586-1597},
  shortjournal = {Comput. J.},
  title        = {Metaheuristic-enabled artificial neural network framework for multimodal biometric recognition with local fusion visual features},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continual leakage-resilient hedged public-key encryption.
<em>COMJNL</em>, <em>65</em>(6), 1574–1585. (<a
href="https://doi.org/10.1093/comjnl/bxaa204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hedged public-key encryption (HPKE), introduced by Bellare ; . (ASIACRYPT 2009), provides useful security when the per-message randomness fails to be uniform due to faulty implementations or adversarial actions. The HPKE scheme achieves IND-CPA (chosen plaintext attack) security when the randomness they used is of high quality, but, when the randomness is poor quality, rather than breaking completely, it achieves a weaker but a useful notion of security called IND-CDA (chosen distribution attack) as long as the message and randomness together have sufficient min-entropy. However, little research on HPKE in the presence of key leakage was done. In this paper, we study HPKE featuring key leakage-resilience and formulate appropriate security notion for key leakage-resilient HPKE. We work in the continual key leakage model where the secret key is refreshed periodically and an adversary can learn arbitrary but bounded leakage on the secret key between the updates. We present two generic constructions of continual leakage-resilient HPKE in the standard model by using a continual leakage-resilient all-but-one lossy trapdoor function. Finally, we give an instantiation of leakage-resilient HPKE under the linear assumption in bilinear groups.},
  archive      = {J_COMJNL},
  author       = {Huang, Meijuan and Yang, Bo and Zhou, Yanwei and Hu, Xuewei},
  doi          = {10.1093/comjnl/bxaa204},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1574-1585},
  shortjournal = {Comput. J.},
  title        = {Continual leakage-resilient hedged public-key encryption},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). New division property propagation table: Applications to
block ciphers with large s-boxes. <em>COMJNL</em>, <em>65</em>(6),
1560–1573. (<a href="https://doi.org/10.1093/comjnl/bxaa203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The division property method is a technique for automatic searching integral distinguishers on block ciphers. Previous methods only use word-based division property to search integral distinguishers for block ciphers with large S-boxes. Since using bit-based division property may find longer integral distinguishers than word-based division property, we propose a method to automatically search the integral distinguishers based on bit-based division property for block ciphers with large S-boxes. To achieve this goal, we propose a new division property propagation table for S-boxes. Theoretically, we prove that using both the new table and the traditional method to describe the bit-based division property propagation rule of S-box will lead to the same integral distinguishers. Technically, we design a mixed-integer linear programming-based tool to search the integral distinguisher based on the new table, which helps to search new integral distinguishers for block ciphers with large S-boxes efficiently. As a result, we apply our tool to derive new integral distinguishers and get the tight bound on the rounds that no integral distinguishers exist for ICEBERG, KHAZAD, Camellia, CS-Cipher, ITUbee and SMS4. Besides, to show the availability of our integral distinguishers, we form the present best five-round and the first six-round integral attack for ICEBERG as an example.},
  archive      = {J_COMJNL},
  author       = {Hu, Xichao and Li, Yongqiang and Jiao, Lin and Wang, Mingsheng},
  doi          = {10.1093/comjnl/bxaa203},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1560-1573},
  shortjournal = {Comput. J.},
  title        = {New division property propagation table: Applications to block ciphers with large S-boxes},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fog-assisted energy efficient cyber physical system for
panic-based evacuation during disasters. <em>COMJNL</em>,
<em>65</em>(6), 1540–1559. (<a
href="https://doi.org/10.1093/comjnl/bxaa201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disasters around the world have adversely affected every aspect of life and panic-health of stranded persons is one such category. An effective and on-time evacuation from disaster-affected areas can avoid any panic-related health problems of the stranded persons. Although the nature of disasters differ in terms of how they occur, the evacuation of stranded persons faces approximately same set of issues related to the communication, time-sensitive computation and energy efficiency of the devices operated in the disaster-affected areas. In this paper, a cyber physical system (CPS) is proposed that takes into account various challenges of the disaster evacuation, so an efficient on-time and orderly evacuation of stranded panicked persons could be realized. The system employs fog-assisted mobile and UAV devices for time-sensitive computation services, data relaying and energy-aware computation. The system uses a fog-assisted two-factor energy-aware computation approach using data reduction, which enables the energy-efficient data reception and transmission (DRecTrans) operations at the fog nodes and compensates to extend the period for other functionalities. The data reduction at fog devices employs Novel Events Identification (NEI) and Principal Component Analysis (PCA) for detecting consecutive duplicate traffic and data summarization of high dimensional data, respectively. The proposed system operates in two spaces: physical and cyber. Physical space facilitates real-world data acquisition and information sharing with the concerned stakeholders (stranded persons, evacuation teams and medical professionals). The cyber space houses various data-analytics layers and comprises of two subspaces: fog and cloud. The fog space helps in providing real-time panic-health diagnostic and alert services and enables the optimized energy consumption of devices operate in disaster-affected areas, whereas the cloud space facilitates the monitoring and prediction of panic severity of the stranded persons, using a conditional probabilistic model and seasonal auto regression integrated moving average (SARIMA), respectively. Cloud space also facilitates the disaster mapping for converging the evacuation map to the actual situation of the disaster-affected area, and geographical population analysis (GPA) for the identification of the panic severity-based critical regions. The performance evaluation of the proposed CPS acknowledges its Logistic Regression-based panic-well being determination and real-time alert generation efficiency. The simulated implementation of NEI and PCA depicts the fog-assisted energy efficiency of the DRecTrans operations of the fog nodes. The performance evaluation of the proposed CPS also acknowledges the prediction efficiency of the SARIMA and disaster mapping accuracy through GPA. The proposed system also discusses a case study related to the pandemic disaster of coronavirus disease 2019 (COVID-19), where the system can help in panic-based selective testing of the persons, and preventing panic due to distressing period of COVID-19 outbreak.},
  archive      = {J_COMJNL},
  author       = {Sahil, Sahil and Sood, Sandeep Kumar},
  doi          = {10.1093/comjnl/bxaa201},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1540-1559},
  shortjournal = {Comput. J.},
  title        = {Fog-assisted energy efficient cyber physical system for panic-based evacuation during disasters},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computational prediction of cervical cancer diagnosis using
ensemble-based classification algorithm. <em>COMJNL</em>,
<em>65</em>(6), 1527–1539. (<a
href="https://doi.org/10.1093/comjnl/bxaa198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cervical cancer is one of the most common cancers among women in the world. As at the earlier stage, cervical cancer has fewer symptoms. Cancer research is vital as the prognosis of cancer enables clinical applications for patients. In this study, we demonstrate a new approach that applies an ensemble approach to machine learning models for the automatic diagnosis of cervical cancer. The dataset used in the study is the cervical cancer dataset available at the University of California Irvine database repository. Initially, missing values are imputed (k-nearest neighbors) and then the data are balanced (oversampled). Two feature selection approaches are used to extract the most significant features. The proposed stacking architecture, applied for the first time on the cervical cancer dataset, used time elapse of 5.6 s and achieved an area under the curve score of 99.7\% performing better than the methods used in previous works. The objective of the study is to propose a computational model that can predict the diagnosis of cervical cancer efficiently. Further, the proposed learning architecture is gauged with several ensemble approaches like random forest, gradient boosting, voting ensemble and weighted voting ensemble to perceive the enhancement.},
  archive      = {J_COMJNL},
  author       = {Gupta, Surbhi and Gupta, Manoj K},
  doi          = {10.1093/comjnl/bxaa198},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1527-1539},
  shortjournal = {Comput. J.},
  title        = {Computational prediction of cervical cancer diagnosis using ensemble-based classification algorithm},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient algorithms for storage load balancing of
outsourced data in blockchain network. <em>COMJNL</em>, <em>65</em>(6),
1512–1526. (<a href="https://doi.org/10.1093/comjnl/bxaa196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized storage of data is one of the typical applications in the blockchain network. However, most of the existing works neglected the storage balancing problem in the blockchain network, which has an immediate impact on the availability and stability of the network. Therefore, this paper proposes a storage balancing problem for non-local data storage in the blockchain network and proves that the problem is non-deterministic polynomial (NP)-hard. The criterion of the storage balance is established by a balanced coefficient in the proposed scheme. A heuristic matching algorithm (HMA), a genetic algorithm (GA) and a tabu search algorithm (TSA) are customized to solve the problem of imbalanced storage formalized in this paper. Compared with our previous algorithm fast matching algorithm (FMA), experimental results demonstrate that HMA achieves better performance in terms of accuracy, computation overhead and storage overhead. Specifically, the computation overhead of HMA is lower than that of FMA by 84.45\% on average, whereas the storage overhead of HMA is lower than that of FMA by 32.26\% on average. By using the initial solution of HMA, TSA achieves the highest accuracy among GA, TSA and moth-flame optimization (MFO). Meanwhile, by using the initial solution of FMA, TSA achieves the highest accuracy among GA, TSA and MFO.},
  archive      = {J_COMJNL},
  author       = {Liu, Tonglai and Wu, Jigang and Li, Jiaxing and Li, Jingyi and Zhang, Zikai},
  doi          = {10.1093/comjnl/bxaa196},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1512-1526},
  shortjournal = {Comput. J.},
  title        = {Efficient algorithms for storage load balancing of outsourced data in blockchain network},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Binary tree encryption with constant-size public key in the
standard model. <em>COMJNL</em>, <em>65</em>(6), 1489–1511. (<a
href="https://doi.org/10.1093/comjnl/bxaa194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary tree encryption is an intriguing primitive that enables many practical applications to achieve an increasing important security feature, forward security. However, the public key size of existing constructions grows linearly with the depth of the underlying binary tree in the standard model. To support more secret keys associated with nodes, it is often expected that the tree has a sufficiently large depth. This places a burden on employing it implicitly or explicitly in real world. In this work, we show how to compress linear-size public key down to constant-size public key and give our construction featuring constant-size public key in the standard model. We prove that our construction achieves an improved security, adaptive security, under the matrix decision Diffie–Hellman assumption, which is a generalization of standard ; -Lin assumption. Moreover, our key-generation, key-derivation and encryption algorithms have lower time complexity than that of the prior construction, leading to further efficiency improvements. To illustrate these improvements in practice, we give an implementation of our construction and the prior one and then evaluate the performance in the tree depth.},
  archive      = {J_COMJNL},
  author       = {Feng, Shengyuan and Gong, Junqing and Chen, Jie},
  doi          = {10.1093/comjnl/bxaa194},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1489-1511},
  shortjournal = {Comput. J.},
  title        = {Binary tree encryption with constant-size public key in the standard model},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximate string matching with SIMD. <em>COMJNL</em>,
<em>65</em>(6), 1472–1488. (<a
href="https://doi.org/10.1093/comjnl/bxaa193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the ; mismatches version of approximate string matching for a single pattern and multiple patterns. For these problems, we present new algorithms utilizing the single instruction multiple data (SIMD) instruction set extensions for patterns of up to 32 characters. We apply SIMD computation in three ways: in counting of mismatches, in comparison of substrings and in calculation of fingerprints. We show the competitiveness of the new algorithms by practical experiments.},
  archive      = {J_COMJNL},
  author       = {Fiori, Fernando J and Pakalén, Waltteri and Tarhio, Jorma},
  doi          = {10.1093/comjnl/bxaa193},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1472-1488},
  shortjournal = {Comput. J.},
  title        = {Approximate string matching with SIMD},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multispectral palm print and palm vein acquisition platform
and recognition method based on convolutional neural network.
<em>COMJNL</em>, <em>65</em>(6), 1461–1471. (<a
href="https://doi.org/10.1093/comjnl/bxaa190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometrics recognition takes advantage of feature extraction and pattern recognition to analyze the physical and behavioral characteristics of biological individuals to achieve the purpose of individual identification. As a typical biometric technology, palm print and palm vein have the characteristics of high recognition rate, stable features, easy location and good image quality, which have attracted the attention of researchers. This paper designs and develops a multispectral palm print and palm vein acquisition platform, which can quickly acquire palm spectrum and palm vein multispectral images with seven different wavelengths. We propose a multispectral palm print palmar vein recognition framework, and feature-level image fusion is performed after extracting features of palm print palmar vein images at different wavelengths. Through the multispectral palm print palm vein image fusion experiment, a more feasible multispectral palm print and palm vein image fusion scheme is proposed. Based on the results of image fusion, we further propose an improved convolutional neural network (CNN) for model training to achieve identity recognition based on multispectral palm print palm vein images. Finally, the effects of different CNN network structures and learning rates on the recognition results were analyzed and compared experimentally.},
  archive      = {J_COMJNL},
  author       = {Wang, Lei and Zhang, Qiang and Qian, Qing and Wang, Jishuai and Pan, Yujun and Yang, Renbing and Cheng, Wenbo},
  doi          = {10.1093/comjnl/bxaa190},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1461-1471},
  shortjournal = {Comput. J.},
  title        = {Multispectral palm print and palm vein acquisition platform and recognition method based on convolutional neural network},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sensitivity analysis of conflicting goals in the i* goal
model. <em>COMJNL</em>, <em>65</em>(6), 1434–1460. (<a
href="https://doi.org/10.1093/comjnl/bxaa189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Requirements engineering (RE) has been developed as a discipline to identify and then translate stakeholders’ needs into system requirements. Hence, RE is used to produce a set of specifications for developing a software system. The specifications can be applied to satisfy stakeholders and can be implemented, deployed and maintained by using their alternative design options. The past several years have seen significant improvements in RE, whereby the discipline supports the modelling and analysis of stakeholders’ goals (objectives) beyond merely incorporating these goals. Goals further help in deriving functional and non-functional requirements (NFRs) of a system. Goals play an important role in the RE process by helping elaborate the requirements. Goal-oriented requirements engineering (GORE) refers to the use of goals in RE for eliciting requirements. GORE is then used in the process of elaboration, organization, specification, analysis, negotiation, documentation and evolution of the elicited requirements. To model the software system requirements, GORE is implemented by using goals in view of goal models. Stakeholders’ goals are then represented through these goal models to assess their non-functional needs. We developed a technique for analysing conflicting goals of inter-dependent actors in a goal model. In this proposal, to ascertain stakeholders’ NFRs, we applied the cost-effectiveness analysis (CEA) to a multi-objective optimisation model in the ; * goal model. This optimisation model can handle large, sophisticated systems. The requirements analyst can use information derived from the input data. The CEA further facilitates the requirements analyst by including the sensitivity of conflicting goals in the ; * goal model. Based on the inter-dependency relationships, the proposed approach includes the optimisation of each objective function. This approach also uses sensitivity analysis based on the economic evaluation of derived optimal values to prioritize design options. The most cost-effective design option can hence be chosen and used to further the aim of achieving conflicting goals. This proposal uses a Telemedicine System case study, making evaluations through a simulation-based analysis.},
  archive      = {J_COMJNL},
  author       = {Sumesh, Sreenithya and Krishna, Aneesh},
  doi          = {10.1093/comjnl/bxaa189},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1434-1460},
  shortjournal = {Comput. J.},
  title        = {Sensitivity analysis of conflicting goals in the i* goal model},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimization of data assignment for parallel processing in a
hybrid heterogeneous environment using integer linear programming.
<em>COMJNL</em>, <em>65</em>(6), 1412–1433. (<a
href="https://doi.org/10.1093/comjnl/bxaa187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the paper we investigate a practical approach to application of integer linear programming for optimization of data assignment to compute units in a multi-level heterogeneous environment with various compute devices, including CPUs, GPUs and Intel Xeon Phis. The model considers an application that processes a large number of data chunks in parallel on various compute units and takes into account computations, communication including bandwidths and latencies, partitioning, merging, initialization, overhead for computational kernel launch and cleanup. We show that theoretical results from our model are close to real results as differences do not exceed 5\% for larger data sizes, with up to 16.7\% for smaller data sizes. For an exemplary workload based on solving systems of equations of various sizes with various compute-to-communication ratios we demonstrate that using an integer linear programming solver (lp_solve) with timeouts allows to obtain significantly better total (solver+application) run times than runs without timeouts, also significantly better than arbitrary chosen ones. We show that OpenCL 1.2’s device fission allows to obtain better performance in heterogeneous CPU+GPU environments compared to the GPU-only and the default CPU+GPU configuration, where a whole device is assigned for computations leaving no resources for GPU management.},
  archive      = {J_COMJNL},
  author       = {Boiński, Tomasz and Czarnul, Paweł},
  doi          = {10.1093/comjnl/bxaa187},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1412-1433},
  shortjournal = {Comput. J.},
  title        = {Optimization of data assignment for parallel processing in a hybrid heterogeneous environment using integer linear programming},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Role mining heuristics for permission-role-usage cardinality
constraints. <em>COMJNL</em>, <em>65</em>(6), 1386–1411. (<a
href="https://doi.org/10.1093/comjnl/bxaa186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Role-based access control (RBAC) has become a ; standard to control access to restricted resources in complex systems and is widely deployed in many commercially available applications, including operating systems, databases and other softwares. The migration process towards RBAC, starting from the current access configuration, relies on the design of role mining techniques, whose aim is to define suitable roles that implement the given access policies. Some constraints can be used to transform the roles automatically output by the mining procedures and effectively capture the organization’s status under analysis. Such constraints can limit the final configuration characteristics, such as the number of roles assigned to a user, or the number of permissions included in a role, and produce a resulting role set that is effectively usable in real-world situations. In this paper, we consider two constraints: the number of permissions a role can include and the number of roles assigned to any user. In particular, we present two heuristics that produce roles compliant with both constraints and evaluate their performances using both real-world and synthetic datasets.},
  archive      = {J_COMJNL},
  author       = {Blundo, Carlo and Cimato, Stelvio and Siniscalchi, Luisa},
  doi          = {10.1093/comjnl/bxaa186},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1386-1411},
  shortjournal = {Comput. J.},
  title        = {Role mining heuristics for permission-role-usage cardinality constraints},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new blind watermark embedding model: Spiral updated rider
optimization algorithm. <em>COMJNL</em>, <em>65</em>(6), 1365–1385. (<a
href="https://doi.org/10.1093/comjnl/bxaa185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this era, the exchanging of multimedia content has become much comfortable. Since the distribution and the illegal copying of digital multimedia content have become much easier, an author’s intellectual property copyrights have suffered from the violations that led to many damage to their benefits in many applications. This paper intends to propose a novel digital watermarking model for multimedia copyright protection that is based on 3D wavelet transformation. Initially, histogram-based motion frame extraction is conducted on the color video for which keyframes are extracted based on the extracted motion frames. To embed the watermark in the keyframes, 3D wavelet coefficients are selected followed by applying the spread spectrum technique. To conduct precise and effective embedding, it is planned to introduce fine-tuning aspects in the embedding process. As the intensity factor plays a major role in the embedding process, this paper tunes it rather than selecting randomly. Moreover, the Mersenne-Twister algorithm is known for spreading the power spectrum of watermark data to keep invisible. Hence, it is also intended to tune the seed factor of the respective algorithm to further effectuate the spreading process. These tuning processes can be solved as an optimization problem for which this paper proposes a new hybrid optimization algorithm named Spiral Updated Rider Optimization Algorithm (SU-ROA). SU-ROA is a hybrid variant of the Rider Optimization Algorithm (ROA) and Whale Optimization Algorithm (WOA). The performance of the proposed work is compared and proved over the state-of-the-art models in terms of both embedding and extracting effectiveness.},
  archive      = {J_COMJNL},
  author       = {Narasimhulu, C Venkata},
  doi          = {10.1093/comjnl/bxaa185},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1365-1385},
  shortjournal = {Comput. J.},
  title        = {A new blind watermark embedding model: Spiral updated rider optimization algorithm},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A divide &amp; conquer approach to leads-to model checking.
<em>COMJNL</em>, <em>65</em>(6), 1353–1364. (<a
href="https://doi.org/10.1093/comjnl/bxaa183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper proposes a new technique to mitigate the state explosion in model checking. The technique is called a divide &amp; conquer approach to leads-to model checking. As indicated by the name, the technique is dedicated to leads-to properties. It is known that many important systems requirements can be expressed as leads-to properties, thus it is worth focusing on leads-to properties. The technique divides an original leads-to model checking problem into multiple smaller model checking problems and tackles each smaller one. We prove a theorem that the multiple smaller model checking problems are equivalent to the original leads-to model checking problem. We conduct two case studies demonstrating the power of the proposed technique.},
  archive      = {J_COMJNL},
  author       = {Phyo, Yati and Minh Do, Canh and Ogata, Kazuhiro},
  doi          = {10.1093/comjnl/bxaa183},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1353-1364},
  shortjournal = {Comput. J.},
  title        = {A divide &amp; conquer approach to leads-to model checking},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Corrigendum to: Video anomaly detection using the
optimization-enabled deep convolutional neural network. <em>COMJNL</em>,
<em>65</em>(5), 1352. (<a
href="https://doi.org/10.1093/comjnl/bxab037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Philip, Felix M and V, Jayakrishnan and F, Ajesh and P, Haseena},
  doi          = {10.1093/comjnl/bxab037},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1352},
  shortjournal = {Comput. J.},
  title        = {Corrigendum to: Video anomaly detection using the optimization-enabled deep convolutional neural network},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prediction of stock prices using statistical and machine
learning models: A comparative analysis. <em>COMJNL</em>,
<em>65</em>(5), 1338–1351. (<a
href="https://doi.org/10.1093/comjnl/bxab008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of machine learning, numerous approaches have been proposed to forecast stock prices. Various models have been developed to date such as Recurrent Neural Networks, Long Short-Term Memory, Convolutional Neural Network sliding window, etc., but were not accurate enough. Here, the aim is to predict the price of a stock and compare the results obtained using three major algorithms namely Kalman filters, XGBoost and ARIMA. Kalman filters are recursive and use a feedback mechanism to perform error correction. This correction makes them best suited for making accurate predictions as they can factor in the market volatility, whereas XGBoost is a promising technique for datasets that are nonlinear and can gather knowledge by detecting patterns and relationships in the data. XGBoost is also capable of capturing the time dependency of features efficiently. ARIMA refers to an Auto Regressive Integrated Moving Average model that has become very popular in recent times. It is mostly used on time series data and works by eliminating its stationarity. Finally, a hybrid model combining Kalman filters and XGBoostis discussed and a comparison of the results of each of the four models, are made to provide a better clarity for making investments by forecasting the price of a stock.},
  archive      = {J_COMJNL},
  author       = {Prasad, Venkata Vara and Gumparthi, Srinivas and Venkataramana, Lokeswari Y and Srinethe, S and Sruthi Sree, R M and Nishanthi, K},
  doi          = {10.1093/comjnl/bxab008},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1338-1351},
  shortjournal = {Comput. J.},
  title        = {Prediction of stock prices using statistical and machine learning models: A comparative analysis},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Research on reversible visible watermarking algorithms based
on vectorization compression method. <em>COMJNL</em>, <em>65</em>(5),
1320–1337. (<a href="https://doi.org/10.1093/comjnl/bxaa184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In current research on reversible visible watermarking algorithm, the original visible watermark image plays an important auxiliary role, and some algorithms also entirely depend on it to restore host image without any distortion. Therefore, in order to realize semi-blind reversible visible watermarking algorithm, the conventional reversible watermarking algorithm is used to embed compressed visible watermark image data into non-visible-watermarked region of host image. However, the amount of compressed image data obtained by conventional image compression algorithm is relatively large. Therefore, a method based on vectorization compression for the visible watermark image is proposed in this paper. Firstly, it performs edge detection on visible watermark image to obtain a discrete points set ; of vector contour curve. Then, the discrete points in ; are simplified by improved Douglas–Peucker algorithm, after that it obtains compressed vector contour data of visible watermark image. In addition, a reversible visible watermarking algorithm based on convolutional relief and image alpha fusion is proposed, which realizes reversible embedding of visible watermark image and lossless restoration of host image. The experimental results show that the proposed vectorization compression method has more advantages than traditional image compression algorithms, which greatly reduces the storage space of visible watermark image with high fidelity. Additionally, the embedded watermarking image has translucent 3D relief effect, and the fusion of host image and visible watermark image becomes more natural and harmonious.},
  archive      = {J_COMJNL},
  author       = {Qi, Wenfa and Guo, Sirui and Liu, Yuxin and Wang, Xiang and Guo, Zongming},
  doi          = {10.1093/comjnl/bxaa184},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1320-1337},
  shortjournal = {Comput. J.},
  title        = {Research on reversible visible watermarking algorithms based on vectorization compression method},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Area convergence of monoculus robots with additional
capabilities. <em>COMJNL</em>, <em>65</em>(5), 1306–1319. (<a
href="https://doi.org/10.1093/comjnl/bxaa182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the area convergence problem, which requires a group of robots to gather in a small area not defined ; . While it is known that robots can gather at a point if they can precisely measure distances, we, in this paper, show that without any agreement on the coordinate system, it is impossible for robots to converge to an area if they cannot measure distances or angles. We denote these robots without the ability to measure distances or angles as ; . We present a counterexample showing that monoculus robots fail in area convergence even with the capability of measuring angles. However, monoculus robots with a weak notion of distance or minimal agreement on the coordinate system are sufficient to achieve area convergence. In particular, we present area convergence algorithms in asynchronous model for such monoculus robots with one of the two following simple additional capabilities: (1) locality detection (; ), a notion of distance ; (2) orthogonal line agreement (; ), a notion of direction. We discuss extensions corresponding to multiple dimensions and the termination. Additionally, we validate our findings using simulation and show the robustness of our algorithms in the presence of errors in observation or movement.},
  archive      = {J_COMJNL},
  author       = {Pattanayak, Debasish and Mondal, Kaushik and Mandal, Partha Sarathi and Schmid, Stefan},
  doi          = {10.1093/comjnl/bxaa182},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1306-1319},
  shortjournal = {Comput. J.},
  title        = {Area convergence of monoculus robots with additional capabilities},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid learning approach toward situation recognition and
handling. <em>COMJNL</em>, <em>65</em>(5), 1293–1305. (<a
href="https://doi.org/10.1093/comjnl/bxaa179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel hybrid learning approach to gain situation awareness in smart environments by introducing a new situation identifier that combines an expert system and a machine learning approach. Traditionally, expert systems and machine learning approaches have been widely used independently to detect ongoing situations as the main functionality in smart environments in various domains. Expert systems lack the functionality to adapt the system to each user and are expensive to design based on each setting. On the other hand, machine learning approaches fail in the challenge of cold start and making explainable decisions. Using both of these approaches enables the system to use user’s feedback and capture environmental changes while exploiting the initial expert knowledge to solve the mentioned challenges. We use decision trees and situation templates as the core structure to interpret sensor data. To evaluate the proposed method, we generate a new human-annotated dataset simulating a smart environment. Our experiments show superior results compared with the initial expert system and the machine learning approach while preserving the initial expert system’s interpretability.},
  archive      = {J_COMJNL},
  author       = {Rajaby Faghihi, Hossein and Fazli, MohammadAmin and Habibi, Jafar},
  doi          = {10.1093/comjnl/bxaa179},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1293-1305},
  shortjournal = {Comput. J.},
  title        = {Hybrid learning approach toward situation recognition and handling},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Video anomaly detection using the optimization-enabled deep
convolutional neural network. <em>COMJNL</em>, <em>65</em>(5),
1272–1292. (<a href="https://doi.org/10.1093/comjnl/bxaa177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In video surveillance, automatic detection of the anomalies is the active research area in computer technology. Even though various video anomaly detection methods are introduced, detecting anomalous events, such as illegal actions and crimes, is a major challenging issue in video surveillance. Thus, an effective automatic video anomaly detection strategy based on the deep convolutional neural network (deep CNN) is developed in this research. Initially, the input video surveillance is passed into the spatiotemporal feature descriptor, named Histograms of Optical Flow Orientation and Magnitude. The features obtained from the descriptor provide the optical flow details with the aspect of normal patterns from the scene. These patterns are further subjected to the deep CNN, which is trained using the proposed dragonfly-rider optimization algorithm (DragROA) to assure the classification either as an anomalous activity or normal. The proposed DragROA is the combination of the standard dragonfly optimization algorithm and the standard rider optimization algorithm. The implementation of the proposed DragROA-based deep CNN is carried out using two datasets, namely anomaly detection dataset and UMN dataset; the performance is analyzed using the metrics, namely accuracy, sensitivity and specificity. From the analysis, it is depicted that the proposed method obtains the maximum accuracy, sensitivity and specificity of 0.9922, 0.9809 and 1, respectively, for the UCSD dataset.},
  archive      = {J_COMJNL},
  author       = {Philip, Felix M and V, Jayakrishnan and F, Ajesh and P, Haseena},
  doi          = {10.1093/comjnl/bxaa177},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1272-1292},
  shortjournal = {Comput. J.},
  title        = {Video anomaly detection using the optimization-enabled deep convolutional neural network},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid multi-channel EEG filtering method for ocular and
muscular artifact removal based on the 3D spline interpolation
technique. <em>COMJNL</em>, <em>65</em>(5), 1257–1271. (<a
href="https://doi.org/10.1093/comjnl/bxaa175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present work develops a novel hybrid method for ocular and muscular artifact removal from electroencephalography (EEG) signals, EFICA-TQWT. It is a combination of efficient fast independent component analysis (EFICA) method with the tunable Q-factor wavelet transform (TQWT). The main contribution of this paper is to apply the 3D interpolation method in the filtering system. Three EEG datasets are used in this work, two healthy and one epileptic. The choice of subjects for each dataset is made with the help of an expert in physiology. The selection criterion adopted is the presence of muscular and ocular artifacts in the processed recordings. First, a noisy channel automatic classification is performed by the support vector machine (SVM) with radial basis function in order to delete the signal(s) corresponding to the noisiest channel(s) from each EEG recording. The results of the automatic classification by the SVM were compared with those found by the expert’s classification. An accuracy of 97.45\%, a sensitivity of 86.66\% and a 100\% specificity are provided by the SVM classification. The hybrid method of artifact removal will be applied on the rest of the EEG channels of international 10/20 system for each subject. Then, a reconstruction of the eliminated channel signal(s) will be performed in order to obtain a well-filtered signal. The proposed filtering process is evaluated by calculating the mean squared error (MSE) and the signal to noise ratio (SNR). Both for the healthy and pathological EEG datasets, a comparative study of the proposed method (EFICA-TQWT) and other filtering techniques (Fast-ICA, DWT, TQWT and EFICA) is generated. The EFICA-TQWT method gave the best results with a minimum of MSE and a maximum of SNR, more particularly in the case of the application of the 3D interpolation method. Besides, in order to optimize the computing time of the proposed system, a parallel implementation of this filtering system is developed based on graphical processing units using compute unified device architecture.},
  archive      = {J_COMJNL},
  author       = {Abidi, Afef and Nouira, Ibtihel and Assali, Ines and Saafi, Mohamed Ali and Bedoui, Mohamed Hedi},
  doi          = {10.1093/comjnl/bxaa175},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1257-1271},
  shortjournal = {Comput. J.},
  title        = {Hybrid multi-channel EEG filtering method for ocular and muscular artifact removal based on the 3D spline interpolation technique},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust anomaly detection in feature-evolving time series.
<em>COMJNL</em>, <em>65</em>(5), 1242–1256. (<a
href="https://doi.org/10.1093/comjnl/bxaa174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the anomaly detection problem in feature-evolving systems such as server machines, cyber security, financial markets and so forth where in every millisecond, ; -dimensional feature-evolving heterogeneous time series are generated. However, due to stochasticity and uncertainty in evolving heterogeneous time series coupled with temporal dependencies, their anomaly detection are extremely challenging. Furthermore, it is practically impossible to train an anomaly detection model per single time series across millions of metrics, leave alone memory space required to maintain the model and evolving data points in memory for timely processing in feature-evolving data streams. Thus, this paper proposes ; ne sketch ; its all ; lgorithm (OFA), which is a real-time stochastic recurrent deep neural network anomaly detector built on assumption-free probabilistic conditional quantile regression with well-calibrated predictive uncertainty estimates. The proposed framework is capable of detecting anomalies robustly, accurately and efficiently in real time while handling randomness and variabilities in feature-evolving heterogeneous time series. Extensive experiments and rigorous evaluation on large-scale real-world data sets showcase that OFA outperforms other competitive state-of-the-art anomaly detector methods.},
  archive      = {J_COMJNL},
  author       = {Wambura, Stephen and Huang, Jianbin and Li, He},
  doi          = {10.1093/comjnl/bxaa174},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1242-1256},
  shortjournal = {Comput. J.},
  title        = {Robust anomaly detection in feature-evolving time series},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Object detection and localization using sparse-FCM and
optimization-driven deep convolutional neural network. <em>COMJNL</em>,
<em>65</em>(5), 1225–1241. (<a
href="https://doi.org/10.1093/comjnl/bxaa173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection and localization attract the researchers to address the challenges associated with the computer vision. The literature presents numerous unsupervised methods to detect and localize the objects, but with inaccuracies and inconsistencies. The problem is tackled through proposing a novel model based on the optimization algorithm. The object in the image is detected using the Sparse Fuzzy C-Means (Sparse FCM) that is the enhanced Fuzzy C-Means algorithm used to manage the high-dimensional data. The detected objects are subjected to the object localization, which is performed using the proposed Cat Crow Optimization (CCO)-based Deep Convolutional Neural Network. The proposed CCO is the integration of Cat Swarm Optimization Algorithm and Crow Search Algorithm and inherits the advantages of both the optimization algorithms. The experimentation of the proposed method is performed using images obtained from the Visual Object Classes Challenge 2012 dataset. The analysis revealed that the proposed method acquired an average accuracy, precision, and recall of 0.8278, 0.8549, and 0.7911, respectively.},
  archive      = {J_COMJNL},
  author       = {Raghu, A Francis Alexander and Ananth, J P},
  doi          = {10.1093/comjnl/bxaa173},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1225-1241},
  shortjournal = {Comput. J.},
  title        = {Object detection and localization using sparse-FCM and optimization-driven deep convolutional neural network},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Channel estimation for millimeter wave massive MIMO system:
Proposed hybrid optimization with heuristic-enabled precoding and
combining. <em>COMJNL</em>, <em>65</em>(5), 1211–1224. (<a
href="https://doi.org/10.1093/comjnl/bxaa170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multiple-input multiple-output (MIMO), millimeter wave (mmWave) is considered as a promising technology for advanced communication over wireless networks due to its rich frequency spectral resources. However, recognizing the mmWave in MIMO remains a complex task that faces the issues like increased propagation loss. Therefore, this paper proposes a new optimization-assisted estimation algorithm to estimate the mmWave channel parameters. The channel estimation and hybrid precoding performance on mmWave massive MIMO system are proposed by adopting optimization process in the codebook design principles. In fact, the existing works have performed uniform distribution of azimuth angles in the codebook design, whereas the proposed work evaluates it as a single objective optimization problem without excluding the angle characteristics. In order to solve the mentioned optimization problem, dragonfly-evaluated gray wolf optimization (DA-GWO) model is introduced that hybridizes the concepts of dragonfly algorithm and GWO, respectively. Finally, the performance of proposed work is compared and validated over other state-of-the-art models with respect to channel state information and error measures. Accordingly, from the analysis, the proposed DA-GWO model concerning (64, 64) combination for 400th channel bandwidth is 80\% and 95.53\% superior to adaptive channel estimation and projected gradient factorization algorithms.},
  archive      = {J_COMJNL},
  author       = {Srinivasa Rao, Y and Madhu, R},
  doi          = {10.1093/comjnl/bxaa170},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1211-1224},
  shortjournal = {Comput. J.},
  title        = {Channel estimation for millimeter wave massive MIMO system: Proposed hybrid optimization with heuristic-enabled precoding and combining},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhanced context sensitive flash codes. <em>COMJNL</em>,
<em>65</em>(5), 1200–1210. (<a
href="https://doi.org/10.1093/comjnl/bxaa169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A major property of flash memory is that a 0-bit can be changed into a 1-bit, but the symmetric task of switching from a 1-bit to a zero may only be performed in blocks and is therefore often prohibited. This led to the development of rewriting codes using the same storage space more than once, subject to the constraint that 0-bits can be changed into 1-bits, but not vice versa. Context sensitive rewriting codes extend this idea by also incorporating information gathered from surrounding bits. Several new context sensitive rewriting codes are presented and analyzed, some of which are better than the state of the art for sparse input. Empirical simulations show a good match with the theoretical results.},
  archive      = {J_COMJNL},
  author       = {Baruch, Gilad and Klein, Shmuel T and Shapira, Dana},
  doi          = {10.1093/comjnl/bxaa169},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1200-1210},
  shortjournal = {Comput. J.},
  title        = {Enhanced context sensitive flash codes},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A structural evolution-based anomaly detection method for
generalized evolving social networks. <em>COMJNL</em>, <em>65</em>(5),
1189–1199. (<a href="https://doi.org/10.1093/comjnl/bxaa168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, text-based anomaly detection methods have obtained impressive results in social network services, but their applications are limited to social texts provided by users. To propose a method for generalized evolving social networks that have limited structural information, this study proposes a novel structural evolution-based anomaly detection method (; ), which mainly consists of an evolutional state construction algorithm (; ) and an optimized evolutional observation algorithm (; ). ; characterizes the structural evolution of the evolving social network and constructs the evolutional state to represent the macroscopic evolution of the evolving social network. Subsequently, ; reconstructs the quantum-inspired genetic algorithm to discover the optimized observation vector of the evolutional state, which maximally reflects the state change of the evolving social network. Finally, ; combines ; and ; to evaluate the state change degrees and detect anomalous changes to report anomalies. Experimental results on real-world evolving social networks with artificial and real anomalies show that our proposed ; outperforms the state-of-the-art anomaly detection methods.},
  archive      = {J_COMJNL},
  author       = {Wang, Huan and Gao, Qing and Li, Hao and Wang, Hao and Yan, Liping and Liu, Guanghua},
  doi          = {10.1093/comjnl/bxaa168},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1189-1199},
  shortjournal = {Comput. J.},
  title        = {A structural evolution-based anomaly detection method for generalized evolving social networks},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Watson–crick jumping finite automata: Combination,
comparison and closure. <em>COMJNL</em>, <em>65</em>(5), 1178–1188. (<a
href="https://doi.org/10.1093/comjnl/bxaa166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new model of computation called Watson–Crick jumping finite automata was introduced by Mahalingam ; , and the authors study the computing power and closure properties of the variants of the model. There are four variants of the model: no state, 1-limited, all-final and simple Watson–Crick jumping finite automata. In this paper, we introduce a restricted version that is a combination of variants of the existing model. It becomes essential to explore the computing power and closure properties of these combinations. The combination variants are extensively compared with Chomsky hierarchy, general jumping finite automata family and among themselves. We also explore the closure properties of such restricted automata.},
  archive      = {J_COMJNL},
  author       = {Mishra, U K and Mahalingam, K and Rama, R},
  doi          = {10.1093/comjnl/bxaa166},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1178-1188},
  shortjournal = {Comput. J.},
  title        = {Watson–Crick jumping finite automata: Combination, comparison and closure},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Knee muscle force estimating model using machine learning
approach. <em>COMJNL</em>, <em>65</em>(5), 1167–1177. (<a
href="https://doi.org/10.1093/comjnl/bxaa160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various simulation type tools and conventional algorithms are being used to determine knee muscle forces of human during dynamic movement. These all may be good for clinical uses, but have some drawbacks, such as higher computational times, muscle redundancy and less cost-effective solution. Recently, there has been an interest to develop supervised learning-based prediction model for the computationally demanding process. The present research work is used to develop a cost-effective and efficient machine learning (ML) based models to predict knee muscle force for clinical interventions for the given input parameter like height, mass and angle. A dataset of 500 human musculoskeletal, have been trained and tested using four different ML models to predict knee muscle force. This dataset has obtained from anybody modeling software using AnyPyTools, where human musculoskeletal has been utilized to perform squatting movement during inverse dynamic analysis. The result based on the datasets predicts that the random forest ML model outperforms than the other selected models: neural network, generalized linear model, decision tree in terms of mean square error (MSE), coefficient of determination (R; ), and Correlation (r). The MSE of predicted vs actual muscle forces obtained from the random forest model for Biceps Femoris, Rectus Femoris, Vastus Medialis, Vastus Lateralis are 19.92, 9.06, 5.97, 5.46, Correlation are 0.94, 0.92, 0.92, 0.94 and R; are 0.88, 0.84, 0.84 and 0.89 for the test dataset, respectively.},
  archive      = {J_COMJNL},
  author       = {Sohane, Anurag and Agarwal, Ravinder},
  doi          = {10.1093/comjnl/bxaa160},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1167-1177},
  shortjournal = {Comput. J.},
  title        = {Knee muscle force estimating model using machine learning approach},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-driven simulation of elastic OCCI cloud resources.
<em>COMJNL</em>, <em>65</em>(5), 1144–1166. (<a
href="https://doi.org/10.1093/comjnl/bxaa159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying a cloud configuration in a real cloud platform is mostly cost- and time- consuming, as large number of cloud resources have to be rented for the time needed to run the configuration. Thereafter, cloud simulation tools are used as a cheap alternative to test cloud configuration. However, most of the existing cloud simulation tools require extensive technical skills and do not support simulation of any kind of cloud resources. In this context, using a model-driven approach can be helpful as it allows developers to efficiently describe their needs at a high level of abstraction. To do, we propose, in this article, a model-driven engineering approach based on the Open Cloud Computing Interface(OCCI) standard metamodel and CloudSim toolkit. We firstly extend OCCI metamodel for the supporting simulation of any kind of cloud resources. Afterward, to illustrate the extensibility of our approach, we enrich the proposed metamodel by new simulation capabilities. As proof of concept, we study the elasticity and pricing strategies of Amazon Web Services (AWS). This article benefits from ; to design an OCCI simulation extension and to provide a simulation designer for designing cloud configurations to be simulated. We detail the approach process from defining an OCCI simulation extension until the generation and the simulation of the OCCI cloud configurations. Finally, we validate the proposed approach by providing a realistic experimentation to study its usability, the resources coverage rate and the cost. The results are compared with the ones computed from AWS.},
  archive      = {J_COMJNL},
  author       = {Ahmed-Nacer, Mehdi and Kallel, Slim and Zalila, Faiez and Merle, Philippe and Gaaloul, Walid},
  doi          = {10.1093/comjnl/bxaa159},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1144-1166},
  shortjournal = {Comput. J.},
  title        = {Model-driven simulation of elastic OCCI cloud resources},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The component diagnosability of hypercubes with large-scale
faulty nodes. <em>COMJNL</em>, <em>65</em>(5), 1129–1143. (<a
href="https://doi.org/10.1093/comjnl/bxaa155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diagnosability is one of the most important measures of the reliability of networks. Consider the setting where there are large-scale failures that disconnect the network and result in many components. Then, the diagnosability is closely related to the number of components. In this paper, we define and study the ; -component diagnosability of network ; , which is denoted by ; and has not been addressed before. ; is the maximum number of nodes in the faulty node set ; of ; such that ; has at least ; components and diagnosis model can identify all nodes in ; . Under PMC and MM; diagnosis models, we show that, in the hypercube ; , ; when ; . Moreover, we determine the ; -component diagnosability ; for ; .},
  archive      = {J_COMJNL},
  author       = {Zhang, Shurong and Liang, Dongyue and Chen, Lin and Li, Ronghua and Yang, Weihua},
  doi          = {10.1093/comjnl/bxaa155},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1129-1143},
  shortjournal = {Comput. J.},
  title        = {The component diagnosability of hypercubes with large-scale faulty nodes},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep belief network and closed polygonal line for lung
segmentation in chest radiographs. <em>COMJNL</em>, <em>65</em>(5),
1107–1128. (<a href="https://doi.org/10.1093/comjnl/bxaa148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the varying appearance in the upper clavicle bone region, sharp corner at the costophrenic angle, the presence of strong edges at the rib cage and clavicle and the lack of a consistent anatomical shape among different individuals, accurate segmentation of lung on chest radiographs remains challenging. In this work, we propose a novel segmentation method for lung segmentation, containing two subnetworks, where few manually delineated points are used as the approximate initialization. The first one is a preprocessing subnetwork based on a deep learning model (i.e. Deep Belief Network and K-Nearest Neighbor). The second one is a refinement subnetwork, designed to make the preprocessed result to be optimized by combining an improved principal curve method and a machine learning method. To prove the performance of the proposed method, several public datasets were evaluated with Dice Similarity Coefficient (; ), overlap score (Ω), Sensitivity (; ), Positive Predictive Value (; ), global Error (; ) and execution time (; ). Compared with state-of-the-art methods, our method reaches superior segmentation performance.},
  archive      = {J_COMJNL},
  author       = {Peng, Tao and Xu, Thomas Canhao and Wang, Yihuai and Li, Fanzhang},
  doi          = {10.1093/comjnl/bxaa148},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1107-1128},
  shortjournal = {Comput. J.},
  title        = {Deep belief network and closed polygonal line for lung segmentation in chest radiographs},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effectiveness of adopting cloud-based e-learning at qassim
university. <em>COMJNL</em>, <em>65</em>(5), 1098–1106. (<a
href="https://doi.org/10.1093/comjnl/bxaa146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth of cloud computing (CC) is noticeable in Saudi Arabia, especially in educational institutions. To have an effective E-learning platform, CC is widely used due to its capabilities. This research studying the factors affects the adoption of cloud-based E-learning at Qassim University (QU) from the student’s perspective. A model proposed to measure the effectiveness of the current E-learning system at QU and to identify the significant factors required to encourage students to keep using it. The proposed model includes the theory of motivation, the theory of technology acceptance model and characteristics of CC. Data collected from 114 students analyzed using SmartPLS. Results show the perceived ease of use and extrinsic motivation are significant factors that means have high effects on the intention to use (ITU). While other factors such as availability, collaboration and intrinsic motivation are insignificant that have less or no effect on the ITU cloud-based E-learning.},
  archive      = {J_COMJNL},
  author       = {Alajlan, Norah and Hadwan, Mohammed and Ibrahim, Dina M},
  doi          = {10.1093/comjnl/bxaa146},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1098-1106},
  shortjournal = {Comput. J.},
  title        = {Effectiveness of adopting cloud-based E-learning at qassim university},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel approach for determining meal plan for gestational
diabetes mellitus using artificial intelligence. <em>COMJNL</em>,
<em>65</em>(5), 1088–1097. (<a
href="https://doi.org/10.1093/comjnl/bxaa145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating energy expenditure and meal plan plays important roles in the treatment of gestational diabetes mellitus, which is defined as any degree of glucose intolerance with onset or first recognition during pregnancy. Some approaches have been proposed; however, they have limitations including high cost, relative complexity, trained personnel requirements or locality. In this study, we propose an approach for estimating the energy expenditure and meal plan by using artificial intelligence. The proposed approach consists of three main stages including energy expenditure estimation, macronutrient intake estimation and meal plan determination. The neural network is used to estimate the energy expenditure, and then the meal plan is determined by using the genetic algorithm (GA), which is a popular method for solving optimization problems based on natural selection and genetics. The fitness function with penalty was used in GA to deal with constraint problems. The proposed method can obtain the root mean square error and mean absolute percentage error of 15.23 ± 7.4 kcal and 1 ± 0.8\%, respectively. The Pearson correlation coefficient, which measures the strength of the association between the two measurements, was 0.99. In meal plan determination, the results from GA agreed with those from nutritionists. The Pearson correlation coefficient was 0.95. The energy expenditure and meal plan are determined by soft computing with flexible ways. They can adapt to particular regions or group of patients.},
  archive      = {J_COMJNL},
  author       = {Huynh, Hieu Trung and Hoang, Tran Minh},
  doi          = {10.1093/comjnl/bxaa145},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1088-1097},
  shortjournal = {Comput. J.},
  title        = {A novel approach for determining meal plan for gestational diabetes mellitus using artificial intelligence},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Compression with wildcards: From CNFs to orthogonal DNFs by
imposing the clauses one-by-one. <em>COMJNL</em>, <em>65</em>(5),
1073–1087. (<a href="https://doi.org/10.1093/comjnl/bxaa142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel technique for converting a Boolean conjunctive normal form (CNF) into an orthogonal disjunctive normal form (DNF), aka exclusive sum of products. Our method (which will be pitted against a hardwired command from Mathematica) zooms in on the models of the CNF by imposing its clauses one by one. Clausal imposition invites parallelization, and wildcards beyond the common don’t-care symbol compress the output. The method is most efficient for few but large clauses. Generalizing clauses, one can in fact impose superclauses. By definition, superclauses are obtained from clauses by substituting each positive literal by an arbitrary conjunction of positive literals.},
  archive      = {J_COMJNL},
  author       = {Wild, Marcel},
  doi          = {10.1093/comjnl/bxaa142},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1073-1087},
  shortjournal = {Comput. J.},
  title        = {Compression with wildcards: From CNFs to orthogonal DNFs by imposing the clauses one-by-one},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Text summarization as a multi-objective optimization task:
Applying harmony search to extractive multi-document summarization.
<em>COMJNL</em>, <em>65</em>(5), 1053–1072. (<a
href="https://doi.org/10.1093/comjnl/bxaa139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, automated extractive text summarization is one of the most common techniques for organizing information. In extractive summarization, the most appropriate sentences are selected from the text and build a representative summary. Therefore, probing for the best sentences is a fundamental task.; This paper has coped with extractive summarization as a multi-objective optimization problem and proposed a language-independent, semantic-aware approach that applies the harmony search algorithm to generate appropriate multi-document summaries. It learns the objective function from an extra set of reference summaries and then generates the best summaries according to the trained function. The system also performs some supplementary activities for better achievements. It expands the sentences by using an inventive approach that aims at tuning conceptual densities in the sentences towards important topics. Furthermore, we introduced an innovative clustering method for identifying important topics and reducing redundancies. A sentence placement policy based on the Hamiltonian shortest path was introduced for producing readable summaries.; The experiments were conducted on DUC2002, DUC2006 and DUC2007 datasets. Experimental results showed that the proposed framework could assist the summarization process and yield better performance. Also, it was able to generally outperform other cited summarizer systems.},
  archive      = {J_COMJNL},
  author       = {Bidoki, M and Fakhrahmad, M and Moosavi, M R},
  doi          = {10.1093/comjnl/bxaa139},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1053-1072},
  shortjournal = {Comput. J.},
  title        = {Text summarization as a multi-objective optimization task: Applying harmony search to extractive multi-document summarization},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-objective randomly updated beetle swarm and
multi-verse optimization for brain tumor segmentation and
classification. <em>COMJNL</em>, <em>65</em>(4), 1029–1052. (<a
href="https://doi.org/10.1093/comjnl/bxab171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper plans to develop the optimal brain tumor classification model with diverse intelligent methods. The main phases of the proposed model are ‘(a) image pre-processing, (b) skull stripping, (c) tumor segmentation, (d) feature extraction and (e) classification’. At first, pre-processing of the image is performed by converting the image from red green blue to gray followed by median filtering. Further, skull stripping is done for removing the extra-meningeal tissue from the head image, which is done by Otsu thresholding. As the main contribution, the tumor segmentation is done by the optimized threshold-based tumor segmentation using multi-objective randomly updated beetle swarm and multi-verse optimization (RBS-MVO). The objective constraints considered for the segmentation of the tumor is entropy and variance. Next, the feature extraction techniques like gray level co-occurrence matrix, local binary pattern and gray-level run length matrix is accomplished to extract the set of features. The classification side uses the combination of neural network (NN) and deep learning model called convolutional neural network (CNN) for tumor classification. The extracted features are subjected to NN, and the segmented image is taken as input to CNN. In addition, the weight function of NN and hidden neurons of CNN is optimized by the RBS-MVO.},
  archive      = {J_COMJNL},
  author       = {Kumar, Katukuri Arun and Boda, Ravi},
  doi          = {10.1093/comjnl/bxab171},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {1029-1052},
  shortjournal = {Comput. J.},
  title        = {A multi-objective randomly updated beetle swarm and multi-verse optimization for brain tumor segmentation and classification},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward both privacy and efficiency of homomorphic MACs for
polynomial functions and its applications. <em>COMJNL</em>,
<em>65</em>(4), 1020–1028. (<a
href="https://doi.org/10.1093/comjnl/bxab042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Homomorphic message authentication codes (MACs) allow a user to outsource data to an untrusted server and verify the correctness of returned computation results over the outsourced data. Many cloud applications need delegation computations over outsourced data with dual capabilities. On one hand, they need to keep the outsourced data secret such that the server cannot trace and infer any sensitive information from the computation results. On the other hand, the user should be able to efficiently verify the computation results. Unfortunately, the state-of-the-art homomorphic MAC schemes are not so desirable due to either poor privacy or low verification efficiency. In this paper, we first put forward a new cryptographic primitive called ; that simultaneously provides data privacy and efficient verification. Then, we present a PHMAC construction capable for the evaluation of polynomials of fixed degree ; , in which the tag does not reveal any information of underlying authenticated data while being verifiable in constant time (in an amortized sense). As an application, we give a generic construction of homomorphic authenticated encryption (HAE) from proposed PHMAC and homomorphic encryption. Benefited from the functionalities of underlying PHMAC scheme, the derived HAE enjoys stronger authenticity and supports larger classes of functions than that of Lai ; (Verifiable Computation on Outsourced Encrypted Data. In ; , Wroclaw, Poland, September 7–11, Part I, pp. 273–291. Springer, Berlin). Such HAE enables verifiable delegation computations over growing outsourced encrypted data in an efficient way.},
  archive      = {J_COMJNL},
  author       = {Li, Shimin and Wang, Xin and Xue, Rui},
  doi          = {10.1093/comjnl/bxab042},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {1020-1028},
  shortjournal = {Comput. J.},
  title        = {Toward both privacy and efficiency of homomorphic MACs for polynomial functions and its applications},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Salient object detection based on multiscale segmentation
and fuzzy broad learning. <em>COMJNL</em>, <em>65</em>(4), 1006–1019.
(<a href="https://doi.org/10.1093/comjnl/bxaa158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Saliency detection has been a hot topic in the field of computer vision. In this paper, we propose a novel approach that is based on multiscale segmentation and fuzzy broad learning. The core idea of our method is to segment the image into different scales, and then the extracted features are fed to the fuzzy broad learning system (FBLS) for training. More specifically, it first segments the image into superpixel blocks at different scales based on the simple linear iterative clustering algorithm. Then, it uses the local binary pattern algorithm to extract texture features and computes the average color information for each superpixel of these segmentation images. These extracted features are then fed to the FBLS to obtain multiscale saliency maps. After that, it fuses these saliency maps into an initial saliency map and uses the label propagation algorithm to further optimize it, obtaining the final saliency map. We have conducted experiments based on several benchmark datasets. The results show that our solution can outperform several existing algorithms. Particularly, our method is significantly faster than most of deep learning-based saliency detection algorithms, in terms of training and inferring time.},
  archive      = {J_COMJNL},
  author       = {Lin, Xiao and Wang, Zhi-Jie and Ma, Lizhuang and Li, Renjie and Fang, Mei-E},
  doi          = {10.1093/comjnl/bxaa158},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {1006-1019},
  shortjournal = {Comput. J.},
  title        = {Salient object detection based on multiscale segmentation and fuzzy broad learning},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A trust framework utilization in cloud computing environment
based on multi-criteria decision-making methods. <em>COMJNL</em>,
<em>65</em>(4), 997–1005. (<a
href="https://doi.org/10.1093/comjnl/bxaa138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing (CC) is a complete online system of specialized technical resources for data computing and storage management. Many organizations, including small and start-ups, have recommended using CC technology rather than building their own high-costed data centers. Due to the diversity of cloud service providers, identifying appropriate cloud services that can meet users’ and organizations’ requirements, including trust, is a major challenge. In this paper, we propose a trust framework utilization to evaluate cloud services trust using multi-criteria decision-making and fuzzy logic techniques from different perspectives based on performance, agility, finance, security and usability criteria. The comparison of obtained results with related ones shows that ours are efficient and promising decisions for cloud users and organizations as well.},
  archive      = {J_COMJNL},
  author       = {Trabay, Doaa Wagdy and El-Henawy, Ibrahim and Gharibi, Wajeb},
  doi          = {10.1093/comjnl/bxaa138},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {997-1005},
  shortjournal = {Comput. J.},
  title        = {A trust framework utilization in cloud computing environment based on multi-criteria decision-making methods},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust detection of copy-move forgery based on wavelet
decomposition and firefly algorithm. <em>COMJNL</em>, <em>65</em>(4),
983–996. (<a href="https://doi.org/10.1093/comjnl/bxaa137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the whole world is covered with the revolution of digital information so that digital data are easy to transfer, access and process. Modern image processing tools are used to create forged images; by using these tools, forgers introduced forged region in the original image to spread the rumor in the public. Now there is a challenge for the forensic department to prove the authenticity of the original image. So there is required a new efficient copy-move tamper detection method. To fulfill this requirement, a new computational efficient method for copy-move forgery detection using firefly algorithm is proposed.},
  archive      = {J_COMJNL},
  author       = {Kashyap, Abhishek and Suresh, B and Gupta, Hariom},
  doi          = {10.1093/comjnl/bxaa137},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {983-996},
  shortjournal = {Comput. J.},
  title        = {Robust detection of copy-move forgery based on wavelet decomposition and firefly algorithm},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fully retroactive minimum spanning tree problem.
<em>COMJNL</em>, <em>65</em>(4), 973–982. (<a
href="https://doi.org/10.1093/comjnl/bxaa135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article describes an algorithm that solves a fully dynamic variant of the minimum spanning tree (MST) problem. The fully retroactive MST allows adding an edge to time ; , or to obtain the current MST at time ; . By using the square root technique and a data structure link-cut tree, it was possible to obtain an algorithm that runs each query in ; amortized, in which ; is the number of nodes in graph ; and ; is the size of the timeline. We use a different approach to solve the MST problem instead of the standard algorithms, such as Prim or Kruskal, and this allows using the square root technique to improve the final complexity of the algorithm. Our empirical analysis shows that the proposed algorithm runs faster than re-executing the standard algorithms, and this difference only increases when the number of nodes in these graphs is larger.},
  archive      = {J_COMJNL},
  author       = {de Andrade Júnior, José Wagner and Duarte Seabra, Rodrigo},
  doi          = {10.1093/comjnl/bxaa135},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {973-982},
  shortjournal = {Comput. J.},
  title        = {Fully retroactive minimum spanning tree problem},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning in age-invariant face recognition: A
comparative study. <em>COMJNL</em>, <em>65</em>(4), 940–972. (<a
href="https://doi.org/10.1093/comjnl/bxaa134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents comparative evaluation of an application of deep convolutional neural networks (dCNNs) to age invariant face recognition. To this end, we use four distinct dCNN models, the AlexNet, VGGNet, GoogLeNet and ResNet. We assess their performance to recognize face images across aging variations, firstly by fine-tuning the models and secondly using them as face feature extractor. We also suggest a novel synthesized aging augmentation technique suitable for age-invariant face recognition using dCNNs. The face recognition experiments are conducted on three challenging FG-NET, MORPH and LAG aging datasets, and results are benchmarked with a simple CNN. The comparative study allows us to answer (i) when and why transfer learning or feature extraction strategies are useful in age-invariant face recognition scenarios, (ii) the potential of aging synthesized augmentation to increase accuracy and (iii) the choice of appropriate feature normalization and distance metrics to be used with deeply learned features. The extensive experiments, and valuable insights presented in this study can be extended to the design of effective age-invariant face recognition algorithms.},
  archive      = {J_COMJNL},
  author       = {Sajid, Muhammad and Ali, Nouman and Ratyal, Naeem Iqbal and Usman, Muhammad and Butt, Faisal Mehmood and Riaz, Imran and Musaddiq, Usman and Aziz Baig, Mirza Jabbar and Baig, Shahbaz and Ahmad Salaria, Umair},
  doi          = {10.1093/comjnl/bxaa134},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {940-972},
  shortjournal = {Comput. J.},
  title        = {Deep learning in age-invariant face recognition: A comparative study},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FLAKE: Fuzzy graph centrality-based automatic keyword
extraction. <em>COMJNL</em>, <em>65</em>(4), 926–939. (<a
href="https://doi.org/10.1093/comjnl/bxaa133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keyword extraction is one of the most important aspects of text mining. Keywords help in identifying the document context. Many researchers have contributed their work to keyword extraction. They proposed approaches based on the frequency of occurrence, the position of words or the similarity between two terms. However, these approaches have shown shortcomings. In this paper, we propose a method that tries to overcome some of these shortcomings and present a new algorithm whose efficiency has been evaluated against widely used benchmarks. It is found from the analysis of standard datasets that the position of word in the document plays an important role in the identification of keywords. In this paper, a fuzzy logic-based automatic keyword extraction (FLAKE) method is proposed. FLAKE assigns weights to the keywords by considering the relative position of each word in the entire document as well as in the sentence coupled with the total occurrences of that word in the document. Based on the above data, candidate keywords are selected. Using WordNet, a fuzzy graph is constructed whose nodes represent candidate keywords. At this point, the most important nodes (based on fuzzy graph centrality measures) are identified. Those important nodes are selected as final keywords. The experiments conducted on various datasets show that proposed approach outperforms other keyword extraction methodologies by enhancing precision and recall.},
  archive      = {J_COMJNL},
  author       = {Jain, Amita and Mittal, Kanika and Vaisla, Kunwar Singh},
  doi          = {10.1093/comjnl/bxaa133},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {926-939},
  shortjournal = {Comput. J.},
  title        = {FLAKE: Fuzzy graph centrality-based automatic keyword extraction},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic analysis to identify students’ feedback.
<em>COMJNL</em>, <em>65</em>(4), 918–925. (<a
href="https://doi.org/10.1093/comjnl/bxaa130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, an automated analysis is performed on students’ chat and text data generated by social media platforms over the course of one semester and thoroughly analyzed for potential feedback about teaching, exams, and course contents. A data crawler is developed that performs horizontal and vertical samplings of the data. After data crawling, a few preprocessing steps are performed including text extraction, noise removal, stop-word removal, word stemming, text classification, and feature extraction. The intensity of a review is determined using four measures containing knowledge and understanding, course contents, teaching style, and assessment procedures for a specific course. The proposed system contains features from text mining and web mining to automatically identify a review whenever a user writes comments on their studies. This system aims to provide curriculum development committees with valuable online student feedback and assist in curriculum improvements. By comparing these automated reviews to results obtained from manual student survey forms, we found that the automated system yields the same output but at a fraction of the cost and time typically spent on collecting and analyzing manual student surveys.},
  archive      = {J_COMJNL},
  author       = {Masood, Khalid and Khan, Muhammad Adnan and Saeed, Usman and Al Ghamdi, Mohammed A and Asif, Muhammad and Arfan, Muhammad},
  doi          = {10.1093/comjnl/bxaa130},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {918-925},
  shortjournal = {Comput. J.},
  title        = {Semantic analysis to identify students’ feedback},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Smart affect recognition system for real-time biometric
surveillance using hybrid features and multilayered binary structured
support vector machine. <em>COMJNL</em>, <em>65</em>(4), 897–917. (<a
href="https://doi.org/10.1093/comjnl/bxaa125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human affect recognition (HAR) using images of facial expression and electrocardiogram (ECG) signal plays an important role in predicting human intention. This system improves the performance of the system in applications like the security system, learning technologies and health care systems. The primary goal of our work is to recognize individual affect states automatically using the multilayered binary structured support vector machine (MBSVM), which efficiently classify the input into one of the four affect classes, relax, happy, sad and angry. The classification is performed efficiently by designing an efficient support vector machine (SVM) classifier in multilayer mode operation. The classifier is trained using the 8-fold cross-validation method, which improves the learning of the classifier, thus increasing its efficiency. The classification and recognition accuracy is enhanced and also overcomes the drawback of ‘facial mimicry’ by using hybrid features that are extracted from both facial images (visual elements) and physiological signal ECG (signal features). The reliability of the input database is improved by acquiring the face images and ECG signals experimentally and by inducing emotions through image stimuli. The performance of the affect recognition system is evaluated using the confusion matrix, obtaining the classification accuracy of 96.88\%.},
  archive      = {J_COMJNL},
  author       = {W, Thamba Meshach and S, Hemajothi and E A, Mary Anita},
  doi          = {10.1093/comjnl/bxaa125},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {897-917},
  shortjournal = {Comput. J.},
  title        = {Smart affect recognition system for real-time biometric surveillance using hybrid features and multilayered binary structured support vector machine},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis on the component connectivity of enhanced
hypercubes. <em>COMJNL</em>, <em>65</em>(4), 890–896. (<a
href="https://doi.org/10.1093/comjnl/bxaa122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliability evaluation of interconnection networks is of significant importance to the design and maintenance of interconnection networks. The component connectivity is an important parameter for the reliability evaluation of interconnection networks and is a generalization of the traditional connectivity. The ; -component connectivity ; of a non-complete connected graph ; is the minimum number of vertices whose deletion results in a graph with at least ; components. Determining the ; -component connectivity is still an unsolved problem in many interconnection networks. Let ; (; ) denote the ; -enhanced hypercube. In this paper, let ; and ; , we determine ; for ; . The previous result in Zhao and Yang (2019, Conditional connectivity of folded hypercubes. ; , ; , 388–392) is extended.},
  archive      = {J_COMJNL},
  author       = {Xu, Liqiong and Guo, Litao},
  doi          = {10.1093/comjnl/bxaa122},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {890-896},
  shortjournal = {Comput. J.},
  title        = {Analysis on the component connectivity of enhanced hypercubes},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cost- and time-based data deployment for improving
scheduling efficiency in distributed clouds. <em>COMJNL</em>,
<em>65</em>(4), 874–889. (<a
href="https://doi.org/10.1093/comjnl/bxaa121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, with the continuous development of internet of things and cloud computing technologies, data intensive applications have gotten more and more attention. In the distributed cloud environment, the access of massive data is often the bottleneck of its performance. It is very significant to propose a suitable data deployment algorithm for improving the utilization of cloud server and the efficiency of task scheduling. In order to reduce data access cost and data deployment time, an optimal data deployment algorithm is proposed in this paper. By modeling and analyzing the data deployment problem, the problem is solved by using the improved genetic algorithm. After the data are well deployed, aiming at improving the efficiency of task scheduling, a task progress aware scheduling algorithm is proposed in this paper in order to make the speculative execution mechanism more accurate. Firstly, the threshold to detect the slow tasks and fast nodes are set. Then, the slow tasks and fast nodes are detected by calculating the remaining time of the tasks and the real-time processing ability of the nodes, respectively. Finally, the backup execution of the slow tasks is performed on the fast nodes. While satisfying the load balancing of the system, the experimental results show that the proposed algorithms can obviously reduce data access cost, service-level agreement (SLA) default rate and the execution time of the system and optimize data deployment for improving scheduling efficiency in distributed clouds.},
  archive      = {J_COMJNL},
  author       = {Li, Chunlin and Zhang, Yihan and Qu, Xiaomei and Luo, Youlong},
  doi          = {10.1093/comjnl/bxaa121},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {874-889},
  shortjournal = {Comput. J.},
  title        = {Cost- and time-based data deployment for improving scheduling efficiency in distributed clouds},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Querying tenuous group in attributed networks.
<em>COMJNL</em>, <em>65</em>(4), 858–873. (<a
href="https://doi.org/10.1093/comjnl/bxaa115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding groups in networks is very common in many practical applications, and most work mainly focus on dense groups. However, in scenarios like reviewer selection or weak social friends recommendation, we need to emphasize the privacy of individuals or minimize the possibility of information dissemination. So the internal relationship between individuals should be as tenuous as possible, but existing works cannot suit well to the requirement. Some works have focused on finding tenuous groups. However, these works only aim to find the most tenuous group and do not consider containing certain vertices. In this paper, we study the problem of finding tenuous groups in attributed networks that contain specific vertices. We first propose a new problem called Tenuous Attributed Group Query, and a new indicator, k-tenuity, to measure the structural tenuity of a group. Then we propose a method TAG-Basic to find proper groups by gradually selecting the vertices with optimal influence. We further design an advanced method TAG-ADV to improve the efficiency by forming a candidate set before selecting the optimal vertex. Experiment results show that k-tenuity is more effective than other state-of-the-art measurements, and our methods obtain the best result on group quality compared with other benchmark methods.},
  archive      = {J_COMJNL},
  author       = {Li, Yang and Sun, Heli and He, Liang and Huang, Jianbin and Chen, Jiyin and He, Hui and Jia, Xiaolin},
  doi          = {10.1093/comjnl/bxaa115},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {858-873},
  shortjournal = {Comput. J.},
  title        = {Querying tenuous group in attributed networks},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Redis-based messaging queue and cache-enabled parallel
processing social media analytics framework. <em>COMJNL</em>,
<em>65</em>(4), 843–857. (<a
href="https://doi.org/10.1093/comjnl/bxaa114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extensive usage of social media polarity analysis claims the need for real-time analytics and runtime outcomes on dashboards. In data analytics, only 30\% of the time is consumed in modeling and evaluation stages and 70\% is consumed in data engineering tasks. There are lots of machine learning algorithms to achieve a desirable outcome in prediction points of view, but they lack in handling data and their transformation so-called data engineering tasks, and reducing its time remained still challenging. The contribution of this research paper is to encounter the mentioned challenges by presenting a parallelly, scalable, effective, responsive and fault-tolerant framework to perform end-to-end data analytics tasks in real-time and batch-processing manner. An experimental analysis on Twitter posts supported the claims and signifies the benefits of parallelism of data processing units. This research has highlighted the importance of processing mentioned URLs and embedded images along with post content to boost the prediction efficiency. Furthermore, this research additionally provided a comparison of naive Bayes, support vector machines, extreme gradient boosting and long short-term memory (LSTM) machine learning techniques for sentiment analysis on Twitter posts and concluded LSTM as the most effective technique in this regard.},
  archive      = {J_COMJNL},
  author       = {Singh, Ravindra Kumar and Verma, Harsh Kumar},
  doi          = {10.1093/comjnl/bxaa114},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {843-857},
  shortjournal = {Comput. J.},
  title        = {Redis-based messaging queue and cache-enabled parallel processing social media analytics framework},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A machine learning-based model to evaluate readability and
assess grade level for the web pages. <em>COMJNL</em>, <em>65</em>(4),
831–842. (<a href="https://doi.org/10.1093/comjnl/bxaa113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating readability of web documents has gained attention due to several factors such as improving the effectiveness of writing and to reach a wider spectrum of audience. Current practices in this direction follow several statistical measures in evaluating readability of the document. In this paper, we have proposed a machine learning-based model to compute readability of web pages. The minimum educational standards required (grade level) to understand the contents of a web page are also computed. The proposed model classifies the web pages into highly readable, readable or less readable using specified feature set. To classify a web page with the aforementioned categories, we have incorporated the features such as sentence count, word count, syllable count, type-token ratio and lexical ambiguity. To increase the usability of the proposed model, we have developed an accessible browser extension to perform the assessments of every web page loaded into the browser.},
  archive      = {J_COMJNL},
  author       = {Pantula, Muralidhar and Kuppusamy, K S},
  doi          = {10.1093/comjnl/bxaa113},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {831-842},
  shortjournal = {Comput. J.},
  title        = {A machine learning-based model to evaluate readability and assess grade level for the web pages},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly supervised sentiment-specific region discovery for
VSA. <em>COMJNL</em>, <em>65</em>(4), 818–830. (<a
href="https://doi.org/10.1093/comjnl/bxaa112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local information has significant contributions to visual sentiment analysis (VSA). Recent studies about local region discovery need manually annotate region location. Affective local information learning and automatic discovery of sentiment-specific region are still the challenges in VSA. In this paper, we propose an end-to-end VSA method for weakly supervised sentiment-specific region discovery. Our method contains two branches: an automatic sentiment-specific region discovery branch and a sentiment analysis branch. In the sentiment-specific region discovery branch, a region proposal network with multiple convolution kernels is proposed to generate candidate affective regions. Then, we design the multiple instance learning (MIL) loss to remove redundant and noisy candidate regions. Finally, the sentiment analysis branch integrates both holistic and localized information obtained in the first branch by feature map coupling for final sentiment classification. Our method automatically discovers sentiment-specific regions by the constraint of MIL loss function without object-level labels. Quantitative and qualitative evaluations on four benchmark affective datasets demonstrate that our proposed method outperforms the state-of-the-art methods.},
  archive      = {J_COMJNL},
  author       = {Xue, Luoyang and Xu, Ang and Mao, Qirong and Gao, Lijian and Chen, Jie},
  doi          = {10.1093/comjnl/bxaa112},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {818-830},
  shortjournal = {Comput. J.},
  title        = {Weakly supervised sentiment-specific region discovery for VSA},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning model for multi-view visualization of
medical images. <em>COMJNL</em>, <em>65</em>(4), 805–817. (<a
href="https://doi.org/10.1093/comjnl/bxaa111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imaging techniques such as X-ray, computerized tomography scan and magnetic resonance imaging are useful in the correct diagnosis of a disease or deformity in the organ. Two-dimensional imaging techniques such as X-ray give a clear picture of simple bone deformity but fail in visualizing multiple fractures in a bone. Moreover, these lack in providing a multi-angle view of a bone. Three-dimensional techniques such as computerized tomography scan and magnetic resonance imaging present a correct orientation of fracture geometry. Computerized tomography scan is a collection of multiple slices of an image. These slices provide a fair idea about a fracture but fail in the measurement of correct dimensions of a fractured fragment and to observe its geometry. It also exposes a patient with carcinogenic radiations. Magnetic resonance imaging induces a strong magnetic field. So, it becomes ineffective for organs containing metallic implants. The high cost of three-dimensional imaging techniques makes them inaccessible for economic weaker section of society. The limitations of two- and three-dimensional imaging techniques motivate researchers to propose an innovative machine learning model ‘CT slices to ; -D convertor’ that accepts multiple slices of an image and yields a multi-dimensional view at all possible angles from 0 degree to 360 degree for an input image.},
  archive      = {J_COMJNL},
  author       = {Pradhan, Nitesh and Singh Dhaka, Vijaypal and Rani, Geeta and Chaudhary, Himanshu},
  doi          = {10.1093/comjnl/bxaa111},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {805-817},
  shortjournal = {Comput. J.},
  title        = {Machine learning model for multi-view visualization of medical images},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Breast cancer diagnosis using multi-stage weight adjustment
in the MLP neural network. <em>COMJNL</em>, <em>65</em>(4), 788–804. (<a
href="https://doi.org/10.1093/comjnl/bxaa109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is the most common kind of cancer, which is the cause of death among the women worldwide. There is evidence that shows that the early detection and treatment can increase the survival rate of patients who suffered this disease. Therefore, this paper proposes an automatic breast cancer diagnosis technique using a genetic algorithm for simultaneous feature selection and parameter optimization of an Multi Layer Perceptron (MLP) neural network. The aim of this paper is to propose a hybrid classification algorithm based on Multi-stage Weights Adjustment in the MLP (MWAMLP) neural network in two parts to improve the breast cancer diagnosis. In the first part, the three classifiers are trained simultaneously on the learning dataset. The output of the first part classifier together with the learning dataset is placed in a new dataset. This dataset uses a hybrid classifier method to model the mapping between the outputs of each ordinary classifier of the first part with real output labels. The proposed algorithm is implemented with three different variations of the backpropagation (BP) technique, namely the Levenberg–Marquardt, resilient BP and gradient descent with momentum for fine tuning of the weight of MLP neural network and their performances are compared. Interestingly, one of the proposed algorithms titled MWAMLP-RP produces the best and on average, 99.35\% and 98.74\% correct classification, respectively, on the Wisconsin Breast Cancer Database dataset, which is comparable with the obtained results from the methods titled GP-DLNN, GAANN and CAFS and other works found in the literature.},
  archive      = {J_COMJNL},
  author       = {Rezaeipanah, Amin and Ahmadi, Gholamreza},
  doi          = {10.1093/comjnl/bxaa109},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {788-804},
  shortjournal = {Comput. J.},
  title        = {Breast cancer diagnosis using multi-stage weight adjustment in the MLP neural network},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Work-efficient parallel non-maximum suppression kernels.
<em>COMJNL</em>, <em>65</em>(4), 773–787. (<a
href="https://doi.org/10.1093/comjnl/bxaa108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of object detection, sliding-window classifiers and single-shot convolutional neural network (CNN) meta-architectures typically yield multiple overlapping candidate windows with similar high scores around the true location of a particular object. Non-maximum suppression (NMS) is the process of selecting a single representative candidate within this cluster of detections, so as to obtain a unique detection per object appearing on a given picture. In this paper, we present a highly scalable NMS algorithm for embedded graphics processing unit (GPU) architectures that is designed from scratch to handle workloads featuring thousands of simultaneous detections on a given picture. Our kernels are directly applicable to other sequential NMS algorithms such as FeatureNMS, Soft-NMS or AdaptiveNMS that share the inner workings of the classic greedy NMS method. The obtained performance results show that our parallel NMS algorithm is capable of clustering 1024 simultaneous detected objects per frame in roughly 1 ms on both Tegra X1 and Tegra X2 on-die GPUs, while taking 2 ms on Tegra K1. Furthermore, our proposed parallel greedy NMS algorithm yields a 14–40x speed up when compared to state-of-the-art NMS methods that require learning a CNN from annotated data.},
  archive      = {J_COMJNL},
  author       = {Oro, David and Fernández, Carles and Martorell, Xavier and Hernando, Javier},
  doi          = {10.1093/comjnl/bxaa108},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {773-787},
  shortjournal = {Comput. J.},
  title        = {Work-efficient parallel non-maximum suppression kernels},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). S-DolLion-MSVNN: A hybrid model for developing the
super-resolution image from the multispectral satellite image.
<em>COMJNL</em>, <em>65</em>(4), 757–772. (<a
href="https://doi.org/10.1093/comjnl/bxaa106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Super-resolution offers a new image with high resolution from the low-resolution (LR) image that is highly employed for the numerous remote sensing applications. Most of the existing techniques for formation of the super-resolution image exhibit the loss of quality and deviation from the original multi-spectral LR image. Thus, this paper aims at proposing an efficient super-resolution method using the hybrid model. The hybrid model is developed using the support vector regression model and multi-support vector neural network (MSVNN), and the weights of the MSVNN is tuned optimally using the proposed algorithm. The proposed DolLion algorithm is the integration of the dolphin echolocation algorithm and lion optimization algorithm that exhibits better convergence and offers a global optimal solution. The experimentation is performed using the datasets taken from the multi-spectral scene images. The optimal and effective formation of the super-resolution image using the proposed hybrid model outperforms the existing methods, and the analysis using the second-derivative-like measure of enhancement (SDME) ensures that the proposed method is better and yields a maximum SDME of 67.6755 dB.},
  archive      = {J_COMJNL},
  author       = {Gavade, Anil B and Rajpurohit, Vijay S},
  doi          = {10.1093/comjnl/bxaa106},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {757-772},
  shortjournal = {Comput. J.},
  title        = {S-DolLion-MSVNN: A hybrid model for developing the super-resolution image from the multispectral satellite image},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Agent-based autonomic semantic context-aware platform for
smart health monitoring and disease detection. <em>COMJNL</em>,
<em>65</em>(3), 736–755. (<a
href="https://doi.org/10.1093/comjnl/bxab075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, mobile applications have been widely used in various health and social domains. While there are large number of smart devices that connect and transmit their context data in free manner, these mobile applications have limitations regarding the increasing number of heterogeneous context data sent by devices, conducting the problem of identifying user-centric situations and providing services in real-time. Many smart mobile health systems were proposed to ensure context-aware personal health monitoring and diseases detection, while most of them are failed to ensure a right level of dynamicity and enough flexibility for assisting users everywhere, anytime, through widespread sensors and mobile devices. In fact, it becomes necessary to rethink a new way to minimize the response time combining competitive agents with semantic-based situation reasoning strategy. In this paper, we introduce a novel agent-based platform with three-layered ontology for the semantic description and parallel management of services selection approach dedicated to context-aware health mobile applications. In addition, we propose an innovative parallel services discovery and optimal selection process, which involved a set of filtered and classified semantic health multipath according to user’s context and preferences with consistent mobility of users and limited resources (low battery, processing capabilities, memory and others). Experimental results show the effectiveness of the proposed approach as it includes the semantic service information on cooperative agents. In addition, the proposed approach ensures fast response time and prolongs the continuity of the mobile application.},
  archive      = {J_COMJNL},
  author       = {Alti, Adel and Laouamer, Lamri},
  doi          = {10.1093/comjnl/bxab075},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {736-755},
  shortjournal = {Comput. J.},
  title        = {Agent-based autonomic semantic context-aware platform for smart health monitoring and disease detection},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lost in translation: Exposing hidden compiler optimization
opportunities. <em>COMJNL</em>, <em>65</em>(3), 718–735. (<a
href="https://doi.org/10.1093/comjnl/bxaa103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing iterative compilation and machine learning-based optimization techniques have been proven very successful in achieving better optimizations than the standard optimization levels of a compiler. However, they were not engineered to support the tuning of a compiler’s optimizer as part of the compiler’s daily development cycle. In this paper, we first establish the required properties that a technique must exhibit to enable such tuning. We then introduce an enhancement to the classic nightly routine testing of compilers, which exhibits all the required properties and thus is capable of driving the improvement and tuning of the compiler’s common optimizer. This is achieved by leveraging resource usage and compilation information collected while systematically exploiting prefixes of the transformations applied at standard optimization levels. Experimental evaluation using the LLVM v6.0.1 compiler demonstrated that the new approach was able to reveal hidden cross-architecture and architecture-dependent potential optimizations on two popular processors: the Intel i5-6300U and the Arm Cortex-A53-based Broadcom BCM2837 used in the Raspberry Pi 3B+. As a case study, we demonstrate how the insights from our approach enabled us to identify and remove a significant shortcoming of the CFG simplification pass of the LLVM v6.0.1 compiler.},
  archive      = {J_COMJNL},
  author       = {Georgiou, Kyriakos and Chamski, Zbigniew and Amaya Garcia, Andres and May, David and Eder, Kerstin},
  doi          = {10.1093/comjnl/bxaa103},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {718-735},
  shortjournal = {Comput. J.},
  title        = {Lost in translation: Exposing hidden compiler optimization opportunities},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Similarity of sentences with contradiction using semantic
similarity measures. <em>COMJNL</em>, <em>65</em>(3), 701–717. (<a
href="https://doi.org/10.1093/comjnl/bxaa100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short text or sentence similarity is crucial in various natural language processing activities. Traditional measures for sentence similarity consider word order, semantic features and role annotations of text to derive the similarity. These measures do not suit short texts or sentences with negation. Hence, this paper proposes an approach to determine the semantic similarity of sentences and also presents an algorithm to handle negation. In sentence similarity, word pair similarity plays a significant role. Hence, this paper also discusses the similarity between word pairs. Existing semantic similarity measures do not handle antonyms accurately. Hence, this paper proposes an algorithm to handle antonyms. This paper also presents an antonym dataset with 111-word pairs and corresponding expert ratings. The existing semantic similarity measures are tested on the dataset. The results of the correlation proved that the expert ratings are in order with the correlation obtained from the semantic similarity measures. The sentence similarity is handled by proposing two algorithms. The first algorithm deals with the typical sentences, and the second algorithm deals with contradiction in the sentences. SICK dataset, which has sentences with negation, is considered for handling the sentence similarity. The algorithm helped in improving the results of sentence similarity.},
  archive      = {J_COMJNL},
  author       = {Krishna Siva Prasad, M and Sharma, Poonam},
  doi          = {10.1093/comjnl/bxaa100},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {701-717},
  shortjournal = {Comput. J.},
  title        = {Similarity of sentences with contradiction using semantic similarity measures},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An energy-efficient step-counting algorithm for smartphones.
<em>COMJNL</em>, <em>65</em>(3), 689–700. (<a
href="https://doi.org/10.1093/comjnl/bxaa096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Step counting is not only the key component of pedometers (which is a fundamental service on smartphones), but is also closely related to a range of applications, including motion monitoring, behavior recognition, indoor positioning and navigation. Due to the limited battery capacity of current smartphones, it is of great value to reduce the energy consumption of such a popular service. Therefore, this paper focuses on the energy efficiency of step-counting algorithms. First of all, we formulate a theoretical error model based on the well-known auto-correlation coefficient step-counting (ACSC) algorithm, so as to analyze the factors affecting step-counting accuracy. And then, in light of this model and an adaptive sampling strategy, we propose a novel energy-efficient step-counting algorithm by adaptively substituting the computationally intensive auto-correlation with simple mean absolute deviation. On these grounds, an Android pedometer is implemented. Two individual experiments are carried out and verify both the theoretical error model and the proposed algorithm. It is shown that our algorithm outperforms two famous counterparts, i.e. the original ACSC algorithm and peak detection step-counting algorithm, in terms of both accuracy and energy efficiency.},
  archive      = {J_COMJNL},
  author       = {Yang, Runze and Song, Jian and Huang, Baoqi and Li, Wuyungerile and Qi, Guodong},
  doi          = {10.1093/comjnl/bxaa096},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {689-700},
  shortjournal = {Comput. J.},
  title        = {An energy-efficient step-counting algorithm for smartphones},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Considering fine-grained and coarse-grained information for
context-aware recommendations. <em>COMJNL</em>, <em>65</em>(3), 679–688.
(<a href="https://doi.org/10.1093/comjnl/bxaa095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In context-aware recommendation systems, most existing methods encode users’ preferences by mapping item and category information into the same space, which is just a stack of information. The item and category information contained in the interaction behaviours is not fully utilized. Moreover, since users’ preferences for a candidate item are influenced by the changes in temporal and historical behaviours, it is unreasonable to predict correlations between users and candidates by using users’ fixed features. A fine-grained and coarse-grained information based framework proposed in our paper which considers multi-granularity information of users’ historical behaviours. First, a parallel structure is provided to mine users’ preference information under different granularities. Then, self-attention and attention mechanisms are used to capture the dynamic preferences. Experiment results on two publicly available datasets show that our framework outperforms state-of-the-art methods across the calculated evaluation metrics.},
  archive      = {J_COMJNL},
  author       = {Luo, Yiqin and Sun, Yanpeng and Chang, Liang and Gu, Tianlong and Bin, Chenzhong and Li, Long},
  doi          = {10.1093/comjnl/bxaa095},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {679-688},
  shortjournal = {Comput. J.},
  title        = {Considering fine-grained and coarse-grained information for context-aware recommendations},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integrating geospatial data and social media in
bidirectional long-short term memory models to capture human nature
interactions. <em>COMJNL</em>, <em>65</em>(3), 667–678. (<a
href="https://doi.org/10.1093/comjnl/bxaa094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contact with nature has been linked to human health, but little information is available for how individuals utilize urban nature. We developed a bidirectional long short-term memory model for classifying whether tweets describe the proposed pathways through which nature influences health: exercise, aesthetic stimulation, stress reduction, safety, air pollution mediation, and/or social interaction. To adjust for regional variations in urban nature context, we integrated OpenStreetMap data on nature and non-nature features for each long-short term memory cell. Training (n = 63073), development (n = 5000), and test (n = 5000) sets consisted of labeled tweets from Portland, Oregon. Tweets from New York City (NYC) (n = 5000) were also labeled to test generalizability. The model was applied retrospectively to 20 million tweets from 2017 and continuously to Meetup posts for 7,708 cities in North America. F1Scores ranged from 0.54 to 0.82 in the NYC dataset, a 24\% to 92\% improvement over current methods. Precision ranged from 0.58 to 0.83, while recall ranged from 0.39 to 0.81. Adding OpenStreetMap features led to greater percent and absolute F1Scores in NYC compared to Portland. Average F1Scores were greater in models with a nature label in addition to human behavior labels (0.59 vs. 0.65), suggesting health behaviors are influenced by urban nature.},
  archive      = {J_COMJNL},
  author       = {Larkin, Andrew and Hystad, Perry},
  doi          = {10.1093/comjnl/bxaa094},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {667-678},
  shortjournal = {Comput. J.},
  title        = {Integrating geospatial data and social media in bidirectional long-short term memory models to capture human nature interactions},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lemuria: A novel future crop prediction algorithm using data
mining. <em>COMJNL</em>, <em>65</em>(3), 655–666. (<a
href="https://doi.org/10.1093/comjnl/bxaa093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agriculture exhibitions an important role in the progression and enlargement of the economy of any country. Prediction of crop yield will be useful for farmers, but it is difficult to predict crop yield because of the climatic factors such as rainfall, soil factors and so on. To tackle these issues, we are implementing a novel algorithm called Lemuria by applying data mining in agriculture especially for crop yield analysis and prediction. This novel algorithm is the hybridization of classifiers for pre-training, training and testing: deep belief network for feature learning, ; -means clustering together with particle swarm optimization (PSO) to get the global solution as well as naïve Bayes clustering with PSO for testing. The performance of the Lemuria algorithm is evaluated in Python, which provides an accuracy of 97.74\% for crop prediction by considering the rainfall dataset and also stated that this gives the optimum results in comparison with the existing methodologies.},
  archive      = {J_COMJNL},
  author       = {Tamil Selvi, M and Jaison, B},
  doi          = {10.1093/comjnl/bxaa093},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {655-666},
  shortjournal = {Comput. J.},
  title        = {Lemuria: A novel future crop prediction algorithm using data mining},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic frequency scaling of a single-core processor using
machine learning paradigms. <em>COMJNL</em>, <em>65</em>(3), 631–654.
(<a href="https://doi.org/10.1093/comjnl/bxaa092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic frequency scaling (DFS) is one of the most important approaches for on-the-fly power optimization in modern-day processors. Owing to the trend of chip size shrinkage and increasing the complexity of system design, the problem of achieving an efficient DFS depends upon multi-parametric, non-linear optimization. Hence, it becomes extremely important to identify an optimal underclocking frequency on-the-fly, which depends upon numerous parameters that do not share direct relationship amongst each other. This paper proposes a machine learning approach to DFS of a ubiquitous single-core processor. Several performance parameters of the processor were monitored under an application of a number of clocking frequencies. The dataset thus generated was used to train four artificial neural networks (ANNs) viz. generalized regression (GRNN), decision tree classifier, random forest classifier and backpropagation technique. Under changing parametric conditions of the proposed network, the modes were fit to data while running three applications, i.e. 64- and 1024-point fast fourier transform (FFT) and basicmath applications. The performance of all ANNs was found to be promising and good generalization was obtained with all datasets. In the view of optimizing both speed and power of a system, the results indicate towards suitability of trained GRNN for on-chip deployment for implementing DFS.},
  archive      = {J_COMJNL},
  author       = {Thethi, Sukhmani K and Kumar, Ravi},
  doi          = {10.1093/comjnl/bxaa092},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {631-654},
  shortjournal = {Comput. J.},
  title        = {Dynamic frequency scaling of a single-core processor using machine learning paradigms},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved tactile perception of 3D geometric bumps using
coupled electrovibration and mechanical vibration stimuli.
<em>COMJNL</em>, <em>65</em>(3), 621–630. (<a
href="https://doi.org/10.1093/comjnl/bxaa091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, the tactile perception of 3D geometric bumps (such as sinusoidal bumps, Gaussian bumps, triangular bumps, etc.) on touchscreens is mainly realized by mapping the local gradients of rendered virtual surfaces to lateral electrostatic friction, while maintaining the constant normal feedback force. The latest study has shown that the recognition rate of 3D visual objects with electrovibration is lower by 27; than that using force-feedback devices. Based on the custom-designed tactile display coupling with electrovibration and mechanical vibration stimuli, this paper proposes a novel tactile rendering algorithm of 3D geometric bumps, which simultaneously generates the lateral and the normal perceptual dimensions. Specifically, a mapping relationship with the electrostatic friction proportional to the gradient of 3D geometric bumps is firstly established. Then, resorting to the angle between the lateral friction force and the normal feedback force, a rendering model of the normal feedback force using mechanical vibration is further determined. Compared to the previous works with electrovibration, objective evaluations with 12 participants showed that the novel version significantly improved recognition rates of 3D bumps on touchscreens.},
  archive      = {J_COMJNL},
  author       = {Sun, Xiaoying and Zhang, Chen and Liu, Guohong},
  doi          = {10.1093/comjnl/bxaa091},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {621-630},
  shortjournal = {Comput. J.},
  title        = {Improved tactile perception of 3D geometric bumps using coupled electrovibration and mechanical vibration stimuli},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A genetically based combination of visual saliency and
roughness for FR 3D mesh quality assessment: A statistical study.
<em>COMJNL</em>, <em>65</em>(3), 606–620. (<a
href="https://doi.org/10.1093/comjnl/bxaa089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a full-reference quality assessment metric based on the information of visual saliency. The saliency information is provided under the form of degrees associated to each vertex of the surface mesh. From these degrees, statistical attributes reflecting the structures of the reference and distorted meshes are computed. These are used by four comparisons functions genetically optimized that quantify the structure differences between a reference and a distorted mesh. We also present a statistical comparison study of six full-reference quality assessment metrics for 3D meshes. We compare the objective metrics results with humans subjective scores of quality considering the 3D meshes in one hand and the distorsion types in the other hand. Also, we show which metrics are statistically superior to their counterparts. For these comparisons we use the Spearman Rank Ordered Correlation Coefficient and the hypothetic test of Student (ttest). To attest the pertinence of the proposed approach, a comparison with a ground truth saliency and an application associated to the assessment of the visual rendering of smoothing algorithms are presented. Experimental results show that the proposed metric is very competitive with the state-of-the-art.},
  archive      = {J_COMJNL},
  author       = {Nouri, Anass and Charrier, Christophe and Lézoray, Olivier},
  doi          = {10.1093/comjnl/bxaa089},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {606-620},
  shortjournal = {Comput. J.},
  title        = {A genetically based combination of visual saliency and roughness for FR 3D mesh quality assessment: A statistical study},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Subgraph reliability of alternating group graph with uniform
and nonuniform vertex fault-free probabilities. <em>COMJNL</em>,
<em>65</em>(3), 589–605. (<a
href="https://doi.org/10.1093/comjnl/bxaa088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the size of a multiprocessor system grows, the probability that faults occur in this system increases. One measure of the reliability of a multiprocessor system is the probability that a fault-free subsystem of a certain size still exists with the presence of individual faults. In this paper, we use the probabilistic fault model to establish the subgraph reliability for ; , the ; -dimensional alternating group graph. More precisely, we first analyze the probability ; that at least one subgraph with dimension ; is fault-free in ; , when given a uniform probability of a single vertex being fault-free. Since subgraphs of ; intersect in rather complicated manners, we resort to the principle of inclusion–exclusion by considering intersections of up to five subgraphs and obtain an upper bound of the probability. Then we consider the probabilistic fault model when the probability of a single vertex being fault-free is nonuniform, and we show that the upper bound under these two models is very close to the lower bound obtained in a previous result, and it is better than the upper bound deduced from that of the arrangement graph, which means that the upper bound we obtained is very tight.},
  archive      = {J_COMJNL},
  author       = {Huang, Yanze and Lin, Limei and Xu, Li},
  doi          = {10.1093/comjnl/bxaa088},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {589-605},
  shortjournal = {Comput. J.},
  title        = {Subgraph reliability of alternating group graph with uniform and nonuniform vertex fault-free probabilities},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Central station-based demand prediction for determining
target inventory in a bike-sharing system. <em>COMJNL</em>,
<em>65</em>(3), 573–588. (<a
href="https://doi.org/10.1093/comjnl/bxaa086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the bike demand can help rebalance the bikes and improve the service quality of a bike-sharing system. A lot of works focus on predicting the bike demand for all the stations, which is unnecessary as the travel cost of rebalance operations increases sharply as the number of stations increases. In this paper, we propose a framework for predicting the hourly bike demand based on the central stations we define. Firstly, we propose Two-Stage Station Clustering Algorithm to assign central stations and common stations into each cluster. Secondly, we propose a hierarchical prediction model to predict the hourly bike demand for every cluster and each central station progressively. Thirdly, we use a well-studied queuing model to determine the target initial inventory for each central station. The most innovative contribution of this paper is proposing the concept of central station, the use of a novel algorithm to cluster the central stations and present a hierarchical model, containing the Time and Weather Similarity Weighted K-Nearest Neighbor Algorithm and a linear model to predict the bike demand for central stations. The experimental results on the New York citi bike system demonstrate that our proposed method is more accurate than other methods in solving existing problems.},
  archive      = {J_COMJNL},
  author       = {Huang, Jianbin and Sun, Heli and Li, He and Huang, Longji and Li, Ao and Wang, Xiangyu},
  doi          = {10.1093/comjnl/bxaa086},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {573-588},
  shortjournal = {Comput. J.},
  title        = {Central station-based demand prediction for determining target inventory in a bike-sharing system},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Algorithms based on path contraction carrying weights for
enumerating subtrees of tricyclic graphs. <em>COMJNL</em>,
<em>65</em>(3), 554–572. (<a
href="https://doi.org/10.1093/comjnl/bxaa084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ; of a graph, defined as the number of subtrees, attracts much attention recently. Finding a proper algorithm to compute this index is an important but difficult problem for a general graph. Even for unicyclic and bicyclic graphs, it is not completely trivial, though it can be figured out by try and error. However, it is complicated for tricyclic graphs. This paper proposes path contraction carrying weights (PCCWs) algorithms to compute the subtree number index for the nontrivial case of bicyclic graphs and all 15 cases of tricyclic graphs, based on three techniques: PCCWs, generating function and structural decomposition. Our approach provides a foundation and useful methods to compute subtree number index for graphs with more complicated cycle structures and can be applied to investigate the novel structural property of some important nanomaterials such as the pentagonal carbon nanocone.},
  archive      = {J_COMJNL},
  author       = {Yang, Yu and Chen, Beifang and Zhang, Guoping and Li, Yongming and Sun, Daoqiang and Liu, Hongbo},
  doi          = {10.1093/comjnl/bxaa084},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {554-572},
  shortjournal = {Comput. J.},
  title        = {Algorithms based on path contraction carrying weights for enumerating subtrees of tricyclic graphs},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incorporating biterm correlation knowledge into topic
modeling for short texts. <em>COMJNL</em>, <em>65</em>(3), 537–553. (<a
href="https://doi.org/10.1093/comjnl/bxaa079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence of short texts on the Web has made mining the latent topic structures of short texts a critical and fundamental task for many applications. However, due to the lack of word co-occurrence information induced by the content sparsity of short texts, it is challenging for traditional topic models like latent Dirichlet allocation (LDA) to extract coherent topic structures on short texts. Incorporating external semantic knowledge into the topic modeling process is an effective strategy to improve the coherence of inferred topics. In this paper, we develop a novel topic model—called biterm correlation knowledge-based topic model (BCK-TM)—to infer latent topics from short texts. Specifically, the proposed model mines biterm correlation knowledge automatically based on recent progress in word embedding, which can represent semantic information of words in a continuous vector space. To incorporate external knowledge, a knowledge incorporation mechanism is designed over the latent topic layer to regularize the topic assignment of each biterm during the topic sampling process. Experimental results on three public benchmark datasets illustrate the superior performance of the proposed approach over several state-of-the-art baseline models.},
  archive      = {J_COMJNL},
  author       = {Zhang, Kai and Zhou, Yuan and Chen, Zheng and Liu, Yufei and Tang, Zhuo and Yin, Li and Chen, Jihong},
  doi          = {10.1093/comjnl/bxaa079},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {537-553},
  shortjournal = {Comput. J.},
  title        = {Incorporating biterm correlation knowledge into topic modeling for short texts},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MeSH-based semantic indexing approach to enhance biomedical
information retrieval. <em>COMJNL</em>, <em>65</em>(3), 516–536. (<a
href="https://doi.org/10.1093/comjnl/bxaa073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the tremendous size of electronic biomedical documents, users encounter difficulties in seeking useful biomedical information. An efficient and smart access to the relevant biomedical information has become a fundamental need. In this research paper, we set forward a novel biomedical MeSH-based semantic indexing approach to enhance biomedical information retrieval. The proposed semantic indexing approach attempts to strengthen the content representation of both documents and queries by incorporating unambiguous MeSH concepts as well as the adequate senses of ambiguous MeSH concepts. For this purpose, our proposed approach relies on a disambiguation method to identify the adequate senses of ambiguous MeSH concepts and introduces four representation enrichment strategies so as to identify the best appropriate representatives of the adequate sense in the textual entities representation. To prove its effectiveness, the proposed semantic indexing approach was evaluated by intensive experiments. These experiments were carried out on OHSUMED test collection. The results reveal that our proposal outperforms the state-of-the-art approaches and allow us to highlight the most effective strategy.},
  archive      = {J_COMJNL},
  author       = {Kammoun, Hager and Gabsi, Imen and Amous, Ikram},
  doi          = {10.1093/comjnl/bxaa073},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {516-536},
  shortjournal = {Comput. J.},
  title        = {MeSH-based semantic indexing approach to enhance biomedical information retrieval},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An approach for the evaluation and correction of manually
designed video game levels using deep neural networks. <em>COMJNL</em>,
<em>65</em>(3), 495–515. (<a
href="https://doi.org/10.1093/comjnl/bxaa071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current state of the video game productions, most of the video game levels are created by the human operators working as level designers. This manual process is not only time-consuming and resource-intensive but also hard to guarantee uniform quality in the contents created by the level designers. One way to address this issue is to use computer-assisted level design techniques. In this paper, we have proposed a novel framework for computer-assisted video game level design that leverages neural networks, particularly generative adversarial networks (GANs) and autoencoders. The general idea is to learn over a dataset of high-quality levels and subsequently improve the ones created by the level designers. The proposed method is independent of the graphical dimensionality of the game and will work for 2D and 3D games in general. The autoencoder is used to create an intermediate representation of the level that is itself changed using the backpropagation technique according to the feedback obtained by feeding the output of the autoencoder to the discriminator component of the GAN. After performing a series of evaluations on the proposed framework and by automatically improving a series of purposefully corrupted game levels, the results demonstrate a noticeable improvement compared with the usage of simple autoencoders used to improve the video game levels in the previous researches.},
  archive      = {J_COMJNL},
  author       = {Davoodi, Omid and Ashtiani, Mehrdad and Rajabi, Morteza},
  doi          = {10.1093/comjnl/bxaa071},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {495-515},
  shortjournal = {Comput. J.},
  title        = {An approach for the evaluation and correction of manually designed video game levels using deep neural networks},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enabling smart city with intelligent congestion control
using hops with a hybrid computational approach. <em>COMJNL</em>,
<em>65</em>(3), 484–494. (<a
href="https://doi.org/10.1093/comjnl/bxaa068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a smart city, the subject of the congestion-free traffic has been leading objectives from the past decade, and many approaches are adopted to make congestion-free roads. These approaches and signals at one junction are not inter-linked with the signal at the previous one. Therefore, the traffic flow on the same road and at associative roads is not smooth. The study proposed a model with a hybrid computational approach in which the current signal incorporates the associative signals information. Simulation results have shown that the proposed approach gives more attractive results as compared to previously published approaches. It will help improve the flow of traffic and reduce traffic congestion.},
  archive      = {J_COMJNL},
  author       = {Abbas, Sagheer and Khan, Muhammad Adnan and Athar, Atifa and Shan, Syed Ali and Saeed, Anwar and Alyas, Tahir},
  doi          = {10.1093/comjnl/bxaa068},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {484-494},
  shortjournal = {Comput. J.},
  title        = {Enabling smart city with intelligent congestion control using hops with a hybrid computational approach},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SkySlide: A hybrid method for landslide susceptibility
assessment based on landslide-occurring data only. <em>COMJNL</em>,
<em>65</em>(3), 473–483. (<a
href="https://doi.org/10.1093/comjnl/bxaa063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Landslide susceptibility assessment is the problem of determining the likelihood of a landslide occurrence in a particular area with respect to the geographical and morphological properties of the area. This paper presents a hybrid method, namely ; , that incorporates clustering, skyline operator, classification and majority voting principle for region-scale landslide susceptibility assessment. Clustering and skyline operator are utilized to model landslides while classification and majority voting principle are utilized to assess landslide susceptibility. The contribution of the study is 2-fold. First, the proposed method requires properties of landslide-occurring data only to model landslides. Second, the proposed method is evaluated on imbalanced data and experimental results include performance metrics of imbalanced data. Experiments conducted on two real-life datasets show that clustering greatly improves performance of ; . Experiments further demonstrate that ; achieves higher class balance accuracy, Matthews correlation coefficient, geometric mean and bookmaker informedness scores compared with the most commonly used methods for landslide susceptibility assessment such as support vector machines, logistic regression and decision trees.},
  archive      = {J_COMJNL},
  author       = {Mutlu, Alev and Goz, Furkan},
  doi          = {10.1093/comjnl/bxaa063},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {473-483},
  shortjournal = {Comput. J.},
  title        = {SkySlide: A hybrid method for landslide susceptibility assessment based on landslide-occurring data only},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast learning through deep multi-net CNN model for violence
recognition in video surveillance. <em>COMJNL</em>, <em>65</em>(3),
457–472. (<a href="https://doi.org/10.1093/comjnl/bxaa061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The violence detection is mostly achieved through handcrafted feature descriptors, while some researchers have also employed deep learning-based representation models for violent activity recognition. Deep learning-based models have achieved encouraging results for fight activity recognition on benchmark data sets such as hockey and movies. However, these models have limitations in learning discriminating features for violence activity classification with abrupt camera motion. This research work investigated deep representation models using transfer learning for handling the issue of abrupt camera motion. Consequently, a novel deep multi-net (DMN) architecture based on AlexNet and GoogleNet is proposed for violence detection in videos. AlexNet and GoogleNet are top-ranked pre-trained models for image classification with distinct pre-learnt potential features. The fusion of these models can yield superior performance. The proposed DMN unleashed the integrated potential by concurrently coalescing both networks. The results confirmed that DMN outperformed state-of-the-art methods by learning finest discriminating features and achieved 99.82\% and 100\% accuracy on hockey and movies data sets, respectively. Moreover, DMN has faster learning capability i.e. 1.33 and 2.28 times faster than AlexNet and GoogleNet, which makes it an effective learning architecture on images and videos.},
  archive      = {J_COMJNL},
  author       = {Mumtaz, Aqib and Bux Sargano, Allah and Habib, Zulfiqar},
  doi          = {10.1093/comjnl/bxaa061},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {457-472},
  shortjournal = {Comput. J.},
  title        = {Fast learning through deep multi-net CNN model for violence recognition in video surveillance},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rumor identification and verification for text in social
media content. <em>COMJNL</em>, <em>65</em>(2), 436–455. (<a
href="https://doi.org/10.1093/comjnl/bxab118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twitter led a remarkable breakthrough in information sharing on online social media. The eminent technology can propagate a piece of rumor to a large community of people in a short period. The timely detection of rumor tweets in social media curtails panic among the public during critical situations. Traditional machine learning techniques are not capable of categorizing rumor information effectively. To address this problem, the author has proposed a novel neural network approach called veracity detection neural network for identifying the rumor-related Twitter posts’ content in real-time events. This algorithm utilized the convolutional sentence encoder–bi-directional long short-term memory (CSE-BiLSTM) model with pre-trained vectorization methods such as Word2vec, fastText and universal sentence encoder (USE). The hybrid CSE-BiLSTM with USE vectorization technique yields the best results for the performance metrics of accuracy, F1-score, precision and recall. The proposed algorithm achieves 90.56\%, 86.18\% and 93.89\% accuracy values to classify the tweet into rumor or non-rumor for the datasets such as PHEME, newly emerged rumors on Twitter and #gaja, respectively. Finally, a comparative study shows that the proposed neural network model outperformed all other existing rumor text classification systems.},
  archive      = {J_COMJNL},
  author       = {Suthanthira Devi, P and Karthika, S},
  doi          = {10.1093/comjnl/bxab118},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {436-455},
  shortjournal = {Comput. J.},
  title        = {Rumor identification and verification for text in social media content},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid intelligent text watermarking and natural language
processing approach for transferring and receiving an authentic english
text via internet. <em>COMJNL</em>, <em>65</em>(2), 423–435. (<a
href="https://doi.org/10.1093/comjnl/bxab087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rapid increase in the exchange of text information via internet networks, the security and the reliability of digital content have become a major research issue. The main challenges faced by researchers are authentication, integrity verification, and tampering detection of the digital contents. In this paper, a Robust English Text Watermarking and Natural Language Processing Approach (RETWNLPA) is proposed based on word mechanism and first level order of Markov model to improve the accuracy of tampering detection of sensitive English text. The RETWNLPA approach embeds and detects the watermark logically without altering the original text document. Based on the hidden Markov model (HMM), the first-level order of word mechanism is used to analyze the interrelationship between English text. The extracted features are used as watermark information and integrated with text zero-watermarking techniques. To detect eventual tampering, RETWNLPA has been implemented and validated with attacked English text. Experiments were performed on four datasets of varying sizes under random locations of common tampering attacks. The simulation results prove the tampering detection accuracy of our method against all kinds of tampering attacks. Comparison results show that RETWNLPA outperforms baseline approaches HNLPZWA (an intelligent hybrid of natural language processing and zero-watermarking approach) and ZWAFWMMM (Zero-Watermarking Approach based on Fourth level order of Word Mechanism of Markov Model) in terms of tampering detection accuracy.},
  archive      = {J_COMJNL},
  author       = {Hilal, Anwer Mustafa and Al-Wesabi, Fahd N and Abdelmaboud, Abdelzahir and Hamza, Manar Ahmed and Mahzari, Mohammad and Hassan, Abdulkhaleq Q A},
  doi          = {10.1093/comjnl/bxab087},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {423-435},
  shortjournal = {Comput. J.},
  title        = {A hybrid intelligent text watermarking and natural language processing approach for transferring and receiving an authentic english text via internet},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An attention enhanced cross-modal image–sound mutual
generation model for birds. <em>COMJNL</em>, <em>65</em>(2), 410–422.
(<a href="https://doi.org/10.1093/comjnl/bxaa188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal bird image–audio mutual generation has appealing potential benefits for bird classification. To achieve promising cross-modal bird visual–audio mutual generation, we propose an attention enhanced cross-modal cycle adversarial generation network. Specifically, the attention module endows our model with long-term intra-modality dependency and inter-modality dependency capabilities, which can provide more information during the generation process and further improve the generation performance. Moreover, because there was no dataset concerning bird visual–audio mutual generation, the authors established a novel bird cross-modal generation dataset, called Bird_Crossmodal_Generation (BCG). Based on BCG, our model obtains promising performance and achieves significant improvement under both inception score and Frechet inception distance criteria. The experimental results validate the feasibility of the proposed task and the superiority of our model. Additionally, this investigation provides a basis for more researchers to develop cross-modality methods for bird visual–audio generation.},
  archive      = {J_COMJNL},
  author       = {Hao, Wangli and Han, Meng and Li, Shancang and Li, Fuzhong},
  doi          = {10.1093/comjnl/bxaa188},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {410-422},
  shortjournal = {Comput. J.},
  title        = {An attention enhanced cross-modal Image–Sound mutual generation model for birds},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detection of 2D and 3D video transitions based on EEG power.
<em>COMJNL</em>, <em>65</em>(2), 396–409. (<a
href="https://doi.org/10.1093/comjnl/bxaa116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the long and extensive history of 3D technology, it has recently attracted the attention of researchers. This technology has become the center of interest of young people because of the real feelings and sensations it creates. People see their environment as 3D because of their eye structure. In this study, it is hypothesized that people lose their perception of depth during sleepy moments and that there is a sudden transition from 3D vision to 2D vision. Regarding these transitions, the EEG signal analysis method was used for deep and comprehensive analysis of 2D and 3D brain signals. In this study, a single-stream anaglyph video of random 2D and 3D segments was prepared. After watching this single video, the obtained EEG recordings were considered for two different analyses: the part involving the critical transition (transition state) and the state analysis of only the 2D versus 3D or 3D versus 2D parts (steady state). The main objective of this study is to see the behavioral changes of brain signals in 2D and 3D transitions. To clarify the impacts of the human brain’s power spectral density (PSD) in 2D-to-3D (2D_3D) and 3D-to-2D (3D_2D) transitions of anaglyph video, nine visual healthy individuals were prepared for testing in this pioneering study. Spectrogram graphs based on short time Fourier transform (STFT) were considered to evaluate the power spectrum analysis in each EEG channel of transition or steady state. Thus, in 2D and 3D transition scenarios, important channels representing EEG frequency bands and brain lobes will be identified. To classify the 2D and 3D transitions, the dominant bands and time intervals representing the maximum difference of PSD were selected. Afterward, effective features were selected by applying statistical methods such as standard deviation, maximum (max) and Hjorth parameters to epochs indicating transition intervals. Ultimately, ; -nearest neighbors, support vector machine and linear discriminant analysis (LDA) algorithms were applied to classify 2D_3D and 3D_2D transitions. The frontal, temporal and partially parietal lobes show 2D_3D and 3D_2D transitions with a good classification success rate. Overall, it was found that Hjorth parameters and LDA algorithms have 71.11\% and 77.78\% classification success rates for transition and steady state, respectively.},
  archive      = {J_COMJNL},
  author       = {Manshouri, Negin and Melek, Mesut and Kayıkcıoglu, Temel},
  doi          = {10.1093/comjnl/bxaa116},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {396-409},
  shortjournal = {Comput. J.},
  title        = {Detection of 2D and 3D video transitions based on EEG power},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local pollination-based moth search algorithm for
task-scheduling heterogeneous cloud environment. <em>COMJNL</em>,
<em>65</em>(2), 382–395. (<a
href="https://doi.org/10.1093/comjnl/bxaa053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, Cloud computing is a new computing model in the field of information technology and research. Generally, the cloud environment aims in providing the resource that depends upon the user’s necessity. The major problem caused by cloud computing is task scheduling. Nevertheless, the previous scheduling methods concentrate only on the resource needs, memory, implementation time and cost. In this paper, we introduced an optimal task-scheduling algorithm of the local pollination-based moth search algorithm (LPMSA), which is the hybridization of moth search algorithm (MSA) and flower pollination algorithm (FPA). The proposed LPMSA chooses an optimal solution for proper task scheduling in the cloud. Moreover, the exploitation capacity of MSA is improved by using the local search of the FPA algorithm. In this work, we use 2-fold simulation processes that are implemented under the platform of JAVA. The proposed LPMSA for task-scheduling performance is evaluated using low and high heterogeneous machines with uniform and non-uniform parameters. The experimental analysis demonstrates that the proposed LPMSA approach is well suitable for cloud task scheduling thereby reducing the makespan and energy consumption during proper task scheduling.},
  archive      = {J_COMJNL},
  author       = {Gokuldhev, M and Singaravel, G},
  doi          = {10.1093/comjnl/bxaa053},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {382-395},
  shortjournal = {Comput. J.},
  title        = {Local pollination-based moth search algorithm for task-scheduling heterogeneous cloud environment},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sclera recognition based on efficient sclera segmentation
and significant vessel matching. <em>COMJNL</em>, <em>65</em>(2),
371–381. (<a href="https://doi.org/10.1093/comjnl/bxaa051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visible blood vein in sclera has unique characteristics for each person, and it can be captured in visible light conditions. Therefore, the sclera vessels have been used as an important feature to improve the performance of identification recognition systems, for example, mobile payment and phone encryption system. But recent studies show that unsatisfactory sclera segmentation and tedious matching process are the two critical issues to degrade the performance of sclera identification system. In this paper, we propose a robust sclera recognition system with high accuracy and efficiency. First, a novel sclera segmentation method that provides adaptive threshold is proposed. Second, we have designed an improved vessels enhancement method based on the local grey distribution and local texture information, and least square linearization and moment invariants are utilized to extract features. Finally, an efficient matching strategy is put forward based on the detected significant vessels. The experimental results on one database with images captured in colour illumination prove that the proposed sclera recognition method has an obvious improvement regarding efficiency and accuracy over other sclera recognition systems.},
  archive      = {J_COMJNL},
  author       = {Xu, Dong and Dong, Wei and Zhou, Han},
  doi          = {10.1093/comjnl/bxaa051},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {371-381},
  shortjournal = {Comput. J.},
  title        = {Sclera recognition based on efficient sclera segmentation and significant vessel matching},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Palmprint for individual’s personality behavior analysis.
<em>COMJNL</em>, <em>65</em>(2), 355–370. (<a
href="https://doi.org/10.1093/comjnl/bxaa045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Palmprint is an important key player in biometric family and also informs some extra basic personality details of an individual. In this paper, we utilize these extra information and designed an automated mobile vision (MV) system to extract principal lines from human palm and analyze them for behavioral significances. Hence, the main concern of this paper is to come up with a simple yet powerful low-level MV solution to extract the complex challenging features from palmprint. In the proposed system, the computational tasks are offloaded to a dedicated palmistry server and efficiently minimizes the energy consumption of mobile device after performing some preliminary computational low-level tasks. The implementation is divided into four major phases: (i) hand-image acquisition and pre-processing, (ii) region-of-interest extraction from the palm images, (iii) post-processing to extract principal lines and (iv) features computation for behavior analysis. The basic palmistry uses line lengths, angles, curves and branches to identify a person’s behavior. The exhaustive experiments show that the proposed system achieves an average accuracy of 96\%, 92\% and 84\% for heart, life and head line detection and personality prediction, respectively. Finally, mapping the extracted results with the original palmprint is augmented back to the use for better visualization.},
  archive      = {J_COMJNL},
  author       = {Prasad, Shitala and Chai, Tingting},
  doi          = {10.1093/comjnl/bxaa045},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {355-370},
  shortjournal = {Comput. J.},
  title        = {Palmprint for individual’s personality behavior analysis},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Review-based recommender systems: A proposed rating
prediction scheme using word embedding representation of reviews.
<em>COMJNL</em>, <em>65</em>(2), 345–354. (<a
href="https://doi.org/10.1093/comjnl/bxaa044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems nowadays play an important role in providing helpful information for users, especially in ecommerce applications. Many of the proposed models use rating histories of the users in order to predict unknown ratings. Recently, users’ reviews as a valuable source of knowledge have attracted the attention of researchers in this field and a new category denoted as review-based recommender systems has emerged. In this study, we make use of the information included in user reviews as well as available rating scores to develop a review-based rating prediction system. The proposed scheme attempts to handle the uncertainty problem of the rating histories, by fuzzifying the given ratings. Another advantage of the proposed system is the use of a word embedding representation model for textual reviews, instead of using traditional models such as binary bag of words and TFIDF ; vector space. It also makes use of the helpfulness voting scores, in order to prune data and achieve better results. The effectiveness of the rating prediction scheme as well as the final recommender system was evaluated against the Amazon dataset. Experimental results revealed that the proposed recommender system outperforms its counterparts and can be used as a suitable tool in ecommerce environments.},
  archive      = {J_COMJNL},
  author       = {Hasanzadeh, S and Fakhrahmad, S M and Taheri, M},
  doi          = {10.1093/comjnl/bxaa044},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {345-354},
  shortjournal = {Comput. J.},
  title        = {Review-based recommender systems: A proposed rating prediction scheme using word embedding representation of reviews},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep extreme learning machine-based optical character
recognition system for nastalique urdu-like script languages.
<em>COMJNL</em>, <em>65</em>(2), 331–344. (<a
href="https://doi.org/10.1093/comjnl/bxaa042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical character recognition systems convert printed or handwritten scripts into digital text formats like ASCII or UNICODE. Urdu-like script languages like Urdu, Punjabi and Sindhi are widely spoken languages of the world, especially in Asia. An enormous amount of printed and handwritten text of such languages exist, which needs to be converted into computer-understandable formats for knowledge extraction. In this study, extreme learning machine’s (ELM’s) most recently proposed variant called deep extreme learning machine (DELM)-based optical character recognition (OCR) system is proposed to enhance Urdu-like script language’s character recognition rate. The proposed DELM-based character recognition model is optimizing the OCR process by reducing the overhead of Pre-processing, Segmentation and Feature Extraction Layer. The proposed system evaluations accomplished 98.75\% training accuracy with 1.492 × 10; RMSE and 98.12\% testing accuracy with 1.587 × 10; RMSE, with six DELM hidden layers. The results show that the proposed system has attained the foremost recognition rate as compared to any previously proposed Urdu-like script language OCR system. This technique is applicable for machine-printed text and fractionally useful for handwritten text as well. This study will aid in the advancement of more accurate Urdu-like script OCR’s software systems in the future.},
  archive      = {J_COMJNL},
  author       = {Rizvi, Syed Saqib Raza and Khan, Muhammad Adnan and Abbas, Sagheer and Asadullah, Muhammad and Anwer, Nida and Fatima, Areej},
  doi          = {10.1093/comjnl/bxaa042},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {331-344},
  shortjournal = {Comput. J.},
  title        = {Deep extreme learning machine-based optical character recognition system for nastalique urdu-like script languages},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the behaviour of p-adic scaled space filling curve
indices for high-dimensional data. <em>COMJNL</em>, <em>65</em>(2),
310–330. (<a href="https://doi.org/10.1093/comjnl/bxaa036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Space filling curves are widely used in computer science. In particular, Hilbert curves and their generalizations to higher dimension are used as an indexing method because of their nice locality properties. This article generalizes this concept to the systematic construction of ; -adic versions of Hilbert curves based on special affine transformations of the ; -adic Gray code and develops a scaled indexing method for data taken from high-dimensional spaces based on these new curves, which with increasing dimension is shown to be less space consuming than the optimal standard static Hilbert curve index. A measure is derived, which allows to assess the local sparsity of a dataset, and is tested on some real-world data.},
  archive      = {J_COMJNL},
  author       = {Bradley, Patrick Erik and Jahn, Markus Wilhelm},
  doi          = {10.1093/comjnl/bxaa036},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {310-330},
  shortjournal = {Comput. J.},
  title        = {On the behaviour of p-adic scaled space filling curve indices for high-dimensional data},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Auto-scale resource provisioning in IaaS clouds.
<em>COMJNL</em>, <em>65</em>(2), 297–309. (<a
href="https://doi.org/10.1093/comjnl/bxaa030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users of cloud computing technology can lease resources instead of spending an excessive charge for their ownership. For service delivery in the infrastructure-as-a-service model of the cloud computing paradigm, virtual machines (VMs) are created by the hypervisor. This software is installed on a bare-metal server, called the host, and acted as a broker between the hardware of the host and its VMs. The host is responsible for the allocation of required resources, such as CPU, RAM and network bandwidth, for VMs. Therefore, allocating resources to a VM is equivalent to finding the location of the VM on the hosts. In this paper, we propose a model for resource allocation of a datacenter that includes clusters of hosts. This model is based on the birth–death process of queueing systems and continuous-time Markov chains. We will focus on RAM-intensive VMs and consider the allocation of RAM for a VM as a job in the queueing systems. The purpose of this modeling is to keep the number of running hosts minimum while guaranteeing the quality of service in terms of response. When the utilization of active hosts reaches a predefined threshold value, a new host is added to prevent response time violation, and when host utilization is reduced to a certain threshold, one of the hosts can be deactivated. The experimental results show that, in the long run, the odds of working with more jobs are increased.},
  archive      = {J_COMJNL},
  author       = {Salmanian, Zolfaghar and Izadkhah, Habib and Isazadeh, Ayaz},
  doi          = {10.1093/comjnl/bxaa030},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {297-309},
  shortjournal = {Comput. J.},
  title        = {Auto-scale resource provisioning in IaaS clouds},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). “Will i regret for this tweet?”—twitter user’s behavior
analysis system for private data disclosure. <em>COMJNL</em>,
<em>65</em>(2), 275–296. (<a
href="https://doi.org/10.1093/comjnl/bxaa027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twitter is an extensively used micro-blogging site for publishing user’s views on recent happenings. This wide reachability of messages over large audience poses a threat, as the degree of personally identifiable information disclosed might lead to user regrets. The Tweet-Scan-Post system scans the tweets contextually for sensitive messages. The tweet repository was generated using cyber-keywords for personal, professional and health tweets. The Rules of Sensitivity and Contextuality was defined based on standards established by various national regulatory bodies. The naive sensitivity regression function uses the Bag-of-Words model built from short text messages. The imbalanced classes in dataset result in misclassification with 25\% of sensitive and 75\% of insensitive tweets. The system opted stacked classification to combat the problem of imbalanced classes. The system initially applied various state-of-art algorithms and predicted 26\% of the tweets to be sensitive. The proposed stacked classification approach increased the overall proportion of sensitive tweets to 35\%. The system contributes a vocabulary set of 201 Sensitive Privacy Keyword using the boosting approach for three tweet categories. Finally, the system formulates a sensitivity scaling called TSP’s Tweet Sensitivity Scale based on Senti-Cyber features composed of Sensitive Privacy Keywords, Cyber-keywords with Non-Sensitive Privacy Keywords and Non-Cyber-keywords to detect the degree of disclosed sensitive information.},
  archive      = {J_COMJNL},
  author       = {Geetha, R and Karthika, S and Kumaraguru, Ponnurangam},
  doi          = {10.1093/comjnl/bxaa027},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {275-296},
  shortjournal = {Comput. J.},
  title        = {‘Will i regret for this Tweet?’—Twitter user’s behavior analysis system for private data disclosure},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhanced interactive performance of zoom-in/out gestures
using electrostatic tactile feedback on touchscreens. <em>COMJNL</em>,
<em>65</em>(2), 261–274. (<a
href="https://doi.org/10.1093/comjnl/bxaa026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tactile feedback added to touchscreens provides users with a high-quality interactive experience. The effect of tactile feedback on typical interaction gestures requires to be evaluated. With a custom-designed electrostatic tactile feedback device, we explore the effects of tactile feedback on zoom-in/out gestures and determine the issues satisfied by the relationship between completion time (CT) and index of difficulty (ID). Specifically, we compare the effect of electrostatic tactile feedback on the efficiency and accuracy of zoom-in/out gestures in three conditions, that is, no tactile feedback, linearly increasing tactile feedback force over operation process, and tactile feedback only in a target area. Then, we study the relationship between CT and ID with tactile feedback added to the target area. Results of experimental data from 12 participants show that tactile feedback added only to a target area can significantly increase operational efficiency and accuracy of zoom-in/out gestures. Furthermore, the relationship between CT and ID agrees well with Fitts’ law, and the correlation coefficient is larger than 0.9.},
  archive      = {J_COMJNL},
  author       = {Wang, Qinglong and Sun, Xiaoying and Cao, Dekun and Liu, Guohong},
  doi          = {10.1093/comjnl/bxaa026},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {261-274},
  shortjournal = {Comput. J.},
  title        = {Enhanced interactive performance of zoom-In/Out gestures using electrostatic tactile feedback on touchscreens},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Clustering-evolutionary random support vector machine
ensemble for fMRI-based asperger syndrome diagnosis. <em>COMJNL</em>,
<em>65</em>(2), 251–260. (<a
href="https://doi.org/10.1093/comjnl/bxaa023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a hot spot in the field of computer application to diagnose complex brain diseases such as Asperger syndrome (AS) using machine learning technology. To identify AS patients and detect lesions, this paper proposes a novel clustering-evolutionary random support vector machine (SVM) ensemble (CERSVME) based on graph theory. Firstly, we extract graph theory indexes from the resting-state functional magnetic resonance imaging (fMRI) data as sample features and construct an ensemble learner by integrating multiple SVM classifiers. Secondly, the base learners with high redundancy and poor classification ability are deleted through clustering evolutions to improve the performance of the model. Then the CERSVME model is used to classify fMRI image of AS patients and healthy controls. According to the classification results, a multi-stage analysis scheme is designed to find the AS-related brain areas. We validate the proposed approach on 135 participants from autism brain imaging data exchange cohort. The highest accuracy reported by the CERSVME reaches 95.24\%. More importantly, the diseased brain areas such as middle frontal gyrus, hippocampus and precuneus are found based on their contributions to classification performances of the CERSVME. Our study provides useful assistances for the clinical detection of patients with AS.},
  archive      = {J_COMJNL},
  author       = {Bi, Xia-an and Wu, Hao and Hu, Xi and Fu, Yu and Peng, Shaoliang},
  doi          = {10.1093/comjnl/bxaa023},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {251-260},
  shortjournal = {Comput. J.},
  title        = {Clustering-evolutionary random support vector machine ensemble for fMRI-based asperger syndrome diagnosis},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Early diabetes discovery from tongue images.
<em>COMJNL</em>, <em>65</em>(2), 237–250. (<a
href="https://doi.org/10.1093/comjnl/bxaa022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical imaging shows the internal structures hidden in the skin and bone to diagnose the disease. Diabetes mellitus (DM) is a metabolic disorder that causes high blood sugar levels due to the inadequate secretion of insulin or the body does not respond properly to the secreted insulin. This paper proposes a non-invasive method to detect DM at an early stage based on the physiognomy extracted from tongue images. The tongue extends to identify the disease of a human body. However, unpredictable response of the human body parts such as the stomach, pancreas, liver and intestines revert in the tongue. The changes in the tongue ensure the dereliction of the internal organs of the human being. The changes are difference in the color and surface of the tongue. Processing of tongue image is done by fractional order Darwinian particle swarm optimization (FODPSO) algorithm. The system framework involves obtaining the image, alluring of the image, identifying the texture and color feature and finally classified as normal or diabetic. In this paper, the authors propose to diagnose DM at an early stage from tongue digital image. The tongue image is acquired and processed with FODPSO to extract edge and texture features. Tongue reflects and diagnoses diabetes in a person.},
  archive      = {J_COMJNL},
  author       = {Naveed, Safia and G, Geetha and S, Leninisha},
  doi          = {10.1093/comjnl/bxaa022},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {237-250},
  shortjournal = {Comput. J.},
  title        = {Early diabetes discovery from tongue images},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). International expansion selection model by machine
learning—a proprietary model. <em>COMJNL</em>, <em>65</em>(2), 217–236.
(<a href="https://doi.org/10.1093/comjnl/bxaa018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to explain a simple but crucial complex problem often faced by multinational enterprises: why multinational companies choose to enter the markets of certain countries. Accordingly, this study developed an international expansion selection model by using the machine learning method. The priority targets for enterprises’ international expansion and the strategic country groups for classification can be identified on the basis of ideas expressed in three primary business concepts, namely ‘market attractiveness’, ‘enterprise’ resources and capabilities’ and ‘customer-oriented approach’; the identified priority targets and strategic country groups are useful for multinational enterprises when designing different configurations for limited resources and can ultimately assist the business managers with making international business decisions. Models can elucidate the complexity behind enterprise decisions. By contrast, strategic grouping based on simple rules can aid the managers to make instantaneous decisions and respond according to the changing market. This study constructed an exclusive strategic model based on the international expansion strategy selection modes adopted by a leading Taiwan enterprise in electronics industry and the unique characteristics possessed by this enterprise.},
  archive      = {J_COMJNL},
  author       = {Hsieh, Ping-Chi and Horng, Der-Juinn and Chang, Hong-Yi},
  doi          = {10.1093/comjnl/bxaa018},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {217-236},
  shortjournal = {Comput. J.},
  title        = {International expansion selection model by machine Learning—A proprietary model},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Small target recognition using dynamic time warping and
visual attention. <em>COMJNL</em>, <em>65</em>(2), 203–216. (<a
href="https://doi.org/10.1093/comjnl/bxaa015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microaneurysm is a kind of small targets in color retinal image, and it is an essential work to recognize the small target for the early diagnosis of diabetic retinopathy. This paper proposes an efficient method to accurately recognize microaneurysm. A symmetric extended curvature Gabor wavelet is presented to generate candidate objects, where some novel features are extracted for classification. A kind of statistic features is generated to distinguish between microaneurysm and thin vessels, in terms of the shape similarity of cross-section profiles. Furthermore, the visual attention-based features are proposed to compute local contrast of small targets in complex background. Random undersampling with AdaBoost (RUSBoost) classifier is employed to discriminate true microaneurysm from an overwhelming amount of candidate objects. Experimental results demonstrate that the proposed method achieves significant sensitivity and accuracy on the public datasets, in comparison to the state-of-the-arts.},
  archive      = {J_COMJNL},
  author       = {Zhang, Xinpeng and Wu, Jigang and Meng, Min},
  doi          = {10.1093/comjnl/bxaa015},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {203-216},
  shortjournal = {Comput. J.},
  title        = {Small target recognition using dynamic time warping and visual attention},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Appraisal of two arabic opinion summarization methods:
Statistical versus machine learning. <em>COMJNL</em>, <em>65</em>(2),
192–202. (<a href="https://doi.org/10.1093/comjnl/bxaa007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose to overcome the challenge of digesting opinions in a news article. Our objective is to provide a summary of opinions delivered by many sources about a main topic in an Arabic news article. In literature, several studies addressed issues related to opinion summarization. However, we noticed a lack of studies that address this problem in Arabic language. So, we have proposed two different methods: multi-criteria and machine learning-based methods. We proceed by comparing the results provided by the proposed methods for opinionated sentence extraction. The proposed methods were evaluated using two feature types: text-based features and opinion-specific features. Experimental results show the robustness of machine learning method to extract opinionated sentences with consideration of two sets of features.},
  archive      = {J_COMJNL},
  author       = {Touati, Imen and Ellouze, Mariem and Graja, Marwa and Hadrich Belguith, Lamia},
  doi          = {10.1093/comjnl/bxaa007},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {192-202},
  shortjournal = {Comput. J.},
  title        = {Appraisal of two arabic opinion summarization methods: Statistical versus machine learning},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Classification model on big data in medical diagnosis based
on semi-supervised learning. <em>COMJNL</em>, <em>65</em>(2), 177–191.
(<a href="https://doi.org/10.1093/comjnl/bxaa006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data in medical diagnosis can provide abundant value for clinical diagnosis, decision support and many other applications, but obtaining a large number of labeled medical data will take a lot of time and manpower. In this paper, a classification model based on semi-supervised learning algorithm using both labeled and unlabeled data is proposed to process big data in medical diagnosis, which includes structured, semi-structured and unstructured data. For the medical laboratory data, this paper proposes a self-training algorithm based on repeated labeling strategy to solve the problem that mislabeled samples weaken the performance of classifiers. Aiming at medical record data, this paper extracts features with high correlation of classification results based on domain expert knowledge base first, and then chooses the unlabeled medical record data with the highest confidence to expand the training set and optimizes the performance of the classifiers of tri-training algorithm, which uses supervised learning algorithm to train three basic classifiers. The experimental results show that the proposed medical diagnosis data classification model based on semi-supervised learning algorithm has good performance.},
  archive      = {J_COMJNL},
  author       = {Wang, Lei and Qian, Qing and Zhang, Qiang and Wang, Jishuai and Cheng, Wenbo and Yan, Wei},
  doi          = {10.1093/comjnl/bxaa006},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {177-191},
  shortjournal = {Comput. J.},
  title        = {Classification model on big data in medical diagnosis based on semi-supervised learning},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatiotemporal clustering and analysis of road accident
hotspots by exploiting GIS technology and kernel density estimation.
<em>COMJNL</em>, <em>65</em>(2), 155–176. (<a
href="https://doi.org/10.1093/comjnl/bxz158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic accidents are a common problem in any transportation network. Road traffic accidents are predicted to be the seventh leading cause of deaths by the year 2030. Recently research in the integration of geographical information systems (GIS) for analyzing accidents, road design and safety management has increased considerably. The perpetual use of GIS tools, lead this study to propose the identification of accident hotspots by exploiting GIS technology coupled with kernel density estimation (KDE). This paper proposes the use of KDE technique and GIS technology to automatically identify the accident hotspots using UK as the study area. Analysis shows that most of the accidents occur when there is a 30 mph speed limit, a weekend, in the evening time, during the months of October and November, on the single carriageway, where there is ‘T’ or staggered junction and on ‘A’ road class. Moreover, this study also proposed techniques to classify the accident severity that is classified as either fatal, serious or slight. The driver behavior and environmental features achieved an accuracy up to 85\% on the severity classification with Bagging technique. Further, the shortcomings, limitations and recommendations for future work are also identified.},
  archive      = {J_COMJNL},
  author       = {Kazmi, Syed Saqib Ali and Ahmed, Mehreen and Mumtaz, Rafia and Anwar, Zahid},
  doi          = {10.1093/comjnl/bxz158},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {155-176},
  shortjournal = {Comput. J.},
  title        = {Spatiotemporal clustering and analysis of road accident hotspots by exploiting GIS technology and kernel density estimation},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performance evaluation of learning models for identification
of suicidal thoughts. <em>COMJNL</em>, <em>65</em>(1), 139–154. (<a
href="https://doi.org/10.1093/comjnl/bxab060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The suicidal death rate is growing rapidly. Depression and stress levels among the people have increased significantly, which is considered to be a risk factor for suicidal thoughts. Social media is gradually more popular and people use them for sharing their sentiments and harmful emotions related to suicidal thoughts. An effective approach is required to investigate for identifying risk factors associated with suicide on social media. The objective is to propose some learning models to evaluate social media data to identify persons having suicidal tendencies. A large data consisting of 8452 tweets are collected from Twitter, pre-processed and bags of words were applied. Different machine learning and deep learning algorithms such as Random Forest, Decision Tree, Bernoulli Naïve Bayes, Multinomial Naïve Bayes, Recurrent Neural Network, Artificial Neural Network and Long Short Term Memory were applied for classifying the tweets in two sets: suicidal and non-suicidal. The performance of these learning models is further evaluated on three parameters: accuracy, precision and recall. These models have shown significant results on the parameters.},
  archive      = {J_COMJNL},
  author       = {Chadha, Akshma and Kaushik, Baijnath},
  doi          = {10.1093/comjnl/bxab060},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {139-154},
  shortjournal = {Comput. J.},
  title        = {Performance evaluation of learning models for identification of suicidal thoughts},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved hybrid bag-boost ensemble with k-means-SMOTE–ENN
technique for handling noisy class imbalanced data. <em>COMJNL</em>,
<em>65</em>(1), 124–138. (<a
href="https://doi.org/10.1093/comjnl/bxab039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A class imbalance problem plays a vital role while dealing with classes with rare number of instances. Noisy class imbalanced datasets create considerable effect on the machine learning classification of classes. Data resampling techniques commonly used for handling class imbalance problem show insignificant behavior in noisy imbalanced datasets. To cure curse of data resampling technique in noisy class imbalanced data, we have proposed improved hybrid bag-boost with proposed resampling technique model. This model contains proposed resampling technique used for handling noisy imbalanced datasets. Proposed resampling technique comprises K-Means SMOTE (Synthetic Minority Oversampling TEchnique) as an oversampling technique and edited nearest neighbor (ENN) undersampling technique used as noise removal. This resampling technique is used to mitigate noise in imbalanced datasets at three levels, i.e. first clusters datasets using K-Means clustering technique, SMOTE inside clusters for handling imbalance by inducing synthetic instances of class in minority and lastly, using ENN technique to remove instances that create noise afterwards. Experiments were performed using 11 binary imbalanced datasets by varying attribute noise percentages, and by using area under receiver operating curve as performance metrics. Experimental results confirmed that proposed model shows better results than the rest. Moreover, it is also confirmed that proposed technique performs better with an increased noise percentage in binary imbalanced datasets.},
  archive      = {J_COMJNL},
  author       = {Puri, Arjun and Kumar Gupta, Manoj},
  doi          = {10.1093/comjnl/bxab039},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {124-138},
  shortjournal = {Comput. J.},
  title        = {Improved hybrid bag-boost ensemble with K-means-SMOTE–ENN technique for handling noisy class imbalanced data},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting cognitive features of videos using EEG signal.
<em>COMJNL</em>, <em>65</em>(1), 105–123. (<a
href="https://doi.org/10.1093/comjnl/bxaa180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) emerged as a highly relevant signal to human emotion, brain diagnosing and brain–computer interfaces (BCI) applications. In this paper, the EEG signal is used to evaluate the cognitive response of subjects during watching test video clips. The measurements are performed with 25 subjects using eight channels while simultaneously running the video clips. The ; and ; waves of the EEG signal are used to extract the features that represent the evoked activity in each group of frames using the Peak-Over-Threshold (POT) technique. Significant EEG patterns are derived from the time-correlated measurements, which can be related to the subjects’ interests. In addition, the conjunctions that represent the occurrence of segments-of-interest in more than one channel are determined. The results show that ~15\% of the segments attracted the attention of the viewers in each test video clip. Such a technique can potentially be implemented in neuromarketing analysis or to develop a new video compression technique that depends on the human cognitive system.},
  archive      = {J_COMJNL},
  author       = {Qananwah, Qasem and Alqudah, Ali Mohammad and Alodat, Moh’d and Dagamseh, Ahmad and Hayden, Oliver},
  doi          = {10.1093/comjnl/bxaa180},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {105-123},
  shortjournal = {Comput. J.},
  title        = {Detecting cognitive features of videos using EEG signal},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated detection of oral pre-cancerous tongue lesions
using deep learning for early diagnosis of oral cavity cancer.
<em>COMJNL</em>, <em>65</em>(1), 91–104. (<a
href="https://doi.org/10.1093/comjnl/bxaa136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering oral cavity cancer (OCC) at an early stage is an effective way to increase patient survival rate. However, current initial screening process is done manually and is expensive for the average individual, especially in developing countries worldwide. This problem is further compounded due to the lack of specialists in such areas. Automating the initial screening process using artificial intelligence (AI) to detect pre-cancerous lesions can prove to be an effective and inexpensive technique that would allow patients to be triaged accordingly to receive appropriate clinical management. In this study, we have applied and evaluated the efficacy of six deep convolutional neural network (DCNN) models using transfer learning, for identifying pre-cancerous tongue lesions directly using a small dataset of clinically annotated photographic images to diagnose early signs of OCC. DCNN models were able to differentiate between benign and pre-cancerous tongue lesions and were also able to distinguish between five types of tongue lesions, i.e. hairy tongue, fissured tongue, geographic tongue, strawberry tongue and oral hairy leukoplakia with high classification performances. Preliminary results using an (AI + Physician) ensemble model demonstrate that an automated pre-screening process of oral tongue lesions using DCNNs can achieve ‘near-human’ level classification performance for diagnosing early signs of OCC in patients.},
  archive      = {J_COMJNL},
  author       = {Shamim, Mohammed Zubair M and Syed, Sadatullah and Shiblee, Mohammad and Usman, Mohammed and Ali, Syed Jaffar and Hussein, Hany S and Farrag, Mohammed},
  doi          = {10.1093/comjnl/bxaa136},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {91-104},
  shortjournal = {Comput. J.},
  title        = {Automated detection of oral pre-cancerous tongue lesions using deep learning for early diagnosis of oral cavity cancer},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enriching topic coherence on reviews for cross-domain
recommendation. <em>COMJNL</em>, <em>65</em>(1), 80–90. (<a
href="https://doi.org/10.1093/comjnl/bxaa008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of e-commerce sites and social media, users express their preferences and tastes freely through user-generated content such as reviews and comments. In order to promote cross-selling, e-commerce sites such as eBay and Amazon regularly use such inputs from multiple domains and suggest items with which users may be interested. In this paper, we propose a topic coherence-based cross-domain recommender model. The core concept is to use topic modeling to extract topics from user-generated content such as reviews and combine them with reliable semantic coherence techniques to link different domains, using Wikipedia as a reference corpus. We experiment with different topic coherence methods such as pointwise mutual information (PMI) and explicit semantic analysis (ESA). Experimental results presented demonstrate that our approach, using PMI as topic coherence, yields 22.6\% and using ESA yields 54.4\% higher precision as compared with cross-domain recommender system based on semantic clustering.},
  archive      = {J_COMJNL},
  author       = {Saraswat, Mala and Chakraverty, Shampa},
  doi          = {10.1093/comjnl/bxaa008},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {80-90},
  shortjournal = {Comput. J.},
  title        = {Enriching topic coherence on reviews for cross-domain recommendation},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fog–cloud assisted IoT-based hierarchical approach for
controlling dengue infection. <em>COMJNL</em>, <em>65</em>(1), 67–79.
(<a href="https://doi.org/10.1093/comjnl/bxaa005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past five decades have witnessed the unprecedented contribution of arboviral diseases towards global morbidity and disability. It is primarily attributed due to unplanned urbanization, population explosion and globalization. Out of these, dengue is considered the most important arboviral disease because of its predominant growth in the past. The presented study explores the immense potential of Internet of things (IoT), fog and cloud computing for providing technology-based healthcare solutions for dengue virus (DENV) infection. In this paper, a hierarchical healthcare computing system for controlling DENV infection using fog–cloud-assisted IoT is proposed. This system provides a real-time remote diagnosis of DENV infection in individuals and monitors and predicts their health sensitivity during its infection period. The system uses fog computing to diagnose the DENV infection status of the individuals using ; -means clustering and generates immediate diagnostic alerts to individuals, at the fog layer. Furthermore, the system uses cloud computing to monitor and predict the probabilistic health sensitivity of the DENV-infected individuals using Bayesian belief network and artificial neural network, respectively, at the cloud layer. The prediction of health sensitivity in the proposed system helps the infected individuals and healthcare agencies in determining the health vulnerability of DENV-infected individuals and preventing severe or permanent health losses in the future. The proposed system is experimentally evaluated using well-defined approaches, which conform to its validity and applicability. The results obtained from the experimental evaluations of the proposed system acknowledge the performance superiority and high efficiency of the system in delivering DENV-related healthcare services in real time.},
  archive      = {J_COMJNL},
  author       = {Sood, Sandeep Kumar and Sood, Vaishali and Mahajan, Isha and Sahil, Sahil},
  doi          = {10.1093/comjnl/bxaa005},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {67-79},
  shortjournal = {Comput. J.},
  title        = {Fog–Cloud assisted IoT-based hierarchical approach for controlling dengue infection},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sentiment classification using two effective optimization
methods derived from the artificial bee colony optimization and
imperialist competitive algorithm. <em>COMJNL</em>, <em>65</em>(1),
18–66. (<a href="https://doi.org/10.1093/comjnl/bxz163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial bee colony (ABC) optimization and imperialist competitive algorithm (ICA) are two famous metaheuristic methods. In ABC, exploration is good because each bee moves toward random neighbors in the first and second phases. In ABC, exploitation is poor because it does not try to examine a promising region of search space carefully to see if it contains a good local minimum. In this study, ICA is considered to improve ABC exploitation, and two novel swarm-based hybrid methods called ABC–ICA and ABC–ICA1 are proposed, which combine the characteristics of ABC and ICA. The proposed methods improve the evaluations results in both continuous and discrete environments compared to the baseline methods. The second method improves the first optimization method as well. Feature selection can be considered to be an optimization problem because selecting the appropriate feature subset is very important and the action of appropriate feature selection has a great influence on the efficiency of classifier algorithms in supervised methods. Therefore, to focus on feature selection is a key issue and is very important. In this study, different discrete versions of the proposed methods have been introduced that can be used in feature selection and feature scoring problems, which have been successful in evaluations. In this study, a problem called cold start is introduced, and a solution is presented that has a great impact on the efficiency of the proposed methods in feature scoring problem. A total of 16 UCI data sets and 2 Amazon data sets have been used for the evaluation of the proposed methods in feature selection problem. The parameters that have been compared are classification accuracy and the number of features required for classification. Also, the proposed methods can be used to create a proper sentiment dictionary. Evaluation results confirm the better performance of the proposed methods in most experiments.},
  archive      = {J_COMJNL},
  author       = {Osmani, Amjad and Mohasefi, Jamshid Bagherzadeh and Gharehchopogh, Farhad Soleimanian},
  doi          = {10.1093/comjnl/bxz163},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {18-66},
  shortjournal = {Comput. J.},
  title        = {Sentiment classification using two effective optimization methods derived from the artificial bee colony optimization and imperialist competitive algorithm},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SIFT-based visual tracking using optical flow and belief
propagation algorithm. <em>COMJNL</em>, <em>65</em>(1), 1–17. (<a
href="https://doi.org/10.1093/comjnl/bxz155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Perceptible visual tracking acts as an important module for distinct perception tasks of autonomous robots. Better features help in easier decision-making process. The evaluation of tracking objects, dynamic positions and their visual information in results are quite difficult tasks. Until now, most real-time visual tracking algorithms suffer from poor robustness and low occurrence as they deal with complex real-world data. In this paper, we have proposed more robust and faster visual tracking framework using scale invariant feature transform (SIFT) and the optical flow in belief propagation (BF) algorithm for efficient processing in real scenarios. Here, a new feature-based optical flow along with BF algorithm is utilized to compute the affine matrix of a regional center on SIFT key points in frames. Experimental results depict that the proposed approach is more efficient and more robust in comparison with the state-of-the-art tracking algorithms with more complex scenarios.},
  archive      = {J_COMJNL},
  author       = {Biswas, Biswajit and Kr Ghosh, Swarup and Hore, Moumita and Ghosh, Anupam},
  doi          = {10.1093/comjnl/bxz155},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Comput. J.},
  title        = {SIFT-based visual tracking using optical flow and belief propagation algorithm},
  volume       = {65},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
