<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJSEKE_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijseke---70">IJSEKE - 70</h2>
<ul>
<li><details>
<summary>
(2022). SWEBOK matters: Report and reflection of a SEKE panel on the
educational and professional implications of SWEBOK. <em>IJSEKE</em>,
<em>32</em>(11n12), 1771–1781. (<a
href="https://doi.org/10.1142/S0218194022500590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a summary of the proceedings of a panel on the Guide to the Software Engineering Body of Knowledge (SWEBOK) held under the auspices of the Thirty-Fourth International Conference on Software Engineering and Knowledge Engineering (SEKE 2022), as well as the discussion that ensued thereafter among the panelists. In this regard, a synopsis of the underlying motivation and structure of SWEBOK is given, and the means for integrating SWEBOK in software-intensive organizations and educational institutions offering software engineering-related courses are highlighted and illustrated by pedagogically-oriented examples.},
  archive      = {J_IJSEKE},
  author       = {Pankaj Kamthan and Hironori Washizaki},
  doi          = {10.1142/S0218194022500590},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1771-1781},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {SWEBOK matters: Report and reflection of a SEKE panel on the educational and professional implications of SWEBOK},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FAC: A music recommendation model based on fusing audio and
chord features (115). <em>IJSEKE</em>, <em>32</em>(11n12), 1753–1770.
(<a href="https://doi.org/10.1142/S0218194022500577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Music content has recently been identified as useful information to promote the performance of music recommendations. Existing studies usually feed low-level audio features, such as the Mel-frequency cepstral coefficients, into deep learning models for music recommendations. However, such features cannot well characterize music audios, which often contain multiple sound sources. In this paper, we propose to model and fuse chord, melody, and rhythm features to meaningfully characterize the music so as to improve the music recommendation. Specially, we use two user-based attention mechanisms to differentiate the importance of different parts of audio features and chord features. In addition, a Long Short-Term Memory layer is used to capture the sequence characteristics. Those features are fused by a multilayer perceptron and then used to make recommendations. We conducted experiments with a subset of the last.fm-1b dataset. The experimental results show that our proposal outperforms the best baseline by 3 . 5 2 % on HR@10.},
  archive      = {J_IJSEKE},
  author       = {Weite Feng and Junrui Liu and Tong Li and Zhen Yang and Di Wu},
  doi          = {10.1142/S0218194022500577},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1753-1770},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {FAC: A music recommendation model based on fusing audio and chord features (115)},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parallel ensemble matching based on subscription
partitioning for content-based publish/subscribe systems.
<em>IJSEKE</em>, <em>32</em>(11n12), 1733–1752. (<a
href="https://doi.org/10.1142/S0218194022500619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The content-based publish/subscribe system is an effective paradigm for implementing on-demand event distribution. Each event needs to be matched against subscriptions to identify the target subscribers. To improve the matching performance, many novel data structures have been proposed. However, the predicates contained in subscriptions are handled the same way in most existing data structures, without considering their differences in matching probability. In this paper, we propose the concept of parallel ensemble matching (PEM) based on subscription partitioning. The basic idea is that we have the right algorithm handling the right subscriptions at the right time. First of all, we design a PEM framework by classifying subscriptions according to their matching probabilities and use the proper algorithms to process each subscription category. Furthermore, to deal with high-dimensional subscriptions, we propose a fine-grained PEM (fgPEM) that exploits matching algorithms with complementary behaviors by partitioning subscriptions into sub-subscriptions. We implement the prototype of PEM and fgPEM based on two existing algorithms. The experiment results show that PEM improves the matching performance by 43%. On the basis of PEM, fgPEM further improves the performance by 31%.},
  archive      = {J_IJSEKE},
  author       = {Junshen Li and Yufeng Deng and Shiyou Qian and Jian Cao and Guangtao Xue},
  doi          = {10.1142/S0218194022500619},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1733-1752},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Parallel ensemble matching based on subscription partitioning for content-based Publish/Subscribe systems},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving multi-class code readability classification with
an enhanced data augmentation approach (130). <em>IJSEKE</em>,
<em>32</em>(11n12), 1709–1731. (<a
href="https://doi.org/10.1142/S0218194022500656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Being a critical factor affecting the maintainability and reusability of the software, code readability is growing crucial in modern software development, where a metric for classifying code readability levels is both applicable and desired. However, most prior research has treated code readability classification as a binary classification task due to the lack of labeled data. To support the training of multi-class code readability classification models, we propose an enhanced data augmentation approach that could be used to generate sufficient readability data and well train a multi-class code readability model. The approach includes the use of domain-specific data transformation and GAN-based data augmentation. We conduct a series of experiments to verify our augmentation approach and gain a state-of-the-art multi-class code readability classification performance with 69.5% Micro-F1, 54.0% Macro-F1 and 67.7% Macro-AUC. Compared to the results where no augmented data is used, the improvements on Micro-F1, Macro-F1 and Macro-AUC are significant with 6.9%, 11.3% and 11.2%, respectively. As an innovative work of proposing multi-class code readability classification and an enhanced code readability data augmentation approach, our method is proved to be effective.},
  archive      = {J_IJSEKE},
  author       = {Qing Mi and Luo Wang and Lisha Hu and Liwei Ou and Yang Yu},
  doi          = {10.1142/S0218194022500656},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1709-1731},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Improving multi-class code readability classification with an enhanced data augmentation approach (130)},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MIAR: A context-aware approach for app review intention
mining. <em>IJSEKE</em>, <em>32</em>(11n12), 1689–1708. (<a
href="https://doi.org/10.1142/S0218194022500796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the highly competitive and dynamic mobile application (app) market, app developers need to release new versions regularly to improve existing features and provide new features for users. To accomplish the maintenance and evolution tasks more effectively and efficiently, app developers should collect and analyze user reviews, which contain a rich source of information from user perspective. Although there are many approaches based on intention mining that can automatically predict the intention of reviews for better understanding valuable information, those approaches are limited since contextual information of the whole review text may be lost. In this paper, we propose Mining Intention from App Reviews (MIAR), a novel deep learning model to predict the intention of app reviews automatically. We adopt a Contextual Feature Extractor to capture the context semantic information and fuse it with the local feature through a fusion mechanism. The experiment results demonstrate that MIAR has made significant improvement over the baseline approaches in Precision, Recall and F 1 -score evaluation metrics, achieving state-of-the-art performance in this task. Our model also performs well in other intention mining tasks, proving its generalization ability and robustness.},
  archive      = {J_IJSEKE},
  author       = {Jinwei Lu and Yimin Wu and Jiayan Pei and Zishan Qin and Shizhao Huang and Chao Deng},
  doi          = {10.1142/S0218194022500796},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1689-1708},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {MIAR: A context-aware approach for app review intention mining},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mapping modern JVM language code to analysis-friendly
graphs: A study with kotlin. <em>IJSEKE</em>, <em>32</em>(11n12),
1667–1688. (<a href="https://doi.org/10.1142/S0218194022500735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kotlin is a modern JVM language, gaining adoption rapidly and becoming Android official programming language. With its wide usage, the need for code analysis of Kotlin is increasing. Exposing code semantics explicitly with a properly structured format is the first step in code analysis and the construction of such representation is the foundation for downstream tasks. Recently, graph-based approaches became a promising way of encoding source code semantics. However, this work mainly focuses on representation learning with limited interpretability and shallow domain knowledge. The known evolvements of code semantics in new-generation programming languages have been overlooked. How to establish an effective mapping between naturally concise Kotlin source code with graph-based representation needs to be studied by analyzing known language features. Moreover, the feasibility of enhancing the mapping with code semantics automatically learned from the program needs to be explored. In this paper, we first propose a first-sight, rule-based mapping method, using composite representation with AST, CFG, DFG and language features. To examine the possibility of exposing code semantics in the mapped graph, we use Latent Semantic Indexing-based source code summarization to learn more features of each method, and then enrich the attributes of the corresponding node in the graph. We evaluate these mapping strategies with comparative experiments by simulating a code search solution as a downstream task. The experiment result shows that the graph-based method with built-in language features outperforms the text-based way without introducing greater complexity. Comparative experiments also prove that adding code semantics to the graph benefits the capacity of downstream tasks. When exploring the whole mapping process, our study explicitly revealed the practical barriers to extracting and exposing the hidden semantics from Kotlin source code, which may help enlighten source code representations for other modern languages.},
  archive      = {J_IJSEKE},
  author       = {Lu Li and Yan Liu},
  doi          = {10.1142/S0218194022500735},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1667-1688},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Mapping modern JVM language code to analysis-friendly graphs: A study with kotlin},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid model with multi-level code representation for
multi-label code smell detection (077). <em>IJSEKE</em>,
<em>32</em>(11n12), 1643–1666. (<a
href="https://doi.org/10.1142/S0218194022500723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code smell is an indicator of potential problems in a software design that have a negative impact on readability and maintainability. Hence, detecting code smells in a timely and effective manner can provide guides for developers in refactoring. Fortunately, many approaches like metric-based, heuristic-based, machine-learning-based and deep-learning-based have been proposed to detect code smells. However, existing methods, using the simple code representation to describe different code smells unilaterally, cannot efficiently extract enough rich information from source code. In addition, one code snippet often has several code smells at the same time and there is a lack of multi-label code smell detection based on deep learning. In this paper, we present a large-scale dataset for the multi-label code smell detection task since there is still no publicly sufficient dataset for this task. The release of this dataset would push forward the research in this field. Based on it, we propose a hybrid model with multi-level code representation to further optimize the code smell detection. First, we parse the code into the abstract syntax tree (AST) with control and data flow edges and the graph convolution network is applied to get the prediction at the syntactic and semantic level. Then we use the bidirectional long-short term memory network with attention mechanism to analyze the code tokens at the token-level in the meanwhile. Finally, we get the fusion prediction result of the models. Experimental results illustrate that our proposed model outperforms the state-of-the-art methods not only in single code smell detection but also in multi-label code smell detection.},
  archive      = {J_IJSEKE},
  author       = {Yichen Li and An Liu and Lei Zhao and Xiaofang Zhang},
  doi          = {10.1142/S0218194022500723},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1643-1666},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Hybrid model with multi-level code representation for multi-label code smell detection (077)},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). KEMA++: A full representative knowledge-graph embedding
model (036). <em>IJSEKE</em>, <em>32</em>(11n12), 1619–1641. (<a
href="https://doi.org/10.1142/S0218194022500760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, representing entities and relations in a machine understandable way through Knowledge graph embedding (KGE) has been proven as an effective approach for predicting missing links in knowledge graphs (KGs). Mainly, the success of such approach depends on the model ability to infer the patterns of the relations. Indeed, most of the existing KG models highly focus on modeling simple relation patterns such as symmetry, anti-symmetry, inversion, and composition. However, there are few models in the literature that take into consideration the modeling of complex relation patterns like 1- N , N -1 and N - N , which are common in real-world applications. To overcome this challenge, this paper presents a new KGE model called KEMA + + , i.e. KGE using Modular Arithmetic, that relies on the combination of projection and modular arithmetic. The main idea behind KEMA + + is to project the entities of a relation to represent the relations of a KG, before applying a modular arithmetic over it. Thus, KEMA + + will be able to infer all simple and complex relation patterns for any KGE applications. Through extensive experiments on several datasets, we demonstrated the relevance of KEMA + + in terms of effectively representing all model relations in a KG. Simulations on several tested datasets show that KEMA + + obtains the good scores for Mean Rank (MR) and Hits@1 tests. Moreover, KEMA + + obtains the good Hits@1 score compared to the existing models.},
  archive      = {J_IJSEKE},
  author       = {Hussein Baalbaki and Hussein Hazimeh and Hassan Harb and Rafael Angarita},
  doi          = {10.1142/S0218194022500760},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1619-1641},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {KEMA++: A full representative knowledge-graph embedding model (036)},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Making technical debt visible using hybrid sankey diagrams:
An industrial case study. <em>IJSEKE</em>, <em>32</em>(10), 1583–1616.
(<a href="https://doi.org/10.1142/S0218194022500632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Context: Technical debt (TD) is a challenge for companies who develop software on which their critical operations depend. To properly manage TD, it is necessary to make it visible to the different stakeholders involved to support informed decisions. Objective: To validate a TD visualization approach based on hybrid Sankey diagrams that makes the TD visible by showing (a) technical and business aspects, and (b) the flow of value and TD impacts. This approach regards visualizations as boundary objects. Method: We performed a multi-case study in a large multi-industry state-owned company. The objective was to validate the effectiveness of such visualizations and to explore their possible uses in TD management. We first used a retrospective case study on a TD decision-making scenario and, later, visualization usage scenarios using focus groups to evaluate its usefulness. Results: The results suggest that the proposed approach: (a) provides a structured process for systematic TD visualization to help the decision-making process; (b) enables the communication at knowledge boundaries between stakeholders to make informed decisions; (c) uses flow representations that are important for assessing the impact in multiple functional areas; and (d) enables documentation and reuse. Conclusion: The study results suggest that TD decision-making events can benefit from using our TD visualizations based on hybrid Sankey diagrams as boundary objects to portray the impact of TD in business, services, and technical aspects.},
  archive      = {J_IJSEKE},
  author       = {Alexia Pacheco and Gabriela Marín-Raventós and Rodrigo Spínola and Gustavo López and Carolyn Seaman},
  doi          = {10.1142/S0218194022500632},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1583-1616},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Making technical debt visible using hybrid sankey diagrams: An industrial case study},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approach to predict software vulnerability based on
multiple-level n-gram feature extraction and heterogeneous ensemble
learning. <em>IJSEKE</em>, <em>32</em>(10), 1559–1582. (<a
href="https://doi.org/10.1142/S0218194022500620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software vulnerabilities are one of the roots of computer security problems. The traditional static analysis and dynamic analysis methods based on software source code mainly have some deficiencies, such as high false positive rate, high false negative rate and insufficient semantic information captured. Nevertheless, the application of machine learning, Natural Language Processing and other technologies in software vulnerability prediction can effectively mitigate such issues. This paper proposed a vulnerability prediction method based on multiple-level N -gram feature extraction and heterogeneous ensemble learning. First, by code intermediate representation and constructing a multiple-level N -gram feature generation model, two kinds of N -gram semantic features with different window size and different granularity at word and char level were extracted to retain the semantic and structural information of code. Second, TF–IDF was used to construct the vector space model as the input of prediction model. As a single classifier was prone to overfitting and poor generalization, this paper conducted benchmark testing on five classical machine learning algorithms (NB, SVM, DT, LR, RF), and then combined four (SVM, DT, LR, RF) among them, which had better performance as the base classifiers to form the stacking heterogeneous ensemble method to build the vulnerability prediction model. Finally, the proposed method was verified on buffer overflow vulnerability and resource management vulnerability datasets, with a lowest false positive rate and false negative rate which can reach 1.58% and 4.06%, respectively.},
  archive      = {J_IJSEKE},
  author       = {Bing Zhang and Yuan Gao and Jingyi Wu and Ning Wang and Qian Wang and Jiadong Ren},
  doi          = {10.1142/S0218194022500620},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1559-1582},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Approach to predict software vulnerability based on multiple-level N-gram feature extraction and heterogeneous ensemble learning},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Creativity and design thinking as facilitators in
requirements elicitation. <em>IJSEKE</em>, <em>32</em>(10), 1527–1558.
(<a href="https://doi.org/10.1142/S0218194022500607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Context : The use of Creativity and Design Thinking (C&amp;DT) techniques favor the generation of new ideas based on the needs of users and stakeholders, and can support software developers during the process of requirements elicitation. Objectives : In this work, we aim to identify C&amp;DT techniques to perform requirements elicitation proposed in the literature and in the industry and investigate the perception of software developers about using these techniques. Methods : We conducted a systematic literature review (SLR) to identify the C&amp;DT techniques in the literature and a regional survey with software development teams from several companies in Brazil to identify which techniques found in the literature are currently being used by organizations. The survey also investigated the level of knowledge that software developers have regarding the C&amp;DT techniques, and whether they agree that the use of these techniques can help to achieve a more effective process of requirements elicitation. Results : In the SLR, we identified 86 C&amp;DT techniques that support requirement elicitation activities. In the survey, most developers outlined that C&amp;DT techniques facilitate requirements elicitation and stated that they have more knowledge and usage experience with DT techniques than creativity techniques. The most used DT techniques mentioned by survey participants were: interview, brainstorming, uses cases, activity analysis, user story, and rapid prototyping, whereas for creativity techniques were: analogies, creativity workshops, focus group, questions list, clarification, none and combining ideas. Conclusions : The results showed that despite the existence of a large number of techniques in the literature, the developers’ lack of knowledge about these techniques makes them not used in the industry. However, the developers’ responses showed that the use of C&amp;DT techniques helps to make requirements elicitation more effective.},
  archive      = {J_IJSEKE},
  author       = {Edna Dias Canedo and Angelica Toffano Seidel Calazans and Geovana Ramos Sousa Silva and Pedro Henrique Teixeira Costa and Rodrigo Pereira de Mesquita and Eloisa Toffano Seidel Masson},
  doi          = {10.1142/S0218194022500607},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1527-1558},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Creativity and design thinking as facilitators in requirements elicitation},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring the impact of balanced and imbalanced learning in
source code suggestion. <em>IJSEKE</em>, <em>32</em>(10), 1499–1526. (<a
href="https://doi.org/10.1142/S0218194022500589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies have confirmed the robust performance of machine learning classifiers for various source code modeling tasks. In general, machine learning approaches are incapable of handling imbalanced datasets, since they are sensitive to the choice of diverse classes. Therefore, these approaches may lean towards the classes with a large percentage of observations. In this work, we investigate and explore the impact of balanced and imbalanced learning on source code suggestion task otherwise known as code completion, covering a large number of imbalanced classes. We further explore the impact of vocabulary size on modeling performance. First, we provide the essentials to formulate the problem of source code suggestion as a classification task and investigate the level of imbalanced classes. Second, we train the four most adapted neural language models as a baseline to assess the modeling performance. Third, we impose two diverse class balancing techniques, TomekLinks and AllKNN, to balance the datasets and evaluate their impact on the modeling performance. Finally, we trained these models with a weighted imbalanced learning approach and compared the performance with balanced learning approaches. Additionally, we train models by varying the vocabulary size to study their impact. In total, we trained 230 models on 10 real-world software projects and extensively evaluated these models with widely used performance metrics such as Precision, Recall, FScore, mean reciprocal rank (MRR), and Receiver operating characteristics (ROC). Additionally, we employed ANOVA statistical analysis to study the statistical significance and differences between these approaches. This study has demonstrated that the modeling performance decreases during balanced model training, whereas the weighted imbalance training produces comparable results and is more efficient in terms of time cost. Additionally, this study exhibits that a large size of vocabulary does not necessarily improve the modeling performance when out-of-vocabulary predictions are disregarded.},
  archive      = {J_IJSEKE},
  author       = {Yasir Hussain and Zhiqiu Huang and Yu Zhou and Izhar Ahmed Khan},
  doi          = {10.1142/S0218194022500589},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1499-1526},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Exploring the impact of balanced and imbalanced learning in source code suggestion},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). End-to-end automated UI testing workflow for web sites with
intensive user–system interactions. <em>IJSEKE</em>, <em>32</em>(10),
1477–1497. (<a href="https://doi.org/10.1142/S0218194022500541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The user interface (UI) is the software component with which the end users interact the most in the mobile and web world today. Therefore, when evaluating end-user satisfaction with a software, besides the functional features, the possibilities provided by the UIs are significant. End-user interfaces offer several capabilities today, necessitating a complex structure. Examples include interfaces developed in technologies such as Google Flutter and React. Therefore, we foresee a need for UI testing business workflows allowing comprehensive testing of UIs to improve customer satisfaction. Within the scope of this research, we propose an end-to-end approached automated UI testing business workflow for testing UI components on websites with intense user actions.},
  archive      = {J_IJSEKE},
  author       = {Ramazan Faruk Oguz and Izzettin Erdem and Erdi Olmezogullari and Mehmet S. Aktas},
  doi          = {10.1142/S0218194022500541},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1477-1497},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {End-to-end automated UI testing workflow for web sites with intensive User–System interactions},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving just-in-time comment updating via AST edit
sequence. <em>IJSEKE</em>, <em>32</em>(10), 1455–1476. (<a
href="https://doi.org/10.1142/S0218194022500516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code comments are valuable for program comprehension and software maintenance. However, comments can be inconsistent or out-of-date after code changes. To tackle this problem, Just-In-Time (JIT) comment updating aims to automatically update comments with code changes. Existing approaches for this task use edit sequences of source code to model code changes. Meanwhile, recent researches indicate that neural models based on abstract syntax trees (AST) can help represent source code. In this paper, we propose a new method to learn code changes by combining code edit sequences with AST edit sequences, so that the generated new comment can be more accurate. Our approach utilizes three encoders to encode code edit sequences, AST edit sequences and old comment token sequences, respectively. The outputs of the encoders are then decoded into a sequence of edit actions, which is parsed to generate a new comment. The proposed method is evaluated on a public dataset using seven metrics, and the experimental results show that our approach outperforms the baselines. Furthermore, when the new comment has a larger edit distance than the old one, our model shows better performance.},
  archive      = {J_IJSEKE},
  author       = {Jiawen Huang and Huiqun Yu and Guisheng Fan and Ziyi Zhou and Mingchen Li},
  doi          = {10.1142/S0218194022500516},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1455-1476},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Improving just-in-time comment updating via AST edit sequence},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Notice of retraction - enhancing reusability: An integrated
framework for software requirements classification and prioritization.
<em>IJSEKE</em>, <em>32</em>(9), 1453. (<a
href="https://doi.org/10.1142/S0218194022930019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJSEKE},
  author       = {Tariq Ali and Saif Ur Rehman and Asif Nawaz and Munir Ahmed},
  doi          = {10.1142/S0218194022930019},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1453},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Notice of retraction - enhancing reusability: An integrated framework for software requirements classification and prioritization},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Return instruction classification in binary code using
machine learning. <em>IJSEKE</em>, <em>32</em>(9), 1419–1452. (<a
href="https://doi.org/10.1142/S0218194022500565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary code analysis is vital in source code unavailable cases, such as malware analysis and software vulnerability mining. Its first step could be function identification. Most function identification methods are based on function prologs/epilogs. However, functions may not have standard prologs/epilogs. To identify these functions, we need to use other methods. One approach is to identify return instructions first and then identify the start of a function. Currently, the multi-layer perceptron model is exploited to identify and validate a return instruction at a specific location. On this basis, a new approach is proposed to improve accuracy and provide more details. Specifically, a return instruction is classified into three classes: (1) false return instruction, (2) true return instruction inner a function but not the last instruction, and (3) true return instruction at the end of a function. The evaluation is performed on 5782 real-world binaries. Meanwhile, common classifiers including fully connected neural network, Two-layer Bidirectional Recurrent Neural Network (TBRNN), Two-layer Bidirectional Gate Recurrent Unit (TBGRU), Two-layer Bidirectional Long Short-term Memory Network (TBLSTM), Decision Tree, Random Forest, XGBoost, and Support Vector Machine (SVM) are evaluated on the same data set. The result shows that TBLSTM achieves an accuracy of 99.78%, which is higher than that of other classifiers in the evaluation, including the state-of-the-art tool IDA Pro 7.7.},
  archive      = {J_IJSEKE},
  author       = {Jing Qiu and Xiaoxu Geng and Feng Dong},
  doi          = {10.1142/S0218194022500565},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1419-1452},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Return instruction classification in binary code using machine learning},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identifying high-impact bug reports with imbalance
distribution by instance fuzzy entropy. <em>IJSEKE</em>, <em>32</em>(9),
1389–1417. (<a href="https://doi.org/10.1142/S021819402250053X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bug tracking systems, such as Bugzilla, contain bug reports collected from sources such as development teams, testing teams and end users. Developers often depend on bug reports to fix identified bugs. Frequently used bug reports are the so-called severe bug reports. Although severe bug reports can be manually detected within bug reports in bug tracking systems, they impose heavy burdens on management of bug tracking systems. Consequently, an automated mechanism to examine the severity of bug reports is desirable to augment productivity. Unfortunately, identifying the severity of bug reports from thousands of bug reports in a bug tracking system is not an easy feat, because of the problem of low-quality and imbalance distributions that could affect the performance of automated mechanisms. In this paper, we propose an approach, namely FER, to counter low-quality and imbalanced distributions of bug reports relative to their severity. First, FER approach gets high-quality bug reports based on instance fuzzy entropy. Then, FER approach weakens the imbalancedness degree of class distribution according to the high-quality bug reports to train classifiers to recognize the severity of bug reports. Several experiments are conducted on bug reports from three open source projects ( Eclipse , Mozilla , GNOME ) and they reveal that our approach is robust against the low-quality and imbalance distributions of bug reports, while identifying the severity of bug reports.},
  archive      = {J_IJSEKE},
  author       = {Hui Li and Xuexin Qi and Mengxuan Li and Yang Qu and Xin Ge},
  doi          = {10.1142/S021819402250053X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1389-1417},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Identifying high-impact bug reports with imbalance distribution by instance fuzzy entropy},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Class change prediction by incorporating community smell: An
empirical study. <em>IJSEKE</em>, <em>32</em>(9), 1369–1388. (<a
href="https://doi.org/10.1142/S0218194022500528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To adapt to changing software requirements, developers need to maintain and modify software through code changes. Predicting change-prone code can help developers to reduce the cost of software maintenance in advance. Prior work confirmed code smell intensity is a reliable metric for predicting change-prone classes. Community smell is a derivation of the concept of code smell in open-source software development community, it refers to poor communication and collaboration problems among developers. We add community smell to existing change prediction models, and propose a software class change prediction model integrating process metrics, code smell intensity metrics, anti-pattern metrics and community smell metrics, which takes into account the technicality and organizational aspects of software development. Experimental results demonstrate that when Multilayer Perceptron is used to build a change prediction model, community smell improves the baseline model by 4.4% and 31.5% in terms of F -Measure and Recall. In addition, community smell improves baseline model performance to a greater extent in terms of Recall and Precision than code smell-related information.},
  archive      = {J_IJSEKE},
  author       = {Qingyuan Dou and Junhua Chen and Jianhua Gao and Zijie Huang},
  doi          = {10.1142/S0218194022500528},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1369-1388},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Class change prediction by incorporating community smell: An empirical study},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An approach to software defect prediction combining semantic
features and code changes. <em>IJSEKE</em>, <em>32</em>(9), 1345–1368.
(<a href="https://doi.org/10.1142/S0218194022500504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defect prediction (SDP), which predicts defective code regions, can help developers reasonably allocate limited resources for locating bugs and prioritizing their testing efforts. Previous work on defect prediction has used machine learning and artificial software metrics. However, traditional defect prediction features extracted from artificial software metrics often fail to capture the syntactic and semantic information of defective modules. This work on defect prediction mostly focuses on abstract syntax tree (AST). Moreover, because current research on AST technology is relatively mature, it is difficult to further improve the accuracy of defect prediction when only using AST to characterize codes. In this paper, in order to capture more semantic features, we extract semantic information both from the sequences of AST tokens and code change tokens. In addition, to leverage the traditional features extracted from statistical metrics, we also combine the semantic features with traditional defect prediction features to perform SDP, and use the gated fusion mechanism to determine the combination ratio of the two kinds of features. In our empirical studies, 10 open-source Java projects from the PROMISE repository are chosen as our empirical subjects. Experimental results show that our proposed approach can perform better than several state-of-the-art baseline SDP methods.},
  archive      = {J_IJSEKE},
  author       = {Chuanqi Tao and Tao Wang and Hongjing Guo and Jingxuan Zhang},
  doi          = {10.1142/S0218194022500504},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1345-1368},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An approach to software defect prediction combining semantic features and code changes},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A semantic web-enabled approach for dependency management.
<em>IJSEKE</em>, <em>32</em>(9), 1307–1343. (<a
href="https://doi.org/10.1142/S0218194022500498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of external libraries in today’s software projects allows developers to take advantage of features provided by such application programming interfaces (APIs) without having to reinvent the wheel. However, APIs have also introduced new challenges to the software engineering community (e.g. API incompatibilities, software vulnerabilities, and license violations) that extend beyond traditional project boundaries and often involve different software artifacts. One potential solution to these challenges is to provide a technology-independent representation of software dependency semantics and its integration with knowledge from other software artifacts. In our research, we take advantage of the semantic web (SW) and its technology stack to establish a unified knowledge representation of build and dependency repositories. Given this knowledge base, we can now extend and integrate other (heterogeneous) resources to allow for a flexible and comprehensive global impact analysis approach. To illustrate the applicability of our SW-enabled modeling approach, we discuss two different applications. These applications illustrate how our modeling approach can not only integrate and reuse knowledge from dependency management systems and other software artifacts, but also take advantage of inference services provided by the SW to support novel software analytics services across artifact and project boundaries.},
  archive      = {J_IJSEKE},
  author       = {Ellis E. Eghan and Juergen Rilling},
  doi          = {10.1142/S0218194022500498},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1307-1343},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A semantic web-enabled approach for dependency management},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Characterizing and detecting methods to be benchmarked under
performance unit test. <em>IJSEKE</em>, <em>32</em>(9), 1279–1305. (<a
href="https://doi.org/10.1142/S0218194022500486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous integration is a growing trend in the software engineering community and industry. Performance testing is becoming more important in this context. To support precise and fine-grained monitoring, performance unit tests are applied for small software components. However, the benchmarks for performance unit testing are still insufficient, which means that benchmark coverage is low and there is a room for improvement. Therefore, focusing on the most important parts of the software, such as methods, and ensuring that their performance is monitored closely with performance unit tests can greatly reduce the amount of work that needs to be done for testing and to prepare benchmarks. This paper aims to provide an assisting approach for detecting methods that need to be benchmarked in performance unit tests. We start by defining 30 features to characterize the methods in the projects and show that they can be used to tell the benchmarked methods (short for BDMs ) from those that are not. Then, using the proposed features, we build machine learning-based models to detect BDMs . We perform an experiment with 10 open source projects from GitHub to see how well our approach works. First, we use seven binary classification techniques to evaluate the prediction performance of our machine learning models. We find that Random Forest makes the best predictions where AUC and MCC are between 0.77 and 0.89 and 0.5 and 0.75, respectively. In terms of cost effectiveness, the experiment reveals that by inspecting only 5% of the candidate methods detected by our model, 43% of the total real BDMs can be retrieved. Second, we conduct feature importance evaluations for individual features and feature categories. We find that eight features related to Scope, History, and Complexity are individually important for good predictions and that the combination of all features in the Scope category is paramount for our model, while the combination of features in the Control Flow category is less important. Third, we investigate the performance of our detection approach with different feature selection strategies and data sources. Our results show that we can make good predictions about whether a method needs to be benchmarked by using machine learning models. Practitioners can use our method and the results of the study to deal with BDMs detection effectively.},
  archive      = {J_IJSEKE},
  author       = {Jie Chen and Haiyang Hu and Dongjin Yu},
  doi          = {10.1142/S0218194022500486},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1279-1305},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Characterizing and detecting methods to be benchmarked under performance unit test},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Novel strategy generating variable-length state machine test
paths. <em>IJSEKE</em>, <em>32</em>(8), 1247–1278. (<a
href="https://doi.org/10.1142/S0218194022500474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finite State Machine is a popular modeling notation for various systems, especially software and electronic. Test paths (TPs) can be automatically generated from the system model to test such systems using a suitable algorithm. This paper presents a strategy that generates TPs and allows to start and end TPs only in defined states of the finite state machine. The strategy also simultaneously supports generating TPs only of length in a given range. For this purpose, alternative system models, test coverage criteria, and a set of algorithms are developed. The strategy is compared with the best alternative based on the reduction of the test set generated by the established N-switch coverage approach on a mix of 171 industrial and artificially generated problem instances. The proposed strategy outperforms the compared variant in a smaller number of TP steps. The extent varies with the used test coverage criterion and preferred TP length range from none to two and half fold difference. Moreover, the proposed technique detected up to 30% more simple artificial defects inserted into experimental SUT models per one test step than the compared alternative technique. The proposed strategy is well applicable in situations where a possible TP starts and ends in a state machine needs to be reflected and, concurrently, the length of the TPs has to be in a defined range.},
  archive      = {J_IJSEKE},
  author       = {Vaclav Rechtberger and Miroslav Bures and Bestoun S. Ahmed and Hynek Schvach},
  doi          = {10.1142/S0218194022500474},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1247-1278},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Novel strategy generating variable-length state machine test paths},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intrusion detection algorithm based on convolutional neural
network and light gradient boosting machine. <em>IJSEKE</em>,
<em>32</em>(8), 1229–1245. (<a
href="https://doi.org/10.1142/S0218194022500462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the limitations of existing algorithms of network intrusion detection in dealing with complex data of imbalance and high dimensionality, this paper proposes an intrusion detection algorithm based on convolutional neural network (CNN) and Light Gradient Boosting Machine (LightGBM). First, the data-type conversion, oversampling technology and image data conversion are included in the data preprocessing to make the data balanced and adapt to the input format. Then, by the convolutional layer, pooling layer and fully connected layer of the CNN model, the main features are abstracted from the converted image data. Finally, data of the main features is used for training and testing the LightGBM model, so as to get the final classification results. This paper uses KDDCUP99 dataset to carry out multi-classification experiments. By comparing the experiments before and after balancing the dataset, and comparing with similar algorithms, it verifies the superiority of the proposed algorithm in the classification performance of intrusion detection, especially for the minority attack classes.},
  archive      = {J_IJSEKE},
  author       = {Qian Wang and Wenfang Zhao and Xiaoyu Wei and Jiadong Ren and Yuying Gao and Bing Zhang},
  doi          = {10.1142/S0218194022500462},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1229-1245},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Intrusion detection algorithm based on convolutional neural network and light gradient boosting machine},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sequence-aware API recommendation based on collaborative
filtering. <em>IJSEKE</em>, <em>32</em>(8), 1203–1228. (<a
href="https://doi.org/10.1142/S0218194022500437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {API recommendation is crucial to improve programmers’ productivity. A lot of work has been proposed to improve the accuracy of API recommendations. In the existing work, many metrics, such as Precision , Recall , and MAP are used to evaluate the accuracy of the recommendation. These metrics can well reflect the ability to distinguish useful APIs from the candidate set, but they cannot evaluate the ability to determine the priority of useful APIs with each other. The priority between related APIs directly determines whether the recommended results are practical for developers. From this perspective, inspired by the sequence-aware recommendation, this paper constructs an API recommendation method with sequence awareness and designs new metrics to evaluate the method’s ability to determine the priority of useful APIs. The experimental results show that, compared with the baseline, the proposed method not only achieves better results on the common widely-used metrics but also outperforms the baseline method concerning the newly proposed sequence metrics.},
  archive      = {J_IJSEKE},
  author       = {Yongchao Wang and Yu Zhou and Taolue Chen and Jingxuan Zhang and Wenhua Yang and Zhiqiu Huang},
  doi          = {10.1142/S0218194022500437},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1203-1228},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Sequence-aware API recommendation based on collaborative filtering},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fault localization approach based on BiRNN and
multi-dimensional features. <em>IJSEKE</em>, <em>32</em>(8), 1179–1201.
(<a href="https://doi.org/10.1142/S0218194022500425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software fault localization is notoriously tedious and time-consuming. Developed rapidly, machine learning techniques have been adopted for fault localization by researchers. Most existing approaches use the test coverage information as feature input to the learning model, ignoring the limited ability of the single-dimensional features. The effectiveness of fault localization is not greatly improved. To overcome the limitation, we propose a fault localization approach based on Bidirectional Recurrent Neural Networks (BiRNNs) and multi-dimensional features. Our approach collects suspiciousness-based, text similarity-based and fault-proneness-based features from the traditional fault localization areas and software metrics. To evaluate our approach, the experiments have been studied on the real-fault benchmark Defects4J and seeded fault program NanoXML. The experimental results show that our approach effectively improves fault localization accuracy.},
  archive      = {J_IJSEKE},
  author       = {Yue Yan and Shujuan Jiang and Rongcun Wang and Cheng Zhang and Chen Wang and Shengang Zhang and Min Wen},
  doi          = {10.1142/S0218194022500425},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1179-1201},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A fault localization approach based on BiRNN and multi-dimensional features},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Root cause analysis of anomalies based on graph
convolutional neural network. <em>IJSEKE</em>, <em>32</em>(8),
1155–1177. (<a href="https://doi.org/10.1142/S0218194022500395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the gradual increase of network complexity and network scale in the cloud environment, Root Cause Analysis (RCA) of node failures has become a systematic problem of great research significance. This paper proposes Graph-Attention-Sage (GASage) algorithm, which is a fault RCA algorithm and scheme. The algorithm solves the RCA by incorporating TOPK sampling and Attention-Aggregation with GraphSage algorithm in large-scale and complex microservice network environment. The GASage algorithm is based on graph convolutional neural network and graph attention mechanism, which profoundly combines the characteristics of network fault RCA problems. TOPK sampling mechanism is applied in GASage to select the neighboring nodes with the top K highest correlation as the objectives to be aggregated. GASage adopts an attention mechanism when aggregated, which aggregates the features of the central node according to the weights of the adjacent nodes and the central node. The weight aggregation method can greatly improve the node representation effect in the RCA scenario. The empirical results of our experiments have demonstrated that the model learned with GASage can outperform other model with the same learning framework and achieves more than 10% improvement in precision.},
  archive      = {J_IJSEKE},
  author       = {Zhongliang Li and Yaofeng Tu and Zongmin Ma},
  doi          = {10.1142/S0218194022500395},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1155-1177},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Root cause analysis of anomalies based on graph convolutional neural network},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ATLDesigner: ATL model checking using an attribute grammar.
<em>IJSEKE</em>, <em>32</em>(8), 1125–1154. (<a
href="https://doi.org/10.1142/S0218194022500450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we use attribute grammars as a formal approach for model checkers development. Our aim is to design an Alternating-Time Temporal Logic (ATL) model checker from a context-free grammar which generates the language of the ATL formulae. An attribute grammar may be informally defined as a context-free grammar which is extended with a set of attributes and a collection of semantic rules. We provide a formal definition for an attribute grammar used as input for Another Tool for Language Recognition (ANTLR) to generate an ATL model checker. The original implementation of the model-checking algorithm is based on Relational Databases and Web Services. Several database systems and Web Services technologies were used for evaluating the system performance in verification of large ATL models.},
  archive      = {J_IJSEKE},
  author       = {Laura F. Stoica and Florin Stoica},
  doi          = {10.1142/S0218194022500450},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1125-1154},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {ATLDesigner: ATL model checking using an attribute grammar},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Clean and learn: Improving robustness to spurious solutions
in API question answering. <em>IJSEKE</em>, <em>32</em>(7), 1101–1123.
(<a href="https://doi.org/10.1142/S0218194022500449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of a question answering (QA) system for application programming interface (API) documentation can greatly facilitate developers in API-related tasks. However, when applying deep learning technology, API QA systems suffer from the spurious solution problem. That is, the answer can literally appear in multiple positions (i.e. start-end indices) in the API documentation, though only one of them (called golden solution) correctly solves the question given its context. The other incorrect candidates (called spurious solutions) hinder the neural network model to learn reasonable solutions or correct answers. In this work, we propose Clean-and-Learn , an effective and robust method for API QA over documents. In order to reduce the spuriousness of candidate solutions used for training, we design several scoring functions to rank the candidate occurrences ( clean ). Only high-quality (top- k ) candidate solutions are involved in training. Then, we perform multi-task learning by weighing the losses computed from the top- k occurrences ( learn ). We evaluate our method on the constructed APIQASet dataset. The experiment results show that Clean-and-Learn achieves a ROUGE-L score of 75.8 and accuracy of 70.5% in API QA, which significantly outperforms state-of-the-art approaches.},
  archive      = {J_IJSEKE},
  author       = {Shuai Yuan and Haozhe Qin and Xiaodong Gu and Beijun Shen},
  doi          = {10.1142/S0218194022500449},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {1101-1123},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Clean and learn: Improving robustness to spurious solutions in API question answering},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving large-gap clone detection recall using multiple
features. <em>IJSEKE</em>, <em>32</em>(7), 1071–1099. (<a
href="https://doi.org/10.1142/S0218194022500413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code clone refers to two or more identical or similar source code fragments. Research on code clone detection has lasted for decades. Investigation and evaluation of existing clone detection techniques indicate that they are resilient to function-level clone detection. Still, there may be room for further research in block-level clone detection. Particularly, type-3 clones that include large gaps, are ongoing challenges. To solve these problems, we propose a clone detection method based on multiple code features. It aims to improve the recall rate of code block clone detection and overcome large-gap and hard-to-detect type-3 clones. This method first splits the source code files based on the program’s structural features and context features to obtain code blocks. The collection of code blocks obtained in this way is complete, and the large gaps in clone pairs will also be removed. In addition, we only need to compute the similarity between code blocks with the same structural features, which can also significantly save time and resources. The similarity is obtained by calculating the proportion of the same tokens between two code blocks. Moreover, since different types of tokens have different weights in similarity calculation, we use supervised learning to obtain a classifier model between token features and code clone. We divide the tokens into 13 types and train the machine learning model with the manually confirmed clone or non-clone pair. Finally, we develop a prototype system and compare our tools with existing tools under the Mutation Framework and in several actual C projects. The experimental results also demonstrate the advancement and practicality of our prototype.},
  archive      = {J_IJSEKE},
  author       = {Peng Dai and Qianjin Zhang and Yawen Wang and Dahai Jin and Yunzhan Gong},
  doi          = {10.1142/S0218194022500413},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {1071-1099},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Improving large-gap clone detection recall using multiple features},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generation of c++ code from isabelle/HOL specification.
<em>IJSEKE</em>, <em>32</em>(7), 1043–1069. (<a
href="https://doi.org/10.1142/S0218194022500401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic code generation plays an important role in ensuring the reliability and correctness of software programs. Reliable programs can be obtained automatically from verified program specifications by code generators. The target languages of the existing code generators are mainly functional languages, which are relatively less used than C/C + + . As C/C + + is widely used in the industry and many fundamental software facilities and the correctness verification of C/C + + programs is difficult and cumbersome, this paper provides an automatic conversion framework that allows to generate C + + implementation from verified Isabelle/HOL specifications. The framework is characterized by combining the verification convenience of Isabelle/HOL and the efficiency of C + + . Since the correctness of the functional Isabelle/HOL specification can be guaranteed by interactive proofs, the correctness of the relevant generated C + + implementation can also be maintained.},
  archive      = {J_IJSEKE},
  author       = {Dongchen Jiang and Bo Xu},
  doi          = {10.1142/S0218194022500401},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {1043-1069},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Generation of c++ code from Isabelle/HOL specification},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An ontology-based information extraction system for
residential land-use suitability analysis. <em>IJSEKE</em>,
<em>32</em>(7), 1019–1042. (<a
href="https://doi.org/10.1142/S0218194022500383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an Ontology-Based Information Extraction (OBIE) system to automate the extraction of the criteria and values applied in Land-Use Suitability Analysis (LUSA) from bylaw and regulation documents related to the geographic area of interest. The results obtained by our proposed LUSA OBIE system (land-use suitability criteria and their values) are presented as an ontology populated with instances of the extracted criteria and property values. This latter output ontology is incorporated into a Multi-Criteria Decision-Making (MCDM) model applied for constructing suitability maps for different kinds of land uses. The resulting maps may be the final desired product or can be incorporated into the cellular automata urban modeling and simulation for predicting future urban growth. A case study has been conducted where the output from LUSA OBIE is applied to help produce a suitability map for the City of Regina, Saskatchewan, to assist in the identification of suitable areas for residential development. A set of Saskatchewan bylaw and regulation documents were downloaded and input to the LUSA OBIE system. We accessed the extracted information using both the populated LUSA ontology and the set of annotated documents. In this regard, the LUSA OBIE system was effective in producing a final suitability map.},
  archive      = {J_IJSEKE},
  author       = {Munira Al-Ageili and Malek Mouhoub},
  doi          = {10.1142/S0218194022500383},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {1019-1042},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An ontology-based information extraction system for residential land-use suitability analysis},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Falsification-aware semantics for temporal logics and their
inconsistency-tolerant subsystems: Theoretical foundations of
falsification-aware model checking. <em>IJSEKE</em>, <em>32</em>(7),
971–1017. (<a href="https://doi.org/10.1142/S0218194022500371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model checking is well known to be a computer-aided method for verifying concurrent systems. Temporal logics and their Kripke-style semantics have been widely used in model checking. Falsification-aware Kripke-style semantics for temporal logics have been required for the theoretical basis of model checking because falsification plays a critical role in obtaining counterexample traces for the underlying object specifications in model checking. However, a useful falsification-aware Kripke-style semantics has yet to be developed for standard temporal logics. Hence, this study introduces two types of falsification-aware Kripke-style semantics for standard temporal logics that have been typically used in model checking. The equivalences among the proposed falsification-aware and standard Kripke-style semantics for standard temporal logics are proved. Furthermore, some inconsistency-tolerant subsystems of standard temporal logics are semantically obtained from the proposed falsification-aware Kripke-style semantics for standard temporal logics by deleting a characteristic condition on the labeling function of the semantics. The proposed semantic framework for standard and inconsistency-tolerant temporal logics is regarded as a unified framework for generalizing and combining the existing standard, inconsistency-tolerant, and many-valued semantic frameworks. This unified semantic framework is useful for obtaining a theoretical basis for generalized (inconsistency-tolerant) model checking, referred to here as falsification-aware model checking.},
  archive      = {J_IJSEKE},
  author       = {Norihiro Kamide},
  doi          = {10.1142/S0218194022500371},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {971-1017},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Falsification-aware semantics for temporal logics and their inconsistency-tolerant subsystems: Theoretical foundations of falsification-aware model checking},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic analysis of available source code of top
artificial intelligence conference papers. <em>IJSEKE</em>,
<em>32</em>(7), 947–970. (<a
href="https://doi.org/10.1142/S0218194022500358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source code is essential for researchers to reproduce the methods and replicate the results of artificial intelligence (AI) papers. Some organizations and researchers manually collect AI papers with available source code to contribute to the AI community. However, manual collection is a labor-intensive and time-consuming task. To address this issue, we propose a method to automatically identify papers with available source code and extract their source code repository URLs. With this method, we find that 20.5% of regular papers of 10 top AI conferences published from 2010 to 2019 are identified as papers with available source code and that 8.1% of these source code repositories are no longer accessible. We also create the XMU NLP Lab README Dataset, the largest dataset of labeled README files for source code document research. Through this dataset, we have discovered that quite a few README files have no installation instructions or usage tutorials provided. Further, a large-scale comprehensive statistical analysis is made for a general picture of the source code of AI conference papers. The proposed solution can also go beyond AI conference papers to analyze other scientific papers from both journals and conferences to shed light on more domains.},
  archive      = {J_IJSEKE},
  author       = {Jialiang Lin and Yingmin Wang and Yao Yu and Yu Zhou and Yidong Chen and Xiaodong Shi},
  doi          = {10.1142/S0218194022500358},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {947-970},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Automatic analysis of available source code of top artificial intelligence conference papers},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online judge system: Requirements, architecture, and
experiences. <em>IJSEKE</em>, <em>32</em>(6), 917–946. (<a
href="https://doi.org/10.1142/S0218194022500346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development and operation of Online Judge System (OJS), which is used to evaluate the correctness of programs, is a nontrivial and difficult task due to the various functional and non-functional requirements. However, although many OJSs have been developed and operated, and their usefulness reported, the theory for constructing OJSs has not been sufficiently discussed. In this paper, we present the functional and nonfunctional requirements oriented to OJS as well as demonstrate the internal components and software architecture of an OJS, which has been in operation for over a decade and has evaluated over six million solutions. We also present real-world experiences and challenges encountered during this long journey of our OJS.},
  archive      = {J_IJSEKE},
  author       = {Yutaka Watanobe and Md. Mostafizer Rahman and Taku Matsumoto and Uday Kiran Rage and Penugonda Ravikumar},
  doi          = {10.1142/S0218194022500346},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {917-946},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Online judge system: Requirements, architecture, and experiences},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic identification of high-impact bug report by
product and test code quality. <em>IJSEKE</em>, <em>32</em>(6), 893–916.
(<a href="https://doi.org/10.1142/S021819402250036X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bug reports are submitted by the software stakeholders to foster the location and elimination of bugs. However, in large-scale software systems, it may be impossible to track and solve every bug, and thus developers should pay more attention to High-Impact Bugs (HIBs). Previous studies analyzed textual descriptions to automatically identify HIBs, but they ignored the quality of code, which may also indicate the cause of HIBs. To address this issue, we integrate the features reflecting the quality of production (i.e. CK metrics) and test code (i.e. test smells) into our textual similarity based model to identify HIBs. Our model outperforms the compared baseline by up to 39% in terms of AUC-ROC and 64% in terms of F-Measure. Then, we explain the behavior of our model by using SHAP to calculate the importance of each feature, and we apply case studies to empirically demonstrate the relationship between the most important features and HIB. The results show that several test smells (e.g. Assertion Roulette, Conditional Test Logic, Duplicate Assert, Sleepy Test) and product metrics (e.g. NOC, LCC, PF, and ProF) have important contributions to HIB identification.},
  archive      = {J_IJSEKE},
  author       = {Jianshu Ding and Guisheng Fan and Huiqun Yu and Zijie Huang},
  doi          = {10.1142/S021819402250036X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {893-916},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Automatic identification of high-impact bug report by product and test code quality},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generating optimal class integration test orders using
genetic algorithms. <em>IJSEKE</em>, <em>32</em>(6), 871–892. (<a
href="https://doi.org/10.1142/S0218194022500309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, many intelligent optimization algorithms have been applied to the class integration and test order (CITO) problem. These algorithms also have been proved to be able to efficiently solve the problem. Here, the design of fitness function is a key task to generate the optimal solution. To better solve the class integration and test order problem, we propose a new fitness function to generate the optimal solution that achieves a balanced compromise between the different measures (objectives) such as the total number of stubs and the total stubbing complexity in this paper. We used some programs to compare and evaluate the different approaches. The experimental results show that our proposed approach is encouraging to some extent in solving the class integration and test order problem.},
  archive      = {J_IJSEKE},
  author       = {Yanmei Zhang and Shujuan Jiang and Yanru Ding and Guan Yuan and Junjie Liu and Dongyu Lu and Junyan Qian},
  doi          = {10.1142/S0218194022500309},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {871-892},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Generating optimal class integration test orders using genetic algorithms},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An empirical study on bugs in PHP. <em>IJSEKE</em>,
<em>32</em>(6), 845–870. (<a
href="https://doi.org/10.1142/S0218194022500292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PHP (Hypertext Preprocessor) is a scripting language that has been widely used in web development. This paper conducts an empirical study on bugs in PHP. By analyzing 35,921 bug reports, 6524 revisions, and root causes of randomly selected 500 bugs, we find that: (1) Among all the 385 versions involved in these bugs, there are the most bugs in PHP 4.0.4, PHP 4.0.6, and PHP 4.0.3; Documentation bugs are mainly distributed in PHP 4.y.z and PHP 5.y.z; Security bugs are distributed primarily in the relatively later normal versions of PHP 5.y.z. (2) Documentation, Compile, and Scripting Engine packages are greatly affected by bugs; 73.71% of documentation bugs affect documentation; PHAR, EXIF, and GD are more affected by security bugs. (3) It may be not difficult to repair most bugs since the number of modified lines of code and files are limited; However, nearly 11% of bugs need more than one year to repair; Compared with documentation bugs, security bugs are more difficult to be repaired; The duration of bugs in PHP 8.y.z is shorter than in other versions. (4) Semantic bugs and documentation bugs are the more common root causes of bugs than others. Besides, among semantic bugs, the “Missing Features” bugs and “Processing” bugs are more than others. These results could indicate some potential problems during the detecting and repairing of PHP’s bugs. These findings reveal some laws of bugs in PHP. It could assist developers of PHP in improving their development quality, assist maintainers of PHP in detecting and repairing bugs more effectively, and suggest users of PHP evade potential risks.},
  archive      = {J_IJSEKE},
  author       = {Ziyuan Wang and Dexin Bu and Xingpeng Xuan and Jia Gu},
  doi          = {10.1142/S0218194022500292},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {845-870},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An empirical study on bugs in PHP},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Equivalent mutants detection based on weighted software
behavior graph. <em>IJSEKE</em>, <em>32</em>(6), 819–843. (<a
href="https://doi.org/10.1142/S0218194022500322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The equivalent mutants problem is one of the crucial problems in mutation testing. In consequence of its existence, the effectiveness of mutation testing is underestimated. In addition, it will produce a certain amount of useless overhead. Equivalent mutants cannot be detected by any test input. The existing works mostly focus on static analysis to detect, or avoid generating, the equivalent mutants. The essence of these methods is to use prior knowledge to establish some rules of program equivalence. However, (1) it needs a lot of professional labor to sort out the equivalence rules, and (2) only a small part of the rules can be determined in advance, because of the diversity of mutation operators and mutation targets. Consequently, the best result reported so far is 50% of the equivalent mutants can be detected. Since it is generally believed that manual judgment of program equivalence is the most reliable, this paper proposes a novel method to automatically detect equivalent mutants by tracing program behavior like the professionals. The weighted software behavior graph is utilized in the detection of equivalent mutants for the first time. This method can not only figure out different execution paths, but also be sensitive to execution frequency. By comparing the weighted software behavior graphs of an alive mutant and its original program, we are able to examine more precisely whether the alive mutant is the same as the original program, in terms of the state of infection and/or the propagation. Evaluation results on an open dataset of manually evaluated equivalent mutants show that our approach can detect 77.5% of all the equivalent mutants, which is much higher than the existing static methods.},
  archive      = {J_IJSEKE},
  author       = {Dan Gong and Tiantian Wang and Xiaohong Su and Yanhang Zhang},
  doi          = {10.1142/S0218194022500322},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {819-843},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Equivalent mutants detection based on weighted software behavior graph},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finding memory bound of cloned objects in software
transactional memory programs. <em>IJSEKE</em>, <em>32</em>(6), 791–818.
(<a href="https://doi.org/10.1142/S0218194022500139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software transactional memory (STM) programs usually use more memory resources than traditional programs. Therefore, estimating an upper bound of memory resources used by an STM program is crucial for optimizing the program and reducing the risks of out-of-memory runtime exceptions. However, due to the complex nesting of transactions and threads, the estimation problem is challenging. In our previous work, we have developed several type systems to address the problem for core imperative languages with STM primitives. This work advances our previous works, in which we add object-oriented constructs to the language while keeping the STM primitives, to make the language closer to practical languages. Then, we built a type system to statically estimate the maximum memory required by well-typed programs of the language.},
  archive      = {J_IJSEKE},
  author       = {Ngoc-Khai Nguyen and Anh-Hoang Truong and Duc-Hanh Dang},
  doi          = {10.1142/S0218194022500139},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {791-818},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Finding memory bound of cloned objects in software transactional memory programs},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated localization testing of mobile applications
method. <em>IJSEKE</em>, <em>32</em>(5), 769–790. (<a
href="https://doi.org/10.1142/S0218194022500280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As more mobile applications become available to a broader user base, the need to localize applications to various languages and cultures grows. The need for localization demands localization testing. In this paper, the user interface localization problems are identified and categorized. The automated testing method containing automated localization defects detection algorithms using applications’ screenshots analysis is presented. The proposed method was validated experimentally by comparing automated testing and manual review. The experiment contained an automated and manual review of 781 Android applications. The manual review results were compared with the proposed automated testing method results. The comparison shows that automated methods can save localization testing time and discover more defects. However, the proposed method does not fully substitute manual testing but can act as a helper.},
  archive      = {J_IJSEKE},
  author       = {Šarūnas Packevičius and Greta Rudžionienė and Eduardas Bareiša},
  doi          = {10.1142/S0218194022500280},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {769-790},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Automated localization testing of mobile applications method},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An empirical study on the impact of python dynamic typing on
the project maintenance. <em>IJSEKE</em>, <em>32</em>(5), 745–768. (<a
href="https://doi.org/10.1142/S0218194022500243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Python is a popular typical dynamic programming language. In Python, dynamic typing is one of the most critical dynamic features. The lack of type information is likely to hinder the maintenance of Python projects. However, existing work has seldom focused on studying the impact of Python dynamic typing on project maintenance. This paper focuses on the two most common practices of Python dynamic typing, i.e. inconsistent-type assignments (ITA) and inconsistent variable types (IVT). Two approaches are proposed to identify ITA and IVT, i.e. identifying ITA by analyzing Abstract Syntax Trees and comparing identifiers types and identifying IVT by constructing a type dependency graph. In empirical experiments, we first locate the usage of ITA and IVT in 10 open-source Python projects. Then, we investigate the relations between the occurrence of ITA and IVT and the results of maintenance tasks. The study results show that projects are more prone to change as the number of dynamic typing identifiers increases. There is a weak connection between change-proneness and variable dynamic typing. There is a high probability that maintenance time and the acceptance of commits decrease as dynamic typing identifiers increase in projects. These results implicate that dynamic and static variables should be divided while developing new programming languages. Dynamic typing identifiers may not be the direct root causes for most software bugs. The categories of these bugs are worth exploring.},
  archive      = {J_IJSEKE},
  author       = {Xinmeng Xia and Yanyan Yan and Xincheng He and Di Wu and Lei Xu and Baowen Xu},
  doi          = {10.1142/S0218194022500243},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {745-768},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An empirical study on the impact of python dynamic typing on the project maintenance},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The safety and performance of prominent programming
languages. <em>IJSEKE</em>, <em>32</em>(5), 713–744. (<a
href="https://doi.org/10.1142/S0218194022500231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background : The current primary focus of programming language benchmarking studies in the literature is performance with less attention on safety. However, this context has a research gap because the software industry has focused more on software safety than performance to safeguard clients. This paper attempts to address this research gap by benchmarking languages in both safety and performance. Furthermore, this study includes Rust, a relatively new language with promising safety and performance features. Methods : This paper compares six prominent programming languages (in alphabetical order: C, C + + , Go, Java, Python and Rust) to determine which is the best in terms of safety and performance using quantitative and qualitative methods through actual testing of code and analysis of existing information. Results : The comparisons show that Rust was the safest language, outperforming all the other languages. Regarding performance, Rust, C and C + + performed comparably to each other and generally outperformed Go, Java and Python. Conclusion : It is possible to achieve a superior balance of software safety and performance with, at worst, a minimal performance drop; as Rust clearly demonstrates.},
  archive      = {J_IJSEKE},
  author       = {William Bugden and Ayman Alahmar},
  doi          = {10.1142/S0218194022500231},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {713-744},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {The safety and performance of prominent programming languages},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An empirical study on rule violation history of JavaScript
code blocks on stack overflow. <em>IJSEKE</em>, <em>32</em>(5), 693–712.
(<a href="https://doi.org/10.1142/S0218194022500310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {JavaScript code blocks on Stack Overflow (SO) are often used in software projects. However, little is known about the issue of rule violation risk in SO JavaScript code blocks. Rule violation is one of the factors which degrades the quality of Java Script code. To prevent prevalence of rule violation by reusing SO JavaScript code blocks, it is needed to investigate how secure SO JavaScript code blocks are against rule violation. To examine the issue, we performed a quantitative analysis to investigate how many rule violations are, when first rule violation occurs and what is the trend of rule violations in evolution history of Stack Overflow JavaScript code blocks. We collected SO posts related to JavaScript and extracted the code blocks contained in the posts. By using ESLint, the most popular rule violation detection tool, we identified rule violations in the evolution history of our target code blocks. We then performed quantitative analyses on the identified rule violations. As the results of the analyses, we found that: (1) 60% of the studied code blocks evolve with any rule violations. (2) In the rule violated code blocks, 92% of the code blocks get first rule violation occurrence in the early phase of their evolution. (3) 80% of the rule violated code blocks never fix existing rule violations during their evolution. Our findings suggest that SO should provide a policy which can reduce rule violations in submitted JavaScript code blocks. The findings can also make SO users attend to rule violations when reusing SO JavaScript code blocks.},
  archive      = {J_IJSEKE},
  author       = {Jungil Kim and Eunjoo Lee},
  doi          = {10.1142/S0218194022500310},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {693-712},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An empirical study on rule violation history of JavaScript code blocks on stack overflow},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting compiler bugs via a deep learning-based framework.
<em>IJSEKE</em>, <em>32</em>(5), 661–691. (<a
href="https://doi.org/10.1142/S0218194022500206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compiler testing is the most widely used way to assure compiler quality. However, since compilers require a large number of sophisticated test programs as inputs, the existing approaches in compiler testing still have a limited capability in generating both syntactically valid and diverse test programs. In this paper, we propose DeepGen, a deep learning-based approach to support compiler testing through the inference of a generative model for compiler inputs. First, DeepGen trains a Transformer-XL model based on a large corpus of seed programs, and uses the trained model to generate syntactically valid programs. Then, DeepGen adopts a sampling strategy in the inference phase to generate diverse test programs. Finally, DeepGen leverages differential testing on the generated programs to discover compiler bugs. We have evaluated DeepGen over two popular C++ compilers GCC and LLVM, and the results confirm the effectiveness of our approach. DeepGen detects 35.29%, 53.33%, and 187.50% more bugs than three existing approaches, i.e. DeepSmith, DeepFuzz, and Csmith, respectively. In addition, 30.43% bugs detected by DeepGen are not detected by other approaches. Furthermore, DeepGen has successfully detected 38 bugs in the latest development versions of GCC and LLVM; 21 of them have been confirmed/fixed by the developers.},
  archive      = {J_IJSEKE},
  author       = {Yixuan Tang and Zhilei Ren and He Jiang and Lei Qiao and Dong Liu and Zhide Zhou and Weiqiang Kong},
  doi          = {10.1142/S0218194022500206},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {661-691},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Detecting compiler bugs via a deep learning-based framework},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A literature-based thematic network to provide a
comprehensive understanding of agile teamwork (106). <em>IJSEKE</em>,
<em>32</em>(5), 645–659. (<a
href="https://doi.org/10.1142/S0218194022500176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agile Software Development (ASD) has become the mainstream software development method of choice. Its core fundamentals are based on Teamwork factors and the higher value of individuals and their interactions over processes and tools. However, there is no common understanding regarding the factors that should be considered for defining an ASD Teamwork construct. Driven by this problem, we present a thematic network that synthesizes the information presented in the literature, and eases knowledge sharing by defining a terminology. The thematic network is the result of the following process: (i) studies definition to be used as data source through a literature review; (ii) data extraction from these studies; (iii) data translation into codes; (iv) codes translation into themes; (v) creation of higher-order themes model; and (vi) assessment of synthesis trustworthiness. The resulting thematic network comprises four higher-order themes: Cohesion, Orientation, Shared Leadership, and Autonomy. We also evaluate the applicability of the identified themes in ASD Teamwork constructs in the literature. We concluded that the constructed thematic network can be generalized to ASD, and used as basis by researchers who intend to explore ASD Teamwork. Further, practitioners can use our results to understand agile teams’ dynamics better and improve their efficiency.},
  archive      = {J_IJSEKE},
  author       = {Arthur Freire and Manuel Neto and Mirko Perkusich and Alexandre Costa and Kyller Gorgónio and Hyggo Almeida and Angelo Perkusich},
  doi          = {10.1142/S0218194022500176},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {645-659},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A literature-based thematic network to provide a comprehensive understanding of agile teamwork (106)},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Knowledge graph construction for SOFL formal specifications.
<em>IJSEKE</em>, <em>32</em>(4), 605–644. (<a
href="https://doi.org/10.1142/S0218194022500279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Formal specifications can provide a solid foundation for software development and support for techniques of software quality assurance, such as specification-based inspection and testing. To ensure that these techniques can be applied effectively in practice, efficiently and accurately understanding specifications becomes extremely important. While this may be relatively easy for well-trained developers in formal methods, it can be rather difficult for computer since computer does not easily understand specifications. This difficulty poses a challenge for realizing automatic specification-based verification techniques that are in high demand for reducing development cost and improving software reliability. In this paper, we address this problem by discussing how the formal specification can be transformed into a knowledge graph to provide comprehensible, well-organized details of the specification for developers and computers. The transformation is done by extracting and storing information about attributes of each component and by establishing relationships between components in a formal specification. We elaborate on a top-down approach of constructing a knowledge graph from a specification, including creating an ontology, designing the Entity–Relationship (ER) diagram of the relational database based on the created ontology, extracting and storing attribute and relationship information in the relational database, mapping ontology to its instances and relational data to RDF triples, and displaying knowledge graph. Further, we present a case study to show how our approach works on the formal specification of an ATM system. Finally, we describe three experiments to evaluate its performance in improving specification readability, effectively guiding inspectors to establish traceability links between specifications and programs, and detecting defects through program inspection, respectively.},
  archive      = {J_IJSEKE},
  author       = {Jiandong Li and Shaoying Liu and Ai Liu and Runhe Huang},
  doi          = {10.1142/S0218194022500279},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {605-644},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Knowledge graph construction for SOFL formal specifications},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gradle-autofix: An automatic resolution generator for gradle
build error. <em>IJSEKE</em>, <em>32</em>(4), 583–603. (<a
href="https://doi.org/10.1142/S0218194022500218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradle is one of the widely used tools to automatically build a software project. While developers execute the Gradle build for projects, they face various build errors in practice. However, fixing build errors is not easy because developers should manually find out the cause of the build error and its resolution on their project. For this reason, developers spend much time fixing them, and especially it can be worse if a developer lacks the experience of handling build errors. To address this issue, we propose a novel approach named Gradle-AutoFix to automatically fix build errors along with providing their causes and resolutions. In this approach, we collect build errors to group their causes and resolutions and then generate feature vectors from build error messages by applying Bag-of-Word (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), Bigram, and an embedding layer. The feature vectors are utilized for training two classification models on cause and resolution. Next, we analyze fixing patterns and define seven resolution rules to fix the build error automatically. Based on our trained models and defined resolution rules, we built Gradle-AutoFix. For the evaluation, we measured how appropriately Gradle-AutoFix provides causes of build errors and resolutions. As a result, we obtained 96% and 91% accuracy, respectively. Also, we assessed how properly Gradle-AutoFix fixes the project’s build error based on the seven resolution rules. The outcome showed a 64.5% build error resolution rate for 231 projects.},
  archive      = {J_IJSEKE},
  author       = {Mingu Kang and Taeyoung Kim and Suntae Kim and Duksan Ryu},
  doi          = {10.1142/S0218194022500218},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {583-603},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Gradle-autofix: An automatic resolution generator for gradle build error},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical model checking for stochastic and hybrid
autonomous driving based on spatio-clock constraints. <em>IJSEKE</em>,
<em>32</em>(4), 553–582. (<a
href="https://doi.org/10.1142/S0218194022500188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous driving vehicles are a kind of typical cyber-physical systems integrating complex interactions between hardware and software components such as collaborative computation, distributed communication, and spatio-clock synchronous control with surrounding traffic environment. They can percept the environment, communicate with surroundings, and react fast enough to control independently. The purpose of autonomous driving emergence is to improve driving safety, reduce environmental pollution, and ease the traffic congestion. However, new features with surrounding open and dynamic environment make systems design and verification becoming more and more complex than ever, such as stochastic communication delay, hardware spontaneous failure distribution, and natively hybrid behaviors described by ordinary differential equations. Spatial and time collision avoidance remains crucial obstacles on the path to becoming ubiquitous and dependable. In this paper, we adopt statistical model checking (SMC) to enlighten possible hazards affected by stochastic and hybrid features in the design phase of autonomous driving systems. In order to provide safety and accountability, we first propose a dedicated multi-lane spatio-clock stochastic specification language (MLSCL) to describe safety invariants and guards in domain-specific autonomous driving systems. Then, we present the semantic mapping rules between MLSCL and UPPAAL SMC models, and design the spatio-clock stochastic and hybrid automata based on MLSCL in order to model inherently stochastic and hybrid behaviors. Finally, we present an illustrative lane-change case study to verify spatio-clock stochastic and hybrid-related properties adopting SMC, and demonstrate the effectiveness of our proposed approach.},
  archive      = {J_IJSEKE},
  author       = {Jinyong Wang and Zhiqiu Huang and Yi Zhu and Guohua Shen},
  doi          = {10.1142/S0218194022500188},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {553-582},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Statistical model checking for stochastic and hybrid autonomous driving based on spatio-clock constraints},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identifying possible improvements of software development
life cycle (SDLC) process of a bank by using process mining.
<em>IJSEKE</em>, <em>32</em>(4), 525–552. (<a
href="https://doi.org/10.1142/S0218194022400010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software development with its unique characteristics having knowledge-intensive and human-oriented aspects and complex domains, challenges organizations. The timely outcomes with high quality and desired cost that directly affect customer satisfaction have an important place in many organizations, including banks. In the last decade, as an emerging technique for business processes management, process mining has been applied in many domains, including manufacturing, supply chain, government, healthcare, and software engineering. There are limited number of studies on process mining techniques carried out for the software process, especially in the banking sector. A lack of tool infrastructure enabling to run the entire software development process and the challenges in integrating processed data from separated varying tools and assets complicate the use of process mining for software processes. This paper aims to identify the improvement points in the software development process of the Kuveyt Turk Participation Bank in Turkey through the surfacing actions. The findings and results are gathered by the application of process mining techniques of bupaR, and evaluation is provided by experts in the bank. After that, the relevant process improvements are identified. The results of this paper show that using process mining provides the organization with beneficial results, in particular, and a comprehensive view of the end-to-end Software Development Life Cycle (SDLC) processes.},
  archive      = {J_IJSEKE},
  author       = {Sedat Taskesenlioglu and Necmettin Ozkan and Tugba Gurgen Erdogan},
  doi          = {10.1142/S0218194022400010},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {525-552},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Identifying possible improvements of software development life cycle (SDLC) process of a bank by using process mining},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extracting prerequisite relations among concepts from the
course descriptions. <em>IJSEKE</em>, <em>32</em>(4), 503–523. (<a
href="https://doi.org/10.1142/S0218194022400034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, online learning is becoming more and more popular. Various online learning platforms provide a huge amount of learning resources for learners around the world. When choosing or sorting learning resources, learners often need to know what important knowledge concepts are addressed in each learning resource. Exploring the prerequisite relations among concepts is of great significance to educational planning. In this paper, we extracted concepts from the content of course descriptions and proposed a new approach that uses both course-based features and Wikipedia-based features to discover the prerequisite relations between knowledge concepts. Experiments on both English and Chinese datasets show that the proposed method outperforms existing baselines.},
  archive      = {J_IJSEKE},
  author       = {Kui Xiao and Youheng Bai and Zesong Wang},
  doi          = {10.1142/S0218194022400034},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {503-523},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Extracting prerequisite relations among concepts from the course descriptions},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The experience of tests during the COVID-19 pandemic-induced
emergency remote teaching. <em>IJSEKE</em>, <em>32</em>(4), 481–501. (<a
href="https://doi.org/10.1142/S0218194022400022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dire circumstances presented by the COVID-19 pandemic have had a severely debilitating global impact on education, and led to an urgent transition from the onsite environment (OSE) to the online environment (OLE) for teaching and learning. In that regard, this paper describes the experiences of us and students of our involvement in oral and written tests in multiple software engineering-related courses during 2020 and 2021. The challenges encountered along with the interventions are discussed, and educational lessons based on the reactions and responses of the students are given. The results of a preliminary survey of the students of their learning experience in the OLE are presented and, related to it, the comments from the students highlighting their preferences of the OSE or the OLE are included. The test procedures, processes, and/or practices herein are, in principle, generalizable and potentially applicable to other courses in computer science or software engineering, during emergency remote teaching or even otherwise.},
  archive      = {J_IJSEKE},
  author       = {Pankaj Kamthan},
  doi          = {10.1142/S0218194022400022},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {481-501},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {The experience of tests during the COVID-19 pandemic-induced emergency remote teaching},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guest editor’s introduction. <em>IJSEKE</em>,
<em>32</em>(4), 479. (<a
href="https://doi.org/10.1142/S0218194022020016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJSEKE},
  author       = {Shi-Kuo Chang},
  doi          = {10.1142/S0218194022020016},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {479},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Guest editor’s introduction},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Code generation with hybrid of structural and semantic
features retrieval. <em>IJSEKE</em>, <em>32</em>(3), 457–478. (<a
href="https://doi.org/10.1142/S0218194022500267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the growing need for faster software delivery, code generation has attracted more and more attention, since it could improve code maintainability by providing suggestions for coding. In the model of generating program source code from natural language (NL), the most effective method is to generate an intermediate architecture (such as Abstract Syntax Tree) combined with a deep learning model. However, these models have the following drawbacks: (1) The data structural information is underutilized and the correlation between samples is not considered. (2) Lack of the ability to memorize large and complex structures, so that complex codes cannot be generated correctly. To address these issues, we propose HRCODE model, a code generation architecture based on H ybrid of structural and semantic features R etrieval CODE model. We transform the NL description into an intermediate structure with structural features. Then, the NL and the intermediate structure are embedded into a vector through weight mixing, and we calculate the similarity score between each vector to retrieve the most relevant samples. Finally, the new input is brought into the PLBART model to generate code. Experiments show that HRCODE is at least 4.7% higher than the state-of-the-art models in the ACC metric and at least 10.3% higher in the BLEU-4 score. We have released our code at https://github.com/jesokang/HRCODE.},
  archive      = {J_IJSEKE},
  author       = {Kang Yang and Huiqun Yu and Guisheng Fan and Zijie Huang and Ziyi Zhou},
  doi          = {10.1142/S0218194022500267},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {457-478},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Code generation with hybrid of structural and semantic features retrieval},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identifying temporal corpus for enhanced user comments
analysis. <em>IJSEKE</em>, <em>32</em>(3), 439–456. (<a
href="https://doi.org/10.1142/S021819402250022X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User comments provide valuable information for requirements analysis. To effectively extract requirements from user comments, it is important to determine informative user comments. However, existing studies have mainly focused on NLP-based methods for analyzing user comments, rather than defining a set of user comments to be analyzed. If target user comments are not clearly determined, duplicate requirements can be discovered or new requirements cannot be discovered. To tackle this problem, we present a new method which defines a set of target corpora from user comments. Our method automatically defines a set of target corpora to be analyzed by identifying underlying temporal changes in user comments. We applied our method to real-world user comments collected from a mobile application store. We confirmed that our method successfully defined a set of corpora which aids the effective requirements elicitation, and facilitated discovering new requirements while avoiding the derivation of redundant requirements.},
  archive      = {J_IJSEKE},
  author       = {Jongwook Jeong and Youn Kyu Lee},
  doi          = {10.1142/S021819402250022X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {439-456},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Identifying temporal corpus for enhanced user comments analysis},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel method for identifying microservices by considering
quality expectations and deployment constraints. <em>IJSEKE</em>,
<em>32</em>(3), 417–437. (<a
href="https://doi.org/10.1142/S021819402250019X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, microservice architecture has become a dominant software development and deployment paradigm. Decomposing a system into loosely coupled, highly cohesive, and fine-grained microservices while meeting various technical constraints and implementing business capabilities is particularly important for microservice system (MS) designers. When an MS has a large number of functionalities and complex interconnections, it is a big challenge to identify microservices solely based on the experience of MS designers. We propose a structured and automated microservice identification method to decompose a system into appropriate microservices for this challenge. We model a system as unified modeling language (UML) class and sequence diagrams. In the identification phase, we take into account not only the traditional coupling-related criteria but also the quality expectation and deployment constraints, both of which have not yet been fully concerned in previous studies. Based on the criteria, a microservice identification algorithm using the clustering technique is designed. A case study of elderly care services illustrates the identification process. Experiments are conducted to evaluate and compare the proposed method against state-of-the-art methods. Results indicate that the proposed method significantly outperforms those compared from the literature.},
  archive      = {J_IJSEKE},
  author       = {Jianan Li and Hanchuan Xu and Xiaofei Xu and Zhongjie Wang},
  doi          = {10.1142/S021819402250019X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {417-437},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A novel method for identifying microservices by considering quality expectations and deployment constraints},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A constructive heuristic for automated parallel tests
assembly. <em>IJSEKE</em>, <em>32</em>(3), 395–415. (<a
href="https://doi.org/10.1142/S0218194022500164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel tests contain different items but have the same measurement properties. They are administered at the same or different time slots and their measurement results must be comparable. The problem of automated parallel tests assembly is studied for a long time, and many (mostly improvement) heuristic solutions are proposed and elaborated in literature. Such approaches frequently suffer from algorithms of unpredictable execution time, forcing the methods to terminate execution when some time limit or solution quality is reached. This paper proposes an efficient method of polynomial complexity, as a complete solution to the automated parallel tests assembly problem. The method uses the idea of Nawaz, Enscore, and Ham constructive heuristic algorithm to reduce the number of examined permutations, originally exploited for solving the permutation flow-shop sequencing problem. We compared the experimental results of the proposed method with two methods based on improvement heuristics that solve the same problem formulation, simulated annealing and variable neighborhood search. The main advantages of the proposed method are predictable execution time and implementation simplicity. Achieved quality of assembled tests, combined with predictable test assembly execution time, may be of particular interest in cases when computational resources for test assembly and administering are overloaded.},
  archive      = {J_IJSEKE},
  author       = {Miroslava M. Ignjatović and Igor I. Tartalja},
  doi          = {10.1142/S0218194022500164},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {395-415},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A constructive heuristic for automated parallel tests assembly},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Security versus compliance: An empirical study of the impact
of industry standards compliance on application security.
<em>IJSEKE</em>, <em>32</em>(3), 363–393. (<a
href="https://doi.org/10.1142/S0218194022500152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of security aspects into software development is an open topic, especially in highly regulated industries where standards are accompanied by a high degree of complexity. The research question of this paper relates to the misconception of industry standards compliance and security in the field of software development. Cyber attackers are constantly inventing new tools to penetrate systems and exploit even the most minor flaws, and adherence to an industry standard is not a solution. In this study, an empirical investigation is conducted over a six-month period to observe various customer relationship management (CRM) systems. To analyze and anticipate the vulnerabilities of various CRMs, penetration testing methodologies and cross-project prediction approaches are employed. Classification using multiple machine learning approaches is utilized in the study to increase the discovery of vulnerable components in each CRM. The Student t -test is also used to assess if the mean values of the two CRM datasets are substantially different from each other in order to evaluate the efficacy of overall security and its features. The results show that security best practices during application development have a significant influence on applications created in regulated environments. The action research approach used to validate this study provided positive results and its feasibility in practice to optimize security throughout the application development. This study adds to the literature on information security management systems (ISMS) and best practices in application development in terms of creating and implementing opportunities based on broader information security management measures.},
  archive      = {J_IJSEKE},
  author       = {Harrison Stewart},
  doi          = {10.1142/S0218194022500152},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {363-393},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Security versus compliance: An empirical study of the impact of industry standards compliance on application security},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reuse-based agile development process for drone software
systems. <em>IJSEKE</em>, <em>32</em>(3), 347–362. (<a
href="https://doi.org/10.1142/S0218194022500255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drones can perform air operations that are hard to be executed using manned aircrafts. The usage of drones in different domains brings significant environmental benefits and economic savings while decreasing risks to human life. Recently, a number of approaches have been introduced to support the development of drone software systems. However, developing customized drone software based on end-user needs is still a time consuming process. Such delay in software production does not match end-users expectations. Therefore, in the COMP4DRONES project (C4D, for short), we propose an agile-development process that is based on reuse to shorten the drone software development. In this process, based on the user requirements, a number of reusable components are selected from a repository that matches the user requirements. These components are then integrated to have a fully functioning drone system. This repository will be filled with reusable components that are being developed during the C4D project (i.e. the key enabling technologies for drones).},
  archive      = {J_IJSEKE},
  author       = {Mahmoud Hussein and Réda Nouacer},
  doi          = {10.1142/S0218194022500255},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {347-362},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Reuse-based agile development process for drone software systems},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BPMN data model for multi-perspective process mining on
blockchain. <em>IJSEKE</em>, <em>32</em>(2), 317–345. (<a
href="https://doi.org/10.1142/S0218194022500115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mining mainly focuses on discovering control flow models, conformance checking and analyzing bottlenecks. It extends the scope by looking at the other perspectives like time, data and resources by connecting events in the event logs to this process model. These perspectives are not isolated and are all related to each other. For each perspective, there is a different technique, which is dedicated to the relevant perspective, applied and these techniques may need to consume the results of one another in a sequence of process mining analyses. As a result, a holistic process model is created by attaching and binding related attributes of the event logs to the backbone (control flow) of the model. Therefore, representing the holistic model and keeping what is produced from each perspective in a secure and immutable way while applying the multiple perspectives become important. In this study, a BPMN-extended Data Model is proposed to put together the models from the multi-perspective process mining and a tool is developed to keep this data model as an asset into a private blockchain developed by using Hyperledger Fabric. The practical relevance and validity of the approach are shown in the case studies that use real-life data from two different domains.},
  archive      = {J_IJSEKE},
  author       = {Burakcan Ekici and Tugba Gurgen Erdogan and Ayça Kolukısa Tarhan},
  doi          = {10.1142/S0218194022500115},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {317-345},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {BPMN data model for multi-perspective process mining on blockchain},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Formal modeling and performance evaluation for hybrid
systems: A probabilistic hybrid process algebra-based approach.
<em>IJSEKE</em>, <em>32</em>(2), 283–315. (<a
href="https://doi.org/10.1142/S0218194022500103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic behavior is omnipresent in computer-controlled systems, in particular, so-called safety-critical hybrid systems, due to various reasons, like uncertain environments or fundamental properties of nature. In this paper, we extend the existing hybrid process algebra ACP h s s r t with probability without sacrificing the nondeterministic choice operator. The existing approximate probabilistic bisimulation relation is fragile and not robust in the sense of being dependent on the deviation range of the transition probability. To overcome this defect, a novel approximate probabilistic bisimulation is proposed which is inspired by the idea of Probably Approximately Correct (PAC) by relaxing the constraints of transition probability deviation range. Traditional temporal logics, even probabilistic temporal logics, are expressive enough, but they are limited to producing only true or false responses, as they are still logics and not suitable for performance evaluation. To settle this problem, we present a new performance evaluation language that expands quantitative analysis from the value range of { 0 , 1 } to real number to reason over probabilistic systems. After that, the corresponding algorithms for performance evaluation are given. Finally, an industrial example is given to demonstrate the effectiveness of our method.},
  archive      = {J_IJSEKE},
  author       = {Fujun Wang and Zining Cao and Lixing Tan and Zhen Li},
  doi          = {10.1142/S0218194022500103},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {283-315},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Formal modeling and performance evaluation for hybrid systems: A probabilistic hybrid process algebra-based approach},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correlation of agile principles and practices to software
project performance: An AHP–delphi analysis. <em>IJSEKE</em>,
<em>32</em>(2), 257–281. (<a
href="https://doi.org/10.1142/S0218194022500127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, Extreme Programming, Scrum, and Kanban are the three most commonly used methods in agile software development (ASD) projects. Each method has different practices and shares a set of agile principles, where quality, time, and cost are the three project performance indicators. Companies may focus on and prioritize certain indicators based on industry or project differences. Therefore, choosing appropriate practices that fit the specific performance indicator is an important decision for organizations. This study utilizes a hierarchical consensus model to examine the correlation between four agile practice groups, six agile principle categories, and three project performance indicators. The modified Delphi method was applied to collect the pairwise comparison data, and the analytic hierarchy process was utilized to analyze the data. A Delphi panel of experts from both academia and industry was established to reach a consensus on the correlation priority using pairwise comparison matrices. The principle of cooperation between customer and developer is considered the most important principle related to project time and cost performance, while the technical excellence principle is the most important principle related to project quality performance. These results can assist organizations and practitioners in adopting the ASD practices that will best enhance their competitive advantage.},
  archive      = {J_IJSEKE},
  author       = {Yulianus Palopak and Sun-Jen Huang},
  doi          = {10.1142/S0218194022500127},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {257-281},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Correlation of agile principles and practices to software project performance: An AHP–Delphi analysis},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting duplicate questions in stack overflow via source
code modeling. <em>IJSEKE</em>, <em>32</em>(2), 227–255. (<a
href="https://doi.org/10.1142/S0218194022500073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stack Overflow is one of the most popular Question-Answering sites for programmers. However, it faces the problem of question duplication, where newly created questions are identical to previous questions. Existing works on duplicate question detection in Stack Overflow extract a set of textual features on the question pairs and use supervised learning approaches to classify duplicate question pairs. However, they do not consider the source code information in the questions. While in some cases, the intention of a question is mainly represented by the source code. In this paper, we aim to learn the semantics of a question by combining both text features and source code features. We use word embedding and convolutional neural networks to extract textual features from questions to overcome the lexical gap issue. We use tree-based convolutional neural networks to extract structural and semantic features from source code. In addition, we perform multi-task learning by combining the duplication question detection task with a question tag prediction side task. We conduct extensive experiments on the Stack Overflow dataset and show that our approach can detect duplicate questions with higher recall and MRR compared with baseline approaches on Python and Java programming languages.},
  archive      = {J_IJSEKE},
  author       = {Wei Gao and Jian Wu and Guandong Xu},
  doi          = {10.1142/S0218194022500073},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {227-255},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Detecting duplicate questions in stack overflow via source code modeling},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GASSER: A multi-objective evolutionary approach for test
suite reduction. <em>IJSEKE</em>, <em>32</em>(2), 193–225. (<a
href="https://doi.org/10.1142/S0218194022500085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression testing is a practice that ensures a System Under Test (SUT) still works as expected after changes have been implemented. The simplest approach for regression testing is Retest-all , which consists of re-executing the entire Test Suite (TS) on the changed version of the SUT. Retest-all could be expensive in case a SUT and its TS grow in size and, if resources are insufficient, its application could be impracticable. A Test Suite Reduction (TSR) approach aims to overcome these issues by reducing the size of TSs, while preserving their fault-detection capability. In this paper, we introduce and validate an approach for TSR based on a multi-objective evolutionary algorithm, namely, Non-dominated Sorting Genetic Algorithm II (NSGA-II). This approach seeks to reduce TSs by maximizing both statement coverage and diversity of test cases of the reduced TSs, while minimizing the size of the reduced TSs. We named this approach Genetic Algorithm for teSt SuitE Reduction (GASSER). To assess GASSER, we conducted an experiment on 19 versions of four software systems from a public dataset—i.e. Software-artifact Infrastructure Repository (SIR). We compared GASSER with nine baseline approaches. The comparison was based on the size of the reduced TSs and their fault-detection capability. The most important take-away result is that GASSER, as compared with the baseline approaches, reduces more the size of the TSs with a non-significant effect on their fault-detection capability. The results of our empirical assessment suggest that the application of multi-objective evolutionary algorithms and, in particular, NSGA-II might represent a viable means to deal with TSR.},
  archive      = {J_IJSEKE},
  author       = {Carmen Coviello and Simone Romano and Giuseppe Scanniello and Giuliano Antoniol},
  doi          = {10.1142/S0218194022500085},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {193-225},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {GASSER: A multi-objective evolutionary approach for test suite reduction},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A systematic mapping study of search-based software
engineering for enterprise application integration. <em>IJSEKE</em>,
<em>32</em>(2), 163–191. (<a
href="https://doi.org/10.1142/S0218194022500140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Search-Based Software Engineering (SBSE) is widely used in different fields of Software Engineering, notoriously, in Enterprise Application Integrations (EAIs). EAI encompasses methodologies, techniques and tools that a software engineer can use to create integration solutions. SBSE is currently an active research topic of increasing interest. The number and diversity of publications produced yearly are large to the extent that it is hard to identify the active research groups, their locations, techniques used and research topics that have not received enough attention. To answer these questions categorically, we have conducted systematic mapping study of the literature. In this paper, we report our methodology and findings. In our study, we used systematic search strategies that resulted in the retrieval of 560 articles, of which we first selected 25. Second, on the basis of the authors’ experience, we included eight additional articles. Finally, we used a snowballing sample technique to include another 12 articles. The results demonstrate that during the last two decades (1999–2020) EAI has benefited from the use of Search-Based Software Engineering techniques.},
  archive      = {J_IJSEKE},
  author       = {Angela Mazzonetto and Rafael Z. Frantz and Fabricia Roos-Frantz and Carlos Molina-Jimenez and Sandro Sawicki},
  doi          = {10.1142/S0218194022500140},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {163-191},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A systematic mapping study of search-based software engineering for enterprise application integration},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Inconsistency-tolerant hierarchical probabilistic CTL model
checking: Logical foundations and illustrative examples.
<em>IJSEKE</em>, <em>32</em>(1), 131–162. (<a
href="https://doi.org/10.1142/S0218194022500061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, an inconsistency-tolerant hierarchical probabilistic computation tree logic (IHpCTL) is developed to establish a new extended model-checking paradigm referred to as IHpCTL model checking, which is intended to verify randomized, open, large, and complex concurrent systems. The proposed IHpCTL is constructed based on several previously established extensions of the standard probabilistic temporal logic known as probabilistic computation tree logic (pCTL), which is widely used for probabilistic model checking. IHpCTL is shown to be embeddable into pCTL and is relatively decidable with respect to pCTL. This means that the decidability of pCTL with certain probability measures implies the decidability of IHpCTL. The results indicate that we can effectively reuse the previously proposed pCTL model-checking algorithms for IHpCTL model checking. Moreover, in this study, some new illustrative examples for clinical reasoning verification are addressed based on IHpCTL model checking.},
  archive      = {J_IJSEKE},
  author       = {Norihiro Kamide},
  doi          = {10.1142/S0218194022500061},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {131-162},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Inconsistency-tolerant hierarchical probabilistic CTL model checking: Logical foundations and illustrative examples},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting ride hailing service demand using autoencoder and
convolutional neural network. <em>IJSEKE</em>, <em>32</em>(1), 109–129.
(<a href="https://doi.org/10.1142/S021819402250005X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ride hailing services, such as Uber, Lyft, and Grab, have become a major transportation mode in the last decade. The number of current passenger requests is one of the important factors for such services routing and pricing algorithms. Therefore, predicting future passenger request for ride hailing services can boost the efficiency of the service for both drivers and riders by pre-planning the allocation of vehicles and avoiding traffic congestions. Demand forecasting for ride hailing services relies upon the spatial and temporal correlations of its features. The existing literatures mostly divide the target area into rectangular grids (based on the longitude and latitude), consider only adjacent grids for spatial correlation, and calculate demand for each grid independently. An individual grid can contain different regions with high and low demand or have a major part of it outside the land area, which obscures the granularity and precision of estimations and predictions. This paper attempts to mitigate the limitations of grid-based methods by estimating and predicting ride hailing service demand between geographic regions as pickup and destination zones. For predicting demand, a convolutional neural network is integrated with a recurrent autoencoder network to best capture the spatial–temporal correlations of features, including time of the day, month, year, weekend, holiday, pickup zone, destination, and demand. In our experiments, we forecast the demand for each pickup–destination pair for the next day at a certain hour by observing the demands over the past 2 weeks during the same hour in the New York City hire vehicle data set. Using the same model (CNN-biLSTM-AE) to predict demand for geographical regions, it achieved an R 2 of 0.984, while predicting demand for cells in the grid achieved an R 2 0.545. While using the geographical regions instead of grids for partitioning the space, we compared our deep learning model with LSTM, CNN, CNN-LSTM, and LSTM-AE models and observed an improvement in R 2 from 0.632 to 0.767 and an improvement in RMSE from 20.53 to 16.33 against CNN.},
  archive      = {J_IJSEKE},
  author       = {Zinat Ara and Mahdi Hashemi},
  doi          = {10.1142/S021819402250005X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {109-129},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Predicting ride hailing service demand using autoencoder and convolutional neural network},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature selection and spatial-temporal forecast of oceanic
niño index using deep learning. <em>IJSEKE</em>, <em>32</em>(1), 91–107.
(<a href="https://doi.org/10.1142/S0218194022500048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {El Niño-Southern Oscillation (ENSO) is a climate phenomenon caused due to irregular periodic oscillation in easterly winds and sea surface temperature (SST) over the tropical Pacific Ocean. ENSO is one of the main drivers of Earth’s inter-annual climate variability, which causes climate anomalies in the form of tropical cyclones, severe storms, heavy rainfalls and droughts. Due to the impact of ENSO on global climate, forecasting ENSO is of great importance. However, forecast accuracy of ENSO for a lead time of one year is low. ENSO events are forecasted through Oceanic Niño Index (ONI), which is the three-month running mean of SST anomalies over the Niño 3.4 region (5 ∘ N-5 ∘ S, 120 ∘ W-170 ∘ W). Features, such as SST, sea level pressure, zonal wind speed and meridional wind speed that contribute in determining ONI are mapped on spatial or geographical grids, where each spatial or geographical grid represents the values of one feature at a snapshot. Juxtaposing the spatial grids of all features creates a layered map at a snapshot. The layered spatial feature map is constructed at different snapshots, and they all are fed to the CLSTM to forecast ONI at lead times of 1, 3, 6, 9 and 12 months. This study employs backward stepwise feature selection based on generalization accuracy to find the most effective features. SST showed to be the best feature for forecasting ONI. Experiments showed that the CLSTM outperforms Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM) and Standard Neural Network (SNN) in terms of coefficient of determination ( R 2 ), Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). More specifically, an improvement in R 2 values by 27.6%, 20.9%, 25% and 15.2% over CNN is observed for lead times of 3, 6, 9 and 12 months, respectively.},
  archive      = {J_IJSEKE},
  author       = {Jahnavi Jonnalagadda and Mahdi Hashemi},
  doi          = {10.1142/S0218194022500048},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {91-107},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Feature selection and spatial-temporal forecast of oceanic niño index using deep learning},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DockerPedia: A knowledge graph of software images and their
metadata. <em>IJSEKE</em>, <em>32</em>(1), 71–89. (<a
href="https://doi.org/10.1142/S0218194022500036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An increasing amount of researchers use software images to capture the requirements and code dependencies needed to carry out computational experiments. Software images preserve the computational environment required to execute a scientific experiment and have become a crucial asset for reproducibility. However, software images are usually not properly documented and described, making it challenging for scientists to find, reuse and understand them. In this paper, we propose a framework for automatically describing software images in a machine-readable manner by (i) creating a vocabulary to describe software images; (ii) developing an annotation framework designed to automatically document the underlying environment of software images and (iii) creating DockerPedia, a Knowledge Graph with over 150,000 annotated software images, automatically described using our framework. We illustrate the usefulness of our approach in finding images with specific software dependencies, comparing similar software images, addressing versioning problems when running computational experiments; and flagging problems with vulnerable software dependencies.},
  archive      = {J_IJSEKE},
  author       = {Maximiliano Osorio and Carlos Buil-Aranda and Idafen Santana-Perez and Daniel Garijo},
  doi          = {10.1142/S0218194022500036},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {71-89},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {DockerPedia: A knowledge graph of software images and their metadata},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A context-aware architecture for realizing business process
adaptation strategies using fuzzy planning. <em>IJSEKE</em>,
<em>32</em>(1), 37–70. (<a
href="https://doi.org/10.1142/S0218194022500024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Business competency emerges in flexibility and reliability of services that an enterprise provides. To reach that, executing business processes on a context-aware business process management suite which is equipped with monitoring, modeling and adaptation mechanisms and smart enough to react properly using adaptation strategies at runtime, are a major requisite. In this paper, a context-aware architecture is described to bring adaptation to common business process execution software. The architecture comes with the how-to-apply methodology and is established based on process standards like business process modeling notation (BPMN), business process execution language (BPEL), etc. It follows MAPE-K adaptation cycle in which the knowledge, specifically contextual information and their related semantic rules — as the input of adaptation unit — is modeled in our innovative context ontology, which is also extensible for domain-specific purposes. Furthermore, to support separation of concerns, we took apart event-driven adaptation requirements from process instances; these requirements are triggered based on ontology reasoning. Also, the architecture supports fuzzy-based planning and extensible adaptation realization mechanisms to face new or changing situations adequately. We characterized our work in comparison with related studies based on five key adaptation metrics and also evaluated it using an online learning management system case study.},
  archive      = {J_IJSEKE},
  author       = {Leila Kord Toudeshki and Mir Ali Seyyedi and Afshin Salajegheh},
  doi          = {10.1142/S0218194022500024},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {37-70},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A context-aware architecture for realizing business process adaptation strategies using fuzzy planning},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An empirical study on higher-order mutation-based fault
localization. <em>IJSEKE</em>, <em>32</em>(1), 1–35. (<a
href="https://doi.org/10.1142/S0218194022500012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault localization is one of the most expensive activities in software debugging. Mutation-based fault localization (MBFL) is a commonly studied technique that applied mutation analysis to find the location of faults in the programs. Previous studies showed that MBFL adopted First-Order-Mutants (FOMs) that could achieve promising results in single-fault localization, but it did not perform well in multiple-fault localization. Recently, Higher-Order-Mutants (HOMs) were proposed for modeling complex faults but whether HOMs can help in fault localization is still unknown. In this paper, we investigate the performance of MBFL with FOMs and HOMs on single- and multiple-fault localization. Moreover, to study the characteristics of HOMs, we divide HOMs into three groups (i.e. Accurate HOMs, Partially accurate HOMs, and Inaccurate HOMs) by considering different mutation locations. Based on the empirical results on 186 versions of six real-world programs, we find that (1) In single-fault localization, FOMs can achieve better performance than HOMs. (2) However, in multiple-fault localization, HOMs (2-HOMs) localize more faults than FOMs. (3) Furthermore, different types of HOMs have different fault localization effectiveness, where Accurate HOMs outperform the other two HOMs categories. Therefore, the researchers should propose methods to find HOMs more useful for fault localization.},
  archive      = {J_IJSEKE},
  author       = {Haifeng Wang and Zheng Li and Yong Liu and Xiang Chen},
  doi          = {10.1142/S0218194022500012},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {1-35},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An empirical study on higher-order mutation-based fault localization},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
