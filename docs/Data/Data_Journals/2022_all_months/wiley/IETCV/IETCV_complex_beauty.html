<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IETCV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ietcv---58">IETCV - 58</h2>
<ul>
<li><details>
<summary>
(2022). Bounding-box deep calibration for high performance face
detection. <em>IETCV</em>, <em>16</em>(8), 747–758. (<a
href="https://doi.org/10.1049/cvi2.12122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern convolutional neural networks (CNNs)-based face detectors have achieved tremendous strides due to large annotated datasets. However, misaligned results with high detection confidence but low localization accuracy restrict the further improvement of detection performance. In this paper, the authors first predict high confidence detection results on the training set itself. Surprisingly, a considerable part of them exist in the same misalignment problem. Then, the authors carefully examine these cases and point out that annotation misalignment is the main reason. Later, a comprehensive discussion is given for the replacement rationality between predicted and annotated bounding-boxes. Finally, the authors propose a novel Bounding-Box Deep Calibration (BDC) method to reasonably replace misaligned annotations with model predicted bounding-boxes and offer calibrated annotations for the training set. Extensive experiments on multiple detectors and two popular benchmark datasets show the effectiveness of BDC on improving models&#39; precision and recall rate, without adding extra inference time and memory consumption. Our simple and effective method provides a general strategy for improving face detection, especially for light-weight detectors in real-time situations.},
  archive      = {J_IETCV},
  author       = {Shi Luo and Xiongfei Li and Xiaoli Zhang},
  doi          = {10.1049/cvi2.12122},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {747-758},
  shortjournal = {IET Comput. Vis.},
  title        = {Bounding-box deep calibration for high performance face detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive feedback connection with a single-level feature for
object detection. <em>IETCV</em>, <em>16</em>(8), 736–746. (<a
href="https://doi.org/10.1049/cvi2.12121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From the perspective of detector optimisation, detecting objects using only a one-level feature cannot provide good performance for a wide range of scales. Various complex feature pyramidal structures address this problem using the divide-and-conquer strategy and multi-scale feature fusion. However, this requires adding too many additional convolutional layers and fusion operations. To address the issue, a simple detection part is proposed, which includes three components, namely a one-level feature map for detection, the encoder structure with feedback connection, and a decoupled head. The redesigned encoder and decoupled head can successfully address the performance decline caused by the one-level feature-based detection. Moreover, the proposed method can accelerate the convergence of the detector and achieve a faster inference time. Based on the optimised detection part, an adaptive feedback connection with a single-level feature (AFS) is proposed for object detection. The experiments conducted on the MS COCO 2017 benchmark show that the proposed method can achieve comparable results with its multi-scale pyramid counterpart, You Only Look Once v4 (YOLOv4). In addition, AFS can help the YOLOv4 achieve 44.9 mAP at 27 frame per second and converging 82 epochs earlier under the image size of 608×608, which represents a 42.1% improvements in the convergence speed.},
  archive      = {J_IETCV},
  author       = {Zhongling Ruan and Jianzhong Cao and Hao Wang and Huinan Guo and Xin Yang},
  doi          = {10.1049/cvi2.12121},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {736-746},
  shortjournal = {IET Comput. Vis.},
  title        = {Adaptive feedback connection with a single-level feature for object detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A tooth surface design method combining semantic guidance,
confidence, and structural coherence. <em>IETCV</em>, <em>16</em>(8),
727–735. (<a href="https://doi.org/10.1049/cvi2.12120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on tooth surface design based on deep neural networks has recently achieved progress in terms of both accuracy and execution efficiency. However, unrealistic outputs are still a challenging issue, partially because of (1) the lack of semantic guidance, (2) the inability to discover and rectify false results, and (3) the lack of exploration of structural coherence in intermediate layers. In this paper, we present an approach to predict depth images for designed teeth based on a conditional generative adversarial network (CGAN) by incorporating semantic guidance. Moreover, the uncertainty of semantic inference is employed to improve the model outputs, and a structural coherence loss is proposed for adversarial learning to enhance the discrimination capability of the network in intermediate layers. We evaluate the performance of our approach with the Shining3D tooth dataset. The experimental results show that our method produces better results than the other available approaches in terms of accuracy.},
  archive      = {J_IETCV},
  author       = {Peng Wang and Yan Tian and Nali Liu and Jialei Wang and Shuangming Chai and Xun Wang and Ruili Wang},
  doi          = {10.1049/cvi2.12120},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {727-735},
  shortjournal = {IET Comput. Vis.},
  title        = {A tooth surface design method combining semantic guidance, confidence, and structural coherence},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robust and efficient method for skeleton-based human
action recognition and its application for cross-dataset evaluation.
<em>IETCV</em>, <em>16</em>(8), 709–726. (<a
href="https://doi.org/10.1049/cvi2.12119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based human action recognition has emerged recently thanks to its compactness and robustness to appearance variations. Although impressive results have been obtained in recent years, the performance of skeleton-based action recognition methods has to be improved to be deployed in real-time applications. Recently, a lightweight network structure named Double-feature Double-motion Network (DD-Net) has been proposed for the skeleton-based human action recognition. With high speed, the DD-Net achieves state-of-the-art performance on hand and body actions. The DD-Net could not distinguish actions if they have a weak connection with the global trajectories. However, the DD-Net is suitable for human action recognition where actions strongly correlate to the global trajectories. In this paper, the authors propose TD-Net, an improved version of the DD-Net in which a new branch is added. The new branch takes the normalised coordinates of joints (NCJ) to enrich the spatial information. On five datasets for skeleton-based human activity recognition that are MSR-Action3D, CMDFall, JHMDB, FPHAB, and NTU RGB + D, the TD-Net consistently obtains superior performance compared with the baseline model DD-Net. The proposed method outperforms different state-of-the-art methods, including both hand-designed and deep learning-based methods on four datasets (MSR-Action3D, CMDFall, JHMDB, and FPHAB). Furthermore, the generalisation of the proposed method is confirmed through cross-dataset evaluation. To illustrate the potential use of the model for real-time human action recognition, the authors have deployed an application on an edge device. The experimental result shows that the application can process up to 40 fps for pose estimation using MediaPipe. It takes only 0.04 ms to recognise an action from skeleton sequences.},
  archive      = {J_IETCV},
  author       = {Tien-Thanh Nguyen and Dinh-Tan Pham and Hai Vu and Thi-Lan Le},
  doi          = {10.1049/cvi2.12119},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {709-726},
  shortjournal = {IET Comput. Vis.},
  title        = {A robust and efficient method for skeleton-based human action recognition and its application for cross-dataset evaluation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-stage partial image-text clustering (TPIT-c).
<em>IETCV</em>, <em>16</em>(8), 694–708. (<a
href="https://doi.org/10.1049/cvi2.12117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep multi-model clustering is a challenging task for data analysis since it learns a universal semantic representation to find correct clusters from heterogeneous samples. However, most existing methods 1) lack an effective approach to getting a global representation of visual instances, which results in a huge semantic gap between visual and textual space. 2) hardly consider partial multi-modal, where each instance is represented by only one modality. In reality, the pairing information for modalities is not available for all instances. To tackle the above issues, we propose a novel model called the Two-Stage Partial Image-Text Clustering (TPIT-C) model. Firstly, we build an interpretable reasoning network to obtain the salient regions and semantic concepts of the scene in order to generate global semantic concepts. Secondly, we construct an adversarial learning module to align textual and visual instances into a unified space by virtue of cycle-consistency. The experimental evaluations on public unpaired multi-model datasets illustrated that the proposed method has better performance and the effectiveness of our algorithm in the partial image-text clustering task.},
  archive      = {J_IETCV},
  author       = {Dongjin Guo and Xiaoming Su and Yahong Lian and Limin Liu and Haibo Wang},
  doi          = {10.1049/cvi2.12117},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {694-708},
  shortjournal = {IET Comput. Vis.},
  title        = {Two-stage partial image-text clustering (TPIT-c)},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contour loss for instance segmentation via k-step distance
transformation image. <em>IETCV</em>, <em>16</em>(8), 683–693. (<a
href="https://doi.org/10.1049/cvi2.12114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance segmentation aims to locate targets in the image and segment each target at the pixel level, which is one of the most important tasks in computer vision. Mask R-CNN is a classic method of instance segmentation, but we find that its predicted masks are unclear and inaccurate near contours. To cope with this problem, we draw on the idea of contour matching based on distance transformation image and propose a novel loss function called contour loss. Contour loss is designed to specifically optimise the contour parts of the predicted masks, thus can assure more accurate instance segmentation. To make the proposed contour loss be jointly trained under modern neural network frameworks, we design a differentiable k-step distance transformation image calculation module, which can approximately compute truncated distance transformation images of the predicted mask and the corresponding ground-truth mask online. The proposed contour loss can be integrated into existing instance segmentation methods such as Mask R-CNN, and combined with their original loss functions without modification of the structures of inference network, thus has strong versatility. Experimental results on COCO show that contour loss is effective, which can further improve instance segmentation performances.},
  archive      = {J_IETCV},
  author       = {Xiaolong Guo and Xiaosong Lan and Kunfeng Wang and Shuxiao Li},
  doi          = {10.1049/cvi2.12114},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {683-693},
  shortjournal = {IET Comput. Vis.},
  title        = {Contour loss for instance segmentation via k-step distance transformation image},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning in the grading of diabetic retinopathy: A
review. <em>IETCV</em>, <em>16</em>(8), 667–682. (<a
href="https://doi.org/10.1049/cvi2.12116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic Retinopathy (DR) grading into different stages of severity continues to remain a challenging issue due to the complexities of the disease. Diabetic Retinopathy grading classifies retinal images to five levels of severity ranging from 0 to 5, which represents No DR, Mild non-proliferative diabetic retinopathy (NPDR), Moderate NPDR, Severe NPDR, and proliferative diabetic retinopathy. With the advancement of Deep Learning, studies on the application of the Convolutional Neural Network (CNN) in DR grading have been on the rise. High accuracy and sensitivity are the desired outcome of these studies. This paper reviewed recently published studies that employed CNN for DR grading to 5 levels of severity. Various approaches are applied in classifying retinal images which are, (i) by training CNN models to learn the features for each grade and (ii) by detecting and segmenting lesions using information about their location such as microaneurysms, exudates, and haemorrhages. Public and private datasets have been utilised by researchers in classifying retinal images for DR. The performance of the CNN models was measured by accuracy, specificity, sensitivity, and area under the curve. The CNN models and their performance varies for every study. More research into the CNN model is necessary for future work to improve model performance in DR grading. The Inception model can be used as a starting point for subsequent research. It will also be necessary to investigate the attributes that the model uses for grading.},
  archive      = {J_IETCV},
  author       = {Nurul Mirza Afiqah Tajudin and Kuryati Kipli and Muhammad Hamdi Mahmood and Lik Thai Lim and Dayang Azra Awang Mat and Rohana Sapawi and Siti Kudnie Sahari and Kasumawati Lias and Suriati Khartini Jali and Mohammed Enamul Hoque},
  doi          = {10.1049/cvi2.12116},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {667-682},
  shortjournal = {IET Comput. Vis.},
  title        = {Deep learning in the grading of diabetic retinopathy: A review},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scale robust point matching-net: End-to-end scale point
matching using lie group. <em>IETCV</em>, <em>16</em>(7), 655–666. (<a
href="https://doi.org/10.1049/cvi2.12134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud matching is an important procedure in a variety of computer vision tasks. Traditional point cloud matching methods have made great progress, while neural network-based approaches are becoming a trend, powered by their strong capabilities of feature extraction. Existing point matching neural networks, however, mainly focus on the rigid transformation. More complex transformations should also be considered in many scenarios. In this regard, the authors extend the rigid registration to non-rigid cases and propose a network called the Scale Robust Point Matching (SRPM)-Net for scale point matching. This robust structure-preserving network is implemented by incorporating Lie group parametrisation. It is conducted by Lie group linearisation representation with the constraints of parameters under the corresponding basis of Lie algebra. SRPM-Net preserves the structure of the solution and avoids degeneration. The contributions of this paper lie in two aspects: Most importantly, SRPM-Net provides an extendable framework for handling complicated transformations. Secondly, it introduces a new feature learning module, which better preserves the shape structure by aggregating the high-dimensional feature and calculating the normal vector of point cloud surface automatically. Experimental results show that SRPM-Net is more robust and accurate than existing traditional and recent deep learning methods under various situations.},
  archive      = {J_IETCV},
  author       = {Xin Wang and Hui Ding and Guangwei Zhao and Yaxin Peng and Chaomin Shen},
  doi          = {10.1049/cvi2.12134},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {655-666},
  shortjournal = {IET Comput. Vis.},
  title        = {Scale robust point matching-net: End-to-end scale point matching using lie group},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse point-voxel aggregation network for efficient point
cloud semantic segmentation. <em>IETCV</em>, <em>16</em>(7), 644–654.
(<a href="https://doi.org/10.1049/cvi2.12131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective and efficient semantic segmentation of 3D point cloud data is important for many tasks. Many methods for point cloud semantic segmentation rely on computationally expensive sampling and grouping layers to process irregular points, while others convert irregular points into regular volumetric grids and process them with a 3D U-Net-based semantic segmentation network. However, most of these methods suffer from high computational costs and cannot be applied to the real-time processing of large-scale point clouds. To address these issues, we propose a computationally efficient point-voxel-based network architecture named Sparse Point-Voxel Aggregation Network (SPVAN) for point cloud semantic segmentation. It consists of an encoding layer that consists of sparse convolution and MLP layers and a new decoding layer called Point Feature Aggregation Layer (PFAL) that is only composed of feature interpolation and MLP layers. Compared with recent popular point-voxel-based methods with the U-Net-based network, our method does not need 3D convolution networks in the decoding layer and thus achieves a higher speed. Experimental results on the large-scale SemanticKITTI dataset show that our method gets a good balance between the efficiency and the performance. Moreover, our method achieves on-par or better performance than previous methods for semantic segmentation on the challenging S3DIS dataset.},
  archive      = {J_IETCV},
  author       = {Zheng Fang and Binyu Xiong and Fei Liu},
  doi          = {10.1049/cvi2.12131},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {644-654},
  shortjournal = {IET Comput. Vis.},
  title        = {Sparse point-voxel aggregation network for efficient point cloud semantic segmentation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PCCN-RE: Point cloud colourisation network based on
relevance embedding. <em>IETCV</em>, <em>16</em>(7), 632–643. (<a
href="https://doi.org/10.1049/cvi2.12112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The colour of the point cloud provides abundant information for perception tasks such as autonomous driving and virtual reality. However, few prior work studied the automatic colourisation of colourless point clouds. In this paper, the authors propose a novel method named as Point cloud Colorization Network based on Relevance Embedding (PCCN-RE) relying on three structures: a relevance embedding structure that efficiently captures local information through the calculation of a covariance matrix within nearby points; a weighted pooling structure designed to facilitate the fusion of features; an enhanced spatial transform network structure that keeps the invariance of input point clouds. On the ShapeNetCore dataset, our PCCN-RE generates more authentic colour than state-of-the-art methods for colourless point clouds and achieves the highest results by obtaining a Peak Signal to Noise Ratio of 9.40 and a Structural Similarity Index of 0.62.},
  archive      = {J_IETCV},
  author       = {Feiran Wang and Xiaoqiang Li and Jitao Liu},
  doi          = {10.1049/cvi2.12112},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {632-643},
  shortjournal = {IET Comput. Vis.},
  title        = {PCCN-RE: Point cloud colourisation network based on relevance embedding},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Point cloud completion by dynamic transformer with adaptive
neighbourhood feature fusion. <em>IETCV</em>, <em>16</em>(7), 619–631.
(<a href="https://doi.org/10.1049/cvi2.12098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud completion aims to reconstruct detailed structures from partial observations. However, previous methods often suffered from inaccurate neighbourhood feature extraction and rough reconstruction of complex structures, which is difficult to complete detailed semantic shapes. To solve this problem, we present a method that applies dynamic transformers with adaptive neighbourhood feature fusion operations to resume complete point clouds. Firstly, we propose an adaptive neighbourhood feature extraction module, which contains a learnable global neighbourhood selection strategy and a traditional local k-nearest neighbour strategy to dynamically select neighbourhood points according to the shape of different objects. Secondly, we observed that traditional point generation methods based on folding-series operations limit their capacity of generating complex and faithful shapes. Inspired by cell division, we regard the process of reconstructing as point splitting and propose a genetic hierarchical point generation module, which means current points can inherit shape features from previous points and generate more detailed structures of their own. Extensive experiments of point cloud completion are carried out on PCN and Completion3D datasets to verify the effectiveness of our method. The average category chamfer distances of our method are 7.17(×10 −3 ) in PCN and 7.96(×10 −4 ) in Completion3D, which has better completion performance than other methods.},
  archive      = {J_IETCV},
  author       = {Xinpu Liu and Guoquan Xu and Ke Xu and Jianwei Wan and Yanxin Ma},
  doi          = {10.1049/cvi2.12098},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {619-631},
  shortjournal = {IET Comput. Vis.},
  title        = {Point cloud completion by dynamic transformer with adaptive neighbourhood feature fusion},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Auto calibration of multi-camera system for human pose
estimation. <em>IETCV</em>, <em>16</em>(7), 607–618. (<a
href="https://doi.org/10.1049/cvi2.12130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-camera calibration is an essential step for many spatially aware applications, such as robotic navigation, augmented reality, and 3D human pose estimation. Traditional calibration methods use off-the-shelf checkerboards or triangles as the known world coordinate system and their corresponding corners are set as control points, which heavily depends on specific calibration patterns and is not suitable for calibration pattern-denied environments. In this paper, an automatic calibration method is proposed to calibrate the multi-camera system without the aid of a known calibration pattern. The key idea of the proposed method is that the authors consider the human body, which is always available, as the counterpart of the calibration pattern. The authors’ approach starts with binocular camera calibration, in which the extrinsic and intrinsic parameters are calculated in order and followed by a joint optimisation. With the results of each pair of binocular camera calibration, the multi-camera system calibration is carried out in three steps: (i) parameters initialisation, (ii) extrinsic parameters optimisation, and (iii) jointly optimising intrinsic and extrinsic parameters. Since the authors’ approach does not require additional calibration patterns except for one visible person, it is flexible and easy to be implemented. Real experiments are conducted in different scenes, camera angles, and camera settings. Human pose estimation with the multi-camera system is additionally performed for exhaustive experiments. The experimental results demonstrate that the authors’ method shows superior performance than the traditional method with the aid of a specific calibration pattern.},
  archive      = {J_IETCV},
  author       = {Kang Liu and Lingling Chen and Liang Xie and Jian Yin and Shuwei Gan and Ye Yan and Erwei Yin},
  doi          = {10.1049/cvi2.12130},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {607-618},
  shortjournal = {IET Comput. Vis.},
  title        = {Auto calibration of multi-camera system for human pose estimation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient 6D object pose estimation based on attentive
multi-scale contextual information. <em>IETCV</em>, <em>16</em>(7),
596–606. (<a href="https://doi.org/10.1049/cvi2.12101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {6D pose estimation has been pervasively applied to various robotic applications, such as service robots, collaborative robots, and unmanned warehouses. However, accurate 6D pose estimation is still a challenge problem due to the complexity of application scenarios caused by illumination changes, occlusion and even truncation between objects, and additional refinement is required for accurate 6D object pose estimation in prior work. Aiming at the efficiency and accuracy of 6D object pose estimation in these complex scenes, this paper presents a novel end-to-end network, which effectively utilises the contextual information within a neighbourhood region of each pixel to estimate the 6D object pose from RGB-D images. Specifically, our network first applies the attention mechanism to extract effective pixel-wise dense multimodal features, which are then expanded to multi-scale dense features by integrating pixel-wise features at different scales for pose estimation. The proposed method is evaluated extensively on the LineMOD and YCB-Video datasets, and the experimental results show that the proposed method is superior to several state-of-the-art baselines in terms of average point distance and average closest point distance.},
  archive      = {J_IETCV},
  author       = {Fang Gao and Qingyi Sun and Shaodong Li and Wenbo Li and Yong Li and Jun Yu and Feng Shuang},
  doi          = {10.1049/cvi2.12101},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {596-606},
  shortjournal = {IET Comput. Vis.},
  title        = {Efficient 6D object pose estimation based on attentive multi-scale contextual information},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EAGAN: Event-based attention generative adversarial networks
for optical flow and depth estimation. <em>IETCV</em>, <em>16</em>(7),
581–595. (<a href="https://doi.org/10.1049/cvi2.12115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event camera is a new vision sensor that produces independent asynchronous responses to each pixel&#39;s change of illumination intensity. The unique principle of event camera has many advantages over traditional cameras, such as low latency, high temporal resolution, and high dynamic range (HDR). These advantages make event camera ideal for dealing with high speed, HDR visual tasks, especially in automatic driving scenes. In this study, we propose an image generation network named Event-based attention generative adversarial networks (EAGAN), which simultaneously deals with optical flow and depth estimation of monocular event camera data. In addition to the innovative network architecture and loss function suitable for depth estimation, we are also the first to process incomplete training data to obtain more dense and uniform prediction results. Experiments on the multi-vehicle stereo event camera dataset show that our EAGAN is competitive on the depth estimation task and achieves the state-of-the-art effect in the optical flow estimation task.},
  archive      = {J_IETCV},
  author       = {Xiuhong Lin and Chenhui Yang and Xuesheng Bian and Weiquan Liu and Cheng Wang},
  doi          = {10.1049/cvi2.12115},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {581-595},
  shortjournal = {IET Comput. Vis.},
  title        = {EAGAN: Event-based attention generative adversarial networks for optical flow and depth estimation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DEMVSNet: Denoising and depth inference for unstructured
multi-view stereo on noised images. <em>IETCV</em>, <em>16</em>(7),
570–580. (<a href="https://doi.org/10.1049/cvi2.12102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most deep-learning-based multi-view stereo series studies are concerned with improving the depth prediction accuracy of noise-free images. However, it is difficult to obtain off-the-set clean images in practice and 3D convolutional neural networks require a lot of computing resources. To make full use of its computing power, different types of information can be processed simultaneously in the network. For these two issues, this paper proposes a novel multi-stage network architecture to address depth inference and denoising simultaneously. Specifically, 2D feature maps are first converted into 3D cost volumes containing pixel information and depth information through differentiable homography and Gaussian probability mapping. Then, the cost volume is input into the regularisation module in each network stage to obtain the predicted probability volumes. Furthermore, simple static weights lead to training failure, and it is necessary to dynamically adjust the loss function by gradient normalisation. The proposed method can dispose of pixel information and depth information simultaneously and both reach an excellent level. Extensive experimental results show that the authors’ work surpasses the state-of-the-art denoising on the DTU dataset (adding Gaussian–Poisson noise) and is more robust to noise images in depth inference.},
  archive      = {J_IETCV},
  author       = {Jiawei Han and Xiaomei Chen and Yongtian Zhang and Weimin Hou and Zibo Hu},
  doi          = {10.1049/cvi2.12102},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {570-580},
  shortjournal = {IET Comput. Vis.},
  title        = {DEMVSNet: Denoising and depth inference for unstructured multi-view stereo on noised images},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning for 3D vision. <em>IETCV</em>, <em>16</em>(7),
567–569. (<a href="https://doi.org/10.1049/cvi2.12141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IETCV},
  author       = {Yulan Guo and Hanyun Wang and Ronald Clark and Stefano Berretti and Mohammed Bennamoun},
  doi          = {10.1049/cvi2.12141},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {567-569},
  shortjournal = {IET Comput. Vis.},
  title        = {Deep learning for 3D vision},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gaussian guided IoU: A better metric for balanced learning
on object detection. <em>IETCV</em>, <em>16</em>(6), 556–566. (<a
href="https://doi.org/10.1049/cvi2.12113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most anchor-based detectors use intersection over union (IoU) to assign targets to anchors during training. However, IoU did not pay enough attention to the proximity of the anchor&#39;s centre to the centre of the truth box, resulting in two issues: (1) the most slender objects were given just one anchor, resulting in insufficient supervision information for slender objects during training; (2) IoU cannot accurately represent the degree of alignment between the feature&#39;s receptive field at the anchor&#39;s centre and the object. As a result, some features with good alignment degrees are missing, while others with poor alignment degrees are used, reducing the model&#39;s localisation accuracy. To address these issues, we first created a Gaussian Guided IoU (GGIoU), which prioritises the proximity of the anchor&#39;s centre to the truth box&#39;s centre. We then proposed GGIoU-balanced learning methods, including GGIoU-guided assignment strategy and GGIoU-balanced localisation loss. This method can assign multiple anchors to each slender object, favouring features that are well-aligned with the objects during the training process. A large number of experiments show that GGIoU-balanced learning can solve the aforementioned problems and significantly improve the detection model&#39;s performance.},
  archive      = {J_IETCV},
  author       = {Lijun Gou and Shengkai Wu and Jinrong Yang and Hangcheng Yu and Xiaoping Li},
  doi          = {10.1049/cvi2.12113},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {556-566},
  shortjournal = {IET Comput. Vis.},
  title        = {Gaussian guided IoU: A better metric for balanced learning on object detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual feature fusion network: A dual feature fusion network
for point cloud completion. <em>IETCV</em>, <em>16</em>(6), 541–555. (<a
href="https://doi.org/10.1049/cvi2.12111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud data in the real world is often affected by occlusion and light reflection, leading to incompleteness of the data. Large-region missing point clouds will cause great deviations in downstream tasks. A dual feature fusion network (DFF-Net) is proposed to improve the accuracy of the completion of a large missing region of the point cloud. First, a dual feature encoder is designed to extract and fuse the global and local features of the input point cloud. Subsequently, a decoder is used to directly generate a point cloud of missing region that retains local details. In order to make the generated point cloud more detailed, a loss function with multiple terms is employed to emphasise the distribution density and visual quality of the generated point cloud. A large number of experiments show that the authors’ DFF-Net is better than the previous state-of-the-art methods in the aspect of point cloud completion.},
  archive      = {J_IETCV},
  author       = {Fang Gao and Pengbo Shi and Jiabao Wang and Wenbo Li and Yaoxiong Wang and Jun Yu and Yong Li and Feng Shuang},
  doi          = {10.1049/cvi2.12111},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {541-555},
  shortjournal = {IET Comput. Vis.},
  title        = {Dual feature fusion network: A dual feature fusion network for point cloud completion},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ghost shuffle lightweight pose network with effective
feature representation and learning for human pose estimation.
<em>IETCV</em>, <em>16</em>(6), 525–540. (<a
href="https://doi.org/10.1049/cvi2.12110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite their success, existing human pose estimation approaches mostly have complex architectures, high cost, and lack of lightweight modules. To address this problem, this paper proposes a Ghost Shuffle Lightweight Pose Network (GSLPN) with a more lightweight and efficient network architecture than the popular Lightweight Pose Network. First, in order to condense the scale of the network while maintaining its performance, we stack two lightweight modules, depthwise convolution and the Ghost module, to build our initial prototype bottleneck. Then, we impose a channel shuffle operation on the prototype bottleneck to shuffle the sequence of the feature maps for constructing Ghost Shuffle Bottleneck (Ghost Shuffle Bottleneck) with effective feature representation so as to develop a GSLPN. Second, a lightweight, efficient parallel attention mechanism, Lightweight Pose Parallel Attention, is proposed to improve keypoint locating accuracy. An experiment validating the proposed method showed that GSLPN achieved competitive performance with a smaller model size and less computational complexity than state-of-the-art methods, indicating that the GSLPN is a superior approach for human pose estimation.},
  archive      = {J_IETCV},
  author       = {Senquan Yang and Jiajun Wen and Junjun Fan},
  doi          = {10.1049/cvi2.12110},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {525-540},
  shortjournal = {IET Comput. Vis.},
  title        = {Ghost shuffle lightweight pose network with effective feature representation and learning for human pose estimation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Redefining prior feature space via finetuning a triplet
network for few-shot learning. <em>IETCV</em>, <em>16</em>(6), 514–524.
(<a href="https://doi.org/10.1049/cvi2.12109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning is to distinguish novel concepts with few annotated data, which has attracted much attention due to its requirement of limited training data for target classes. Recent few-shot learning methods usually pretrain a feature extractor with images from the base set to boost the performance of few-shot tasks and classify novel categories in this prior feature space. However, it is difficult for the pretrained feature extractor to extract accurate representations for novel categories, resulting in large amounts of overlapping areas between new classes. To address these issues, the prior feature space with a triplet network to learn a more discriminative space is refined, where features belonging to same class are pulled together and that from different classes are pushed apart. Specifically, the authors first follow recent paradigm of pretraining to obtain a prior feature space. Then, a triplet network with contrastive learning is trained to project the features from this space into a low-dimensional one. The main difference lies in that the authors’ model is based on Maximum A Posteriori (MAP) and the triplet network with hallucinated features is finetuned from it to make them generalise well to novel categories. Finally, the authors conduct classification tasks in the finetuned space. The authors’ intuition is that the overlapping areas in novel categories can be separated by finetuning the triplet network pretrained on base set with contrastive learning. Experimental results on four few-shot benchmarks show that it significantly outperforms the baseline methods, improves around 1.09% ∼ 13.09% than the best results in each dataset on both 1- and 5-shot tasks.},
  archive      = {J_IETCV},
  author       = {Jiaying Wu and Jinglu Hu},
  doi          = {10.1049/cvi2.12109},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {514-524},
  shortjournal = {IET Comput. Vis.},
  title        = {Redefining prior feature space via finetuning a triplet network for few-shot learning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Person re-identification based on feature erasure and
diverse feature learning. <em>IETCV</em>, <em>16</em>(6), 504–513. (<a
href="https://doi.org/10.1049/cvi2.12108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust and comprehensive modelling of targets is the key to successful person re-identification. However, some useful information may be ignored, since CNNs tend to learn from the most distinctive feature region of the human body. In the present study, a multi-branch lightweight network structure that can enhance the ability of diverse feature retrieval is introduced. The proposed network consists of three branches. In the feature erasure branch, a drop block model is added to remove the horizontal region with the highest activation degree from feature vectors so as to allow the network to learn relatively low discrimination features. The global branch is used as an essential supplement to the feature erasure branch. A unified horizontal segmentation strategy is adopted in the local branch to avoid the influence of feature dislocation. Finally, diverse feature learning is achieved through the branch network structure. The proposed method can achieve state-of-the-art results on Market-1501, CUHK03 and DukeMTMC-Reid data sets, thereby demonstrating the effectiveness of the method.},
  archive      = {J_IETCV},
  author       = {Lu Peng and Zhang Jidong and Zhang Zhen and Wang Wei and Dou Yamei},
  doi          = {10.1049/cvi2.12108},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {504-513},
  shortjournal = {IET Comput. Vis.},
  title        = {Person re-identification based on feature erasure and diverse feature learning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight and deep appearance embedding for multiple
object tracking. <em>IETCV</em>, <em>16</em>(6), 489–503. (<a
href="https://doi.org/10.1049/cvi2.12106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main challenge of Multiple Object Tracking (MOT) is that there is great uncertainty in data association when using the tracked predicted values and tracked trajectories. Meanwhile, the MOT is complex and time-consuming. When equipment resources are limited or real-time requirement is high, its application is very limited. Therefore, we propose a Lightweight Deep Appearance Embedding (LDAE) to assist the association of trajectories. Firstly, in addition to motion information in data association, we also introduce more discriminative appearance features to participate in the affinity measure to effectively distinguish similar targets. Secondly, according to the idea of feature mapping, we design a lightweight deep appearance embedding module. It can help extract appearance features with less computation. Finally, we propose a simulated occlusion strategy for the training of the LDAE, which helps improve the ability to recognise different targets in dense scenes. The LDAE dramatically reduces the computational cost and improves the accuracy of data association. Extensive experiments are conducted on the MOT datasets (MOT16, MOT17 and MOT20), which prove that the LDAE outperforms several state-of-the-art trackers in the tracking accuracy and anti-occlusion performance. Furthermore, we apply the LDAE to escalators, which can achieve fast and stable tracking effect.},
  archive      = {J_IETCV},
  author       = {Liangling Ye and Weida Li and Lixin Zheng and Yuanyue Zeng},
  doi          = {10.1049/cvi2.12106},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {489-503},
  shortjournal = {IET Comput. Vis.},
  title        = {Lightweight and deep appearance embedding for multiple object tracking},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel visual classification framework on panoramic
attention mechanism network. <em>IETCV</em>, <em>16</em>(6), 479–488.
(<a href="https://doi.org/10.1049/cvi2.12105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained classification is a challenging task due to the difficulty of finding discriminative features and the localization of feature regions. To handle these challenges, a novel visual classification framework on panoramic attention mechanism that combines multiple attention networks to locate and identify features with more semantic interest is proposed. Firstly, based on the classical convolutional neural network, the global information of the image feature is expressed by linear fusion. Secondly, the foreground attention branch is used to further extract the distinguishing details of the salient features. Then, more features are mined from the complementary object area through the background attention branch to learn more perfect fine-grained feature expression. Finally, three network branches are trained together to enhance the network&#39;s ability to express representative features of fine-grained images. Our model can be viewed as a multi-branch network, which benefits each other and optimizes the network together. Experiments were conducted on CUB-200-2011, Stanford Dogs and FGVC-Aircraft datasets, and the accuracy was used as the quantitative measurement. Experimental results show that the proposed method has the highest accuracy; the average accuracy is 89.8%. It is effective and superior to the current advanced methods.},
  archive      = {J_IETCV},
  author       = {Wenshu Li and Shenhao Li and Lingzhi Yin and Xiaoying Guo and Xu Yang},
  doi          = {10.1049/cvi2.12105},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {479-488},
  shortjournal = {IET Comput. Vis.},
  title        = {A novel visual classification framework on panoramic attention mechanism network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Classification of hyperspectral images via improved
cycle-MLP. <em>IETCV</em>, <em>16</em>(5), 468–478. (<a
href="https://doi.org/10.1049/cvi2.12104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pixel-wise classification of hyperspectral image (HSI) is a hot spot in the field of remote sensing. The classification of HSI requires the model to be more sensitive to dense features, which is quite different from the modelling requirements of traditional image classification tasks. Cycle-Multilayer Perceptron (MLP) has achieved satisfactory results in dense feature prediction tasks because it is an expert in extracting high-resolution features. In order to obtain a more stable receptive field and enhance the effect of feature extraction in multiple directions, we propose an MLP-like model called DriftNet for HSI classification inspired by Cycle-MLP and deformable convolution. Besides, the proposed DriftNet uses a unique ladder-like fully connected structure to achieve progressive activation of neurons and facilitates the fusion of spatial and spectral information, thereby obtaining more sensitive location information for better classification results. Experimental results on three public hyperspectral datasets demonstrate the effectiveness and generalisation of DriftNet.},
  archive      = {J_IETCV},
  author       = {Na Gong and Chunlei Zhang and Heng Zhou and Kai Zhang and Zhongyuan Wu and Xin Zhang},
  doi          = {10.1049/cvi2.12104},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {468-478},
  shortjournal = {IET Comput. Vis.},
  title        = {Classification of hyperspectral images via improved cycle-MLP},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring the spatio-temporal aware graph for video
captioning. <em>IETCV</em>, <em>16</em>(5), 456–467. (<a
href="https://doi.org/10.1049/cvi2.12103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Describing video content automatically in natural language sentences is a long-standing challenge in computer vision. Although existing methods that capture relational information among objects have made significant strides in the past years, the detailed geometrical and temporal information of objects remains to be further explored. To address this problem, a novel Spatio-Temporal Aware Graph is proposed to capture more elaborate visual representations, which are able to exploit the detailed spatio-temporal clues of the extracted object features. By performing graph-structured aggregation, the proposed model is capable of capturing not only the interactions among objects but also the detailed spatio-temporal relations. Meanwhile, a Frame Similarity Graph is constructed on frame features to learn comprehensive representations, which aim to extract the global information that the object feature lacks. Moreover, to capture rich video semantics from different perspectives, multiple video representations, that is appearance and motion information, are utilised to learn discriminative representations. Experiments on the prevalent benchmarks: Microsoft Video Description Corpus and Microsoft Research Video to Text demonstrate that the proposed approach achieves state-of-the-art performance in several widely used evaluation metrics: BLEU-4, METEOR, ROUGE, and CIDEr.},
  archive      = {J_IETCV},
  author       = {Ping Xue and Bing Zhou},
  doi          = {10.1049/cvi2.12103},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {456-467},
  shortjournal = {IET Comput. Vis.},
  title        = {Exploring the spatio-temporal aware graph for video captioning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-stage attention network for video-based person
re-identification. <em>IETCV</em>, <em>16</em>(5), 445–455. (<a
href="https://doi.org/10.1049/cvi2.12100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based person re-identification (Re-ID) has received increasing attention in video surveillance analysis in recent years. To extract relevant information of the target, many existing methods utilise the attention mechanism in the residual block of the ResNet. However, these methods only focus on the residual block and ignore the output of the shortcut part, which also contains rich information about the person. To solve this problem, a different aspect of network design is investigated: the insert position of the attention module. To simultaneously explore the discriminative information in both the residual block and the shortcut, a novel multi-stage attention method is proposed by inserting the attention mechanism between stages of ResNet. Using this method can effectively extract the rich discriminative features of the target to better distinguish different pedestrians and improve the feature extraction capabilities of the model. Extensive experiments are conducted on four popular video-based person Re-ID datasets to demonstrate the effectiveness of the authors’ proposed method and display its superiority with the existing video-based person Re-ID methods.},
  archive      = {J_IETCV},
  author       = {Fan Yang and Wei Li and Binbin Liang and Songchen Han and Xuan Zhu},
  doi          = {10.1049/cvi2.12100},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {445-455},
  shortjournal = {IET Comput. Vis.},
  title        = {Multi-stage attention network for video-based person re-identification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Semantic-meshed and content-guided transformer for image
captioning. <em>IETCV</em>, <em>16</em>(5), 431–444. (<a
href="https://doi.org/10.1049/cvi2.12099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transformer architecture has been the dominant framework for today&#39;s image captioning tasks because of its superior performance. However, existing methods based on transformer often lack the integrated use of multi-level semantic information and are weak in maintaining the relevance of captions to the image. In this paper, a semantic-meshed and content-guided transformer network is introduced for image captioning to solve these problems. The semantic-meshed mechanism allows the model to generate words by selecting semantic information of multiple interaction levels adaptively through attention-based reconstruction. And the content-guided module guides the words generation by using attribute features that represent the image content, which aims to keep the generated caption consistent with the main content of the image. Experiments on dataset on the MSCOCO captioning dataset are conducted to validate the authors’ model and achieve superior results compared to other state-of-the-art method approaches.},
  archive      = {J_IETCV},
  author       = {Xuan Li and Wenkai Zhang and Xian Sun and Xin Gao},
  doi          = {10.1049/cvi2.12099},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {431-444},
  shortjournal = {IET Comput. Vis.},
  title        = {Semantic-meshed and content-guided transformer for image captioning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DD-YOLO: An object detection method combining knowledge
distillation and differentiable architecture search. <em>IETCV</em>,
<em>16</em>(5), 418–430. (<a
href="https://doi.org/10.1049/cvi2.12097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although YOLOv4 has a high detection accuracy, its backbone network CSPDarknet53 requires a large number of parameters, which reduces the inference speed of the detection network to some extent. To address this drawback, a high-efficiency detection network search strategy is proposed. First, the concept of Differentiable Architecture Search (DARTS) is used to search networks with a small number of parameters on the COCO2017 datasets; then, the backbone of YOLOv4 is redesigned by stacking cells. This strategy can reduce the number of parameters of the network. Second, the authors use ResNet101 as the teacher network and DD-YOLO searched by DARTS as the student network to refine the knowledge model and improve the detection accuracy of the model. The proposed model is evaluated by conducting experiments on the COCO2017 test-dev datasets. The results show that DD-YOLO achieves 43.5% mean average precision and 2.3% model accuracy improvement. Meanwhile, the model complexity can be reduced to 61.4% of the original. Moreover, compared with YOLOv4, DD-YOLO is more suitable for mobile deployment.},
  archive      = {J_IETCV},
  author       = {Zhiqiang Xing and Xi Chen and Fengqian Pang},
  doi          = {10.1049/cvi2.12097},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {418-430},
  shortjournal = {IET Comput. Vis.},
  title        = {DD-YOLO: An object detection method combining knowledge distillation and differentiable architecture search},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LineGAN: An image colourisation method combined with a line
art network. <em>IETCV</em>, <em>16</em>(5), 403–417. (<a
href="https://doi.org/10.1049/cvi2.12096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The work on grayscale image colourisation has been significantly improved. Currently, learning-based methods have achieved some great colourisation effects, but existing colour edge bleeding, especially when colourful cartoon characters. In this paper, we focus on the colourisation of cartoon characters from a series in an adversarial environment with a line art network, whose name is LineGAN . LineGAN learns the corresponding colour mapping from datasets, improving the accuracy of image colourisation. Our methods limit the colour boundary overflow by adding a line art frame in the generator. Extensive experiment results on cartoon image colourisation tasks demonstrate that the proposed method can achieve effective results.},
  archive      = {J_IETCV},
  author       = {Dahua Lv and Yuanyuan Pu and Rencan Nie},
  doi          = {10.1049/cvi2.12096},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {403-417},
  shortjournal = {IET Comput. Vis.},
  title        = {LineGAN: An image colourisation method combined with a line art network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). View graph construction for scenes with duplicate structures
via graph convolutional network. <em>IETCV</em>, <em>16</em>(5),
389–402. (<a href="https://doi.org/10.1049/cvi2.12095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {View graph construction aims to effectively organise disordered image dataset through image retrieval technique before structure from motion (SfM). Existing view graph construction methods usually fail to handle scenes with duplicate structure, because these methods solely treat the construction of view graph as a process of image-pair-wise matching and lack in exploiting images&#39; topological details in dataset. In this paper, we handle this problem from a novel perspective to construct view graph in a global paradigm by introducing an end-to-end graph convolutional network (GCN). First, a location-aware embedding module is introduced to encode images into a feature space that takes into account the feature&#39;s location by using Vision Transformer architecture, improving the distinction between features of duplicate structure. Second, graph convolutional network that consists of topological relationship preserving module and feature metric learning module is proposed. Topological relationship preserving network is proposed to help nodes maintain their connected neighbourhood features. By merging the topological connected information into images&#39; embedding, our method can process image matching in a global mode, thus improving the disambiguation ability for images with duplicate scenes. Then a feature metric learning network is embedded into GCN to dynamically compute the linkage prediction among nodes based on their features. Finally, our method combines these three parts to jointly optimise nodes&#39; features and linkage prediction in an end-to-end paradigm. We make qualitative and quantitative comparisons based on three public benchmark datasets and demonstrate that our proposed method performs favourably against other state-of-the-art methods.},
  archive      = {J_IETCV},
  author       = {Yang Peng and Shen Yan and Yuxiang Liu and Yu Liu and Maojun Zhang},
  doi          = {10.1049/cvi2.12095},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {389-402},
  shortjournal = {IET Comput. Vis.},
  title        = {View graph construction for scenes with duplicate structures via graph convolutional network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distribution probability-based self-adaption metric learning
for person re-identification. <em>IETCV</em>, <em>16</em>(4), 376–387.
(<a href="https://doi.org/10.1049/cvi2.12094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification addresses the problem of pedestrian image matching in a non-overlapped surveillance network. Traditional metric learning methods try to learn a fixed pedestrian images matching metric model. However, existing metric learning-based methods have the problem of overfitting the training data. In order to solve this problem, a sample-specific metric learning-based method is proposed. In the perspective of probability distribution, the over-fitting problem is attributed to the problem that the generalisation ability of projection features is weak. Firstly, the proposed method learns a metric subspace, which projects the raw feature to a discriminative subspace. Then, a projection feature selection method is established based on the probability distribution of positive image pairs. According to the proposed method, the probability that the test samples following the training data distribution is closely related to the adaptability. The proposed projection feature selection method selects different projection features for each individual&#39;s similarity distance measure. Finally, extensive experiments on four published datasets verify the effectiveness of the proposed metric learning method. It performs favourably against the comparison methods, especially on the rank-1 rate.},
  archive      = {J_IETCV},
  author       = {Yutao Ren and Zhangcan Huang},
  doi          = {10.1049/cvi2.12094},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {376-387},
  shortjournal = {IET Comput. Vis.},
  title        = {Distribution probability-based self-adaption metric learning for person re-identification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Topic scene graphs for image captioning. <em>IETCV</em>,
<em>16</em>(4), 364–375. (<a
href="https://doi.org/10.1049/cvi2.12093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When describing an image, people can rapidly extract the topic from the image and find the main object, generating sentences that match the main idea of the image. However, most of the scene graph generation methods do not emphasise the importance of the topic of the image. Consequently, the captions generated by the scene graph-based image captioning models cannot reflect the topic in the image then expressing the central idea of the image. In this paper, we propose a method for image captioning based on topic scene graphs (TSG). Firstly, we propose the structure of topic scene graphs that express images&#39; topics and the relationships between objects. Then, combined with the topic scene graph, we utilise the salient object detection to generate the topic scene graph highlighting the salient objects of the image. Note that our framework is agnostic to any scene graph-based image captioning model and thus can be widely applied in the community which seeks salient object predictions. We compare the performance of our topic scene graph with the state-of-the-art scene graph generation models and mainstream image captioning models on MSCOCO and Visual Genome datasets, both achieving better performance.},
  archive      = {J_IETCV},
  author       = {Min Zhang and Jingxiang Chen and Pengfei Li and Ming Jiang and Zhe Zhou},
  doi          = {10.1049/cvi2.12093},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {364-375},
  shortjournal = {IET Comput. Vis.},
  title        = {Topic scene graphs for image captioning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). M-CoTransT: Adaptive spatial continuity in visual tracking.
<em>IETCV</em>, <em>16</em>(4), 350–363. (<a
href="https://doi.org/10.1049/cvi2.12092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual tracking is an important area in computer vision. Based on the Siamese network, current tracking methods employ the self-attention block in convolutional networks to extract semantic features containing the image structure information of an object. However, spatial continuity is a point of contradiction between two seemingly unrelated challenges, that is, occlusion and similar distractor, in tracking methods. At the same time, it is a spatially discontinuous task to locate a target reappearing after occlusion accurately. The prediction of bounding boxes should be constrained by spatial continuity to prevent them from jumping into similar distractors. This study proposes a novel tracking method for introducing spatial continuity in visual tracking called M-CoTransT; the novel tracking method is developed through the confidence-based adaptive Markov motion model (M-model) and a novel correlation-based feature fusion network (CoTransT). In particular, the M-model provides confidence for the nodes of the Markov motion model to estimate the motion state continuity. It also predicts a more accurate search region for CoTransT, which then adds a cross-correlation branch into the self-attention tracking network to enhance the continuity of target appearance in the feature space. Extensive experiments on five challenging datasets (LaSOT, GOT-10k, TrackingNet, OTB-2015 and UAV123) demonstrated the effectiveness of the proposed M-CoTransT in visual tracking.},
  archive      = {J_IETCV},
  author       = {Chunxiao Fan and Runqing Zhang and Yue Ming},
  doi          = {10.1049/cvi2.12092},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {350-363},
  shortjournal = {IET Comput. Vis.},
  title        = {M-CoTransT: Adaptive spatial continuity in visual tracking},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A binocular reconstruction based on perspective projection
constraints and its application on robot eye-hand coordination.
<em>IETCV</em>, <em>16</em>(4), 333–349. (<a
href="https://doi.org/10.1049/cvi2.12091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo matching algorithms have been developed for many years but basically focus only on the implementation of existing datasets and are rarely applied to real scenarios, such as industrial robot scenarios. Traditional stereo matching algorithms have a high error rate, and deep learning algorithms are difficult to obtain good results in real scenarios because of their weak generalisation ability and difficult access to training data. In order to use stereo matching algorithms for industrial robot guidance, it is better to design a new traditional algorithm with low time complexity for the characteristics of industrial robot scenarios dominated by planar facets. This paper proposes a new matching method based on subrows of pixels, instead of individual pixels, in order to improve robustness of matching and reduce running time. First, the pixel strings from the same row of the left and right images are divided into several colour-identical or colour-gradient segments. Then, the colour and length of the two left and right pixel segment are used as clues to determine a matching relation and obtain the matching type. Then, all match types can be determined according to non-crossing mapping. Each match type can reason backward to the corresponding spatial state of the stimulus source so that the disparity of pixels in pixel segments representing the spatial state can be calculated. This new matching method makes full use of the stimulus homology constraints and projective geometric constraints of row-aligned images. The method can obtain good results in industrial robot scenarios and be applied for industrial robot guidance.},
  archive      = {J_IETCV},
  author       = {Hui Wei and Lingjiang Meng},
  doi          = {10.1049/cvi2.12091},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {333-349},
  shortjournal = {IET Comput. Vis.},
  title        = {A binocular reconstruction based on perspective projection constraints and its application on robot eye-hand coordination},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic-aware spatial regularization correlation filter for
visual tracking. <em>IETCV</em>, <em>16</em>(4), 317–332. (<a
href="https://doi.org/10.1049/cvi2.12090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlation filters with convolutional neural network (CNN) features have been successfully applied to visual tracking owing to their impressive combined capability for object representation. Unfortunately, further performance improvement is limited due to unwanted boundary effects of the circular structure. In this work, through an in-depth study of the features’ characteristics, the authors propose a novel tracking strategy to achieve simultaneous filter matching and regularization with CNN features when tracking is on the fly. With a feature decomposed transform matrix, a spatial semantic regularization is generated to reduce the boundary effect effectively during filter optimization. Before each output, the regularized filter is then back performed to match with the extracted features of a search region to find the optimum candidate. Specifically, the most important advantage of the proposed spatial semantic map is to initialize only in the first frame as all the other tracking strategies. Besides, the authors design a novel updating strategy to tackle the cases where the object is occluded or disappeared in the scene. At this time, the maximum of the map is small, even negative. A substantial experiment has been carried out on the popular benchmark tracking datasets; the reliable results have demonstrated that the authors’ method is able to outperform most of the state-of-the-art tracking works in both accuracy and robustness.},
  archive      = {J_IETCV},
  author       = {Yufei Zha and Peng Zhang and Lei Pu and Lichao Zhang},
  doi          = {10.1049/cvi2.12090},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {317-332},
  shortjournal = {IET Comput. Vis.},
  title        = {Semantic-aware spatial regularization correlation filter for visual tracking},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dense sampling and detail enhancement network: Improved
small object detection based on dense sampling and detail enhancement.
<em>IETCV</em>, <em>16</em>(4), 307–316. (<a
href="https://doi.org/10.1049/cvi2.12089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small objects only occupy a few pixels in an image, which results in low performance of small object detection for existing object detection algorithms. Therefore, the authors propose a dense sampling and detail enhancement network (DSDE-Net) to address this issue. The network contains a dense sampling module used to increase the resolution of feature maps and expand the receptive field, which includes an atrous spatial pyramid pooling network and a coordinate attention mechanism to systematically process feature maps. Simultaneously, the authors introduce a detail enhancement branch that contains edge and detailed information to generate detailed enhancement feature maps through Gaussian filtering to compensate for the loss of small object information that occurs in the feature extraction process. The experimental results demonstrate that the proposed network outperformed related methods. Compared with the state-of-the-art algorithm DetectoRS, it effectively achieves approximately 4.6% improvement on the minicoco2021 dataset and 4.2% improvement on the remotely sensed dataset VisDrone.},
  archive      = {J_IETCV},
  author       = {Hong Qin and Yirong Wu and Fangmin Dong and Shuifa Sun},
  doi          = {10.1049/cvi2.12089},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {307-316},
  shortjournal = {IET Comput. Vis.},
  title        = {Dense sampling and detail enhancement network: Improved small object detection based on dense sampling and detail enhancement},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FCNet: A feature context network based on ensemble framework
for image retrieval. <em>IETCV</em>, <em>16</em>(4), 295–306. (<a
href="https://doi.org/10.1049/cvi2.12088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to train an end-to-end model for better performance in distinguishing superclasses and make the model more sensitive to inter-class differences, the authors propose a Feature Context Network (FCNet), a novel ensemble framework for image retrieval based on clustering (EFC) and metric learning. In the authors&#39; approach, the EFC framework consists of one common feature extractor and multiple learning branches. This tree-like structure not only promotes the stability of the training process but also keeps the diversity by using multiple branches. Each branch is learnt by an independent random re-labelling operation that distributes original classes into superclasses. In such a way, the framework thus can accelerate the training process. To make the EFC framework focus on the different attributes across the classes, the authors utilize k-means clustering to divide the training set into subsets and train the model sequentially with those subsets. Meanwhile, to reduce the impact of over-fitting problems, the authors design a branch weight structure during training. Further, FCNet fusions different scales of the neighbourhood information and consequently makes the feature map learn richer information with the different receptive fields. Simultaneously, it can reduce the layer of the network and the number of model parameters. Extensive experiments demonstrate that this approach outperforms the tested representative methods on CARS-196, CUB-200-2011 datasets.},
  archive      = {J_IETCV},
  author       = {Siyang Li and Yu Guo and Hao Ren and Ziyi Wang and Keyan Ren and Chunsheng Liu and Hua Lin and Jianbo Shi},
  doi          = {10.1049/cvi2.12088},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {295-306},
  shortjournal = {IET Comput. Vis.},
  title        = {FCNet: A feature context network based on ensemble framework for image retrieval},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Without detection: Two-step clustering features with
local–global attention for image captioning. <em>IETCV</em>,
<em>16</em>(3), 280–294. (<a
href="https://doi.org/10.1049/cvi2.12087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current image captioning methods usually integrate an object detection network to obtain image features at the level of objects and other salient regions. However, the detection network needs to be independently pre-trained on additional data. Thus, mainly due to the demand for extra training data and computing resources, the detection network&#39;s utilization will impose higher training costs on the overall captioning model. In this work, the authors propose a local–global attention model based on two-step clustering features for image captioning. The two-step clustering features can be obtained at a relatively low cost and have the presentation ability in objects or other salient image regions. To make the model perceive the image better, the authors introduce a novel local–global attention mechanism. The model will analyse the clustering features from local perspectives to global ones at each time step, making the model better understand the image contents. The authors evaluate the proposed method on the MSCOCO test server, achieving BLEU-4/METEOR/ROUGE-L scores of 36.8, 27.4, and 57.2, respectively. With the benefit of reducing training costs, the authors&#39; model also achieves closing results compared with the models using detection features.},
  archive      = {J_IETCV},
  author       = {Xuan Li and Wenkai Zhang and Xian Sun and Xin Gao},
  doi          = {10.1049/cvi2.12087},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {280-294},
  shortjournal = {IET Comput. Vis.},
  title        = {Without detection: Two-step clustering features with local–global attention for image captioning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Temporal-enhanced graph convolution network for
skeleton-based action recognition. <em>IETCV</em>, <em>16</em>(3),
266–279. (<a href="https://doi.org/10.1049/cvi2.12086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolution networks (GCNs) have drawn attention for skeleton-based action recognition. They have achieved remarkable performance by adaptively learning spatial features of human action dynamics. However, the existing methods are limited in temporal sequence modelling of human actions. To give adequate consideration to temporal factors in action modelling, a novel temporal-enhanced graph convolution network is presented. First, a Causal Convolution layer is introduced to ensure no future information leakage at each time step for keeping ordering information of inputs. Second, a novel cross-spatial-temporal graph convolution layer that extends an adaptive graph from the spatial to the temporal domain to capture local cross-spatial-temporal dependencies among joints is presented. Third, a temporal attention layer is designed to enhance the modelling capability of long-range temporal dependencies, helping the network to directly focus on important time steps. Experimental results on three large-scale datasets, NTU-RGB + D, Kinetics-Skeleton, and UAV-Human, indicate that the authors’ network achieves accuracy improvement with better generalisation capability over previous methods. The authors’ code and data are available at https://github.com/xieyulai/TE-GCN .},
  archive      = {J_IETCV},
  author       = {Yulai Xie and Yang Zhang and Fang Ren},
  doi          = {10.1049/cvi2.12086},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {266-279},
  shortjournal = {IET Comput. Vis.},
  title        = {Temporal-enhanced graph convolution network for skeleton-based action recognition},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring complementary information of self-supervised
pretext tasks for unsupervised video pre-training. <em>IETCV</em>,
<em>16</em>(3), 255–265. (<a
href="https://doi.org/10.1049/cvi2.12084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the problem of the unsupervised pre-training of video representation learning. The authors&#39; focus is on two common approaches: knowledge distillation and self-supervised learning. The insight provided is that knowledge distillation and the rapidly advancing self-supervised technique can be mutually beneficial. Combining these two approaches, a unified framework of self-supervised learning and image-based distillation (SSID) for unsupervised video pre-training is proposed. The effectiveness of SSID in comparison to both image-based distillation methods and the existing self-supervised pre-training baseline is demonstrated. In particular, the authors&#39; model leverages three signals from the unlabelled data. First, the authors distil from the classifier of a 2D pre-trained model as a soft label. To regularize the training process, the authors then build a novel positive pair of contrastive learning on the representation of the 2D/3D model. Finally, a self-supervised pretext task is introduced to enhance the authors&#39; model to become aware of the temporal evolution. The authors&#39; experiment results showed that the learnt features achieved the best performance when transferred to action recognition tasks on UCF101 and HMDB51, reaching increases of 2.4% and 1.9% compared to the existing unsupervised pre-training model, respectively.},
  archive      = {J_IETCV},
  author       = {Wei Zhou and Yi Hou and Kewei Ouyang and Shilin Zhou},
  doi          = {10.1049/cvi2.12084},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {255-265},
  shortjournal = {IET Comput. Vis.},
  title        = {Exploring complementary information of self-supervised pretext tasks for unsupervised video pre-training},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tensor local linear embedding with global subspace
projection optimisation. <em>IETCV</em>, <em>16</em>(3), 241–254. (<a
href="https://doi.org/10.1049/cvi2.12083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel tensor dimensionality reduction (TDR) approach is proposed, which maintains the local geometric structure of tensor data by tensor local linear embedding and explores the global feature by optimising global subspace projection. Firstly, we analyse the local linear feature of tensor data for learning the linear separable embedding of the tenor data. Furthermore, a global subspace projection distance minimisation strategy is introduced to extract the global characteristic of the tensor data. The aim of this strategy is to find an optimal low-dimensional subspace for TDR. In particular, two novel TDR algorithms are developed by the ensemble of tensor local feature preservation and global subspace projection distance minimisation, which express the subspace projection optimisation as an iteration optimisation problem and a Rayleigh quotient problem, respectively. The extensive experimental results on tensor data classification and clustering have demonstrated the proposed algorithms performed well.},
  archive      = {J_IETCV},
  author       = {Guo Niu and Zhengming Ma},
  doi          = {10.1049/cvi2.12083},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {241-254},
  shortjournal = {IET Comput. Vis.},
  title        = {Tensor local linear embedding with global subspace projection optimisation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Batch quadratic programming network with maximum entropy
constraint for anomaly detection. <em>IETCV</em>, <em>16</em>(3),
230–240. (<a href="https://doi.org/10.1049/cvi2.12082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The difficulty of anomaly detection lies in balancing the impact of noise on the network (noise suppression) and distinguishing the real anomaly from the noise (abnormal exposure). So, a deep anomaly detector Batch Quadratic Programming (BQP) network with Maximum Entropy Constraint is proposed. It imposes quadratic programming constraints on Support Vector Data Description through the BQP output layer to achieve noise suppression. In BQP network processes’ batch data, Maximum Entropy Constraint is used to balance abnormal samples and noise. The experiment compared the shallow method with the currently popular deep method on MNIST and CIFAR-10 data sets and proved that the BQP network with Maximum Entropy Constraint has excellent performance.},
  archive      = {J_IETCV},
  author       = {Di Zhou and Weigang Chen and Chunsheng Guo and Mark Zhang},
  doi          = {10.1049/cvi2.12082},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {230-240},
  shortjournal = {IET Comput. Vis.},
  title        = {Batch quadratic programming network with maximum entropy constraint for anomaly detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HQA-trans: An end-to-end high-quality-awareness image
translation framework for unsupervised cross-domain pedestrian
detection. <em>IETCV</em>, <em>16</em>(3), 218–229. (<a
href="https://doi.org/10.1049/cvi2.12081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised cross-domain pedestrian detection has attracted attention in recent years. Although some works adopted unsupervised image translation frameworks to generate an intermediate domain to narrow the gap between source and target domains, the images in the intermediate domain tend to be distorted due to the instability of the generation network. In this work, we propose a new framework to improve the image quality of the generated intermediate domain via an end-to-end translation framework. First, an image quality assessment index is adopted and adjusted appropriately. The part that controls the image quality is kept, and the part that adversely affects the domain style translation is discarded. Secondly, the adjusted image quality assessment index is integrated into the unsupervised image translation framework, where a new loss with the index&#39;s weight is proposed. An end-to-end high-quality-awareness image translation framework is constructed to generate a high-quality intermediate domain directly through this process. Finally, the intermediate domain with high-quality images is applied for cross-domain pedestrian detection. Experimental results on benchmark datasets show that the proposed framework can effectively improve unsupervised cross-domain pedestrian detection performance. Compared with some state-of-the-art works, the proposed framework can also achieve superior performance under miss rate metrics.},
  archive      = {J_IETCV},
  author       = {Gelin Shen and Yang Yu and Zhi-Ri Tang and Haoqiang Chen and Zongtan Zhou},
  doi          = {10.1049/cvi2.12081},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {218-229},
  shortjournal = {IET Comput. Vis.},
  title        = {HQA-trans: An end-to-end high-quality-awareness image translation framework for unsupervised cross-domain pedestrian detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial-temporal slowfast graph convolutional network for
skeleton-based action recognition. <em>IETCV</em>, <em>16</em>(3),
205–217. (<a href="https://doi.org/10.1049/cvi2.12080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In skeleton-based action recognition, the graph convolutional network (GCN) has achieved great success. Modelling skeleton data in a suitable spatial-temporal way and designing the adjacency matrix are crucial aspects for GCN-based methods to capture joint relationships. In this study, we propose the spatial-temporal slowfast graph convolutional network (STSF-GCN) and design the adjacency matrices for the skeleton data graphs in STSF-GCN. STSF-GCN contains two pathways: (1) the fast pathway is in a high frame rate, and joints of adjacent frames are unified to build ‘small’ spatial-temporal graphs. A new spatial-temporal adjacency matrix is proposed for these ‘small’ spatial-temporal graphs. Ablation studies verify the effectiveness of the proposed adjacency matrix. (2) The slow pathway is in a low frame rate, and joints from all frames are unified to build one ‘big’ spatial-temporal graph. The adjacency matrix for the ‘big’ spatial-temporal graph is obtained by computing self-attention coefficients of each joint. Finally, outputs from two pathways are fused to predict the action category. STSF-GCN can efficiently capture both long-range and short-range spatial-temporal joint relationships. On three datasets for skeleton-based action recognition, STSF-GCN can achieve state-of-the-art performance with much less computational cost.},
  archive      = {J_IETCV},
  author       = {Zheng Fang and Xiongwei Zhang and Tieyong Cao and Yunfei Zheng and Meng Sun},
  doi          = {10.1049/cvi2.12080},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {205-217},
  shortjournal = {IET Comput. Vis.},
  title        = {Spatial-temporal slowfast graph convolutional network for skeleton-based action recognition},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Error refactor loss based on error analysis in image
classification. <em>IETCV</em>, <em>16</em>(2), 192–203. (<a
href="https://doi.org/10.1049/cvi2.12079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The loss function is a criterion to evaluate the learning quality of a deep convolutional neural network, which represents the gap between prediction and ground truth. However, as the most commonly used loss function in image classification tasks, Cross-Entropy loss does not encourage the model to distinguish the similarity between features. In this work, the authors investigate inter-class separability of similar features learnt by convolutional networks and propose a loss function called Error Refactor Loss (ER-Loss). ER-Loss is based on the error caused by convolutional networks; it can improve the inter-class separability and is simple to implement and can easily replace the Cross-Entropy loss. Compared with softmax loss, ER-Loss adds a dynamic penalty item which can help ER-Loss monitor the actual situation of model training and adjust the value of the penalty item according to model training. The ER-Loss on CIFAR100 and part of ImageNet ILSVRC 2012 is evaluated and the experimental result showed that the ER-Loss can improve the accuracy of the model.},
  archive      = {J_IETCV},
  author       = {Xiaoyu Yu and Yinglu Chen and Guofu Zhou and Yan Liu and Fuchao Li and Zhifei Wang},
  doi          = {10.1049/cvi2.12079},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {192-203},
  shortjournal = {IET Comput. Vis.},
  title        = {Error refactor loss based on error analysis in image classification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-stream densely connected network for semantic
segmentation. <em>IETCV</em>, <em>16</em>(2), 180–191. (<a
href="https://doi.org/10.1049/cvi2.12078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a challenging task in computer vision which is widely used in autonomous driving and scene understanding. State-of-the-art semantic segmentation networks, like DeepLab and PSPNet, make full use of multiple feature information to improve spatial resolution. However, the feature resolution in the scale-axis is not dense enough for practical applications. To tackle this problem, a multi-stream network is designed with atrous convolutional layers at multiple rates to capture objects and context at multiple scales. Furthermore, intra-connections and inter-connections are designed to fuse multi-scale features densely which produce a feature pyramid with much larger scale diversity and larger receptive field by involving small quantity of computation. The proposed module can be easily used in other methods and it helps to increase the performance. Compared with existing methods, the proposed network, called Multi-stream Densely Connected Network, reaches competitive results on ADE20K dataset, PASCAL VOC 2012 dataset, and Cityscapes dataset.},
  archive      = {J_IETCV},
  author       = {Dayu Jia and Jiale Cao and Jing Pan and Yanwei Pang},
  doi          = {10.1049/cvi2.12078},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {180-191},
  shortjournal = {IET Comput. Vis.},
  title        = {Multi-stream densely connected network for semantic segmentation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved quadruple sparse census transform and adaptive
multi-shape aggregation algorithms for precise stereo matching.
<em>IETCV</em>, <em>16</em>(2), 159–179. (<a
href="https://doi.org/10.1049/cvi2.12076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo matching is an important computer vision tool for three-dimensional (3D) positioning, 3D visual representation, precision detection and recognition with 3D information. For 3D visual representation, for example, the depth image-based rendering (DIBR) needs high accuracy depth maps to synthesise multiple views for 3D naked-eyes displays. It is hard to predict high-precision disparity for smooth regions and occlusion areas by traditional census-based stereo matching methods. In this work, two new improved algorithms, called the improved quadruple sparse census transform and the adaptive multi-shape aggregation, are proposed to achieve a precise stereo matching system. Instead of a binary threshold, the improved quadruple auxiliary census function with an adaptive threshold and the patch mean can raise the discrimination performance for better stereo matching. The selected sparse census patch is proposed to reduce the computation of the pixel-matching cost. For effective aggregation of matching costs, multi-shape windows and a texture-aware algorithm is suggested to decide a suitable window. The proposed adaptive multi-shape aggregation method performs better and solves the discontinuous depth problem occurring near the object boundaries. Experimental results show that the proposed stereo matching system with the proposed quadruple sparse census transform and multi-shape cost aggregation achieves better raw depth maps than the existing algorithms for stereo matching.},
  archive      = {J_IETCV},
  author       = {Chih-Hsuan Huang and Jar-Ferr Yang},
  doi          = {10.1049/cvi2.12076},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {159-179},
  shortjournal = {IET Comput. Vis.},
  title        = {Improved quadruple sparse census transform and adaptive multi-shape aggregation algorithms for precise stereo matching},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-stream adaptive spatial-temporal attention graph
convolutional network for skeleton-based action recognition.
<em>IETCV</em>, <em>16</em>(2), 143–158. (<a
href="https://doi.org/10.1049/cvi2.12075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition algorithms have been widely applied to human action recognition. Graph convolutional networks (GCNs) generalize convolutional neural networks (CNNs) to non-Euclidean graphs and achieve significant performance in skeleton-based action recognition. However, existing GCN-based models have several issues, such as the topology of the graph is defined based on the natural skeleton of the human body, which is fixed during training, and it may not be applied to different layers of the GCN model and diverse datasets. Besides, the higher-order information of the joint data, for example, skeleton and dynamic information is not fully utilised. This work proposes a novel multi-stream adaptive spatial-temporal attention GCN model that overcomes the aforementioned issues. The method designs a learnable topology graph to adaptively adjust the connection relationship and strength, which is updated with training along with other network parameters. Simultaneously, the adaptive connection parameters are utilised to optimise the connection of the natural skeleton graph and the adaptive topology graph. The spatial-temporal attention module is embedded in each graph convolution layer to ensure that the network focuses on the more critical joints and frames. A multi-stream framework is built to integrate multiple inputs, which further improves the performance of the network. The final network achieves state-of-the-art performance on both the NTU-RGBD and Kinetics-Skeleton action recognition datasets. The simulation results prove that the proposed method reveals better results than existing methods in all perspectives and that shows the superiority of the proposed method.},
  archive      = {J_IETCV},
  author       = {Lubin Yu and Lianfang Tian and Qiliang Du and Jameel Ahmed Bhutto},
  doi          = {10.1049/cvi2.12075},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {143-158},
  shortjournal = {IET Comput. Vis.},
  title        = {Multi-stream adaptive spatial-temporal attention graph convolutional network for skeleton-based action recognition},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TRC-YOLO: A real-time detection method for lightweight
targets based on mobile devices. <em>IETCV</em>, <em>16</em>(2),
126–142. (<a href="https://doi.org/10.1049/cvi2.12072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is one of the main tasks of computer vision. Object detection algorithms usually rely on deep convolutional neural networks, which require the host device to have high computing capabilities, greatly limiting the application of object detection methods for mobile devices with limited computing capabilities, such as embedded devices. Among the current object detection algorithms, the you only look once (YOLO) series takes both speed and accuracy into consideration and is one of the most commonly used methods for object detection. In this article, TRC-YOLO is proposed, which improves the mean average precision (mAP) and real-time detection speed of the model while reducing the size of the model. In TRC-YOLO, the convolution kernel of YOLO v4-tiny is pruned and an expansive convolution layer is introduced into the residual module of the network to produce an hourglass Cross Stage Partial ResNet (CSPResNet) structure. A receptive field block (RFB) that simulates human vision is also added, increasing the receptive field of the model and strengthening the feature extraction ability of the network. In addition, the convolutional block attention module is applied, which combines spatial attention and channel attention, to enhance the effective features of the model and reduce the negative impact of noise on the model. The size of the TRC-YOLO model is 17.8 MB, which is 5.9 MB smaller than YOLO v4-tiny, and the model parameter is 2.983 billion floating point operations per second (BFLOP/s) (3.834 BFLOP/s less than YOLO v4-tiny). In addition, TRC-YOLO achieves a real-time performance of 36.9 frames per second on a Jetson Xavier NX, and its mAP on the PASCAL VOC dataset is 66.4 (3.83 higher than YOLO v4-tiny). In addition, the mAP of TRC-YOLO on the MS COCO dataset is 37.7 , which is 1.9 higher than that of the baseline model.},
  archive      = {J_IETCV},
  author       = {Guanbo Wang and Hongwei Ding and Zhijun Yang and Bo Li and Yihao Wang and Liyong Bao},
  doi          = {10.1049/cvi2.12072},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {126-142},
  shortjournal = {IET Comput. Vis.},
  title        = {TRC-YOLO: A real-time detection method for lightweight targets based on mobile devices},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gait-d: Skeleton-based gait feature decomposition for gait
recognition. <em>IETCV</em>, <em>16</em>(2), 111–125. (<a
href="https://doi.org/10.1049/cvi2.12070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The general silhouette-based gait recognition methods usually rely on binary human silhouette, which is easily affected by external factors, making it unsuitable for situations while wearing heavy clothes or carrying objects, etc. In this study, a new skeleton-based gait recognition model is proposed. The model first extracts the spatial and temporal features of gait using the space and time relationship between body joints, and second, it eliminates redundant features by decomposing the feature map, to achieve a better recognition accuracy in the presence of external factors. Through abundant experiments on two common datasets, CASIA-B and OUMVLP-Pose, the proposed model has been proved to have higher recognition accuracy and remarkable robustness.},
  archive      = {J_IETCV},
  author       = {Shuo Gao and Jing Yun and Yumeng Zhao and Limin Liu},
  doi          = {10.1049/cvi2.12070},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {111-125},
  shortjournal = {IET Comput. Vis.},
  title        = {Gait-D: Skeleton-based gait feature decomposition for gait recognition},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-scale feature fusion connection for a YOLO detector.
<em>IETCV</em>, <em>16</em>(2), 99–110. (<a
href="https://doi.org/10.1049/cvi2.12069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-scale feature fusion is often used to address the issue of scale variations in object detection. However, most of the proposed network architectures only combine the features of two adjacent levels sequentially, so the first fusion nodes in both top-down and bottom-up pathways must be blank nodes that only have one input with no feature fusion. In this work, cross-scale feature fusion connection (CFFC) is proposed which aims to enhance the entire feature hierarchy by propagating the features of each level more efficiently. The proposed method reuses and aggregates all the features of other scales to the blank nodes in both top-down and bottom-up pathways. Furthermore, the authors remove the 1 × 1 convolutional layer and replace the shortcut with concatenation before fusing multiple features. These concatenated feature maps are then supervised by the channel attention block at the fusion nodes. This modification allows the network to learn the important degree of each level in concatenated feature maps along the channel dimension. It is also observed that the proposed method alleviates the inconsistency in feature pyramids with fewer parameters. The performance of a YOLO object detector equipped with the proposed method on the COCO test-dev 2017 is evaluated. The results show that the proposed method outperforms other architectures presented in the literature.},
  archive      = {J_IETCV},
  author       = {Zhongling Ruan and Hao Wang and Jianzhong Cao and Hongbo Zhang},
  doi          = {10.1049/cvi2.12069},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {99-110},
  shortjournal = {IET Comput. Vis.},
  title        = {Cross-scale feature fusion connection for a YOLO detector},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A dynamic routing CapsNet based on increment prototype
clustering for overcoming catastrophic forgetting. <em>IETCV</em>,
<em>16</em>(1), 83–97. (<a
href="https://doi.org/10.1049/cvi2.12068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In continual learning, previously learnt knowledge tends to be overlapped by the subsequent training tasks. This bottleneck, known as catastrophic forgetting, has recently been relieved between vision tasks involving pixel shuffles etc. Nevertheless, the challenge lies in the continuous classification of the sequential sets discriminated by global transformations, such as excessively spatial rotations. Aim at this, a novel strategy of dynamic memory routing is proposed to dominate the forward paths of capsule network (CapsNet) according to the current input sets. To recall previous knowledge, a binary routing table is maintained among these sequential tasks. Then, an increment procedure of competitive prototype clustering is integrated to update the routing of the current task. Moreover, a sparsity measurement is employed to decouple the salient routing among the different learnt tasks. The experimental results demonstrate the superiority of the proposed memory network over the state–of–the–art approaches by the recalling evaluations on extended sets of Cifar–100, CelebA and Tiny ImageNet etc.},
  archive      = {J_IETCV},
  author       = {Meng Wang and Zhengbing Guo and Huafeng Li},
  doi          = {10.1049/cvi2.12068},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {83-97},
  shortjournal = {IET Comput. Vis.},
  title        = {A dynamic routing CapsNet based on increment prototype clustering for overcoming catastrophic forgetting},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DPANet: Dual pooling-aggregated attention network for fish
segmentation. <em>IETCV</em>, <em>16</em>(1), 67–82. (<a
href="https://doi.org/10.1049/cvi2.12065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sustainable development of marine fisheries depends on the accurate measurement of data on fish stocks. Semantic segmentation methods based on deep learning can be applied to automatically obtain segmentation masks of fish in images to obtain measurement data. However, general semantic segmentation methods cannot accurately segment fish objects in underwater images. In this study, a Dual Pooling-aggregated Attention Network (DPANet) to adaptively capture long-range dependencies through an efficient and computing-friendly manner to enhance feature representation and improve segmentation performance is proposed. Specifically, a novel pooling-aggregate position attention module and a pooling-aggregate channel attention module are designed to aggregate contexts in the spatial dimension and channel dimension, respectively. These two modules adopt pooling operations along the channel dimension and along the spatial dimension to aggregate information, respectively, thus reducing computational costs. In these modules, attention maps are generated by four different paths and are aggregated into one. The authors conduct extensive experiments to validate the effectiveness of the DPANet and achieve new state-of-the-art segmentation performance on the well-known fish image dataset DeepFish as well as on the underwater image dataset SUIM, achieving a Mean IoU score of 91.08% and 85.39% respectively, while significantly reducing FLOPs of attention modules by about 93%.},
  archive      = {J_IETCV},
  author       = {Wenbo Zhang and Chaoyi Wu and Zhenshan Bao},
  doi          = {10.1049/cvi2.12065},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {67-82},
  shortjournal = {IET Comput. Vis.},
  title        = {DPANet: Dual pooling-aggregated attention network for fish segmentation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). End-to-end global to local convolutional neural network
learning for hand pose recovery in depth data. <em>IETCV</em>,
<em>16</em>(1), 50–66. (<a
href="https://doi.org/10.1049/cvi2.12064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite recent advances in 3-D pose estimation of human hands, thanks to the advent of convolutional neural networks (CNNs) and depth cameras, this task is still far from being solved in uncontrolled setups. This is mainly due to the highly non-linear dynamics of fingers and self-occlusions, which make hand model training a challenging task. In this study, a novel hierarchical tree-like structured CNN is exploited, in which branches are trained to become specialised in predefined subsets of hand joints called local poses. Further, local pose features, extracted from hierarchical CNN branches, are fused to learn higher order dependencies among joints in the final pose by end-to-end training. Lastly, the loss function used is also defined to incorporate appearance and physical constraints about doable hand motions and deformations. Finally, a non-rigid data augmentation approach is introduced to increase the amount of training depth data. Experimental results suggest that feeding a tree-shaped CNN, specialised in local poses, into a fusion network for modelling joints&#39; correlations and dependencies, helps to increase the precision of final estimations, showing competitive results on NYU, MSRA, Hands17 and SyntheticHand datasets.},
  archive      = {J_IETCV},
  author       = {Meysam Madadi and Sergio Escalera and Xavier Baró and Jordi Gonzàlez},
  doi          = {10.1049/cvi2.12064},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {50-66},
  shortjournal = {IET Comput. Vis.},
  title        = {End-to-end global to local convolutional neural network learning for hand pose recovery in depth data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). KText: Arbitrary shape text detection using modified
k-means. <em>IETCV</em>, <em>16</em>(1), 38–49. (<a
href="https://doi.org/10.1049/cvi2.12052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text detection methods based on grouping characters have emerged and have achieved promising performance. Nevertheless, previous methods that grouped characters by learning the relation of adjacent characters or used the heuristic clustering method with a handcrafted feature are unsuitable for dense, curved, or long texts. An effective manner of grouping characters is proposed by introducing K-Means that is modified by the law of universal gravitation, an outlier detection mechanism and sufficient context information. Based on that, corresponding text detector is presented, named the Text Detector, based on modified K-Means (KText), which can generate the bounding boundary of word-level texts with an arbitrary shape. In the experimental stage, two novel stratagems are presented to replenish character-level annotations to several datasets that provide only word-level annotations. To evaluate the effectiveness of the method, experiments are carried out on three benchmarks, ICDAR2013, ICDAR2015 and Total-Text, which contain horizontal, oriented and curved text. The results show that KText performs more competently than most state-of-the-art text detectors when handling dense texts with an arbitrary shape.},
  archive      = {J_IETCV},
  author       = {Zhuo Qi and Wenyi Chen and Xiaofei Sun and Wangqian Sun and Hui Yang},
  doi          = {10.1049/cvi2.12052},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {38-49},
  shortjournal = {IET Comput. Vis.},
  title        = {KText: Arbitrary shape text detection using modified K-means},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parallax-based second-order mixed attention for stereo image
super-resolution. <em>IETCV</em>, <em>16</em>(1), 26–37. (<a
href="https://doi.org/10.1049/cvi2.12063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo image pairs can effectively enhance the performance of super-resolution (SR) since both intra-view and cross-view information can be used. However, exploiting cross-view information accurately is extremely challenging. Most recent methods use the attention mechanism to get stereo correspondence. But these methods ignore the high-frequency information since they only utilise first-order statistics, which leads to reducing the discriminative ability of the network. To address this issue, in this work, a parallax-based second-order mixed attention stereo SR network (PSMASSRnet) is proposed to integrate the cross-view information from a stereo image pair for SR. Specifically, a novel parallax-based second-order mixed attention module (PSMAM) is developed to combine second-order channel features and spatial features to obtain more discriminative representations. Furthermore, a dense cross-atrous spatial pyramid pooling (ASPP) module is also presented, which can effectively explore the local and the multi-scale features with different dilation rates to extract more discriminative features with fewer parameters and less execution. The extensive experiments on the KITTI2012, KITTI2015 and Middlebury datasets have demonstrated the superiority of the proposed PSMASSRnet over the state-of-the-art methods in the aspects of both the quantitative metrics and the visual quality.},
  archive      = {J_IETCV},
  author       = {Chenyang Duan and Nanfeng Xiao},
  doi          = {10.1049/cvi2.12063},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {26-37},
  shortjournal = {IET Comput. Vis.},
  title        = {Parallax-based second-order mixed attention for stereo image super-resolution},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accurate localization of moving objects in dynamic
environment for small unmanned aerial vehicle platform using global
averaging. <em>IETCV</em>, <em>16</em>(1), 12–25. (<a
href="https://doi.org/10.1049/cvi2.12053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small unmanned aerial vehicles (UAVs) have developed rapidly and are widely used for disaster relief, traffic monitoring and military surveillance. To perform these tasks better, it is necessary to improve the environmental perception ability of UAVs in a dynamic environment, including their static and dynamic perception ability. Specifically, both three-dimensional reconstruction for a static scene and localization for moving objects are required. Simultaneous Localization And Mapping technology has made great progress in static scene structure reconstruction and UAV self-motion estimation. However, accurate real-time localization of moving objects is still challenging. In this article, a global averaging based localization method is proposed to locate moving objects for a small UAV platform. Inspired by global structure from motion, this idea is applied to the localization of moving objects. To solve moving object localization, the relative motion estimation and global position optimisation methods are proposed. The proposed method was tested in various scenarios with a several trajectories. The extensive experimental results demonstrate the robustness and effectiveness of the proposed method.},
  archive      = {J_IETCV},
  author       = {Xiuchuan Xie and Tao Yang and Yanning Zhang and Bang Liang and Linfeng Liu},
  doi          = {10.1049/cvi2.12053},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {12-25},
  shortjournal = {IET Comput. Vis.},
  title        = {Accurate localization of moving objects in dynamic environment for small unmanned aerial vehicle platform using global averaging},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhancing transferability of adversarial examples via
rotation-invariant attacks. <em>IETCV</em>, <em>16</em>(1), 1–11. (<a
href="https://doi.org/10.1049/cvi2.12054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks are vulnerable to adversarial examples. However, existing attacks exhibit relatively low efficacy in generating transferable adversarial examples. Improved transferability to address this issue is proposed via a rotation-invariant attack method that maximizes the loss function w.r.t the random rotated image instead of the original input at each iteration, thus mitigating the high correlation between the adversarial examples and the source models and making the adversarial examples more transferable. Extensive experiments show that the proposed method can significantly improve the transferability of the adversarial examples with almost no extra computational cost and can be integrated into various methods. In addition, when this method is easily applied through a plug-in, the average attack success rate against six robustly trained models increases by 5.4% over the state-of-the-art baseline method, demonstrating its effectiveness and efficiency. The codes used are publicly available at https://github.com/YeXinD/Rotation-Invariant-Attack .},
  archive      = {J_IETCV},
  author       = {Yexin Duan and Junhua Zou and Xingyu Zhou and Wu Zhang and Jin Zhang and Zhisong Pan},
  doi          = {10.1049/cvi2.12054},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {1-11},
  shortjournal = {IET Comput. Vis.},
  title        = {Enhancing transferability of adversarial examples via rotation-invariant attacks},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
