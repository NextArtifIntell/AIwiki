<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SADM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sadm---51">SADM - 51</h2>
<ul>
<li><details>
<summary>
(2022). Coupled support tensor machine classification for multimodal
neuroimaging data. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>15</em>(6), 797–818. (<a
href="https://doi.org/10.1002/sam.11587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multimodal data arise in various applications where information about the same phenomenon is acquired from multiple sensors and across different imaging modalities. Learning from multimodal data is of great interest in machine learning and statistics research as this offers the possibility of capturing complementary information among modalities. Multimodal modeling helps to explain the interdependence between heterogeneous data sources, discovers new insights that may not be available from a single modality, and improves decision-making. Recently, coupled matrix–tensor factorization has been introduced for multimodal data fusion to jointly estimate latent factors and identify complex interdependence among the latent factors. However, most of the prior work on coupled matrix–tensor factors focuses on unsupervised learning and there is little work on supervised learning using the jointly estimated latent factors. This paper considers the multimodal tensor data classification problem. A coupled support tensor machine (C-STM) built upon the latent factors jointly estimated from the advanced coupled matrix–tensor factorization is proposed. C-STM combines individual and shared latent factors with multiple kernels and estimates a maximal-margin classifier for coupled matrix–tensor data. The classification risk of C-STM is shown to converge to the optimal Bayes risk, making it a statistically consistent rule. C-STM is validated through simulation studies as well as a simultaneous analysis on electroencephalography with functional magnetic resonance imaging data. The empirical evidence shows that C-STM can utilize information from multiple sources and provide a better classification performance than traditional single-mode classifiers.},
  archive  = {J},
  author   = {Peide Li and Seyyid Emre Sofuoglu and Selin Aviyente and Tapabrata Maiti},
  doi      = {10.1002/sam.11587},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {797-818},
  title    = {Coupled support tensor machine classification for multimodal neuroimaging data},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Out-of-bag stability estimation for k-means clustering.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>15</em>(6), 781–796. (<a
href="https://doi.org/10.1002/sam.11593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Clustering data is a challenging problem in unsupervised learning where there is no gold standard. Results depend on several factors, such as the selection of a clustering method, measures of dissimilarity, parameters, and the determination of the number of reliable groupings. Stability has become a valuable surrogate to performance and robustness that can provide insight to an investigator on the quality of a clustering, and guidance on subsequent cluster prioritization. This work develops a framework for stability measurements that is based on resampling and OB estimation. Bootstrapping methods for cluster stability can be prone to overfitting in a setting that is analogous to poor delineation of test and training sets in supervised learning. Stability that relies on OB items from a resampling overcomes these issues and does not depend on a reference clustering for comparisons. Furthermore, OB stability can provide estimates at the level of the item, cluster, and as an overall summary, which has good interpretive value. This framework is extended to develop stability estimates for determining the number of clusters (model selection) through contrasts between stability estimates on clustered data, and stability estimates of clustered reference data with no signal. These contrasts form stability profiles that can be used to identify the largest differences in stability and do not require a direct threshold on stability values, which tend to be data specific. These approaches can be implemented using the R package bootcluster that is available on the Comprehensive R Archive Network.},
  archive  = {J},
  author   = {Tianmou Liu and Han Yu and Rachael Hageman Blair},
  doi      = {10.1002/sam.11593},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {781-796},
  title    = {Out-of-bag stability estimation for k-means clustering},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel bayesian method for variable selection and
estimation in binary quantile regression. <em>Statistical Analysis and
Data Mining: The ASA Data Science Journal</em>, <em>15</em>(6), 766–780.
(<a href="https://doi.org/10.1002/sam.11591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we develop a Bayesian hierarchical model and associated computation strategy for simultaneously conducting parameter estimation and variable selection in binary quantile regression. We specify customary asymmetric Laplace distribution on the error term and assign quantile-dependent priors on the regression coefficients and a binary vector to identify the model configuration. Thanks to the normal-exponential mixture representation of the asymmetric Laplace distribution, we proceed to develop a novel three-stage computational scheme starting with an expectation–maximization algorithm and then the Gibbs sampler followed by an importance re-weighting step to draw nearly independent Markov chain Monte Carlo samples from the full posterior distributions of the unknown parameters. Simulation studies are conducted to compare the performance of the proposed Bayesian method with that of several existing ones in the literature. Finally, two real-data applications are provided for illustrative purposes.},
  archive  = {J},
  author   = {Mai Dao and Min Wang and Souparno Ghosh},
  doi      = {10.1002/sam.11591},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {766-780},
  title    = {A novel bayesian method for variable selection and estimation in binary quantile regression},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kernel learning with nonconvex ramp loss. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>15</em>(6), 751–765. (<a
href="https://doi.org/10.1002/sam.11588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We study the kernel learning problems with ramp loss, a nonconvex but noise-resistant loss function. In this work, we justify the validity of ramp loss under the classical kernel learning framework. In particular, we show that the generalization bound for empirical ramp risk minimizer is similar to that of convex surrogate losses, which implies kernel learning with such loss function is not only noise-resistant but, more importantly, statistically consistent. For adapting to real-time data streams, we introduce PA-ramp, a heuristic online algorithm based on the passive-aggressive framework, to solve this learning problem. Empirically, with fewer support vectors, this algorithm achieves robust empirical performances on tested noisy scenarios.},
  archive  = {J},
  author   = {Xijun Liang and Zhipeng Zhang and Xingke Chen and Ling Jian},
  doi      = {10.1002/sam.11588},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {751-765},
  title    = {Kernel learning with nonconvex ramp loss},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ensembled sparse-input hierarchical networks for
high-dimensional datasets. <em>Statistical Analysis and Data Mining: The
ASA Data Science Journal</em>, <em>15</em>(6), 736–750. (<a
href="https://doi.org/10.1002/sam.11579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In high-dimensional datasets where the number of covariates far exceeds the number of observations, the most popular prediction methods make strong modeling assumptions. Unfortunately, these methods struggle to scale up in model complexity as the number of observations grows. To this end, we consider using neural networks because they span a wide range of model capacities, from sparse linear models to deep neural networks. Because neural networks are notoriously tedious to tune and train, our aim is to develop a convenient procedure that employs a minimal number of hyperparameters. Our method, Ensemble by Averaging Sparse-Input hiERarchical networks (EASIER-net), employs only two L 1 -penalty parameters, one that controls the input sparsity and another for the number of hidden layers and nodes. EASIER-net selects the true support with high probability when there is sufficient evidence; otherwise, it performs variable selection with uncertainty quantification, where strongly correlated covariates are selected at similar rates. On a large collection of gene expression datasets, EASIER-net achieved higher classification accuracy and selected fewer genes than existing methods. We found that EASIER-net adaptively selected the model complexity: it fit deep networks when there was sufficient information to learn nonlinearities and interactions and fit sparse logistic models for smaller datasets with less information.},
  archive  = {J},
  author   = {Jean Feng and Noah Simon},
  doi      = {10.1002/sam.11579},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {736-750},
  title    = {Ensembled sparse-input hierarchical networks for high-dimensional datasets},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local support vector machine based dimension reduction.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>15</em>(6), 722–735. (<a
href="https://doi.org/10.1002/sam.11600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Motivated by several recent work that adopt support vector machines into the sufficient dimension reduction research, we propose a local support vector machine based dimension reduction approach. The proposal deals with continuous and binary responses, linear and nonlinear dimension reduction in a unified framework. The localization can also help relax the stringent probabilistic assumptions required by the global methods. Numerical experiments and a real data application demonstrate the efficacy of the proposed approach.},
  archive  = {J},
  author   = {Linxi Li and Qin Wang and Chenlu Ke},
  doi      = {10.1002/sam.11600},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {722-735},
  title    = {Local support vector machine based dimension reduction},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning and neural network based model predictions
of soybean export shares from US gulf to china. <em>Statistical Analysis
and Data Mining: The ASA Data Science Journal</em>, <em>15</em>(6),
707–721. (<a href="https://doi.org/10.1002/sam.11595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we propose a general model for the soybean export market share dynamics and provide several theoretical analyses related to a special case of the general model. We implement machine and neural network algorithms to train, analyze, and predict US Gulf soybean market shares (target variable) to China using weekly time series data consisting of several features between January 6, 2012 and January 3, 2020. Overall, the results indicate that US Gulf soybean market shares to China are volatile and can be effectively explained (predicted) using a set of logical input variables. Some of the variables, including shipments due at US Gulf port in 10 days, cost of transporting soybean shipments via barge at Mid-Mississippi, and soybean exports loaded at US Gulf port in the past 7 days, and binary variables have shown significant influence in predicting soybean market shares.},
  archive  = {J},
  author   = {Shantanu Awasthi and Indranil SenGupta and William Wilson and Prithviraj Lakkakula},
  doi      = {10.1002/sam.11595},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {707-721},
  title    = {Machine learning and neural network based model predictions of soybean export shares from US gulf to china},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed dimension reduction with nearly oracle rate.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>15</em>(6), 692–706. (<a
href="https://doi.org/10.1002/sam.11592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We consider sufficient dimension reduction for heterogeneous massive data. We show that, even in the presence of heterogeneity and nonlinear dependence, the minimizers of convex loss functions of linear regression fall into the central subspace at the population level. We suggest a distributed algorithm to perform sufficient dimension reduction, where the convex loss functions are approximated with surrogate quadratic losses. This allows to perform dimension reduction in a unified least squares framework and facilitates to transmit the gradients in our distributed algorithm. The minimizers of these surrogate quadratic losses possess a nearly oracle rate after a finite number of iterations. We conduct simulations and an application to demonstrate the effectiveness of our proposed distributed algorithm for heterogeneous massive data.},
  archive  = {J},
  author   = {Zhengtian Zhu and Liping Zhu},
  doi      = {10.1002/sam.11592},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {692-706},
  title    = {Distributed dimension reduction with nearly oracle rate},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Frequentist model averaging for zero-inflated poisson
regression models. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>15</em>(6), 679–691. (<a
href="https://doi.org/10.1002/sam.11598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper considers frequentist model averaging for estimating the unknown parameters of the zero-inflated Poisson regression model. Our proposed weight choice procedure is based on the minimization of an unbiased estimator of a conditional quadratic loss function. We prove that the resulting model average estimator enjoys optimal asymptotic property and improves finite sample properties over the two commonly used information-based model selection estimators and their model average estimators via simulation studies. The proposed method is illustrated by a real data example.},
  archive  = {J},
  author   = {Jianhong Zhou and Alan T. K. Wan and Dalei Yu},
  doi      = {10.1002/sam.11598},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {679-691},
  title    = {Frequentist model averaging for zero-inflated poisson regression models},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A tree-based gene–environment interaction analysis with rare
features. <em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>15</em>(5), 648–674. (<a
href="https://doi.org/10.1002/sam.11578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Gene–environment (G-E) interaction analysis plays a critical role in understanding and modeling complex diseases. Compared to main-effect-only analysis, it is more seriously challenged by higher dimensionality, weaker signals, and the unique “main effects, interactions” variable selection hierarchy. In joint G-E interaction analysis under which a large number of G factors are analyzed in a single model, effort tailored to rare features (e.g., SNPs with low minor allele frequencies) has been limited. Existing investigations on rare features have been mostly focused on marginal analysis, where various data aggregation techniques have been developed, and hypothesis testings have been conducted to identify significant aggregated features. However, such techniques cannot be extended to joint G-E interaction analysis. In this study, building on a very recent tree-based data aggregation technique, which has been developed for main-effect-only analysis, we develop a new G-E interaction analysis approach tailored to rare features. The adopted data aggregation technique allows for more efficient information borrowing from neighboring rare features. Similar to some existing state-of-the-art ones, the proposed approach adopts penalization for variable selection, regularized estimation, and respect of the variable selection hierarchy. Simulation shows that it has more accurate identification of important interactions and main effects than several competing alternatives. In the analysis of NFBC1966 study, the proposed approach leads to findings different from the alternatives and with satisfactory prediction and stability performance.},
  archive  = {J},
  author   = {Mengque Liu and Qingzhao Zhang and Shuangge Ma},
  doi      = {10.1002/sam.11578},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {648-674},
  title    = {A tree-based gene–environment interaction analysis with rare features},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A modified least angle regression algorithm for interaction
selection with heredity. <em>Statistical Analysis and Data Mining: The
ASA Data Science Journal</em>, <em>15</em>(5), 630–647. (<a
href="https://doi.org/10.1002/sam.11577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In many practical problems, the main effects alone may not be enough to capture the relationship between the response and predictors, and the interaction effects are often of interest to scientific researchers. In considering a regression model with main effects and all possible two-way interaction effects, which we call the two-way interaction model, there is an important challenge—computational burden. One way to reduce the aforementioned problems is to consider the heredity constraint between the main and interaction effects. The heredity constraint assumes that a given interaction effect is significant only when the corresponding main effects are significant. Various sparse penalized methods to reflect the heredity constraint have been proposed, but those algorithms are still computationally demanding and can be applied to data where the dimension of the main effects is only few hundreds. In this paper, we propose a modification of the LARS algorithm for selecting interaction effects under the heredity constraint, which can be applied to high-dimensional data. Our numerical studies confirm that the proposed modified LARS algorithm is much faster and spends less memory than its competitors but has comparable prediction accuracies when the dimension of covariates is large.},
  archive  = {J},
  author   = {Woosung Kim and Seonghyeon Kim and Myung Hwan Na and Yongdai Kim},
  doi      = {10.1002/sam.11577},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {630-647},
  title    = {A modified least angle regression algorithm for interaction selection with heredity},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regression-based bayesian estimation and structure learning
for nonparanormal graphical models. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>15</em>(5), 611–629. (<a
href="https://doi.org/10.1002/sam.11576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A nonparanormal graphical model is a semiparametric generalization of a Gaussian graphical model for continuous variables in which it is assumed that the variables follow a Gaussian graphical model only after some unknown smooth monotone transformations. We consider a Bayesian approach to inference in a nonparanormal graphical model in which we put priors on the unknown transformations through a random series based on B-splines. We use a regression formulation to construct the likelihood through the Cholesky decomposition on the underlying precision matrix of the transformed variables and put shrinkage priors on the regression coefficients. We apply a plug-in variational Bayesian algorithm for learning the sparse precision matrix and compare the performance to a posterior Gibbs sampling scheme in a simulation study. We finally apply the proposed methods to a microarray dataset. The proposed methods have better performance as the dimension increases, and in particular, the variational Bayesian approach has the potential to speed up the estimation in the Bayesian nonparanormal graphical model without the Gaussianity assumption while retaining the information to construct the graph.},
  archive  = {J},
  author   = {Jami J. Mulgrave and Subhashis Ghosal},
  doi      = {10.1002/sam.11576},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {611-629},
  title    = {Regression-based bayesian estimation and structure learning for nonparanormal graphical models},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data twinning. <em>Statistical Analysis and Data Mining: The
ASA Data Science Journal</em>, <em>15</em>(5), 598–610. (<a
href="https://doi.org/10.1002/sam.11574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this work, we develop a method named Twinning for partitioning a dataset into statistically similar twin sets. Twinning is based on SPlit , a recently proposed model-independent method for optimally splitting a dataset into training and testing sets. Twinning is orders of magnitude faster than the SPlit algorithm, which makes it applicable to Big Data problems such as data compression. Twinning can also be used for generating multiple splits of a given dataset to aid divide-and-conquer procedures and k -fold cross validation.},
  archive  = {J},
  author   = {Akhil Vakayil and V. Roshan Joseph},
  doi      = {10.1002/sam.11574},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {598-610},
  title    = {Data twinning},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development and validation of models for two-week mortality
of inpatients with COVID-19 infection: A large prospective cohort study.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>15</em>(5), 586–597. (<a
href="https://doi.org/10.1002/sam.11572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recognizing COVID-19 patients at a greater risk of mortality assists medical staff to identify who benefits from more serious care. We developed and validated prediction models for two-week mortality of inpatients with COVID-19 infection based on clinical predictors. A prospective cohort study was started in February 2020 and is still continuing. In total, 57,705 inpatients with both a positive reverse transcription-polymerase chain reaction test and positive chest CT findings for COVID-19 were included. The outcome was mortality within 2 weeks of admission. Three prognostic models were developed for young, adult, and senior patients. Data from the capital province (Tehran) of Iran were used for validation, and data from all other provinces were used for development of the models. The model Young, was well-fitted to the data ( p &lt; 0.001, Nagelkerke R 2 = 0.697, C-statistics = 0.88) and the models Adult ( p &lt; 0.001, Nagelkerke R 2 = 0.340, C-statistics = 0.70) and Senior ( p &lt; 0.001, Nagelkerke R 2 = 0.208, C-statistics = 0.68) were also significant. Intubation, saturated O 2 &lt; 93%, impaired consciousness, acute respiratory distress syndrome, and cancer treatment were major risk factors. Elderly people were at greater risk of mortality. Young patients with a history of blood hypertension, vomiting, and fever; and adults with diabetes mellitus and cardiovascular disease had more mortality risk. Young people with myalgia; and adult patients with nausea, anorexia, and headache showed less risk of mortality than others.},
  archive  = {J},
  author   = {Mohammad Fathi and Nader Markazi Moghaddam and Leila Kheyrati},
  doi      = {10.1002/sam.11572},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {586-597},
  title    = {Development and validation of models for two-week mortality of inpatients with COVID-19 infection: A large prospective cohort study},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weighted AutoEncoding recommender system. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>15</em>(5), 570–585. (<a
href="https://doi.org/10.1002/sam.11571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recommender systems are information filtering tools that seek to match customers with products or services of interest. Most of the prevalent collaborative filtering recommender systems, such as matrix factorization and AutoRec, suffer from the “cold-start” problem, where they fail to provide meaningful recommendations for new users or new items due to informative-missing from the training data. To address this problem, we propose a weighted AutoEncoding model to leverage information from other users or items that share similar characteristics. The proposed method provides an effective strategy for borrowing strength from user or item-specific clustering structure as well as pairwise similarity in the training data, while achieving high computational efficiency and dimension reduction, and preserving nonlinear relationships between user preferences and item features. Simulation studies and applications to three real datasets show advantages in prediction accuracy of the proposed model compared to current state-of-the-art approaches.},
  archive  = {J},
  author   = {Shuying Zhu and Weining Shen and Annie Qu},
  doi      = {10.1002/sam.11571},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {570-585},
  title    = {Weighted AutoEncoding recommender system},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Measure inducing classification and regression trees for
functional data. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>15</em>(5), 553–569. (<a
href="https://doi.org/10.1002/sam.11569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a tree-based algorithm ( μ CART) for classification and regression problems in the context of functional data analysis, which allows to leverage measure learning and multiple splitting rules at the node level, with the objective of reducing error while retaining the interpretability of a tree. For each internal node, our main contribution is the idea of learning a weighted functional space by means of constrained convex optimization, which is then used to extract multiple weighted integral features from the functional predictors, in order to determine the binary split. The approach is designed to manage multiple functional predictors and/or responses, by defining suitable splitting rules and loss functions that can depend on the specific problem and can also be combined with additional scalar and categorical predictors, as the tree is grown with the original greedy CART algorithm. We focus on the case of scalar-valued functional predictors defined on unidimensional domains and illustrate the effectiveness of our method in both classification and regression tasks, through a simulation study and four real-world applications.},
  archive  = {J},
  author   = {Edoardo Belli and Simone Vantini},
  doi      = {10.1002/sam.11569},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {553-569},
  title    = {Measure inducing classification and regression trees for functional data},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A tutorial on generative adversarial networks with
application to classification of imbalanced data. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>15</em>(5), 543–552. (<a
href="https://doi.org/10.1002/sam.11570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A challenge unique to classification model development is imbalanced data. In a binary classification problem, class imbalance occurs when one class, the minority group, contains significantly fewer samples than the other class, the majority group. In imbalanced data, the minority class is often the class of interest (e.g., patients with disease). However, when training a classifier on imbalanced data, the model will exhibit bias towards the majority class and, in extreme cases, may ignore the minority class completely. A common strategy for addressing class imbalance is data augmentation. However, traditional data augmentation methods are associated with overfitting, where the model is fit to the noise in the data. In this tutorial we introduce an advanced method for data augmentation: generative adversarial networks (GANs). The advantages of GANs over traditional data augmentation methods are illustrated using the Breast Cancer Wisconsin study. To promote the adoption of GANs for data augmentation, we present an end-to-end pipeline that encompasses the complete life cycle of a machine learning project along with alternatives and good practices both in the paper and in a separate video. Our code, data, full results and video tutorial are publicly available in the paper&#39;s GitHub repository ( https://github.com/yuxiaohuang/research/tree/master/gwu/accepted/sam_2021 ).},
  archive  = {J},
  author   = {Yuxiao Huang and Kara G. Fields and Yan Ma},
  doi      = {10.1002/sam.11570},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {543-552},
  title    = {A tutorial on generative adversarial networks with application to classification of imbalanced data},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal ratio for data splitting. <em>Statistical Analysis
and Data Mining: The ASA Data Science Journal</em>, <em>15</em>(4),
531–538. (<a href="https://doi.org/10.1002/sam.11583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {It is common to split a dataset into training and testing sets before fitting a statistical or machine learning model. However, there is no clear guidance on how much data should be used for training and testing. In this article, we show that the optimal training/testing splitting ratio is p : 1 $$ \sqrt{p}:1 $$ , where p $$ p $$ is the number of parameters in a linear regression model that explains the data well.},
  archive  = {J},
  author   = {V. Roshan Joseph},
  doi      = {10.1002/sam.11583},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {531-538},
  title    = {Optimal ratio for data splitting},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarially robust subspace learning in the spiked
covariance model. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>15</em>(4), 521–530. (<a
href="https://doi.org/10.1002/sam.11580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We study the problem of robust subspace learning when there is an adversary who can attack the data to increase the projection error. By deriving the adversarial projection risk when data follows the multivariate Gaussian distribution with the spiked covariance, or so-called the Spiked Covariance model, we propose to use the empirical risk minimization method to obtain the optimal robust subspace. We then find a non-asymptotic upper bound of the adversarial excess risk, which implies the empirical risk minimization estimator is close to the optimal robust adversarial subspace. The optimization problem can be solved easily by the projected gradient descent algorithm for the rank-one spiked covariance model. However, in general, it is computationally intractable to solve the empirical risk minimization problem. Thus, we propose to minimize an upper bound of the empirical risk to find the robust subspace for the general spiked covariance model. Finally, we conduct numerical experiments to show the robustness of our proposed algorithms.},
  archive  = {J},
  author   = {Fei Sha and Ruizhi Zhang},
  doi      = {10.1002/sam.11580},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {521-530},
  title    = {Adversarially robust subspace learning in the spiked covariance model},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local influence analysis for the sliced average third-moment
estimation. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>15</em>(4), 509–520. (<a
href="https://doi.org/10.1002/sam.11575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sliced average third-moment estimation (SATME) is a typical method for sufficient dimension reduction (SDR) based on high-order conditional moment. It is useful, particularly in the scenarios of regression mixtures. However, as SATME uses the third-order conditional moment of the predictors given the response, it may not as robust as some other SDR methods that use lower order moments, say, sliced inverse regression (SIR) and slice average variance estimation (SAVE). Based on the space displacement function, a local influence analysis framework of SATME is constructed including a statistic of influence assessment for the observations. Furthermore, a data-trimming strategy is suggested based on the above influence assessment. The proposed methodologies solve a typical issue that also exists in some other SDR methods. A real-data analysis and simulations are presented.},
  archive  = {J},
  author   = {Weidong Rao and Xiaofei Liu and Fei Chen},
  doi      = {10.1002/sam.11575},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {509-520},
  title    = {Local influence analysis for the sliced average third-moment estimation},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A linear time method for the detection of collective and
point anomalies. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>15</em>(4), 494–508. (<a
href="https://doi.org/10.1002/sam.11586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The challenge of efficiently identifying anomalies in data sequences is an important statistical problem that now arises in many applications. Although there has been substantial work aimed at making statistical analyses robust to outliers, or point anomalies, there has been much less work on detecting anomalous segments, or collective anomalies, particularly in those settings where point anomalies might also occur. In this article, we introduce collective and point anomalies (CAPA), a computationally efficient approach that is suitable when collective anomalies are characterized by either a change in mean, variance, or both, and distinguishes them from point anomalies. Empirical results show that CAPA has close to linear computational cost as well as being more accurate at detecting and locating collective anomalies than other approaches. We demonstrate the utility of CAPA through its ability to detect exoplanets from light curve data from the Kepler telescope and its capacity to detect machine faults from temperature data.},
  archive  = {J},
  author   = {Alexander T. M. Fisch and Idris A. Eckley and Paul Fearnhead},
  doi      = {10.1002/sam.11586},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {494-508},
  title    = {A linear time method for the detection of collective and point anomalies},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Factor analysis of mixed data for anomaly detection.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>15</em>(4), 480–493. (<a
href="https://doi.org/10.1002/sam.11585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Anomaly detection aims to identify observations that deviate from the typical pattern of data. Anomalous observations may correspond to financial fraud, health risks, or incorrectly measured data in practice. We focus on unsupervised detection and the continuous and categorical (mixed) variable case. We show that detecting anomalies in mixed data is enhanced through first embedding the data then assessing an anomaly scoring scheme. We propose a kurtosis-weighted Factor Analysis of Mixed Data for anomaly detection to obtain a continuous embedding for anomaly scoring. We illustrate that anomalies are highly separable in the first and last few ordered dimensions of this space, and test various anomaly scoring experiments within this subspace. Results are illustrated for both simulated and real datasets, and the proposed approach is highly accurate for mixed data throughout these diverse scenarios.},
  archive  = {J},
  author   = {Matthew Davidow and David S. Matteson},
  doi      = {10.1002/sam.11585},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {480-493},
  title    = {Factor analysis of mixed data for anomaly detection},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intuitively adaptable outlier detector. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>15</em>(4), 463–479. (<a
href="https://doi.org/10.1002/sam.11562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Nowadays, we have been dealing with a large amount of data in which anomalies occur naturally for many reasons, both due to hardware and humans. Therefore, it is necessary to develop efficient tools that are easily adaptable to various data. The paper presents an innovative use of classical statistical tools to detect outliers in multidimensional data sets. The proposed approach uses well-known statistical methods in an innovative way and allows for a high level of efficiency to be achieved using multi-level aggregation. The effectiveness of the proposed innovative method is demonstrated by a series of numerical experiments.},
  archive  = {J},
  author   = {Krystyna Kiersztyn},
  doi      = {10.1002/sam.11562},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {463-479},
  title    = {Intuitively adaptable outlier detector},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Biclustering high-frequency financial time series based on
information theory. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>15</em>(4), 447–462. (<a
href="https://doi.org/10.1002/sam.11581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Clustering a large number of time series into relatively homogeneous groups is a well-studied unsupervised learning technique that has been widely used for grouping financial instruments (say, stocks) based on their stochastic properties across the entire time period under consideration. However, clustering algorithms ignore the notion of biclustering, that is, grouping of stocks only within a subset of times rather than over the entire time period. Biclustering algorithms enable grouping of stocks and times simultaneously, and thus facilitate improved pattern extraction for informed trading strategies. While biclustering methods may be employed for grouping low-frequency (daily) financial data, their use with high-frequency financial time series of intra-day trading data is especially useful. This paper develops a biclustering algorithm based on pairwise or groupwise mutual information between one-minute averaged stock returns within a trading day, using jackknife estimation of mutual information (JMI). We construct a multiple day time series biclustering (MI-MDTSB) algorithm that can capture refined and local comovement patterns between groups of stocks over a subset of continuous time points. Extensive numerical studies based on high-frequency returns data reveal interesting intra-day patterns among different asset groups and sectors.},
  archive  = {J},
  author   = {Haitao Liu and Jian Zou and Nalini Ravishanker},
  doi      = {10.1002/sam.11581},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {447-462},
  title    = {Biclustering high-frequency financial time series based on information theory},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A general iterative clustering algorithm. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>15</em>(4), 433–446. (<a
href="https://doi.org/10.1002/sam.11573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The quality of a cluster analysis of unlabeled units depends on the quality of the between units dissimilarity measures. Data-dependent dissimilarity is more objective than data independent geometric measures such as Euclidean distance. As suggested by Breiman, many data driven approaches are based on decision tree ensembles, such as a random forest (RF), that produce a proximity matrix that can easily be transformed into a dissimilarity matrix. An RF can be obtained using labels that distinguish units with real data from units with synthetic data. The resulting dissimilarity matrix is input to a clustering program and units are assigned labels corresponding to cluster membership. We introduce a general iterative cluster (GIC) algorithm that improves the proximity matrix and clusters of the base RF. The cluster labels are used to grow a new RF yielding an updated proximity matrix, which is entered into the clustering program. The process is repeated until convergence. The same procedure can be used with many base procedures such as the extremely randomized tree ensemble. We evaluate the performance of the GIC algorithm using benchmark and simulated data sets. The properties measured by the Silhouette score are substantially superior to the base clustering algorithm. The GIC package has been released in R: https://cran.r-project.org/web/packages/GIC/index.html .},
  archive  = {J},
  author   = {Ziqiang Lin and Eugene Laska and Carole Siegel},
  doi      = {10.1002/sam.11573},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {433-446},
  title    = {A general iterative clustering algorithm},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Some bayesian biclustering methods: Modeling and inference.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>15</em>(4), 413–432. (<a
href="https://doi.org/10.1002/sam.11584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Standard one-way clustering methods form homogeneous groups in a set of objects. Biclustering (or, two-way clustering) methods simultaneously cluster rows and columns of a rectangular data array in such a way that responses are homogeneous for all row-cluster by column-cluster cells. We propose a Bayes methodology for biclustering and corresponding MCMC algorithms. Our method not only identifies homogeneous biclusters, but also provides posterior probabilities that particular instances or features are clustered together. We further extend our proposal to address the biclustering problem under the commonly occurring situation of incomplete datasets. In addition to identifying homogeneous sets of rows and sets of columns, as in the complete data scenario, our approach also generates plausible predictions for missing/unobserved entries in the rectangular data array. Performances of our methodology are illustrated through simulation studies and applications to real datasets.},
  archive  = {J},
  author   = {Abhishek Chakraborty and Stephen B. Vardeman},
  doi      = {10.1002/sam.11584},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {413-432},
  title    = {Some bayesian biclustering methods: Modeling and inference},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Portability analysis of data mining models for fog events
forecasting. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>15</em>(3), 396–408. (<a
href="https://doi.org/10.1002/sam.11568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article describes an analytical method for comparing geographical sites and transferring fog forecasting models, trained by Data Mining techniques on a fixed site, across Italian airports. This portability method uses a specific intersite similarity measure based on the Euclidean distance between the performance vectors associated with each airport site. Performance vectors are useful for characterizing geographical sites. The components of a performance vector are the performance metrics of an Ensemble descriptive model. In the tests carried out, the comparison method provided very promising results, and the forecast model, when applied and evaluated on a new compatible site, shows only a small decrease in performance. The portability schema provides a meta-learning methodology for applying predictive models to new sites where a new model cannot be trained from scratch owing to the class imbalance problem or the lack of data for a specific learning. The methodology offers a measure for clustering geographical sites and extending weather knowledge from one site to another.},
  archive  = {J},
  author   = {Gaetano Zazzaro},
  doi      = {10.1002/sam.11568},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {396-408},
  title    = {Portability analysis of data mining models for fog events forecasting},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature selection for imbalanced data with deep sparse
autoencoders ensemble. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>15</em>(3), 376–395. (<a
href="https://doi.org/10.1002/sam.11567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Class imbalance is a common issue in many domain applications of learning algorithms. Oftentimes, in the same domains it is much more relevant to correctly classify and profile minority class observations. This need can be addressed by feature selection (FS), that offers several further advantages, such as decreasing computational costs, aiding inference and interpretability. However, traditional FS techniques may become suboptimal in the presence of strongly imbalanced data. To achieve FS advantages in this setting, we propose a filtering FS algorithm ranking feature importance on the basis of the reconstruction error of a deep sparse autoencoders ensemble (DSAEE). We use each DSAE trained only on majority class to reconstruct both classes. From the analysis of the aggregated reconstruction error, we determine the features where the minority class presents a different distribution of values w.r.t. the overrepresented one, thus identifying the most relevant features to discriminate between the two. We empirically demonstrate the efficacy of our algorithm in several experiments, both simulated and on high-dimensional datasets of varying sample size, showcasing its capability to select relevant and generalizable features to profile and classify minority class, outperforming other benchmark FS methods. We also briefly present a real application in radiogenomics, where the methodology was applied successfully.},
  archive  = {J},
  author   = {Michela Carlotta Massi and Francesca Gasperoni and Francesca Ieva and Anna Maria Paganoni},
  doi      = {10.1002/sam.11567},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {376-395},
  title    = {Feature selection for imbalanced data with deep sparse autoencoders ensemble},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Handwriting identification using random forests and
score-based likelihood ratios. <em>Statistical Analysis and Data Mining:
The ASA Data Science Journal</em>, <em>15</em>(3), 357–375. (<a
href="https://doi.org/10.1002/sam.11566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Handwriting analysis is conducted by forensic document examiners who are able to visually recognize characteristics of writing to evaluate the evidence of writership. Recently, there have been incentives to investigate how to quantify the similarity between two written documents to support the conclusions drawn by experts. We use an automatic algorithm within the “handwriter” package in R, to decompose a handwritten sample into small graphical units of writing. These graphs are sorted into 40 exemplar groups or clusters. We hypothesize that the frequency with which a person contributes graphs to each cluster is characteristic of their handwriting. Given two questioned handwritten documents, we can then use the vectors of cluster frequencies to quantify the similarity between the two documents. We extract features from the difference between the vectors and combine them using a random forest. The output from the random forest is used as the similarity score to compare documents. We estimate the distributions of the similarity scores computed from multiple pairs of documents known to have been written by the same and by different persons, and use these estimated densities to obtain score-based likelihood ratios (SLRs) that rely on different assumptions. We find that the SLRs are able to indicate whether the similarity observed between two documents is more or less likely depending on writership.},
  archive  = {J},
  author   = {Madeline Quinn Johnson and Danica M. Ommen},
  doi      = {10.1002/sam.11566},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {357-375},
  title    = {Handwriting identification using random forests and score-based likelihood ratios},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient importance sampling imputation algorithms for
quantile and composite quantile regression. <em>Statistical Analysis and
Data Mining: The ASA Data Science Journal</em>, <em>15</em>(3), 339–356.
(<a href="https://doi.org/10.1002/sam.11565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Nowadays, missing data in regression model is one of the most well-known topics. In this paper, we propose a class of efficient importance sampling imputation algorithms (EIS) for quantile and composite quantile regression with missing covariates. They are an EIS in quantile regression (EIS Q ) and its three extensions in composite quantile regression (EIS CQ ). Our EIS Q uses an interior point (IP) approach, while EIS CQ algorithms use IP and other two well-known approaches: Majorize-minimization (MM) and coordinate descent (CD). The aims of our proposed EIS algorithms are to decrease estimated variances and relieve computational burden at the same time, which improves the performances of coefficients estimators in both estimated and computational efficiencies. To compare our EIS algorithms with other existing competitors including complete cases analysis and multiple imputation, the paper carries out a series of simulation studies with different sample sizes and different levels of missing rates under different missing mechanism models. Finally, we apply all the algorithms to part of the examination data in National Health and Nutrition Examination Survey.},
  archive  = {J},
  author   = {Hao Cheng},
  doi      = {10.1002/sam.11565},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {339-356},
  title    = {Efficient importance sampling imputation algorithms for quantile and composite quantile regression},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural-network transformation models for counting processes.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>15</em>(3), 322–338. (<a
href="https://doi.org/10.1002/sam.11564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {While many survival models have been invented, the Cox model and the proportional odds model are among the most popular ones. Both models are special cases of the linear transformation model. The linear transformation model typically assumes a linear function on covariates, which may not reflect the complex relationship between covariates and survival outcomes. Nonlinear functional form can also be specified in the linear transformation model. Nonetheless, the underlying functional form is unknown and mis-specifying it leads to biased estimates and reduced prediction accuracy of the model. To address this issue, we develop a neural-network transformation model. Similar to neural networks, the neural-network transformation model uses its hierarchical structure to learn complex features from simpler ones and is capable of approximating the underlying functional form of covariates. It also inherits advantages from the linear transformation model, making it applicable to both time-to-event analyses and recurrent event analyses. Simulations demonstrate that the neural-network transformation model outperforms the linear transformation model in terms of estimation and prediction accuracy when the covariate effects are nonlinear. The advantage of the new model over the linear transformation model is also illustrated via two real applications.},
  archive  = {J},
  author   = {Rongzi Liu and Chenxi Li and Qing Lu},
  doi      = {10.1002/sam.11564},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {322-338},
  title    = {Neural-network transformation models for counting processes},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bag of little bootstraps for massive and distributed
longitudinal data. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>15</em>(3), 314–321. (<a
href="https://doi.org/10.1002/sam.11563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Linear mixed models are widely used for analyzing longitudinal datasets, and the inference for variance component parameters relies on the bootstrap method. However, health systems and technology companies routinely generate massive longitudinal datasets that make the traditional bootstrap method infeasible. To solve this problem, we extend the highly scalable bag of little bootstraps method for independent data to longitudinal data and develop a highly efficient Julia package MixedModelsBLB.jl. Simulation experiments and real data analysis demonstrate the favorable statistical performance and computational advantages of our method compared to the traditional bootstrap method. For the statistical inference of variance components, it achieves 200 times speedup on the scale of 1 million subjects (20 million total observations), and is the only currently available tool that can handle more than 10 million subjects (200 million total observations) using desktop computers.},
  archive  = {J},
  author   = {Xinkai Zhou and Jin J. Zhou and Hua Zhou},
  doi      = {10.1002/sam.11563},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {314-321},
  title    = {Bag of little bootstraps for massive and distributed longitudinal data},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-scale affinities with missing data: Estimation and
applications. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>15</em>(3), 303–313. (<a
href="https://doi.org/10.1002/sam.11561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Many machine learning algorithms depend on weights that quantify row and column similarities of a data matrix. The choice of weights can dramatically impact the effectiveness of the algorithm. Nonetheless, the problem of choosing weights has arguably not been given enough study. When a data matrix is completely observed, Gaussian kernel affinities can be used to quantify the local similarity between pairs of rows and pairs of columns. Computing weights in the presence of missing data, however, becomes challenging. In this paper, we propose a new method to construct row and column affinities even when data are missing by building off a co-clustering technique. This method takes advantage of solving the optimization problem for multiple pairs of cost parameters and filling in the missing values with increasingly smooth estimates. It exploits the coupled similarity structure among both the rows and columns of a data matrix. We show these affinities can be used to perform tasks such as data imputation, clustering, and matrix completion on graphs.},
  archive  = {J},
  author   = {Min Zhang and Gal Mishne and Eric C. Chi},
  doi      = {10.1002/sam.11561},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {303-313},
  title    = {Multi-scale affinities with missing data: Estimation and applications},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The fairness-accuracy pareto front. <em>Statistical Analysis
and Data Mining: The ASA Data Science Journal</em>, <em>15</em>(3),
287–302. (<a href="https://doi.org/10.1002/sam.11560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Algorithmic fairness seeks to identify and correct sources of bias in machine learning algorithms. Confoundingly, ensuring fairness often comes at the cost of accuracy. We provide formal tools in this work for reconciling this fundamental tension in algorithm fairness. Specifically, we put to use the concept of Pareto optimality from multiobjective optimization and seek the fairness-accuracy Pareto front of a neural network classifier. We demonstrate that many existing algorithmic fairness methods are performing the so-called linear scalarization scheme, which has severe limitations in recovering Pareto optimal solutions. We instead apply the Chebyshev scalarization scheme which is provably superior theoretically and no more computationally burdensome at recovering Pareto optimal solutions compared to the linear scheme.},
  archive  = {J},
  author   = {Susan Wei and Marc Niethammer},
  doi      = {10.1002/sam.11560},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {287-302},
  title    = {The fairness-accuracy pareto front},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven sparse partial least squares. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>15</em>(2), 264–282. (<a
href="https://doi.org/10.1002/sam.11558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the supervised high dimensional settings with a large number of variables and a low number of individuals, variable selection allows a simpler interpretation and more reliable predictions. That subspace selection is often managed with supervised tools when the real question is motivated by variable prediction. We propose a partial least square (PLS) based method, called data-driven sparse PLS (ddsPLS), allowing variable selection both in the covariate and the response parts using a single hyperparameter per component. The subspace estimation is also performed by tuning a number of underlying parameters. The ddsPLS method is compared with existing methods such as classical PLS and two well established sparse PLS methods through numerical simulations. The observed results are promising both in terms of variable selection and prediction performance. This methodology is based on new prediction quality descriptors associated with the classical and , and uses bootstrap sampling to tune parameters and select an optimal regression model.},
  archive  = {J},
  author   = {Hadrien Lorenzo and Olivier Cloarec and Rodolphe Thiébaut and Jérôme Saracco},
  doi      = {10.1002/sam.11558},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {264-282},
  title    = {Data-driven sparse partial least squares},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Factor analysis for high-dimensional time series: Consistent
estimation and efficient computation. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>15</em>(2), 247–263. (<a
href="https://doi.org/10.1002/sam.11557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {To deal with the factor analysis for high-dimensional stationary time series, this paper suggests a novel method that integrates three ideas. First, based on the eigenvalues of a non-negative definite matrix, we propose a new approach for consistently determining the number of factors. The proposed method is computationally efficient with a single step procedure, especially when both weak and strong factors exist in the factor model. Second, a fresh measurement of the difference between the factor loading matrix and its estimate is recommended to overcome the nonidentifiability of the loading matrix due to any geometric rotation. The asymptotic results of our proposed method are also studied under this measurement, which enjoys “blessing of dimensionality.” Finally, with the estimated factors, the latent vector autoregressive (VAR) model is analyzed such that the convergence rate of the estimated coefficients is as fast as when the samples of VAR model are observed. In support of our results on consistency and computational efficiency, the finite sample performance of the proposed method is examined by simulations and the analysis of one real data example.},
  archive  = {J},
  author   = {Qiang Xia and Heung Wong and Shirun Shen and Kejun He},
  doi      = {10.1002/sam.11557},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {247-263},
  title    = {Factor analysis for high-dimensional time series: Consistent estimation and efficient computation},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive batching for gaussian process surrogates with
application in noisy level set estimation. <em>Statistical Analysis and
Data Mining: The ASA Data Science Journal</em>, <em>15</em>(2), 225–246.
(<a href="https://doi.org/10.1002/sam.11556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We develop adaptive replicated designs for Gaussian process metamodels of stochastic experiments. Adaptive batching is a natural extension of sequential design heuristics with the benefit of replication growing as response features are learned, inputs concentrate, and the metamodeling overhead rises. Motivated by the problem of learning the level set of the mean simulator response, we develop five novel schemes: Multi-Level Batching (MLB), Ratchet Batching (RB), Adaptive Batched Stepwise Uncertainty Reduction (ABSUR), Adaptive Design with Stepwise Allocation (ADSA), and Deterministic Design with Stepwise Allocation (DDSA). Our algorithms simultaneously (MLB, RB, and ABSUR) or sequentially (ADSA and DDSA) determine the sequential design inputs and the respective number of replicates. Illustrations using synthetic examples and an application in quantitative finance (Bermudan option pricing via Regression Monte Carlo) show that adaptive batching brings significant computational speed-ups with minimal loss of modeling fidelity.},
  archive  = {J},
  author   = {Xiong Lyu and Michael Ludkovski},
  doi      = {10.1002/sam.11556},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {225-246},
  title    = {Adaptive batching for gaussian process surrogates with application in noisy level set estimation},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A family of mixture models for biclustering. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>15</em>(2), 206–224. (<a
href="https://doi.org/10.1002/sam.11555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Biclustering is used for simultaneous clustering of the observations and variables when there is no group structure known a priori. It is being increasingly used in bioinformatics, text analytics, and so on. Previously, biclustering has been introduced in a model-based clustering framework by utilizing a structure similar to a mixture of factor analyzers. In such models, observed variables are modeled using a latent variable that is assumed to be from . Clustering of variables are introduced by imposing constraints on the entries of the factor loading matrix to be 0 and 1 that results in block diagonal covariance matrices. However, this approach is overly restrictive as off-diagonal elements in the blocks of the covariance matrices can only be 1 which can lead to unsatisfactory model fit on complex data. Here, the latent variable is assumed to be from a where is a diagonal matrix. This ensures that the off-diagonal terms in the block matrices within the covariance matrices are non-zero and not restricted to be 1. This leads to a superior model fit on complex data. A family of models is developed by imposing constraints on the components of the covariance matrix. For parameter estimation, an alternating expectation conditional maximization (AECM) algorithm is used. Finally, the proposed method is illustrated using simulated and real datasets.},
  archive  = {J},
  author   = {Wangshu Tu and Sanjeena Subedi},
  doi      = {10.1002/sam.11555},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {206-224},
  title    = {A family of mixture models for biclustering},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-dimensional classification based on nonparametric
maximum likelihood estimation under unknown and inhomogeneous variances.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>15</em>(2), 193–205. (<a
href="https://doi.org/10.1002/sam.11554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a new method in high-dimensional classification based on estimation of high-dimensional mean vector under unknown and unequal variances. Our proposed method is based on a semi-parametric model that combines nonparametric and parametric models for mean and variance, respectively. Our proposed method is designed to be robust to the structure of the mean vector, while most existing methods are developed for some specific cases such as either sparse or non-sparse case of the mean vector. In addition, we also consider estimating mean and variance separately under nonparametric empirical Bayes framework that has advantage over existing nonparametric empirical Bayes classifiers based on standardization. We present simulation studies showing that our proposed method outperforms a variety of existing methods. Application to real data sets demonstrates robustness of our method to various types of data sets, while all other methods produce either sensitive or poor results for different data sets.},
  archive  = {J},
  author   = {Hoyoung Park and Seungchul Baek and Junyong Park},
  doi      = {10.1002/sam.11554},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {193-205},
  title    = {High-dimensional classification based on nonparametric maximum likelihood estimation under unknown and inhomogeneous variances},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regrouped design in privacy analysis for multinomial
microdata. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>15</em>(2), 179–192. (<a
href="https://doi.org/10.1002/sam.11553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we are dealing with the dual goals for protecting privacy and making statistical inferences from the disseminated data using the regrouped design. It is not difficult to protect the privacy of patients by perturbing data. The problem is to perturb the data in such a way that privacy is protected, and also, the released data are useful for research. By applying the regrouped design, the dataset is released with the dummy groups associated with the actual groups via a pre-specified transition probability matrix. Small stagnation probabilities of regrouped design are recommended to reach a small disclosure risk and a higher power of hypothesis testing. The power of test statistic in the released data increases as the stagnation probabilities depart from 0.5. The disclosure risk can be reduced further if more quasi-identifiers are relocated. An example of National Health Insurance Research Database is given to illustrate the use of the regrouped design to protect the privacy and make the statistical inference.},
  archive  = {J},
  author   = {Shu-Mei Wan and Wen-Yaw Chung and Monica Mayeni Manurung and Kwang-Hwa Chang and Chien-Hua Wu},
  doi      = {10.1002/sam.11553},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {179-192},
  title    = {Regrouped design in privacy analysis for multinomial microdata},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tracking clusters and anomalies in evolving data streams.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>15</em>(2), 156–178. (<a
href="https://doi.org/10.1002/sam.11552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Data-driven anomaly detection methods typically build a model for the normal behavior of the target system, and score each data instance with respect to this model. A threshold is invariably needed to identify data instances with high (or low) scores as anomalies. This presents a practical limitation on the applicability of such methods, since most methods are sensitive to the choice of the threshold, and it is challenging to set optimal thresholds. The issue is exacerbated in a streaming scenario, where the optimal thresholds vary with time. We present a probabilistic framework to explicitly model the normal and anomalous behaviors and probabilistically reason about the data. An extreme value theory based formulation is proposed to model the anomalous behavior as the extremes of the normal behavior. As a specific instantiation, a joint nonparametric clustering and anomaly detection algorithm (INCAD) is proposed that models the normal behavior as a Dirichlet process mixture model. Results on a variety of datasets, including streaming data, show that the proposed method provides effective and simultaneous clustering and anomaly detection without requiring strong initialization and threshold parameters.},
  archive  = {J},
  author   = {Sreelekha Guggilam and Varun Chandola and Abani Patra},
  doi      = {10.1002/sam.11552},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {156-178},
  title    = {Tracking clusters and anomalies in evolving data streams},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analyzing relevance vector machines using a single penalty
approach. <em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>15</em>(2), 143–155. (<a
href="https://doi.org/10.1002/sam.11551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Relevance vector machine (RVM) is a popular sparse Bayesian learning model typically used for prediction. Recently it has been shown that improper priors assumed on multiple penalty parameters in RVM may lead to an improper posterior. Currently in the literature, the sufficient conditions for posterior propriety of RVM do not allow improper priors over the multiple penalty parameters. In this article, we propose a single penalty relevance vector machine (SPRVM) model in which multiple penalty parameters are replaced by a single penalty and we consider a semi-Bayesian approach for fitting the SPRVM. The necessary and sufficient conditions for posterior propriety of SPRVM are more liberal than those of RVM and allow for several improper priors over the penalty parameter. Additionally, we also prove the geometric ergodicity of the Gibbs sampler used to analyze the SPRVM model and hence can estimate the asymptotic standard errors associated with the Monte Carlo estimate of the means of the posterior predictive distribution. Such a Monte Carlo standard error cannot be computed in the case of RVM, since the rate of convergence of the Gibbs sampler used to analyze RVM is not known. The predictive performance of RVM and SPRVM is compared by analyzing two simulation examples and three real life datasets.},
  archive  = {J},
  author   = {Anand Dixit and Vivekananda Roy},
  doi      = {10.1002/sam.11551},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {143-155},
  title    = {Analyzing relevance vector machines using a single penalty approach},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Residual’s influence index (RINFIN), bad leverage and
unmasking in high dimensional l2-regression. <em>Statistical Analysis
and Data Mining: The ASA Data Science Journal</em>, <em>15</em>(1),
125–138. (<a href="https://doi.org/10.1002/sam.11550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In linear regression of Y on X (∈ R p ) with parameters β (∈ R p +1 ), statistical inference is unreliable when observations are obtained from gross-error model, F ϵ , G = (1 − ϵ ) F + ϵG , instead of the assumed probability F ; G is gross-error probability, 0 &lt; ϵ &lt; 1. Residual&#39;s influence index ( RINFIN ) at ( x , y ) is introduced, with components measuring also the local influence of x in the residual and large value flagging a bad leverage case (from G ), thus causing unmasking. Large sample properties of RINFIN are presented to confirm significance of the findings, but often the large difference in the RINFIN scores of the data is indicative. RINFIN is successful with microarray data, simulated, high dimensional data and classic regression data sets. RINFIN &#39;s performance improves as p increases and can be used in multiple response linear regression.},
  archive  = {J},
  author   = {Yannis G. Yatracos},
  doi      = {10.1002/sam.11550},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {125-138},
  title    = {Residual&#39;s influence index (RINFIN), bad leverage and unmasking in high dimensional l2-regression},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparison of merging strategies for building machine
learning models on multiple independent gene expression data sets.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>15</em>(1), 112–124. (<a
href="https://doi.org/10.1002/sam.11549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {High-dimensional gene expression data are regularly studied for their ability to separate different groups of samples by means of machine learning (ML) models. Meanwhile, a large number of such data are publicly available. Several approaches for meta-analysis on independent sets of gene expression data have been proposed, mainly focusing on the step of feature selection, a typical step in fitting a ML model. Here, we compare different strategies of merging the information of such independent data sets to train a classifier model. Specifically, we compare the strategy of merging data sets directly (strategy A), and the strategy of merging the classification results (strategy B). We use simulations with pure artificial data as well as evaluations based on independent gene expression data from lung fibrosis studies to compare the two merging approaches. In the simulations, the number of studies, the strength of batch effects, and the separability are varied. The comparison incorporates five standard ML techniques typically used for high-dimensional data, namely discriminant analysis, support vector machines, least absolute shrinkage and selection operator, random forest, and artificial neural networks. Using cross-study validations, we found that direct data merging yields higher accuracies when having training data of three or four studies, and merging of classification results performed better when having only two training studies. In the evaluation with the lung fibrosis data, both strategies showed a similar performance.},
  archive  = {J},
  author   = {Jessica Krepel and Magdalena Kircher and Moritz Kohls and Klaus Jung},
  doi      = {10.1002/sam.11549},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {112-124},
  title    = {Comparison of merging strategies for building machine learning models on multiple independent gene expression data sets},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian modeling of location, scale, and shape parameters
in skew-normal regression models. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>15</em>(1), 98–111. (<a
href="https://doi.org/10.1002/sam.11548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we propose Bayesian skew-normal regression models where the location, scale and shape parameters follow (linear or nonlinear) regression structures, and the variable of interest follows the Azzalini skew-normal distribution. A Bayesian method is developed to fit the proposed models, using working variables to build the kernel transition functions. To illustrate the performance of the proposed Bayesian method and application of the model to analyze statistical data, we present results of simulated studies and of the application to studies of forced displacement in Colombia.},
  archive  = {J},
  author   = {Martha Lucía Corrales and Edilberto Cepeda-Cuervo},
  doi      = {10.1002/sam.11548},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {98-111},
  title    = {Bayesian modeling of location, scale, and shape parameters in skew-normal regression models},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient k-modes algorithm for clustering categorical
datasets. <em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>15</em>(1), 83–97. (<a
href="https://doi.org/10.1002/sam.11546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Mining clusters from data is an important endeavor in many applications. The k -means method is a popular, efficient, and distribution-free approach for clustering numerical-valued data, but does not apply for categorical-valued observations. The k -modes method addresses this lacuna by replacing the Euclidean with the Hamming distance and the means with the modes in the k -means objective function. We provide a novel, computationally efficient implementation of k -modes, called Optimal Transfer Quick Transfer (OTQT). We prove that OTQT finds updates to improve the objective function that are undetectable to existing k -modes algorithms. Although slightly slower per iteration due to algorithmic complexity, OTQT is always more accurate and almost always faster (and only barely slower on some datasets) to the final optimum. Thus, we recommend OTQT as the preferred, default algorithm for k -modes optimization.},
  archive  = {J},
  author   = {Karin S. Dorman and Ranjan Maitra},
  doi      = {10.1002/sam.11546},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {83-97},
  title    = {An efficient k-modes algorithm for clustering categorical datasets},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predictive models with end user preference. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>15</em>(1), 69–82. (<a
href="https://doi.org/10.1002/sam.11545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Classical machine learning models typically try to optimize the model based on the most discriminatory features of the data; however, they do not usually account for end user preferences. In certain applications, this can be a serious issue as models not aware of user preferences could become costly, untrustworthy, or privacy-intrusive to use, thus becoming irrelevant and/or uninterpretable. Ideally, end users with domain knowledge could propose preferable features that the predictive model could then take into account. In this paper, we propose a generic modeling method that respects end user preferences via a relative ranking system to express multi-criteria preferences and a regularization term in the model&#39;s objective function to incorporate the ranked preferences. In a more generic perspective, this method is able to plug user preferences into existing predictive models without creating completely new ones. We implement this method in the context of decision trees and are able to achieve a comparable classification accuracy while reducing the use of undesirable features.},
  archive  = {J},
  author   = {Yifan Zhao and Xian Yang and Carolina Bolnykh and Steve Harenberg and Nodirbek Korchiev and Saavan Raj Yerramsetty and Bhanu Prasad Vellanki and Ramakanth Kodumagulla and Nagiza F. Samatova},
  doi      = {10.1002/sam.11545},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {69-82},
  title    = {Predictive models with end user preference},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weighted validation of heteroscedastic regression models for
better selection. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>15</em>(1), 57–68. (<a
href="https://doi.org/10.1002/sam.11544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we suggest a method for improving model selection in the presence of heteroscedasticity. For this purpose, we measure the heteroscedasticity in the data using the inter-quartile range (IQR) of the fitted values under the framework of cross-validation. To find the IQR, we fit 0.25 and 0.75 generic quantile regression using the training data. The two models then predict the values of the response variable at 0.25 and 0.75 quantiles in the test data, which yields predicted IQR. To reduce the effect of heteroscedastic data in the model selection, we propose to use weighted prediction error. The inverse of the predicted IQR is utilized to estimate the weights. The proposed method reduces the impact of large prediction errors via weighted prediction and leads to better model and parameter selection. The benefits of the proposed method are demonstrated in simulations and with two real data sets.},
  archive  = {J},
  author   = {Yoonsuh Jung and Hayoung Kim},
  doi      = {10.1002/sam.11544},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {57-68},
  title    = {Weighted validation of heteroscedastic regression models for better selection},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sketched stochastic dictionary learning for large-scale data
and application to high-throughput mass spectrometry. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>15</em>(1), 43–56. (<a
href="https://doi.org/10.1002/sam.11542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Factorization of large data corpora has emerged as an essential technique to extract dictionaries (sets of patterns that are meaningful for sparse encoding). Following this line, we present a novel algorithm based on compressive learning theory. In this framework, the (arbitrarily large) dataset of interest is replaced by a fixed-size sketch resulting from a random sampling of the data distribution characteristic function. We apply our algorithm to the extraction of chromatographic elution profiles in mass spectrometry data, where it demonstrates its efficiency and interest compared to other related algorithms.},
  archive  = {J},
  author   = {Olga Permiakova and Thomas Burger},
  doi      = {10.1002/sam.11542},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {43-56},
  title    = {Sketched stochastic dictionary learning for large-scale data and application to high-throughput mass spectrometry},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modal linear regression models with multiplicative
distortion measurement errors. <em>Statistical Analysis and Data Mining:
The ASA Data Science Journal</em>, <em>15</em>(1), 15–42. (<a
href="https://doi.org/10.1002/sam.11541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We consider modal linear regression models when neither the response variable nor the covariates can be directly observed, but are measured with multiplicative distortion measurement errors. Four calibration procedures are used to estimate parameters in the modal linear regression models, namely, conditional mean calibration, conditional absolute mean calibration, conditional variance calibration, and conditional absolute logarithmic calibration. The asymptotic properties for the estimators based on four calibration procedures are established. Monte Carlo simulation experiments are conducted to examine the performance of the proposed estimators. The proposed estimators are applied to analyze a forest fires dataset for an illustration.},
  archive  = {J},
  author   = {Jun Zhang and Gaorong Li and Yiping Yang},
  doi      = {10.1002/sam.11541},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {15-42},
  title    = {Modal linear regression models with multiplicative distortion measurement errors},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sample selection bias in evaluation of prediction
performance of causal models. <em>Statistical Analysis and Data Mining:
The ASA Data Science Journal</em>, <em>15</em>(1), 5–14. (<a
href="https://doi.org/10.1002/sam.11559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Causal models are notoriously difficult to validate because they make untestable assumptions regarding confounding. New scientific experiments offer the possibility of evaluating causal models using prediction performance. Prediction performance measures are typically robust to violations in causal assumptions. However, prediction performance does depend on the selection of training and test sets. Biased training sets can lead to optimistic assessments of model performance. In this work, we revisit the prediction performance of several recently proposed causal models tested on a genetic perturbation data set of Kemmeren. We find that sample selection bias is likely a key driver of model performance. We propose using a less-biased evaluation set for assessing prediction performance and compare models on this new set. In this setting, the causal models have similar or worse performance compared to standard association-based estimators such as Lasso. Finally, we compare the performance of causal estimators in simulation studies that reproduce the Kemmeren structure of genetic knockout experiments but without any sample selection bias. These results provide an improved understanding of the performance of several causal models and offer guidance on how future studies should use Kemmeren.},
  archive  = {J},
  author   = {James P. Long and Min Jin Ha},
  doi      = {10.1002/sam.11559},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {5-14},
  title    = {Sample selection bias in evaluation of prediction performance of causal models},
  volume   = {15},
  year     = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
