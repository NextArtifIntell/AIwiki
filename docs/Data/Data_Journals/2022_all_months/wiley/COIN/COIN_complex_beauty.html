<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COIN_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="coin---90">COIN - 90</h2>
<ul>
<li><details>
<summary>
(2022). Hybridizing graph-based gaussian mixture model with machine
learning for classification of fraudulent transactions. <em>COIN</em>,
<em>38</em>(6), 2134–2160. (<a
href="https://doi.org/10.1111/coin.12561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been observed that a good number of financial organizations often face a number of threats due to credit card fraud that affects consistently to the card holder as well as the organizations. This is one of the fastest-growing frauds of its kind and the most emerging problems for the institutions to prevent. A number of researchers and analysts have shown interest to work on this area in order to identify such issues in an effective manner by applying various supervised as well as unsupervised learning approaches. In this assessment, three classification techniques such as support vector machine (SVM), k -nearest neighbor ( k -NN), and extreme learning machine (ELM) that come under supervised learning category are applied to the BankSim data to categorize the normal and fraudulent class transactions in credit card. These algorithms are incorporated with the graph features extracted from the dataset by using a database tool Neo4j . The nodes of the graph represent the transactional data samples and the edges create relationships among the nodes to find the patterns of data using connected data analysis. k-fold cross validation approach in Gaussian mixture model (GMM) has been applied for classification of the credit card transaction data in a single distribution. Further, a combined graph-based Gaussian mixture model (CGB-GMM) has been proposed to effectively detect the fraudulent instances in credit card transactions with the application of graph algorithms such as degree centrality, LPA, page rank, and so forth. Each of the learning algorithms are implemented with and without the application of graph algorithms and their performances are assessed empirically for analysis.},
  archive      = {J_COIN},
  author       = {Debachudamani Prusti and Ranjan Kumar Behera and Santanu Kumar Rath},
  doi          = {10.1111/coin.12561},
  journal      = {Computational Intelligence},
  month        = {12},
  number       = {6},
  pages        = {2134-2160},
  shortjournal = {Comput. Intell.},
  title        = {Hybridizing graph-based gaussian mixture model with machine learning for classification of fraudulent transactions},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Categorization of emotions in dog behavior based on the deep
neural network. <em>COIN</em>, <em>38</em>(6), 2116–2133. (<a
href="https://doi.org/10.1111/coin.12559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this article is to present a neural system based on stock architecture for recognizing emotional behavior in dogs. Our considerations are inspired by the original work of Franzoni et al. on recognizing dog emotions. An appropriate set of photographic data has been compiled taking into account five classes of emotional behavior in dogs of one breed, including joy, anger, licking, yawning, and sleeping. Focusing on a particular breed makes it easier to experiment and recognize the emotional behavior of dogs. To broaden our conclusions, in our research study we compare our system with other systems of different architectures. In addition, we also use modern transfer learning with augmentation and data normalization techniques. The results show that VGG16 and VGG19 are the most suitable backbone networks. Therefore, a certain deep neural network, named mVGG16, based on the suboptimal VGG16 has been created, trained and fine-tuned with transfer (without augmentation and normalization). The developed system is then tested against an internal test dataset. In addition, to show the robustness of the system, a set of external data outside the breed is also taken into account. Being able to detect unsafe dog behavior and rely on a generalization for other breeds is worth popularizing. Equally important are the possible applications of the system to monitor the behavior of pets in the absence of their owners.},
  archive      = {J_COIN},
  author       = {Zdzisław Kowalczuk and Michał Czubenko and Weronika Żmuda-Trzebiatowska},
  doi          = {10.1111/coin.12559},
  journal      = {Computational Intelligence},
  month        = {12},
  number       = {6},
  pages        = {2116-2133},
  shortjournal = {Comput. Intell.},
  title        = {Categorization of emotions in dog behavior based on the deep neural network},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical dirichlet and pitman–yor process mixtures of
shifted-scaled dirichlet distributions for proportional data modeling.
<em>COIN</em>, <em>38</em>(6), 2095–2115. (<a
href="https://doi.org/10.1111/coin.12558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, first, we propose a novel unsupervised learning method based on a hierarchical Dirichlet process mixture of shifted-scaled Dirichlet (SSD) distributions. Second, we extend it to a hierarchical Pitman–Yor process mixture of SSD distributions. The goal is to find a model that properly fits complex real-world data. Our models are based on SSD distributions that are more flexible than Dirichlet distribution in fitting proportional data. Simultaneous data fitting (parameter estimate) and model selection (model complexity determination) are possible with the suggested methods. We applied batch and online variational inference for learning the models. The online setting allows us to feed our models with large-scale streaming data. The effectiveness of our proposed models is evaluated by four realistic and challenging applications, namely, spam email detection, texture clustering, traffic sign detection, and vehicle detection. Experimental results demonstrate the potential of our models to fit proportional data.},
  archive      = {J_COIN},
  author       = {Ali Baghdadi and Narges Manouchehri and Zachary Patterson and Wentao Fan and Nizar Bouguila},
  doi          = {10.1111/coin.12558},
  journal      = {Computational Intelligence},
  month        = {12},
  number       = {6},
  pages        = {2095-2115},
  shortjournal = {Comput. Intell.},
  title        = {Hierarchical dirichlet and Pitman–Yor process mixtures of shifted-scaled dirichlet distributions for proportional data modeling},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse neural network regression with variable selection.
<em>COIN</em>, <em>38</em>(6), 2075–2094. (<a
href="https://doi.org/10.1111/coin.12557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper reports on our study of a sparse neural network regression method based on a single hidden layer architecture and sparsity-inducing penalties. To determine the size of network in a data-adaptive way, we adopt the lasso and group lasso penalty functions to simultaneously induce sparsity at node and predictor levels. We also devise several techniques to improve performance of the neural network regression estimator. We adopt a B-spline activation function with a compact support to identify local trends of data. In addition, we develop an algorithm based on node addition process, in which additional nodes come into the network as a complexity parameter decreases. At each value of the complexity parameter, the algorithm initializes the additional nodes along the direction that best captures the unexplained functional relationship between the response and predictors. The optimization step is conducted based on an efficient coordinate descent algorithm. Numerical studies based on simulated and real datasets illustrate that the combination of the aforementioned devices significantly improves the performance of the neural network estimator.},
  archive      = {J_COIN},
  author       = {Jae-Kyung Shin and Kwan-Young Bak and Ja-Yong Koo},
  doi          = {10.1111/coin.12557},
  journal      = {Computational Intelligence},
  month        = {12},
  number       = {6},
  pages        = {2075-2094},
  shortjournal = {Comput. Intell.},
  title        = {Sparse neural network regression with variable selection},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep learning framework for time series classification
using normal cloud representation and convolutional neural network
optimization. <em>COIN</em>, <em>38</em>(6), 2056–2074. (<a
href="https://doi.org/10.1111/coin.12556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time Series Classification (TSC) started getting a lot of attention recently, mostly due to the important real-world applications of time series such as in the financial industry. In this context, various approaches have been proposed to treat this important and challenging problem in data mining. These approaches include deep learning models that are outperformed the traditional classification techniques. However, the choice of an optimal and efficient deep neural network for TSC is still a major problem. Motivated by this challenge, we propose in this paper, an optimized deep learning framework for TSC tasks, using normal cloud representation and convolutional neural networks optimization (NCR-CNNO). Our approach consists of two phases: In the first one, we convert the raw time series to a matrix of characteristics using two-dimensional normal cloud representation. In the second phase, we suggest a CNN to handle the matrix generated in the first phase, as well as, an optimization model is proposed to optimize the unnecessary and redundant parameters. Experiments conducted on extensive time series datasets demonstrate that NCR-CNNO yields significant improvement in the performance of time series classification compared to the state of the art.},
  archive      = {J_COIN},
  author       = {El houssaine Hssayni and Nour-Eddine Joudar and Mohamed Ettaouil},
  doi          = {10.1111/coin.12556},
  journal      = {Computational Intelligence},
  month        = {12},
  number       = {6},
  pages        = {2056-2074},
  shortjournal = {Comput. Intell.},
  title        = {A deep learning framework for time series classification using normal cloud representation and convolutional neural network optimization},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient credit card fraud detection approach using
cost-sensitive weak learner with imbalanced dataset. <em>COIN</em>,
<em>38</em>(6), 2035–2055. (<a
href="https://doi.org/10.1111/coin.12555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The usage of credit cards is increasing in the digitization era to purchase goods online or offline, and the number of fraudulent credit card transactions has also increased. Fraudulent transaction detection is a big issue in the credit card domain because the data set is highly imbalanced. The performance of traditional classification techniques has decreased due to this issue. Therefore, a dynamic fraud detection system is required to overcome this issue. This article proposes an efficient cost-sensitive weak learner approach with a bagging and random forest classifier (CSWLB) to minimize misclassification problems and overcome the class imbalance issue. The proposed CSWLB approach is included in an adaptive algorithmic method (cost-sensitive) with two weak learner ensembles (bagging-random forest), which assigns high weight to the fraudulent transactions using the cost-sensitive learning classifier and generates weight bags using a bagging ensemble classifier. Therefore, the random forest classifier has been implemented on the weight bags to improve classification accuracy. The proposed approach&#39;s effectiveness and efficiency have been computed on the Brazilian credit card data set. Its results have been compared with sampling, cost-sensitive, and machine learning techniques. The proposed CSWLB approach obtained 765 total costs and 97.361% accuracy. The experimental results show that the proposed approach has outperformed compared to other techniques.},
  archive      = {J_COIN},
  author       = {Ajeet Singh and Anurag Jain},
  doi          = {10.1111/coin.12555},
  journal      = {Computational Intelligence},
  month        = {12},
  number       = {6},
  pages        = {2035-2055},
  shortjournal = {Comput. Intell.},
  title        = {An efficient credit card fraud detection approach using cost-sensitive weak learner with imbalanced dataset},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intelligent short term traffic forecasting using deep
learning models with bayesian contextual hyperband tuning.
<em>COIN</em>, <em>38</em>(6), 2009–2034. (<a
href="https://doi.org/10.1111/coin.12554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An intelligent transport system (ITS) is fully valuable only if it can dynamically and aptly integrate all the latest cutting-edge technologies. An ITS focuses on providing services like promptly offering real-time road traffic information to interested parties, finding ways to reduce the average waiting time and offer secure and reliable services for commuters using past statistics. Short-term traffic prediction is one such area in which the research community has focused in the past decade. Existing models developed for prediction has scope to improve in terms of accuracy and training time. There is a necessity to develop a best-performing model that is computationally affluent to train with the optimal hyperparameter configuration as input which leads to improved performance. This article proposes a model that captures the traffic flow trend present in the past data to predict the flow for a future time interval. This model is an amalgamation of seasonal global trend (SGT) model and long short-term memory (LSTM) model with attention mechanism. A novel hyperparameter tuning algorithm is also proposed which is based on multi-armed bandit strategy with context, incorporating the right trade-off between exploitation and exploration of the hyperparameter space using successive halving. Experimental results conducted proves that our forecast model in combination with the proposed hyperparameter tuning algorithm outperforms the existing models like SGT, LSTM models in terms of accuracy and time.},
  archive      = {J_COIN},
  author       = {Lakshmi Priya Swaminatha Rao and Suresh Jaganathan},
  doi          = {10.1111/coin.12554},
  journal      = {Computational Intelligence},
  month        = {12},
  number       = {6},
  pages        = {2009-2034},
  shortjournal = {Comput. Intell.},
  title        = {Intelligent short term traffic forecasting using deep learning models with bayesian contextual hyperband tuning},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Systematic literature review on predictive maintenance of
vehicles and diagnosis of vehicle’s health using machine learning
techniques. <em>COIN</em>, <em>38</em>(6), 1990–2008. (<a
href="https://doi.org/10.1111/coin.12553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many industries, inclusive of the automobile industry, have shifted their attention toward predictive maintenance. The automobile industry finds predictive maintenance a key player in improving the servicing and vehicles they deliver. It is also vital for the vehicle owners to diagnose the vehicles to prevent risks the vehicle may face by timely servicing. Even though, with so many benefits of predictive maintenance, it is challenging to detect a breakdown in advance in the automobile sector. This is due to the restricted accessibility to sensors and the unavailability of some design applications. However, with the continuous advances in technology, machine learning (ML) methods have come forward as a viable solution to analyze data and develop solutions even when the data is scarce. This article intends to provide the literature review of ML techniques used for predictive maintenance of automobiles and diagnosis of the vehicle&#39;s health using ML. This review focuses on machine learning techniques in practice, extraction of data from the onboard diagnosis system and difficulties that models face. The article also explores the possibility and scope of positive work efforts in this area.},
  archive      = {J_COIN},
  author       = {Muskan Jain and Dipit Vasdev and Kunal Pal and Vishal Sharma},
  doi          = {10.1111/coin.12553},
  journal      = {Computational Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1990-2008},
  shortjournal = {Comput. Intell.},
  title        = {Systematic literature review on predictive maintenance of vehicles and diagnosis of vehicle&#39;s health using machine learning techniques},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RMCL: A deep learning based recursive malicious context
learner in social networks. <em>COIN</em>, <em>38</em>(6), 1956–1989.
(<a href="https://doi.org/10.1111/coin.12552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Propagation of malicious content and its promotion is a recurrent problem prominent in social networks. Detection and interactive labeling of malicious context promotion on-the-fly is a very challenging and difficult task due to the lack of complete knowledge about the underlying context. Modern research in this field use neural networks and text encoders to analyze the information dynamically which is inefficient, in terms of time-space consumed in the overall process. This article proposes an online active learning system to label the data streams on-the-fly by sampling them in the form of tweet-retweet-follow (trf) sequences in social networks. A heuristically pretrained recursive malicious context learner is fixed for knowledge acquisition, data accommodation and pseudonymization in the form of a transformer in the tree structured recursive neural network. The data stream is trained using recursive bidirectional training to capture long-term dependencies. The COVID-19 data streams are selectively sampled on-the-fly using psycho linguistic words in the proposed experiment and are labeled deceptive/nondeceptive based on knowledge learned during pretraining. The proposed recursive malicious context learner successfully resolves the problem of on-the-fly-interactive labeling of a dynamically changing data stream.},
  archive      = {J_COIN},
  author       = {Devisha Arunadevi Tiwari},
  doi          = {10.1111/coin.12552},
  journal      = {Computational Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1956-1989},
  shortjournal = {Comput. Intell.},
  title        = {RMCL: A deep learning based recursive malicious context learner in social networks},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GAN-c: A generative adversarial network with a classifier
for effective event prediction. <em>COIN</em>, <em>38</em>(6),
1922–1955. (<a href="https://doi.org/10.1111/coin.12550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event prediction is essential in social network (SN) analysis to study the SN&#39;s evolutionary patterns (communities). Machine learning (ML) models are often used to predict events in SN communities. The ML algorithms manifest the results with biasness for the same dataset. Hence, the performance of an ML model requires validation for the unseen data to avoid biasness in learning. Generally, researchers use the generative adversarial network (GAN) model for generating realistic sample data to enhance the prediction of events. It is challenging for the discriminator to learn features using a single layer with similar weights in a conventional GAN technique. Therefore, this article proposes an improved version of the GAN model named generative adversarial network classifier (GAN-C). The proposed GAN-C model contains an additional layer, called classifier , that generates different feature maps. Wherein weights are adjusted dynamically based on the conditions to predict the events. To be precise, conditioning the weights in a classifier layer using entropy values of the features is a simple and effective way to minimize the classifier loss function (categorical cross-entropy). GAN-C model results in 16% loss up to 10 batches, and after that, the loss becomes negligible and can generate non-overlapping events. The gaps between such non-overlapping events are analyzed using the Jensen–Shannon divergence technique. The experimental results show that the existing single-GAN and multi-GAN methods predict events with 79.34% and 82.17% accuracy, respectively. While the proposed GAN-C comparatively predicts events with improved accuracy of 88.56% on the same dataset. The data generated by the GAN-C model are also approximately 55.85% and 80.59% more realistic than multi-GAN and single-GAN, respectively, based on root mean square error and inception score comparisons. GAN-C is also approximately 68.01% faster than other GAN models. Thus, this article&#39;s theoretical and experimental analysis justifies that GAN-C works suitably for predicting events in a massive dataset.},
  archive      = {J_COIN},
  author       = {B. S. A. S. Rajita and Vrutik Halani and Dhruvil Shah and Subhrakanta Panda},
  doi          = {10.1111/coin.12550},
  journal      = {Computational Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1922-1955},
  shortjournal = {Comput. Intell.},
  title        = {GAN-C: A generative adversarial network with a classifier for effective event prediction},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cluster-based improvement rates for trust establishment
models in single or distributed multi-agent systems. <em>COIN</em>,
<em>38</em>(5), 1884–1919. (<a
href="https://doi.org/10.1111/coin.12546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent agents within open and dynamic multi-agent systems are becoming increasingly capable in their decision-making abilities and rely upon the notion of trustworthiness to determine which agents to interact with. To improve the overall performance of trust establishment models which trustees individually select and equip to improve their trustworthiness with trustors, while balancing the resources being spent, a cluster-based trust establishment model update mechanism is proposed. This cluster-based approach is applicable to robust trust establishment models which utilize dynamic improvement and disimprovement rate variables to adjust a trustee&#39;s behaviors toward trustors to improve or maintain trust with the trustor. By storing a single trust establishment model&#39;s dynamic improvement and disimprovement rate variables independently for each trustor and by clustering similar trustors together based on observed experiences, a model can more accurately update a trustee&#39;s behaviors toward trustors. Through simulated experiments comparing the performance of the existing integrated trust establishment (ITE) model with and without the cluster-based approach, with varying trustor to trustee ratios to diversify the agent behaviors, the cluster-based approach consistently improves a trustee&#39;s ability to fully meet a trustor&#39;s needs, for less resources than ITE, while minimizing the corresponding impact to the trustee&#39;s overall trust.},
  archive      = {J_COIN},
  author       = {Julian Templeton and Thomas Tran},
  doi          = {10.1111/coin.12546},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1884-1919},
  shortjournal = {Comput. Intell.},
  title        = {Cluster-based improvement rates for trust establishment models in single or distributed multi-agent systems},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recommendation system using a deep learning and graph
analysis approach. <em>COIN</em>, <em>38</em>(5), 1859–1883. (<a
href="https://doi.org/10.1111/coin.12545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a user connects to the Internet to fulfill his needs, he often encounters a huge amount of related information. Recommender systems are the techniques for massively filtering information and offering the items that users find them satisfying and interesting. The advances in machine learning methods, especially deep learning, have led to great achievements in recommender systems, although these systems still suffer from challenges such as cold-start and sparsity problems. To solve these problems, context information such as user communication network is usually used. In this article, we have proposed a novel recommendation method based on matrix factorization and graph analysis methods, namely Louvain for community detection and HITS for finding the most important node within the trust network. In addition, we leverage deep autoencoders to initialize users and items latent factors, and the Node2vec deep embedding method gathers users&#39; latent factors from the user trust graph. The proposed method is implemented on Ciao and Epinions standard datasets. The experimental results and comparisons demonstrate that the proposed approach is superior to the existing state-of-the-art recommendation methods. Our approach outperforms other comparative methods and achieves great improvements, that is, 15.56% RMSE improvement for Epinions and 18.41% RMSE improvement for Ciao.},
  archive      = {J_COIN},
  author       = {Mahdi Kherad and Amir Jalaly Bidgoly},
  doi          = {10.1111/coin.12545},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1859-1883},
  shortjournal = {Comput. Intell.},
  title        = {Recommendation system using a deep learning and graph analysis approach},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An effective context-focused hierarchical mechanism for
task-oriented dialogue response generation. <em>COIN</em>,
<em>38</em>(5), 1831–1858. (<a
href="https://doi.org/10.1111/coin.12544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task-oriented dialogue system (TOD) is one kind of application of artificial intelligence (AI). The response generation module is a key component of TOD for replying to user&#39;s questions and concerns in sequential natural words. In the past few years, the works on response generation have attracted increasing research attention and have seen much progress. However, existing works ignore the fact that not each turn of dialogue history contributes to the dialogue response generation and give little consideration to the different weights of utterances in a dialogue history. In this article, we propose a hierarchical memory network mechanism with two steps to filter out unnecessary information of dialogue history. First, an utterance-level memory network distributes various weights to each utterance (coarse-grained). Second, a token-level memory network assigns higher weights to keywords based on the former&#39;s output (fine-grained). Furthermore, the output of the token-level memory network will be employed to query the knowledge base (KB) to capture the dialogue-related information. In the decoding stage, we take a gated-mechanism to generate response word by word from dialogue history, vocabulary, or KB. Experiments show that the proposed model achieves superior results compared with state-of-the-art models on several public datasets. Further analysis demonstrates the effectiveness of the proposed method and the robustness of the model in the case of an incomplete training set.},
  archive      = {J_COIN},
  author       = {Meng Zhao and Zejun Jiang and Lifang Wang and Ronghan Li and Xinyu Lu and Zhongtian Hu and Daqing Chen},
  doi          = {10.1111/coin.12544},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1831-1858},
  shortjournal = {Comput. Intell.},
  title        = {An effective context-focused hierarchical mechanism for task-oriented dialogue response generation},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Malware classification using word embeddings algorithms and
long-short term memory networks. <em>COIN</em>, <em>38</em>(5),
1802–1830. (<a href="https://doi.org/10.1111/coin.12543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of malicious software applications, or malware programs, increases every year. Their development becomes more sophisticated as new techniques are used to bypass program scanning software applications, such as antiviruses. Thereby, deep learning-based methods emerge as a new promising way to identify these threats. Our main purpose and contribution in this work is proposing and implementing a successful approach to tackle both binary and multiclass malware classification problems. We used unsupervised word embedding algorithms for representing software applications to be analyzed and long-short term memory for classifying the software applications. For evaluating our pipeline, we introduce a new dataset for binary and multiclass malware classification because we could not find large datasets containing sufficient samples of cleanware and the various malware types for multiclass classification that could be used to evaluate classification models. Our experimental results reached an accuracy of 88.94% for binary classification and 75.13% for multiclass classification. These results suggest that the proposed dataset is challenging, and using it can help in the training of better malware classifiers, improving security.},
  archive      = {J_COIN},
  author       = {Eduardo de O. Andrade and José Viterbo and Joris Guérin and Flavia Bernardini},
  doi          = {10.1111/coin.12543},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1802-1830},
  shortjournal = {Comput. Intell.},
  title        = {Malware classification using word embeddings algorithms and long-short term memory networks},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Partially latent factors based multi-view subspace learning.
<em>COIN</em>, <em>38</em>(5), 1772–1801. (<a
href="https://doi.org/10.1111/coin.12540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view subspace clustering always performs well in high-dimensional data analysis, but is sensitive to the quality of data representation. To this end, a two-stage fusion strategy is proposed to embed representation learning into the process of multi-view subspace clustering. This article first proposes a novel matrix factorization method that can separate the coupling consistent and complementary information from observations of multiple views. Based on the obtained latent representations, we further propose two subspace clustering strategies: feature-level fusion and subspace-level hierarchical strategy. The feature-level method concatenates all kinds of latent representations from multiple views, and the original problem therefore degenerates to a single-view subspace clustering process. The subspace-level hierarchical method performs different self-expressive reconstruction processes on the corresponding complementary and consistent latent representations coming from each view, that is, the prior constraints imposed on different types of subspace representations are related to the relevant input factors. Finally, extensive experimental results on real-world datasets demonstrate the superiority of our proposed methods by comparing them against some state-of-the-art subspace clustering algorithms.},
  archive      = {J_COIN},
  author       = {Run-kun Lu and Jian-Wei Liu and Ze-Yu Liu and Jinzhong Chen},
  doi          = {10.1111/coin.12540},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1772-1801},
  shortjournal = {Comput. Intell.},
  title        = {Partially latent factors based multi-view subspace learning},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A two-stage deep learning framework for image-based android
malware detection and variant classification. <em>COIN</em>,
<em>38</em>(5), 1748–1771. (<a
href="https://doi.org/10.1111/coin.12532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of the internet and smartphones, malware on smartphones has increased dramatically. In addition, the ubiquity and openness of the Android operating system have made it a lucrative platform for cybercriminals to develop malware. Traditional malware detection techniques require a lot of time and manual effort to classify malware accurately. Recently, deep learning (DL) based malware detection and classification techniques have been developed to solve this issue. This article proposes a DL-based two-stage framework that detects Android malware and classifies its variants using image-based malware representations of the Android DEX files. The framework uses the EfficientNetB0 convolutional neural network (CNN) to extracts relevant features from the malware color images. The extracted features are then passed through a global average pooling layer and fed into a stacking classifier. The stacking classifier employs linear support vector machine (SVM) and random forest (RF) algorithms as base-level classifiers and logistic regression as the meta-level classifier. This method obtained an accuracy of 100% in the binary classification of Android malware images and a 92.9% accuracy in 5-class (Adsware, Adware + Adware, Clicker + Trojan, Spyware, and Benign) classification, and an 88.6% accuracy in 4-class (Adsware, Adware + Adware, Clicker + Trojan, and Spyware) classification. We compared our method with 26 state-of-the-art pretrained CNN models (including the original EfficientNetB0) and large-scale learning classifiers such as EfficientNetB0-SVM and EfficientNetB0-RF. The proposed framework outperformed the compared methods in all performance metrics. Experiments also demonstrate that substituting the softmax layer of CNNs with a large-scale learning classifier or stacking classifier results in an enhanced performance over the original network.},
  archive      = {J_COIN},
  author       = {Pooja Yadav and Neeraj Menon and Vinayakumar Ravi and Sowmya Vishvanathan and Tuan D. Pham},
  doi          = {10.1111/coin.12532},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1748-1771},
  shortjournal = {Comput. Intell.},
  title        = {A two-stage deep learning framework for image-based android malware detection and variant classification},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel lung nodule accurate detection of computerized
tomography images based on convolutional neural network and probability
graph model. <em>COIN</em>, <em>38</em>(5), 1728–1747. (<a
href="https://doi.org/10.1111/coin.12531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precisely detecting lung nodules from original computerized tomography (CT) images is a critical technology in the earlier screening of lung cancer. Therefore, the domain of accurate detection has gradually attracted the attention of researchers. However, due to the complex characteristics of pulmonary nodules and the limitations of CT imaging property, detecting nodules with high accuracy from lung CT images is a challenging task. This article proposes an effective and robust detection network to accurately detect lung nodules by innovatively implementing a probability graph model in the candidate detection and false-positive reduction phase. Different from previous works which use complex 3-dimensional image information to reduce false positives, we propose two effective probability graph mechanisms, which analyze multiscale information and continuous slices (interslice changes) motion information to improve performance. We evaluated our method on an open-source LIDC-IDRI dataset which contains a total of 243,958 CT images and achieved high-precision lung nodule detection results (sensitivity score of 0.945). Via introducing multiscale information and the dynamic information of the interslice, the task of lung nodule detection obtains higher precision detection results than other similar methods.},
  archive      = {J_COIN},
  author       = {Xunpeng Xia and Rongfu Zhang and Xufeng Yao and Gang Huang and Tiequn Tang},
  doi          = {10.1111/coin.12531},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1728-1747},
  shortjournal = {Comput. Intell.},
  title        = {A novel lung nodule accurate detection of computerized tomography images based on convolutional neural network and probability graph model},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Meta-feature based few-shot siamese learning for urdu
optical character recognition. <em>COIN</em>, <em>38</em>(5), 1707–1727.
(<a href="https://doi.org/10.1111/coin.12530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard convolution neural network (CNN) achieves high level of accuracy for the recognition of characters in different languages. However, like other deep neural networks, training of CNN requires a substantial amount of data. Lack of sufficient training data invokes dataset bias, during learning process, which leads to a decay in the performance of CNN. The limitation of training data can be addressed by using few-shot learners. In this research, CNN-based few-shot Siamese learner is trained on meta-features, extracted from Urdu text images using a novel graph-based normal to tangent line (GNTL) technique, for Urdu optical character recognition (OCR) across different font sizes. The learner is trained on three corpora (datasets) including one benchmark corpus “Centre for Language Engineering Text Images” and two other corpora, that is, “Urdu Thickness Graphs” (UTG) and “Urdu OCR Font 16 to 36” (UOF) which are developed and released in this research. 80% of data is used for training while 20% of data is used for testing. To create UTG corpus, the proposed novel feature extraction technique GNTL is used and a meta-features-based corpus is developed in form of thickness graphs. The third corpus UOF is based on five different font sizes, that is, 16, 20, 26, 30, and 36. The performance of few-shot Siamese learner is compared with a standard CNN, trained on the same three corpora. Meta-feature based few-shot Siamese learner achieves a promising recognition accuracy and outperforms standard CNN by around 3%. On average, the performance of few-shot Siamese learner is 96.82% while standard CNN reveals an average performance of 93.96%.},
  archive      = {J_COIN},
  author       = {Asma Naseer and Kashif Zafar},
  doi          = {10.1111/coin.12530},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1707-1727},
  shortjournal = {Comput. Intell.},
  title        = {Meta-feature based few-shot siamese learning for urdu optical character recognition},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detection of pulmonary hypertension with six training
strategies based on deep learning technology. <em>COIN</em>,
<em>38</em>(5), 1684–1706. (<a
href="https://doi.org/10.1111/coin.12527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pulmonary hypertension (PH) is a progressive condition with high mortality. At present, the most accurate diagnosis method is invasive. However, noninvasive methods existed are inaccurate, and could not be used for continuous monitoring. The heart sound (HS) signals allow the earliest monitoring to diagnose a possible cardiovascular pathology. It has become an important tool for evaluating hemodynamic status of heart in outpatient monitoring. The primary aim of this study is to detect the PH disease using the HS signals. The authors develop and test a wavelet scattering convolution network (WSCN) based on long short term memory (LSTM) approach to detect the PH by using the HS signals. We conduct validation using 131 subjects through five cross-validations. Six methods are used to test the different training strategies and models. We have validated our experiment based on subject dependent, subject independent, subject without segmentation, classification with feature extraction, classification without feature extraction, and classification by CNN and ensemble methods, respectively. Our best general model has a classification accuracy of 94.40%, which outperforms the previous best performance in terms of PH in the literature. The capability of the method is high as it is assessed by the five evaluation indicators, such as the accuracy, specificity, precision, sensitivity, and F1 score. Moreover, establishing the diagnosis with the WSCN+LSTM approach is less time cost. The WSCN based on the LSTM approach is developed to detect PH disease. The system for PH classification, allows for noninvasive assessment of a heart condition in suspect patients without many cost and without risk of invasive detection.},
  archive      = {J_COIN},
  author       = {Miao Wang and JiWen Wang and YaTing Hu and BinBin Guo and Hong Tang},
  doi          = {10.1111/coin.12527},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1684-1706},
  shortjournal = {Comput. Intell.},
  title        = {Detection of pulmonary hypertension with six training strategies based on deep learning technology},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep learning-based approach for diagnosing COVID-19 on
chest x-ray images, and a test study with clinical experts.
<em>COIN</em>, <em>38</em>(5), 1659–1683. (<a
href="https://doi.org/10.1111/coin.12526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pneumonia is among the common symptoms of the virus that causes COVID-19, which has turned into a worldwide pandemic. It is possible to diagnose pneumonia by examining chest radiographs. Chest x-ray (CXR) is a fast, low-cost, and practical method widely used in this field. The fact that different pathogens other than COVID-19 also cause pneumonia and the radiographic images of all are similar make it difficult to detect the source of the disease. In this study, automatic detection of COVID-19 cases over CXR images was tried to be performed using convolutional neural network (CNN), a deep learning technique. Classifications were carried out using six different architectures on the dataset consisting of 15,153 images of three different types: healthy, COVID-19, and other viral-induced pneumonia. In the classifications performed with five different state-of-art models, ResNet18, GoogLeNet, AlexNet, VGG16, and DenseNet161, and a minimal CNN architecture specific to this study, the most successful result was obtained with the ResNet18 architecture as 99.25% accuracy. Although the minimal CNN model developed for this study has a simpler structure, it was observed that it has a success to compete with more complex models. The performances of the models used in this study were compared with similar studies in the literature and it was revealed that they generally achieved higher success. The model with the highest success was transformed into a test application, tested by 10 volunteer clinicians, and it was concluded that it provides 99.06% accuracy in practical use. This result reveals that the conducted study can play the role of a successful decision support system for experts.},
  archive      = {J_COIN},
  author       = {Onur Sevli},
  doi          = {10.1111/coin.12526},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1659-1683},
  shortjournal = {Comput. Intell.},
  title        = {A deep learning-based approach for diagnosing COVID-19 on chest x-ray images, and a test study with clinical experts},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robust ensemble feature selection technique for
high-dimensional datasets based on minimum weight threshold method.
<em>COIN</em>, <em>38</em>(5), 1616–1658. (<a
href="https://doi.org/10.1111/coin.12524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble feature selection (EFS) is a valuable technique for developing accurate and robust machine-learning (ML) models. Data variation plays a crucial role in the success of EFS models; however, it also causes some outliers in the ranked lists. In this study, we proposed the minimum weight threshold method-based EFS (MWT-EFS) to address the outlier problem and use the true power of EFS. The proposed method employs the support vector classifier to assign weights for features, and the MWT method handles outliers in the ranked feature lists while creating the ensemble list. First, a threshold value is determined. After that, the feature weights below the threshold are replaced with this value. This approach eliminates the negative effect of outliers. After the new feature weights are assigned, the average of the feature weights is calculated (mean aggregation) for all features, and the ensemble (final) feature list is created accordingly. The experiment results showed that the proposed method significantly improves gene selection stability while maintaining classification performance and reducing computational complexity. In conclusion, the proposed method led to an accurate and robust classification that can help domain experts to make more confident decisions with less effort, resources, and time.},
  archive      = {J_COIN},
  author       = {Huseyin Guney and Huseyin Oztoprak},
  doi          = {10.1111/coin.12524},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1616-1658},
  shortjournal = {Comput. Intell.},
  title        = {A robust ensemble feature selection technique for high-dimensional datasets based on minimum weight threshold method},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Retracted: High utility itemset mining using genetic
algorithm assimilated with off policy reinforcement learning to
adaptively calibrate crossover operation. <em>COIN</em>, <em>38</em>(5),
1596–1615. (<a href="https://doi.org/10.1111/coin.12490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining high utility itemsets (HUI) is a current thrust field in data mining that has received numerous methodologies for addressing it effectively. The difficulty with HUI is to locate a number of items that have a high degree of utility in comparison to other different sets in a transaction database. Traditional accurate HUIM algorithms usually have to solve the exponential problem of big search spaces when the size or number of different items in the database is quite vast. Evolutionary computation (EC)-based algorithms have been offered as an alternate and successful technique to solving HUIM issues since they may generate a collection of approximately optimum solutions in a short amount of time. Many genetic algorithm (GA)-based approaches have been developed in recent years to efficiently mine HUI from transaction databases. The selection technique, crossover probability, mutation probability, and finishing criteria of the genetic algorithm have an greater impact on generating a reasonably decent solution and the processing time. Particularly crossover is a convergence operation which is intended to pull the population toward a local minimum/maximum. During HUIM using GA both low and high crossover rate will have the problem of decreasing the quality of intermediate itemset and take longer time to converge to some optima and vice versa. This problem can be solved by adjusting the crossover rate adaptively depending on environmental inputs. The proposed approach describe a hybrid system that employs a reinforcement learning (RL) agent to adaptively calibrate the crossover operation to increase the performance of a genetic algorithm. To estimate state-action utility values, the RL agent employs Q-learning, which it then employs to execute high-level adaptive control over the crossover operation in the genetic algorithm. To evaluate the performance of the proposed methodology, extensive experiments were conducted on a four benchmark datasets and compared with three state-of-art EC approaches HUPE UMU -GRAM, Bio-HUIF-GA, HUIM-BPSO, and one exact approach HUP-Miner . The result analysis witnessed that proposed approach outperforms EC approaches in terms of execution time, discovered HUIs and convergence.},
  archive      = {J_COIN},
  author       = {K Logeswaran and P Suresh},
  doi          = {10.1111/coin.12490},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1596-1615},
  shortjournal = {Comput. Intell.},
  title        = {Retracted: High utility itemset mining using genetic algorithm assimilated with off policy reinforcement learning to adaptively calibrate crossover operation},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid optimization algorithm based feature selection for
mammogram images and detecting the breast mass using multilayer
perceptron classifier. <em>COIN</em>, <em>38</em>(4), 1559–1593. (<a
href="https://doi.org/10.1111/coin.12522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is the second most frequent malignant tumor in the world. Early findings of breast cancer can significantly improve treatment effectiveness. Manual methods of breast cancer diagnosis are prone to human fault and inaccuracy, and they take time. A computer-aided diagnosis can assist radiologists in making better choices by overcoming the disadvantages of manual methods. One of the significant steps in the breast cancer diagnosis process is feature selection. In recent decades, many studies have proposed numerous hybrid optimization methods to select the optimal features in the breast cancer detection system. However, many hybrid optimization algorithms are trapped in local optima and have slow convergence speed. Thus, it reduces the classification accuracy. For resolving these issues, this work proposes a hybrid optimization algorithm that combines the grasshopper optimization algorithm and the crow search algorithm for feature selection and classification of the breast mass with multilayer perceptron. The simulation is experimented with using MATLAB 2019a. The efficacy of the proposed hybrid grasshopper optimization-crow search algorithm with multilayer perceptron system is compared to multilayer perceptron based algorithms of enhanced and adaptive genetic algorithm, teaching learning-based whale optimization algorithm, butterfly optimization algorithm, whale optimization algorithm, and grasshopper optimization algorithm. From the results obtained, the proposed grasshopper optimization-crow search algorithm with the multilayer perceptron method outperforms the comparative models in terms of classification accuracy (97.1%), sensitivity (98%), and specificity (95.4%) for the mammographic image analysis society dataset.},
  archive      = {J_COIN},
  author       = {Reenadevi Rajendran and Sathiyabhama Balasubramaniam and Vinayakumar Ravi and Sankar Sennan},
  doi          = {10.1111/coin.12522},
  journal      = {Computational Intelligence},
  month        = {8},
  number       = {4},
  pages        = {1559-1593},
  shortjournal = {Comput. Intell.},
  title        = {Hybrid optimization algorithm based feature selection for mammogram images and detecting the breast mass using multilayer perceptron classifier},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Malware classification using a byte-granularity feature
based on structural entropy. <em>COIN</em>, <em>38</em>(4), 1536–1558.
(<a href="https://doi.org/10.1111/coin.12521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapidly evolving malware has become a major cybersecurity threat. Several feature-engineering techniques have been proposed to defend against malware attacks. An entropy is a typical indicator used in identifying malware. Structural entropy is a sequence of entropy values where an entropy of a segment is calculated by the equation of the entropy itself. However, entropy-based features are likely to be abstract and miss important information. This article proposes a feature engineering technique that involves the concept of structural entropy. This technique allows every segment to be represented as 256 entropy values for every byte value, but not as an entropy value. Our research, fine-granularity structural entropy (FiG_SE), incorporates global patterns across all segments, local patterns across adjacent segments, and internal patterns within the segments. To extract higher-level characteristics from our entropy feature, we use a convolutional neural network (CNN) architecture because it is effective for extracting local and global patterns, and especially for shift-invariant patterns. Our malware classification based on CNN with the proposed feature outperforms the previous classification methods that use byte streams, entropy streams, and structural-entropy-based streams as inputs. Moreover, our research combined with CNN is highly resilient to obfuscation techniques and is also well suited to malware detection.},
  archive      = {J_COIN},
  author       = {Joon-Young Paik and Rize Jin and Eun-Sun Cho},
  doi          = {10.1111/coin.12521},
  journal      = {Computational Intelligence},
  month        = {8},
  number       = {4},
  pages        = {1536-1558},
  shortjournal = {Comput. Intell.},
  title        = {Malware classification using a byte-granularity feature based on structural entropy},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey of active and passive concept drift handling
methods. <em>COIN</em>, <em>38</em>(4), 1492–1535. (<a
href="https://doi.org/10.1111/coin.12520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, concept drift in the nonstationary data stream is showing trends with different speeds and different degrees of severity, which has brought great challenges to many fields like data mining and machine learning. In the past two decades, a lot of methods dedicated to handling concept drift in the nonstationary data stream have emerged. A novel perspective is proposed to classify these methods, and the current concept drift handling methods are comprehensively explained from the active handling methods and the passive handling methods. In particular, active handling methods are analyzed from the perspective of handling one specific type of concept drift and handling multiple types of concept drift, and passive handling methods are analyzed from the perspective of single learner and ensemble learning. Many concept drift handling methods in this survey are analyzed and summarized in terms of the comparing algorithms, learning model, applicable drift type, advantages, and disadvantages of the algorithms. Finally, further research directions are given, including the active and passive mixing methods, class imbalance, the existence of novel class in the data stream, and the noise in the data stream.},
  archive      = {J_COIN},
  author       = {Meng Han and Zhiqiang Chen and Muhang Li and Hongxin Wu and Xilong Zhang},
  doi          = {10.1111/coin.12520},
  journal      = {Computational Intelligence},
  month        = {8},
  number       = {4},
  pages        = {1492-1535},
  shortjournal = {Comput. Intell.},
  title        = {A survey of active and passive concept drift handling methods},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stock price forecasting based on improved time convolution
network. <em>COIN</em>, <em>38</em>(4), 1474–1491. (<a
href="https://doi.org/10.1111/coin.12519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock forecasting is difficult because of its complexity and uncertainty. To better predict stock prices and then provide stockholders with reasonable suggestions, this paper proposes an improved time convolution network (TCN) model for predicting stock prices. The model used can make up for some of the shortcomings of the traditional neural network, use the trading data in the stock market, and put the preprocessed data of financial news into the model for training to improve the accuracy of prediction. Using the Shanghai Securities Exchange (SSE) 50 Index (Shanghai Securities Exchange 50 Index selects the most representative 50 stocks with large scale and good liquidity in Shanghai stock market as sample stocks) and news text crawled from financial web pages as samples, predict the direction of the SSE 50 Index&#39;s rise and fall. After using different network structure hyperparameters to adjust the model structure, the prediction effect is compared with other models, and it is found that the proposed improved TCN model can effectively improve the effect of predicting the rise and fall of the SSE 50 index, and can complete the model training and predict the stock price faster.},
  archive      = {J_COIN},
  author       = {Wenchao Guo and Zhigang Li and Chuang Gao and Ying Yang},
  doi          = {10.1111/coin.12519},
  journal      = {Computational Intelligence},
  month        = {8},
  number       = {4},
  pages        = {1474-1491},
  shortjournal = {Comput. Intell.},
  title        = {Stock price forecasting based on improved time convolution network},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computer-aided diagnosis of cataract severity using retinal
fundus images and deep learning. <em>COIN</em>, <em>38</em>(4),
1450–1473. (<a href="https://doi.org/10.1111/coin.12518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cataracts are lenticular opacities that can appear in different parts of the lens in the eye and are a leading cause of blindness globally. Accurate detection and early-stage diagnosis can prevent the cataract and improve the quality of life for cataract patients. However, clinical cataract detection and grading require the expertise of trained eye specialists, which may impede everyone&#39;s early intervention due to the underlying expenses. This article proposed a computer aid diagnosis method for cataract detection, which also grades the severity of cataracts from fundus retinal images such as normal, mild, moderate, and severe. The proposed method uses a hybrid approach in which various pre-trained convolutional neural networks (AlexNet, VGGNet, ResNet) with transfer learning are used to extract features. These feature vectors of each network individually and in the fused form are applied on the support vector machine classifiers for 4-stage cataract classification. This architecture also takes advantage of ensemble learning by applying a majority voting scheme on the predictions of these SVM classifiers. The fundus cataract images are obtained from several open-access datasets and arranged into 4-classes with the assistance of an eye specialist. Since all the collected images are not suitable for diagnosis, an image quality selection module is included with this method to determine the quality of fundus images. The proposed method achieved 96.25% 4-class classification accuracy. According to the experimental results, the proposed method is effective for cataract classification and outperforms conventional methods.},
  archive      = {J_COIN},
  author       = {Jay Kant Pratap Singh Yadav and Sunita Yadav},
  doi          = {10.1111/coin.12518},
  journal      = {Computational Intelligence},
  month        = {8},
  number       = {4},
  pages        = {1450-1473},
  shortjournal = {Comput. Intell.},
  title        = {Computer-aided diagnosis of cataract severity using retinal fundus images and deep learning},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Investigating classification performance of hybrid deep
learning and machine learning architectures on activity recognition.
<em>COIN</em>, <em>38</em>(4), 1402–1449. (<a
href="https://doi.org/10.1111/coin.12517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition (HAR) has become a popular field to recognize people&#39;s activities from signals obtained using various types of body placed sensors. The increase in the elderly population will increase cognitive and physical decline due to aging and may cause to serious injuries and deaths if immediate assistance is not provided. For this reason, temporal dynamics and important features should automatically be extracted in order to support the daily life of the elderly and to recognize their physical activities correctly and real time. In the study, the classification performances of deep models (CNNLSTM: CNN long short-term memory network, ConvLSTM: convolutional LSTM, LSTM: long short-term memory network) and four other machine learning algorithms (SMVs: support vector machines, k-NN: k -nearest neighbor classifier, DT: decision tree classifier, ERF: ensemble random forest) which are known to be successful in HAR were investigated. Features used features used in deep models are automatically generated however in machine learning models were generated by hand. The deep models are developed using with a huge set of activities containing 2520 tests. In the tests, each activity of the volunteers was recorded with three axis accelerometer, gyroscope and magnetometer sensors placed in the waist region of body. As a result, ConvLSTM reached the highest accuracy with 99.86% in deep models, while SVMs achieved the highest accuracy in fall detection among machine learning algorithms with 98.47%. When the classification of 36 activities was examined, the highest accuracy was obtained with CNNLSTM and SVMs with 86.94% and 74.58%, respectively. Deep models proposed in this study are considered to be more applicable in real-world HAR scenarios where sensors&#39; data of indefinite length are obtained.},
  archive      = {J_COIN},
  author       = {Esma Uzunhisarcıklı and Erhan Kavuncuoğlu and Ahmet Turan Özdemir},
  doi          = {10.1111/coin.12517},
  journal      = {Computational Intelligence},
  month        = {8},
  number       = {4},
  pages        = {1402-1449},
  shortjournal = {Comput. Intell.},
  title        = {Investigating classification performance of hybrid deep learning and machine learning architectures on activity recognition},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parallel model exploration for tumor treatment simulations.
<em>COIN</em>, <em>38</em>(4), 1379–1401. (<a
href="https://doi.org/10.1111/coin.12515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational systems and methods are often being used in biological research, including the understanding of cancer and the development of treatments. Simulations of tumor growth and its response to different drugs are of particular importance, but also challenging complexity. The main challenges are first to calibrate the simulators so as to reproduce real-world cases, and second, to search for specific values of the parameter space concerning effective drug treatments. In this work, we combine a multi-scale simulator for tumor cell growth and a genetic algorithm (GA) as a heuristic search method for finding good parameter configurations in reasonable time. The two modules are integrated into a single workflow that can be executed in parallel on high performance computing infrastructures. In effect, the GA is used to calibrate the simulator, and then to explore different drug delivery schemes. Among these schemes, we aim to find those that minimize tumor cell size and the probability of emergence of drug resistant cells in the future. Experimental results illustrate the effectiveness and computational efficiency of the approach.},
  archive      = {J_COIN},
  author       = {Charilaos Akasiadis and Miguel Ponce-de-Leon and Arnau Montagud and Evangelos Michelioudakis and Alexia Atsidakou and Elias Alevizos and Alexander Artikis and Alfonso Valencia and Georgios Paliouras},
  doi          = {10.1111/coin.12515},
  journal      = {Computational Intelligence},
  month        = {8},
  number       = {4},
  pages        = {1379-1401},
  shortjournal = {Comput. Intell.},
  title        = {Parallel model exploration for tumor treatment simulations},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Retracted: An artificial intelligence-based smart health
system for biological cognitive detection based on wireless
telecommunication. <em>COIN</em>, <em>38</em>(4), 1365–1378. (<a
href="https://doi.org/10.1111/coin.12513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The framework described in this work is a propelled explication to diagnosis of the health condition of the individuals who is affecting from arthritis disease. The innovation underlying this is the Internet of Things (IoT), a propelled system in the current trend to combine sensor data to the visualization platform for improved comprehension. In this work, we fabricated a thin film-based bio-sensor for detecting uric acid (UA) in the blood which is used to diagnose the arthritis disease. The fabricated sensor is integrated with node microcontroller unit (MCU) unit to store the uric acid content data where it is processed and recorded. The logged information can be forwarded by utilizing IoT technology using Thing Speak application programming interfaces (APIs) key and the information refreshed from the framework can be available in the cloud connector. The data for visualization can be obtained by API, validated with the private key, and accessed using Thing Speak APIs on multiple platforms such as mobile, tablet, and laptop connected to the internet. So, in summary, Thing Speak API Handler ensures the legitimacy of data retrieval from the cloud and visualization across several platforms.},
  archive      = {J_COIN},
  author       = {Manikam Babu and Thangaraju Jesudas},
  doi          = {10.1111/coin.12513},
  journal      = {Computational Intelligence},
  month        = {8},
  number       = {4},
  pages        = {1365-1378},
  shortjournal = {Comput. Intell.},
  title        = {Retracted: An artificial intelligence-based smart health system for biological cognitive detection based on wireless telecommunication},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robust object detection system with occlusion handling for
mobile devices. <em>COIN</em>, <em>38</em>(4), 1338–1364. (<a
href="https://doi.org/10.1111/coin.12511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques help computer vision automatically learn the intrinsic patterns within complex data. This research mainly concentrates on creating a mobile application based on augmented reality for elderly mobile users that helps in identifying the traffic signals and other signboards in real-time using deep learning techniques. TensorFlow serves as an implementation platform to build the object detection system with deep learning. The single shot multibox detector (SSD) model and the two-stage faster-regional convolutional neural network (RCNN) models from TensorFlow&#39;s object detection application programming interface (API) are compared in this study. The SSD model with MobileNet as a backbone network serves well for this study as it is faster than the RCNN model with comparable accuracy. However, unconstrained environments like occlusions can be an obstacle to the effective performance of an object detection system. This research provides a solution to handle occlusions by developing a robust object detection system through image segmentation techniques. The model introduced is based on the SSD MobileNet model which enables it to be deployable on mobile devices for real-time offline detection. The developed model exhibits faster performance than the state-of-the-art instance segmentation model, Mask RCNN with comparable accuracy. Elaborated implementation of this system and results are presented in further sections.},
  archive      = {J_COIN},
  author       = {Devi M. Chilukuri and Sun Yi and Younho Seong},
  doi          = {10.1111/coin.12511},
  journal      = {Computational Intelligence},
  month        = {8},
  number       = {4},
  pages        = {1338-1364},
  shortjournal = {Comput. Intell.},
  title        = {A robust object detection system with occlusion handling for mobile devices},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Retracted: An enhanced ensemble machine learning
classification method to detect attention deficit hyperactivity for
various artificial intelligence and telecommunication applications.
<em>COIN</em>, <em>38</em>(4), 1327–1337. (<a
href="https://doi.org/10.1111/coin.12509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention Deficit Hyperactivity Disorder (ADHD) is a common mental health disorder in teenager groups and it consists of a combination of problems. ADHD is a neurodevelopmental condition that manifests itself in children and adolescents as inattention, hyperactivity, and impulsivity. This research proposes a novel classification approach using BoostAlexNet model for ADHD automatic diagnosis. It consists of different pretrained methods like ResNet 101, NASNet, Xception, MobileNet, and InceptionV3. Based on the pretrained model, input MRI images are processed and integrated for the detection of abnormalities in ADHD MRI brain images of patients. The BoostAlexNet model is evaluated and comparatively observed with the existing techniques. The dataset for processing consists of 1359 CT images composed of ADHD and non-ADHD. The validation range is set as 50 for each case with a total value of 150 and the network is trained with MRI images of 1069 for classification. The analysis of results expressed that BoostAlexNet exhibits higher accuracy, sensitivity, and specificity value of 93.67%, 0.93, and 0.97, respectively. The proposed BoostAlexNet classification technique achieves an accuracy of 93.67%. The developed model provides improved accuracy than the existing techniques.},
  archive      = {J_COIN},
  author       = {Meeran Sheriff and Rajagopal Gayathri},
  doi          = {10.1111/coin.12509},
  journal      = {Computational Intelligence},
  month        = {8},
  number       = {4},
  pages        = {1327-1337},
  shortjournal = {Comput. Intell.},
  title        = {Retracted: An enhanced ensemble machine learning classification method to detect attention deficit hyperactivity for various artificial intelligence and telecommunication applications},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integration of optimized local directional weber pattern
with faster region convolutional neural network for enhanced medical
image retrieval and classification. <em>COIN</em>, <em>38</em>(4),
1287–1326. (<a href="https://doi.org/10.1111/coin.12506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With a broad usage of digital imaging data in hospitals, the medical image repository size is growing in a very rapid manner. This creates difficulty in querying and managing these vast databases that in turn leads to the requirement of Content-Based Medical Image Retrieval (CBMIR) systems. The CBMIR is considered as the major ambiguous and challenging task for reducing the semantic gap among the human queries and images in the datasets having more information content. The main intention of this paper is to enhance medical image retrieval and classification using the improved pattern descriptor and deep learning. For the medical image retrieval, this paper develops the Optimized Local Directional Weber Pattern with a multi-objective similarity function. This similarity function focuses on the measures like Structural Similarity Index Measure, Peak Signal-to-Noise Ratio, Mean Squared Error MSE, and correlation. Likewise, the development of an improved Faster-Region Convolutional Neural Network (Faster-RCNN) is adopted in the classification phase. Both the retrieval and classification are enhanced by the Modified Wind Speed-based Deer Hunting Optimization Algorithm. Considering classification and retrieval tasks, the experiments on benchmark datasets reveal an enhancement from the introduced algorithm when compared over the existing methods.},
  archive      = {J_COIN},
  author       = {Dhupam Bhanu Mahesh and Bindu Madhuri and Rajya Lakshmi D},
  doi          = {10.1111/coin.12506},
  journal      = {Computational Intelligence},
  month        = {8},
  number       = {4},
  pages        = {1287-1326},
  shortjournal = {Comput. Intell.},
  title        = {Integration of optimized local directional weber pattern with faster region convolutional neural network for enhanced medical image retrieval and classification},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep learning and heuristic methodology for predicting
breakups in social network structures. <em>COIN</em>, <em>38</em>(4),
1258–1286. (<a href="https://doi.org/10.1111/coin.12502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Literature have focused on studying the apparent and latent interactions within social graphs as an n-ary operation, which yields binary outputs comprising positives (friends, likes, etc.) and negatives (foes, dislikes, etc.). Inasmuch as interactions constitute the bedrock of any given social network (SN) structure; there exist scenarios where an interaction, which was once considered a positive, transmutes into a negative as a result of one or more indicators which have affected the interaction quality. At present, this transmutation has to be manually executed by the affected actors in the SN. These manual transmutations can be quite inefficient, ineffective, and a mishap might have been incurred by the constituent actors and the SN structure prior to a resolution. Our problem statement aims at automatically flagging positive ties that should be considered for breakups or rifts (negative-tie state), as they tend to pose potential threats to actors and the SN. Therefore, we have proposed ClasReg: a unique framework capable of breakup and link predictions.},
  archive      = {J_COIN},
  author       = {Bonaventure Chidube Molokwu and Shaon Bhatta Shuvo and Ziad Kobti},
  doi          = {10.1111/coin.12502},
  journal      = {Computational Intelligence},
  month        = {8},
  number       = {4},
  pages        = {1258-1286},
  shortjournal = {Comput. Intell.},
  title        = {A deep learning and heuristic methodology for predicting breakups in social network structures},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved self-attentive musical instrument digital interface
content-based music recommendation system. <em>COIN</em>,
<em>38</em>(4), 1232–1257. (<a
href="https://doi.org/10.1111/coin.12501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic music recommendation is an open research problem that has seen much work in recent years. A common and successful music recommendation approach is collaborative filtering, which has worked well in this domain. One major drawback of this method is that it suffers from a cold-start problem, and it requires a lot of user-personalized information. It is an ineffective mechanism for recommending new and unpopular songs as well as for new users. In this article, we report a hybrid methodology that uses the song&#39;s content information. We use MIDI (Musical Instrument Digital Interface) content data, a compressed version of an audio song that contains digital information about a song and is machine-readable. We describe a model called MSA-SRec (MIDI Based Self Attentive Sequential Music Recommendation), a latent factor-based self-attentive deep learning model that uses a substantial amount of sequential information as content information of the song for recommendation generation. We use MIDI data of a song that is under-explored content information for music recommendation. We show that using MIDI as content data with user and item latent vector produces reasonable recommendations. We also demonstrate that using MIDI over other music metadata performs better with various state-of-the-art models of recommendation systems.},
  archive      = {J_COIN},
  author       = {Naina Yadav and Anil Kumar Singh and Sukomal Pal},
  doi          = {10.1111/coin.12501},
  journal      = {Computational Intelligence},
  month        = {8},
  number       = {4},
  pages        = {1232-1257},
  shortjournal = {Comput. Intell.},
  title        = {Improved self-attentive musical instrument digital interface content-based music recommendation system},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Retracted: An empirical intrusion detection system based on
XGBoost and bidirectional long-short term model for 5G and other
telecommunication technologies. <em>COIN</em>, <em>38</em>(4),
1216–1231. (<a href="https://doi.org/10.1111/coin.12497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, intrusion detection system (IDS) has become an essential remedy to protect networks from malicious activities and attacks. The attacks force the network activities under risk and cause network data as an unrecoverable one. The attacks have different features in nature and their detection rate is low. An efficient attack detection system requires a huge volume of network transactions and relevant features for training. But the existing datasets have significant and nonsignificant features together. Hence, the significant feature selection plays a major role to detect the attacks. For this reason, this paper adopts the XGBoost approach to identify the relevant (significant) features of the attacks in the dataset. After this XGBoost, this article applies the bidirectional long short term model (Bi-LSTM) for the detection and classification of the attacks. This Bi-LSTM model is a novel approach to the IDS and very effective to increase the detection rate. Thus, this article focuses on feature reduction and classification of the attacks. The experimental results show that the proposed XGBoost and Bi-LSTM combination outperforms in detecting attacks.},
  archive      = {J_COIN},
  author       = {Chinnathangam Karthikraja and Jayaprakasam Senthilkumar and Rajadurai Hariharan and Gandhi Usha Devi and Yuvaraj Suresh and Vijayakumar Mohanraj},
  doi          = {10.1111/coin.12497},
  journal      = {Computational Intelligence},
  month        = {8},
  number       = {4},
  pages        = {1216-1231},
  shortjournal = {Comput. Intell.},
  title        = {Retracted: An empirical intrusion detection system based on XGBoost and bidirectional long-short term model for 5G and other telecommunication technologies},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evolutionary iterated local search meta-heuristic for the
antenna positioning problem in cellular networks. <em>COIN</em>,
<em>38</em>(3), 1183–1214. (<a
href="https://doi.org/10.1111/coin.12454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio network planning is a core problem in cellular networks. It includes coverage, capacity and parameter planning. This paper investigates the Antenna Positioning Problem (APP) which is a main task in cellular networks planning. The aim is to find a trade-off between maximizing coverage and minimizing costs. APP is the task of selecting a subset of potential locations where installing the base stations to cover the entire area. In theory, the APP is NP-hard. To solve it in practice, we propose a new meta-heuristic called Evolutionary Iterated Local Search that merges the local search method and some evolutionary operations of crossover and mutation. The proposed method is implemented and evaluated on realistic, synthetic and random instances of the problem of different sizes. The numerical results and the comparison with the state-of-the-art show that the proposed method succeeds in finding good results for the considered problem.},
  archive      = {J_COIN},
  author       = {Larbi Benmezal and Belaid Benhamou and Dalila Boughaci},
  doi          = {10.1111/coin.12454},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {1183-1214},
  shortjournal = {Comput. Intell.},
  title        = {Evolutionary iterated local search meta-heuristic for the antenna positioning problem in cellular networks},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neurological effects of long-term diet on obese and
overweight individuals: An electroencephalogram and event-related
potential study. <em>COIN</em>, <em>38</em>(3), 1163–1182. (<a
href="https://doi.org/10.1111/coin.12444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of overweight and obesity across all age groups can be considered as an epidemic. A high body mass index not only has effects on longevity and quality of life of people who suffer from it but also has cognitive and neurological consequences. Executive function (EF)—or the neural regions that support it might act as causes of obesity. The aim of this study is to compare the differences between healthy and obese/overweight individuals and to understand how a prolonged diet of 2 months can affect EF event-related potential (ERP) components. Among the ERP metrics, N1, N2, and P3 are measured. Subjects underwent a diet program and their electroencephalogram was recorded every 4–6 weeks using a Stroop paradigm experiment. As the diet progressed, it was observed that perhaps more neural networks and executive control are engaged as the diet begins to take effect. Attention to nonfood increases and more control is exhibited when participants are presented with food images and food word stimuli. Increase in the N2 and P3 components highlight how the diet affects the brain during various stages of diet plans.},
  archive      = {J_COIN},
  author       = {Muhammad Ammar Ali and Süreyya Özöğür-Akyüz and Adil Deniz Duru and Melisa Caliskan and Ceylan Demir and Tuğba Bostancı and Farouk Elsallak and Mohammad Shkokani and Zümray Dokur and Tamer Ölmez and Can Ergün and Nerses Bebek and Gizem Yilmaz},
  doi          = {10.1111/coin.12444},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {1163-1182},
  shortjournal = {Comput. Intell.},
  title        = {Neurological effects of long-term diet on obese and overweight individuals: An electroencephalogram and event-related potential study},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Grey wolf optimizer for optimal sizing of hybrid wind and
solar renewable energy system. <em>COIN</em>, <em>38</em>(3), 1133–1162.
(<a href="https://doi.org/10.1111/coin.12349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By taking facts such as oil depletion, increasing number of population and energy demand into account, alternative electric generation scheme called renewable energy has entered into a new phase. These new energy sources are environmentally clean, exhaustible and friendly with affordable cost, and high reliability. Nowadays, energy generators such as photovoltaic (PV), wind turbine (WT), and geothermal energies are among the commonly used renewable sources. In this article, grey wolf optimization (GWO) methodology is proposed for minimizing the total annual cost of hybrid of wind and solar renewable energy system. Here, determining the optimal number of solar panels, WTs, and batteries which can satisfy the desired load is the main objective of this research. The obtained result shows that the proposed methodology finds optimal solution of sizing of the hybrid system with relatively lower total annual cost and fast convergence rate. To check whether the obtained result was feasible, GWO results are compared with the results of PSO, iteration method and by the work of other scholars in literature. Here the superior capabilities of GWO algorithm have been seen. It is hoped that this research would be beneficial and can be benchmark for researchers of the field.},
  archive      = {J_COIN},
  author       = {Diriba Kajela Geleta and Mukhdeep Singh Manshahia and Pandian Vasant and Anirban Banik},
  doi          = {10.1111/coin.12349},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {1133-1162},
  shortjournal = {Comput. Intell.},
  title        = {Grey wolf optimizer for optimal sizing of hybrid wind and solar renewable energy system},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gravitational search algorithm-based optimization of hybrid
wind and solar renewable energy system. <em>COIN</em>, <em>38</em>(3),
1106–1132. (<a href="https://doi.org/10.1111/coin.12336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the issue of environmental protection coupled with high energy demand, there was an initiation for exploration of different renewable energy sources. This article aims to optimize the total annual cost of hybrids of wind and solar renewable energy system to satisfy the predesigned load. Minimization of the total annual cost of the system by determining appropriate numbers of the components, so that the desired load can be economically and reliably satisfied under the given constraints. Gravitational Search Algorithm (GSA) was employed for the optimization process. GSA is a recently proposed metaheuristic algorithm which is based on Newton&#39;s universal gravitational law of gravity and mass interactions. It uses stochastic rules to escape local optima and find the global optimal solutions. MATLAB codes were designed for the developed fitness function and employed algorithm. The proposed methodology was run for the fitness function through the code and the results were discussed. The result was compared with the results of Particle Swarm Optimization (PSO) and also shown that: GSA has some advantage over PSO algorithm. Even though, the algorithm has several parameters to be adjusted, it is strong in both local and global optimal searches.},
  archive      = {J_COIN},
  author       = {Diriba Kajela Geleta and Mukhdeep Singh Manshahia},
  doi          = {10.1111/coin.12336},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {1106-1132},
  shortjournal = {Comput. Intell.},
  title        = {Gravitational search algorithm-based optimization of hybrid wind and solar renewable energy system},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A synergy thompson sampling hyper-heuristic for the feature
selection problem. <em>COIN</em>, <em>38</em>(3), 1083–1105. (<a
href="https://doi.org/10.1111/coin.12325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To classify high-dimensional data, feature selection plays a key role to eliminate irrelevant attributes and enhance the classification accuracy and efficiency. Since feature selection is an NP-Hard problem, many heuristics and metaheuristics have been used to tackle in practice this problem. In this article, we propose a novel approach that consists in a probabilistic selection hyper-heuristic called the synergy Thompson sampling hyper-heuristic. The Thompson sampling selection strategy is a probabilistic reinforcement learning mechanism to assess the behavior of the low-level heuristics, and to predict which one will be more efficient at each point during the search process. The proposed hyper-heuristic is combined with a 1 nearest neighbor classifier from the Weka framework. It aims to find the best subset of features that maximizes the classification accuracy rate. Experimental results show a good performance in favor of the proposed method when comparing with other existing approaches.},
  archive      = {J_COIN},
  author       = {Mourad Lassouaoui and Dalila Boughaci and Belaid Benhamou},
  doi          = {10.1111/coin.12325},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {1083-1105},
  shortjournal = {Comput. Intell.},
  title        = {A synergy thompson sampling hyper-heuristic for the feature selection problem},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal power flow incorporating renewable uncertainty
related opportunity costs. <em>COIN</em>, <em>38</em>(3), 1057–1082. (<a
href="https://doi.org/10.1111/coin.12316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an optimal power flow solution method incorporating a cost model that associates the uncertainty-related expense incurred with the use of renewable energy sources, viz., solar and wind, is demonstrated. Wind speed and solar radiation are assumed to follow Weibull and normal distributions and the uncertainty is simulated using Monte-Carlo approach. Wind turbine mathematical model is used to estimate the wind generator output, while the same for solar PV is estimated using PV-inverter models. The uncertainty-induced opportunity cost for both the renewable sources is composed of the costs due to both power excess and deficit. These cost components are indicative of the reserve requirement and loss of benefit, due to the unavailability of the corresponding generation. This research models and integrates the opportunity costs of renewable generation into a conventional OPF formulation, which is then solved using four variants of particle swarm optimization method. Among these, mutation-based PSO approach provided better results than others. The test system used is modified IEEE 39-bus network and the performance of the method as well as the effect of the uncertainty cost is evaluated under multiple renewable penetration levels. The results also indicate that solar generation is preferred over wind in terms of the uncertainty cost, while the use of stochastic natured renewable systems is economically justified and preferred over thermal generators.},
  archive      = {J_COIN},
  author       = {Titipong Samakpong and Weerakorn Ongsakul and Nimal Madhu Manjiparambil},
  doi          = {10.1111/coin.12316},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {1057-1082},
  shortjournal = {Comput. Intell.},
  title        = {Optimal power flow incorporating renewable uncertainty related opportunity costs},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An inverse model-based multiobjective estimation of
distribution algorithm using random-forest variable importance methods.
<em>COIN</em>, <em>38</em>(3), 1018–1056. (<a
href="https://doi.org/10.1111/coin.12315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing methods of multiobjective estimation of distributed algorithms apply the estimation of distribution of the Pareto-solution on the decision space during the search and little work has proposed on making a regression-model for representing the final solution set. Some inverse-model-based approaches were reported, such as inversed-model of multiobjective evolutionary algorithm (IM-MOEA), where an inverse functional mapping from Pareto-Front to Pareto-solution is constructed on nondominated solutions based on Gaussian process and random grouping technique. But some of the effective inverse models, during this process, may be removed. This paper proposes an inversed-model based on random forest framework. The main idea is to apply the process of random forest variable importance that determines some of the best assignment of decision variables ( x n ) to objective functions ( f m ) for constructing Gaussian process in inversed-models that map all nondominated solutions from the objective space to the decision space. In this work, three approaches have been used: classical permutation, Naïve testing approach, and novel permutation variable importance. The proposed algorithm has been tested on the benchmark test suite for evolutionary algorithms [modified Deb K, Thiele L, Laumanns M, Zitzler E (DTLZ) and Walking Fish Group (WFG)] and indicates that the proposed method is a competitive and promising approach.},
  archive      = {J_COIN},
  author       = {Pezhman Gholamnezhad and Ali Broumandnia and Vahid Seydi},
  doi          = {10.1111/coin.12315},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {1018-1056},
  shortjournal = {Comput. Intell.},
  title        = {An inverse model-based multiobjective estimation of distribution algorithm using random-forest variable importance methods},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cosine adapted modified whale optimization algorithm for
control of switched reluctance motor. <em>COIN</em>, <em>38</em>(3),
978–1017. (<a href="https://doi.org/10.1111/coin.12310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whale optimization algorithm (WOA) imitates social conduct of humpback whales which is inspired by bubble net hunting strategy of humpback whales. In the present study, Cosine adapted modified whale optimization algorithm (CamWOA) which is a modified version of WOA, has been proposed where cosine function is incorporated for the selection of control parameter “ d ” which governs the position of whales during optimization process. Also, correction factors are employed to modify the movement of search agents during the search process. These changes provide a proper balance between exploration and exploitation phases in CamWOA technique. The performance of CamWOA is analyzed by testing on a set of benchmark functions and compared with other state-of-the-art algorithms. It is observed that CamWOA outperforms other state-of-the-art metaheuristic algorithms in majority of benchmark functions. The efficiency of CamWOA is also evaluated by solving a multiobjective engineering problem pertaining to control of switched reluctance motor. The simulation results confirm that CamWOA yields very promising and competitive results compared to that of WOA and other metaheuristic optimization algorithms.},
  archive      = {J_COIN},
  author       = {Nutan Saha and Sidhartha Panda},
  doi          = {10.1111/coin.12310},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {978-1017},
  shortjournal = {Comput. Intell.},
  title        = {Cosine adapted modified whale optimization algorithm for control of switched reluctance motor},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel improved symbiotic organisms search algorithm.
<em>COIN</em>, <em>38</em>(3), 947–977. (<a
href="https://doi.org/10.1111/coin.12290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For last two decades, nature-inspired metaheuristic algorithms together with their modified, improved, and hybrid versions have been gaining huge popularity in the field of optimization in solving continuous and complex real-life optimization problems. In this work, a novel improved symbiosis organism search (SOS) algorithm, called self-adaptive beneficial factor-based improved SOS (SaISOS, in short) is suggested. The self-adaptive benefit factors and a modified mutualism phase (called “Three-way mutualism phase”) have been introduced here to upgrade the performance of SOS algorithm. A random weighted reflection coefficient and a new control operator have also been introduced. To validate the proposed algorithm and to compare its performance with other state-of-the-art algorithms, 15 IEEE-CEC 2015 functions have been employed and the experimental results confirm that SaISOS provides competitive results on most occasions. Also, the proposed algorithm is used to solve five real-world optimization problems. Considering the average output, it is observed that the proposed method performs significantly better in solving the real-world problems compared to the alternative state-of-the art techniques considered in this work.},
  archive      = {J_COIN},
  author       = {Sukanta Nama and Apu Kumar Saha and Sushmita Sharma},
  doi          = {10.1111/coin.12290},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {947-977},
  shortjournal = {Comput. Intell.},
  title        = {A novel improved symbiotic organisms search algorithm},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using visual analytics to develop human and machine-centric
models: A review of approaches and proposed information technology.
<em>COIN</em>, <em>38</em>(3), 921–946. (<a
href="https://doi.org/10.1111/coin.12289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of a visual analytical system in machine learning is the basis for the integration of human and the use of his intellectual capabilities in the construction of models. At the same time, visual analytics is used to expand human knowledge and is used as a research tool. We investigate the forms and goals of using visual analytics workflow toward the formation of the final product. Workflow is divided into human-oriented and machine-oriented in order to build a model as an information processor and decision-making mechanism. Models are built on the basis of the end user, which can be either a machine or a human. The concepts of model building and the role of machines and humans in these processes are investigated. A practical implementation of the classification information technology in the studied concept “using opposite model” in the machine-oriented visual analytics workflow for using the machine model is proposed. The basis for this model is a model formed and used by human. To classify data, human intellectual abilities are used. The boundaries of classes are determined by a human and then projected into a hyperspace of attributes with the formation of a classification model that the machine uses. Information technology allows the machine to use a model built for humans.},
  archive      = {J_COIN},
  author       = {Iurii Krak and Olexander Barmak and Eduard Manziuk},
  doi          = {10.1111/coin.12289},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {921-946},
  shortjournal = {Comput. Intell.},
  title        = {Using visual analytics to develop human and machine-centric models: A review of approaches and proposed information technology},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Influence of renewable energy sources on the scheduling on
thermal power stations and its optimization for CO2 reduction.
<em>COIN</em>, <em>38</em>(3), 903–920. (<a
href="https://doi.org/10.1111/coin.12477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The thermal power of the total power demand accounts for the total of 60% in India, with the penetration of the nonconventional energy sources the grid management becomes a tedious job in maintaining the grid stability, and adding to the woes the environmental norms become stricter as it is a tough job to maintain to lesser CO 2 emissions in the atmosphere. Hence this article aims with the integration of the grid management with the renewable energy sources and reduction of CO 2 and also strives for the grid stability with the smart grid. And the thermal scheduling optimization is done with the novel bat algorithm.},
  archive      = {J_COIN},
  author       = {Ramalingam Mathi and Subbaiahan Jayalalitha},
  doi          = {10.1111/coin.12477},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {903-920},
  shortjournal = {Comput. Intell.},
  title        = {Influence of renewable energy sources on the scheduling on thermal power stations and its optimization for CO2 reduction},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An ontology matching approach for semantic modeling: A case
study in smart cities. <em>COIN</em>, <em>38</em>(3), 876–902. (<a
href="https://doi.org/10.1111/coin.12474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the semantic modeling of smart cities and proposes two ontology matching frameworks, called Clustering for Ontology Matching-based Instances (COMI) and Pattern mining for Ontology Matching-based Instances (POMI). The goal is to discover the relevant knowledge by investigating the correlations among smart city data based on clustering and pattern mining approaches. The COMI method first groups the highly correlated ontologies of smart-city data into similar clusters using the generic k-means algorithm. The key idea of this method is that it clusters the instances of each ontology and then matches two ontologies by matching their clusters and the corresponding instances within the clusters. The POMI method studies the correlations among the data properties and selects the most relevant properties for the ontology matching process. To demonstrate the usefulness and accuracy of the COMI and POMI frameworks, several experiments on the DBpedia, Ontology Alignment Evaluation Initiative, and NOAA ontology databases were conducted. The results show that COMI and POMI outperform the state-of-the-art ontology matching models regarding computational cost without losing the quality during the matching process. Furthermore, these results confirm the ability of COMI and POMI to deal with heterogeneous large-scale data in smart-city environments.},
  archive      = {J_COIN},
  author       = {Youcef Djenouri and Hiba Belhadi and Karima Akli-Astouati and Alberto Cano and Jerry Chun-Wei Lin},
  doi          = {10.1111/coin.12474},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {876-902},
  shortjournal = {Comput. Intell.},
  title        = {An ontology matching approach for semantic modeling: A case study in smart cities},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An adapting soft computing model for intrusion detection
system. <em>COIN</em>, <em>38</em>(3), 855–875. (<a
href="https://doi.org/10.1111/coin.12433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network security in smart cities has become a key problem in the rapid development of computer networks over the past few years. Intrusion detection systems play a fundamental part in the integrity, confidentiality, and resource accessibility among the multiple network security policies. The classification of the genuineness of packets is main object of the presents research work, the soft computing has applied to classify the genuineness of packets. The complexity of soft computing is greatly reduced if the numbers of features in a dataset are reduced. Managing and analysis of the dimensionality reduction is novelty of the proposed model. The existence of uncertainty and the imprecise nature of the intrusions appear to create suitable fuzzy logic systems for such structures. The neural-fuzzy algorithm is one of the effective methods that incorporate fuzzy logic systems into adaptive and analysis capacities. In this research work, soft computing fuzzy logic system is proposed to enhance network security through intrusion detection. Three network datasets are demonstrated to test and estimate the proposed system. Feature selection has used to remove irrelevant features from entire network data which are obstacle classification processes. The Information Gain method was applied to select importance features for detection intrusion. Adaptive Neuro-Fuzzy Inference System (ANFIS) is further used to process the significant features of the classification network data as normal or attacks packets. Two functions named Jang&#39;s Neuro-fuzzy and faster-scaled conjugate gradient (SCG) based on the ANFIS system. Obviously, the experimental results demonstrate the proposed system has attained higher precision in detecting normal or attack. The experimental results have suggested that the proposed system results are better in accuracy and time process for classification compared with the existing models. The Overall Results show that the proposed system can be able to detect various intrusions efficiently and effectively.},
  archive      = {J_COIN},
  author       = {Husam Ibrahiem Husain Alsaadi and Rafah M. ALmuttari and Osman Nuri Ucan and Oguz Bayat},
  doi          = {10.1111/coin.12433},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {855-875},
  shortjournal = {Comput. Intell.},
  title        = {An adapting soft computing model for intrusion detection system},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Management and entrepreneurship management mechanism of
college students based on support vector machine algorithm.
<em>COIN</em>, <em>38</em>(3), 842–854. (<a
href="https://doi.org/10.1111/coin.12430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the employment and entrepreneurship management of college students, the application of big data technology can effectively improve their work efficiency, that is, the support vector machine algorithm is applied to the employment and entrepreneurship management of college students. Based on deep learning technology, the deep neural network is constructed based on SVR and restrictive Boltzmann machine, namely, SVR-DBN, including theoretical derivation of model architecture, design and selection of model training algorithms, and the modeling steps and flow charts are given, and finally applied to the influence factor analysis. The multiangle comparison proves that the proposed depth model has excellent feature extraction ability and regression prediction. The results show that the algorithm has higher accuracy and has a 26% improvement over traditional algorithms. The research is of great significance to the improvement of the efficiency of employment and entrepreneurship management and the application of support vector machine algorithms.},
  archive      = {J_COIN},
  author       = {Chao Wang and Yazhi Dong and Yuejun Xia and Guoxu Li and Oscar Sanjuán Martínez and Rubén González Crespo},
  doi          = {10.1111/coin.12430},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {842-854},
  shortjournal = {Comput. Intell.},
  title        = {Management and entrepreneurship management mechanism of college students based on support vector machine algorithm},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic planning method of economic operation optimization
model. <em>COIN</em>, <em>38</em>(3), 829–841. (<a
href="https://doi.org/10.1111/coin.12423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enterprise dynamic planning is a branch of operations research and multi-stage decision-making processes. This method can be used to solve multi-objective decision-making problems, thereby optimizing the overall effect of the system. Aiming at the dynamic programming method of economic operation optimization model, this article studies a multi-objective dynamic programming algorithm and improves the algorithm. Taking the optimization arrangement of enterprise production plan as an example, according to the production capacity, production cost and inventory cost of the enterprise, a dynamic planning method is adopted to establish a production plan arrangement model. The experimental results show that using the method proposed in this article to dynamically plan the economic operation optimization model can not only improve the convergence of the results but also improve its accuracy. This result solves the optimization problem of the enterprise&#39;s product production plan.},
  archive      = {J_COIN},
  author       = {Jing Jing},
  doi          = {10.1111/coin.12423},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {829-841},
  shortjournal = {Comput. Intell.},
  title        = {Dynamic planning method of economic operation optimization model},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling and verifying a resource allocation algorithm for
secure service migration for commercial cloud systems. <em>COIN</em>,
<em>38</em>(3), 811–828. (<a
href="https://doi.org/10.1111/coin.12421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing is the delivery of on-demand computing resources. It shares the resources or provides vir-utilization that enables single user to access various Cloud services such as CPU, memory, storage devices, network, and so on. However, more commercial cloud services offered by several cloud service providers (CSPs) are available in the market place. Most CSPs must, therefore, deal with the dynamic resource allocation where the mobile services are migrating from one cloud to another cloud environment to provide heterogeneous resources based on user needs. There is still a lack of heuristics that are able to check requested resources and available resources to allocate and deallocate before it begins the secure service migration. We proposed a resource allocation security protocol that allows resources to be allocated and migrated efficiently in a secure service migration between cloud infrastructures. Furthermore, formal methods can be used for protocols to verify the desired properties, detecting attacks and producing accurate outcomes. This article presents formal modeling and verification of this abstract protocol using ProVerif cryptographic tool to validate the security properties such as secrecy of resources, authentication from both parties and key exchange in order to securely migrate resources in commercial cloud environments.},
  archive      = {J_COIN},
  author       = {Gayathri Karthick and Glenford Mapp and Florian Kammueller and Mahdi Aiash},
  doi          = {10.1111/coin.12421},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {811-828},
  shortjournal = {Comput. Intell.},
  title        = {Modeling and verifying a resource allocation algorithm for secure service migration for commercial cloud systems},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simulation based study on parameter variation of Si0.9Ge0.1
junction-less SELBOX FinFET for high-performance application.
<em>COIN</em>, <em>38</em>(3), 801–810. (<a
href="https://doi.org/10.1111/coin.12416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a high-performance SiGe junctionless FinFET (JLFinFET) on insulator by selective growth of buried oxide (SELBOX) layer device with better electrostatics. The mole fraction x = 0.1, 0.2, 0.3, 0.4, 0.5, and 0.7 with fin width 10 nm and gate length of 20 nm are considered for simulation using technology computer aided design (TCAD) Sentaurus device. With Si 0.9 Ge 0.1 JLFinFET on insulator by SELBOX layer, nearly an ideal subthreshold swing of 61.51 mV/decade and enhanced on-current, off-current has been achieved. Evaluation of on-off current ratio, DIBL, SS for different parameters, such as doping concentration (10 15 -10 19 /cm 3 ), channel length (10-40 nm), temperature (200-700°K) on JLJFinFET on insulator by SELBOX layer are presented. Three-dimensional device simulation using the TCAD software tool Sentaurus Device is used to simulate and the results are compared with a SELBOX JLFinFET.},
  archive      = {J_COIN},
  author       = {G Vidhya Sagar and D Vijayakumar},
  doi          = {10.1111/coin.12416},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {801-810},
  shortjournal = {Comput. Intell.},
  title        = {Simulation based study on parameter variation of Si0.9Ge0.1 junction-less SELBOX FinFET for high-performance application},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient energy consumption system using heuristic
renewable demand energy optimization in smart city. <em>COIN</em>,
<em>38</em>(3), 784–800. (<a
href="https://doi.org/10.1111/coin.12412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of urban development in recent years required reliable as well as realistic smart solutions to transport, system infrastructure, environmental conditions, and quality of life in smart cities. Furthermore, several innovative and comprehensive applications for smart cities are accessible through the Internet of Things that plays a significant role in reducing the utilization of energy requirements and other environmental effects. Based on the demands in reducing energy consumption, this article designed and developed a combined heat and power design based on the renewable energy system and energy storage system, which helps to minimize the utilization of energy consumption in smart cities. In these concerns, a standardized heuristic renewable demand energy optimization in the smart city (HRDEOSC) architecture is presented, where the smart area domain is distributed into a wide area network. In comparison with the overall domestic energy consumption of electricity and transport, the energy demand for desalination processes is very small and it has been achieved by HRDEOSC. Here, the designed computational model demonstrate that the developed system contributes significantly to our challenges and proved to be an economical approach for the development of the smart structural design which helps to reduce the energy consumption of smart cities.},
  archive      = {J_COIN},
  author       = {Ming Shu and Shizhong Wu and Tong Wu and Zhonglin Qiao and Nai Wang and Fei Xu and A. Shanthini and Bala Anand Muthu},
  doi          = {10.1111/coin.12412},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {784-800},
  shortjournal = {Comput. Intell.},
  title        = {Efficient energy consumption system using heuristic renewable demand energy optimization in smart city},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analyzing and forecasting COVID-19 pandemic in the kingdom
of saudi arabia using ARIMA and SIR models. <em>COIN</em>,
<em>38</em>(3), 770–783. (<a
href="https://doi.org/10.1111/coin.12407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The novel coronavirus COVID-19 is spreading all across the globe. By June 29, 2020, the World Health Organization announced that the number of cases worldwide had reached 9 994 206 and resulted in more than 499 024 deaths. The earliest case of COVID-19 in the Kingdom of Saudi Arabia (KSA) was registered on March 2 in 2020. Since then, the number of infections as per the outcome of the tests increased gradually on a daily basis. The KSA has 182 493 cases, with 124 755 recoveries and 1551 deaths on June 29, 2020. There have been significant efforts to develop models that forecast the risks, parameters, and impacts of this epidemic. These models can aid in controlling and preventing the outbreak of these infections. In this regard, this article details the extent to which the infection cases, prevalence, and recovery rate of this pandemic are in the country and the predictions that can be made using the past and current data. The well-known classical SIR model was applied to predict the highest number of cases that may be realized and the flattening of the curve afterward. On the other hand, the ARIMA model was used to predict the prevalence cases. Results of the SIR model indicate that the repatriation plan reduced the estimated reproduction number. The results further affirm that the containment technique used by Saudi Arabia to curb the spread of the disease was efficient. Moreover, using the results, close interaction between people, despite the current measures remains a great risk factor to the spread of the disease. This may force the government to take even more stringent measures. By validating the performance of the applied models, ARIMA proved to be a good forecasting method from current data. The past data and the forecasted data, as per the ARIMA model provided high correlation, showing that there were minimum errors.},
  archive      = {J_COIN},
  author       = {Khaled Ali Abuhasel and Mosaad Khadr and Mohammed M. Alquraish},
  doi          = {10.1111/coin.12407},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {770-783},
  shortjournal = {Comput. Intell.},
  title        = {Analyzing and forecasting COVID-19 pandemic in the kingdom of saudi arabia using ARIMA and SIR models},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An experimental approach to evaluate machine learning models
for the estimation of load distribution on suspension bridge using FBG
sensors and IoT. <em>COIN</em>, <em>38</em>(3), 747–769. (<a
href="https://doi.org/10.1111/coin.12406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the tragedies on any bridge structure have been the cause of high-density crowd behavior as a response to trampling as well as the crushing scenario. Therefore, it is most important to monitor such unforeseen situations by sensing the load imposed on the bridge structures. This scenario may arise where crowd movement is huge on these types of bridges. Similarly, the fiber Bragg grating (FBG) is a promising technology for structural health monitoring applications. In this work, an Internet of Things based FBG optical sensing scheme is proposed to monitor real-time strain distribution throughout the bridge structures and localization of load imposed on the structure from a central control room. A suspension bridge model is designed by referring to a real bridge scenario and these FBG sensors are deployed to validate the proposed machine learning models. In this article, the performances of two machine learning strategies are discussed for the accurate estimation of load and its position by acquiring high sensitive FBG sensors signals at a very high data rate. The algorithms include K-nearest neighbor (KNN) and random forest (RF); which are applied on each sensing data source, and then validated using a prototype suspension bridge model integrated with three FBG sensors (1532 nm, 1538 nm, and1541 nm) on a single optical fiber cable.},
  archive      = {J_COIN},
  author       = {Ambarish G. Mohapatra and Ashish Khanna and Deepak Gupta and Maitri Mohanty and Victor Hugo C. de Albuquerque},
  doi          = {10.1111/coin.12406},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {747-769},
  shortjournal = {Comput. Intell.},
  title        = {An experimental approach to evaluate machine learning models for the estimation of load distribution on suspension bridge using FBG sensors and IoT},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Research on group behavior model based on neural network
computing. <em>COIN</em>, <em>38</em>(3), 731–746. (<a
href="https://doi.org/10.1111/coin.12403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compute is a term coined from the etymology of French and Latin words computer and computare respectively, so is computing. This field of computing has grown enormously over the years. From the simple, traditional Turing machine invented in 1936 by Alan Turing to the current neural network (NN) computing. NNs, a field of artificial intelligence (AI) was exhilarated from the structure and inner workings of the brain. Just as the brain is, that is, an interconnection of neurons, so is the NN which is an interconnection of basic structures known as the perceptron. They do not differ much in structure. Their only difference is that one is artificial while the other is entirely biological. The hierarchical intricacies of the NN can be represented in three layers: the perceptron, artificial NN (ANN), and deep NN (DNN). With the influx of mental and behavioral disorders, basic surveillance, and the urgency to improve the mental health of people, studying the behavioral dynamics of people is requisite. CCTV and street cameras can only do so much, thus the need to employ the field of NN which makes use of supervised learning in training the models to perfect and automate surveillance. The results of this retrospective research indicate that the use of the NN model surpasses those of traditional methods in terms of efficiency and reliability.},
  archive      = {J_COIN},
  author       = {Jinfeng Wei and Yuan Tian and Jingui Geng},
  doi          = {10.1111/coin.12403},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {731-746},
  shortjournal = {Comput. Intell.},
  title        = {Research on group behavior model based on neural network computing},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive weighted dynamic differential evolution algorithm
for emergency material allocation and scheduling. <em>COIN</em>,
<em>38</em>(3), 714–730. (<a
href="https://doi.org/10.1111/coin.12389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emergency material allocation and scheduling is a combination optimization problem, which is essentially a Non-deterministic Polynomial (NP) problem. Aiming at the problems such as slow convergence, easy prematurely falling into local optimum, and parameter constraints to solve high-dimensional and multi-modal combination optimization problems, this article proposes an adaptive weighted dynamic differential evolution (AWDDE) algorithm. The algorithm uses a chaotic mapping strategy to initialize the population. By weighting the standard differential evolution (DE) mutation strategy, a new weighted mutation operator is proposed. The scaling factor and cross probability can be adaptively adjusted. A disturbance operator is introduced to randomly generate the perturbation mutation and to accelerate the premature individuals to jump out of the local optimum. The algorithm is applied to the problem of emergency material allocation and scheduling, and a two-stage emergency material allocation and scheduling model is established. Compared with the standard DE algorithm and the chaos adaptive particle swarm algorithm, the results show that the AWDDE algorithm has the characteristics of stronger global optimization ability and faster convergence speed compared with other optimization algorithms, which provide assistance for smart cities research, including smart city services, applications, case studies, and policymaking considerations for emergency management.},
  archive      = {J_COIN},
  author       = {Tiejun Wang and Kaijun Wu and Tiaotiao Du and Xiaochun Cheng},
  doi          = {10.1111/coin.12389},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {714-730},
  shortjournal = {Comput. Intell.},
  title        = {Adaptive weighted dynamic differential evolution algorithm for emergency material allocation and scheduling},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tri-transformer hawkes process via dot-product attention
operations with event type and temporal encoding. <em>COIN</em>,
<em>38</em>(2), 690–712. (<a
href="https://doi.org/10.1111/coin.12496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asynchronous event sequences widely exist in the real world, such as social networks, electronic medical records, financial data, and genome analysis. For modeling asynchronous event sequences in the continuous time domain, point process has become the underpinning. In the initial research stage, Hawkes process is widely used because it can capture the self-triggering and mutual triggering modes between different events in a variety of point process functions. In recent years, due to the development of neural networks, deep point process (also known as neural point process) can learn models with the stronger fitting ability and reduce the dependence on prior knowledge by using the powerful capacity of neural networks. The proposal of the transformer Hawkes process (THP) has led to a huge performance improvement, so a new climax of the transformer-based deep Hawkes process is set off. However, THP does not make full use of the event and temporal information underlying the asynchronous event sequence, meanwhile, if we simply take the event type encoding and temporal encoding as the sequence encoding, a single transformer may suffer from learning bias. In order to circumvent these problems, we propose a tri-transformer Hawkes process model (TTHP), in which the event and temporal information are introduced to the dot-product attention operations as auxiliary information to form different multihead attention, respectively, and are utilized to build three heterogeneous learners. A series of well-designed experiments on synthetic and real-world datasets validate the effectiveness of the proposed TTHP.},
  archive      = {J_COIN},
  author       = {Zhi-yan Song and Jian-wei Liu},
  doi          = {10.1111/coin.12496},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {690-712},
  shortjournal = {Comput. Intell.},
  title        = {Tri-transformer hawkes process via dot-product attention operations with event type and temporal encoding},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving the classification of phishing websites using a
hybrid algorithm. <em>COIN</em>, <em>38</em>(2), 667–689. (<a
href="https://doi.org/10.1111/coin.12494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a hybrid algorithm has been proposed for the identification of phishing and legitimate websites. The dataset may have an imbalanced class distribution and may consist of irrelevant features. Therefore, in the data preprocessing, the adaptive synthetic sampling approach has been used to handle the imbalanced data. Irrelevant or redundant features are removed from the balanced data using the proposed binary version of Rao algorithms. The S-shaped and V-shaped transfer functions are applied for mapping continuous search space to discrete search space. Also, the results of these S-shaped and V-shaped transfer functions are analyzed for proposed algorithms. The performance is improved by optimizing the value of the k parameter in the kNN classifier. The dataset used in this article has been taken from the UCI machine-learning repository. The performance of the proposed approach has been evaluated using the polygon area metric. The obtained classification accuracy is 97.044%. A comparison of the proposed hybrid algorithm with the other state-of-the-art techniques is also made for validation. Moreover, the proposed approach has been compared with seven metaheuristic feature selection algorithms and six filter methods for performance analysis. Additionally, we have applied the proposed approach to URLs that are registered on the PhishTank website.},
  archive      = {J_COIN},
  author       = {Suvita Rani Sharma and Birmohan Singh and Manpreet Kaur},
  doi          = {10.1111/coin.12494},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {667-689},
  shortjournal = {Comput. Intell.},
  title        = {Improving the classification of phishing websites using a hybrid algorithm},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An experimental framework for evaluating loss minimization
in multi-label classification via stochastic process. <em>COIN</em>,
<em>38</em>(2), 641–666. (<a
href="https://doi.org/10.1111/coin.12491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One major challenge multi-label classification faces, are the conditions for evaluating multi-label algorithms. Simplistic experimental setups based on artificial data may not capture crucial situations for analyzing these algorithms. This article introduces an experimental framework for evaluating multi-label algorithms by artificially generating the probabilistic label distributions. The proposed framework has the benefits of considering a wide variety of labels distributions, and enables users to simulate probability label distributions with a better control of the label dependence and the difficulty of the problem. An experimental study was conducted using the framework where new findings with respect to five methods, binary relevance, classifier chain, dependent binary relevance, calibrated label ranking by pairwise comparison and probabilistic classifier chain, were revealed. This framework will facilitate conducting new experimental studies for analysing the effects of changing label dependence and the difficulty of the problem on the performance of new multi-label algorithms.},
  archive      = {J_COIN},
  author       = {Lucas Henrique Sousa Mello and Flávio M. Varejão and Alexandre L. Rodrigues},
  doi          = {10.1111/coin.12491},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {641-666},
  shortjournal = {Comput. Intell.},
  title        = {An experimental framework for evaluating loss minimization in multi-label classification via stochastic process},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient clinical support system for heart disease
prediction using TANFIS classifier. <em>COIN</em>, <em>38</em>(2),
610–640. (<a href="https://doi.org/10.1111/coin.12487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today&#39;s world, the advancement of telediagnostic equipment plays an essential role to monitor heart disease. The earlier diagnosis of heart disease proliferates the compatibility of treatment of patients and predominantly provides an expeditious diagnostic recommendation from clinical experts. However, the feature extraction is a major challenge for heart disease prediction where the high dimensional data increases the learning time for existing machine learning classifiers. In this article, a novel efficient Internet of Things-based tuned adaptive neuro-fuzzy inference system (TANFIS) classifier has been proposed for accurate prediction of heart disease. Here, the tuning parameters of the proposed TANFIS are optimized through Laplace Gaussian mutation-based moth flame optimization and grasshopper optimization algorithm. The simulation scenario can be carried out using11 different datasets from the UCI repository. The proposed method obtains an accuracy of 99.76% for heart disease prediction and it has been improved upto 5.4% as compared with existing algorithms.},
  archive      = {J_COIN},
  author       = {Jayachitra Sekar and Prasanth Aruchamy and Haleem Sulaima Lebbe Abdul and Amin Salih Mohammed and Shaik Khamuruddeen},
  doi          = {10.1111/coin.12487},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {610-640},
  shortjournal = {Comput. Intell.},
  title        = {An efficient clinical support system for heart disease prediction using TANFIS classifier},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive partitioning-based copy-move image forgery
detection using optimal enabled deep neuro-fuzzy network. <em>COIN</em>,
<em>38</em>(2), 586–609. (<a
href="https://doi.org/10.1111/coin.12484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of photo editing applications, like Adobe Photoshop, has manipulated the operation of digital images into a simple task. However, these manipulations of images misrepresent the content of the original image for misleading the public. Various copy move forgery detection techniques are developed, but these show less robustness on the image with noise and blurring. This article develops an optimization-driven deep learning technique for image forgery detection. The purpose is to develop a copy-move image forgery detection technique using a deep neuro-fuzzy network and a newly developed optimization algorithm. Here, adaptive partitioning is adapted using a rectangular search for splitting the image into different parts. In addition, the features like local Gabor XOR pattern and Texton features are extracted from the partition. Furthermore, the forgery is detected using the deep neuro-fuzzy network. Finally, the deep neuro-fuzzy network training is performed using the proposed multi-verse invasive weed optimization (MVIWO) technique. The proposed MVIWO method will be newly designed by integrating the multi-verse optimizer and invasive weed optimization technique. Thus, the copy-move image forgery detection is effectively performed using the proposed MVIWO-based deep neuro-fuzzy network. The developed MVIWO-based deep neuro-fuzzy network offers superior performance with the highest specificity of 93.54%, highest accuracy of 94.01%, and highest sensitivity of 97.75%.},
  archive      = {J_COIN},
  author       = {Geetha Mariappan and Aravapalli Rama Satish and P. V. Bhaskar Reddy and Balajee Maram},
  doi          = {10.1111/coin.12484},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {586-609},
  shortjournal = {Comput. Intell.},
  title        = {Adaptive partitioning-based copy-move image forgery detection using optimal enabled deep neuro-fuzzy network},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast computation of deep neural network and its real-time
implementation for image recognition. <em>COIN</em>, <em>38</em>(2),
560–585. (<a href="https://doi.org/10.1111/coin.12481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The convolution is widely used for deep neural networks to extract the key features, which requires many additions and multiplications. In this study, the fast computational algorithm is presented to reduce the number of arithmetic when the accuracy is kept. The order of deep convolution is alternative to save the computational operators. To verify the performance, the proposed algorithm is embedded to the typical deep neural network VggNet. The structure of VggNet is further modified using the proposed summation and concatenation techniques to improve the computational accuracy and to reduce the processing time. Compared with the original VggNet, the simulations show that the operational FLOPs can be greatly reduced at least 50% with various datasets testing. Besides, the training time with epoch per batch can save about 10%–20%. The proposed fast algorithm can lessen the parameters and the mode size over 90%. The recognition accuracy can be improved with 1%–4% from various datasets testing. Based on the fast network, real-time FPGA had been realized, which the hardware performance can achieve 371 GOPs with 642 DSP cores. The processing speed can achieve near to 1 k frames per second, and the real-time recognition rate can achieve over 90%.},
  archive      = {J_COIN},
  author       = {Shih-Chang Hsia and Szu-Hong Wang and Feng-Yang Kuo},
  doi          = {10.1111/coin.12481},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {560-585},
  shortjournal = {Comput. Intell.},
  title        = {Fast computation of deep neural network and its real-time implementation for image recognition},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A heterogeneous stacking ensemble based sentiment analysis
framework using multiple word embeddings. <em>COIN</em>, <em>38</em>(2),
530–559. (<a href="https://doi.org/10.1111/coin.12478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Word embedding techniques have been proposed in the literature to analyze and determine the sentiments expressed in various textual documents such as social media posts, online product reviews, and so forth. However, it is difficult to capture the entire gamut of intricate inter-dependencies among words in the textual documents using a specific word embedding technique. In this article, we aim to address this issue by proposing a computation-efficient stacking ensemble based sentiment analysis framework using multiple word embeddings. The proposed framework uses a combination of three distinct word embeddings generated by three different state of the art word embedding techniques, namely, Word2Vec , GloVe , and BERT for performing the sentiment analysis task. It uses an explicitly trained Word2Vec model to generate the first set of 200-dimensional word embedding. Similarly, pre-trained GloVe and BERT models are used to generate the other two sets of 200-dimensional and a 768-dimensional word embeddings, respectively. These three distinct word embedding sets are then used to train a heterogeneous stacking ensemble based classifier model comprising LSTM , GRU , and Bi-GRU based base-level classifiers, and a LSTM based meta-level classifier. Experimental results on four different datasets, namely, Sentiment140 , IMDB Review , Twitter conversation thread , and Twitter Emotion show that the proposed framework achieves high performance with low false positive rate. The proposed framework is also shown to outperform other sentiment analysis frameworks proposed in the literature.},
  archive      = {J_COIN},
  author       = {Basant Subba and Simpy Kumari},
  doi          = {10.1111/coin.12478},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {530-559},
  shortjournal = {Comput. Intell.},
  title        = {A heterogeneous stacking ensemble based sentiment analysis framework using multiple word embeddings},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning for breast cancer classification: Enhanced
tangent function. <em>COIN</em>, <em>38</em>(2), 506–529. (<a
href="https://doi.org/10.1111/coin.12476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning using convolutional neural network (CNN) has been used successfully to classify the images of breast cells accurately. However, the accuracy of manual classification of those histopathological images is comparatively low. This research aims to increase the accuracy of the classification of breast cancer images by utilizing a patch-based classifier (PBC) along with deep learning architecture. The proposed system consists of a deep convolutional neural network that helps in enhancing and increasing the accuracy of the classification process. This is done by the use of the PBC. CNN has completely different layers where images are first fed through convolutional layers using hyperbolic tangent function together with the max-pooling layer, drop out layers, and SoftMax function for classification. Further, the output obtained is fed to a PBC that consists of patch-wise classification output followed by majority voting. The results are obtained throughout the classification stage for breast cancer images that are collected from breast-histology datasets. The proposed solution improves the accuracy of classification whether or not the images had normal, benign, in-situ, or invasive carcinoma from 87% to 94% with a decrease in processing time from 0.45 to 0.2 s on average. The proposed solution focused on increasing the accuracy of classifying cancer in the breast by enhancing the image contrast and reducing the vanishing gradient. Finally, this solution for the implementation of the contrast limited adaptive histogram equalization technique and modified tangent function helps in increasing the accuracy.},
  archive      = {J_COIN},
  author       = {Ashu Thapa and Abeer Alsadoon and P. W. C. Prasad and Simi Bajaj and Omar Hisham Alsadoon and Tarik A. Rashid and Rasha S. Ali and Oday D. Jerew},
  doi          = {10.1111/coin.12476},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {506-529},
  shortjournal = {Comput. Intell.},
  title        = {Deep learning for breast cancer classification: Enhanced tangent function},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamically adaptive and diverse dual ensemble learning
approach for handling concept drift in data streams. <em>COIN</em>,
<em>38</em>(2), 463–505. (<a
href="https://doi.org/10.1111/coin.12475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift refers to the change in data distributions and evolving relationships between input and output variables with the passage of time. To analyze such variations in learning environments and generate models which can accommodate changing performance of predictive systems is one of the challenging machine learning applications. In general, the majority of the existing schemes consider one of the specific drift types: gradual, abrupt, recurring, or mixed, with traditional voting setup. In this work, we propose a novel data stream framework, dynamically adaptive and diverse dual ensemble (DA-DDE) which responds to multiple drift types in the incoming data streams by combining online and block-based ensemble techniques. In the proposed scheme, a dual diversified ensemble-based system is constructed with the combination of active and passive ensembles, updated over a diverse set of resampled input space. The adaptive weight setting method is proposed in this work which utilizes the overall performance of learners on historic as well as recent concepts of distributions. Further a dual voting system has been used for hypothesis generation by considering dynamic adaptive credibility of ensembles in real time. Comparative analysis with 14 state-of-the-art algorithms on 24 artificial and 11 real datasets shows that DA-DDE is highly effective in handling various drift types.},
  archive      = {J_COIN},
  author       = {Kanu Goel and Shalini Batra},
  doi          = {10.1111/coin.12475},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {463-505},
  shortjournal = {Comput. Intell.},
  title        = {Dynamically adaptive and diverse dual ensemble learning approach for handling concept drift in data streams},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TRUST-based features for detecting the intruders in the
internet of things network using deep learning. <em>COIN</em>,
<em>38</em>(2), 438–462. (<a
href="https://doi.org/10.1111/coin.12473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) is a trending domain and has acquired much interest for various kinds of civilian applications. The purpose of IoT is to make objects accessible and interconnected via internet. Hence, security to IoT devices is a major issue because devices connected to the IoT network are resource-constrained. In IoT, the nodes exchange information using insecure internet, which makes the network exposed to different attacks. This article proposes a new intrusion detection strategy, namely, Taylor-spider monkey optimization-based deep belief network (Taylor-SMO-based DBN). The KDD features and the trust factors are employed for intrusion detection. The KDD features are subjected to the classification, which is progressed using a newly devised optimization algorithm, namely, Taylor-spider monkey optimization (Taylor-SMO)-based DBN. The proposed Taylor-SMO algorithm is designed by integrating the Taylor series and spider monkey optimization (SMO) algorithm and is employed to train the deep belief network (DBN) to achieve accurate intrusion detection. The proposed Taylor-SMO-based DBN outperformed other methods with maximal accuracy of 90%, false alarm rate of10%, precision of 90%, and recall of 92%, respectively.},
  archive      = {J_COIN},
  author       = {Harsh Namdev Bhor and Mukesh Kalla},
  doi          = {10.1111/coin.12473},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {438-462},
  shortjournal = {Comput. Intell.},
  title        = {TRUST-based features for detecting the intruders in the internet of things network using deep learning},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep reinforcement learning based decision-making approach
for avoiding crowd situation within the case of covid’19 pandemic.
<em>COIN</em>, <em>38</em>(2), 416–437. (<a
href="https://doi.org/10.1111/coin.12516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individuals&#39; flow&#39;s fluidifcation in the same way as the thinning of the population&#39;s concentration remains among major concerns within the context of the pandemic crisis situations. The recent COVID-19 pandemic crisis is a typical example of the aforementioned where on despite of the containment phases that radically isolate the population but are not applicable persistently, people have to adapt their behavior to new daily-life situations tempering Individuals&#39; stream, avoiding tides, and watering down population&#39;s concentration. Crowd evacuation is one of the well-known research domains that can play a pertinent role to face the challenge of the COVID-19 pandemic. In fact, considering the population&#39;s concentration thinning within the slant of the “crowd evacuation” paradigm allows managing the flow of the population, and consequently, decreasing the probable number of infected cases. In other words, crowd evacuation modeling and simulation with the aim of better-exploiting individuals&#39; flow allow the study and analysis of different possible outcomes for designing population&#39;s concentration thinning strategies. In this article, a new decision-making approach is proposed in order to cope with the aforesaid challenges, which relies on an independent Deep Q Network with an improved SIR model (IDQN-I-SIR). The machine-learning component (i.e., IDQN) is in charge of the agent&#39;s movements control and I-SIR (improved “susceptible-infected-recovered” individuals) model is responsible to control the virus spread. We demonstrate the effectiveness of IDQN-I-SIR through a case-study of individuals&#39; flow&#39;s management with infected cases&#39; avoidance in an emergency department (often overcrowded in context of a pandemic crisis).},
  archive      = {J_COIN},
  author       = {Wejden Abdallah and Dalel Kanzari and Dorsaf Sallami and Kurosh Madani and Khaled Ghedira},
  doi          = {10.1111/coin.12516},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {416-437},
  shortjournal = {Comput. Intell.},
  title        = {A deep reinforcement learning based decision-making approach for avoiding crowd situation within the case of covid&#39;19 pandemic},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image classification with parallel KPCA-PCA network.
<em>COIN</em>, <em>38</em>(2), 397–415. (<a
href="https://doi.org/10.1111/coin.12503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis (PCA) is widely used in computer vision for object detection. In this article, we take advantage of the algorithms of PCA and kernel principal component analysis (KPCA) to construct a deep learning model named parallel KPCA-PCA network (PK-PCANet). In the proposed model, both of the given PCA and KPCA algorithm are aiming to calculate the filters that will be used in the following convolution layers. The extracted features from PCANet and KPCANet are fused by the strategy of parallel feature fusion. With the aim of reducing the dimensionality of the learned features, the algorithm of compressed sensing is incorporated in the proposed network. According to the cooperative advantages of deep learning network and compressed sensing, the proposed PK-PCANet model obtains some improvements in several visual recognition tasks. Extensively experiments are performed on face recognition, hand-written digit recognition and object classification, and the experimental results on various image classification benchmarks such as Extended Yale B, AR, MNIST, CIFAR-10, and VOC 2007 validated the efficiency of the proposed method of PK-PCANet.},
  archive      = {J_COIN},
  author       = {Feng Yang and Zheng Ma and Mei Xie},
  doi          = {10.1111/coin.12503},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {397-415},
  shortjournal = {Comput. Intell.},
  title        = {Image classification with parallel KPCA-PCA network},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A composite framework of deep multiple view human joints
feature extraction and selection strategy with hybrid adaptive sunflower
optimization-whale optimization algorithm for human action recognition
in video sequences. <em>COIN</em>, <em>38</em>(2), 366–396. (<a
href="https://doi.org/10.1111/coin.12499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computer vision and pattern recognition field, video-based human action recognition (HAR) is the most predominant research area. Object recognition is needed to recognize the subjects regarding video contents, which allows reactive enquiry in a large number of camera contents, mainly in security based platforms where there is a prevalent growth of closed circuit television cameras. Generally, object detectors that have high performance are trained on a large collection of public benchmarks. Identifying human activities from unconstrained videos is the primary challenging task. Further, the feature extraction and feature selection from these unconstrained videos is also considered as a challenging issue. For that, in this article a new composite framework of HAR model is constructed by introducing an efficient feature extraction and selection strategy. The proposed feature extraction model extracts multiple view features, human joints features based on the domain knowledge of the action and fuses them with deep high level features extracted by an improved fully resolution convolutional neural networks. Also, it optimizes the feature selection strategy using the hybrid whale optimization algorithm and adaptive sun flower optimization that maximizes the feature entropy, correlation. It minimizes the error rate for improving the recognition accuracy of the proposed composite framework. The proposed model is validated on four different datasets, namely, Olympics sports, Virat Release 2.0, HMDB51, and UCF 50 sports action dataset to prove its effectiveness. The simulation results show that the proposed composite framework outperforms all the existing human recognition model in terms of classification accuracy and detection rate.},
  archive      = {J_COIN},
  author       = {Rajitha Jasmine Rajappan and Thyagharajan Kondampatti Kandaswamy},
  doi          = {10.1111/coin.12499},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {366-396},
  shortjournal = {Comput. Intell.},
  title        = {A composite framework of deep multiple view human joints feature extraction and selection strategy with hybrid adaptive sunflower optimization-whale optimization algorithm for human action recognition in video sequences},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3D face expression recognition with ensemble deep learning
exploring congruent features among expressions. <em>COIN</em>,
<em>38</em>(2), 345–365. (<a
href="https://doi.org/10.1111/coin.12498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic and accurate 3D face expression categorization has been a challenging task in many computer vision applications. This article presents a novel approach named 3D face expression recognition with ensemble deep learning (3D-FER-EDL). The framework comprises three levels. Each of the first two levels gives preference to a subset of labels, considering the congruent features among expressions. Each 3D expressive face is represented as a CDS features vector, which combines corner, distance, and slope features. CDS feature vector is given as input to the first level learners. Expression probability vectors from the first level learners are jointly given as the input to the second level learners. The probability vectors from level one and two are given as the input to the third level, which is the meta classifier. The experiments are conducted on the Bosphorus database and the CASIA 3D face database to evaluate the effectiveness of the proposed approach.},
  archive      = {J_COIN},
  author       = {Suganya Devi Rajagopal and Baskaran Ramachandran},
  doi          = {10.1111/coin.12498},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {345-365},
  shortjournal = {Comput. Intell.},
  title        = {3D face expression recognition with ensemble deep learning exploring congruent features among expressions},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Retracted: IoT incorporated deep learning model combined
with SmartBin technology for real-time solid waste management.
<em>COIN</em>, <em>38</em>(2), 323–344. (<a
href="https://doi.org/10.1111/coin.12495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With a view of the massive human resources and time requirements, the need for an automated, more accurate, and quicker method for handling the classification of solid wastes is felt more than ever worldwide. In this work, an attempt has been made to develop a model named SmartBin. Two different approaches have been followed to classify solid wastes as biodegradable and non-biodegradable efficiently. The first approach is based on convolutional neural network (CNN) and Internet of Things (IoT), while the second approach adds several sensors to the model developed using the first approach. CNN-based IoT is applied on datasets collected using three methods. The first one is Images from Kaggle; the second method adopted searches through Google and Bing, whereas the third one involved captured manually under a controlled environment. It is observed that the second approach has proved to be better, with an accuracy level of 98.57, which is a significantly improved performance over the first approach with an accuracy of 95.24%.},
  archive      = {J_COIN},
  author       = {Muthuramalingam Sivakumar and Perumal Renuka and Pandian Chitra and Sundararajan Karthikeyan},
  doi          = {10.1111/coin.12495},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {323-344},
  shortjournal = {Comput. Intell.},
  title        = {Retracted: IoT incorporated deep learning model combined with SmartBin technology for real-time solid waste management},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Retracted: Adaptive neuro fuzzy inference system to enhance
the classification performance in smart irrigation system.
<em>COIN</em>, <em>38</em>(2), 308–322. (<a
href="https://doi.org/10.1111/coin.12492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional agricultural systems require huge amounts of money for on-site irrigation power. Irrigation is the process of giving water to plants for growth and development. The cost of global warming creates the potential effects of water adaptation measures to ensure food production and water availability for consumption. It requires continuous monitoring of soil moisture content at the root zone and initiating irrigation as per the pre-programmed schedule depending upon the plant&#39;s nature, growth, soil type, and environment. Thus, the cost to farmers using conventional drip irrigation has become so high that they have to go manually and monitor the land frequently. So when the previous method was used for irrigation, it did not give correct classification results. Agricultural data mining technology provides the best crops for travel water to increase the number of crops produced. Agricultural data can be collected more efficiently using data collection and capable analytics sensors. This proposed work introduces an adaptive neuro-fuzzy inference system (ANFIS) technique for analyzing agricultural plant growth based on soil, water level, temperature, and moisture conditions. The proposed technique has three phases: environment data collection using sensor node, preprocessing, feature selection, and classification. The initial phase captures the plant&#39;s environment condition via a sensor device, then transmits data to servers with the help of an edge node or gateway. This recursive adaptive filter method normalizes the data and removes irrelevant noise data from the sensor node. The feature selection method to extract the environment feature value with the help of a linear regression model. The ANFIS method&#39;s plant environment feature value has trained the neurons with the help of the stochastic gradient descent method. The classification method introduces a fuzzy rule to compute and validate the input parameters (e.g., soil moisture, temperature, and humidity) to predict any environment value changes. The comparison results prove that efficient monitoring is obtained through the proposed smart irrigation system.},
  archive      = {J_COIN},
  author       = {VijayaRangan Vivekanandhan and Subramaniam Sakthivel and Muthaiyan Manikandan},
  doi          = {10.1111/coin.12492},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {308-322},
  shortjournal = {Comput. Intell.},
  title        = {Retracted: Adaptive neuro fuzzy inference system to enhance the classification performance in smart irrigation system},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An IoT assisted clinical decision support system for wound
healthcare monitoring. <em>COIN</em>, <em>38</em>(1), 269–306. (<a
href="https://doi.org/10.1111/coin.12482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The patients with surgical and nonsurgical skin wounds shall maintain regular check on wounds in order to assure in-time wound healing. There are numerous measures need to verify during wound checking, that is, wound appearance (wound color and size), wound environment. The wound appearance could verify by a number of clinical techniques presented by researchers of wound care domain, however measurement of wound environment shall fulfill by verification of different environmental factors as wound environment is composition of different factors which collectively formed wound internal environment and external environment. In current research, we presented a framework based on clinical decision support system for measurement of wound environment both internal and external. This framework contains different working module, which shall collaborate in order to check wound environment. We elaborated structural and implementation sketch of all these modules. The presented framework based on Internet of Things and machine learning algorithms to collect and analyze data effectively.},
  archive      = {J_COIN},
  author       = {Hina Sattar and Imran Sarwar Bajwa and Umar Farooq Shafi},
  doi          = {10.1111/coin.12482},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {269-306},
  shortjournal = {Comput. Intell.},
  title        = {An IoT assisted clinical decision support system for wound healthcare monitoring},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A direct classification approach to recognize stress levels
in virtual reality therapy for patients with multiple sclerosis.
<em>COIN</em>, <em>38</em>(1), 249–268. (<a
href="https://doi.org/10.1111/coin.12480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple sclerosis (MS) is a chronic, debilitating, and often progressive inflammatory disease of the nervous system. The disease is highly stressful, which accelerates the risk of depression in people diagnosed with it and possibly exacerbates MS activity. One approach to reducing stress levels in such patients is to utilize virtual reality (VR). Using VR technology and recording physiological signals before and after displaying different environments to the individual, this article proposed a novel therapy procedure for improving stress levels. In the first phase, by distinguishing the stress level obtained from each environment watched by the patient, their corresponding labels are determined by two psychiatrists. Accordingly, the automated model is designed based on the analysis of VR scenes and can accurately classify MS patients&#39; stress levels after watching the 3D environment. The proposed model consists of a fractal descriptor and SVM-RBF classifier to recognize VR scenes that can significantly reduce the stress level in MS patients. The accuracy of estimating MS patients&#39; stress levels after watching different simulated VR environments is higher than 97%. By employing this method to classify VR scenes better and rehabilitate MS patients, it will be possible to significantly reduce their stress levels.},
  archive      = {J_COIN},
  author       = {Khosro Rezaee and Shina Zolfaghari},
  doi          = {10.1111/coin.12480},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {249-268},
  shortjournal = {Comput. Intell.},
  title        = {A direct classification approach to recognize stress levels in virtual reality therapy for patients with multiple sclerosis},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Telemedicine virtual reality based skin image in children’s
dermatology medical system. <em>COIN</em>, <em>38</em>(1), 229–248. (<a
href="https://doi.org/10.1111/coin.12458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, with the rapid development of computer and communication technology and people&#39;s demand for medical technology services, telemedicine virtual reality technology has become a high-tech hot spot in the information age and a new trend in the development of the medical industry. However, my country&#39;s high-quality medical resources are mainly concentrated in large and medium-sized cities, and there is an urgent need to improve the fairness of medical services. Telemedicine virtual reality technology has received more and more attention from people, has been widely used worldwide, and has become an important part of hospital information engineering. This article first introduces the basic concepts of telemedicine virtual reality technology, the latest development of telemedicine virtual reality technology and related technologies, and then introduces the system in detail. The system is functionally divided into two parts: consulting management and system management. Consulting management includes all remote business consulting processes; system management includes management of medical resources and user data, and then detailed description and visualization of the development and use of disease model libraries. Telemedicine completely separates the rehabilitation area of skin patients from the hospital, and patients can complete relief and rehabilitation outdoors or even when they are discharged from the hospital. As a remote access device, telemedicine virtual reality technology has been popularized to a certain extent. The general patient group represented by patients with stable skin conditions has improved their understanding of telemedicine technology, which is beneficial to the development of telemedicine virtual reality technology. The utilization rate, safety, rehabilitation effect and completion rate of telemedicine virtual reality technology are as high as 85%, 92%, and 96%, respectively, saving a lot of time and medical expenses. Therefore, telemedicine virtual reality technology is also the direction of my country&#39;s next medical reform, and it can also be used in other aspects of continuing education of telemedicine virtual reality technology.},
  archive      = {J_COIN},
  author       = {Juanjuan Gao and Chong Lyu and Xianhua Qiao and Fei Tian},
  doi          = {10.1111/coin.12458},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {229-248},
  shortjournal = {Comput. Intell.},
  title        = {Telemedicine virtual reality based skin image in children&#39;s dermatology medical system},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). To assess the multiperiod market risk with deep learning
method taking the boosting additive quantile regression as an example.
<em>COIN</em>, <em>38</em>(1), 216–228. (<a
href="https://doi.org/10.1111/coin.12456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with current risks, future risks are more important for investment decisions and risk management. This paper modifies the Square Root of Time Rule with the boosting additive quantile regression model to forecast multi-period horizon market risk. In J. P. Morgan&#39;s RiskMetrics Model, the k -period horizon value at risk (VaR) equals to . Since its assumptions are too strict, expected capacity of Risk Metrics is not so well. Taylor relaxed assumptions of this model, used the GARCH model to replace the IGARCH model, and obtained multi-period horizon VaR, which is a nonlinear function of the one-step-ahead volatility forecast .The conditional mean μ t is zero in Taylor&#39;s model, but Tsay pointed out that this assumption ( μ t = 0) does not always hold. Therefore, we relax this assumption about the conditional mean, and obtain the VaR which is mixed function consisting of two parts, one is a linear function of conditional mean, and the other is a nonlinear function of , given the holding horizon k . For our mixed VaR function, we chose a more appropriate method, the boosting additive quantile regression model, to forecast multi-period horizon VaR. Taking log-returns of the Hang Seng Index from January 1, 2007 to November 1, 2016 as the sample, weforecast the 5-, 10-, 15-, and 20-day horizon VaRs, and compare the prediction accuracy of Morgan&#39;s model withour quantile regression model through likelihood ratio tests. Results show that VaR based on the quantile regression model is not only more accurate, but also sensitive to volatility, and is conducive to maintaining a reasonable risk reserve level for financial institutions, enabling them to pay less for regulation andachieve incentive compatibility.},
  archive      = {J_COIN},
  author       = {Min Guan},
  doi          = {10.1111/coin.12456},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {216-228},
  shortjournal = {Comput. Intell.},
  title        = {To assess the multiperiod market risk with deep learning method taking the boosting additive quantile regression as an example},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Text segmentation for patent claim simplification via
bidirectional long-short term memory and conditional random field.
<em>COIN</em>, <em>38</em>(1), 205–215. (<a
href="https://doi.org/10.1111/coin.12455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text simplification is a vital work for comprehending patent claims due to its complex syntactic structures and lengthy sentences. Therefore, almost all patent analysis practitioners cannot be able to directly and intuitively understand patent essence even through some common natural language processing (NLP) tools are applied to parse these patent claim paragraph or sentences. Universal text analysis tools above is almost useless, or even crashed when applied to some complex paragraphs of patent claims. Therefore, it is necessary to propose a patent text oriented simplification approach to help patent researchers grasp the essence of patent quickly and intuitively. Motivated by the above reason, we in this article propose a simplification method based on deep learning to segment patent claim into shorter and comprehensible sentences for downstream tasks of patent analysis. The proposed approach contains two stages: on one stage, we use a machine learning approach of conditional random field (CRF) to decompose syntactically complex paragraphs into coarse-grained level sentences with simplified structures and complete semantics; on another stage, a deep Learning architecture of bidirectional long-short term memory (Bi-LSTM)-CRF is applied to segment coarse-grained and lengthy sentences of former stage into fined-grained and shorter sentences. Compared with a series of baselines, our patent segmentation architecture based on deep learning of Bi-LSTM-CRF achieves higher performance than any other methods on the evaluation measures of precision, recall, and F1.},
  archive      = {J_COIN},
  author       = {Boting Geng},
  doi          = {10.1111/coin.12455},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {205-215},
  shortjournal = {Comput. Intell.},
  title        = {Text segmentation for patent claim simplification via bidirectional long-short term memory and conditional random field},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning based six-dimensional pose estimation in
virtual reality. <em>COIN</em>, <em>38</em>(1), 187–204. (<a
href="https://doi.org/10.1111/coin.12453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality technology, with its continuous development, is gradually applied to healthcare, education, business, and other fields. In the application of the technology, position and attitude estimation, as a space positioning technology, is indispensable. Traditional pose estimation has the problems of high dependence on environment and great complexity. But convolutional neural network (CNN) and other technologies with computational intelligence provide a strong guarantee for the progress of pose estimation. This article, based on the theory of CNN in deep learning, as well as monocular vision system and target sample set with markers, proposes a method for estimation of target position and attitude, and at the same time, describes in detail a general way of making dataset with markers based on simulation environment. In this article, the comparative experiments of different network structures show that this measurement method can avoid manual extraction of complex image features, and realize fast, arbitrary and accurate measurement, which plays a key role in pose and attitude measurement. Moreover, the visual correspondence between the world coordinate system and the pixel coordinate system is proved effectively by quaternion.},
  archive      = {J_COIN},
  author       = {Jiachen Yang and Yutian Lei and Ying Tian and Meng Xi},
  doi          = {10.1111/coin.12453},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {187-204},
  shortjournal = {Comput. Intell.},
  title        = {Deep learning based six-dimensional pose estimation in virtual reality},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comprehensive data-level investigation of cancer diagnosis
on imbalanced data. <em>COIN</em>, <em>38</em>(1), 156–186. (<a
href="https://doi.org/10.1111/coin.12452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is one of the leading causes of death in the world. Cancer research is vital as the prognosis of cancer enables clinical applications for patients. In this study, we have proposed the Stacked Ensemble Model (Stacking of bagged and boosted learners) for the automatic disease diagnosis. The experimental results prove the superiority of the proposed method to conventional machine learning techniques. In the empirical study, the performance of eight data handling methods and 14 classification methods is compared to obtain prediction results. The performance of the model has been evaluated on five benchmark datasets. The appreciable Area under the Curve scores achieved by the proposed methodology on Cervical Cancer (0.98), Mesothelioma (0.93), Breast Cancer (0.99), Prostate Cancer (0.97), and Hepatitis-C Virus (0.998) datasets make this work more significant than the previously published works. The experimental results show that our proposed method is superior to conventional machine learning techniques and the proposed model contributes in the form of an efficient computational model.},
  archive      = {J_COIN},
  author       = {Surbhi Gupta and Manoj Kumar Gupta},
  doi          = {10.1111/coin.12452},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {156-186},
  shortjournal = {Comput. Intell.},
  title        = {A comprehensive data-level investigation of cancer diagnosis on imbalanced data},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Brain–computer interface-based target recognition system
using transfer learning: A deep learning approach. <em>COIN</em>,
<em>38</em>(1), 139–155. (<a
href="https://doi.org/10.1111/coin.12451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional target recognition and classification is mostly done manually, with low efficiency and high cost. Improving the level of target recognition automatically has become an important research topic. This paper proposes a target recognition method based on transfer learning to effectively complete the classification and recognition of targets using a brain–computer interface (BCI) model. Based on the construction of the faster-RCNN deep learning model, the pre-training of the model is achieved by VGG-16 and Inception-v2, and the transfer learning algorithm is used to optimize the faster-RCNN deep learning model based on the kinematics model. Experiments are carried out with the aim to detect tableware by the persons whose brain signals recognition rate has been substantially improved using faster-RCNN. Compared with the traditional recognition methods, the results at the lab-scale level illustrated that the proposed algorithm can effectively improve the speed and accuracy of target recognition by using the BCI model to classify tableware of different colors and shapes in a complex background.},
  archive      = {J_COIN},
  author       = {Ning Chen and Yimeng Zhang and Jielong Wu and Hongyi Zhang and Vinay Chamola and Victor Hugo C. de Albuquerque},
  doi          = {10.1111/coin.12451},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {139-155},
  shortjournal = {Comput. Intell.},
  title        = {Brain–computer interface-based target recognition system using transfer learning: A deep learning approach},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contrast-enhanced ultrasound combined with augmented reality
medical technology in the treatment of rabbit liver cancer with
high-energy focused knife. <em>COIN</em>, <em>38</em>(1), 121–138. (<a
href="https://doi.org/10.1111/coin.12450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented reality technology can enhance people&#39;s perception and interactive experience of real scenes due to its characteristics of fusion of virtual and real, and has become a popular technology in the field of computer science. This article mainly studies the application of contrast-enhanced ultrasound combined with augmented reality medical technology in the treatment of rabbit liver cancer with high-energy focused knife. Sixty healthy male New Zealand white rabbits were used in this study. Seldinger&#39;s method was used to superselective embolize the artery supplying tumor with the help of hepatic angiography, which can kill tumor cells to the greatest extent and reduce the damage to normal liver cells. At the same time, another assistant under the guidance of ultrasound inserted the temperature probe into the position 1 cm away from the microwave radiation source, and then started the microwave ablation (MWA) treatment. High intensity focused ultrasound (HIFU) treatment is carried out under general anesthesia, the breathing rhythm of patients is controlled by ventilator, the proper position is set before operation, the real-time monitoring is carried out by ultrasound monitoring mediated by medium water, and the HIFU three-dimensional scanning treatment system is used to accurately cover all tumor treatment areas from point to line and from line to surface, so as to kill tumor cells to the greatest extent. The data of each group were processed by SPSS17.0 statistical software, and the measurement data were expressed as mean ± SD. ANOVA analysis of variance was used to compare the differences among the groups, and LSD method was used to make pairwise comparison within the group. p &lt; 0.05 was considered as having statistical difference. The growth rate of tumor was 56.1% and 60.3%, respectively. Contrast-enhanced ultrasound combined with augmented reality medical technology provides a new way for the treatment of rabbit liver cancer.},
  archive      = {J_COIN},
  author       = {Fengyu Na and Li Wang and Cuicui Wu and Yan Ding},
  doi          = {10.1111/coin.12450},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {121-138},
  shortjournal = {Comput. Intell.},
  title        = {Contrast-enhanced ultrasound combined with augmented reality medical technology in the treatment of rabbit liver cancer with high-energy focused knife},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Point cloud computing algorithm on object surface based on
virtual reality technology. <em>COIN</em>, <em>38</em>(1), 106–120. (<a
href="https://doi.org/10.1111/coin.12449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality technology is also known as VR technology. Accurately acquiring the three-dimensional model of the point cloud on the surface of an object is a key part of virtual reality technology, and is also a research hotspot of virtual reality technology. The purpose of this article is to study the object surface point cloud computing algorithm based on virtual reality technology and its application problems and to solve the problems in the virtual reality technology object surface point cloud data algorithm, by reconstructing the geometry of the object surface in VR technology. The calculation rules at the time of data acquisition have been carried out in detailed research on the point cloud space carving algorithm based on the surface of multi-eye visual objects. The research results show that the point cloud space carving algorithm on the surface of the virtual reality technology has a high degree of independence and the algorithm is very flexible. Compared with other algorithms, it reduces the operation steps by 35%, the 3D effect is increased by 21%, and the operation cost is reduced by 30%. The object surface point cloud computing algorithm based on virtual reality technology can greatly improve the computing efficiency, compared with the traditional two-dimensional visual image integration algorithm, the efficiency is increased by 35%, and the computing accuracy in virtual reality technology is the same as the traditional two The accuracy of the integrated algorithm of dimensional visual pictures is improved by 44% compared to the accuracy. The experimental results verify the feasibility of applying the spatial carving algorithm to the three-dimensional reconstruction of point cloud data on the surface of virtual reality technology objects.},
  archive      = {J_COIN},
  author       = {Wanyi Zhang and Xiuhua Fu and Wei Li},
  doi          = {10.1111/coin.12449},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {106-120},
  shortjournal = {Comput. Intell.},
  title        = {Point cloud computing algorithm on object surface based on virtual reality technology},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Animation of virtual medical system under the background of
virtual reality technology. <em>COIN</em>, <em>38</em>(1), 88–105. (<a
href="https://doi.org/10.1111/coin.12446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality technology is a technology that uses a variety of scientific technologies such as computer systems, image displays, and sensors to perform human–computer interaction. Since the advent of virtual reality technology, the virtual reality + medical model has been highly expected. At present, three-dimensional (3D) animation technology can be used in combination with virtual reality technology in medical systems. 3D animation technology can transform the transformed human medical image into a 3D character model, and animate the corresponding bones, and observe the model on the screen and operate the model like real surgery, completing virtual medical content learning, virtual medical skill training, virtual surgery preview. Applying 3D animation technology to virtual reality surgery training can effectively bypass the limitations of traditional surgery training modes, form multiple interactive real-time effects, and bring a rich perceptual experience to the audience. Based on the above background, the research content of this article is the study of virtual medical system animation under the background of virtual reality technology. This paper analyzes the feasibility of virtual reality technology and 3D animation modeling, and designs a virtual medical system based on this. Finally, through experimental simulations, the results show that the gap h is getting larger and larger after the first approximation, and the gap between the Euler explicit method and the true value is larger, while the implicit method is more stable, and the gap does not increases too quickly with the increase of h . Hence choosing the implicit method to calculate Δ t to obtain a larger value will not affect the stability of the solution.},
  archive      = {J_COIN},
  author       = {Liang Li and Tingting Li},
  doi          = {10.1111/coin.12446},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {88-105},
  shortjournal = {Comput. Intell.},
  title        = {Animation of virtual medical system under the background of virtual reality technology},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design of intervention APP for children with autism based on
visual cue strategy. <em>COIN</em>, <em>38</em>(1), 70–87. (<a
href="https://doi.org/10.1111/coin.12445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of the mobile Internet, APP treatment models for autism have also emerged. In China, there are not many applications of health education for children with autism. Its functions are mainly focused on prevention and improvement of knowledge. Many behavioral therapies include applying pressure to the invisible external pressure of autism to achieve appropriate stimulation and correction. This has brought great injustice to the wishes of the children. When children with autism strongly oppose this unfair behavior, it may lead to other extreme behaviors in children with autism. This article adopts the method of letting children with autism actually apply this APP for comprehensive intervention treatment, and let this APP combine with their daily courses: comprehensive sports training, speech training, cognitive training, game training, self-care training. For courses such as art training and computer games, the children were evaluated for the first time before training, and then evaluated every 2 months. Data were collected to compare whether the children&#39;s background factors had a significant impact on the rehabilitation effect of comprehensive interventions. The results of the experiment proved that in the five assessments of perception, gross motor, fine motor, language and communication, cognition, the children had statistically significant differences in the three scores ( P &lt; 0.05). This also shows from the side that our APP has a certain effect on children with autism, and is of great strategic significance for adjuvant treatment of autism.},
  archive      = {J_COIN},
  author       = {Bingchen Zhang and Yanqun Wang},
  doi          = {10.1111/coin.12445},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {70-87},
  shortjournal = {Comput. Intell.},
  title        = {Design of intervention APP for children with autism based on visual cue strategy},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Realization of interactive animation creation based on
artificial intelligence technology. <em>COIN</em>, <em>38</em>(1),
51–69. (<a href="https://doi.org/10.1111/coin.12443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In film and television animation works, animated characters are the soul and core of the work. The behavior, language expression, and emotional expression of animated characters play an important role in the expression of the animation theme and content. Aiming at the problem that the mobile animation system can only add and change actions for a single virtual character, and the characters cannot interact with each other, this paper analyzes the technical principles, technical characteristics, and application scope of human–computer interaction (HCI), taking sensors as the research object. An algorithm for separating the human body from the background environment in the depth image is proposed. Through the calculation of the depth value, the calculation results are compared, and the target human body and the background are effectively separated. In the depth data processing, the algorithm of judging the pixel offset value is used to identify the body part, and a sensor-based HCI system is designed. The depth-of-field data map acquired by the sensor is used to identify human body parts and determine actions, thereby realizing HCI based on action recognition. Simulation test results show that the effective rate of the system is 80%, and the design of animated characters can be put into the visualization stage. Using the algorithm in this paper, the physical signs of the animated characters can be quickly identified, so that the next action of the animation can be more clearly captured. Has a certain practical value.},
  archive      = {J_COIN},
  author       = {YuXin Cai and Haibin Dong and Wei Wang and Hongchang Song},
  doi          = {10.1111/coin.12443},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {51-69},
  shortjournal = {Comput. Intell.},
  title        = {Realization of interactive animation creation based on artificial intelligence technology},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Test items of 3D printed copper alloy parts based on virtual
reality technology. <em>COIN</em>, <em>38</em>(1), 38–50. (<a
href="https://doi.org/10.1111/coin.12442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, with the rapid development of virtual reality technology, 3D printing technology has gradually gained people&#39;s attention. Copper alloy parts have a wide range of uses in our lives, but due to the high technology required, production is more difficult. In order to make copper alloy parts more widely used, this paper uses 3D printing technology to print copper alloy parts, and through comparative experiments, the printed copper alloy parts are analyzed in detail. In the printing process, by adjusting the printing parameters, different parts are printed, and the printing results are analyzed for different parameters. The experimental results show that the experimental 3D printing technology can successfully print copper alloy parts. During the printing process, the input parameters have an extremely important effect on the results. The experimental results show that there are related parameters in the printing process, such as layer height, length, density, etc., which will cause deviations in the printing results. Among them, the layer height and length have the most important effect on the results, accounting for more than 50% of the deviation value. This shows that based on virtual reality technology, the use of 3D printing copper alloy parts is achievable.},
  archive      = {J_COIN},
  author       = {Jiaofei Huo and Guangpeng Zhang},
  doi          = {10.1111/coin.12442},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {38-50},
  shortjournal = {Comput. Intell.},
  title        = {Test items of 3D printed copper alloy parts based on virtual reality technology},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-angle face expression recognition based on generative
adversarial networks. <em>COIN</em>, <em>38</em>(1), 20–37. (<a
href="https://doi.org/10.1111/coin.12437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of the different features of the same facial expressions from different angles, most of the methods are only suitable for face images, and the accuracy of facial expression recognition is low. Therefore, a multi-angle facial expression recognition method based on generative adversarial networks (GAN) is proposed. Firstly, the depth regression network is used to detect the key points of the face image to achieve face alignment, so as to reduce the difficulty of feature extraction. Then, the image is input to GAN. The generator is composed of encoder and decoder, and a skip connection is designed. In the encoding phase, the generator can unlock the correlation between the facial expression image and the angle, and in the decoding stage, it can generate different angle facial expression images by adding other angle information. Finally, the multi-angle facial expression images are sent to the convolution neural network for classification and learning, in which the loss weight is adjusted dynamically to improve the recognition accuracy by introducing resistance loss, recognition loss, content loss, and center loss. The experimental results on Multi-pose illumination expression (PIE) and celebrities in frontal profile (CFP) datasets show that the performance of the proposed method is the best when the learning rate is 0.0002, and the recognition accuracy under different angles is higher than other comparison methods, so it has practical application significance.},
  archive      = {J_COIN},
  author       = {Lihua Lu},
  doi          = {10.1111/coin.12437},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {20-37},
  shortjournal = {Comput. Intell.},
  title        = {Multi-angle face expression recognition based on generative adversarial networks},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multilayer weighted integrated self-learning algorithm for
automatic diagnosis of epileptic electroencephalogram signals.
<em>COIN</em>, <em>38</em>(1), 3–19. (<a
href="https://doi.org/10.1111/coin.12414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epilepsy is a common mental disorder that affects about 70 million people worldwide. Epileptic electroencephalogram (EEG) signal, an important means to judge epileptic seizure, needs neurologists&#39; prior knowledge to mark manually. This marking method is time-consuming and laborious. Currently, the existing automated diagnosis methods have achieved good results on one benchmark EEG dataset, most of which can achieve accuracy of more than 0.95. However, the method has limitations on the dataset, and the accuracy of the diagnosis results on another new dataset drops sharply to nearly 0.5. Aiming at the existing EEG signal diagnosis lacks stability and generalization ability, this paper proposed a multilayer-weighted integrated self-learning algorithm for different classifiers. For this algorithm, weighted voting was first conducted on the the diagnostic results by different classifiers to obtain a result, which was weighted again to produce the final diagnostic results. This algorithm improves the problem that the traditional self-learning algorithm is greatly affected by data noise, which shows a strong stability in different data sets and in clinical epileptic EEG signal data detection, so as to reduce the workload of neurologists and provide support and assistance for the diagnosis and treatment of epilepsy. The experiment result shows that the algorithm can improve the stability and reliability of EEG automatic diagnosis of epilepsy. The accuracy and AUC area of its classification in two different public data sets and clinical data can reach 0.80 to 0.95.},
  archive      = {J_COIN},
  author       = {Jian Zhao and Di Zhao and Lijuan Shi and Zhejun Kuang and Weipeng Jing and Huihui Wang},
  doi          = {10.1111/coin.12414},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {3-19},
  shortjournal = {Comput. Intell.},
  title        = {Multilayer weighted integrated self-learning algorithm for automatic diagnosis of epileptic electroencephalogram signals},
  volume       = {38},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
