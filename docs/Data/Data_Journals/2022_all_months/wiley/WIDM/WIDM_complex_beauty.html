<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>WIDM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="widm---44">WIDM - 44</h2>
<ul>
<li><details>
<summary>
(2022). Automatic diagnosis of sleep apnea from biomedical signals
using artificial intelligence techniques: Methods, challenges, and
future works. <em>WIDM</em>, <em>12</em>(6), e1478. (<a
href="https://doi.org/10.1002/widm.1478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Apnea is a sleep disorder that stops or reduces airflow for a short time during sleep. Sleep apnea may last for a few seconds and happen for many while sleeping. This reduction in breathing is associated with loud snoring, which may awaken the person with a feeling of suffocation. So far, a variety of methods have been introduced by researchers to diagnose sleep apnea, among which the polysomnography (PSG) method is known to be the best. Analysis of PSG signals is very complicated. Many studies have been conducted on the automatic diagnosis of sleep apnea from biological signals using artificial intelligence (AI), including machine learning (ML) and deep learning (DL) methods. This research reviews and investigates the studies on the diagnosis of sleep apnea using AI methods. First, computer aided diagnosis system (CADS) for sleep apnea using ML and DL techniques along with its parts including dataset, preprocessing, and ML and DL methods are introduced. This research also summarizes the important specifications of the studies on the diagnosis of sleep apnea using ML and DL methods in a table. In the following, a comprehensive discussion is made on the studies carried out in this field. The challenges in the diagnosis of sleep apnea using AI methods are of paramount importance for researchers. Accordingly, these obstacles are elaborately addressed. In another section, the most important future works for studies on sleep apnea detection from PSG signals and AI techniques are presented. Ultimately, the essential findings of this study are provided in the conclusion section. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Parisa Moridian and Afshin Shoeibi and Marjane Khodatars and Mahboobeh Jafari and Ram Bilas Pachori and Ali Khadem and Roohallah Alizadehsani and Sai Ho Ling},
  doi          = {10.1002/widm.1478},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1478},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Automatic diagnosis of sleep apnea from biomedical signals using artificial intelligence techniques: Methods, challenges, and future works},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Short-term photovoltaic power forecasting with adaptive
stochastic configuration network ensemble. <em>WIDM</em>,
<em>12</em>(6), e1477. (<a
href="https://doi.org/10.1002/widm.1477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The volatility and intermittency of solar energy seriously restrict the development of the photovoltaic (PV) industry. Accurate forecast of short-term PV power generation is essential for the optimal balance and dispatch of power plants in the smart grid. This article presents a machine learning approach for analyzing the volt-ampere characteristics and influential factors on PV data. A correlation analysis is employed to discover some hidden characteristic variables. Then, an adaptive ensemble method with stochastic configuration networks as base models (AE-SCN) is proposed to construct the PV prediction model, which integrates bagging and adaptive weighted data fusion algorithms. Compared with the original SCN, SCN ensemble (SCNE) and random vector functional-link network (RVFLN), linear regression model, random forest model and autoregressive integrated moving average (ARMA) model, AE-SCN performs favorably in the terms of the prediction accuracy. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Xifeng Guo and Xinlu Wang and Yanshuang Ao and Wei Dai and Ye Gao},
  doi          = {10.1002/widm.1477},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1477},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Short-term photovoltaic power forecasting with adaptive stochastic configuration network ensemble},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Review of automated time series forecasting pipelines.
<em>WIDM</em>, <em>12</em>(6), e1475. (<a
href="https://doi.org/10.1002/widm.1475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting is fundamental for various use cases in different domains such as energy systems and economics. Creating a forecasting model for a specific use case requires an iterative and complex design process. The typical design process includes five sections (1) data preprocessing, (2) feature engineering, (3) hyperparameter optimization, (4) forecasting method selection, and (5) forecast ensembling, which are commonly organized in a pipeline structure. One promising approach to handle the ever-growing demand for time series forecasts is automating this design process. The article, thus, reviews existing literature on automated time series forecasting pipelines and analyzes how the design process of forecasting models is currently automated. Thereby, we consider both automated machine learning (AutoML) and automated statistical forecasting methods in a single forecasting pipeline. For this purpose, we first present and compare the identified automation methods for each pipeline section. Second, we analyze these automation methods regarding their interaction, combination, and coverage of the five pipeline sections. For both, we discuss the reviewed literature that contributes toward automating the design process, identify problems, give recommendations, and suggest future research. This review reveals that the majority of the reviewed literature only covers two or three of the five pipeline sections. We conclude that future research has to holistically consider the automation of the forecasting pipeline to enable the large-scale application of time series forecasting. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Stefan Meisenbacher and Marian Turowski and Kaleb Phipps and Martin Rätz and Dirk Müller and Veit Hagenmeyer and Ralf Mikut},
  doi          = {10.1002/widm.1475},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1475},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Review of automated time series forecasting pipelines},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on artificial intelligence in histopathology image
analysis. <em>WIDM</em>, <em>12</em>(6), e1474. (<a
href="https://doi.org/10.1002/widm.1474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing adoption of the whole slide image (WSI) technology in histopathology has dramatically transformed pathologists&#39; workflow and allowed the use of computer systems in histopathology analysis. Extensive research in Artificial Intelligence (AI) with a huge progress has been conducted resulting in efficient, effective, and robust algorithms for several applications including cancer diagnosis, prognosis, and treatment. These algorithms offer highly accurate predictions but lack transparency, understandability, and actionability. Thus, explainable artificial intelligence (XAI) techniques are needed not only to understand the mechanism behind the decisions made by AI methods and increase user trust but also to broaden the use of AI algorithms in the clinical setting. From the survey of over 150 papers, we explore different AI algorithms that have been applied and contributed to the histopathology image analysis workflow. We first address the workflow of the histopathological process. We present an overview of various learning-based, XAI, and actionable techniques relevant to deep learning methods in histopathological imaging. We also address the evaluation of XAI methods and the need to ensure their reliability on the field. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Mohammed M. Abdelsamea and Usama Zidan and Zakaria Senousy and Mohamed Medhat Gaber and Emad Rakha and Mohammad Ilyas},
  doi          = {10.1002/widm.1474},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1474},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A survey on artificial intelligence in histopathology image analysis},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Corporate investment prediction using a weighted temporal
graph neural network. <em>WIDM</em>, <em>12</em>(6), e1472. (<a
href="https://doi.org/10.1002/widm.1472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corporate investment is an important part of corporate financial decision-making and affects the future profit and value of the corporation. Predicting corporate investment provides great significance for capital market investors to understand the future operation and development of a corporation. Many researchers have studied independent prediction methods. However, individual firms imitate each other&#39;s investment in the actual decision-making process. This phenomenon of investment convergence indicates investment correlation among individual firms, which is ignored in these existing methods. In this article, we first identify key variables in multivariate sequences by our designed two-way fixed effects model for precise corporate network construction. Then, we propose a weighted temporal graph neural network called weighted temporal graph neural network (WTGNN) for graph learning and investment prediction over the corporate network. WTGNN improves the graph convolution capability by weighted sampling with attention and multivariate time series aggregation. We conducted extensive experiments using real-world financial reporting data. The results show that WTGNN can achieve excellent graph learning performance and outperforms existing methods in the investment prediction task. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Jianing Li and Xin Yao},
  doi          = {10.1002/widm.1472},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1472},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Corporate investment prediction using a weighted temporal graph neural network},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the application of machine learning in astronomy and
astrophysics: A text-mining-based scientometric analysis. <em>WIDM</em>,
<em>12</em>(5), e1476. (<a
href="https://doi.org/10.1002/widm.1476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the beginning of the 21st century, the fields of astronomy and astrophysics have experienced significant growth at observational and computational levels, leading to the acquisition of increasingly huge volumes of data. In order to process this vast quantity of information, artificial intelligence (AI) techniques are being combined with data mining to detect patterns with the aim of modeling, classifying or predicting the behavior of certain astronomical phenomena or objects. Parallel to the exponential development of the aforementioned techniques, the scientific output related to the application of AI and machine learning (ML) in astronomy and astrophysics has also experienced considerable growth in recent years. Therefore, the increasingly abundant articles make it difficult to monitor this field in terms of which research topics are the most prolific or novel, or which countries or authors are leading them. In this article, a text-mining-based scientometric analysis of scientific documents published over the last three decades on the application of AI and ML in the fields of astronomy and astrophysics is presented. The VOSviewer software and data from the Web of Science (WoS) are used to elucidate the evolution of publications in this research field, their distribution by country (including co-authorship), the most relevant topics addressed, and the most cited elements and most significant co-citations according to publication source and authorship. The obtained results demonstrate how application of AI/ML to the fields of astronomy/astrophysics represents an established and rapidly growing field of research that is crucial to obtaining scientific understanding of the universe. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {José-Víctor Rodríguez and Ignacio Rodríguez-Rodríguez and Wai Lok Woo},
  doi          = {10.1002/widm.1476},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1476},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {On the application of machine learning in astronomy and astrophysics: A text-mining-based scientometric analysis},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Open source intelligence extraction for terrorism-related
information: A review. <em>WIDM</em>, <em>12</em>(5), e1473. (<a
href="https://doi.org/10.1002/widm.1473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this contemporary era, where a large part of the world population is deluged by extensive use of the internet and social media, terrorists have found it a potential opportunity to execute their vicious plans. They have got a befitting medium to reach out to their targets to spread propaganda, disseminate training content, operate virtually, and further their goals. To restrain such activities, information over the internet in context of terrorism needs to be analyzed to channel it to appropriate measures in combating terrorism. Open Source Intelligence (OSINT) accounts for a felicitous solution to this problem, which is an emerging discipline of leveraging publicly accessible sources of information over the internet by effectively utilizing it to extract intelligence. The process of OSINT extraction is broadly observed to be in three phases (i) Data Acquisition, (ii) Data Enrichment, and (iii) Knowledge Inference. In the context of terrorism, researchers have given noticeable contributions in compliance with these three phases. However, a comprehensive review that delineates these research contributions into an integrated workflow of intelligence extraction has not been found. The paper presents the most current review in OSINT, reflecting how the various state-of-the-art tools and techniques can be applied in extracting terrorism-related textual information from publicly accessible sources. Various data mining and text analysis-based techniques, that is, natural language processing, machine learning, and deep learning have been reviewed to extract and evaluate textual data. Additionally, towards the end of the paper, we discuss challenges and gaps observed in different phases of OSINT extraction. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Megha Chaudhary and Divya Bansal},
  doi          = {10.1002/widm.1473},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1473},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Open source intelligence extraction for terrorism-related information: A review},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data mining in predictive maintenance systems: A taxonomy
and systematic review. <em>WIDM</em>, <em>12</em>(5), e1471. (<a
href="https://doi.org/10.1002/widm.1471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive maintenance is a field of study whose main objective is to optimize the timing and type of maintenance to perform on various industrial systems. This aim involves maximizing the availability time of the monitored system and minimizing the number of resources used in maintenance. Predictive maintenance is currently undergoing a revolution thanks to advances in industrial systems monitoring within the Industry 4.0 paradigm. Likewise, advances in artificial intelligence and data mining allow the processing of a great amount of data to provide more accurate and advanced predictive models. In this context, many actors have become interested in predictive maintenance research, becoming one of the most active areas of research in computing, where academia and industry converge. The objective of this paper is to conduct a systematic literature review that provides an overview of the current state of research concerning predictive maintenance from a data mining perspective. The review presents a first taxonomy that implies different phases considered in any data mining process to solve a predictive maintenance problem, relating the predictive maintenance tasks with the main data mining tasks to solve them. Finally, the paper presents significant challenges and future research directions in terms of the potential of data mining applied to predictive maintenance. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Aurora Esteban and Amelia Zafra and Sebastián Ventura},
  doi          = {10.1002/widm.1471},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1471},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Data mining in predictive maintenance systems: A taxonomy and systematic review},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Taxonomy of machine learning paradigms: A data-centric
perspective. <em>WIDM</em>, <em>12</em>(5), e1470. (<a
href="https://doi.org/10.1002/widm.1470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning is a field composed of various pillars. Traditionally, supervised learning (SL), unsupervised learning (UL), and reinforcement learning (RL) are the dominating learning paradigms that inspired the field since the 1950s. Based on these, thousands of different methods have been developed during the last seven decades used in nearly all application domains. However, recently, other learning paradigms are gaining momentum which complement and extend the above learning paradigms significantly. These are multi-label learning (MLL), semi-supervised learning (SSL), one-class classification (OCC), positive-unlabeled learning (PUL), transfer learning (TL), multi-task learning (MTL), and one-shot learning (OSL). The purpose of this article is a systematic discussion of these modern learning paradigms and their connection to the traditional ones. We discuss each of the learning paradigms formally by defining key constituents and paying particular attention to the data requirements for allowing an easy connection to applications. That means, we assume a data-driven perspective. This perspective will also allow a systematic identification of relations between the individual learning paradigms in the form of a learning-paradigm graph (LP-graph). Overall, the LP-graph establishes a taxonomy among 10 different learning paradigms. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Frank Emmert-Streib and Matthias Dehmer},
  doi          = {10.1002/widm.1470},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1470},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Taxonomy of machine learning paradigms: A data-centric perspective},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Artificial intelligence for climate change adaptation.
<em>WIDM</em>, <em>12</em>(5), e1459. (<a
href="https://doi.org/10.1002/widm.1459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although artificial intelligence (AI; inclusive of machine learning) is gaining traction supporting climate change projections and impacts, limited work has used AI to address climate change adaptation. We identify this gap and highlight the value of AI especially in supporting complex adaptation choices and implementation. We illustrate how AI can effectively leverage precise, real-time information in data-scarce settings. We focus on supervised learning, transfer learning, reinforcement learning, and multimodal learning to illustrate how innovative AI methods can enable better-informed choices, tailor adaptation measures to heterogenous groups and generate effective synergies and trade-offs. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {So-Min Cheong and Kris Sankaran and Hamsa Bastani},
  doi          = {10.1002/widm.1459},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1459},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Artificial intelligence for climate change adaptation},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mining the online infosphere: A survey. <em>WIDM</em>,
<em>12</em>(5), e1453. (<a
href="https://doi.org/10.1002/widm.1453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of Artificial Intelligence (AI)-based systems and applications have pervaded everyday life to make decisions that have a momentous impact on individuals and society. With the staggering growth of online data, often termed as the online infosphere , it has become paramount to monitor the infosphere to ensure social good as AI-based decisions are severely dependent. This survey aims to provide a comprehensive review of some of the most important research areas related to the infosphere, focusing on the technical challenges and potential solutions. The survey also outlines some of the important future directions. We begin by focussing on the collaborative systems that have emerged within the infosphere with a special thrust on Wikipedia. In the follow-up, we demonstrate how the infosphere has been instrumental in the growth of scientific citations and collaborations, thus fuelling interdisciplinary research. Finally, we illustrate the issues related to the governance of the infosphere, such as the tackling of the (a) rising hateful and abusive behavior and (b) bias and discrimination in different online platforms and news reporting. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Sayantan Adak and Souvic Chakraborty and Paramita Das and Mithun Das and Abhisek Dash and Rima Hazra and Binny Mathew and Punyajoy Saha and Soumya Sarkar and Animesh Mukherjee},
  doi          = {10.1002/widm.1453},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1453},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Mining the online infosphere: A survey},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Privacy protection in smart meters using homomorphic
encryption: An overview. <em>WIDM</em>, <em>12</em>(4), e1469. (<a
href="https://doi.org/10.1002/widm.1469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an overview of the literature on privacy protection in smart meters with a particular focus on homomorphic encryption (HE). Firstly, we introduce the concept of smart meters, the context in which they are inserted the main concerns and oppositions inherent to its use. Later, an overview of privacy protection is presented, emphasizing the need to safeguard the privacy of smart-meter users by identifying, describing, and comparing the main approaches that seek to address this problem. Then, two privacy protection approaches based on HE are presented in more detail and additionally we present two possible application scenarios. Finally, the article concludes with a brief overview of the unsolved challenges in HE and the most promising future research directions. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Zita Abreu and Lucas Pereira},
  doi          = {10.1002/widm.1469},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1469},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Privacy protection in smart meters using homomorphic encryption: An overview},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Epidemiological challenges in pandemic coronavirus disease
(COVID-19): Role of artificial intelligence. <em>WIDM</em>,
<em>12</em>(4), e1462. (<a
href="https://doi.org/10.1002/widm.1462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {World is now experiencing a major health calamity due to the coronavirus disease (COVID-19) pandemic, caused by the severe acute respiratory syndrome coronavirus clade 2. The foremost challenge facing the scientific community is to explore the growth and transmission capability of the virus. Use of artificial intelligence (AI), such as deep learning, in (i) rapid disease detection from x-ray or computed tomography (CT) or high-resolution CT (HRCT) images, (ii) accurate prediction of the epidemic patterns and their saturation throughout the globe, (iii) forecasting the disease and psychological impact on the population from social networking data, and (iv) prediction of drug–protein interactions for repurposing the drugs, has attracted much attention. In the present study, we describe the role of various AI-based technologies for rapid and efficient detection from CT images complementing quantitative real-time polymerase chain reaction and immunodiagnostic assays. AI-based technologies to anticipate the current pandemic pattern, prevent the spread of disease, and face mask detection are also discussed. We inspect how the virus transmits depending on different factors. We investigate the deep learning technique to assess the affinity of the most probable drugs to treat COVID-19. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Abhijit Dasgupta and Abhisek Bakshi and Srijani Mukherjee and Kuntal Das and Soumyajeet Talukdar and Pratyayee Chatterjee and Sagnik Mondal and Puspita Das and Subhrojit Ghosh and Archisman Som and Pritha Roy and Rima Kundu and Akash Sarkar and Arnab Biswas and Karnelia Paul and Sujit Basak and Krishnendu Manna and Chinmay Saha and Satinath Mukhopadhyay and Nitai P. Bhattacharyya and Rajat K. De},
  doi          = {10.1002/widm.1462},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1462},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Epidemiological challenges in pandemic coronavirus disease (COVID-19): Role of artificial intelligence},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine intelligence in dynamical systems: State-of-art
review. <em>WIDM</em>, <em>12</em>(4), e1461. (<a
href="https://doi.org/10.1002/widm.1461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is dedicated to study the impact of machine intelligence (MI) methods viz. various types of Neural models for investigating dynamical systems arising in interdisciplinary areas. Different types of artificial neural network (ANN) methods, viz., recurrent neural network, functional-link neural network, convolutional neural network, symplectic artificial neural network, genetic algorithm neural network, and so on, are addressed by different researchers to investigate these problems. Although various traditional methods have been developed by researchers to solve these dynamical problems but the existing traditional methods may sometimes be problem dependent, require repetitions of the simulations, and fail to solve nonlinearity behavior. In this regard, neural network model based methods are more general and solutions are continuous over the given domain of integration, self-adaptive and can be used as a black box. As such, in this article, we have reviewed and analyzed different MI methods, which are applied to investigate these problems. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Arup Kumar Sahoo and Snehashish Chakraverty},
  doi          = {10.1002/widm.1461},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1461},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Machine intelligence in dynamical systems: \A state-of-art review},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Critical review of bio-inspired data optimization
techniques: An image steganalysis perspective. <em>WIDM</em>,
<em>12</em>(4), e1460. (<a
href="https://doi.org/10.1002/widm.1460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image steganalysis involves the discovery of secret information embedded in an image. The common method is blind image steganalysis, which is a two-class classification problem. Blind steganalysis extracts all possible feature variations in an image due to embedding, select the most appropriate feature data, and then classifies the image. The dimensionality of the extracted image features are high and demand data reduction to identify the most relevant features and to aid accurate classification of an image. The classification is under two classes namely, clean (cover) image and stego (image with embedded secret data) image. Since the classification accuracy depends on selection of most appropriate features, opting for the best data reduction or data optimization algorithms becomes a prime requisite. Research shows that most of the statistical optimization techniques converge to local minima and lead to less classification accuracy as compared to bio-inspired methods. Bio-inspired optimization methods obtain improved classification accuracy by reducing the high-dimensional image features. These methods start with an initial population and then optimize them in steps till a global optimal point is reached. Examples of such methods include Ant Lion Optimization (ALO), Fire Fly Algorithm (FFA), and literature shows around 54 such algorithms. Bio-inspired optimization has been applied in various fields of design optimization and is novel to image steganalysis. This article analyses the various bio-inspired optimization techniques and their accuracy in image steganalysis pertaining to the discovery of embedded information in both JPEG and spatial domain steganalysis. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Anita Christaline Johnvictor and Austin Joe Amalanathan and Ramya Meghana Pariti Venkata and Nishtha Jethi},
  doi          = {10.1002/widm.1460},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1460},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Critical review of bio-inspired data optimization techniques: An image steganalysis perspective},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A review on data fusion in multimodal learning analytics and
educational data mining. <em>WIDM</em>, <em>12</em>(4), e1458. (<a
href="https://doi.org/10.1002/widm.1458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The new educational models such as smart learning environments use of digital and context-aware devices to facilitate the learning process. In this new educational scenario, a huge quantity of multimodal students&#39; data from a variety of different sources can be captured, fused, and analyze. It offers to researchers and educators a unique opportunity of being able to discover new knowledge to better understand the learning process and to intervene if necessary. However, it is necessary to apply correctly data fusion approaches and techniques in order to combine various sources of multimodal learning analytics (MLA). These sources or modalities in MLA include audio, video, electrodermal activity data, eye-tracking, user logs, and click-stream data, but also learning artifacts and more natural human signals such as gestures, gaze, speech, or writing. This survey introduces data fusion in learning analytics (LA) and educational data mining (EDM) and how these data fusion techniques have been applied in smart learning. It shows the current state of the art by reviewing the main publications, the main type of fused educational data, and the data fusion approaches and techniques used in EDM/LA, as well as the main open problems, trends, and challenges in this specific research area. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Wilson Chango and Juan A. Lara and Rebeca Cerezo and Cristóbal Romero},
  doi          = {10.1002/widm.1458},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1458},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A review on data fusion in multimodal learning analytics and educational data mining},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A review of bus arrival time prediction using artificial
intelligence. <em>WIDM</em>, <em>12</em>(4), e1457. (<a
href="https://doi.org/10.1002/widm.1457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Buses are one of the important parts of public transport system. To provide accurate information about bus arrival and departure times at bus stops is one of the main parameters of good quality public transport. Accurate arrival and departure times information is important for a public transport mode since it enhances ridership as well as satisfaction of travelers. With accurate arrival-time and departure time information, travelers can make informed decisions about their journey. The application of artificial intelligence (AI) based methods/algorithms to predict the bus arrival time (BAT) is reviewed in detail. Systematic survey of existing research conducted by various researchers by applying the different branches of AI has been done. Prediction models have been segregated and are accumulated under respective branches of AI. Thorough discussion is presented to elaborate different branches of AI that have been applied for several aspects of BAT prediction. Research gaps and possible future directions for further research work are summarized. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Nisha Singh and Kranti Kumar},
  doi          = {10.1002/widm.1457},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1457},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A review of bus arrival time prediction using artificial intelligence},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Review and data mining of linguistic studies of english
modal verbs. <em>WIDM</em>, <em>12</em>(4), e1455. (<a
href="https://doi.org/10.1002/widm.1455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modal verbs express modality, and modality is concerned with the status of the proposition that describes an event, it also expresses the opinion and attitude of a speaker toward the proposition of an utterance. Since modalities are directly related to the objective world, subjective world, and language use, they have been a hot topic of philosophers, logicians and linguists. Philosophers concern with the relations between the objective world and the true/false values of the modality; logicians are interested in the relations among the possibility, necessity and the objective world; and linguists pay attention to the modality category, sense category, function, recognition, and application of modal verbs. In recent years, the linguistic studies of modal verbs have extended from general linguistic studies to computational linguistic studies. Since modal verbs are a complex semantic system and they are often indeterminate in senses, they have been a tough issue in linguistic studies and have attracted great attention. To clarify the status of the previous linguistic studies of modal verbs and reveal the characteristics of the studies will be of great significance for the further study. Therefore, this article will focus on the review of the previous linguistic studies of English modal verbs and the data mining of the characteristics of the previous studies, and based on the summary of the previous studies, give suggestions for the further study of the English modal verbs. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Jianping Yu and Jilin Fu and Tana Bai and Xueping Xu},
  doi          = {10.1002/widm.1455},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1455},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Review and data mining of linguistic studies of english modal verbs},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Subgraph mining in a large graph: A review. <em>WIDM</em>,
<em>12</em>(4), e1454. (<a
href="https://doi.org/10.1002/widm.1454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large graphs are often used to simulate and model complex systems in various research and application fields. Because of its importance, frequent subgraph mining (FSM) in single large graphs is a vital issue, and recently, it has attracted numerous researchers, and played an important role in various tasks for both research and application purposes. FSM is aimed at finding all subgraphs whose number of appearances in a large graph is greater than or equal to a given frequency threshold. In most recent applications, the underlying graphs are very large, such as social networks, and therefore algorithms for FSM from a single large graph have been rapidly developed, but all of them have NP-hard (nondeterministic polynomial time) complexity with huge search spaces, and therefore still need a lot of time and memory to restore and process. In this article, we present an overview of problems of FSM, important phases in FSM, main groups of FSM, as well as surveying many modern applied algorithms. This includes many practical applications and is a fundamental premise for many studies in the future. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Lam B. Q. Nguyen and Ivan Zelinka and Vaclav Snasel and Loan T. T. Nguyen and Bay Vo},
  doi          = {10.1002/widm.1454},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1454},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Subgraph mining in a large graph: A review},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comprehensive review on arabic word sense disambiguation
for natural language processing applications. <em>WIDM</em>,
<em>12</em>(4), e1447. (<a
href="https://doi.org/10.1002/widm.1447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In communication, textual data are a vital attribute. In all languages, ambiguous or polysemous words&#39; meaning changes depending on the context in which they are used. The ability to determine the ambiguous word&#39;s correct meaning is a Know-distill challenging task in natural language processing (NLP). Word sense disambiguation (WSD) is an NLP process to analyze and determine the correct meaning of polysemous words in a text. WSD is a computational linguistics task that automatically identifies the polysemous word&#39;s set of senses. Based on the context some word comes into view, WSD recognizes and tags the word to its correct priori known meaning. Semitic languages like Arabic have even more significant challenges than other languages since Arabic lacks diacritics, standardization, and a massive shortage of available resources. Recently, many approaches and techniques have been suggested to solve word ambiguity dilemmas in many different ways and several languages. In this review paper, an extensive survey of research works is presented, seeking to solve Arabic word sense disambiguation with the existing AWSD datasets. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Sanaa Kaddoura and Rowanda D. Ahmed and Jude Hemanth D.},
  doi          = {10.1002/widm.1447},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1447},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A comprehensive review on arabic word sense disambiguation for natural language processing applications},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gaining insights in datasets in the shade of “garbage in,
garbage out” rationale: Feature space distribution fitting.
<em>WIDM</em>, <em>12</em>(3), e1456. (<a
href="https://doi.org/10.1002/widm.1456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article emphasizes comprehending the “Garbage In, Garbage Out” (GIGO) rationale and ensuring the dataset quality in Machine Learning (ML) applications to achieve high and generalizable performance. An initial step should be added in an ML workflow where researchers evaluate the insights gained by quantitative analysis of the datasets sample and feature spaces. This study contributes towards achieving such a goal by suggesting a technique to quantify datasets in terms of feature frequency distribution characteristics. Hence a unique insight is provided into how the features in the available dataset samples are frequent. The technique was demonstrated in 11 benign and malign (malware) Android application datasets belonging to six academic Android mobile malware classification studies. The permissions requested by applications such as CALL_PHONE compose a relatively high-dimensional binary feature space. The results showed that the distributions fit well into two of the four long right-tail statistical distributions : log-normal, exponential, power law, and Poisson. Precisely, log-normal was the most exhibited statistical distribution except the two malign datasets that were in exponential. This study also explores statistical distribution fit/unfit feature analysis that enhances the insights in feature space. Finally, the study compiles phenomena examples in the literature exhibiting these statistical distributions that should be considered for interpreting the fitted distributions. In conclusion, conducting well-formed statistical methods provides a clear understanding of the datasets and intra-class and inter-class differences before proceeding with selecting features and building a classifier model. Feature distribution characteristics should be one to analyze beforehand. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Gürol Canbek},
  doi          = {10.1002/widm.1456},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1456},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Gaining insights in datasets in the shade of “garbage in, garbage out” rationale: Feature space distribution fitting},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on datasets for fairness-aware machine learning.
<em>WIDM</em>, <em>12</em>(3), e1452. (<a
href="https://doi.org/10.1002/widm.1452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As decision-making increasingly relies on machine learning (ML) and (big) data, the issue of fairness in data-driven artificial intelligence systems is receiving increasing attention from both research and industry. A large variety of fairness-aware ML solutions have been proposed which involve fairness-related interventions in the data, learning algorithms, and/or model outputs. However, a vital part of proposing new approaches is evaluating them empirically on benchmark datasets that represent realistic and diverse settings. Therefore, in this paper, we overview real-world datasets used for fairness-aware ML. We focus on tabular data as the most common data representation for fairness-aware ML. We start our analysis by identifying relationships between the different attributes, particularly with respect to protected attributes and class attribute, using a Bayesian network. For a deeper understanding of bias in the datasets, we investigate interesting relationships using exploratory analysis. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Tai Le Quy and Arjun Roy and Vasileios Iosifidis and Wenbin Zhang and Eirini Ntoutsi},
  doi          = {10.1002/widm.1452},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1452},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A survey on datasets for fairness-aware machine learning},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data and text mining from online reviews: An automatic
literature analysis. <em>WIDM</em>, <em>12</em>(3), e1448. (<a
href="https://doi.org/10.1002/widm.1448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper reports on a thorough analysis of the scientific literature using data and text mining to uncover knowledge from online reviews due to their importance as user-generated content. In this context, more than 12,000 papers were extracted from publications indexed in the Scopus database within the last 15 years. Regarding the type of data, most previous studies focused on qualitative textual data to perform their analysis, with fewer looking for quantitative scores and/or characterizing reviewer profiles. In terms of application domains, information management and technology, e-commerce, and tourism stand out. It is also clear that other areas of potentially valuable applications should be addressed in future research, such as arts and education, as well as more interdisciplinary approaches, namely in the spectrum of the social sciences. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Sérgio Moro and Paulo Rita},
  doi          = {10.1002/widm.1448},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1448},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Data and text mining from online reviews: An automatic literature analysis},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Validation of cluster analysis results on validation data: A
systematic framework. <em>WIDM</em>, <em>12</em>(3), e1444. (<a
href="https://doi.org/10.1002/widm.1444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster analysis refers to a wide range of data analytic techniques for class discovery and is popular in many application fields. To assess the quality of a clustering result, different cluster validation procedures have been proposed in the literature. While there is extensive work on classical validation techniques, such as internal and external validation, less attention has been given to validating and replicating a clustering result using a validation dataset. Such a dataset may be part of the original dataset, which is separated before analysis begins, or it could be an independently collected dataset. We present a systematic, structured review of the existing literature about this topic. For this purpose, we outline a formal framework that covers most existing approaches for validating clustering results on validation data. In particular, we review classical validation techniques such as internal and external validation, stability analysis, and visual validation, and show how they can be interpreted in terms of our framework. We define and formalize different types of validation of clustering results on a validation dataset, and give examples of how clustering studies from the applied literature that used a validation dataset can be seen as instances of our framework. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Theresa Ullmann and Christian Hennig and Anne-Laure Boulesteix},
  doi          = {10.1002/widm.1444},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1444},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Validation of cluster analysis results on validation data: A systematic framework},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deepfake attribution: On the source identification of
artificially generated images. <em>WIDM</em>, <em>12</em>(3), e1438. (<a
href="https://doi.org/10.1002/widm.1438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic media or &quot;deepfakes&quot; are making great advances in visual quality, diversity, and verisimilitude, empowered by large-scale publicly accessible datasets and rapid technical progress in deep generative modeling. Heralding a paradigm shift in how online content is trusted, researchers in digital image forensics have responded with different proposals to reliably detect AI-generated images in the wild. However, binary classification of image authenticity is insufficient to regulate the ethical usage of deepfake technology as new applications are developed. This article provides an overview of the major innovations in synthetic forgery detection as of 2020, while highlighting the recent shift in research towards ways to attribute AI-generated images to their generative sources with evidence. We define the various categories of deepfakes in existence, the subtle processing traces and fingerprints that distinguish AI-generated images from reality and each other, and the different degrees of attribution possible with current understanding of generative algorithms. Additionally, we describe the limitations of synthetic image recognition methods in practice, the counter-forensic attacks devised to exploit these limitations, and directions for new research to assure the long-term relevance of deepfake forensics. Reliable, explainable, and generalizable attribution methods would hold malicious users accountable for AI-enabled disinformation, grant plausible deniability to appropriate users, and facilitate intellectual property protection of deepfake technology. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Brandon Khoo and Raphaël C.-W. Phan and Chern-Hong Lim},
  doi          = {10.1002/widm.1438},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1438},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Deepfake attribution: On the source identification of artificially generated images},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computational resources in healthcare. <em>WIDM</em>,
<em>12</em>(3), e1437. (<a
href="https://doi.org/10.1002/widm.1437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare is the most important component in the life of all human beings as each individual wish to have happy, healthy, and wealthy life-span. Most of the branches of science are dedicated to improve the healthcare. In the era of knowledge mining, informatics is playing a crucial role in different branches of research. Thus, a wide range of informatics-based fields have emerged in the last three decades that include medical informatics, bioinformatics, cheminformatics, pharmacoinformatics, immunoinformatics, and clinical informatics. In the past, a number of reviews have been focused on the application of an informatics-based field in the healthcare. In this review, an attempt is made to summarize the major computational resources developed in any informatics-based field that have an application in healthcare. This review enlists computational resources in following groups - drug discovery, toxicity prediction, vaccine designing, disease biomarkers, and Internet of Things. We mainly focused on freely available, functional resources like data repositories, prediction models, standalone software, mobile apps, and web services. In order to provide service to the community, we developed a health portal that maintain links related to healthcare http://webs.iiitd.edu.in/ . This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Neelam Sharma and Leimarembi Devi Naorem and Satakshi Gupta and Gajendra P. S. Raghava},
  doi          = {10.1002/widm.1437},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1437},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Computational resources in healthcare},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning in postgenomic biology and personalized
medicine. <em>WIDM</em>, <em>12</em>(2), e1451. (<a
href="https://doi.org/10.1002/widm.1451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, machine learning (ML) has been revolutionizing biology, biomedical sciences, and gene-based agricultural technology capabilities. Massive data generated in biological sciences by rapid and deep gene sequencing and protein or other molecular structure determination, on the one hand, require data analysis capabilities using ML that are distinctly different from classical statistical methods; on the other, these large datasets are enabling the adoption of novel data-intensive ML algorithms for the solution of biological problems that until recently had relied on mechanistic model-based approaches that are computationally expensive. This review provides a bird&#39;s eye view of the applications of ML in postgenomic biology. Attempt is also made to indicate as far as possible the areas of research that are poised to make further impacts in these areas, including the importance of explainable artificial intelligence in human health. Further contributions of ML are expected to transform medicine, public health, agricultural technology, as well as to provide invaluable gene-based guidance for the management of complex environments in this age of global warming. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Animesh Ray},
  doi          = {10.1002/widm.1451},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1451},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Machine learning in postgenomic biology and personalized medicine},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning methods for generating high dimensional
discrete datasets. <em>WIDM</em>, <em>12</em>(2), e1450. (<a
href="https://doi.org/10.1002/widm.1450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of platforms and techniques for emerging Big Data and Machine Learning applications requires the availability of real-life datasets. A possible solution is to synthesize datasets that reflect patterns of real ones using a two-step approach: first, a real dataset is analyzed to derive relevant patterns and, then, to use such patterns for reconstructing a new dataset that preserves the main characteristics of . This survey explores two possible approaches: (1) Constraint-based generation and (2) probabilistic generative modeling. The former is devised using inverse mining ( ) techniques, and consists of generating a dataset satisfying given support constraints on the itemsets of an input set, that are typically the frequent ones. By contrast, for the latter approach, recent developments in probabilistic generative modeling ( ) are explored that model the generation as a sampling process from a parametric distribution, typically encoded as neural network. The two approaches are compared by providing an overview of their instantiations for the case of discrete data and discussing their pros and cons. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Giuseppe Manco and Ettore Ritacco and Antonino Rullo and Domenico Saccà and Edoardo Serra},
  doi          = {10.1002/widm.1450},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1450},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Machine learning methods for generating high dimensional discrete datasets},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Methods and tools for causal discovery and causal inference.
<em>WIDM</em>, <em>12</em>(2), e1449. (<a
href="https://doi.org/10.1002/widm.1449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causality is a complex concept, which roots its developments across several fields, such as statistics, economics, epidemiology, computer science, and philosophy. In recent years, the study of causal relationships has become a crucial part of the Artificial Intelligence community, as causality can be a key tool for overcoming some limitations of correlation-based Machine Learning systems. Causality research can generally be divided into two main branches, that is, causal discovery and causal inference. The former focuses on obtaining causal knowledge directly from observational data. The latter aims to estimate the impact deriving from a change of a certain variable over an outcome of interest. This article aims at covering several methodologies that have been developed for both tasks. This survey does not only focus on theoretical aspects. But also provides a practical toolkit for interested researchers and practitioners, including software, datasets, and running examples. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Ana Rita Nogueira and Andrea Pugnana and Salvatore Ruggieri and Dino Pedreschi and João Gama},
  doi          = {10.1002/widm.1449},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1449},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Methods and tools for causal discovery and causal inference},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Facial feature discovery for ethnicity recognition.
<em>WIDM</em>, <em>12</em>(2), e1446. (<a
href="https://doi.org/10.1002/widm.1446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_WIDM},
  doi          = {10.1002/widm.1446},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1446},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Facial feature discovery for ethnicity recognition},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The use of machine learning in sport outcome prediction: A
review. <em>WIDM</em>, <em>12</em>(2), e1445. (<a
href="https://doi.org/10.1002/widm.1445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_WIDM},
  doi          = {10.1002/widm.1445},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1445},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {The use of machine learning in sport outcome prediction: A review},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Process mining applications in the healthcare domain: A
comprehensive review. <em>WIDM</em>, <em>12</em>(2), e1442. (<a
href="https://doi.org/10.1002/widm.1442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mining (PM) is a well-known research area that includes techniques, methodologies, and tools for analyzing processes in a variety of application domains. In the case of healthcare, processes are characterized by high variability in terms of activities, duration, and involved resources (e.g., physicians, nurses, administrators, machineries, etc.). Besides, the multitude of diseases that the patients housed in healthcare facilities suffer from makes medical contexts highly heterogeneous. As a result, understanding and analyzing healthcare processes are certainly not trivial tasks, and administrators and doctors look for tools and methods that can concretely support them in improving the healthcare services they are involved in. In this context, PM has been increasingly used for a wide range of applications as reported in some recent reviews. However, these reviews mainly focus on discussion on applications related to the clinical pathways, while a systematic review of all possible applications is absent. In this article, we selected 172 papers published in the last 10 years, that present applications of PM in the healthcare domain. The objective of this study is to help and guide researchers interested in the medical field to understand the main PM applications in the healthcare, but also to suggest new ways to develop promising and not yet fully investigated applications. Moreover, our study could be of interest for practitioners who are considering applications of PM, who can identify and choose PM algorithms, techniques, tools, methodologies, and approaches, toward what have been the experiences of success. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Antonella Guzzo and Antonino Rullo and Eugenio Vocaturo},
  doi          = {10.1002/widm.1442},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1442},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Process mining applications in the healthcare domain: A comprehensive review},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Over-optimism in benchmark studies and the multiplicity of
design and analysis options when interpreting their results.
<em>WIDM</em>, <em>12</em>(2), e1441. (<a
href="https://doi.org/10.1002/widm.1441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the need for neutral benchmark studies that focus on the comparison of methods coming from computational sciences has been increasingly recognized by the scientific community. While general advice on the design and analysis of neutral benchmark studies can be found in recent literature, a certain flexibility always exists. This includes the choice of data sets and performance measures, the handling of missing performance values, and the way the performance values are aggregated over the data sets. As a consequence of this flexibility, researchers may be concerned about how their choices affect the results or, in the worst case, may be tempted to engage in questionable research practices (e.g., the selective reporting of results or the post hoc modification of design or analysis components) to fit their expectations. To raise awareness for this issue, we use an example benchmark study to illustrate how variable benchmark results can be when all possible combinations of a range of design and analysis options are considered. We then demonstrate how the impact of each choice on the results can be assessed using multidimensional unfolding. In conclusion, based on previous literature and on our illustrative example, we claim that the multiplicity of design and analysis options combined with questionable research practices lead to biased interpretations of benchmark results and to over-optimistic conclusions. This issue should be considered by computational researchers when designing and analyzing their benchmark studies and by the scientific community in general in an effort towards more reliable benchmark results. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Christina Nießl and Moritz Herrmann and Chiara Wiedemann and Giuseppe Casalicchio and Anne-Laure Boulesteix},
  doi          = {10.1002/widm.1441},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1441},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Over-optimism in benchmark studies and the multiplicity of design and analysis options when interpreting their results},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel methodology for arabic news classification.
<em>WIDM</em>, <em>12</em>(2), e1440. (<a
href="https://doi.org/10.1002/widm.1440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automated news classification concerns the assignment of news to one or more predefined categories. The automated classified news helps the search engines to mine and categorize the type of news that the user asks for. Most of the researchers focused on the classification of English news and ignore the Arabic news due to the complexity of the Arabic morphology. This article presents a novel methodology to classify the Arabic news. It relies on the use of features extraction and the application of machine learning classifiers which are the Naive Bayes (NB), the Logistic Regression (LR), the Random Forest (RF), the Xtreme Gradient Boosting (XGB), the K-Nearest Neighbors (KNN), the Stochastic Gradient Descent (SGD), the Decision Tree (DT), and the Multi-Layer Perceptron (MLP). The methodology is applied to the Arabic news dataset provided by Mendeley. The accuracy of the classification is more than 95%. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Marco Alfonse and Mariam Gawich},
  doi          = {10.1002/widm.1440},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1440},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A novel methodology for arabic news classification},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting home sale prices: A review of existing methods
and illustration of data stream methods for improved performance.
<em>WIDM</em>, <em>12</em>(2), e1435. (<a
href="https://doi.org/10.1002/widm.1435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for accurate and unbiased assessment of residential real property has always been important not only to financial institutions lending on or holding such assets but also to municipalities that rely on property taxes as their critical source of revenue. The common methodology for predicting residential property sale price is based on traditional multiple regression in spite of known issues. Machine learning methods have been proposed as an alternative approach but the results are far from satisfactory. A review of existing studies and relevant issues can help researchers better assess the pros and cons of the approaches in this important stream of research and move the field forward. This article provides such a review. In our review, we have noticed that common to both the regression-based methods and machine learning methods are the use of batch-mode learning. Thus in addition to providing a review of recent research on batch-based residential property prediction models, this article also explores a new approach to constructing residential property price prediction models by treating past sale records as an evolving data stream. The results of our study show that the data stream approach outperforms the traditional regression method and demonstrate the potential of data stream methods in improving prediction models for residential property prices. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Donghui Shi and Jian Guan and Jozef Zurada and Alan S. Levitan},
  doi          = {10.1002/widm.1435},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1435},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Predicting home sale prices: A review of existing methods and illustration of data stream methods for improved performance},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Themes in data mining, big data, and crime analytics.
<em>WIDM</em>, <em>12</em>(2), e1432. (<a
href="https://doi.org/10.1002/widm.1432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article examines the impact of new AI-related technologies in data mining and big data on important research questions in crime analytics. Because the field is so broad, the review focuses on a selection of the most important topics. Challenges for information management, and in turn law and society, include: AI-powered predictive policing; big data for legal and adversarial decisions; bias using big data and analytics in profiling and predicting criminality; forecasting crime risk and crime rates; and, regulating AI systems. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Giles C. Oatley},
  doi          = {10.1002/widm.1432},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1432},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Themes in data mining, big data, and crime analytics},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on federated learning in data mining.
<em>WIDM</em>, <em>12</em>(1), e1443. (<a
href="https://doi.org/10.1002/widm.1443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data mining is a process to extract unknown, hidden, and potentially useful information from data. But the problem of data island makes it arduous for people to collect and analyze scattered data, and there is also a privacy security issue when mining data. A collaboratively decentralized approach called federated learning unites multiple participants to generate a shareable global optimal model and keeps privacy-sensitive data on local devices, which may bring great hope to us for solving the problems of decentralized data and privacy protection. Though federated learning has been widely used, few systematic studies have been conducted on the subject of federated learning in data mining. Hence, different from prior reviews in this field, we make a comprehensive summary and provide a novel taxonomy of the application of federated learning in data mining. This article starts by providing a thorough description of the relevant definitions and concepts, followed by an in-depth investigation on the challenges faced by federated learning. In this context, we elaborate four taxonomies of major applications of federated learning in data mining, including education, healthcare, IoT, and intelligent transportation, and discuss them comprehensively. Finally, we discuss four promising research directions for further research, that is, privacy enhancement, improvement of communication efficiency, heterogeneous system processing, and reducing economic costs. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Bin Yu and Wenjie Mao and Yihan Lv and Chen Zhang and Yu Xie},
  doi          = {10.1002/widm.1443},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1443},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A survey on federated learning in data mining},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning in histopathology: A review. <em>WIDM</em>,
<em>12</em>(1), e1439. (<a
href="https://doi.org/10.1002/widm.1439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Histopathology is diagnosis based on visual examination of tissue sections under a microscope. With the growing number of digitally scanned tissue slide images, computer-based segmentation and classification of these images is a high-demand area of research. Convolutional neural networks (CNNs) constitute the most popular classification architecture for a variety of image classification problems. However, applying CNNs to histology slides is not a trivial task and has several challenges, ranging from variations in the colors of slides to excessive high resolution and lack of proper labeling. In this advanced review, we introduce the application of CNN-based architectures to digital histological image analysis, discuss some problems associated with such analysis, and look at possible solutions. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Sugata Banerji and Sushmita Mitra},
  doi          = {10.1002/widm.1439},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1439},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Deep learning in histopathology: A review},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blockchain networks: Data structures of bitcoin, monero,
zcash, ethereum, ripple, and iota. <em>WIDM</em>, <em>12</em>(1), e1436.
(<a href="https://doi.org/10.1002/widm.1436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain is an emerging technology that has enabled many applications, from cryptocurrencies to digital asset management and supply chains. Due to this surge of popularity, analyzing the data stored on blockchains poses a new critical challenge in data science. To assist data scientists in various analytic tasks for a blockchain, in this tutorial, we provide a systematic and comprehensive overview of the fundamental elements of blockchain network models. We discuss how we can abstract blockchain data as various types of networks and further use such associated network abstractions to reap important insights on blockchains&#39; structure, organization, and functionality. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Cuneyt Gurcan Akcora and Yulia R. Gel and Murat Kantarcioglu},
  doi          = {10.1002/widm.1436},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1436},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Blockchain networks: Data structures of bitcoin, monero, zcash, ethereum, ripple, and iota},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A critical review of state-of-the-art chatbot designs and
applications. <em>WIDM</em>, <em>12</em>(1), e1434. (<a
href="https://doi.org/10.1002/widm.1434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chatbots are intelligent conversational agents that can interact with users through natural languages. As chatbots can perform a variety of tasks, many companies have committed numerous resources to develop and deploy chatbots to enhance various business processes. However, we lack an up-to-date critical review that thoroughly examines both state-of-the-art technologies and innovative applications of chatbots. In this review, we not only critically analyze the various computational approaches used to develop state-of-the-art chatbots, but also thoroughly review the usability and applications of chatbots for various business sectors. We also identify gaps in chatbot-related studies and propose new research directions to address the shortcomings of existing studies and applications. Our review advances both academic research and practical business applications of state-of-the-art chatbots. We provide guidance for practitioners to fully realize the business value of chatbots and assist in making sensible decisions related to the development and deployment of chatbots in various business contexts. Researchers interested in the design and development of chatbots can also gain useful insights from our critical review and identify fruitful research topics and future research directions based on the research gaps discussed herein. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Bei Luo and Raymond Y. K. Lau and Chunping Li and Yain-Whar Si},
  doi          = {10.1002/widm.1434},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1434},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A critical review of state-of-the-art chatbot designs and applications},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discovery of behavioral patterns in online social commerce
practice. <em>WIDM</em>, <em>12</em>(1), e1433. (<a
href="https://doi.org/10.1002/widm.1433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovery of behavioral patterns in online social commerce practice becomes important in this digital era. In this article, we propose a systematic approach to behavioral pattern discovery, and apply it in an emerging online social commerce venue: live streaming. We investigate behavioral patterns in gifting encouragement in live streaming to understand online social commerce practice. Our proposed approach is based on multiple triangulation, including data source triangulation (i.e., streamers, viewers, and actual behavior) and data collection method triangulation (i.e., interviews, focus groups, and observations). Through multiple triangulation, four behavioral patterns of gifting encouragement are discovered: (i) requesting a certain gift for providing a particular service, (ii) creating a raffle, (iii) eliciting competition between individuals, and (iv) eliciting competition between groups. This research reveals the special behavioral patterns in live streaming, and thus increases our knowledge of social commerce practices. This research provides a systematic approach to discover online behavioral patterns, and provides practical implications in live streaming platforms, especially in marketing and platform design. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Xiaoyun Jia and Ruili Wang and James H. Liu and Chuntao Jiang},
  doi          = {10.1002/widm.1433},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1433},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Discovery of behavioral patterns in online social commerce practice},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting communities using social network analysis in
online learning environments: Systematic literature review.
<em>WIDM</em>, <em>12</em>(1), e1431. (<a
href="https://doi.org/10.1002/widm.1431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncovering community structure has made a significant advancement in explaining, analyzing, and forecasting behaviors and dynamics of networks related to different fields in sociology, criminology, biology, medicine, communication, economics, and academia. Detecting and clustering communities is a powerful step toward identifying the structural properties and the behavioral patterns in social networks. Recently, online learning has been progressively adopted by a lot of educational practices which raise many questions about assessing the learners&#39; engagement, collaboration, and behaviors in the new emerging learning communities. This systematic literature review aims to assess the use of community detection techniques in analyzing the network&#39;s structure in online learning environments. It provides a comprehensive overview of the existing research that adopted those techniques with identifying the educational objectives behind their application as well as suggesting possible future research directions. Our analysis covered 65 studies that found in the literature and applied different community discovery techniques on various types of online learning environments to analyze their users&#39; interactions patterns. Our review revealed the potential of this field in improving educational practices and decisions and in utilizing the massive amount of data generated from interacting with those environments. Finally, we highlighted the need to include automated community discovery techniques in online learning environments to facilitate and enhance their use as well as we stressed on the urge for further advance research to uncover a lot of hidden opportunities. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Sahar Yassine and Seifedine Kadry and Miguel-Angel Sicilia},
  doi          = {10.1002/widm.1431},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1431},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Detecting communities using social network analysis in online learning environments: Systematic literature review},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multivariate temporal data analysis - a review.
<em>WIDM</em>, <em>12</em>(1), e1430. (<a
href="https://doi.org/10.1002/widm.1430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The information technology revolution, especially with the adoption of the Internet of Things, longitudinal data in many domains become more available and accessible for secondary analysis. Such data provide meaningful opportunities to understand process in many domains along time, but also challenges. A main challenge is the heterogeneity of the temporal variables due to the different types of data, whether a measurement or an event, and type of samplings: fixed or irregular. Other variables can be also events that may or not have duration. In this review, we discuss the various types of temporal data, and the various relevant analysis methods. Starting with fixed frequency variables, with forecasting and time series methods, and proceeding with sequential data, and sequential patterns mining, and time intervals mining for events having various time duration. Also the use of various deep learning based architectures for temporal data is discussed. The challenge of heterogeneous multivariate temporal data analysis and discuss various options to deal with it, focusing on an increasingly used option of transforming the data into symbolic time intervals through temporal abstraction and the use of time intervals related patterns discovery for temporal knowledge discovery, clustering, classification prediction, and more. Finally, we discuss the overview of the field, and areas in which more studies and contributions are needed. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Robert Moskovitch},
  doi          = {10.1002/widm.1430},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1430},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Multivariate temporal data analysis - a review},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Explaining artificial intelligence with visual analytics in
healthcare. <em>WIDM</em>, <em>12</em>(1), e1427. (<a
href="https://doi.org/10.1002/widm.1427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To make predictions and explore large datasets, healthcare is increasingly applying advanced algorithms of artificial intelligence. However, to make well-considered and trustworthy decisions, healthcare professionals require ways to gain insights in these algorithms&#39; outputs. One approach is visual analytics, which integrates humans in decision-making through visualizations that facilitate interaction with algorithms. Although many visual analytics systems have been developed for healthcare, a clear overview of their explanation techniques is lacking. Therefore, we review 71 visual analytics systems for healthcare, and analyze how they explain advanced algorithms through visualization, interaction, shepherding, and direct explanation. Based on our analysis, we outline research opportunities and challenges to further guide the exciting rapprochement of visual analytics and healthcare. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Jeroen Ooge and Gregor Stiglic and Katrien Verbert},
  doi          = {10.1002/widm.1427},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1427},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Explaining artificial intelligence with visual analytics in healthcare},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
