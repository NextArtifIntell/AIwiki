<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IETIP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ietip---304">IETIP - 304</h2>
<ul>
<li><details>
<summary>
(2022). Using optical flow algorithm based on dynamic illumination
mode to examine defects on highly reflective turbine blade surface.
<em>IETIP</em>, <em>16</em>(14), 3988–4010. (<a
href="https://doi.org/10.1049/ipr2.12608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Notion of optical flow literally refers to the displacements of intensity patterns. In that sense, extracting interested information from 2D scene is analogy to modulation/demodulation in random signal processing. To address the limitations presented in computer vision based on static image, we propose a novel metal component defect detection method, specified as the instance of turbine blade surface detection, using optical flow estimation.To start the specified pattern recognition in 2D presentation, we modulate the brightness constancy assumption equation as illumination varying model, by sampling the second image with function whose frequency was chosen according to the Nyquist sampling theorem, and a sinusoidal factor was introduced as an additive factor. This tunable channel based on 2D image transfers intensity features into optical modes. Then, we implement optical flow estimation on two sequential images. Experimental results reveal grayscale space shows completness in representing the optical modes of turbine blade with various kinds of surface characteristics. By modifying the index of information content, we propose quantitative index to evaluate the performance of our method. Evaluation reveals optical flow algorithm is qualified to examine defects on highly reflective turbine blade, and our method extends the application of optical flow.},
  archive      = {J_IETIP},
  author       = {Xuan Shang and Wenyan Song and Zhen Chen and Congxuan Zhang},
  doi          = {10.1049/ipr2.12608},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3988-4010},
  shortjournal = {IET Image Process.},
  title        = {Using optical flow algorithm based on dynamic illumination mode to examine defects on highly reflective turbine blade surface},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Infrared and visible image fusion for ship targets based on
scale-aware feature decomposition. <em>IETIP</em>, <em>16</em>(14),
3977–3987. (<a href="https://doi.org/10.1049/ipr2.12607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared (IR) and visible (VI) image fusion play an important role in improving the sea scene perception and ship target detection. Although there have been many studies on image fusion considering the sea scene characteristics, there has still been no adequate extraction method for detailed information of a ship target in a sea scene. To overcome this shortcoming, this paper proposes a fusion method based on a scale-aware feature decomposition, which can accurately extract features of a ship target at different scales. First, a hybrid feature decomposition method based on the scale-aware structure-preserving filter and Gaussian filter is designed. The proposed method separated source images into region, structure, and texture layers, and thus achieved a finer-scale division than traditional multiscale decomposition methods in the sea-clutter background. Then, further decomposition of the texture layer was performed by the shearlet transform to obtain the directional texture feature. According to the characteristics of each layer, different strategies were adopted for the fusion, and the weighting factor was used to obtain the final fusion result. Experimental results indicated that the proposed method could achieve better subjective and objective results than current state-of-the-art methods. This method introduces a scale-aware edge preserving filter to improve the ability of detail extraction and is suitable for image fusion of ship targets.},
  archive      = {J_IETIP},
  author       = {Xin Zheng and Di Kang and Pengbo Si and Qiang Wu},
  doi          = {10.1049/ipr2.12607},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3977-3987},
  shortjournal = {IET Image Process.},
  title        = {Infrared and visible image fusion for ship targets based on scale-aware feature decomposition},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Combine unlabeled with labeled MR images to measure acute
ischemic stroke lesion by stepwise learning. <em>IETIP</em>,
<em>16</em>(14), 3965–3976. (<a
href="https://doi.org/10.1049/ipr2.12606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acute ischemic stroke is a common threat to human health and may obtain timely treatment by fast localizing and quantitatively evaluating the lesions. Most CNN-based methods try to segment and measure the lesions, however, they require a training on a large number of labeled subjects that are labor-intensive and time-consuming to obtain. In this paper, a method is proposed that can combine limited labeled subjects with abundant unlabeled subjects to alleviate the problem. The proposed method consists of two stages: stepwise learning process and segmentation process. Stepwise learning is used to obtain the pretrained encoder. The pretrained encoder and the proposed decoder are connected into a new end-to-end segmentation network, which is retrained on the labeled subjects in the segmentation process. By using 5 labeled subjects and 79 unlabeled subjects, the proposed method achieves a mean dice coefficient of 0.663 0.205, a mean average symmetric surface distance (ASSD) of 2.17 mm and a mean 95 percentile Hausdorff distance (HD) of 18.38 mm on a clinical MR dataset with 179 subjects. More importantly, it achieves lesion-wise F 1 score of 0.857 and a subject-wise detection rate of 0.966.},
  archive      = {J_IETIP},
  author       = {Bin Zhao and Zhiyang Liu and Guohua Liu and Mengran Wu and Chen Cao and Song Jin and Hong Wu and Shuxue Ding},
  doi          = {10.1049/ipr2.12606},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3965-3976},
  shortjournal = {IET Image Process.},
  title        = {Combine unlabeled with labeled MR images to measure acute ischemic stroke lesion by stepwise learning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Diagnose alzheimer’s disease by combining 3D discrete
wavelet transform and 3D moment invariants. <em>IETIP</em>,
<em>16</em>(14), 3948–3964. (<a
href="https://doi.org/10.1049/ipr2.12605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The early stage of Alzheimer&#39;s disease (AD) is called mild cognitive impairment (MCI). According to the final progression of the disease, it can be divided into progressive MCI (pMCI) and stable MCI (sMCI). Clinical treatment shows that early treatment of MCI stage can effectively delay the progression of the disease and even complete recovery. Therefore, accurate diagnosis of AD and its early stage play an important guiding role in the treatment and delay of AD progression. Previous studies have shown that the data of different modalities of AD can reflect different pathological changes. Therefore, the accuracy of combining multi-modality data to classify AD is better than that of using single-modality to classify AD. Based on this, an AD diagnosis method is proposed based on 3D discrete wavelet transform (3D-DWT) and 3D moment invariants (3D-MIs) features from multi-modalities images. First, automatic anatomical landmark (AAL) atlas is used to identify the brain regions of interest (ROIs) from magnetic resonance image (MRI) and positron emission tomography (PET). Next, for each ROI, the 3D-DWT and 3D-MIs extract methods are used to extract features. Finally, a deep neural network with stacked autoencoders (SAE) is trained to detect AD. The experimental results show that, compared with the state-of-the-art AD/MCI classification algorithms based on multi-modality features, this method can significantly improve the classification performance of pMCI and sMCI classification tasks, which has an important guiding role in the early diagnosis of AD.},
  archive      = {J_IETIP},
  author       = {Huan Lao and Xuejun Zhang},
  doi          = {10.1049/ipr2.12605},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3948-3964},
  shortjournal = {IET Image Process.},
  title        = {Diagnose alzheimer&#39;s disease by combining 3D discrete wavelet transform and 3D moment invariants},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active contour driven by adaptive-scale local-energy signed
pressure force function based on bias correction for medical image
segmentation. <em>IETIP</em>, <em>16</em>(14), 3929–3947. (<a
href="https://doi.org/10.1049/ipr2.12604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an active contour model driven by an adaptive-scale local-energy signed pressure force (ALSPF) function based on bias correction for segmenting medical images with intensity inhomogeneity. Firstly, a local energy-driven signed pressure force (LESPF) function is designed as the driving force to extract local image features, which can effectively deal with intensity inhomogeneity. Secondly, to avid selecting the neighbourhood size in the LESPF function, a multi-scale adaptive selection schema is put forward to adaptively choose the local window size according to the degree of intensity inhomogeneity. Finally, a novel single-well potential function proved to be a strictly monotonic function is designed to ensure the stable movement of the neighbourhood points in the vicinity of the evolution curve, which can not only avoid the re-initialization process but also enhance. Experiments on synthetic and medical images demonstrate that the proposed model is more robust than the popular ACMs for segmenting images with intensity inhomogeneity. The code is available at the website: https://github.com/HuaxiangLiu/ALSPF},
  archive      = {J_IETIP},
  author       = {Huaxiang Liu and Youyao Fu and Shiqing Zhang and Jun Liu and Jiangxiong Fang},
  doi          = {10.1049/ipr2.12604},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3929-3947},
  shortjournal = {IET Image Process.},
  title        = {Active contour driven by adaptive-scale local-energy signed pressure force function based on bias correction for medical image segmentation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). X-ray medical image super-resolution via self-organization
neural networks and geometric directional gradient. <em>IETIP</em>,
<em>16</em>(14), 3910–3928. (<a
href="https://doi.org/10.1049/ipr2.12603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, due to the rapid development of the Internet and communication networks, remote disease diagnosis by physicians utilizing medical images such as X-rays have increased. However, the transmitted images lose their true quality and resolutions due to several factors, including noise and image compression techniques used to conserve memory space. This paper presents a novel clinical X-ray image enhancement method based on self-organizing neural networks, the ability of first-order derivative operators to detect edges, and improved classification methods by enhancing image features and extending the nearest neighbor algorithms selection of the best similarity feature vector among all sample feature vectors. The proposed method employs first-order image derivation operators to extract edge details and features from images. The feature vectors were preprocessed and segmented into various classes using self-organizing neural networks. The best matching and most similar vectors among the feature vectors are retrieved via the nearest neighbor algorithm. In contrast to conventional and contemporary image super-resolution techniques, the proposed method does not rely on the cost function. It is independent of error backpropagation methods because the competitive nature of the neural network is used to update the weight vectors rather than the gradient descent method. In addition, the performance and accuracy of the proposed image super-resolution method were improved by increasing the number of feature components in the feature vectors through the use of the two-dimensional image gradient. Furthermore, numerous image quality measurement criteria, such as structural similarity index (SSIM) and peak signal-to-noise ratio (PSNR), were used to compare the reconstructed images of the proposed algorithm with those of other image enhancement algorithms. The results show that the proposed image enhancement algorithms&#39; high-resolution and high-quality reconstructed image significantly outperformed many other medical image enhancement algorithms in reconstructing the edges and boundaries of objects with a smooth slope, as deemed by expert personnel and compared with image quality measurement criteria.},
  archive      = {J_IETIP},
  author       = {Khodabakhsh Ahmadian and Hamid-reza Reza-Alikhani},
  doi          = {10.1049/ipr2.12603},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3910-3928},
  shortjournal = {IET Image Process.},
  title        = {X-ray medical image super-resolution via self-organization neural networks and geometric directional gradient},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A method of underwater bridge structure damage detection
method based on a lightweight deep convolutional network.
<em>IETIP</em>, <em>16</em>(14), 3893–3909. (<a
href="https://doi.org/10.1049/ipr2.12602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of the underwater structure disease of the bridge is increasingly obvious, which has seriously affected the safe operation of the bridge structure, so it is necessary to detect the underwater structure regularly. There are many kinds of bridge underwater structure diseases. This paper targets the bridge underwater structural crack diseases adopts multiple image recognition networks for verification, compares the advantages of different networks, and takes the YOLO-v4 network as the main body to build a lightweight convolutional neural network.Mobilenetv3 replaced CSPDarkent as the backbone feature extraction network, while the feature layer scale of Mobilenetv3 was modified, and the extracted preliminary feature layer was input into the enhanced feature extraction network for feature fusion. The PANet networks are replaced by the depthwise separable convolution. Using ablation experiments to compare the performance of four algorithm combinations in lightweight networks. At the same time, the disease identification accuracy of each network and the performance of the network are tested in various experimental environments, and the feasibility of the lightweight network is verified in the application of bridge underwater structure damage identification.},
  archive      = {J_IETIP},
  author       = {Xiaofei Li and Heming Sun and Taiyi Song and Tian Zhang and Qinghang Meng},
  doi          = {10.1049/ipr2.12602},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3893-3909},
  shortjournal = {IET Image Process.},
  title        = {A method of underwater bridge structure damage detection method based on a lightweight deep convolutional network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature difference for single-shot object detection.
<em>IETIP</em>, <em>16</em>(14), 3876–3892. (<a
href="https://doi.org/10.1049/ipr2.12601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The one-stage detectors achieve a good trade-off between performance and latency, owing to the plain architecture and divergent learning mechanism for classification and localization. However, the two sub-tasks require features with various inherency with which to generate inconsistent detections, fettering detectors. In this study, the misalignment is deeply analyzed via kernel density estimation (KDE) for the first time. Moreover, to address the misalignment, a plug-and-play detection head, named Diff-Head, is devised and embedded in one-stage detectors. Concretely, the authors merge parallel branches into a semi-parallel structure, establishing the correlation between classification and regression. In the regression branch, a feature difference module (FDM) gets rid of the features that favour classification by subtracting salient object features from the original feature map, and position encoding (PE) modules enhance the absolute position information. The flexibility and efficiency of the detection head are retained. Experiments on Pascal visual object classes (VOC) and MS COCO demonstrate that Diff-Head is effective and achieves competitive performance with state-of-the-art detectors. Meanwhile, the amount of parameters is reduced at least 30% and 83.0% average precision (AP) is achieved on Pascal VOC. The analyses of consistency and error show that Diff-Head has better localization and the capability of mitigating the misalignment.},
  archive      = {J_IETIP},
  author       = {Tao Zeng and Feng Xu and Xin Lyu and Xin Li and Xinyuan Wang and Jiale Chen and Caifeng Wu},
  doi          = {10.1049/ipr2.12601},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3876-3892},
  shortjournal = {IET Image Process.},
  title        = {Feature difference for single-shot object detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image encryption algorithm based on cascaded chaotic map and
improved zigzag transform. <em>IETIP</em>, <em>16</em>(14), 3863–3875.
(<a href="https://doi.org/10.1049/ipr2.12600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an image encryption algorithm based on cascaded chaotic map and improved Zigzag transform is proposed. Firstly, a cascade chaotic system is improved to make its chaotic sequence evenly distributed and improve its parameter range. The purpose is to obtain high random chaotic sequence and make the ciphertext image more secure. Secondly, the improved Zigzag transform is used to scramble the image. Compared with the standard Zigzag transform, the improved Zigzag transform can fully scramble all the pixels and improve the randomness of the ciphertext image. Thirdly, a wave-shaped diffusion algorithm is designed, its characteristic is that it can alternate between different row and column for diffusion. The combination of SHA256 algorithm and plain image makes the cipher image better resist the select plaintext attack. Simulation results show that the algorithm has fast encryption efficiency, high security and can effectively resist various attacks.},
  archive      = {J_IETIP},
  author       = {Jiming Zheng and Tianyi Lv},
  doi          = {10.1049/ipr2.12600},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3863-3875},
  shortjournal = {IET Image Process.},
  title        = {Image encryption algorithm based on cascaded chaotic map and improved zigzag transform},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Class-wise boundary regression by uncertainty in temporal
action detection. <em>IETIP</em>, <em>16</em>(14), 3854–3862. (<a
href="https://doi.org/10.1049/ipr2.12599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action detection is a crucial aspect of video understanding. It aims to classify the action as well as locate the start and end boundaries of the action in the untrimmed videos. As deep learning is frequently utilized, the accuracy of annotation is crucial to boundary localization. However, it is observed that some annotation instances are ambiguous and the ambiguity varies between categories. To solve the problem above, a Gaussian model is built to estimate the boundary uncertainty for each instance. Based on instance uncertainty, category uncertainty is applied to describe the uncertainty of each category. By combining instance and category uncertainty, the boundaries of the selected proposals are refined and the ranking of candidate proposals is adjusted. Furthermore, overcorrection is avoided for categories with a high level of uncertainty. With the uncertainty approach, state-of-the-art performance is achieved: 57.5% on THUMOS14 (mAP@0.5) and 35.4% on ActivityNet (mAP@Avg).},
  archive      = {J_IETIP},
  author       = {Yunze Chen and Mengjuan Chen and Qingyi Gu},
  doi          = {10.1049/ipr2.12599},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3854-3862},
  shortjournal = {IET Image Process.},
  title        = {Class-wise boundary regression by uncertainty in temporal action detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic esophagus z-line delineation in endoscopic images
using a new boundary linking method. <em>IETIP</em>, <em>16</em>(14),
3842–3853. (<a href="https://doi.org/10.1049/ipr2.12598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the American cancer society, many people with esophageal adenocarcinoma are not survived. The treatment rate can be significant in the early detection of Barrett&#39;s esophagus (BE) as a premalignant stage for adenocarcinoma. An important landmark to detect BE is the Z-line. BE segmentation is already highly dependent upon the operator&#39;s knowledge and skill. The main aim of this study is automatic Z-line extraction using endoscopic images leading to segmentation of the early BE stage. To this end, a computer-aided detection method exploiting k-means clustering, image segmentation using the edge detector, and a novel boundary linking algorithm is proposed. For the evaluation, the gold standard is considered the average contours of Z-lines extracted by the three experts. The proposed method annotated the Z-line with the accuracy and precision of 0.92 and 0.87, respectively, and the value of the average boundary distance is 5.9 pixels. To the results and visual inspection, the presented method can be used for efficient and robust extraction of the Z-line at the early BE stage. Furthermore, it can be used in other medical imaging applications with complex boundaries and low contrast in the images, limiting the common automatic boundary detection methods.},
  archive      = {J_IETIP},
  author       = {Mehrnaz Aghanouri and Nasim Dadashi Serej and Hossein Rabbani and Peyman Adibi},
  doi          = {10.1049/ipr2.12598},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3842-3853},
  shortjournal = {IET Image Process.},
  title        = {Automatic esophagus Z-line delineation in endoscopic images using a new boundary linking method},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Segmentation of ten fetal heart components with
coarse-to-fine cascading and dynamic feature powering. <em>IETIP</em>,
<em>16</em>(14), 3831–3841. (<a
href="https://doi.org/10.1049/ipr2.12597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting heart components in the apical four-chamber view of fetal echocardiography is of critical significance in clinical practice. However, it is difficult to recognize these components due to small-scale components and the imbalanced ventricular apex orientation. In this study, a novel segmentation framework is proposed to segment ten general fetal heart components for the first time. This framework consists of a multi-directional fine-density (MDFD) data augmentation method and a coarse-to-fine cascade network (CFCN). MDFD enhances the apex orientation diversity and balances the orientation distribution. CFCN has two stages including a coarse network and a fine network. These two stages have similar structures that consist of a feature extractor and a feature refined layer named as Element-Wise Power with Dynamic Exponent layer (EWPDE). EWPDE which is a plug-and-play module for segmentation refines the features from the feature extractor to position small components accurately. By adopting EWPDE, the influence of each pixel is adjusted and hard pixels of small components are segmented precisely. Based on the dataset, the method is proved to be effective with the high mean intersection over union (mIoU) value and low missing ratio (MR). With MDFD and EWPDE, CFCN that adopts DeepLabV3+ as the feature extractor outperforms the best segmentation results (mIoU:0.480, MR:0.035). Compared to the original performance (mIoU:0.407, MR:0.085) of DeepLabV3+, the method improves the results significantly.},
  archive      = {J_IETIP},
  author       = {Tingyang Yang and Ye Zhang and Mengxiao Zhu and Yan Wang and Shan An and Xiaoyan Gu and Xiaowei Liu and Jiancheng Han and Yihua He and Haogang Zhu},
  doi          = {10.1049/ipr2.12597},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3831-3841},
  shortjournal = {IET Image Process.},
  title        = {Segmentation of ten fetal heart components with coarse-to-fine cascading and dynamic feature powering},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spotting micro-movements in image sequence by introducing
intelligent cubic-LBP. <em>IETIP</em>, <em>16</em>(14), 3814–3830. (<a
href="https://doi.org/10.1049/ipr2.12596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Invisible to naked human eyes, micro-movements are barely noticeable. There are wide ranges of micro-movement applications from spotting subtle changes in a volcano, vascular pulse, and blood vessel to micro-expression detection. In the latter, some evil thoughts can be unveiled, resulting in identifying crooks and lawbreakers and it arises from involuntary subtle and short-duration of facial muscles movements. Precise spotting of these tiny movements is possible only when multiple aspects of temporal images are scrutinized. Meanwhile, since motions often happen in one or two directions, it is rudimentary to extract complete feature sets in textural-based approaches such as cubic Local Binary Pattern (cubic-LBP). Approaches like cubic-LBP also have imposed an unnecessary computation burden. Hence, in this research, a novel method named intelligent cubic-LBP is proposed, which incorporates Convolutional Neural Network (CNN) model. This model learns to select the useful plane(s) automatically. Apex is then detected by applying Partial Autocorrelation Coefficient (PACF) on the selected plane(s). The experimental results show significant improvement in micro-movements identification. The accuracy of the apex frame detection has elevated to 10% and 17% in the Chinese Academy of Sciences Micro-Expressions (CASME) and the CASME II databases, respectively.},
  archive      = {J_IETIP},
  author       = {Vida Esmaeili and Mahmood Mohassel Feghhi and Seyed Omid Shahdi},
  doi          = {10.1049/ipr2.12596},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3814-3830},
  shortjournal = {IET Image Process.},
  title        = {Spotting micro-movements in image sequence by introducing intelligent cubic-LBP},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight target detection algorithm based on improved
YOLOv4. <em>IETIP</em>, <em>16</em>(14), 3805–3813. (<a
href="https://doi.org/10.1049/ipr2.12595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional YOLOv4 object detection network is difficult to be applied to mobile embedded devices because it has some deficiencies such as complex structure and too many parameters. In this paper, the authors propose a lightweight object detection algorithm. Firstly, Mobienetv3 is used to replace the original feature extraction network, and the Mish function is used to be the activation function, which reduces the number of model parameters. Secondly, the dilated convolution is used to replace the maximum pooling operation in the original spatial pyramid pooling (SPP) structure. Then, a custom Dcn-Dw structure is used to replace the convolution operation in the original PANet, which improves the accuracy of the model for irregular object detection and reduces the model size. Finally, a CBAM lightweight attention mechanism module is introduced in front of the YOLO Head, which further improves the model accuracy. An experiment on the VOC2007 dataset is carried out, and the results show that the mean average precision (mAP) is 80.3% and frames per second (FPS) is 15. At the expense of 3% accuracy, the detection speed is increased to two to three times, and the model size is reduced to one-fifth of the original model. The lightweight object detection algorithm can suit for real-time detection tasks on resource-constrained embedded devices.},
  archive      = {J_IETIP},
  author       = {Lili Wang and Qinghang Ni and Chen Chen and Hailu Yang},
  doi          = {10.1049/ipr2.12595},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3805-3813},
  shortjournal = {IET Image Process.},
  title        = {Lightweight target detection algorithm based on improved YOLOv4},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel deviation density peaks clustering algorithm and its
applications of medical image segmentation. <em>IETIP</em>,
<em>16</em>(14), 3790–3804. (<a
href="https://doi.org/10.1049/ipr2.12594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The density peaks clustering (DPC) algorithm can identify clusters with various shapes and densities in the underlying dataset. However, the DPC algorithm cannot exactly find the true quantity of clustering centers when computing the local density, and it is difficult to handle non-convex datasets. Moreover, the DPC algorithm is difficult to identify boundary points and outliers without a reasonable allocation strategy when dealing with low-density points. To solve these limitations, a novel deviation density peaks clustering (DeDPC) algorithm is proposed. First, the local deviation of the spatial distance of datasets with different structures is utilised to replace the local density to generate a more reasonable clustering center decision graph. Second, a threshold is defined to further divide and process low-density points. Finally, outliers in low-density points can be accurately found to accurately cluster the dataset. To evaluate the performance of the DeDPC algorithm, experiments are conducted on synthetic and real-world datasets and the DeDPC is compared with other clustering methods. The DeDPC is also applied to medical image segmentation to further demonstrate its capability for medical image processing. The simulation results show that the DeDPC method has good validity and utility for both non-convex datasets and medical image segmentation.},
  archive      = {J_IETIP},
  author       = {Wei Zhou and Limin Wang and Xuming Han and Mingyang Li},
  doi          = {10.1049/ipr2.12594},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3790-3804},
  shortjournal = {IET Image Process.},
  title        = {A novel deviation density peaks clustering algorithm and its applications of medical image segmentation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A sample-proxy dual triplet loss function for object
re-identification. <em>IETIP</em>, <em>16</em>(14), 3781–3789. (<a
href="https://doi.org/10.1049/ipr2.12593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object re-identification, such as vehicle re-identification or pedestrian re-identification, plays a significant role in intelligent video surveillance systems for public security. Due to viewpoint variations and appearance changes, both pedestrians and vehicles usually have complex intra-class variations. However, most existing object re-identification methods often use a sample-level triplet loss function cooperating with a single-proxy softmax loss function, which could not handle complex intra-class variations well. In this paper, a sample-proxy dual triplet (SPDT) loss function is proposed, which works with a multi-proxy softmax (MPS) loss function. The MPS loss function is in charge of learning multiple proxies to represent a class. The SPDT loss function is responsible for enlarging inter-class distances as well as shrinking intra-class distances on both sample and proxy levels. Therefore, the method not only handles multi-proxy intra-class variations but also fully learns discrimination on samples and proxies. Experiments on two large datasets, that is, VeRi776 and DukeMTMC-reID, demonstrate that the method is superior to state-of-the-art object re-identification approaches.},
  archive      = {J_IETIP},
  author       = {Hanxiao Wu and Fei Shen and Jianqing Zhu and Huanqiang Zeng and Xiaobin Zhu and Zhen Lei},
  doi          = {10.1049/ipr2.12593},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3781-3789},
  shortjournal = {IET Image Process.},
  title        = {A sample-proxy dual triplet loss function for object re-identification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Colour balance and contrast stretching for sand-dust image
enhancement. <em>IETIP</em>, <em>16</em>(14), 3768–3780. (<a
href="https://doi.org/10.1049/ipr2.12592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasingly frequent sand-dust weather in the inland areas seriously affects outdoor vision applications, especially autonomous vehicles and security monitoring. To moderate the image&#39;s colour cast and poor contrast caused by sand-dust weather, an effective approach is proposed in this study to enhance the sand-dust images. First, the original degraded image&#39;s colour cast is corrected by a new colour balance and compensation formula, which compensates the blue and green channel information through numerous yellow channel information caused by sand-dust scattering before white balance. Next, in order to avoid the new colour deviation, the corrected image is converted from the RGB colour space to the HSV colour space and use the CLAHE to enhance the V component to improve the contrast. Then, a nonlinear gain function is defined to further adaptively sharpen the V component to enhance image details. Finally, the S component is stretched to improve image saturation. The extensive qualitative and quantitative evaluation shows that this method can improve the image edge clarity and contrast, restore good colour fidelity for all sand-dust images tested. The verification also proves that this method is of much significance in improving the feature point extraction and the target detection results in the sand-dust weather.},
  archive      = {J_IETIP},
  author       = {Zhongwei Hua and Lizhe Qi and Min Guan and Hao Su and Yunquan Sun},
  doi          = {10.1049/ipr2.12592},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3768-3780},
  shortjournal = {IET Image Process.},
  title        = {Colour balance and contrast stretching for sand-dust image enhancement},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). STDC-MA network for semantic segmentation. <em>IETIP</em>,
<em>16</em>(14), 3758–3767. (<a
href="https://doi.org/10.1049/ipr2.12591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is applied extensively in autonomous driving and intelligent transportation with methods that highly demand spatial and semantic information. Here, an STDC-MA network is proposed to meet these demands. First, the STDC-Seg structure is employed in STDC-MA to ensure a lightweight and efficient structure. Subsequently, the feature alignment module is applied to understand the offset between high-level and low-level features, solving the problem of pixel offset related to upsampling on the high-level feature map. The approach implements the effective fusion between high-level features and low-level features. A hierarchical multiscale attention mechanism is adopted to reveal the relationship among attention regions from two different input sizes of one image. Through this relationship, regions receiving much attention are integrated into the segmentation results, thereby reducing the unfocused regions of the input image and improving the effective utilisation of multiscale information. STDC-MA maintains the segmentation speed as the STDC-Seg network while improving the segmentation accuracy of small objects. STDC-MA was verified on the validation dataset of Cityscapes. The segmentation result of STDC-MA attained 78.32% mIOU with the input of 0.5× scale, 4.92% higher than STDC-Seg.},
  archive      = {J_IETIP},
  author       = {Xiaochun Lei and Linjun Lu and Zetao Jiang and Zhaoting Gong and Chang Lu and Jiaming Liang and Junlin Xie},
  doi          = {10.1049/ipr2.12591},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3758-3767},
  shortjournal = {IET Image Process.},
  title        = {STDC-MA network for semantic segmentation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep image inpainting via contextual modelling in ADCT
domain. <em>IETIP</em>, <em>16</em>(14), 3748–3757. (<a
href="https://doi.org/10.1049/ipr2.12590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pixel-based generative image inpainting has been widely researched over recent years and certain level of success via deep learning of feature representations and hallucinations of missing pixel values from surrounding backgrounds have also been reported in the literature. However, existing approaches rely on context-based attentions and progressive inferences to capture the pixel correlations yet such pixel-based approaches often fail to adapt to the constantly varying ranges and distances among surrounding background pixels. On the other hand, the modelling cost is also increasingly expensive whenever correlations of those pixels at longer distance away are to be exploited. To resolve the problem, we implement the principle of learning and hallucinating frequency components rather than pixel values. Therefore, we can avoid the dilemma that, on one hand the wish is to exploit all correlated pixels inside the image no matter how far away they are spatially located, but on the other, the price of increasing the modelling cost incurred by those pixels far away from the missing regions has to be paid. Extensive experiments carried out verify the effectiveness of the proposed method, which outperforms the representative existing state of the arts in terms of all assessment metrics.},
  archive      = {J_IETIP},
  author       = {Adhiyaman Manickam and Jianmin Jiang and Yu Zhou},
  doi          = {10.1049/ipr2.12590},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3748-3757},
  shortjournal = {IET Image Process.},
  title        = {Deep image inpainting via contextual modelling in ADCT domain},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic-difference based generative adversarial network for
coal-rock fracture evolution prediction. <em>IETIP</em>,
<em>16</em>(14), 3737–3747. (<a
href="https://doi.org/10.1049/ipr2.12589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coal-rock fracture evolution has a key role in coal seam permeability. Due to the randomness and uncertainty of coal-rock fractures, the prediction of fracture evolution is difficult and challenging. In this paper, the authors propose a dynamic-difference based generative adversarial network (DDGAN) for coal-rock fracture evolution prediction. Firstly, the spatial-feature encoder and the dynamic-difference encoder are proposed to capture the spatial features and the dynamic fracture evolution information independently. And a channel-attention (CA) module is presented to enhance the contribution of fracture evolution details information in the dynamic-difference encoder. Then, a multi-scale fusion (MSF) module is proposed to fuse the spatial features and the dynamic-difference features, which benefits to refine the detailed structure during decoding. Final, the compound objective function is employed to supervise and guide the network to achieve coal-rock fracture evolution predictions. Compared with the state-of-the-art methods, extensive experiment results demonstrate that the authors’ model can achieve better performance for the task of coal-rock fracture evolution prediction.},
  archive      = {J_IETIP},
  author       = {Fengli Lu and Guoying Zhang and Yi Ding and Yongqi Gan},
  doi          = {10.1049/ipr2.12589},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3737-3747},
  shortjournal = {IET Image Process.},
  title        = {Dynamic-difference based generative adversarial network for coal-rock fracture evolution prediction},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Polarimetric SAR image classification using binary
coding-based polarimetric-morphological features. <em>IETIP</em>,
<em>16</em>(14), 3715–3736. (<a
href="https://doi.org/10.1049/ipr2.12587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polarimetric synthetic aperture radar (POLSAR) systems provide high resolution images containing polarimetric information. So, they have high capability in land cover classification. In this work, a binary coding-based polarimetric-morphological (BCPM) feature extraction is proposed for POLSAR image classification. At first, a set of polarimetric features is proposed. Then, a new morphological framework is introduced for contextual feature extraction from the POLSAR cube. The coherence matrix is composed from diagonal and non-diagonal elements with different information. These elements are analysed separately in the proposed method. Moreover, the amplitude and phase components of the non-diagonal elements are individually analysed using morphological filters by reconstruction. Finally, a binary coding-based polarimetric-spatial feature reduction, which uses the first order statistics, is proposed for feature transformation. The experiments on three real POLSAR images and a synthetic dataset show the superior performance of BCPM compared to several classification methods.},
  archive      = {J_IETIP},
  author       = {Maryam Imani},
  doi          = {10.1049/ipr2.12587},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3715-3736},
  shortjournal = {IET Image Process.},
  title        = {Polarimetric SAR image classification using binary coding-based polarimetric-morphological features},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved nonlocal means-based correction strategy for
mixed noise removal. <em>IETIP</em>, <em>16</em>(14), 3701–3714. (<a
href="https://doi.org/10.1049/ipr2.12586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noise removal is a classic problem. Most researchers focus on Gaussian noise removal due to the regularity of the noise distribution, while mixed noise removal is always challenging because of the uncertainty of the noise distribution. Mixtures of additive white Gaussian noise (AWGN) with salt-and-pepper impulse noise (SPIN) and mixtures of AWGN with random-valued impulse noise (RVIN) are typical examples of mixed noise. Most mixed noise removal methods are effective in the removal of mixed AWGN and SPIN, but perform poorly in the removal of AWGN and RVIN. The main reason is the randomness of RVIN, which leads to poor denoising performance when the RVIN is strong. In this paper, an improved nonlocal means-based correction strategy (INS) is proposed. In INS, an improved nonlocal means strategy is applied to replace the impulse noise pixels to make the mixed noise obey an approximate Gaussian distribution. To prove the validity of INS, a convolutional neural network (CNN) in combination with INS (CNNINS) is applied to remove mixed noise. Experimental results are used to compare the proposed CNNINS with the most advanced mixed noise removal methods.},
  archive      = {J_IETIP},
  author       = {Yuhao Shao and Jielin Jiang and Xiangming Hong},
  doi          = {10.1049/ipr2.12586},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3701-3714},
  shortjournal = {IET Image Process.},
  title        = {An improved nonlocal means-based correction strategy for mixed noise removal},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An effective LRTC model integrated with total α-order
variation and boundary adjustment for multichannel visual data
inpainting. <em>IETIP</em>, <em>16</em>(13), 3684–3699. (<a
href="https://doi.org/10.1049/ipr2.12585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restoring damaged multichannel visual data with high loss ratio is quite a challenging task. To address this problem, an effective LRTC (low-rank tensor completion) model integrated with total α-order variation (TV α ) in the fractional bounded variation space BV α is proposed to perform superior fractional-in-space regularization. Based on using LR constraint to restore global patterns, TV α regularization is integrated to exploit nonlocally-correlated information on each channel to infer the lost data and simultaneously effectively deal with complex details due to the powerful fractional calculus. Then, a nonlocal fractional regularization strategy for multi-dimensional data and an effective numerical optimization method are creatively designed to solve this problem. Two novel fractional derivative matrix approximations are derived and applied to the first two unfolding modes of the tensor respectively to conveniently solve the fractional regularization subproblem by using an element-wise shrinkage-thresholding operation. In addition, boundary extension and adjustment strategy are designed for the unfolded matrices to alleviate the influence of inaccurate boundary conditions in fractional derivative computations. Experiments are conducted to illustrate its performance and efficiency for YUV video, RGB and HSI restoration, especially its ability to effectively recover complex structures and the details of multi-component visual data with relatively high missing rate.},
  archive      = {J_IETIP},
  author       = {Xiuhong Yang and Yi Xue and Zhiyong Lv and Haiyan Jin},
  doi          = {10.1049/ipr2.12585},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3684-3699},
  shortjournal = {IET Image Process.},
  title        = {An effective LRTC model integrated with total α-order variation and boundary adjustment for multichannel visual data inpainting},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blood-contaminated endoscopic image restoration based on
residual VQ-VAE with cascaded structure. <em>IETIP</em>,
<em>16</em>(13), 3669–3683. (<a
href="https://doi.org/10.1049/ipr2.12584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research studies an image reconstruction technology that can help doctors observe endoscopic images obscured by blood in transurethral procedures. A new network structure is proposed, which applies encoder–decoder deep neural network to achieve the purpose of denoising and uses residual learning and cascade learning methods to relieve the problem of insufficient clarity and lack of detail in reconstruction image when restoring endoscopic images suffered from blood occlusion. The residual mechanism is used to estimate noise and compensate the images, instead of reconstructing overall image by an encoder and decoder deep network. It can potentially make the details of the image to be preserved and improve the resolution of the reconstructed image. This denoising model is embedded into a cascade network framework to further improve the quality of the restored image through the idea of repeated learning multiple times. On a customized endoscopic image sample set with real blood noise, for verifying the denoising ability, we designed two approaches of data augmentation to generate noised images obscured by different types and degree blood noises. In experiments, the new model has been compared to other approaches and the certain improved denoising effects can be observed.},
  archive      = {J_IETIP},
  author       = {Jie Lin and Yulong Pan and Yuan Dang and Yige Bao and Hui Zhuo},
  doi          = {10.1049/ipr2.12584},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3669-3683},
  shortjournal = {IET Image Process.},
  title        = {Blood-contaminated endoscopic image restoration based on residual VQ-VAE with cascaded structure},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Corrected rank residual constraint model for image
denoising. <em>IETIP</em>, <em>16</em>(13), 3659–3668. (<a
href="https://doi.org/10.1049/ipr2.12583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel image denoising model is proposed, named Corrected Rank Residual Constraint (CRRC). To overcome the drawback that L 1 norm penalty yields biased estimators and cannot achieve the best estimation performance, the proposed CRRC incorporates the adaptive correction term with L 1 minimization to improve the sparsity of the L 1 norm data fidelity term enforced on the rank residual. The existence and uniqueness of solutions of the proposed model are also studied, and the optimization method for solving the model is given. The studies show that the adaptive correction term can not only improve the sparsity of the rank residual but also overcome the over-shrinkage of large singular values. Experimental results demonstrate that the proposed CRRC model outperforms many state-of-the-art methods in both objective and perceptual quality.},
  archive      = {J_IETIP},
  author       = {Di Wu and Tao Zhang},
  doi          = {10.1049/ipr2.12583},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3659-3668},
  shortjournal = {IET Image Process.},
  title        = {Corrected rank residual constraint model for image denoising},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixed-attention-based regional soft partition network for
vehicle reidentification. <em>IETIP</em>, <em>16</em>(13), 3648–3658.
(<a href="https://doi.org/10.1049/ipr2.12582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of vehicle reidentification is to distinguish the same vehicle from different vehicles using different cameras. The main challenge of this task is the significant intra-instance discrepancy of the same vehicle from different views and the subtle inter-instance differences of similar vehicles from the same views. To address this problem, researchers have attempted to align features from different views, such as using additional metadata (colour, type, key points, mask, etc.) to improve performance. Although these attempts improve the performance of vehicle reidentification, considerable efforts are required to create additional precise annotations to the data before using these methods. This results in expensive data preparation costs, rendering these methods less convenient to use. Therefore, we propose a novel deep learning network, called a mixed-attention-based regional soft partition network, to address this problem. This network does not require additional metadata annotations; it only trains the identity label as a supervision signal and uses soft partition attention to identify specific vehicle regions. Experiments showed that the performance of the proposed method was comparable to that of the state-of-the-art method with additional annotations on the VeRri-776 and VERI-Wild datasets.},
  archive      = {J_IETIP},
  author       = {Zhiyong Li and Yunzhong Luo and Qiaochu Li and Lulu Song and Weiyi Liu},
  doi          = {10.1049/ipr2.12582},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3648-3658},
  shortjournal = {IET Image Process.},
  title        = {Mixed-attention-based regional soft partition network for vehicle reidentification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast robust fuzzy clustering based on bipartite graph for
hyper-spectral image classification. <em>IETIP</em>, <em>16</em>(13),
3634–3647. (<a href="https://doi.org/10.1049/ipr2.12581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyper-spectral image (HSI) clustering has become a hot spot in remote sensing image research. However, due to the large amount and high dimension of HSI data, some traditional image clustering methods are no longer suitable, and even lead to poor clustering performance and long computing time. Therefore, this paper proposes a novel clustering method for HSI to make up for the shortcomings of traditional clustering methods. Firstly, a fuzzy similarity matrix is constructed by using the bipartite graph to obtain low-dimensional hyper-spectral data, which can reduce the complexity and the operation time of the algorithm. Secondly, fuzzy membership mapping is carried out through the constructed bipartite graph, and a non-negative regularization term is added to limit ill-conditioned problems. Thirdly, this paper uses the Geman-McClure function to optimize Euclidean distance. Finally, in this paper, the spectral information of HSI is considered while the neighbourhood information of HSI is added to achieve the unity of space spectrum, which enhances the robustness of the algorithm and improves the clustering performance of the algorithm. Compared with existing HSI clustering algorithms, the proposed algorithm has higher clustering accuracy and efficiency.},
  archive      = {J_IETIP},
  author       = {Han Liu and Chengmao Wu and Changxing Li and Yanqun Zuo},
  doi          = {10.1049/ipr2.12581},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3634-3647},
  shortjournal = {IET Image Process.},
  title        = {Fast robust fuzzy clustering based on bipartite graph for hyper-spectral image classification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IterNet++: An improved model for retinal image segmentation
by curvelet enhancing, guided filtering, offline hard-sample mining, and
test-time augmenting. <em>IETIP</em>, <em>16</em>(13), 3617–3633. (<a
href="https://doi.org/10.1049/ipr2.12580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical medicine, the segmentation of blood vessels in retinal images is essential for subsequent analysis in clinical diagnosis. However, retinal images are often noisy and their vascular structure is relatively tiny, which poses significant challenges for vessel segmentation. To improve the performance of vessel segmentation, an improved model IterNet++ based on the architecture of IterNet is proposed. First, curvelet signal analysis is applied to enhance retinal images. Second, residual convolution (ResConv) blocks and guided filters are introduced to utilise the encoder features of previous iterations in the model to reduce overfitting. Third, offline hard-sample mining is used to improve segmentation performance by utilising training samples with low segmentation accuracy as many possible on a few-sample training set. In addition, a test-time augmentation method is applied to testing samples in test dataset during inference. Extensive experiments show that this model achieves Dice scores of 0.8313, 0.8277, and 0.8372 on DRIVE, CHASE-DB1, and STARE datasets, respectively, demonstrating the best performance compared with IterNet and other baseline models.},
  archive      = {J_IETIP},
  author       = {M. Zhu and K. Zeng and G. Lin and Y. Gong and T. Hao and K. Wattanachote and X. Luo},
  doi          = {10.1049/ipr2.12580},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3617-3633},
  shortjournal = {IET Image Process.},
  title        = {IterNet++: An improved model for retinal image segmentation by curvelet enhancing, guided filtering, offline hard-sample mining, and test-time augmenting},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature hallucination in hypersphere space for few-shot
classification. <em>IETIP</em>, <em>16</em>(13), 3603–3616. (<a
href="https://doi.org/10.1049/ipr2.12579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot classification (FSC) targeting at classifying unseen classes with few labelled samples is still a challenging task. Recent works show that transfer-learning based approaches are competitive with meta-learning ones, which usually pre-train a convolutional neural networks (CNN)-based network using cross-entropy (CE) loss and throw away the last layer to post-process the novel classes. Hereby, they still suffer the issue of getting a more transferable extractor and lacking enough labelled novel samples. Thus, the authors propose the algorithm of feature hallucination in hypersphere space (FHHS) for FSC. On the first stage, the authors pre-train a more transferable feature extractor using a hypersphere loss (HL), which supplies CE with supervised contrastive (SC) loss and self-supervised loss (SSL), in which SC can map the base and novel images onto the hypersphere space densely. On the second stage, the authors generate new samples for unseen classes using their novel algorithm of synthetic novel sampling with the base (SNSB), which linearly interpolate between each novel class prototype and its K nearest neighbour base class prototypes. Comprehensive experiments on multiple popular FSC demonstrate that HL loss can enhance the performance of backbone network and the authors’ feature hallucination method is superior to the existing hallucination-based methods.},
  archive      = {J_IETIP},
  author       = {Sai Yang and Fan Liu and Zhiyu Chen},
  doi          = {10.1049/ipr2.12579},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3603-3616},
  shortjournal = {IET Image Process.},
  title        = {Feature hallucination in hypersphere space for few-shot classification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SMDAF: A novel keypoint based method for copy-move forgery
detection. <em>IETIP</em>, <em>16</em>(13), 3589–3602. (<a
href="https://doi.org/10.1049/ipr2.12578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Copy–move forgery poses a significant threat to social life and has aroused much attention in recent years. Although many copy-move forgery detection (CMFD) methods have been proposed, the most existing CMFD methods are short of adaptability in detecting images, which leads to the limitation on detection effects. To solve this problem, the paper proposes a novel keypoint-based CMFD method: second-keypoint matching and double adaptive filtering (SMDAF). Motivated by image matching based on keypoint, the second-keypoint matching method is designed to match keypoints extracted from copy–move forgery images, which can be used for both the single-CMFD and the multiple-CMFD. Then, a double adaptive filter (DAF) based on the AdaLAM algorithm and the KANN-DBSCAN clustering algorithm to filter wrong keypoint matches adaptively are proposed, according to the distinct distribution of keypoints in each image. Finally, the forgery regions are presented by finding their convex hulls and padding them. Compared with existing methods, extensive experiments show that the SMDAF method significantly provides more efficiency in detecting images under simulated real-world conditions, has better robustness when facing images with different post-treatment attacks, and is more effective in distinguishing images that look copy–move forged but are real.},
  archive      = {J_IETIP},
  author       = {Guangyu Yue and Qing Duan and Renyang Liu and Wenyu Peng and Yun Liao and Junhui Liu},
  doi          = {10.1049/ipr2.12578},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3589-3602},
  shortjournal = {IET Image Process.},
  title        = {SMDAF: A novel keypoint based method for copy-move forgery detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single image reflection removal through multi-scale gradient
refinement. <em>IETIP</em>, <em>16</em>(13), 3579–3588. (<a
href="https://doi.org/10.1049/ipr2.12577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Removing the undesired reflection layer from images taken through glass windows is an important yet challenging task. Many existing CNN-based methods try to utilize the gradient as an important clue to guide the training and achieve better separation. But the scene depth of real-world scenarios is usually uncontrollable, leading to the uncertainty of smooth level in the transmission and reflection layers, which makes it a great challenge to model the two layers in the gradient domain. This paper proposes a multi-scale gradient refinement network to resolve this problem. First, it is suggested that even the two layers are usually partially smooth, their gradients can still be sharp in the down-scaled samples. To this end, the separation is conducted at four different scales by minimizing the similarity of the two layers to boost the gradient sharpness prior. Second, it is considered that the separation performance of downscaled samples is usually superior to that of the high-resolution images because of the sharper edges. For this reason, a cascade architecture is designed that takes the down-scaled predictions to promote the high-resolution decomposition stage-by-stage to recover the full-resolution results. Besides, the scale-wise memory mechanism is introduced into the prediction network to resolve the detail loss issue caused by the multi-stage upscaling refinement process. The experimental results on benchmark datasets indicate that the new model surpasses several state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Kuanhong Cheng and Juan Du and Jia Li and Yuxin Li and Yilan Li and Junhuai Li},
  doi          = {10.1049/ipr2.12577},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3579-3588},
  shortjournal = {IET Image Process.},
  title        = {Single image reflection removal through multi-scale gradient refinement},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Content-augmented feature pyramid network with light linear
spatial transformers for object detection. <em>IETIP</em>,
<em>16</em>(13), 3567–3578. (<a
href="https://doi.org/10.1049/ipr2.12575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the prevalent components, feature pyramid network (FPN) is widely used in current object detection models for improving multi-scale object detection performance. However, its feature fusion mode is still in a misaligned and local manner, thus limiting the representation power. To address the inherited defects of FPN, a novel architecture termed content-augmented feature pyramid network (CA-FPN) is proposed in this paper. Firstly, a global content extraction module (GCEM) is proposed to extract multi-scale context information. Secondly, lightweight linear spatial Transformer connections are added in the top-down pathway to augment each feature map with multi-scale features, where a linearized approximate self-attention function is designed for reducing model complexity. By means of the self-attention mechanism in Transformer, it is no longer needed to align feature maps during feature fusion, thus solving the misaligned defect. By setting the query scope to the entire feature map, the local defect can also be solved. Extensive experiments on COCO and PASCAL VOC datasets demonstrated that the CA-FPN outperforms other FPN-based detectors without bells and whistles and is robust in different settings.},
  archive      = {J_IETIP},
  author       = {Yongxiang Gu and Xiaolin Qin and Yuncong Peng and Lu Li},
  doi          = {10.1049/ipr2.12575},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3567-3578},
  shortjournal = {IET Image Process.},
  title        = {Content-augmented feature pyramid network with light linear spatial transformers for object detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust linear unmixing with enhanced constraint of
classification for hyperspectral remote sensing imagery. <em>IETIP</em>,
<em>16</em>(13), 3557–3566. (<a
href="https://doi.org/10.1049/ipr2.12568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although hyperspectral data, especially spaceborne images, are rich in spectral information, their spatial resolution is usually low due to the limitation of sensor design and other factors. Therefore, for the application of hyperspectral images, unmixing technology is a key processing technology, such as linear mixing model and its derived algorithms have made a certain progress. However, a real scene often contains both pure and mixed pixels. The existing methods usually ignore the consideration and analysis of this situation in the process of model design and simulation experiment. In this context, this paper proposes a robust linear unmixing model with the enhanced constraint of classification for hyperspectral image. In general, it designs a framework combining unmixing and classification. In the task for real scene data, endmembers are extracted first, and then the hard classification term constructed after the expansion of endmembers (training samples) based on similarity is introduced to provide the sparsity constraint of the overall model, so as to realize relatively complete adjustment and effective image unmixing under complex conditions. Considering the scene with different distributions, the simulation experiment designs several groups of data tests, including different proportions of pure and mixing pixels. The unmixing results of three simulated datasets and two real datasets show that the unmixing results of this method are better than those of the other six comparison methods. This model improves the accuracy of unmixing and realizes effective unmixing.},
  archive      = {J_IETIP},
  author       = {Haoyang Yu and Jinxue Chi and Xiaodi Shang and Xueji Shen and Jocelyn Chanussot and Yimin Shi},
  doi          = {10.1049/ipr2.12568},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3557-3566},
  shortjournal = {IET Image Process.},
  title        = {Robust linear unmixing with enhanced constraint of classification for hyperspectral remote sensing imagery},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention transfer from human to neural networks for road
object detection in winter. <em>IETIP</em>, <em>16</em>(13), 3544–3556.
(<a href="https://doi.org/10.1049/ipr2.12562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an essential feature of autonomous road vehicles, obstacle detection must be executed on a real-time onboard platform with high accuracy. Cameras are still the most commonly used sensors in autonomous driving. Most detections using cameras are based on convolutional neural networks. In this regard, a recent teacher–student approach, called transfer learning, has been used to improve the neural network training process. This approach has only been used with a neural network acting as a teacher to the best of our knowledge. This paper proposes a novel way of improving training data based on attention transfer by getting the attention map from a human. The proposed method allows the dataset size reduction by 50%, which leads to up to a 60% decline in the training time. The experimental results indicate that the proposed method can enhance the F1-score of the network by up to 10% in winter conditions.},
  archive      = {J_IETIP},
  author       = {Jonathan Boisclair and Sousso Kelouwani and Follivi Kloutse Ayevide and Ali Amamou and Muhammad Zeshan Alam and Kodjo Agbossou},
  doi          = {10.1049/ipr2.12562},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3544-3556},
  shortjournal = {IET Image Process.},
  title        = {Attention transfer from human to neural networks for road object detection in winter},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A dual quantum image feature extraction method: PSQIFE.
<em>IETIP</em>, <em>16</em>(13), 3529–3543. (<a
href="https://doi.org/10.1049/ipr2.12561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In digital image processing, feature extraction occupies a very important position, which is related to the effect of image classification or recognition. At present, effective quantum feature extraction methods are relatively lacking. And the current feature extraction methods are mainly devoted to the extraction of basic features of images, failing to consider the global features of classical images and the global features of quantum images comprehensively. In this paper, we propose a dual quantum image feature extraction method named PSQIFE, which focuses on the global energy representation of images by constructing dual quantum image global features. The representation of the global features of the dual quantum image is obtained by quantum superposition of two parts of quantum state features. In this paper, quantum image reconstruction and quantum image fidelity tests are performed on the extracted global features by 9 classes of classical images, and the overall fidelity is above 95%. In addition, the effectiveness of PSQIFE dual quantum image feature extraction method is verified by comparing the image classification test with convolutional feature extraction method on Mnist dataset. The method has some reference significance for the research of quantum image feature extraction and classification.},
  archive      = {J_IETIP},
  author       = {Jie Su and Shuhan Lu and Lin Li},
  doi          = {10.1049/ipr2.12561},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3529-3543},
  shortjournal = {IET Image Process.},
  title        = {A dual quantum image feature extraction method: PSQIFE},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DMA-net: Dual multi-instance attention network for x-ray
image classification. <em>IETIP</em>, <em>16</em>(13), 3518–3528. (<a
href="https://doi.org/10.1049/ipr2.12560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Security inspection has been playing a critical role in protecting public space from safety threats. As object detection is a fundamental and mature research filed, it still suffers from numerous challenges such as scale, viewpoint and intra-class variance of X-ray images. The main reasons are, mis-modeling of actual security inspection as well as insufficient effective X-ray samples. To this end, the X-ray inspection task is reconsidered by predicting the dangerous attributes without boxes and categories, a Dual Multi-instance Attention network (DMA-Net) is proposed in this paper to mine the key-instance from both patch and proposal branch. In patch-level multi-instance attention pooling, Recursive Attention Pyramid (RAP) and Spatial-Channel Attention (SCA) are proposed to effectively learn discriminative representation from hierarchical features. Meanwhile, the proposal-level multi-instance attention pooling selectively emphasizes interdependent of crucial regions. The features of dual pooling layers are fused to predict dangerous attributes of X-ray inspection images directly. Further, this paper contributes a large-scale and high-quality X-ray image dataset from railway stations, named Railway Station X-ray (RSXray). Enormous experiments on RSXray, GDXray and SIXray demonstrate that the proposed DMA-Net achieves surprising performance with dual multi instance learning diagrams and attention mechanism. The superiority of the employed framework for applications in real world scenarios are also verified.},
  archive      = {J_IETIP},
  author       = {Shuoyan Liu and Enze Yang and Yuxin Liu and Shitao Zhao},
  doi          = {10.1049/ipr2.12560},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3518-3528},
  shortjournal = {IET Image Process.},
  title        = {DMA-net: Dual multi-instance attention network for X-ray image classification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Atmospheric turbulence degraded image restoration using a
modified dilated convolutional network. <em>IETIP</em>, <em>16</em>(13),
3507–3517. (<a href="https://doi.org/10.1049/ipr2.12559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-distance imaging is generally blurry and warped due to the presence of atmospheric turbulence, which harms the performance of the photoelectric system. Among these, real-time restoration based on a single turbulence-degraded image has always been a challenging topic that everyone is concerned about. The approach performed here optimizes the convolutional neural network using residual learning and smoothed dilated convolutions, which may increase the field of vision under the situation of limited GPU memory. To identify the model&#39;s performance, the authors employ training and test data with strong, medium, and weak levels synthesized by the Fried kernel, the real-time data captured by the Ritchey–Chretien telescope, the Open Turbulent Images Set and the real comparative data. Furthermore, the proposed model is compared to previous state-of-the-art approaches. The experimental results demonstrate that the proposed novel model can recover turbulently degraded images more effectively.},
  archive      = {J_IETIP},
  author       = {Changdong Su and Xiaoqing Wu and Yiming Guo and Shitai Zhang and Zhiyuan Wang and Dongfeng Shi},
  doi          = {10.1049/ipr2.12559},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3507-3517},
  shortjournal = {IET Image Process.},
  title        = {Atmospheric turbulence degraded image restoration using a modified dilated convolutional network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contour information regularized tensor ring completion for
realistic image restoration. <em>IETIP</em>, <em>16</em>(13), 3499–3506.
(<a href="https://doi.org/10.1049/ipr2.12551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor completion has gained considerable research interest in recent years and has been frequently applied to image restoration. This type of method basically employs the low-rank nature of images, implicitly requiring that the whole picture is of globally consistent features. As a result, existing tensor completion algorithms often give reasonably good performance if the target image has only random pixel-level missing. Unfortunately, pixel-level missing is very rare in practice and it is often wanted to restore an image with irregular hole-shaped missing, such as removing electricity poles from landscape photos or irrelevant people from tourist photos. This task is extremely difficult for traditional low-rank based tensor completion methods. To overcome this drawback, a Contour Information regularized Tensor RIng Completion (CITRIC) method is proposed for practical image restoration. Meanwhile, the contour information regularization is used to capture significant local features, whereas the low-rank tensor ring structure is utilized to capture as much global information as possible. The alternating direction method of multipliers (ADMM) is adopted to optimize the cost function. Extensive experimental results using real-world images show that CITRIC is more practical than existing methods and can restore real-world images with irregular hole-shaped missing.},
  archive      = {J_IETIP},
  author       = {Zhi Yu and Yihao Luo and Zhifa Liu and Guoxu Zhou},
  doi          = {10.1049/ipr2.12551},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3499-3506},
  shortjournal = {IET Image Process.},
  title        = {Contour information regularized tensor ring completion for realistic image restoration},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast bilateral complementary network for deep learning
compressed sensing image reconstruction. <em>IETIP</em>,
<em>16</em>(13), 3485–3498. (<a
href="https://doi.org/10.1049/ipr2.12545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Great progress has been made in deep learning image reconstruction, but there are still problems that need to be improved, such as the extraction of image texture details and the improvement of the overall contour quality of the image, and how to reduce the transmission cost is also the focus of research. This paper proposes a fast bilateral network suitable for both grayscale and color image reconstruction. In the compression network, bilinear interpolation, fully connected layer and convolutional neural network are selected for image compression. The reconstruction network is divided into two parts: the texture path and contour path, the former reconstructs the remaining texture details of the image, and the latter performs deep contour reconstruction. The bilateral complementary residual connection method transfers the texture information to the contour path and improves the contour quality, and the improvement of the contour quality can improve the learning of texture details. Through a large number of data tests, it shows that the network in this paper has achieved comparable or better results in terms of image reconstruction quality, time-consuming and robustness against noise. It solves the problem of memory consumption for storing a large number of images, and also provides convenience for shortening space and time during image transmission.},
  archive      = {J_IETIP},
  author       = {Guo Yuan and Jiang Jinlin and Chen Wei},
  doi          = {10.1049/ipr2.12545},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3485-3498},
  shortjournal = {IET Image Process.},
  title        = {Fast bilateral complementary network for deep learning compressed sensing image reconstruction},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integrating deep learning and traditional image enhancement
techniques for underwater image enhancement. <em>IETIP</em>,
<em>16</em>(13), 3471–3484. (<a
href="https://doi.org/10.1049/ipr2.12544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images usually suffer from colour distortion, blur, and low contrast, which hinder the subsequent processing of underwater information. To address these problems, this paper proposes a novel approach for single underwater images enhancement by integrating data-driven deep learning and hand-crafted image enhancement techniques. First, a statistical analysis is made on the average deviation of each channel of input underwater images to that of its corresponding ground truths, and it is found that both the red channel and the green channel of an underwater image contribute to its colour distortion. Concretely, the red channel of an underwater image is usually seriously attenuated, and the green channel is usually over strengthened. Motivated by such an observation, an attention mechanism guided residual module for underwater image colour correction is proposed, where the colour of the red channel of the underwater image and that of the green channel is compensated in a different way, respectively. Coupled with an attention mechanism, the residual module can adaptively extract and integrate the most discriminative features for colour correction. For scene contrast enhancement and scene deblurring, the traditional image enhancement techniques such as CLAHE (contrast limited adaptive histogram equalization) and Gamma correction are coupled with a multi-scale convolutional neural network (MSCNN), where CLAHE and Gamma correction are used as complement to deal with the complex and changeable underwater imaging environment. Experiments on synthetic and real underwater images demonstrate that the proposed method performs favourably against the state-of-the-art underwater image enhancement methods.},
  archive      = {J_IETIP},
  author       = {Zhenghao Shi and Yongli Wang and Zhaorun Zhou and Wenqi Ren},
  doi          = {10.1049/ipr2.12544},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3471-3484},
  shortjournal = {IET Image Process.},
  title        = {Integrating deep learning and traditional image enhancement techniques for underwater image enhancement},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Region-based two-stage MRI bone tissue segmentation of the
knee joint. <em>IETIP</em>, <em>16</em>(13), 3458–3470. (<a
href="https://doi.org/10.1049/ipr2.12475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical image segmentation, the neural network structure of the U-Net family has demonstrated sufficient advantages. However, MRI images have different scan parameters and different scan times, resulting in different feature representation of the images. Furthermore, there is a great class imbalance between bone and cartilage tissues in MRI knee images. To address these issues, a region-based two-stage MRI knee bone tissue segmentation network is proposed in this paper. The segmentation network makes full use of the location characteristics of the three types of bone tissue in the knee joint and uses a two-stage network architecture with a modified U 2 -Net backbone network to segment MRI knee bone tissue. The neural network structure is divided into two phases, the first phase with a simple coded decoding structure for saliency detection to obtain the positional regional relationships of different bone tissues, and the second phase with a segmentation network consisting of 2 modified U 2 -Net, one for segmenting the patella and associated cartilage and the other for segmenting the femur, tibia and associated cartilage. The algorithm was tested with a variety of MRI knee data to verify the effectiveness of the algorithm.},
  archive      = {J_IETIP},
  author       = {Jianping Mao and Peng Men and Hao Guo and Jubai An},
  doi          = {10.1049/ipr2.12475},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3458-3470},
  shortjournal = {IET Image Process.},
  title        = {Region-based two-stage MRI bone tissue segmentation of the knee joint},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Corner-based object detection method for reactivating box
constraints. <em>IETIP</em>, <em>16</em>(13), 3446–3457. (<a
href="https://doi.org/10.1049/ipr2.12576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corner-based detectors usually generate a large number of false detection boxes because of insufficient attention to the detection area. Recent corner-based detectors can achieve good performance, but the training equipment requirements have greatly increased. For example, due to the dense network structure and the large input image size, CenterNet requires expensive equipment for network training (e.g. Tesla V100). Its performance will be greatly reduced when a more mainstream and cheaper device is used for fine-tuning. The high equipment requirements make it difficult for most researchers to follow up these studies. In this work, CenternessNet, a detector that adds additional box-edge length constraints to CenterNet is proposed, thereby allowing the network to be trained on more general devices and obtain a better performance. It simply introduces the box as a constraint into the corner-based network. In this way, the method improves the ability to aggregate corners during training and enhances the model&#39;s ability to discriminate the corners of objects in the same category to some extent. The method achieves a better performance than other corner-based detection networks trained on similar low-memory devices.},
  archive      = {J_IETIP},
  author       = {Guoqing Zhao and Tianyang Dong and Yiming Jiang},
  doi          = {10.1049/ipr2.12576},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3446-3457},
  shortjournal = {IET Image Process.},
  title        = {Corner-based object detection method for reactivating box constraints},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on methods, datasets and implementations for scene
text spotting. <em>IETIP</em>, <em>16</em>(13), 3426–3445. (<a
href="https://doi.org/10.1049/ipr2.12574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text Spotting is the union of the tasks of detection and transcription of the text that is present in images. Due to the various problems often found when retrieving text, such as orientation, aspect ratio, vertical text or multiple languages in the same image, this can be a challenging task. In this paper, the most recent methods and publications in this field are analysed and compared. Apart from presenting features already seen in other surveys, such as their architectures and performance on different datasets, novel perspectives for comparison are also included, such as the hardware, software, backbone architectures, main problems to solve, or programming languages of the algorithms. The review highlights information often omitted in other studies, providing a better understanding of the current state of research in Text Spotting, from 2016 to 2022, current problems and future trends, as well as establishing a baseline for future methods development, comparison of results and serving as guideline for choosing the most appropriate method to solve a particular problem.},
  archive      = {J_IETIP},
  author       = {Pablo Blanco-Medina and Eduardo Fidalgo and Enrique Alegre and Víctor González-Castro},
  doi          = {10.1049/ipr2.12574},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3426-3445},
  shortjournal = {IET Image Process.},
  title        = {A survey on methods, datasets and implementations for scene text spotting},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comprehensive review of video steganalysis.
<em>IETIP</em>, <em>16</em>(13), 3407–3425. (<a
href="https://doi.org/10.1049/ipr2.12573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Steganography is the art of secret communication and steganalysis is the art of detecting the hidden messages embedded in digital media covers. One of the covers that is gaining interest in the field is video. Presently, the global IP video traffic forms the major part of all consumer Internet traffic. It is also gaining attention in the field of digital forensics and homeland security in which threats of covert communications hold serious consequences. Thus, steganography technicians will prefer video to other types of covers like audio files, still images, or texts. Moreover, video steganography will be of more interest because it provides more concealing capacity. Contrariwise, investigation in video steganalysis methods does not seem to follow the momentum even if law enforcement agencies and governments around the world support and encourage investigation in this field. In this paper, the authors review the most important methods used so far in video steganalysis and sketch the future trends. To the best of the authors’ knowledge this is the most comprehensive review of video steganalysis produced so far.},
  archive      = {J_IETIP},
  author       = {Mourad Bouzegza and Ammar Belatreche and Ahmed Bouridane and Mohamed Tounsi},
  doi          = {10.1049/ipr2.12573},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3407-3425},
  shortjournal = {IET Image Process.},
  title        = {A comprehensive review of video steganalysis},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An automatic breast computer-aided diagnosis scheme based on
a weighted fusion of relevant features and a deep CNN classifier.
<em>IETIP</em>, <em>16</em>(12), 3394–3406. (<a
href="https://doi.org/10.1049/ipr2.12572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mammography continues to play a central part in the early breast masses diagnosis and has raised several challenges in breast cancer detection. Nevertheless, it remains still difficult to detect abnormalities in a dense breast and some benign lesions may have similar appearance with masses. This study proposes a novel Computer-Aided Diagnosis (CADx) allowing benign and malignant mass classification. Accordingly, relevant textural-shape descriptors are proposed, which is the aggregation of a new Local Binary based feature, namely the Monogenic Gray Level and Local Difference (MGLLD), where both texture characteristics and breast tissue density information, and the Zernike moments are incorporated. A heuristic algorithm is then devised for optimizing the proposed features with respect to inter-class and intra-class distribution, by a convenient ponderation. Then, a set of classifiers are applied for opting to the best decision making solution. The Deep CNN yields the best accuracy, with an Area Under Curve (AUC) of 0.98 on Curated Breast Imaging Subset of DDSM (CBIS-DDSM). In addition, the authors are based on a subjective approval of the extracted Region Of Interest (ROI) by experts in radiology. The proposed scheme proves its effectiveness especially on some challenging breast cancer cases, corresponding to higher breast tissue density. In medical image processing, these results open new perspectives with respect to the use of the deep learning solutions, where the small sample-number is known to be a limitation (because pathologies cannot be intentionally provoked).},
  archive      = {J_IETIP},
  author       = {Norhène Gargouri and Raouia Mokni and Alima Damak and Dorra Sellami and Riadh Abid},
  doi          = {10.1049/ipr2.12572},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3394-3406},
  shortjournal = {IET Image Process.},
  title        = {An automatic breast computer-aided diagnosis scheme based on a weighted fusion of relevant features and a deep CNN classifier},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention-based multi-channel feature fusion enhancement
network to process low-light images. <em>IETIP</em>, <em>16</em>(12),
3374–3393. (<a href="https://doi.org/10.1049/ipr2.12571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In realistic low-light environments, images captured by imaging devices often have problems such as low brightness and low contrast, serious loss of detail information, and a large amount of noise, posing major challenges to computer vision tasks. Low-light image enhancement can effectively improve the overall quality of the image, which has important significance and application value. In this study, an attention-based multi-channel feature fusion enhancement network (M-FFENet) is proposed to process low-light images. In this network, a feature extraction model is first used to obtain the deep features of the downsampled low-light images and fit them to an affine bilateral grid. Second, the addition of attention-based residual dense blocks (ARDB) allows the network to focus on more details and spatial information. Meanwhile, all color channels are considered. The channel features and bilateral meshes are then linearly interpolated using the feature reconfiguration model (FRM) to obtain high-quality features containing rich color and texture information. Next, the feature fusion module (FFM) is used to fuse features that contain different information. Enhancement model is used to further recover texture and detail in the image. Finally, the enhanced image is output. Numerous experimental results have shown that the method achieves better results in both quantitative and qualitative aspects compared to other methods.},
  archive      = {J_IETIP},
  author       = {Xintao Xu and Jinjiang Li and Zhen Hua and Linwei Fan},
  doi          = {10.1049/ipr2.12571},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3374-3393},
  shortjournal = {IET Image Process.},
  title        = {Attention-based multi-channel feature fusion enhancement network to process low-light images},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast algorithm for box-constrained fractional-order total
variation image restoration with impulse noise. <em>IETIP</em>,
<em>16</em>(12), 3359–3373. (<a
href="https://doi.org/10.1049/ipr2.12570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel variational model with box constraints is proposed to restore images corrupted by impulse noise. The proposed model is composed of fractional-order total variation regularization and L p -fidelity term ( 0 &lt; p &lt; 1 ) $(0&amp;lt;p&amp;lt;1)$ . Moreover, the new model possesses the advantages of preserving sharp edges and removing blocking effect. To solve the proposed model, some auxiliary variables are first introduced to transform it into some easy-to-solve subproblems. Further, the alternating direction method of multipliers, iteratively re-weighted ℓ 1 algorithm and fast iteration technique are adopted to solve the related subproblems. Numerical results show that the proposed model performs better in comparison with the several existing methods, in terms of both quantitative evaluation and visual quality.},
  archive      = {J_IETIP},
  author       = {Jianguang Zhu and Juan Wei and Binbin Hao},
  doi          = {10.1049/ipr2.12570},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3359-3373},
  shortjournal = {IET Image Process.},
  title        = {Fast algorithm for box-constrained fractional-order total variation image restoration with impulse noise},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A selective image encryption scheme using LICC hyperchaotic
system. <em>IETIP</em>, <em>16</em>(12), 3342–3358. (<a
href="https://doi.org/10.1049/ipr2.12569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an improved multi-dimensional hyperchaotic system derived from the logistic map and the ICMIC based on closed-loop coupling (LICC hyperchaotic system) is proposed and it is used in the design of selective image encryption. The performance analysis shows that the LICC hyperchaotic system with a strong coupling degree has parameters whose value can be set more flexibly and can generate a wider and more uniform distribution of chaotic sequences. Then selective image encryption is proposed. Firstly, an adaptive pseudo-random sequence generator based on hash function is designed to ensure the difference of sequences used in different encryption steps. Second, a novel bit-level scrambling is designed to increase the encryption speed. Finally, an efficient rule to select image blocks with weak security is proposed and the second step of the encryption is performed at the selected blocks of the image to ensure security. The encryption performance and security analysis show that the proposed encryption algorithm based on an improved hyperchaotic system is secure enough to resist several types of attacks.},
  archive      = {J_IETIP},
  author       = {Chengjing Wei and Guodong Li},
  doi          = {10.1049/ipr2.12569},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3342-3358},
  shortjournal = {IET Image Process.},
  title        = {A selective image encryption scheme using LICC hyperchaotic system},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel detail weighted histogram equalization method for
brightness preserving image enhancement based on partial statistic and
global mapping model. <em>IETIP</em>, <em>16</em>(12), 3325–3341. (<a
href="https://doi.org/10.1049/ipr2.12567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Histogram equalization (HE) is a classic and widely used image contrast enhancement algorithm for its good performance and high efficiency. However, over-enhancement caused by high peak in the histogram affects the subjective quality of the image processed by HE to a large extent. In this paper, a detail weighted histogram equalization (DWHE) method is proposed based on a novel histogram modification (HM) model named partial statistic and global mapping (PSGM) to alleviate high peak and suppress over-enhancement. Moreover, the authors implement a refined version of gamma correction (GC) named texture enhancement function (TEF) on high-frequency images to reduce the noise amplification effect. At last, the authors propose a novel adaptively weighted pixel-level image fusion method to further reduce the phenomenon of over-enhancement and improve brightness distribution. Both subjective and quantitative evaluations are conducted on images containing a variety of scenes. Compared with several state-of-the-art image enhancement methods, the proposed framework obtained generally the best performance in aspects of both subjective appearance and objective evaluation indices. Therefore, it is proved that the proposed methods can effectively alleviate the over-enhancement, enrich image details, and efficiently improve the visual quality while preserving the brightness of the image.},
  archive      = {J_IETIP},
  author       = {Yu Li and Zifeng Yuan and Kun Zheng and Luheng Jia and Huaqiu Guo and Hongyuan Pan and Jingjing Guo and Lidong Huang},
  doi          = {10.1049/ipr2.12567},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3325-3341},
  shortjournal = {IET Image Process.},
  title        = {A novel detail weighted histogram equalization method for brightness preserving image enhancement based on partial statistic and global mapping model},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spaceborne pose determination based on image to 3D digital
surface model matching. <em>IETIP</em>, <em>16</em>(12), 3314–3324. (<a
href="https://doi.org/10.1049/ipr2.12566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing an accurate onboard-camera pose estimation is one major challenge of satellite systems, and the attempt of improving remote sensing camera pose accuracy never ceases. The camera pose can be recovered by aligning a captured 2D image and a 3D digital surface model of the corresponding scene. In this paper, a novel camera pose estimation method from captured images with the over known real scene 3D products is proposed to enhance remote sensing camera attitude accuracy. The purpose of this estimation is to determine the pose of a camera purely from an image based on a known 3D model, where 3D products of very high spatial resolution are projected onto image space by virtual camera system with error-contained initial exterior orientation parameters, and whether the pose of the camera can be determined precisely depends on the 2D–3D registration result. The process consists of two steps: (1) feature extraction and (2) similarity measure and registration. Furthermore, the proposed method revises the rotation matrix and translation vector by utilizing formulation based on quaternion representation of rotation, respectively. We evaluate our method on challenging simulation data and results show that acceptable accuracy of camera pose can be achieved.},
  archive      = {J_IETIP},
  author       = {Xing Chang and Guiqin Yang and Jiayu Chen and Xiaopeng Wang and Zhanjun Jiang},
  doi          = {10.1049/ipr2.12566},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3314-3324},
  shortjournal = {IET Image Process.},
  title        = {Spaceborne pose determination based on image to 3D digital surface model matching},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MobileTrack: Siamese efficient mobile network for high-speed
UAV tracking. <em>IETIP</em>, <em>16</em>(12), 3300–3313. (<a
href="https://doi.org/10.1049/ipr2.12565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Siamese-based trackers have drawn amounts of attention in visual tracking field because of their excellent performance. However, visual object tracking on Unmanned Aerial Vehicles platform encounters difficulties under circumstances such as small objects and similar objects interference. Most existing tracking methods for aerial tracking adopt deeper networks or inefficient policies to promote performance, but most trackers can hardly meet real-time requirements on mobile platforms with limited computing resources. Thus, in this work, an efficient and lightweight siamese tracker (MobileTrack) is proposed for high-time Unmanned Aerial Vehicles tracking, realising the balance between performance and speed. Firstly, a lightweight convolutional network (D-MobileNet) is designed to enhance the characterisation ability of small objects. Secondly, an efficient object-aware module is proposed for local cross-channel information exchange, enhancing the feature information of the tracking object. Besides, an anchor-free region proposal network is introduced to predict the object pixel by pixel. Finally, deep and shallow feature information is fully utilised by cascading multiple anchor-free region proposal networks for accurate locating and robust tracking. Extensive experiments on the three Unmanned Aerial Vehicles benchmarks show that the proposed tracker achieves outstanding performance while keeping a beyond-real-time speed.},
  archive      = {J_IETIP},
  author       = {Yuanliang Xue and Guodong Jin and Tao Shen and Lining Tan and Jing Yang and Xiaohan Hou},
  doi          = {10.1049/ipr2.12565},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3300-3313},
  shortjournal = {IET Image Process.},
  title        = {MobileTrack: Siamese efficient mobile network for high-speed UAV tracking},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved point feature-based sparse stereo vision.
<em>IETIP</em>, <em>16</em>(12), 3284–3299. (<a
href="https://doi.org/10.1049/ipr2.12564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the limitation on the onboard equipment, the sparse stereo vision is becoming a suitable choice for the deployment of micro air vehicles (MAV) and small robots. However, for the point feature-based sparse stereo, most of the current stereo algorithms ignore the similarity between feature points, so it is hard to achieve high accuracy. In addition, the problem of clustered feature distribution will still affect the performance of point feature-based algorithms in the application. To make up for these deficiencies, the authors propose an improved features from accelerated segment test (FAST) feature detector to suppress the point detection in complex texture regions. Most importantly, the authors present a novel census transform (CT)-based algorithm that contains two encoders ‘texture orientation’ and ‘texture gradient’ to get a more efficient census bit string for the feature point. Instead of randomly selecting pixels to calculate the bit string, we combine the texture characteristics of the census windows where feature points are located. Compared with the original CT, the processing speed of our method is improved, and the average error of our method is reduced by 18.05%. The evaluation results show the presented improved point feature-based sparse stereo algorithm has a great value in engineering applications.},
  archive      = {J_IETIP},
  author       = {Changhao Chen and Bifeng Song and Shuhui Bu and Lei He},
  doi          = {10.1049/ipr2.12564},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3284-3299},
  shortjournal = {IET Image Process.},
  title        = {An improved point feature-based sparse stereo vision},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image restoration via exponential scale mixture-based
simultaneous sparse prior. <em>IETIP</em>, <em>16</em>(12), 3268–3283.
(<a href="https://doi.org/10.1049/ipr2.12563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image prior plays a decisive role in the performance of widely studied model-based restoration methods. To further improve restoration performance, this paper proposes an exponential scale mixture-based simultaneous sparse prior (ESM-SSP) to accurately characterize image prior information. Specifically, first, two structured dictionaries are adaptively learned to explore the local and non-local sparsity of similar patch groups simultaneously. Then, the exponential scale mixture (ESM) is employed to model simultaneous sparse coefficients. The adoption of ESM enables us to accurately estimate simultaneous sparse coefficients by adaptively adjusting the regularization parameters. With the aid of ESM-SSP, an effective image restoration algorithm is developed to preserve more image details. Extensive experimental results on image denoising and deblocking demonstrate that compared with many state-of-the-art model-based methods, the proposed ESM-SSP-based restoration algorithm not only has competitive peak signal-to-noise ratio, but also produces higher structural similarity index and better visuals. More importantly, the proposed method can also compete favourably with the superior deep learning-based restoration methods.},
  archive      = {J_IETIP},
  author       = {Wei Yuan and Han Liu and Lili Liang},
  doi          = {10.1049/ipr2.12563},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3268-3283},
  shortjournal = {IET Image Process.},
  title        = {Image restoration via exponential scale mixture-based simultaneous sparse prior},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly supervised building semantic segmentation via
superpixel-CRF with initial deep seeds guiding. <em>IETIP</em>,
<em>16</em>(12), 3258–3267. (<a
href="https://doi.org/10.1049/ipr2.12558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The segmentation of building from satellite and airborne images is necessary for high-resolution buildings maps generation and it is still challenging. On annotated pixel-level images, trained deep convolutional neural networks (CNNs) were used to improve segmentation of building. The cost of labelling training data is high, which reduces their usage. Human labelling efforts can be significantly reduced using weakly supervised segmentation techniques. Here, a novel weakly supervised framework is introduced for building semantic segmenting that relies on deep seeds to construct a superpixels-CRF model over superpixels segmentation in order to generate high-quality initial pixel-level annotations, as the initialization step. Then, the segmentation network is trained using the initial pixel-level annotations. Next, the CRF model is used to refine the segmentation masks, and the segmentation network is retrained to achieve accurate pixel-level annotations while iteratively optimizing the segmentation. The experimental results on three public building datasets demonstrate that the proposed framework significantly improved the quality of building semantic segmentation while remaining computationally efficient.},
  archive      = {J_IETIP},
  author       = {Khaled Moghalles and Heng-Chao Li and Zaid Al-Huda and Ali Raza and Asad Malik},
  doi          = {10.1049/ipr2.12558},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3258-3267},
  shortjournal = {IET Image Process.},
  title        = {Weakly supervised building semantic segmentation via superpixel-CRF with initial deep seeds guiding},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pyramidal dense attention networks for single image
super-resolution. <em>IETIP</em>, <em>16</em>(12), 3247–3257. (<a
href="https://doi.org/10.1049/ipr2.12557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, residual and dense networks have effectively promoted the development of image super-resolution (SR). However, most dense networks based SR methods do not make full use of dense feature information. To solve this problem, a pyramidal dense attention network for single image super-resolution is proposed in this paper. In this method, the proposed pyramidal dense learning can gradually increase the width of the densely connected layer inside a pyramidal dense block to extract deep features efficiently. Meanwhile, the adaptive group convolution that the number of groups grows linearly with dense convolutional layers is introduced to relieve the parameter explosion. Besides, a novel joint attention to capture cross-dimension interaction between the spatial dimensions and channel dimension in an efficient way for providing rich discriminative feature representations is also proposed. Extensive experimental results show that the method achieves comparable performance in comparison with the state-of-the-art SR methods.},
  archive      = {J_IETIP},
  author       = {Huapeng Wu and Jie Gui and Jun Zhang and James T. Kwok and Zhihui Wei},
  doi          = {10.1049/ipr2.12557},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3247-3257},
  shortjournal = {IET Image Process.},
  title        = {Pyramidal dense attention networks for single image super-resolution},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FishNet: Fish visual recognition with one stage multi-task
learning. <em>IETIP</em>, <em>16</em>(12), 3237–3246. (<a
href="https://doi.org/10.1049/ipr2.12556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of computer vision for fish monitoring in aquaculture fisheries has gained importance. It is crucial to obtain the object box, instance mask and landmarks of the fish to determine their status. There are well-established methods to achieve these tasks, but running them in a serial sequence is inefficient and complex. A multi-tasking framework is proposed that can implement the above three tasks in parallel, FishNet. Unlike other multi-tasking frameworks that use one encoder with multiple decoders, the authors use only one encoder and one decoder to achieve multi-tasking fusion and can be trained end-to-end. A multi-task dataset for fish is produced to validate the framework. It achieved the best speed-accuracy balance on object detection (a 95.3% box AP), instance segmentation (a 53.9% mask AP) and pose recognition (95.1% OKS AP), and reached real-time inference speed (66.3 FPS) on the NVIDIA Tesla V100.},
  archive      = {J_IETIP},
  author       = {Ziwen Chen and Lijie Cao and Qihua Wang and Yu Cai},
  doi          = {10.1049/ipr2.12556},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3237-3246},
  shortjournal = {IET Image Process.},
  title        = {FishNet: Fish visual recognition with one stage multi-task learning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel hashing scheme via image feature map and 2D PCA.
<em>IETIP</em>, <em>16</em>(12), 3225–3236. (<a
href="https://doi.org/10.1049/ipr2.12555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing scheme is a high-efficiency technique for processing massive images. Two critical metrics of the hashing scheme are discrimination and robustness, but most schemes do not get satisfied classification performance between them. This paper proposes a novel hashing scheme via image feature map and 2D PCA. First, the proposed scheme extracts local phase quantization (LPQ) features in the frequency domain and local ternary pattern (LTP) features in the spatial domain, and combines them to construct an image feature map. Second, the proposed scheme conducts dimension reduction via 2D PCA for learning features from the image feature map. Last, the learned features are compressed to generate the hash sequence. Performances are tested on open image datasets. The results demonstrate that the proposed scheme can make a good balance between discrimination and robustness. In addition, the classification and copy detection of the proposed scheme are both superior to those of some famous hashing schemes.},
  archive      = {J_IETIP},
  author       = {Xiaoping Liang and Zhenjun Tang and Sheng Li and Chunqiang Yu and Xianquan Zhang},
  doi          = {10.1049/ipr2.12555},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3225-3236},
  shortjournal = {IET Image Process.},
  title        = {A novel hashing scheme via image feature map and 2D PCA},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A regularised deep matrix factorised model of matrix
completion for image restoration. <em>IETIP</em>, <em>16</em>(12),
3212–3224. (<a href="https://doi.org/10.1049/ipr2.12553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been an important approach of using matrix completion to perform image restoration. Most previous works on matrix completion focus on the low-rank property by imposing explicit constraints on the recovered matrix, such as the constraint of the nuclear norm or limiting the dimension of the matrix factorisation component. Recently, theoretical works suggest that deep linear neural network has an implicit bias towards low rank on matrix completion. In this work, a regularised deep matrix factorised (RDMF) model for image restoration is proposed, which utilises the implicit bias of the low rank of deep neural networks and the explicit bias of total variation. RDMF is a powerful and flexible framework for inverse problems in image processing while the combination of implicit and explicit regularisation represents the intrinsic characteristics of a natural image. The effectiveness of the RDMF model with extensive experiments are demonstrated, in which the method surpasses the recently proposed models in common examples, especially for the restoration from very few observations. This work sheds light on a more general framework for solving other inverse problems by combining the implicit bias of deep learning with explicit regularisation.},
  archive      = {J_IETIP},
  author       = {Zhemin Li and Zhi-Qin John Xu and Tao Luo and Hongxia Wang},
  doi          = {10.1049/ipr2.12553},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3212-3224},
  shortjournal = {IET Image Process.},
  title        = {A regularised deep matrix factorised model of matrix completion for image restoration},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust age estimation model using group-aware contrastive
learning. <em>IETIP</em>, <em>16</em>(12), 3201–3211. (<a
href="https://doi.org/10.1049/ipr2.12552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although great efforts have been devoted to developing lightweight models for age estimation in recent works, the robustness is still unsatisfactory in unconstrained environments. This paper proposes a Group-aware Contrastive Network (GACN), a robust lightweight model, which extracts discriminative features by leveraging contrastive learning rather than increasing model parameters. Specifically, with a carefully designed contrastive loss function, GACN minimizes intra-class distances and maximizes inter-class distances between different age groups in feature space. Thus, faces belonging to the same age group are pulled together, while clusters of faces from different age groups are pushed apart. Unlike existing contrastive learning methods, which are separated from the downstream tasks, GACN integrates contrastive learning into age regression and jointly optimizes them for age representation learning. This allows to achieve robust age estimation using a lightweight network that is 1/662 of the model size of VGGNet. Extensive experiments on IMDB-WIKI, Morph II, and FG-NET demonstrate that the proposed method has a significant improvement over the baseline model and performs comparably to existing compact and bulky methods.},
  archive      = {J_IETIP},
  author       = {Xiaoqiang Li and Chengyu Guo and Yifan Wu and Congcong Zhu and Jide Li},
  doi          = {10.1049/ipr2.12552},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3201-3211},
  shortjournal = {IET Image Process.},
  title        = {Robust age estimation model using group-aware contrastive learning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight single image deraining algorithm incorporating
visual saliency. <em>IETIP</em>, <em>16</em>(12), 3190–3200. (<a
href="https://doi.org/10.1049/ipr2.12550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are still some challenges in the task of single image rain removal, such as artefact remnant, background over-smooth, and increasingly complex and heavy-weight network architecture. Especially too heavy-weight network to fit outdoor detection devices or mobile devices. To address the above challenges, we propose a lightweight single image Deraining algorithm incorporating visual attention saliency mechanisms (LDVS). The proposed network consists of five blocks and two convolution operations, where each block consists of a dilation convolution module and a convolutional block attention module (CBAM). Specifically, visual saliency module CBAM is used for accurate localization of rain streak, and further the combinations of dilated convolution with CBAM is used to extract feature maps of rain streaks faithfully, which is able to remove artefact remnant while maintaining background details. A good tradeoff is presented between the network&#39;s weight size and effect of rain removal. Specifically, with only 48,268 parameters, the proposed model can achieve a guaranteed performance. Extensive experiments on a few typical rainy scenarios on synthetic and real-world datasets have demonstrated that to achieve the same level of performance, the proposed method has far smaller size than most of the baselines under both qualitative and quantitative analyses.},
  archive      = {J_IETIP},
  author       = {Mingdi Hu and Jingbing Yang and Nam Ling and Yuhong Liu and Jiulun Fan},
  doi          = {10.1049/ipr2.12550},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3190-3200},
  shortjournal = {IET Image Process.},
  title        = {Lightweight single image deraining algorithm incorporating visual saliency},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CEFusion: Multi-modal medical image fusion via cross
encoder. <em>IETIP</em>, <em>16</em>(12), 3177–3189. (<a
href="https://doi.org/10.1049/ipr2.12549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing deep learning-based multi-modal medical image fusion (MMIF) methods utilize single-branch feature extraction strategies to achieve good fusion performance. However, for MMIF tasks, it is thought that this structure cuts off the internal connections between source images, resulting in information redundancy and degradation of fusion performance. To this end, this paper proposes a novel unsupervised network, termed CEFusion. Different from existing architecture, a cross-encoder is designed by exploiting the complementary properties between the original image to refine source features through feature interaction and reuse. Furthermore, to force the network to learn complementary information between source images and generate the fused image with high contrast and rich textures, a hybrid loss is proposed consisting of weighted fidelity and gradient losses. Specifically, the weighted fidelity loss can not only force the fusion results to approximate the source images but also effectively preserve the luminance information of the source image through weight estimation, while the gradient loss preserves the texture information of the source image. Experimental results demonstrate the superiority of the method over the state-of-the-art in terms of subjective visual effect and quantitative metrics in various datasets.},
  archive      = {J_IETIP},
  author       = {Ya Zhu and Xue Wang and Luping Chen and Rencan Nie},
  doi          = {10.1049/ipr2.12549},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3177-3189},
  shortjournal = {IET Image Process.},
  title        = {CEFusion: Multi-modal medical image fusion via cross encoder},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised breast histopathological image
classification with self-training based on non-linear distance metric.
<em>IETIP</em>, <em>16</em>(12), 3164–3176. (<a
href="https://doi.org/10.1049/ipr2.12548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Histopathological analysis requires a lot of clinical experience and time for pathologists. Artificial intelligence (AI) may have an important role in assisting pathologists and leading to more efficient and effective histopathological diagnoses. To address the challenge of requiring a large number of labelled images to train deep learning models in breast cancer histopathological image classification, a self-training semi-supervised learning method consisting three components is proposed: Firstly, a pre-trained ResNet-18 was used to extract features and generate pseudo-labels for unlabelled data; secondly, a relational weight network based on the squeeze-and-excitation network (SENet) was trained to calculate the non-linear distance metrices between labelled and unlabelled samples, in order to improve the accuracy of pseudo-labelling; lastly, a consistency loss—maximum mean difference (MMD)—was added into the model to minimize the divergence between distributions of unlabelled and labelled samples. Extensive experiments were conducted on the open access BreakHis dataset. The proposed method outperformed the state-of-the-art semi-supervised methods at all tested annotated percentages (10–70%), and also achieved comparable performance with supervised methods at higher annotated percentages (50%, 70%).},
  archive      = {J_IETIP},
  author       = {Kun Liu and Zhuolin Liu and Sidong Liu},
  doi          = {10.1049/ipr2.12548},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3164-3176},
  shortjournal = {IET Image Process.},
  title        = {Semi-supervised breast histopathological image classification with self-training based on non-linear distance metric},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Double bit range estimation with eight estimators for CABAC
in VVC. <em>IETIP</em>, <em>16</em>(12), 3155–3163. (<a
href="https://doi.org/10.1049/ipr2.12547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work describes the modification of Context-based Adaptive Binary Arithmetic Coding (CABAC) using the double bit range estimation in the VVC engine and the consideration of range updates by using eight hypothetical probability estimators. The focus is on the selected adaptation rates performed in these proposed estimators, which are chosen based on memory consideration and coding efficiency. An investigation of arithmetic coding engines with multi-hypothesis probability estimates and their consideration of contextual modeling of entropy coding at the level of transform coefficients. The proposed scheme enables a quantitative representation of probabilistic predictions linearly and describes the scalability potential for higher accuracy. In addition, this work discusses the hardware implementation, which is based on simple operations such as bitwise operations and subinterval updates. The experimental results validate the effectiveness of the proposed approach specified in VTM framework. The improved results show that it provides more significant gains in terms of RA and LD, which is better than the AI configuration.},
  archive      = {J_IETIP},
  author       = {Ka-Hou Chan and Sio-Kei Im},
  doi          = {10.1049/ipr2.12547},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3155-3163},
  shortjournal = {IET Image Process.},
  title        = {Double bit range estimation with eight estimators for CABAC in VVC},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SREFBN: Enhanced feature block network for single-image
super-resolution. <em>IETIP</em>, <em>16</em>(12), 3143–3154. (<a
href="https://doi.org/10.1049/ipr2.12546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has assisted the field of single-image super-resolution (SR) in achieving new heights. However, the task of restoring a high-resolution (HR) image from a highly degraded low-resolution (LR) image is sophisticated due to poor image restoration quality. A novel and effective lightweight SR method is presented as super-resolution via an enhanced feature block network (SREFBN) that successfully reconstructs an HR image using a corresponding LR image with a purposed deep residual block. In addition, a novel shared parameters approach in the top-down pathway among low-level feature maps is introduced. The experimental results prove that SREFBN achieves remarkable performance. The presented framework requires lower computational cost and outperforms many state-of-the-art methods. It is also highly adaptable with low-end devices, requiring lower multiplication and adding operations. A trade-off comparison between the number of parameters, execution time, and accuracies is given while also showing different variations of our approach to prove the effectiveness and reliability of the shared parameters. Most importantly, the results indicate that our framework has gained state-of-the-art performance on larger scales 3 and 4. Code is available at https://github.com/curzii23/SREFBN .},
  archive      = {J_IETIP},
  author       = {Vachiraporn Ketsoi and Muhammad Raza and Haopeng Chen and Xubo Yang},
  doi          = {10.1049/ipr2.12546},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3143-3154},
  shortjournal = {IET Image Process.},
  title        = {SREFBN: Enhanced feature block network for single-image super-resolution},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IAUFD: A 100k images dataset for automatic football
image/video analysis. <em>IETIP</em>, <em>16</em>(12), 3133–3142. (<a
href="https://doi.org/10.1049/ipr2.12543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, analyzing football videos using computer vision techniques has attracted increasing attention. Significant events detection, football video summarization, football results predictions, statistics etc. are exciting applications in this area. On the other hand, the deep learning approaches are very successful methods for image and video analysis that need much data. Nevertheless, to the best of our knowledge, publicly available datasets in this area are small or individual, which are not enough for such deep learning-based approaches. A public dataset was collected, annotated, and prepared, namely IAUFD * , to meet this gap for researches in this direction. The IAUFD contains 100,000 real-world images from 33 football videos in 2,508 min, annotated in 10 event categories. These categories include the goal, center of the field, celebration, red card, yellow card, the ball, stadium, the referee, penalty-kick, and free-kick. It is believed that these moments are the basis and useful for any high-level action or event exploration. For a generalization of our dataset, we paid attention to various weather (e.g., sunny, rainy, cloudy etc.), season, time of day, and location. We also used two deep neural networks (VggNet-13 and ResNet-18) to evaluate our proposed dataset as the baseline for future studies and comparison.},
  archive      = {J_IETIP},
  author       = {Amirhosein Zanganeh and Mahdi Jampour and Kamran Layeghi},
  doi          = {10.1049/ipr2.12543},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3133-3142},
  shortjournal = {IET Image Process.},
  title        = {IAUFD: A 100k images dataset for automatic football image/video analysis},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A single watermark based scheme for both protection and
authentication of identities. <em>IETIP</em>, <em>16</em>(12),
3113–3132. (<a href="https://doi.org/10.1049/ipr2.12542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The security of a watermarking scheme is mainly categorised as either robust or fragile. The former can withstand an authorised alteration/attack, primarily used in copyright protection. The latter follows a zero tolerance towards any modification, used primarily in content authentication processes. The existing literature in the field projects that two separate watermarks are required to make a watermarking scheme robust and fragile, thus making the overall process cumbersome and complex. A novel image watermarking scheme that uses only one watermark while achieving both goals of copyright protection and authentication of identities is presented. An unconventional concept of checkpointing is introduced, which equips the proposed scheme to be either robust or fragile, making it superior in its application versatility. First, watermark embedding within the host/original image is achieved by a combination of transform domain techniques along with a novel median-based embedding block selection procedure. Second, checkpointing is performed in the spatial domain. The watermarked image in the absence of an attack is correlated to the one that is being attacked, using the template energy comparison-based approach. In the case of the robust watermark, such checkpointing can establish whether the carried out attack is authorised or not, determining the successful recovery of the watermark or vice-versa. Moreover, in the case of the fragile watermark, a sole confirmation of the occurrence of an attack is sufficient to make the watermark recovery impossible. Finally, the experimental analysis of the proposed scheme illustrates its excellent performance and superiority over state-of-the-art methods within the field.},
  archive      = {J_IETIP},
  author       = {S. Sharma and J.J. Zou and G. Fang},
  doi          = {10.1049/ipr2.12542},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3113-3132},
  shortjournal = {IET Image Process.},
  title        = {A single watermark based scheme for both protection and authentication of identities},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video-based action recognition using spurious-3D residual
attention networks. <em>IETIP</em>, <em>16</em>(11), 3097–3111. (<a
href="https://doi.org/10.1049/ipr2.12541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, 3D Convolutional Neural Networks (3D CNNs) have attracted extensive attention in extracting spatial and temporal features in videos for their efficient feature extraction ability. However, it also brings enormous model parameters by training very deep 3D CNNs. Here, a novel network named spurious-3D Residual Attention Networks (S3D RANs) is proposed for video-based action recognition, which has the powerful capacity to learn collaborative spatiotemporal features. In particular, by leveraging the merits from 2D Convolutional Neural Networks (2D CNNs) and 3D CNNs, 2D CNNs are applied rather than 3D CNNs on frames of the single view of volumetric videos data to learn temporal motion features directly. Furthermore, view and channel-wise attention mechanism submodules are employed in the residual unit to learn the importance of each view for action recognition and guide the network to pay more attention to the more useful information for action recognition. Experimental results on UCF-101, HMDB-51 datasets demonstrate that our S3D RANs have higher accuracy and lower model complexity than existing works.},
  archive      = {J_IETIP},
  author       = {Bo Chen and Hongying Tang and Zebin Zhang and Guanjun Tong and Baoqing Li},
  doi          = {10.1049/ipr2.12541},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3097-3111},
  shortjournal = {IET Image Process.},
  title        = {Video-based action recognition using spurious-3D residual attention networks},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scene classification for remote sensing images with
self-attention augmented CNN. <em>IETIP</em>, <em>16</em>(11),
3085–3096. (<a href="https://doi.org/10.1049/ipr2.12540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing scene classification aims to automatically assign a specific semantic label to each image. It is challenging to classify remote sensing scene images due to the images&#39; diversity and rich spatial information. Recently, convolutional neural networks have been widely used to overcome these difficulties, such as the famous Visual Geometry Group (VGG) network. However, the VGG network with local receptive fields cannot model the global information of remote sensing images well. It also needs a large number of parameters and floating point operations to achieve satisfactory accuracy. To overcome these challenges, we introduce the self-attention mechanism to the VGG network. Specifically, we replace the last four convolutional layers in the VGG-19 network with two cascaded self-attention blocks, each consisting of two multi-head self-attention (MHSA) layers with the residual network structure. The new structure can simultaneously explore the local and global information from remote sensing scenes. Such improvements not only reduce model parameters but also improve the classification performance. The effectiveness of the proposed method is validated through experiments on four public data sets, i.e., NaSC-TG2, WHU-RS19, AID and EuroSAT.},
  archive      = {J_IETIP},
  author       = {Zongyin Liu and Anming Dong and Jiguo Yu and Yubing Han and You Zhou and Kai Zhao},
  doi          = {10.1049/ipr2.12540},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3085-3096},
  shortjournal = {IET Image Process.},
  title        = {Scene classification for remote sensing images with self-attention augmented CNN},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast 3D-HEVC inter coding using data mining and machine
learning. <em>IETIP</em>, <em>16</em>(11), 3067–3084. (<a
href="https://doi.org/10.1049/ipr2.12539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Three-Dimensional High Efficiency Video Coding standard is a video compression standard developed based on the two-dimensional video coding standard HEVC and used to encode multi-view plus depth format video. This paper proposes an algorithm based on eXtreme Gradient Boosting to solve the problem of high inter-frame coding complexity in 3D-HEVC. Firstly, explore the correlation between the division depth of the inter-frame coding unit and the texture features in the map, as well as the correlation between the coding unit division structure between each map and each viewpoint. After that, based on the machine learning method, a fast selection mechanism for dividing the depth range of the inter-frame coding tree unit based on the eXtreme Gradient Boosting algorithm is constructed. Experimental results show that, compared with the reference software HTM-16.0, this method can save an average of 35.06% of the coding time, with negligible degradation in terms of coding performance. In addition, the proposed algorithm has achieved different degrees of improvement in coding performance compared with the related works.},
  archive      = {J_IETIP},
  author       = {Ruyi Zhang and Kebin Jia and Yuan Yu and Pengyu Liu and Zhonghua Sun},
  doi          = {10.1049/ipr2.12539},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3067-3084},
  shortjournal = {IET Image Process.},
  title        = {Fast 3D-HEVC inter coding using data mining and machine learning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RAG-net: ResNet-50 attention gate network for accurate iris
segmentation. <em>IETIP</em>, <em>16</em>(11), 3057–3066. (<a
href="https://doi.org/10.1049/ipr2.12538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iris segmentation is an important step in the process of iris recognition. Iris images collected under non-cooperative conditions always contain various noise, which is a challenge for iris segmentation. Most U-Net-based methods have made great achievements in iris segmentation. However, this architecture lacks of focusing on target structures of varying shapes, and robustness in segmenting objects with significant shape variations. In this paper, we propose RAG-Net: an efficient iris segmentation method based on deep learning. In contrast to many previous convolutional neural network (CNN)-based iris segmentation methods, we adopted the attention gate (AG) mechanism and ResNet-50 in the U-Net architecture to improve iris segmentation accuracy, the AG module was included in the skip connection part of the RAG-Net architecture to further identify salient feature regions and prune feature responses, which preserve only the activations relevant to the required information, and the ResNet-50 module was used to improve the robustness of the segmentation performance. Using this model, efficient iris segmentation in a non-cooperative environment can be realized. The proposed method was trained and evaluated using the CASIA.v4-distance, CASIA.v4-thousand, UBIRIS.v2, and MICHE-I databases. From the view of the segmentation results, the proposed RAG-Net is one of effective architecture in iris segmentation methods.},
  archive      = {J_IETIP},
  author       = {Yinyin Wei and Aijun Zeng and Xiangyang Zhang and Huijie Huang},
  doi          = {10.1049/ipr2.12538},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3057-3066},
  shortjournal = {IET Image Process.},
  title        = {RAG-net: ResNet-50 attention gate network for accurate iris segmentation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MCG&amp;BA-net: Retinal vessel segmentation using multiscale
context gating and breakpoint attention. <em>IETIP</em>,
<em>16</em>(11), 3039–3056. (<a
href="https://doi.org/10.1049/ipr2.12537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate segmentation of blood vessels plays a crucial role in screening, diagnosis and treatment of multiple diseases. However, current automated segmentation approaches do not pay enough attention to the vascular topology errors (such as mistaking vessel-breakpoints), resulting in considerable scattered vessel-fragments in segmentation results. This article proposes a retinal vessel segmentation model using multi-scale context gating and breakpoint attention mechanism, called MCG&amp;BA-Net. Specifically, it obtains a feature map containing contextual information of vessels through an introduced multi-scale context module, and then filters the redundant features and noises by a gated structure to highlight target features. Furthermore, a kind of breakpoint attention module is proposed, which can locate and focus on potential breakpoint areas, thereby facilitating accurate segmentation results of tree-like fine vessels. Extensive confirmatory and comparative experiments have been conducted on five public datasets, including three benchmark datasets, that is, DRIVE, CHASDB1 and SATRE, and two clinical datasets, that is, fundusimage1000 and RFMID. The AUC scores on the benchmark datasets are 0.9878, 0.9923 and 0.9942, respectively. Among them, the AUC score on CHADEDB1 and STARE outperforms the state-of-the-art results. In addition, experimental results on the two clinical datasets demonstrate strong generalization capability of the propose method, indicating high clinical application values.},
  archive      = {J_IETIP},
  author       = {Pengfei Xu and Gangjing Zhao and Jinping Liu and Hadi Jahanshahi and Zhaohui Tang and Subo Gong},
  doi          = {10.1049/ipr2.12537},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3039-3056},
  shortjournal = {IET Image Process.},
  title        = {MCG&amp;BA-net: Retinal vessel segmentation using multiscale context gating and breakpoint attention},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gaussian noise parameter estimation based on multiple
singular value decomposition and non-linear fitting. <em>IETIP</em>,
<em>16</em>(11), 3025–3038. (<a
href="https://doi.org/10.1049/ipr2.12536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noise standard deviation (STD) is an important parameter in many digital image processing applications. This paper presents a Gaussian noise parameter estimation algorithm using multiple singular value decomposition (SVD) and non-linear fitting. The proposed algorithm adds known noise to the original noise image many times to generate a noise-corrupted image set and then performs SVD on each image. By analyzing the singular values of the noise-corrupted images, an overdetermined equation system with respect to the noise STD is established. The Gauss–Newton iteration method and backtracking Armijo line search are used to solve the equations, which improve the convergence speed and reduce computational cost. Compared with other methods, the mean error of the proposed algorithm on the TID2008 dataset is 0.028, which is several times lower than other methods. This shows that the performance of our estimator is significantly improved.},
  archive      = {J_IETIP},
  author       = {Jinli Qi and Lei Sun and Kengpeng Li and Lingang Wang},
  doi          = {10.1049/ipr2.12536},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3025-3038},
  shortjournal = {IET Image Process.},
  title        = {Gaussian noise parameter estimation based on multiple singular value decomposition and non-linear fitting},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial and long–short temporal attention correlation
filters for visual tracking. <em>IETIP</em>, <em>16</em>(11), 3011–3024.
(<a href="https://doi.org/10.1049/ipr2.12535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discriminative correlation filter is one of the quick and effective ways for studying visual tracking. However, discriminative correlation filter-based methods still suffer from many challenging questions caused by environmental interferences, such as spatial boundary effect, temporal filter degradation, and tracking drift. A novel appearance optimisation model, named spatial and long–short temporal attention model, has been proposed based on a new spatial regularisation term and a long–short temporal regularisation term for learning the correlation filter to localise the target. On the one hand, our proposed method can improve the classical spatial regularisation term with a new weight matrix to alleviate the spatial boundary effect. On the other hand, two new temporal regularisation terms are designed: a short temporal regularisation term and a long temporal regularisation term. The short temporal regularisation term can enlarge the inner connections of the current frame and all foregoing frames to improve the tracking performances, and the long temporal regularisation term can address the influence of occlusion by using the similarity between the initial filter and the current one. Extensive experiments on various benchmarks illustrate that our proposed tracker performs favourably against several related popular trackers.},
  archive      = {J_IETIP},
  author       = {Jianwei Zhao and Fuyuan Wei and NingNing Chen and Zhenghua Zhou},
  doi          = {10.1049/ipr2.12535},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3011-3024},
  shortjournal = {IET Image Process.},
  title        = {Spatial and long–short temporal attention correlation filters for visual tracking},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust 3D brain segmentation in magnetic resonance image
with weighted feature fusion. <em>IETIP</em>, <em>16</em>(11),
3000–3010. (<a href="https://doi.org/10.1049/ipr2.12534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate tissue segmentation in brain magnetic resonance image is of paramount importance for brain reconstruction and analysis. Supervoxels analysis enables relatively robust segmentation of brain tissues, by assigning the supervoxels to tissue clusters according to the inherent information of multiple features captured from the supervoxels. In this popular framework, difficulties are encountered in clustering imposed by the insufficient selection, deficient utilisation of features, and rough supervoxel boundaries due to the influence of noise. To address these aforementioned challenges, the authors propose a weighted feature fusion clustering segmentation method for dividing supervoxels into three tissues on the basis of a robust feature fused similarity matrix. To comprehensively capture the similarities of features among the supervoxels, we construct three complementary similarity matrices on the basis of multiple features from three aspects, that is, appearance, shape, and space location. Next, the authors propose a weighted similarity network fusion method to discriminatively fuse the three similarity matrices into one single matrix. This novel fusion process cannot only extract the common and complementary information of features, but also automatically adjust the weight of similarities according to its reliability. Finally, we achieve initial segmentation results by performing spectral clustering on the fused similarity matrix. To optimise the segmented boundaries affected by noise in the initial segmentation results, the authors construct a voxel-wise diffusion energy function by considering the intensity information of neighbouring voxels. Experiments on two public brain magnetic resonance image datasets demonstrate the robustness of the proposed approach.},
  archive      = {J_IETIP},
  author       = {Jing Xia and Jintao Sha and Qian Zhang},
  doi          = {10.1049/ipr2.12534},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3000-3010},
  shortjournal = {IET Image Process.},
  title        = {Robust 3D brain segmentation in magnetic resonance image with weighted feature fusion},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A variational level set model with kernel metric induced
local image fitting energy. <em>IETIP</em>, <em>16</em>(11), 2983–2999.
(<a href="https://doi.org/10.1049/ipr2.12533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active contour based methods are effective models for image segmentation. However, they always suffer from the limited performance due to the presence of noise and intensity inhomogeneity. To solve this problem, a kernel metric induced local image fitting (KLIF) variational model is proposed in this paper. Firstly, a kernel metric induced local fitting image (KLFI) is introduced by minimising a kernel metric based energy. The combination of the kernel metric and the local fitting image enables the model to be more robust to the noise and intensity inhomogeneity. And then, using the KLFI, a variational level set model that is a squared l 2 distance between the KLFI and the original image is constructed. Two regularisation terms are employed in the model to keep the level set function to be stable during the evolution. At last, an alternating iterative algorithm combining with fixed-point iteration and gradient descent of three-step time-splitting is introduced to solve the proposed model. The experimental results show the effectiveness of the proposed model for image segmentation in the presence of noise and intensity inhomogeneity, and demonstrate the competitive performance over several state-of-the-art variational models in term of accuracy and robustness.},
  archive      = {J_IETIP},
  author       = {Junxiao Yan and Liming Tang and Yanjun Ren and Honglu Zhang},
  doi          = {10.1049/ipr2.12533},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2983-2999},
  shortjournal = {IET Image Process.},
  title        = {A variational level set model with kernel metric induced local image fitting energy},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic thresholding for video anomaly detection.
<em>IETIP</em>, <em>16</em>(11), 2973–2982. (<a
href="https://doi.org/10.1049/ipr2.12532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is one of the most important applications in video surveillance that involves the temporal localisation of anomaly events in unannotated video sequences. By learning the normal patterns to generate frames and calculating their reconstruction error relative to the ground truth, a frame can be recognised as being abnormal if the reconstruction error exceeds a threshold. Most existing works use a fixed threshold that computes over all the testing data to determine the anomalies. However, fixed threshold strategy cannot address the challenges brought by the dynamic environment, e.g. changes in illumination conditions. In this paper, a dynamic thresholding algorithm (DTA) is proposed, which is fully data-driven and capable of automatically determining thresholds such that the developed anomaly detection system can flexibly adapt to different scenarios. The proposed DTA is independent of the backbone network and can be easily incorporated into most existing video anomaly detection models to help identify the appropriate thresholds. On both synthetic and real-world datasets, the experimental results show that with the proposed DTA, the video anomaly detection methods achieve a better performance considering the changes in dynamic environment.},
  archive      = {J_IETIP},
  author       = {Diyang Jia and Xiao Zhang and Joey Tianyi Zhou and Pan Lai and Yifei Wei},
  doi          = {10.1049/ipr2.12532},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2973-2982},
  shortjournal = {IET Image Process.},
  title        = {Dynamic thresholding for video anomaly detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-layer random walker image segmentation for overlapped
cervical cells using probabilistic deep learning methods.
<em>IETIP</em>, <em>16</em>(11), 2959–2972. (<a
href="https://doi.org/10.1049/ipr2.12531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A method for overlapping cell image segmentation is presented with a focus on multi-layer image processing in a three-phase scheme. In the first phase, a convolutional neural network is developed to provide a coarse cell segmentation with multiple output layers to identify cell cytoplasm, locations of cell nuclei, and the background, all as probabilistic image maps for the layer outputs. In the second phase, the probabilistic image maps from the convolutional neural network are used to identify locations of cell nuclei and cell cytoplasm. Then, multi-layer random walker image segmentation is used with cell nuclei as hard initial seeds and the cytoplasm estimates as soft seeds in a diffusion graph-based segmentation of the cells. With rough cell segmentation from both the trained convolutional neural network and the multi-layer random walker graph-based technique, a third phase combines and refines the cell segmentation using the Hungarian algorithm to optimise the assignment of individual pixel locations for the final cell segmentation. We evaluate the proposed method on cervical cell images generated from the International Symposium on Biomedical Imaging 2014 dataset with results that give a Dice similarity coefficient of 97.2% (compared to 93.2% for competitors) when trained on the generated dataset.},
  archive      = {J_IETIP},
  author       = {Tayebeh Lotfi Mahyari and Richard M. Dansereau},
  doi          = {10.1049/ipr2.12531},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2959-2972},
  shortjournal = {IET Image Process.},
  title        = {Multi-layer random walker image segmentation for overlapped cervical cells using probabilistic deep learning methods},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active contour model of breast cancer DCE-MRI segmentation
with an extreme learning machine and a fuzzy c-means cluster.
<em>IETIP</em>, <em>16</em>(11), 2947–2958. (<a
href="https://doi.org/10.1049/ipr2.12530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the low contrast, blurred boundary and intensity inhomogeneity of the images, accurate segmentation of breast cancer lesions with dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) still has great challenges. This paper proposed an improved active contour model (ACM) for segmenting breast cancer lesions in DCE-MRI images. First, based on the extreme learning machine (ELM) method, a robust function is proposed that combines image intensities and time-domain features to enhance the difference between the lesions and other tissues. Second, an edge-stop function (ESF) is introduced by combining the image intensity, time-domain feature, and Hessian shape index to detect the irregular and blurred boundaries. At the boundary of breast cancer lesions, the energy function of ACM is minimized and the evolution of the contour curve completes, so the accurate lesion region of breast cancer can be segmented. The mean Dice similar coefficient (DICE), Jaccard similarity (JC) and Hausdorff distance (HD) of the segmentation of the proposed model in 50 samples are 85.88±6.62%, 75.72±9.68% and 11.62±4.72 mm, respectively. The results segmented by the proposed ACM are more similar to the manual segmentation than the compared models.},
  archive      = {J_IETIP},
  author       = {Bao Feng and Haoyang Zhou and Jin Feng and Yehang Chen and Yu Liu and Tianyou Yu and Zhuangsheng Liu and Wansheng Long},
  doi          = {10.1049/ipr2.12530},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2947-2958},
  shortjournal = {IET Image Process.},
  title        = {Active contour model of breast cancer DCE-MRI segmentation with an extreme learning machine and a fuzzy C-means cluster},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid method of detecting flame from video stream.
<em>IETIP</em>, <em>16</em>(11), 2937–2946. (<a
href="https://doi.org/10.1049/ipr2.12529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a method of detecting flame from video stream is proposed exploiting the characteristics of the disordered movement, rapid deformation and intense colour of the flame. Firstly, the frame difference between video frame and background frame is calculated to obtain the main part of the moving object, and the difference between frames is calculated frame by frame in time series to obtain the deformation part of the moving object, and then the sum of cumulative difference between frames and the background difference between frames are added to generate a binary image containing the moving object and the deformed part. Secondly, the binary image is morphologically opened, and rectangular segmentation is carried out to obtain multiple suspicious flame regions. Finally, in the light of the intense colour of the flame, the corresponding area is extracted from the original picture by using the segmentation rectangle, and the colour statistics of the area are carried out to further judge whether there is a burning flame in the area. The experimental results show that the algorithm can accurately detect the burning area of flame in the real scene and eliminate the light interference and the movement interference.},
  archive      = {J_IETIP},
  author       = {Zengfa Dou and Xiaoke Ma and Xianghua Xie and Hui Liu and Chubing Guo},
  doi          = {10.1049/ipr2.12529},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2937-2946},
  shortjournal = {IET Image Process.},
  title        = {A hybrid method of detecting flame from video stream},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HollowBox: An anchor-free UAV detection method.
<em>IETIP</em>, <em>16</em>(11), 2922–2936. (<a
href="https://doi.org/10.1049/ipr2.12523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAV) “black flight” incidents cause serious security risks and economic losses to airports. Additionally, existing UAV detection methods are mainly radar technology at airports. Whereas, it is unable to correctly identify the number of UAVs and visualise their size, which undoubtedly poses a serious security risk. Accordingly, an anchor-free UAV detection method HollowBox is proposed to supplement radar detection equipment for the airport “black flight” problem. It is inspired by the FoveaBox object detection method, the object detection feature layers are reset and the allocation ratio of positive and negative samples in the training phase are redefined, the HollowBox UAV detection is proposed according to the multi-size characteristics. Extensive experiments show that, the approach achieves 90.1% AP, 6% false detection rate and 17.2 FPS inference speed, which accomplished a satisfactory performance, as verified by the real-shot data collected of an airport in Tianjin. This work is of great significance for the application in airport UAV detection.},
  archive      = {J_IETIP},
  author       = {Shanliang Liu and Jingyi Qu and Renbiao Wu},
  doi          = {10.1049/ipr2.12523},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2922-2936},
  shortjournal = {IET Image Process.},
  title        = {HollowBox: An anchor-free UAV detection method},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A role-entity based human activity recognition using
inter-body features and temporal sequence memory. <em>IETIP</em>,
<em>16</em>(11), 2911–2921. (<a
href="https://doi.org/10.1049/ipr2.12472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing entities and their corresponding roles are important in human activity recognition. In light of recent advancements, the primary emphasis is recognizing the abstract activities involving person-person interaction. The contribution of this work is proposing an architecture, which utilizes the knowledge of the human body parts coordinates in role detection of each individual. The network preprocesses the coordinates to build intra-body and inter-body features. The extracted features build the relationship between the interacting bodies and learn the temporal relation corresponding to each role using the human memory-inspired hierarchical temporal memory. The model is tested on vague samples of mutual actions in the experimental work. The model is found robust in action and role recognition tasks and performed well per expectations.},
  archive      = {J_IETIP},
  author       = {Rahul Shrivastava and Vivek Tiwari and Swati Jain and Basant Tiwari and Alok Kumar Singh Kushwaha and Vibhav Prakash Singh},
  doi          = {10.1049/ipr2.12472},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2911-2921},
  shortjournal = {IET Image Process.},
  title        = {A role-entity based human activity recognition using inter-body features and temporal sequence memory},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual integration of multi-model with spatial-temporal
occlusion-awareness for visual object tracking. <em>IETIP</em>,
<em>16</em>(11), 2890–2910. (<a
href="https://doi.org/10.1049/ipr2.12462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual object tracking is a complicated problem due to varied challenges such as occlusion, background clutter, target appearance changes, abrupt target motion, and illumination variations. Different tracking algorithms perform well on different challenges because of their unique strengths. In this paper, a dual-integration framework that integrates the strengths of multiple models while avoiding their weaknesses is proposed. In the proposed framework, several evaluation criteria and multiple models updated via different processes are combined. To distinguish the target from the distractors caused by different criteria, the motion dynamic model and the forward-backward analysis is introduced to provide information that is complementary to appearance information. In addition, for discriminating occlusion, a spatial-temporal occlusion-aware approach is further proposed. The detected occlusion results are applied in avoiding contamination of the appearance model. Extensive experiments on multiple benchmarks demonstrate that this proposed method improves the overlap rate of a hand-crafted feature-based tracker with relative gains of 4%, 1.6%, 1.9%, 2.2% and 8.5% on OTB-2015, OTB-2013, Temple-Color, UAV123 and UAV20L, respectively. Also, the experimental results demonstrate that our approach outperforms a deep feature-based tracker in overlap rate by 1.4%, 1.6%, 2.3%, 2.5% and 3.5% on OTB-2015, OTB-2013, Temple-Color, UAV123 and UAV20L, respectively.},
  archive      = {J_IETIP},
  author       = {Fei Wang and Guixi Liu and Yi Zhang},
  doi          = {10.1049/ipr2.12462},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2890-2910},
  shortjournal = {IET Image Process.},
  title        = {Dual integration of multi-model with spatial-temporal occlusion-awareness for visual object tracking},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). New fully automatic approach for tissue identification in
histopathological examinations using transfer learning. <em>IETIP</em>,
<em>16</em>(11), 2875–2889. (<a
href="https://doi.org/10.1049/ipr2.12449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of computational techniques in the processing of histopathological images allows the study of the structural organization of tissues and their changes through diseases. This study aims to develop a tool for classifying histopathological images from breast lesions in the benign and malignant classes through magnification scales by an innovative way of using transfer learning techniques combined with machine learning methods and deep learning. The BreakHis dataset was used in the experiments, consisting of histopathological images of breast cancer with different tumor enlargement scales classified as Malignant or Benign. In this study, various combinations of Extractor-Classifiers were performed, thus seeking to compare the best model. Among the results achieved, the best Extractor-Classifier set formed was CNN DenseNet201, acting as an extractor, with the SVM RBF classifier, obtaining accuracy of 95.39% and precision of 95.43% for the 200X magnification factor. Different models were generated, compared to each other, and validated based on methods in the literature to validate the experiments, thus showing the effectiveness of the proposed model. The proposed method obtained satisfactory results, reaching results in the state-of-the-art for the multi-classification of subclasses from the different scale factors found in the BreakHis dataset and obtaining better results in the classification time.},
  archive      = {J_IETIP},
  author       = {Yongzhao Xu and Matheus A. dos Santos and Luís Fabrício F. Souza and Adriell G. Marques and Lijuan Zhang and José Jerovane da Costa Nascimento and Victor Hugo C. de Albuquerque and Pedro P. Rebouças Filho},
  doi          = {10.1049/ipr2.12449},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2875-2889},
  shortjournal = {IET Image Process.},
  title        = {New fully automatic approach for tissue identification in histopathological examinations using transfer learning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Airport small object detection based on feature enhancement.
<em>IETIP</em>, <em>16</em>(11), 2863–2874. (<a
href="https://doi.org/10.1049/ipr2.12387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object detection is essential for airport surface surveillance, but the objects on the scene are mostly small objects with low resolution, they have no obvious feature information. Due to the scale differences of the objects and the fixed receptive field on the feature maps, detectors cannot model multi-scale context information and cover all objects. In addition, although the video detection algorithm can be used as a method to solve the problem of small object detection, the temporal feature fusion method of current video detection is very dependent on the quality of a single feature map. Therefore, this paper aims to enhance the features of small objects of a single image. First, an attentional multi-scale feature fusion enhancement (A-MSFFE) network is built on the memory-enhanced global-local aggregation (MEGA) to supplement semantic and spatial information of small objects. Then, a context feature enhancement (CFE) module is designed for obtaining different receptive fields through different dilated convolutions. Meanwhile, a video detection dataset about the airport is established. Finally, the experimental results show that the proposed method can improve the detection accuracies of small objects and outperform other state-of-the-art video object detection algorithms in self-built airport dataset.},
  archive      = {J_IETIP},
  author       = {Xuan Zhu and Binbin Liang and Daoyong Fu and Guoxin Huang and Fan Yang and Wei Li},
  doi          = {10.1049/ipr2.12387},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2863-2874},
  shortjournal = {IET Image Process.},
  title        = {Airport small object detection based on feature enhancement},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IMG-forensics: Multimedia-enabled information hiding
investigation using convolutional neural network. <em>IETIP</em>,
<em>16</em>(11), 2854–2862. (<a
href="https://doi.org/10.1049/ipr2.12272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information hiding aims to embed a crucial amount of confidential data records into the multimedia, such as text, audio, static and dynamic image, and video. Image-based information hiding has been a significantly important topic for digital forensics. Here, active image deep steganographic approaches have come forward for hiding data. The least significant bit (LSB) steganography approach is proposed to conceal a secret message into the original image. First, the lightweight stream encryption cryptography encrypts secret information in the cover image to protect embedded information from source to destination. Whereas the encrypted embedded cover information into the carrier of stego-image with the help of the LSB and then transmit. In the proposed investigational scheme, a convolutional neural net is used. A model is trained to detect and extract patterns of image hidden features, encrypted stego-image optimization, and classify original and cover images of steganography. Through the experiment result on the forensic image database for mobile steganography of the Center for Statistics and Application in Forensic Evidence, the overall embedded and extracting that the proposed scheme can achieve information hiding as well as revealing with an accuracy rate of 95.1%. The experimental result shows the robustness of the model in terms of efficiency as compared to other state-of-the-art schemes.},
  archive      = {J_IETIP},
  author       = {Abdullah Ayub Khan and Aftab Ahmed Shaikh and Omar Cheikhrouhou and Asif Ali Laghari and Mamoon Rashid and Muhammad Shafiq and Habib Hamam},
  doi          = {10.1049/ipr2.12272},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2854-2862},
  shortjournal = {IET Image Process.},
  title        = {IMG-forensics: Multimedia-enabled information hiding investigation using convolutional neural network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Falling motion detection algorithm based on deep learning.
<em>IETIP</em>, <em>16</em>(11), 2845–2853. (<a
href="https://doi.org/10.1049/ipr2.12208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Falling is a significant cause of injuries and even death in the elderly. The timely detection of the fall action helps to rescue people who may have physical health problems due to the fall, so fall detection is necessary. The traditional fall detection methods are mostly based on wearable devices, which need to be worn all the time, and the cost of the device is high. In recent years, the fall detection method based on computer vision has become a research hot spot. This paper proposes a framework for falling motion detection based on deep learning. To quickly and accurately classify human movements, a method using bone key points as the feature descriptors of human movements is proposed. The OpenPose algorithm is used to extract the human skeleton point information as the primary human body feature, and then use the deep learning method to classify further and recognise our action features. In this paper, four types of daily actions, such as falling and walking, are classified and recognised. The results show that the algorithm achieves an accuracy of 99.4% on our dataset. Simultaneously, 86.1% accuracy is reached in the public dataset fall detection dataset.},
  archive      = {J_IETIP},
  author       = {Na Zhu and Guangzhe Zhao and Xiaolong Zhang and Zhexue Jin},
  doi          = {10.1049/ipr2.12208},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2845-2853},
  shortjournal = {IET Image Process.},
  title        = {Falling motion detection algorithm based on deep learning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consensus rule for wheat cultivar classification on VL, VNIR
and SWIR imaging. <em>IETIP</em>, <em>16</em>(11), 2834–2844. (<a
href="https://doi.org/10.1049/ipr2.12206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To facilitate the quality assessment of wheat cultivars, diverse imaging tools and techniques have been applied in order to omit the expert decision, which can cause failure in the identification of the wheat cultivar&#39;s label and its quality simultaneously. To minimize the risks caused by the expert&#39;s decision, a promising framework for identification is greatly required to more effectively assess wheat type. Therefore, to be beneficial to this association, two methods have been developed by performing traditional and modern feature extraction algorithms on visible light (VL), visible near-infrared (VNIR) and short-wave infrared (SWIR) imaging as well as a fusion of these imaging systems. The proposed systems are called the bag of word (BoW) framework and convolutional neural networks (CNN) framework. With regard to wheat cultivar detection, the consensus rule has been established based on decisions predicted by CNN and BoW frameworks. The accuracy results obtained by consensus rule indicate that we have achieved 99.94% and 68.94% in case of CNN framework and BoW framework, respectively. Experimental results suggest that BoW features are not suitable to represent and match texture patterns such as repeated wheat kernels in an image, whereas CNN features always outperform handcrafted elements and properties for all datasets.},
  archive      = {J_IETIP},
  author       = {Şahin Işık and Kemal Özkan and Duygu Zeynep Demirez and Erol Seke},
  doi          = {10.1049/ipr2.12206},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2834-2844},
  shortjournal = {IET Image Process.},
  title        = {Consensus rule for wheat cultivar classification on VL, VNIR and SWIR imaging},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved picture-based prediction method of PM2.5
concentration. <em>IETIP</em>, <em>16</em>(11), 2827–2833. (<a
href="https://doi.org/10.1049/ipr2.12204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PM2.5 can bring serious harm to people&#39;s health and life because it easily causes cardiovascular disease and increases the risk of cancer. Hence, monitoring PM2.5 real-timely becomes a key problem in environmental protection. Towards this end, this paper proposes an improved picture-based prediction method of PM2.5 concentration using artificial neural network (ANN). Firstly, the weather image is transformed into Hue, Saturation, Value (HSV) color space to extract its saturation map, then the corresponding spatial and transform-based entropy features of image space are extracted. Secondly, the PM2.5 concentration model is built based on the two extracted features from the weather image using Artificial Neural Network (ANN) theory. Thirdly, an ANN model is trained using the pre-processed data. The training parameters and conditions are also explored through multiple experiments to achieve the best model accuracy. Experimental results show that the model has the best prediction effect when comparing to other state-of-the-art models.},
  archive      = {J_IETIP},
  author       = {Qili Chen and Wenbai Chen and Guangyuan Pan},
  doi          = {10.1049/ipr2.12204},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2827-2833},
  shortjournal = {IET Image Process.},
  title        = {An improved picture-based prediction method of PM2.5 concentration},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph convolutional network-based image matting algorithm
for computer vision applications. <em>IETIP</em>, <em>16</em>(10),
2817–2825. (<a href="https://doi.org/10.1049/ipr2.12528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image matting plays a vital role in a variety of computer vision tasks including video editing and image fusion. Previously presented image matting algorithms might fail in producing favorable results since most of them concentrate on the similarity between the neighboring pixels while neglecting the corresponding spatial relationship. To address this issue, an end-to-end image matting framework through leveraging deep learning mechanism and graph theory is proposed. The proposed pipeline is a concatenation of one deep feature extraction component and a Graph Convolutional Network (GCN). The former part takes an image and its corresponding trimap as inputs and can generate the pixel-wise features, which are then exploited as the input of the GCN locating at the latter part of the proposed framework. The GCN would refine the features for every pixel and predict the alpha matte outcome of the image. The approach outperforms a group of state-of-the-art matting techniques as shown by the theoretical analysis and experimental results in terms of both accuracy and visual effects.},
  archive      = {J_IETIP},
  author       = {Li Dong and Zheng Liang and Yue Wang},
  doi          = {10.1049/ipr2.12528},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2817-2825},
  shortjournal = {IET Image Process.},
  title        = {Graph convolutional network-based image matting algorithm for computer vision applications},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The stereo matching algorithm based on an improved adaptive
support window. <em>IETIP</em>, <em>16</em>(10), 2803–2816. (<a
href="https://doi.org/10.1049/ipr2.12527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In binocular stereo matching, there has been a problem of low matching accuracy and noise immunity in discontinuous regions and weak-textured regions. This paper proposes a stereo matching algorithm based on an improved adaptive support window. In the cost computation stage, first, according to the preset arm length and colour threshold, a cross-based arm is obtained, which centres on the pixel to be matched; then the adaptive regions of the vertical arm and the horizontal arm are constructed respectively, which have different shape and size. Finally, the union of the two adaptive regions is used as the final support window of Census transform. Performance evaluations on Middlebury stereo data sets demonstrate that the proposed algorithm outperforms other seven most challenging stereo matching algorithms. The mismatching rate of this algorithm is greatly reduced, and the anti-noise performance is also improved considerably. Because the construction of the adaptive region is based on strict criteria and comprehensive consideration, the algorithm proposed in the paper can improve the matching accuracy in the weak-textured regions and discontinuous disparity regions.},
  archive      = {J_IETIP},
  author       = {Jiyang Qi and Liang Liu},
  doi          = {10.1049/ipr2.12527},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2803-2816},
  shortjournal = {IET Image Process.},
  title        = {The stereo matching algorithm based on an improved adaptive support window},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptively weighted learning method for magnetic resonance
fingerprinting. <em>IETIP</em>, <em>16</em>(10), 2791–2802. (<a
href="https://doi.org/10.1049/ipr2.12526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In magnetic resonance fingerprinting, every fingerprint evolution is the combined result of multiple intrinsic parameters (such as T 1 and T 2 ) and system parameters. Present learning-based methods do not fully take into consideration of the diversity of parameters, which averages multiple parameters estimation loss. Because of the non-linear coupling nature between fingerprint evolutions and multiple parameters, different parameters have different contributions to the pattern of fingerprints. Even for the same parameter, different value ranges have different contributions to the fingerprints. During the learning processing, neglecting the diversity of parameters induces over fitting or out fitting of the network. To solve this problem, an adaptively weighted learning method is proposed. Taking the estimation uncertainty of each parameter as its weight, a weighted loss function is constructed to train the network. The weights of different parameters compete to obtain the optimal learning direction. Reconstructed fingerprints with 10% random noise is applied to train the network, and the fingerprints with different noise levels (5%–10%) are used to validate the robustness of the network. The results of simulation experiments show that the proposed method obtains better performance in terms of estimation accuracy and precision.},
  archive      = {J_IETIP},
  author       = {Min Li and Zehao Lee and Zhuo Zhang},
  doi          = {10.1049/ipr2.12526},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2791-2802},
  shortjournal = {IET Image Process.},
  title        = {Adaptively weighted learning method for magnetic resonance fingerprinting},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bit-level image encryption algorithm based on
fully-connected-like network and random modification of edge pixels.
<em>IETIP</em>, <em>16</em>(10), 2769–2790. (<a
href="https://doi.org/10.1049/ipr2.12525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A bit-level image encryption algorithm based on Fully-Connected-Like network(FCLN) and random modification of edge pixels is proposed. In the paper, in order to enhance the security of the cryptographic system, random noise is first used to modify the least significant bits of the edge pixels of the image, and the modified image is used as the input image. Later,the chaotic sequence is used to perform cyclic shift transformation on the image. In the subsequent steps, the FCLN is generated based on a fully connected neural network, which can perform scrambling and diffusion operations on the input image. Finally, the bidirectional diffusion method is used to diffuse the image forward and backward. In addition, the image after the edge pixel modification is convolved with the chaotic sequence, and the initial value of the chaotic system is set by the result to establish the correlation between the plain image and the algorithm, which makes the algorithm resistant to known/chosen plaintext attack. Experimental results show that although the image is modified by random noise, the decrypted image is visually the same as the original image. At the same time, through the analysis of common attacks such as differential attacks, noise attacks, and data loss attacks, our algorithm shows high security.},
  archive      = {J_IETIP},
  author       = {Yaohui Sheng and Jinqing Li and Xiaoqiang Di and Zhenlong Man and Zefei Liu},
  doi          = {10.1049/ipr2.12525},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2769-2790},
  shortjournal = {IET Image Process.},
  title        = {Bit-level image encryption algorithm based on fully-connected-like network and random modification of edge pixels},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single image deraining using contrastive perceptual
regularization. <em>IETIP</em>, <em>16</em>(10), 2759–2768. (<a
href="https://doi.org/10.1049/ipr2.12524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rain streaks pollute the image captured from outdoor vision system, and single image deraining approach based on data-driven has witnessed the continuously growing and achieved great success. Here, an end-to-end network for single image deraining is proposed. Firstly, to address the limit of convolution neural network (CNN) which can only extract local feature, a graph based basic block is proposed to extract global feature. The basic block consists of graph convolutional network (GCN) and CNN. The GCN module which combines spatial coherence computing and channel correlation computing is introduced to extract non-local information. While the CNN module, which combines the channel attention and pixel attention, is used to earn more weight from important local features. Secondly, a contrastive perceptual regularization is adopted to enhance the loss function, and a more natural image is restored by utilizing the information from both positive and negative samples with the regularization. The restored image is pulled closer to the positive clear image and pushed farther away to the negative rainy image. The experiment results on several datasets demonstrate that these methods achieve better results than the previous state-of-art methods.},
  archive      = {J_IETIP},
  author       = {Bin Hu},
  doi          = {10.1049/ipr2.12524},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2759-2768},
  shortjournal = {IET Image Process.},
  title        = {Single image deraining using contrastive perceptual regularization},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Supervised dual tight frame learning with deep thresholding
network for phase retrieval. <em>IETIP</em>, <em>16</em>(10), 2752–2758.
(<a href="https://doi.org/10.1049/ipr2.12522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven tight frames are popular for solving imaging inverse problems. However, the imaging quality is limited by the representation ability of single tight frame and thresholds tuned manually. In this work, a supervised dual tight frame learning framework fused with an elaborated deep thresholding network (DTN) is proposed, and the issue of low-quality reconstructions in previous phase retrieval (PR) algorithms is addressed. To effectively learn dual tight frames, a loss function is formed using the mean square error, tight constraint, dual constraint, and sparse constraint terms. Moreover, to determine the thresholds adaptively, the thresholds are extracted from the frame coefficients via DTN. By an end-to-end supervised learning manner, the dual tight frames and DTN are jointly trained from labels and their counterparts corrupted by Gaussian noise. Using the Gaussian denoiser constructed by dual tight frames, a regularisation model is firstly designed, and then exploited to formulate a PR optimisation problem. The image filtering and image updating steps are performed alternatively for solving this problem. Particularly, the image updating subproblem is tackled by an inertial epigraph solver. The simulation experiments show that the proposed PR algorithm can obtain higher-quality reconstructions compared with the benchmark ones.},
  archive      = {J_IETIP},
  author       = {Baoshun Shi and Qiusheng Lian and Yueming Su},
  doi          = {10.1049/ipr2.12522},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2752-2758},
  shortjournal = {IET Image Process.},
  title        = {Supervised dual tight frame learning with deep thresholding network for phase retrieval},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DS-SRI: Diversity similarity measure against scaling,
rotation, and illumination change for robust template matching.
<em>IETIP</em>, <em>16</em>(10), 2738–2751. (<a
href="https://doi.org/10.1049/ipr2.12521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel multi-scale template matching method that can be applied in unconstrained environments. The key component behind this is a general similarity measure is referred to as the diversity similarity measure against scaling, rotation, and illumination (DS-SRI). Specifically, DS-SRI exploits bidirectional diversity calculated from the nearest neighbour matches between two sets of points. Scaling and rotation changes are taken into consideration by introducing normalisation term on the scale change, and geometric consistency term with respect to the polar coordinate system. Moreover, in order to deal with the illumination change and further deformation, illumination-corrected local appearance and rank information are jointly exploited during the nearest neighbour search. All the features of DS-SRI are statistically assessed, and the extensive visual and quantitative results on both synthetic and real-world data show that DS-SRI can significantly outperform state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Yi Zhang and Chao Zhang and Takuya Akashi},
  doi          = {10.1049/ipr2.12521},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2738-2751},
  shortjournal = {IET Image Process.},
  title        = {DS-SRI: Diversity similarity measure against scaling, rotation, and illumination change for robust template matching},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning interactive multi-object segmentation through
appearance embedding and spatial attention. <em>IETIP</em>,
<em>16</em>(10), 2722–2737. (<a
href="https://doi.org/10.1049/ipr2.12520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning approaches to interactive image segmentation are typically formulated as a binary labeling problem. A model trained to make predictions within a fixed set of labels (i.e., foreground and background labels) cannot be used to directly predict the binary masks of multiple objects of interest, which greatly limits its flexibility and adaptivity. The use of different classes of clicks as input is opted for and the first end-to-end learning model for multi-object segmentation, based on a new designed neural network, is developed. The network consists of a visual feature extractor, a recurrent attention module and a dynamic segmentation head, extracts user click-adapted appearance embedding features and spatial attention features, and then learns to transform this information into a segmentation of multiple objects. It is also proposed to train the network using a joint loss function, taking the embedding learning into account for segmentation. Comprehensive experiments are conducted on three benchmark datasets to demonstrate the effectiveness of the proposed method. It performs favorably against state-of-the-art approaches on the multiple object segmentation task, for example, with 0.15 s per image, 0.06 s per object and mean IoU &amp; F1 score of 84.90% on Pascal VOC 2012 validation set. It is further shown that the method can be used in numerous vision applications such as image recoloring and colorization.},
  archive      = {J_IETIP},
  author       = {Yan Gui and Bingqiang Zhou and Jianming Zhang and Cheng Sun and Lingyun Xiang and Jin Zhang},
  doi          = {10.1049/ipr2.12520},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2722-2737},
  shortjournal = {IET Image Process.},
  title        = {Learning interactive multi-object segmentation through appearance embedding and spatial attention},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unmanned aerial vehicles object detection based on image
haze removal under sea fog conditions. <em>IETIP</em>, <em>16</em>(10),
2709–2721. (<a href="https://doi.org/10.1049/ipr2.12519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) have gradually become a major air threat to ships because of small size, good maneuverability, and low cost. Vision-based UAV detection offers one of the main ways to identify and protect against UAVs. Unlike land environment, the weather is complicated at sea. The visibility of an object is undermined by such factors as sea fog and sunlight, which makes it difficult to detect UAVs at sea through vision-based object detection. For the purpose of object detection at sea, this paper proposes a UAV object detection method based on image haze removal. In the proposed method, an improved dark channel haze removal (DCHR) algorithm is utilized to remove haze for and restore video images. Additionally, co-ordinate attention (CoordAttention, CA) is introduced to the lightweight algorithms of You Only Look Once (YOLO) for the object detection in restored video images, so as to improve the precision and speed of detection and reduce the miss rate. Some video images are also taken for detection experiments to verify the feasibility and effectiveness of the proposed method.},
  archive      = {J_IETIP},
  author       = {Wang Pikun and Wu Ling and Qi Jiangxin and Dai Jiashuai},
  doi          = {10.1049/ipr2.12519},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2709-2721},
  shortjournal = {IET Image Process.},
  title        = {Unmanned aerial vehicles object detection based on image haze removal under sea fog conditions},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Component-based nearest neighbour subspace clustering.
<em>IETIP</em>, <em>16</em>(10), 2697–2708. (<a
href="https://doi.org/10.1049/ipr2.12518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the problem of clustering data points that lie near or on a union of independent low-dimensional subspaces is addressed. To this end, the popular spectral clustering-based algorithms usually follow a two-stage strategy that initially builds an affinity matrix and then applies spectral clustering. However, an inappropriate affinity matrix that does not sufficiently connect data points lying on the same subspace will easily lead to the issue of over-segmentation. To alleviate this issue, building the affinity matrix based on subspace hypotheses generated by an iterative sampling operation according to the Random Cluster Model under the framework of energy minimisation is proposed. Specifically, each hypothesis is generated from a large number of data points by sampling a component in a K -nearest neighbour graph. Extensive experiments on synthetic data and real-world datasets show that the proposed method can improve the connectivity of the affinity matrix and provide competitive results against state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Katsuya Hotta and Haoran Xie and Chao Zhang},
  doi          = {10.1049/ipr2.12518},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2697-2708},
  shortjournal = {IET Image Process.},
  title        = {Component-based nearest neighbour subspace clustering},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Morphological geodesic active contour algorithm for the
segmentation of the histogram-equalized welding bead image edges.
<em>IETIP</em>, <em>16</em>(10), 2680–2696. (<a
href="https://doi.org/10.1049/ipr2.12517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessment and evaluation are the essential processes of industrially manufactured products for the determination of the quality and quantity of products. They give justifications in a practical way about whether the machine is perfect or imperfect, which can lead to a better or poorer production. In this study, the authors propose an algorithm that uses morphological geodesic active contour and image processing techniques to perform segmentation and assess the performance of a robot used to manufacture welding beads. The algorithm has four parameters which are pre-processed images, balloon force, smoothing parameter, and number of iterations. To pre-process the images, the algorithm uses an inverse Gaussian gradient operator for edge detection and applies the histogram equalization method to level the distribution. To detect the external contour of the bead, the level set is initialized as the region of interest whereby a balloon force can inflate or deflate towards the edges. To smoothen the contour, a smoothing parameter is applied to convert the jagged lines into a curve over a reasonable number of iterations. Based on the experimental results, the authors’ algorithm used a fixed balloon force of −2, a smoothing parameter value of 4, and 40 iterations to segment images obtained from three different environments. The computation time for the segmentation and evaluation of one image was 0.70, 0.61, and 0.67 s for datasets with high brightness, low brightness, and normal brightness, respectively. Additionally, the authors’ proposed algorithm achieved an outstanding performance of 0.9954, 0.9843, 0.9892, and 0.9435 in terms of recall, precision, F-measure, and IOU, respectively. To justify the performance of the authors’ proposed algorithm, the authors compared it with the existing algorithms and found that it worked better than all the others for segmentation, although it lagged behind the entropy-based algorithm in terms of speed.},
  archive      = {J_IETIP},
  author       = {John N. Mlyahilu and Joseph N. Mlyahilu and Jae Eun Lee and Young Bong Kim and Jong Nam Kim},
  doi          = {10.1049/ipr2.12517},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2680-2696},
  shortjournal = {IET Image Process.},
  title        = {Morphological geodesic active contour algorithm for the segmentation of the histogram-equalized welding bead image edges},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sub-region non-local mean denoising algorithm of synthetic
aperture radar images based on statistical characteristics.
<em>IETIP</em>, <em>16</em>(10), 2665–2679. (<a
href="https://doi.org/10.1049/ipr2.12516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When synthetic aperture radar (SAR) images are denoised by non-local mean (NLM) algorithm, logarithmic transformation will lead to the loss of some image information. To keep the details and smooth the noise of the SAR images better, a new sub-region NLM denoising algorithm with the statistical characteristics of SAR image is proposed in this paper. Firstly, the probability distribution image is generated by calculating the probability value of every pixel. Then the images can be divided into the heterogeneous region and the homogeneous region by the threshold obtained with the variation coefficient of the probability image. A new filtering weight using both the original and probability images is generated based on NLM in the heterogeneous region. The filtering weight is obtained using the probability image in the homogeneous region. This method fully considers the characteristics of noise in different regions. Multi-SAR image experiments demonstrate the advantages of noise smooth and detail protection.},
  archive      = {J_IETIP},
  author       = {Wei Ma and Zhihui Xin and Guisheng Liao and Yu Sun and Zhixu Wang and Jiayu Xuan},
  doi          = {10.1049/ipr2.12516},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2665-2679},
  shortjournal = {IET Image Process.},
  title        = {Sub-region non-local mean denoising algorithm of synthetic aperture radar images based on statistical characteristics},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image fragile watermarking algorithm based on
deneighbourhood mapping. <em>IETIP</em>, <em>16</em>(10), 2652–2664. (<a
href="https://doi.org/10.1049/ipr2.12515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the security risk caused by fixed offset mapping and the limited recoverability of random mapping used in image watermarking, a self-embedding fragile image watermarking algorithm based on deneighbourhood mapping are proposed. First, the image is divided into several 2 × 2 blocks, and authentication watermark and recovery watermark are generated based on the average value of the image blocks. Then, the denighbourhood mapping is implemented as, for each image block, its mapping block is randomly selected outside its neighbourhood. Finally, the authentication watermark and the recovery watermark are embedded into the image block itself and its mapping block. Theoretical analysis indicates that in the case of continuous area tampering, the proposed watermarking algorithm can achieve a better recovery rate than that of the method based on the random mapping. The experimental results verify the rationality and effectiveness of the theoretical analysis. Moreover, compared with the existing embedding algorithms based on random mapping, chaos mapping, and Arnold mapping, in the case of continuous area tampering, the proposed algorithm also achieves a higher average recovery rate.},
  archive      = {J_IETIP},
  author       = {Yilong Wang and Zhenyu Li and Daofu Gong and Haoyu Lu and Fenlin Liu},
  doi          = {10.1049/ipr2.12515},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2652-2664},
  shortjournal = {IET Image Process.},
  title        = {Image fragile watermarking algorithm based on deneighbourhood mapping},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EAOD-net: Effective anomaly object detection networks for
x-ray images. <em>IETIP</em>, <em>16</em>(10), 2638–2651. (<a
href="https://doi.org/10.1049/ipr2.12514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly object detection is the core technology in the application for X-ray images. However, the accuracy of current X-ray anomaly object detection method still needs to be improved. In this paper, an effective anomaly object detection network is proposed to improve the detection accuracy of anomaly object for X-ray images. Firstly, learnable Gabor convolution layer, deformable convolution, and spatial attention mechanism are introduced to enhance the representative ability of features in ResNeXt. Then, dense local regression is applied to predict the offset of multiple dense boxes in region proposal to locate the object accurately. At last, bigger discriminative RoI pooling is proposed to classify the candidate boxes to improve the accuracy of object classification. Experimental results on the SIXray and OPIXray datasets show that compared with the state-of-the-art methods, the proposed EAOD-Net can achieve the competitive detection performance.},
  archive      = {J_IETIP},
  author       = {Chunjie Ma and Li Zhuo and Jiafeng Li and Yutong Zhang and Jing Zhang},
  doi          = {10.1049/ipr2.12514},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2638-2651},
  shortjournal = {IET Image Process.},
  title        = {EAOD-net: Effective anomaly object detection networks for X-ray images},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-light image haze removal with light segmentation and
nonlinear image depth estimation. <em>IETIP</em>, <em>16</em>(10),
2623–2637. (<a href="https://doi.org/10.1049/ipr2.12513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hazy image obtained in the low-light environment has the characteristics of low contrast, non-uniform illumination, color cast and much noise. In this paper, a method is put forward which can be properly applied to recover low-light hazy images. The original image is first decomposed into glow layer and haze layer with a modified color channel transformation for glow artifacts and color balanced. A new light segmentation function is proposed next by using gamma correction of channel difference and setting threshold levels to determine if the pixel belongs to light source regions. Then the ambient illuminance map is estimated using maximum reflectance prior to computing the atmosphere light in the light and non-light regions. Finally, a novel nonlinear image depth estimation model is established to build the relationship between the image depth map and three image features including luminance, saturation and gradient map for the light areas. The experimental results prove that the dehazing algorithm is reliable for removing haze and glow artifacts of active light sources, reducing much noise and improving the visibility.},
  archive      = {J_IETIP},
  author       = {Jianwei Lv and Feng Qian and Bao Zhang},
  doi          = {10.1049/ipr2.12513},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2623-2637},
  shortjournal = {IET Image Process.},
  title        = {Low-light image haze removal with light segmentation and nonlinear image depth estimation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep learning-based method for pixel-level crack detection
on concrete bridges. <em>IETIP</em>, <em>16</em>(10), 2609–2622. (<a
href="https://doi.org/10.1049/ipr2.12512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crack detection of the concrete bridge is an essential index for the safety assessment of bridge structure. It is more important to check the whole structure than to check the accuracy in the damage assessment. However, the traditional deep learning model method cannot completely detect the crack structure, which challenges image-based crack detection. For this reason, we propose deep bridge crack classification (DBCC)-Net as a classification-based deep learning network. By pruning the Yolox, the regression problem of the target detection is converted to the binary classification problem to avoid the network performance degradation caused by the translation invariance of the convolutional neural network (CNN). In addition, the network post-processing and a two-stage crack detection strategy are proposed to enable the network to detect cracks and extract crack morphology in high-resolution images quickly. In the first stage, DBCC-Net realizes the coarse extraction of crack position based on image slice classification. In the second stage, the complete crack morphology is extracted from the location suggested by the semantic segmentation network. Experimental results show that the proposed two-stage method has 19 frames per second (FPS) and 0.79 Miou (mean intersection over union) at the actual bridge images with 2560×2560 pixels. Although FPS is reduced, the Miou value is 7.8% higher than other methods, proving this paper&#39;s practical value.},
  archive      = {J_IETIP},
  author       = {Ji Kun and Zhang Zhenhai and Yu Jiale and Dang Jianwu},
  doi          = {10.1049/ipr2.12512},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2609-2622},
  shortjournal = {IET Image Process.},
  title        = {A deep learning-based method for pixel-level crack detection on concrete bridges},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ROAM: Random layer mixup for semi-supervised learning in
medical images. <em>IETIP</em>, <em>16</em>(10), 2593–2608. (<a
href="https://doi.org/10.1049/ipr2.12511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation is one of the major challenges addressed by machine learning methods. However, these methods profoundly depend on a large amount of annotated data, which is expensive and time-consuming. Semi-supervised learning (SSL) approaches this by leveraging an abundant amount of unlabeled data. Recently, MixUp regularizer has been introduced to SSL methods by augmenting the model with new data points through linear interpolation at the input space. While this provides the model with new data, it is limited and may lead to inconsistent soft labels. It is argued that the linear interpolation at different representations provides the network with novel training signals and overcomes the inconsistency of the soft labels. This paper proposes ROAM as an SSL method that explores the manifold and performs linear interpolation on randomly selected layers to generate virtual data that has never been seen before, which encourages the network to be less confident for interpolated points. Hence it avoids overfitting, enhances the generalization, and shows less sensitivity to the domain shift. Extensive experiments are conducted on publicl datasets on whole-brain and lung segmentation. ROAM achieves state-of-the-art results in fully supervised (89.5%) and semi-supervised (87.0%) settings with relative improvements up to 2.40% and 16.50%, respectively.},
  archive      = {J_IETIP},
  author       = {Tariq Bdair and Benedikt Wiestler and Nassir Navab and Shadi Albarqouni},
  doi          = {10.1049/ipr2.12511},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2593-2608},
  shortjournal = {IET Image Process.},
  title        = {ROAM: Random layer mixup for semi-supervised learning in medical images},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Globality constrained adaptive graph regularized
non-negative matrix factorization for data representation.
<em>IETIP</em>, <em>16</em>(10), 2577–2592. (<a
href="https://doi.org/10.1049/ipr2.12510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from the good physical interpretations and low computational complexity, non-negative matrix factorization (NMF) has attracted wide attentions in data representation learning tasks. Some graph-based NMF approaches make the learned representation encode the topological structure by the local graph Laplacian regularizer, which improves the discriminant ability of data representation. However, the performance of graph-based NMF methods depend heavily on the quality of the predefined graph and the complexity of models is high. Here, a globality constrained adaptive graph regularized non-negative matrix factorization for data representation (GCAG-NMF) model is proposed, which not only uses the self-representation characteristics of data to learn an adaptive graph to describe the sample relationship more accurately, but also proposes a graph factorization technique to reduce the complexity of the model and improve the discriminative ability of data representation. Then, an iterative optimizing strategy with low complexity and strict convergence guarantee is developed to optimize the objective function. Experimental results on some databases demonstrate the effectiveness of the proposed model.},
  archive      = {J_IETIP},
  author       = {Yanfeng Sun and Jie Wang and Jipeng Guo and Yongli Hu and Baocai Yin},
  doi          = {10.1049/ipr2.12510},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2577-2592},
  shortjournal = {IET Image Process.},
  title        = {Globality constrained adaptive graph regularized non-negative matrix factorization for data representation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EU-net: A novel semantic segmentation architecture for
surface defect detection of mobile phone screens. <em>IETIP</em>,
<em>16</em>(10), 2568–2576. (<a
href="https://doi.org/10.1049/ipr2.12509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manual or conventional image processing algorithms are commonly used to detect surface problems on mobile phone screens. However, inefficiency and inflexibility are disadvantages. Although the semantic segmentation method has high adaptability and accuracy, it also has a low defect detection efficiency due to its excessive parameters. In order to increase defect detection efficiency, a novel efficient encoder–decoder architecture termed MB encoder–decoder architecture based on MBConv blocks, and that it reduces the number of parameters used in semantic segmentation methods i presented. In addition, by applying the MB encoder–decoder design to the U-Net, the efficient U-Net (EU-Net) is proposed. It confirms the MB encoder–decoder architecture&#39;s superiority. Then, EU-Net to mobile phone surface defect detection in real industrial scenarios. Experimental results on a dataset show the superiority of the proposed algorithm and it can meet the real-time requirement of industrial production.},
  archive      = {J_IETIP},
  author       = {Jiawei Pan and Deyu Zeng and Qi Tan and Zongze Wu and Zhigang Ren},
  doi          = {10.1049/ipr2.12509},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2568-2576},
  shortjournal = {IET Image Process.},
  title        = {EU-net: A novel semantic segmentation architecture for surface defect detection of mobile phone screens},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A two-stage method for single image de-raining based on
attention smoothed dilated network. <em>IETIP</em>, <em>16</em>(10),
2557–2567. (<a href="https://doi.org/10.1049/ipr2.12504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rain can severely hamper the visibility of scene objects. Although existing deep learning methods have reported promising performance, they often fail to obtain satisfactory results in many practical situations, especially when the input image contains both rain streaks and haze-like degradation. In this paper, a new two-stage method based on attention smoothed dilated network (SDN) is proposed. Unlike most fully-supervised methods, the mixture of rain streaks and haze-like effects is considered in the model. The proposed method consists of two stages. First, a generative adversarial network guided by the rain-streak attention map is proposed to remove rain streaks, where a multi-stage attention module is used to accurately locate rain streaks in the generator. Second, haze-like effects are further removed through SDN with the same structure as the generator. Extensive experiments on multiple datasets show that the method outperforms the state-of-the-art in both objective evaluation and visual quality.},
  archive      = {J_IETIP},
  author       = {Shuangli Du and Hengrui Fan and Minghua Zhao and Haomai Zong and Jing Hu and Peng Li},
  doi          = {10.1049/ipr2.12504},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2557-2567},
  shortjournal = {IET Image Process.},
  title        = {A two-stage method for single image de-raining based on attention smoothed dilated network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A free-form surface flattening algorithm that minimizes
geometric deformation energy. <em>IETIP</em>, <em>16</em>(9), 2544–2556.
(<a href="https://doi.org/10.1049/ipr2.12508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the analysis of the advantages and disadvantages of existing surface flattening algorithms, this paper proposes a free-form surface flattening algorithm that minimizes geometric deformation by combining the advantages of the geometric flattening method and the mechanical energy flattening method to address the problems of many iterations, large changes in convergence, and weak visualization of deformation. The point cloud surface is meshed using the triangular slice search method, and the 3D surface mesh is wrapped around the surface using geometric mapping relationships. The initial correction and deformation analysis of the ring flatten graphic are carried out according to the average flattening error, and a global geometric deformation energy model is established to obtain the energy-minimizing unfolding conditions and optimize the initial flattening graph by iterative optimization. The verification of the algorithm shows that the algorithm is general, has good robustness, high flattening accuracy, and visualizes the flattening deformation. The method is suitable for some design occasions of machining, such as the flattening calculation of automobile outer cladding parts and sheet metal parts. It is especially suitable for the flattening calculation of ring-like parts, and is a good guide for the design calculation and stamping process of sheet-metal-like parts.},
  archive      = {J_IETIP},
  author       = {Pengfei Zheng and Qing Liu and Jingjing Lou and Chengjie Lian and Dajun Lin},
  doi          = {10.1049/ipr2.12508},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2544-2556},
  shortjournal = {IET Image Process.},
  title        = {A free-form surface flattening algorithm that minimizes geometric deformation energy},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast CP-compression layer: Tensor CP-decomposition to
compress layers in deep learning. <em>IETIP</em>, <em>16</em>(9),
2535–2543. (<a href="https://doi.org/10.1049/ipr2.12507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network (DNN) shows its powerful performance in terms of image classification and many other applications. However, as the number of network layers increases, it brings huge pressure on devices with limited resources. In this article, a novel network compression algorithm is proposed that compresses the original network by up to about 60 times. In particular, a tensor Canonical Polyadic(CP) decomposition based algorithm is proposed to compress the weight matrix in the fully connected(FC) layer and the convolution kernel in the convolution layer. Traditional tensor decomposition algorithms are usually to first pre-train the weights, and decompose the weights, finally perform fine-tuning on the factors in the second training phase. Instead, the decomposed factors are directly updated by performing tensor CP decomposition on weight without fine-tuning. The proposed algorithm is called Fast CP-Compression Layer method in this paper. Experiments show that the proposed algorithm cannot only reduce computing time and improve compression factor but also improve accuracy on some datasets.},
  archive      = {J_IETIP},
  author       = {Yuwang Ji and Qiang Wang},
  doi          = {10.1049/ipr2.12507},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2535-2543},
  shortjournal = {IET Image Process.},
  title        = {Fast CP-compression layer: Tensor CP-decomposition to compress layers in deep learning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-stage single image dehazing network using
swin-transformer. <em>IETIP</em>, <em>16</em>(9), 2518–2534. (<a
href="https://doi.org/10.1049/ipr2.12506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hazy images often have color distortion, blur and other visible visual quality degradation, affecting the performance of some advanced visual tasks. Therefore, single image dehazing has always been a challenging and significant problem. Convolutional neural network has been widely used in image dehazing task, but the limitations of convolutional operation limit the development of dehazing task. Nowadays, Transformer offers a holistic approach to CV development and does not grow in location as the network deepens. For this reason, a hierarchical Transformer is introduced for use in the dehazing network. Specifically, the codec is improved and Transformer and CNN are combined to achieve basic feature extraction in the first stage. The encoder only models the global relationship at each layer, reducing the resolution of the feature map continuously and expanding the field of perception. In addition, an inter-block supervision mechanism is added between encoder unit and decoder unit to refine features and supervise and select them, thus improving the efficiency of feature transmission. In the second stage, the original resolution block is used to extract the local features, and then feature fusion and interaction are carried out. In addition, to ensure the authenticity of the transmission of characteristic signals in the first stage and improve the transmission efficiency of the network, fusion attention mechanism is added between stages. It adds the residual image of the early input features to the image acquired in the first stage, then passes to the next stage. Ablation experiments show that the two-stage network has significant benefits for image quality and visual effects. The experimental results on RESIDE, O-Haze, and I-Haze datasets show that the method is superior to advanced methods in dehazing effectiveness.},
  archive      = {J_IETIP},
  author       = {Xiaoling Li and Zhen Hua and Jinjiang Li},
  doi          = {10.1049/ipr2.12506},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2518-2534},
  shortjournal = {IET Image Process.},
  title        = {Two-stage single image dehazing network using swin-transformer},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High resolution representation-based siamese network for
remote sensing image change detection. <em>IETIP</em>, <em>16</em>(9),
2506–2517. (<a href="https://doi.org/10.1049/ipr2.12505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection of high-resolution remote sensing images can help to accurately understand the changes in the earth&#39;s surface. Advanced methods based on deep features have some limitations, including limited accuracy, poor detection effect, and poor robustness. The main reason is that these frameworks have poor feature extraction capabilities, insufficient context aggregation, and inadequate discrimination capabilities. In order to solve these problems, SiHDNet, a Siamese segmentation network based on deep, high-resolution differential feature interaction, is proposed. Specifically, after the high-resolution features of the dual-temporal image are extracted, the difference map is generated through a special fusion module, which contains sufficient and effective change information. Finally, the final binary change map is obtained through the improved spatial pyramid pooling module. Experiments are conducted on the newly released building change detection data set LEVIR-CD and the challenging remote sensing image change detection data set Google Data Set. Five benchmark methods are chosen. The results of quantitative analysis and qualitative comparison show that SiHDNet is superior to the five benchmark methods. The results of the ablation experiment also verify the effectiveness of this method.},
  archive      = {J_IETIP},
  author       = {Zheng Liang and Bin Zhu and Yaoxuan Zhu},
  doi          = {10.1049/ipr2.12505},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2506-2517},
  shortjournal = {IET Image Process.},
  title        = {High resolution representation-based siamese network for remote sensing image change detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A dense r-CNN multi-target instance segmentation model and
its application in medical image processing. <em>IETIP</em>,
<em>16</em>(9), 2495–2505. (<a
href="https://doi.org/10.1049/ipr2.12503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the medical image analysis domain, medical image segmentation has a significant impact on the quantitative analysis of organ or tissue function, as the first and critical component of diagnosis and treatment pipeline. In this paper, a dense R-CNN segmentation model based on dual-attention are proposed for medical images multi-target instance segmentation. The model combines channel and spatial attention mechanism to extract image features and fuse multi-scale feature information hierarchically. It combines up-sampling strategies such as dilated convolution and bilinear interpolation to strengthen the distinguishability between multi-target instances and pixel-level features in other regions. The multi-target detection mechanism of R-CNN is combined with the multi-scale feature extraction and fusion ability of dense convolution network. In the encoding stage, the multi-scale hybrid bottleneck module and deformable convolution are introduced to extract more accurate structural feature information and increase the receptive-field. In the decoding stage, the bilinear interpolation and the adaptive hierarchical fusion mechanism are used to strengthen the distinguishability between the target region and other regions, and improve the accuracy of instance segmentation. Taking cardiac MRI segmentation as an example, the left and right ventricles, and left ventricular myocardium are selected as segmentation targets. The pixel accuracy is 90.82%, the class pixel accuracy is 87.91%, the mean intersection-over-union is 81.52%, the Dice coefficient is 89.82%, and Hausdorff distance is 9.2, which is improved compared with other methods. It verifies the accuracy and applicability of the proposed method for multi-target instance segmentation of medical images.},
  archive      = {J_IETIP},
  author       = {Ruiping Yang and Jiguo Yu and Jian Yin and Kun Liu and Shaohua Xu},
  doi          = {10.1049/ipr2.12503},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2495-2505},
  shortjournal = {IET Image Process.},
  title        = {A dense R-CNN multi-target instance segmentation model and its application in medical image processing},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A saliency guided remote sensing image dehazing network
model. <em>IETIP</em>, <em>16</em>(9), 2483–2494. (<a
href="https://doi.org/10.1049/ipr2.12502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a saliency guided remote sensing image dehazing network model. It consists of the following three blocks: A dense residual based backbone network, a saliency map generator, and a deformed atmospheric scattering model (ASM) based haze removal model, of which the dense residual based backbone network is used to capture the texture detail information of a remote sensing image, the saliency map generator is used to generate the saliency map of the related remote sensing image, and the generated saliency map is used to guide the network to capture more texture details through the guided fusion module. Finally, the deformed atmospheric scattering model (ASM) is used to remove haze from remote sensing images. The model here is compared with several state-of-art dehazing methods on synthetic data sets and real remote sensing images. Experimental results show that on the synthetic data set, the PSNR value of this model is increased by 4.47 db and the SSIM value is increased by 0.045 compared with the best model. On real remote sensing hazy images, the visual effect of our model is also better than that of existing methods. The authors also perform experiments to demonstrate that remote sensing image dehazing is helpful for remote sensing image detection automatically.},
  archive      = {J_IETIP},
  author       = {Zhenghao Shi and Shuai Shao and Zhaorun Zhou},
  doi          = {10.1049/ipr2.12502},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2483-2494},
  shortjournal = {IET Image Process.},
  title        = {A saliency guided remote sensing image dehazing network model},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid mapping algorithm based on 1-DCM and lorenz.
<em>IETIP</em>, <em>16</em>(9), 2467–2482. (<a
href="https://doi.org/10.1049/ipr2.12501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A hybrid chaotic mapping algorithm(HCM) based on 1-DCM (one-dimensional chaotic map) and Lorenz is proposed to address the security issues of low-dimensional chaotic systems and the complexity of high-dimensional chaotic systems. Firstly, the color images are divided into three high 4-bit planes (BP) and low 4-bit planes. By optimizing the parameters and structure of trigonometric function, the new arcsine-sine (AS) function is designedto generate the 1-DCM and permutation matrix, which is constructed to change the plane position and pixel position of the high 12BP to obtain the scrambling plane, which is encrypted by Lorenz chaotic system to produce the selfencryption matrix, with which the low 12BP is encrypted to generate the low bit encryption planes. Then, the ciphertext image is obtained by combining the high bit scrambling plane and low bit encryption plane. The SHA function is also used as the chaotic system&#39;s parameter generator, which improves the correlation between plaintext images and the key and resists selective plaintext attacks. The simulation results show that by modularizing the image&#39;s high and low bit planes, the encryption process is simplified and improved. The security and reliability of color image transmission are also improved.},
  archive      = {J_IETIP},
  author       = {Wen Yan and Su Jingming and Hong Yan and Gong Pingshun},
  doi          = {10.1049/ipr2.12501},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2467-2482},
  shortjournal = {IET Image Process.},
  title        = {Hybrid mapping algorithm based on 1-DCM and lorenz},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new fast estimating floor region based on image
segmentation for smart rovers. <em>IETIP</em>, <em>16</em>(9),
2457–2466. (<a href="https://doi.org/10.1049/ipr2.12500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new algorithm that hardware acceleration has enabled to estimate floor regions in a single image for unmanned ground vehicle (UGV) robots is presented. In general, the cluttered indoor surroundings patterned floors, shadows, and reflections make it extremely difficult to identify floor regions. The proposed algorithm combines extracting surface texture characteristics with specific geometric areas to determine object boundaries and then uses SVM classification to distinguish between the floor and non-floor regions. To achieve real-time performance, the implementation of the proposed algorithm on an SoC FPGA embedded platform gives a heterogeneous hardware acceleration methodology. In experimental results, a public MIT scene dataset and indoor database were used to verify detection accuracy. Compared to other research, the proposed algorithm accuracy can reach up to 94.72% on average without the assistance of any other physical sensors, such as RGB-D or laser ranger sensors.},
  archive      = {J_IETIP},
  author       = {Afaroj Ahamad and Chi-Chia Sun and Nian-Jyun Yang and Wen-Kai Kuo},
  doi          = {10.1049/ipr2.12500},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2457-2466},
  shortjournal = {IET Image Process.},
  title        = {A new fast estimating floor region based on image segmentation for smart rovers},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NHNet: A non-local hierarchical network for image denoising.
<em>IETIP</em>, <em>16</em>(9), 2446–2456. (<a
href="https://doi.org/10.1049/ipr2.12499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the fast development of deep learning models, hierarchical convolutional neural networks have achieved great success in image denoising tasks. To further boost the performance of image denoising, a novel non-local hierarchical network (NHNet) is proposed. Unlike existing U-Net-based hierarchical methods, which mainly focus on downsampling operations, NHNet adopts an initial resolution path and a high resolution path. Specifically, the high-resolution features are obtained through upsampling, where the non-local mechanism is adopted to capture the self-similarity properties, which contribute to a better denoising performance. Cross connections and channel attention layers are added between the two paths to integrate features in different resolutions. Compared with other U-Net-based hierarchical networks, NHNet requires fewer parameters. Experiments show that NHNet achieves state-of-the-art performance in Gaussian denoising tasks and gets competitive results when dealing with real image denoising.},
  archive      = {J_IETIP},
  author       = {Jiahong Zhang and Lihong Cao and Tian Wang and Wenlong Fu and Weiheng Shen},
  doi          = {10.1049/ipr2.12499},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2446-2456},
  shortjournal = {IET Image Process.},
  title        = {NHNet: A non-local hierarchical network for image denoising},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ground glass opacity detection and segmentation using CT
images: An image statistics framework. <em>IETIP</em>, <em>16</em>(9),
2432–2445. (<a href="https://doi.org/10.1049/ipr2.12498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer is one of the most profound causes of cancer-related deaths in the world. Early detection is known to significantly improve the chances of survival. Several detection and diagnostic methods are used for this purpose. CT is one of the most widely used non-invasive medical imaging modalities in this domain. The biggest challenge faced by radiologists in this case is detection and diagnosis of cancerous lung nodules. The growth of ground glass opacity (GGO) lesions is an indication of malignancy. However, GGO is difficult to capture for physicians as it manifests in the form of tiny, faint shadows. This research paper proposes an approach for aiding GGO identification in CT lung images for improved lung cancer prognosis. In the proposed approach, morphological reconstruction is used for segmentation. Once the region of interest (ROI) is extracted, statistical analysis using mean, standard deviation, variance, entropy, skewness, kurtosis, minimum grey scale value, maximum grey scale value and range is performed. The same statistical measures are determined for normal lung and distribution plot is drawn for comparison. It is observed that maximum grey-scale value demonstrates minimum overlap of approximately 7.4%. To reduce this, a joint feature by summing values of feature mean, skewness, and maximum grey-scale value was used. This approach reduced the overlap to approximately 1.32%. Lastly, ANN was used for classification of GGO and non-GGO lung tissue with an achieved accuracy of 99.5%.},
  archive      = {J_IETIP},
  author       = {S.A. Banday and Rafia Nahvi and A.H. Mir and S. Khan and Ahmad Saeed AlGhamdi and Sultan S. Alshamrani},
  doi          = {10.1049/ipr2.12498},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2432-2445},
  shortjournal = {IET Image Process.},
  title        = {Ground glass opacity detection and segmentation using CT images: An image statistics framework},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-scale GAN with residual image learning for removing
heterogeneous blur. <em>IETIP</em>, <em>16</em>(9), 2412–2431. (<a
href="https://doi.org/10.1049/ipr2.12497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing images with heterogeneous blur remains challenging due to multiple degradation aspects that could affect structural properties. This study proposes a deep learning-based multi-scaled generative adversarial network (GAN) with residual image learning to process variant and in-variant blur. Different scaled images with corresponding gradients are concatenated as a multi-channel single input for the proposed GAN. Residual- and dense-networks are combined to explore salient features in the bottleneck section while addressing the vanishing gradient problem. A hybrid content loss function with a gradient penalty minimises the error between generated and ground truth images. Due to structure sparsity, the generated output may lose some information that leads to artifacts. Residual image learning with dilation and end-to-end training is used to resolve this issue by recovering high-resolution anatomical details. Three different datasets: GoPro, Köhler, and Lai, with variant and in-variant blur, are used to perform qualitative and quantitative analyses. Experiments show the proposed method is effective in reducing blur while preserving structural properties compared to multiple preprocessing techniques for image analysis. Moreover, the consistently improved performance over multiple publicly available datasets validates the merits of the proposed method for large data analysis.},
  archive      = {J_IETIP},
  author       = {Rayyan Azam Khan and Yigang Luo and Fang-Xiang Wu},
  doi          = {10.1049/ipr2.12497},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2412-2431},
  shortjournal = {IET Image Process.},
  title        = {Multi-scale GAN with residual image learning for removing heterogeneous blur},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep grouping fusion neural network for multimedia content
understanding. <em>IETIP</em>, <em>16</em>(9), 2398–2411. (<a
href="https://doi.org/10.1049/ipr2.12496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How Deep Neural Networks (DNNs) best cope with the understanding of multimedia contents still remains an open problem, mainly due to two factors. First, conventional DNNs cannot effectively learn the representations of the images with sparse visual information. For example, the images describing knowledge concepts in textbooks. Second, existing DNNs cannot effectively capture the fine-grained interactions between the images and text descriptions. To address these issues, we propose a deep Cross-Media Grouping Fusion Network (CMGFN), which mainly has two distinctive properties: 1) CMGFN can effectively learn visual features from the images with sparse visual information. This is achieved by first progressively adjusting the attention of convolution filters to valuable visual regions, and then enhancing the use of key visual information in feature construction. 2) By a cross-media grouping co-attention mechanism, CMGFN can effectively use the interactions between visual features of different semantics and textual descriptions, to learn cross-media features representing different fine-grained semantics in different groups. Empirical studies demonstrate that CMGFN not only achieves state-of-the-art performance on the multimedia documents containing sparse visual information, but also shows superior general applicability on other multimedia data, e.g., the multimedia fake news.},
  archive      = {J_IETIP},
  author       = {Lingyun Song and Mengzhen Yu and Xuequn Shang and Yu Lu and Jun Liu and Ying Zhang and Zhanhuai Li},
  doi          = {10.1049/ipr2.12496},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2398-2411},
  shortjournal = {IET Image Process.},
  title        = {A deep grouping fusion neural network for multimedia content understanding},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive aggregation with self-attention network for
gastrointestinal image classification. <em>IETIP</em>, <em>16</em>(9),
2384–2397. (<a href="https://doi.org/10.1049/ipr2.12495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic classification of diseases in endoscopic images is essential to the improvement of diagnostic performance and the reduction of colorectal cancer mortality. However, due to the ambiguous boundary between background and foreground, abnormal classification in endoscopic images is still challenging. To tackle such a situation, an adaptive aggregation with self-attention network (AASAN), including a global branch, a local branch, and a fusion branch, is proposed imitating the diagnosis process of endoscopists. On this basis, the self-attention with relative position encoding (SA-RPE) module is designed to capture long-range dependencies and gather lesion neighborhood information. Furthermore, an adaptive aggregation feature (AAF) module is proposed and embedded into the fusion branch for final image label prediction, which is helpful to capture more discriminant features. Extensive experiments show that the classification accuracy of the authors&#39; method on Kvasir public dataset reaches 96.37% in a fivefold cross-validation, higher than the state-of-the-art deep learning algorithms.},
  archive      = {J_IETIP},
  author       = {Sheng Li and Jing Cao and Jiafeng Yao and Jinhui Zhu and Xiongxiong He and Qianru Jiang},
  doi          = {10.1049/ipr2.12495},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2384-2397},
  shortjournal = {IET Image Process.},
  title        = {Adaptive aggregation with self-attention network for gastrointestinal image classification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CFN: A coarse-to-fine network for eye fixation prediction.
<em>IETIP</em>, <em>16</em>(9), 2373–2383. (<a
href="https://doi.org/10.1049/ipr2.12494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many image-to-image computer vision approaches have made great progress by an end-to-end framework with the encoder–decoder architecture. However, the same image-to-image eye fixation prediction task is not the same as those computer vision tasks in that it focuses more on salient regions rather than precise predictions for every pixel. Thus, it is not appropriate to directly apply the end-to-end encoder–decoder to the eye fixation prediction task. In addition, although high-level feature is important, the contribution of low-level feature should also be kept and balanced in computational model. Nevertheless, some low-level features that attract attention are easily neglected while transiting through the deep network. Therefore, the effective way to integrate low-level and high-level features for improving eye fixation prediction performance is still a challenging task. In this paper, a coarse-to-fine network (CFN) that encompasses two pathways with different training strategies are proposed: coarse perceiving network (CFN-Coarse) can be a simple encoder network or any of the existing pretrained network to capture the distribution of salient regions and generate high-quality feature maps; fine integrating network (CFN-Fine) uses fixed parameters from the CFN-Coarse and combines features from deep to shallow in the deconvolution process by adding skip connections between down-sampling and up-sampling paths to efficiently integrate deep and shallow features. The saliency map obtained by the method is evaluated over 6 standard benchmark datasets, namely SALICON, MIT1003, MIT300, Toronto, OSIE, and SUN500. The results demonstrate that the method can surpass the state-of-the-art accuracy of eye fixation prediction and achieves the competitive performance to date under most evaluation metrics on SALICON Saliency Prediction Challenge (LSUN2017).},
  archive      = {J_IETIP},
  author       = {Binwei Xu and Haoran Liang and Ronghua Liang and Peng Chen},
  doi          = {10.1049/ipr2.12494},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2373-2383},
  shortjournal = {IET Image Process.},
  title        = {CFN: A coarse-to-fine network for eye fixation prediction},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Open-set iris recognition based on deep learning.
<em>IETIP</em>, <em>16</em>(9), 2361–2372. (<a
href="https://doi.org/10.1049/ipr2.12493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing iris recognition methods offer excellent recognition performance for known classes, but they do not consider the rejection of unknown classes. It is important to reject an unknown object class for a reliable iris recognition system. This study proposes open-set iris recognition based on deep learning. In the method, by training the deep network, the extracted iris features are clustered near the feature centre of each kind of iris image. Then, the authors build an open-class features outlier network (OCFON) containing distance features, which maps the features extracted by the deep network to a new feature space and classifies them. Finally, the unknown class samples are determined by a SoftMax probability threshold. The authors conducted experiments on the open iris dataset constructed using the iris datasets CASIA-Iris-Twins and CASIA-Iris-Lamp. The experiment shows that the proposed method has good open-set iris recognition performance, can effectively distinguish iris samples of unknown classes, and has little impact on the recognition ability of known classes of iris samples.},
  archive      = {J_IETIP},
  author       = {Jie Sun and Shipeng Zhao and Sheng Miao and Xuan Wang and Yanan Yu},
  doi          = {10.1049/ipr2.12493},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2361-2372},
  shortjournal = {IET Image Process.},
  title        = {Open-set iris recognition based on deep learning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Abnormal area identification of corn ear based on
semi-supervised learning. <em>IETIP</em>, <em>16</em>(9), 2351–2360. (<a
href="https://doi.org/10.1049/ipr2.12492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Screening corn ear is the key link in the breeding process of new varieties. But manual testing is difficult to measure the proportion of abnormal area. Meanwhile, the abnormal areas are mainly caused by mildew, moth and mechanical collision. A new refined semantic segmentation model was proposed based on the semi-supervised learning method of generating antagonistic networks (GAN). Besides k -means algorithm was used to remove a large amount of background information for data preprocessing. By using feature fusion and weighted loss function the model performance was improved. The introduction of transfer learning accelerated model convergence. Through the high-throughput corn ear collection system, 1448 ear images (including abnormal conditions such as mildew, moth and mechanical damage) were collected and labelled. The proposed method was tested on real corn ear images with an accuracy of 0.950, mean precision of 0.933, mean IoU of 0.884, and FwIoU of 0.908. Experimental results show that the proposed method has better performance than general networks.},
  archive      = {J_IETIP},
  author       = {Jian Wei and Qin Ma and Weitao Wang and Hao Guo and Zhe Liu and Jiajing Zhang},
  doi          = {10.1049/ipr2.12492},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2351-2360},
  shortjournal = {IET Image Process.},
  title        = {Abnormal area identification of corn ear based on semi-supervised learning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-step implicit adams predictor-corrector network for
fire detection. <em>IETIP</em>, <em>16</em>(9), 2338–2350. (<a
href="https://doi.org/10.1049/ipr2.12491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fire detection methods based on the Convolutional Neural Networks (CNN) have advantages of high accuracy, wide coverage and robustness, receiving significant attention from researchers. Among CNN-based methods, ResNet has achieved better performance than other CNN frameworks in fire detection system, since it uses stacked residual blocks to enlarge the receptive field to overcome the vanishing gradient problem with residual learning. The merits of ResNet can be attributed to the similarity between ResNet and the single-step explicit solver for Ordinary Differential Equations (ODEs), for example, the Euler method. Motivated by the theory of numerical ODE that a multi-step implicit solver has higher accuracy than a single-step explicit solver, the Multi-step Implicit Adams predictor-corrector (MIAPC) network for fire detection is proposed. The MIAPC method is first mapped to a corresponding predictor-corrector Adams block which achieves higher accuracy than a single-step explicit solver. Then, Adaptive Feature Fusion (AFF) and the Spatial Attention Layer (SAL) are utilized to extract hierarchical features from stacked predictor-corrector Adams blocks, forming the corresponding Adams module. Finally, the 4 Adams modules which are made of 4, 6, 8, 10 predictor-corrector Adams blocks and followed by AFF and SAL form the crucial ODE-based approximation part in the proposed network. By adding a simple feature extraction and detection in front of and after the ODE-based approximation part, the MIAPC network is built. Experiments demonstrate that the method achieves 87% accuracy in the challenging test dataset, outperforming existing methods by at least 6%. Besides, the 5.3M model size with inference speed of 4.7 frames/second in CPU and 65.7 frames/second in GPU enables the proposed method to be used in practical applications.},
  archive      = {J_IETIP},
  author       = {Zhen Deng and Shuhao Hu and Shibai Yin and Yibin Wang and Anup Basu and Irene Cheng},
  doi          = {10.1049/ipr2.12491},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2338-2350},
  shortjournal = {IET Image Process.},
  title        = {Multi-step implicit adams predictor-corrector network for fire detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Salp swarm algorithm based on golden section and adaptive
and its application in target tracking. <em>IETIP</em>, <em>16</em>(9),
2321–2337. (<a href="https://doi.org/10.1049/ipr2.12490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve the problem that the conventional tracker is not adapted to the abrupt motion, a tracking algorithm based on the improved salp swarm algorithm (ISSA) was proposed. Visual tracking is considered to be a process of locating the optimal position through the interaction between leaders and followers in successive images. Firstly, the adaptive mechanism of leader and follower is introduced into the original salp swarm algorithm (SSA) to balance the exploitation and exploration of the algorithm. This method can improve the accuracy and effect of tracking. Secondly, the golden-sine algorithm was used to update the position of followers, considering that the SSA had a single spatial search mode for followers and was easy to fall into the local optimum. By comparing with 19 classical tracking algorithms, qualitative and quantitative analysis is carried out to verify the tracking effect of the proposed method. A large number of experimental results show that the algorithm proposed here has good performance in visual tracking, especially for mutation motion tracking.},
  archive      = {J_IETIP},
  author       = {Zhimin Guo and Yangyang Tian and Yuxing Feng and Huanlong Zhang and Junfeng Liu and Zanfeng Wang},
  doi          = {10.1049/ipr2.12490},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2321-2337},
  shortjournal = {IET Image Process.},
  title        = {Salp swarm algorithm based on golden section and adaptive and its application in target tracking},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FPNFuse: A lightweight feature pyramid network for infrared
and visible image fusion. <em>IETIP</em>, <em>16</em>(9), 2308–2320. (<a
href="https://doi.org/10.1049/ipr2.12473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel deep learning structure of infrared and visible image fusion is proposed. In particular, feature pyramid networks are developed for enhanced feature extraction across multiple convolutional layers. Besides, a fusion strategy is improved based on the channel attention mechanism to highlight the relevant attributes in the fusion stage. The fusion method consists of four parts: encoder, feature pyramid networks, fusion strategy and decoder, respectively. First, the multi-scale deep features are extracted from the source images by encoder with embedded feature pyramid networks, realizing cross-layer interaction. Second, these features are fused by the improved fusion strategy with channel attention for each scale. Finally, the fused features are reconstructed by the designed decoder to produce the informative fused image. The experimental results show that the proposed fusion method achieves state-of-the-art results in both qualitative and quantitative evaluation with a lightweight architecture.},
  archive      = {J_IETIP},
  author       = {Zi-Han Zhang and Xiao-Jun Wu and Tianyang Xu},
  doi          = {10.1049/ipr2.12473},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2308-2320},
  shortjournal = {IET Image Process.},
  title        = {FPNFuse: A lightweight feature pyramid network for infrared and visible image fusion},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pansharpening scheme using spatial detail injection–based
convolutional neural networks. <em>IETIP</em>, <em>16</em>(9),
2297–2307. (<a href="https://doi.org/10.1049/ipr2.12384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pansharpening produces a high spatial-spectral resolution pansharpened image by combining multispectral (MS) and panchromatic (PAN) images. In the traditional multi-resolution analysis (MRA) method, detailed PAN images are extracted by transformation methods that are injected into MS images. This gives spatial and spectral distortions in the pansharpened image. These distortions can be reduced in the pansharpened image by the correct matching of the PAN detail image component. This correct matching is possible by the convolutional neural network (CNN)–based models. This paper obtains the detailed image component using the CNN models. This CNN model extracts the PAN detail image that is suitable for the MRA-based pansharpening scheme which significantly reduces the spatial and spectral distortions. It is demonstrated by qualitative and quantitative analysis applied on GeoEye-1 and IKONOS satellite images and shows the effectiveness of the proposed scheme.},
  archive      = {J_IETIP},
  author       = {Nidhi Saxena and Gaurav Saxena and Neelu Khare and Md Habibur Rahman},
  doi          = {10.1049/ipr2.12384},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2297-2307},
  shortjournal = {IET Image Process.},
  title        = {Pansharpening scheme using spatial detail injection–based convolutional neural networks},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new hough transform operated in a bounded cartesian
coordinate parameter space. <em>IETIP</em>, <em>16</em>(8), 2282–2295.
(<a href="https://doi.org/10.1049/ipr2.12489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new Hough transform is proposed to detect lines with geometric meanings. In the standard Hough transform, lines are parameterized by the length and orientation of the normal vector from the origin to the line. The parameter space of the standard Hough transform is presented in a polar coordinate system. Geometric measurements such as distance and angle are not suitable in polar coordinate space due to the inconsistency of the axes. To deal with this problem, a Hough transform which is carried out in a bounded Cartesian coordinate parameter space is developed. The lines are represented by their perpendicular feet with respect to a fixed point. Since the parameter space is in Cartesian coordinate space, it is intuitive to the researchers. When detecting the peaks in the parameter space, distance and angle constraints can be applied. The effectiveness of the proposed method has been illustrated by two applications, that is, lane line detection and convex polygon extraction. The results show that the proposed Cartesian coordinate Hough transform is suitable for detecting meaningful lines.},
  archive      = {J_IETIP},
  author       = {Gen Yang and Junping Hu and Zhicheng Hou and Gong Zhang and Weijun Wang},
  doi          = {10.1049/ipr2.12489},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2282-2295},
  shortjournal = {IET Image Process.},
  title        = {A new hough transform operated in a bounded cartesian coordinate parameter space},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vision-oriented algorithm for fast decision in 3D video
coding. <em>IETIP</em>, <em>16</em>(8), 2263–2281. (<a
href="https://doi.org/10.1049/ipr2.12488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper designs a novel method to reduce the coding complexity of 3D-HEVC encoder by utilizing the properties of human visual perception. Two vision-oriented edge detections are proposed: for colour texture detection, the authors adopt the Just-Noticeable Distortion (JND); for depth map, the authors combine the Sample Adaptive Offset (SAO) and the Just Noticeable Depth Difference (JNDD) model. The authors also analyse the properties of colour texture and depth map to classify the coding tree unit (CTU) into various kinds of types, including complex-edge CTU, moderate-edge CTU and homogeneous CTU. Besides, fast mode decisions and early termination criteria are performed individually on each type of CTUs according to their characteristics. Especially for those CTUs with more edge information, the proposed projection-based fast mode decision and residual-based early termination preserve important colour texture while speeding up the coding at the same time. The proposed vision-oriented algorithm reduces 31.981% of the overall average coding time with only 1.580% BD-Bitrate increase. Experimental results show that the proposed algorithm can provide considerable time-saving while still maintain the video quality, which outperforms the previous researches.},
  archive      = {J_IETIP},
  author       = {Jie-Ru Lin and Mei-Juan Chen and Chia-Hung Yeh and Shinfeng D. Lin and Kuen-Liang Sue and Lih-Jen Kau and Yi-Sheng Ciou},
  doi          = {10.1049/ipr2.12488},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2263-2281},
  shortjournal = {IET Image Process.},
  title        = {Vision-oriented algorithm for fast decision in 3D video coding},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Taming mode collapse in generative adversarial networks
using cooperative realness discriminators. <em>IETIP</em>,
<em>16</em>(8), 2240–2262. (<a
href="https://doi.org/10.1049/ipr2.12487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks (GANs) are able to produce realistic images. However, GANs may suffer mode collapse in their output data distribution. Here, we theoretically and empirically justify generalizing the GAN framework to multiple discriminators with one generator for improving generative performance. First, a comprehensive perspective is adopted to understand why mode collapse occurs. Second, an array of cooperative realness discriminators is introduced into the GAN framework to combat mode collapse and explore discriminator roles ranging from a formidable adversary to a forgiving teacher. Third, two types of simple yet effective regularization are proposed for generating realistic and diverse images. Experiments on various datasets show the effectiveness of the GAN compared to previous methods in alleviating mode collapse and improving the quality of the generated samples.},
  archive      = {J_IETIP},
  author       = {Jinzhen Mu and Chunyan Chen and Wenshan Zhu and Shuang Li and Yan Zhou},
  doi          = {10.1049/ipr2.12487},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2240-2262},
  shortjournal = {IET Image Process.},
  title        = {Taming mode collapse in generative adversarial networks using cooperative realness discriminators},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image-level dataset synthesis with an end-to-end trainable
framework. <em>IETIP</em>, <em>16</em>(8), 2228–2239. (<a
href="https://doi.org/10.1049/ipr2.12486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dataset synthesis via virtual engines like Unity is attracting much more attention in recent years due to its low cost at obtaining ground-truth labels. For this kind of work, virtual environments are constructed within the engine to mimic the real-world, either with great manual efforts or learning-based methods. The latter shows superiority over the former when the target real-world scenes are changeable, from which the attributes of environments can be automatically adjusted based on the distribution difference between the synthetic and real-world datasets. However, the non-differentiability of whole pipeline hinders the efficiency of attribute optimization. To this end, this paper proposes to simulate synthetic datasets from a fine-grained perspective, such that the system can be trained at an end-to-end manner. Specifically, it is converted into an image-level data synthesis problem, and designs a constraint using the content loss between two images. As the rendering process of virtual engine is mathematically unknown, which blocks the back propagation of the gradients, a generative model is trained to approximate the engine. As a result, the whole framework becomes fully differentiable and the attributes can be optimized efficiently by gradient descent. Experimental result shows the efficiency of our method in obtaining useful synthetic training datasets. Besides, it is found that the image-level method enables to learn the potential distribution of real-world data, which is hard to be achieved by existing methods. As far as we know, it is the first attempt to finish this task with a differentiable process.},
  archive      = {J_IETIP},
  author       = {Zhenfeng Xue and Weijie Mao and Yong Liu},
  doi          = {10.1049/ipr2.12486},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2228-2239},
  shortjournal = {IET Image Process.},
  title        = {Image-level dataset synthesis with an end-to-end trainable framework},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Re-EnGAN: Unsupervised image-to-image translation based on
reused feature encoder in CycleGAN. <em>IETIP</em>, <em>16</em>(8),
2219–2227. (<a href="https://doi.org/10.1049/ipr2.12485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an advanced task for image synthesis without labelled data, unsupervised image-to-image translation refers to the overall conversion of a certain characteristic image domain X to another domain Y. The key point is to learn a mapping relationship that can be transformed between different image domains. Existing methods mainly adopt GANs to generate authentic images. While the discriminators will be abandoned after the training process is completed. In order to avoid this waste of training resources, a feature encoder reusing method is proposed, which could reduce the number of parameters and accelerate the training speed. In addition, we add the adaptive perceptual loss for the purpose of paying more attention to the quality of generated images. This loss directly uses the encoder during training to perform feature-level constraints, and applies the L1-norm on the intermediate feature layer of the generated samples. The experiments illustrate that our framework can generate more natural images and provide an effective solution for unsupervised translation.},
  archive      = {J_IETIP},
  author       = {Yu Lu and Ju Liu and Lin Lv and Xuesong Gao and Weiqiang Chen and Yuyi Zhang},
  doi          = {10.1049/ipr2.12485},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2219-2227},
  shortjournal = {IET Image Process.},
  title        = {Re-EnGAN: Unsupervised image-to-image translation based on reused feature encoder in CycleGAN},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A weighted multi-source domain adaptation approach for
surface defect detection. <em>IETIP</em>, <em>16</em>(8), 2210–2218. (<a
href="https://doi.org/10.1049/ipr2.12484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a weighted multi-source domain adaptation (MSDA) method for industrial surface defect detection. The domain adaptation method is usually used to solve inaccurate results due to lacking target training samples. But in the scene of surface defects detection, this method is not adequate because the inspection samples often contain complex texture features. To get better performance, in this paper, we extend the single-source domain adaptation detection method to the multi-source domain. At the same time, we weighted different source domains samples during the adaptive training process, and prioritize the alignment of the target domain with the most similar source domain. The experimental results show that our proposed method performs well on the target dataset which improves existing methods&#39; limitations for detecting surface defects with complex textures.},
  archive      = {J_IETIP},
  author       = {Bing Hu and Jianhui Wang},
  doi          = {10.1049/ipr2.12484},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2210-2218},
  shortjournal = {IET Image Process.},
  title        = {A weighted multi-source domain adaptation approach for surface defect detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A GPU-free real-time object detection method for apron
surveillance video based on quantized MobileNet-SSD. <em>IETIP</em>,
<em>16</em>(8), 2196–2209. (<a
href="https://doi.org/10.1049/ipr2.12483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection from apron surveillance video is facing enormous storage pressure and computing overhead. Large cloud server cluster is generally used and high-speed network bandwidth is required, also equipped with powerful GPUs for computing support. The design of hardware-friendly and efficient object detection model is challenging. This paper presents a compression method for outdoor apron surveillance videos, which is further combined with a lightweight detection model to make the inference process independent of GPU. First, the gray level variance of dynamic objects is leveraged to binarize the monitoring images, then an improved MobileNet-SSD algorithm is proposed. Moreover, int8 quantization is performed and bit operations are designed to eliminate the floating-point operation and it can simultaneously accelerate and compress CNN models with only minor performance degradation. Experiment results on a large-scale dataset containing 22k monitoring images demonstrate that the compression ratio of quantized image can achieve up to 21 times, combined with quantized model, the detection on apron surveillance images can reach nearly 25FPS in a pure CPU environment, the mAP is 86.83%, and the model size is compressed to 600 kb. Significantly reduced computational complexity can be applied to embedded devices.},
  archive      = {J_IETIP},
  author       = {Zonglei Lyu and Dan Zhang and Jia Luo},
  doi          = {10.1049/ipr2.12483},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2196-2209},
  shortjournal = {IET Image Process.},
  title        = {A GPU-free real-time object detection method for apron surveillance video based on quantized MobileNet-SSD},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic geo-localization framework without GNSS data.
<em>IETIP</em>, <em>16</em>(8), 2180–2195. (<a
href="https://doi.org/10.1049/ipr2.12482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The information unavailability of a global navigation satellite system (GNSS) for geo-localization in many military and tourism applications implies the need to employ other applicable types of information, such as automatic image-based location recognition. Unlike most existing localization methods that use GNSS information to locate an initial position, this paper proposes an automatic geo-localization framework that does not require GNSS data. The proposed framework is a two-stage pipeline that uses a query image and digital elevation model (DEM) data as input. The authors frame automatic geo-localization without GNSS recordings as a skyline matching problem. By extracting the skyline from the DEM data and query images, the query image can be localized by matching the query skyline feature to the DEM skyline database. It has been demonstrated that this low-cost approach can perform efficiently in mountainous or hilly areas to produce reliable localization results. The system was tested on 50 testing site points within a large-scale area (China, 202.6 km 2 ), and an average position error of 43.13 m was detected within 4.5 s.},
  archive      = {J_IETIP},
  author       = {Jin Tang and Cheng Gong and Fan Guo and Zirong Yang and Zhihu Wu},
  doi          = {10.1049/ipr2.12482},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2180-2195},
  shortjournal = {IET Image Process.},
  title        = {Automatic geo-localization framework without GNSS data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Experimental protocol designed to employ nd: YAG laser
surgery for anterior chamber glaucoma detection via UBM. <em>IETIP</em>,
<em>16</em>(8), 2171–2179. (<a
href="https://doi.org/10.1049/ipr2.12481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Angle closure glaucoma leads to fluid deposition in eye, and intraocular pressure occurs that damage the optic nerve, causes blindness and vision loss. Anterior chamber (AC) evaluation is imperative for determining the risk of angle-closure. Previously, techniques were dependent on either Pentacam–Scheimpflug that interprets poor visual information, anterior segment optical coherence tomography is injurious to intercede opaque optical structures. Therefore, in this paper, an experimental protocol is designed for detailed disease analysis based on IBM SPSS statistics via ultrasound biomicroscopy which is superior in evaluating deep structures; first, the affected parameter for AC is analysed, and afterwards the direction that needs laser surgery is explored. Experiments are conducted on large-scale clinical studies from an affiliated hospital in Shanghai, China. The dataset comprised 600 AC images in five directions of 60 subjects. The mean with standard deviation for anterior open distance is 0.15879 0.096779 mm, 0.15863 0.081435 mm, and anterior chamber angle is 18.749 08.0315 , 18.741 08.3889 for left and right eye respectively. It is found that anterior chamber angle in the downside of the AC is wider than the upside. However, this decision is partly based on the narrowest part of the angle to widen the depth of the direction and eliminate pupil block.},
  archive      = {J_IETIP},
  author       = {Saba Ghazanfar Ali and Riaz Ali and Bin Sheng and Yan Chen and Huating Li and Po Yang and Ping Li and Younhyun Jung and Fang Zhu and Ping Lu and Jinman Kim},
  doi          = {10.1049/ipr2.12481},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2171-2179},
  shortjournal = {IET Image Process.},
  title        = {Experimental protocol designed to employ nd: YAG laser surgery for anterior chamber glaucoma detection via UBM},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual-attention guided network for facial action unit
detection. <em>IETIP</em>, <em>16</em>(8), 2157–2170. (<a
href="https://doi.org/10.1049/ipr2.12480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention mechanism has recently aroused increasing concerns in the field of computer vision like Action Unit (AU) detection. Because facial AU exists in a fixed local area of a human face, it is advantageous to apply the attention mechanism to AU detection. A Dual-Attention Guided Network (DAGNet) is proposed for automatically AU detection, which introduces dual attention to selectively extract deep features. Dual attention refers to predefined explicitly models feature dependencies from spatial and channel attention, respectively, based on the semantics of AU label. In addition, since the global and local features show different facial attributes and supplement mutually, the proposed DAGNet learns feature representations from global and local perspectives, respectively. Learning global and local features simultaneously during training can lead to better generalization performance; a fusion module is designed for aggregating all the learned information to construct a unified architecture for end-to-end AU detection. Extensive experiments on two challenging datasets, BP4D and DISFA, result in F1-scores of 64.0% and 62.6%, respectively, which shows that the proposed DAGNet achieves the performance of the state-of-the-art in the field of image-based AU detection.},
  archive      = {J_IETIP},
  author       = {Wenyu Song and Shuze Shi and Yuxuan Wu and Gaoyun An},
  doi          = {10.1049/ipr2.12480},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2157-2170},
  shortjournal = {IET Image Process.},
  title        = {Dual-attention guided network for facial action unit detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A single-stage face detection and face recognition deep
neural network based on feature pyramid and triplet loss.
<em>IETIP</em>, <em>16</em>(8), 2148–2156. (<a
href="https://doi.org/10.1049/ipr2.12479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A practical deep learning face recognition system can be divided into several tasks. These tasks can be time-consuming if each task is executed with the original image as the input data. And the feature extractors used by different tasks may duplicate its function. In this paper, a multi-task training method based on feature pyramid and triplet loss to train a single-stage face detection and face recognition deep neural network is proposed. As a single-stage work, every task&#39;s data is passed through the same backbone network to avoid duplicate computation by sharing the weights and computation. The whole network is established using feature pyramid and anchor boxes to localise the face position, using triplet loss to establish the feature extractor, and finally matching the feature through a simple math function. The benefits of the approach are faster computation speed and less memory usage. On an Nvidia 2080Ti GPU accelerator, this system can achieve 212 FPS for a 640 × 640 resolution input and maintains 92.4% accuracy on the LFW data set.},
  archive      = {J_IETIP},
  author       = {Tsung-Han Tsai and Po-Ting Chi},
  doi          = {10.1049/ipr2.12479},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2148-2156},
  shortjournal = {IET Image Process.},
  title        = {A single-stage face detection and face recognition deep neural network based on feature pyramid and triplet loss},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Blind denoising using dense hybrid convolutional network.
<em>IETIP</em>, <em>16</em>(8), 2133–2147. (<a
href="https://doi.org/10.1049/ipr2.12478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of existing deep convolutional networks is limited when encountering images with different noise levels. In this study, a denoising method with state-of-the-art performance that combines a deep convolutional network with the traditional nonlocal mean denoising method is proposed. The noisy image is first denoised using the nonlocal mean method. Then, the denoised image is input into the proposed dense hybrid convolutional network to be trained, producing a clean image with clear details. The dense hybrid convolutional network comprises three parts: a feature-extracting noise-suppressing module that extracts abstract features from denoised images and suppresses the residual noise by interval convolution; a feature-learning module used for training blurred edges and textures; and a magnifying module that uses deconvolution to restore the feature maps to the original size and reduce the noise again. In contrast to existing denoising algorithms, the method has two desirable properties: 1) it can restore edges and textures clearly while removing the noise; 2) it effectively deals with noise of unknown levels (i.e. blind denoising) with a single network model. The conducted experiments show that the proposed method achieves superior performance compared to those of state-of-the-art denoising methods.},
  archive      = {J_IETIP},
  author       = {Jing Liu and Runchuan Liu and Shanshan Zhao},
  doi          = {10.1049/ipr2.12478},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2133-2147},
  shortjournal = {IET Image Process.},
  title        = {Blind denoising using dense hybrid convolutional network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gesture recognition based on modified yolov5s.
<em>IETIP</em>, <em>16</em>(8), 2124–2132. (<a
href="https://doi.org/10.1049/ipr2.12477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of artificial intelligence technology, human–computer interaction technology through gestures, images and voices has gradually become a hot topic for discussion. A modified Yolov5s gesture recognition method is proposed in the field of human–computer cooperation by optimizing the network structure of Yolov5s backbone, CNN is replaced by Ghostbottleneck module to increase the target occlusion recognition rate. Secondly, tensor stitching is added to the output of Ghostbottleneck module for up sampling to strengthen the reuse of image features. Finally, the detection ability of the improved model in the face of complex environment is verified on the self-made data set. Experimental results show that, the mAP@0.5 (mean average precision) of the modified Yolov5s is 94.49%, the AP (average precision) is 94.2%. By comparing the Yolov5s algorithm, Yolov4 algorithm,Yolov3 algorithm and SSD algorithm, the detection accuracy of the modified method has been significantly improved, which can fully meet the application requirements of real-time detection of gesture-controlled robots.},
  archive      = {J_IETIP},
  author       = {Dunli Hu and Jun Zhu and Jiayu Liu and Jiaju Wang and Xiaoping Zhang},
  doi          = {10.1049/ipr2.12477},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2124-2132},
  shortjournal = {IET Image Process.},
  title        = {Gesture recognition based on modified yolov5s},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-instance inflated 3D CNN for classifying urine red
blood cells from multi-focus videos. <em>IETIP</em>, <em>16</em>(8),
2114–2123. (<a href="https://doi.org/10.1049/ipr2.12476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifying urine red blood cells (U-RBCs) is the core operation in diagnosing urinary system diseases (USDs). In this paper, based on a novel data type named multi-focus video, a multi-instance inflated 3D convolutional neural network (MI3D) is proposed. In order to accurately classifying U-RBCs, the MI3D integrates inflated inception-V1 with multi-instance learning models. Compared with the existent U-RBC classification methods relying on single focus images, the MI3D using multi-focus videos effectively avoids the misclassification caused by the significant deformation of U-RBCs with the focus of microscope changing. In addition, the MI3D can learn the typical shapes and deformation patterns of U-RBCs from multi-focal videos simultaneously. Therefore, the accuracy of MI3D exceeds the mainstream video classification models. There are totally 597 multi-focus videos that include four types of U-RBCs collected to verify the effectiveness of MI3D. Experimental results show that the classification accuracy of MI3D is inspiring with 94.4%, which is obviously higher than that of existed U-RBC classification method (85.6%). The accuracy of MI3D also achieves the comparable level with the results by junior microscopist (95.6%). Lastly, the MI3D has powerful real-time performance, whose classification speed reaches 1.4 times than that of the microscopist.},
  archive      = {J_IETIP},
  author       = {Xinyu Li and Ming Li and Yongfei Wu and Xiaoshuang Zhou and Lifeng Zhang and Xinbo Ping and Xingna Zhang and Wen Zheng},
  doi          = {10.1049/ipr2.12476},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2114-2123},
  shortjournal = {IET Image Process.},
  title        = {Multi-instance inflated 3D CNN for classifying urine red blood cells from multi-focus videos},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A COVID-19 CXR image recognition method based on
MSA-DDCovidNet. <em>IETIP</em>, <em>16</em>(8), 2101–2113. (<a
href="https://doi.org/10.1049/ipr2.12474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, coronavirus disease 2019 (COVID-19) has not been contained. It is a safe and effective way to detect infected persons in chest X-ray (CXR) images based on deep learning methods. To solve the above problem, the dual-path multi-scale fusion (DMFF) module and dense dilated depth-wise separable (D3S) module are used to extract shallow and deep features, respectively. Based on these two modules and multi-scale spatial attention (MSA) mechanism, a lightweight convolutional neural network model, MSA-DDCovidNet, is designed. Experimental results show that the accuracy of the MSA-DDCovidNet model on COVID-19 CXR images is as high as 97.962%, In addition, the proposed MSA-DDCovidNet has less computation complexity and fewer parameter numbers. Compared with other methods, MSA-DDCovidNet can help diagnose COVID-19 more quickly and accurately.},
  archive      = {J_IETIP},
  author       = {Wei Wang and Wendi Huang and Xin Wang and Peng Zhang and Nian Zhang},
  doi          = {10.1049/ipr2.12474},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2101-2113},
  shortjournal = {IET Image Process.},
  title        = {A COVID-19 CXR image recognition method based on MSA-DDCovidNet},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). End-to-end tangut character database building and
recognition method. <em>IETIP</em>, <em>16</em>(8), 2087–2100. (<a
href="https://doi.org/10.1049/ipr2.12471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Character recognition is an important research topic nowadays, and a large amount of excellent work has appeared. In contrast, research related to the recognition of Tangut characters is still in the initial stage. Creating databases and effective recognition methods that can support the recognition of Tangut characters remain a great challenge. In this paper, a labeling method based on Multi-Model and Multi-Prediction (MMMP) is proposed, which built a Tangut character database (TCD) and an enhanced database (called “TCD-E”) covering 6077 classes, and five test sets were also built for specific tasks. To recognize Tangut characters effectively and quickly, a 5-layer end-to-end Tangut Characters Recognition Network (TCRNet) based on CNN using shallow neural networks is designed. Its recognition accuracy on TCD-E reaches 97.96 . Based on TCRNet, an end-to-end Similar Tangut Characters Recognition Network (STCRNet) is further proposed by improving the loss function by combining the softmax loss function with the central loss function, and its test accuracy on similar Tangut characters test set (called “TCD-E-S”) is 0.70 higher than TCRNet. Experiments show that TCD and TCD-E can provide data support for Tangut character recognition. The recognition accuracy of TCRNet and STCRNet surpasses the previous best results.},
  archive      = {J_IETIP},
  author       = {Jinlin Ma and Yunrui Cao and Ziping Ma and Lin Wei and Chaohua Hao},
  doi          = {10.1049/ipr2.12471},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2087-2100},
  shortjournal = {IET Image Process.},
  title        = {End-to-end tangut character database building and recognition method},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variational joint self-attention for image captioning.
<em>IETIP</em>, <em>16</em>(8), 2075–2086. (<a
href="https://doi.org/10.1049/ipr2.12470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image captioning task has attracted great attention from many researchers, and significant progress has been made in the past few years. Existing image captioning models, which mainly apply attention-based encoder-decoder architecture, achieve great developments image captioning. These attention-based models, however, are limited in the caption generation due to the potential errors resulting from the inaccurate detection of objects and incorrect attention to the objects. To alleviate the limitation, a Variational Joint Self-Attention model (VJSA) is proposed to learn a latent semantic alignment between the given image and its label description for guiding better image captioning. Unlike the existing image captioning models, VJSA first uses a self-attention module to encode the effective relationship information of intra-sequence and inter-sequences relationships. And then the variational neural inference module learns a distribution over the latent semantic alignment between the image and its corresponding description. In the decoding, the learned semantic alignment guides the decoder to generate the higher quality image caption. The results of the experiments reveal that the VJSA outperforms the compared models, and the performances of various metrics show that the proposed model is effective and feasible in image caption generation.},
  archive      = {J_IETIP},
  author       = {Xiangjun Shao and Zhenglong Xiang and Yuanxiang Li and Mingjie Zhang},
  doi          = {10.1049/ipr2.12470},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2075-2086},
  shortjournal = {IET Image Process.},
  title        = {Variational joint self-attention for image captioning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-stage part-aware graph convolutional network for
skeleton-based action recognition. <em>IETIP</em>, <em>16</em>(8),
2063–2074. (<a href="https://doi.org/10.1049/ipr2.12469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, graph convolutional networks have shown excellent results in skeleton-based action recognition. This paper presents a multi-stage part-aware graph convolutional network for the problems of model over complication, parameter redundancy and lack of long-dependence feature information. The structure of this network has a multi-stream input and two-stream output, which can greatly reduce the complexity and improve the accuracy of the model without losing sequence information. The two branches of the network have the same backbone, which includes 6 multi-order feature extraction blocks and 3 temporal attention calibration blocks, and the outputs of the two branches are fused together. In multi-order feature extraction block, a channel-spatial attention mechanism and a graph condensation module are proposed, which can extract more distinguishable feature and identify the relationship between parts. In temporal attention calibration block, the temporal dependencies between frames in the skeleton sequence are modeled. Experimental results show that the proposed network outperforms many mainstream methods on NTU and Kinetics datasets, for example, it achieves 92.4% accuracy on the cross-subject benchmark of NTU-RGBD60 dataset.},
  archive      = {J_IETIP},
  author       = {Xiaofei Qin and Hao Li and Yuru Liu and Jiabin Yu and Changxiang He and Xuedian Zhang},
  doi          = {10.1049/ipr2.12469},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2063-2074},
  shortjournal = {IET Image Process.},
  title        = {Multi-stage part-aware graph convolutional network for skeleton-based action recognition},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-scale single image dehazing based on the fusion of
global and local features. <em>IETIP</em>, <em>16</em>(8), 2049–2062.
(<a href="https://doi.org/10.1049/ipr2.12467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problems of incomplete dehazing of a single image and unnaturalness of the restored image, a multi-scale single-image defogging network with local features fused with global features is proposed, using fog and non-fogging image pairs train the network in a direct end-to-end manner. The network is divided into global feature extraction module, multi-scale feature extraction module and deep fusion module. The global feature extraction module extracts global features that characterize the contour; multi-scale feature extraction module extracts features at different scales to improve learning accuracy; in the deep fusion module, the convolutional layer extracts the local features that describe the image content, and then the local features and the global features are merged through skip connections. Comparative experiments were carried out on artificially synthesized fog images and real fog images. The experimental results show that the algorithm proposed here can achieve the ideal dehazing effect, and is superior to other comparison algorithms in subjective and objective aspects.},
  archive      = {J_IETIP},
  author       = {Ziyu Chen and Huaiyu Zhuang and Jia Han and Yani Cui and Jiaxian Deng},
  doi          = {10.1049/ipr2.12467},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2049-2062},
  shortjournal = {IET Image Process.},
  title        = {Multi-scale single image dehazing based on the fusion of global and local features},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image classification using convolutional neural network with
wavelet domain inputs. <em>IETIP</em>, <em>16</em>(8), 2037–2048. (<a
href="https://doi.org/10.1049/ipr2.12466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commonly used convolutional neural networks (CNNs) usually compress high-resolution input images. Although it reduces the computation requirements into a reasonable range, the downsampling operation causes information loss, which affects the accuracy of image classification. How to adopt high-resolution image inputs to improve the quality of input information and thus improve the classification accuracy without changing the overall structure of the pre-defined CNN model or increasing the model parameters is an important issue. Here, a CNN model with wavelet domain inputs is proposed to provide a solving scheme. Specifically, the proposed method applies wavelet packet transform or dual-tree complex wavelet transform to extract information from input images with higher resolutions in the image pre-processing stage. Some subband image channels are selected as the inputs of conventional CNNs where the first several convolutional layers are removed, so that the networks directly learn in the wavelet domain. Experiment results on the Caltech-256 dataset and the Describable Textures Dataset with the ResNet-50 show that the classification accuracy of our method can have a maximum improvement of 2.15% and 10.26%, respectively. These validate the effectiveness of our proposed scheme. This code is publicly available at https://github.com/BeBeBerr/wavelet-cnn .},
  archive      = {J_IETIP},
  author       = {Luyuan Wang and Yankui Sun},
  doi          = {10.1049/ipr2.12466},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2037-2048},
  shortjournal = {IET Image Process.},
  title        = {Image classification using convolutional neural network with wavelet domain inputs},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Research on edge-guided disambiguation module for shadow
detection. <em>IETIP</em>, <em>16</em>(7), 2025–2036. (<a
href="https://doi.org/10.1049/ipr2.12457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In shadow detection area, deep learning-based methods have achieved outstanding performance. However, the potential ambiguity in shadow images is still the main reason for the poor performance of the algorithms. By analysing the relationships among shadow edges, shadow false-positive information, and shadow areas, an edge-guided disambiguation module for better shadow detection performance is proposed. The edge-guided disambiguation module consists two parts: 1) the shadow edge feature is used to guide the comprehensive shadow area feature generation of different resolution. 2) The shadow false-positive features is applied to reduce ambiguity in the comprehensive shadow area features. In this way, the missed detections is added in the first step and the false detections is reduced in the second step. More accurate detection results is obtained. The comprehensive experiments are proceeded on the three public shadow detection datasets: SBU, UCF, and ISTD. The experimental results demonstrated the effectiveness and robustness of the proposed method.},
  archive      = {J_IETIP},
  author       = {Ying Huang and Zhen Yang and Su Liu and Hao Li and Hongjun Zhu},
  doi          = {10.1049/ipr2.12457},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2025-2036},
  shortjournal = {IET Image Process.},
  title        = {Research on edge-guided disambiguation module for shadow detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised person re-identification by part-compensated
soft multi-label learning. <em>IETIP</em>, <em>16</em>(7), 2012–2024.
(<a href="https://doi.org/10.1049/ipr2.12468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing re-ID models are developed based on supervised learning, which relies on plenty of manually labelled pairwise data for training. However, in many practical applications, such deep re-ID models are not scalable because of lacking sufficient annotated data. To address this problem, the superiority of unsupervised person re-ID methods have been reported and become more popular recently. In this paper, an unsupervised framework based on part-compensated soft multi-label learning is presented, which aims to explore the potential label information for unlabelled persons through mining both global and part level visual features. This re-ID framework is mainly composed of global clustering, part clustering and multi-label assignment. During clustering process, the results of part-level clustering are compensated into global clustering, allowing the network to learn more detailed information of pedestrians. Meanwhile, fuzzy clustering is used to generate soft multi-labels instead of commonly used single-label clustering. To further optimize the model, a cross-domain excitation parameter is introduced to relieve the background interference arising in the inter-domain view intersection. The proposed approach is evaluated on three public datasets, including Market-1501, DukeMTMC-reID, and MSMT17. Extensive experiments are carried out and this network can outperform the state-of-the-art unsupervised re-ID models.},
  archive      = {J_IETIP},
  author       = {Weiping Yang and De Zhang},
  doi          = {10.1049/ipr2.12468},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2012-2024},
  shortjournal = {IET Image Process.},
  title        = {Unsupervised person re-identification by part-compensated soft multi-label learning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-scale feature combination for person
re-identification. <em>IETIP</em>, <em>16</em>(7), 2001–2011. (<a
href="https://doi.org/10.1049/ipr2.12465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) is an instance-level task of image retrieval, and its identification accuracy depends on the distinguishable features extracted from people. However, most identification methods based on deep learning only mechanically extract distinguishable features of person images, and some important details are frequently overlooked. For scenes with substantial background differences or occlusions, the Re-ID efficiency is not high and the network scalability is not good. Here, the authors propose a multi-scale feature combination network (MFC-Net) model that combines structural feature information with global comprehensive feature information of the person images through a convolution neural network that can effectively retain distinguishing character detail information. The authors also propose a Gaussian stochastic pooling layer to solve the defects of the pooling layer. For the problem of many network parameters and weak performance, the authors propose an attentive feature convolutions layer. The authors perform many comparative experiments on three benchmark datasets. The results prove that our MFC-Net model performs well in person Re-ID and that its identification accuracy is higher than that of other investigated models.},
  archive      = {J_IETIP},
  author       = {Bailiang Huang and Yan Piao and Hao Zhang and Yanfeng Tang},
  doi          = {10.1049/ipr2.12465},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2001-2011},
  shortjournal = {IET Image Process.},
  title        = {Multi-scale feature combination for person re-identification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An end-to-end perceptual enhancement method for UHD portrait
images. <em>IETIP</em>, <em>16</em>(7), 1988–2000. (<a
href="https://doi.org/10.1049/ipr2.12464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in equipment enable photographers to take ultra-high-definition (UHD) photos, and photo retouching enables them to create an impressive image by artistically adjusting and enhancing the brightness, color etc. However, such image enhancement is an artistic and challenging task, which requires a lot of relevant experience and technology. Thus, using an automated algorithm is an attractive option. In recent years, deep learning has made great progress in image processing, which motivated the authors to explore this application in UHD image enhancement task. In this paper, an end-to-end perceptual enhancement method for UHD portrait images is proposed. Since the artistry of images comes from human perception, including the perception of exposure, structure, color etc., an image preprocessing method is proposed which can help the network learn the brightness change from the original image to the target image more accurately. Then, a composite perceptual loss function is proposed that combines global, structural, and color losses. This loss allows the network to simultaneously optimize the distance and perceptual similarity to a given target image during back-propagation. To preserve the details of the UHD image and reduce the number of network calculations and parameters, a Perceptual Enhancement Network (called PEN) based on dilated convolution is proposed. Extensive experiments on the datasets show that the method excels in baselines in both subjective and objective evaluations.},
  archive      = {J_IETIP},
  author       = {Ying Yang and Mengning Yang and Xin Zhang},
  doi          = {10.1049/ipr2.12464},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1988-2000},
  shortjournal = {IET Image Process.},
  title        = {An end-to-end perceptual enhancement method for UHD portrait images},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CPDINet: Blind image quality assessment via a content
perception and distortion inference network. <em>IETIP</em>,
<em>16</em>(7), 1973–1987. (<a
href="https://doi.org/10.1049/ipr2.12463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, it is still challenging for the blind image quality assessment (BIQA) to accurately predict quality scores of distorted images, since distorted images have rich content information and complex distortions. To solve these problems, a content perception and distortion inference network for BIQA is proposed, which divides IQA task into content perception and distortion inference processes. Since humans try to understand image content before perceiving quality scores, a content feature extractor is designed to explore content information in an image to deal with the content variation problem. To handle the distortion diversity problem, a distortion feature extractor is proposed to capture distortion features in images. Because extracted content features and distortion ones have different characteristics, attention-based fusion blocks to fuse multi-scale content features and distortion ones as guidance to selectively enhance important features based on calculated weight scores are proposed. With fused features, a quality prediction module is designed to regress multi-scale features to quality scores. Experiments are performed on six public IQA datasets, including LIVE, CSIQ, TID2013, LIVEC, KonIQ-10k, and SPAQ. Experimental results show that the proposed method can effectively predict quality scores for both synthetically and authentically distorted images than its peers, including the state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Xiao Shao and Mengqing Liu and Zihan Li and Peiyun Zhang},
  doi          = {10.1049/ipr2.12463},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1973-1987},
  shortjournal = {IET Image Process.},
  title        = {CPDINet: Blind image quality assessment via a content perception and distortion inference network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Research on reversible image steganography of encrypted
image based on image interpolation and difference histogram shift.
<em>IETIP</em>, <em>16</em>(7), 1959–1972. (<a
href="https://doi.org/10.1049/ipr2.12461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In view of the mutual restriction between the image encryption and information hiding process in the ciphertext domain reversible algorithm, the existing ciphertext domain image reversible image steganography algorithm can hardly take into account the performance of security and hiding capacity. This paper proposes a reversible image steganography algorithm for encrypted images based on a new interpolation image and difference histogram shift. The pixel position of the carrier image is changed through the double scrambling operation of random image block recombination and Arnold transform. The chaotic sequence generated by logistic mapping is used as the diffusion sequence of the encryption algorithm, and DNA encoding and calculation are performed respectively to obtain the final encrypted image. According to the newly proposed interpolation algorithm, the encrypted image is enlarged, determine the pixel value of the target interpolated image according to the pixel value of the encrypted image, calculate the difference between the three adjacent pixel values of the target interpolated image, and use the difference histogram translation algorithm to convert the secret Information is embedded in an interpolated image, and only one bit of the pixel value can be modified in a single embedding process, and the embedding method is simple and effective. Experimental results show that under the premise of ensuring information security and image quality, the embedding capacity has also been greatly improved, reaching 1,572,864 bits. When the embedding rate is 0.67 bpp, the PSNR can also reach 58 dB and the image quality is maintained better.},
  archive      = {J_IETIP},
  author       = {Hanmin Ye and Keqin Su and Xiaohui Cheng and Shiming Huang},
  doi          = {10.1049/ipr2.12461},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1959-1972},
  shortjournal = {IET Image Process.},
  title        = {Research on reversible image steganography of encrypted image based on image interpolation and difference histogram shift},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). COLF-GAN: Learning to axial super-resolve focal stacks.
<em>IETIP</em>, <em>16</em>(7), 1949–1958. (<a
href="https://doi.org/10.1049/ipr2.12460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to generate a denser focal stack, a cooperative generative adversarial network is proposed to learn the refocusing ability from light field imaging. To keep the axial continuity, the proposed framework is designed to learn features of a focal stack in both axial directions. Different from the classic generative adversarial network, our generative module consists of a forward prediction sub-network and a backward prediction sub-network, taking the forward-and-backward focal stacks as their inputs, respectively. The bi-directional predictions are then fused by a weighting process, which is guided by an adversarial module. The proposed network is trained on light field focal stacks conducted via digital refocusing. Without loss of the refocus continuity, one can axial super-resolve a focal stack by using the trained model. The effectiveness of the proposed algorithm on different types of focal stacks produced by both light fields and traditional camera shootings is validated. The experimental results indicate that the refocus variation of a focal stack can be well learned and predicted without a complete light field. Therefore, the proposed algorithm outperforms the traditional digital refocusing in terms of run-time.},
  archive      = {J_IETIP},
  author       = {Zhaolin Xiao and Huan Liu and Haiyan Jin},
  doi          = {10.1049/ipr2.12460},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1949-1958},
  shortjournal = {IET Image Process.},
  title        = {COLF-GAN: Learning to axial super-resolve focal stacks},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reliable metrics-based linear regression model for
multilevel privacy measurement of face instances. <em>IETIP</em>,
<em>16</em>(7), 1935–1948. (<a
href="https://doi.org/10.1049/ipr2.12459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social networking sites have made photo sharing convenient and, consequently, users of those sites frequently share photos. The proliferation of these social images throughout the Internet has inadvertently exposed an increasing number of personally identifiable information, particularly from the visual information of the faces in the images. Most existing methods to de-identify faces results in an excessive loss of visual information. To solve this problem, this paper proposes a reliable metrics-based linear regression model for multilevel privacy measurement of face instances using the size information of face instances. The proposed privacy measurement model provides a novel instance-level-based solution to measure privacy levels. The paper also establishes a scientific relationship between the size information of face instances and their privacy levels, quantifying the degree to which face instances need to be de-identified. Finally, the paper proposes a novel k-Same-DT de-identification method to provide reliable metrics for a linear regression model. It is a real-time k-same-related de-identification algorithm that combines the PCA dimensionality reduction strategy and the Delaunay triangle-based face alignment algorithm.The proposed k-Same-DT method creates a de-identified face with a lower identifiable rate and higher structural similarity, and it can provide reliable de-identification metrics for privacy measurement. Experiments using the classical face dataset demonstrates the effectiveness of the proposed de-identification method. Extensive experiments and surveys on real-world social images were also conducted to verify the proposed measurement model.},
  archive      = {J_IETIP},
  author       = {Xuan Li and Zhenghua Huang and Lei Ma and Yuhang Xu and Li Cheng and Zhi Yang},
  doi          = {10.1049/ipr2.12459},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1935-1948},
  shortjournal = {IET Image Process.},
  title        = {Reliable metrics-based linear regression model for multilevel privacy measurement of face instances},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-domain person re-identification based on background
suppression and identity consistency. <em>IETIP</em>, <em>16</em>(7),
1924–1934. (<a href="https://doi.org/10.1049/ipr2.12458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advanced methods address the cross-domain person re-identification (Re-ID) problem primarily by the pseudo label estimation. However, person images with similar backgrounds are usually incorrectly clustered due to the interference of background noise. Furthermore, it is the lack of filtering of incorrect pseudo labels that results in the poor quality of the final generated pseudo labels. Here, a progressive learning approach based on background suppression and identity consistency for cross-domain person Re-ID (BSIC-reID) is proposed. In the background suppression module, background mask attention and reverse attention are combined to effectively extract pedestrian features and suppress background noise, highlighting the foreground person information for person Re-ID. In addition, the BSIC-reID model is used to extract multi-scale person features and generate different perspective pseudo labels for the target domain images. The incorrect pseudo labels are filtered by comparing the potential similarity of multi-scale person features so that the higher quality pseudo labels for the target domain images could be generated. This method is performed on the Market-1501, DukeMTMC-reID, and MSMT17, evaluated by the Cumulated Matching Characteristics (CMC) and mean Average Precision (mAP). The experimental results demonstrate that this method also achieves state-of-the-art performance.},
  archive      = {J_IETIP},
  author       = {Ming Jiang and Juntao Gao and Pengfei Li and Min Zhang},
  doi          = {10.1049/ipr2.12458},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1924-1934},
  shortjournal = {IET Image Process.},
  title        = {Cross-domain person re-identification based on background suppression and identity consistency},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-convex nonlocal adaptive tight frame image deblurring.
<em>IETIP</em>, <em>16</em>(7), 1908–1923. (<a
href="https://doi.org/10.1049/ipr2.12456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of the image restoration is to recover more detailed information from the degraded images. Based on the observations that wavelet frames have efficient representation ability to image details and the nonconvex regularization in the model may admit unbiased solutions, in this paper, in order to recover more details, a wavelet frames nonconvex ℓ p ⁢ ( 0 &lt; p &lt; 1 $\ell _p(0&amp;lt;p&amp;lt;1$ ) regularization image restoration model is established that combing with a nonlocal adaptive mean doubly augmented Lagrangian (MDAL) method and context model. Specifically, a nonlocal adaptive MDAL method is proposed to solve the established nonconvex model. Furthermore, in order to mitigate the trade-off between the regularization error and the noise magnification error, spatially adaptive wavelet thresholding methods based on the context model and nonlocal mean filter are introduced in the algorithm. The convergence analysis of this nonlocal adaptive nonconvex algorithm is also obtained using the KL inequality. Numerical experiments demonstrate that the proposed method has a competent deblurring and denoising ability, also is efficient and is comparable to state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Zhengwei Shen},
  doi          = {10.1049/ipr2.12456},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1908-1923},
  shortjournal = {IET Image Process.},
  title        = {Non-convex nonlocal adaptive tight frame image deblurring},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single image dehazing using generative adversarial networks
based on an attention mechanism. <em>IETIP</em>, <em>16</em>(7),
1897–1907. (<a href="https://doi.org/10.1049/ipr2.12455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing image dehazing methods rely on the solution of the atmospheric scattering model or supervised learning based on paired images. However, owing to incomplete prior knowledge and the lack of paired hazy and haze-free images of the same scenes as training samples, their performances for single image dehazing are unsatisfactory. Here, the authors present an unpaired image learning method based on the attention mechanism for single image dehazing problems. The method uses the constraint transfer learning ability and circulatory structure of CycleGAN to carry out an unsupervised image dehazing task for unpaired data. Considering the complexity of the haze distribution in actual imaging and human visual characteristics, the improved channel attention and domain attention mechanisms are integrated into the network to process different features and different regions non-uniformly. The experimental results show that the proposed method achieves good results on both synthetic datasets and real hazy images.},
  archive      = {J_IETIP},
  author       = {Yongli Ma and Jindong Xu and Fei Jia and Weiqing Yan and Zhaowei Liu and Mengying Ni},
  doi          = {10.1049/ipr2.12455},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1897-1907},
  shortjournal = {IET Image Process.},
  title        = {Single image dehazing using generative adversarial networks based on an attention mechanism},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single infrared image super-resolution based on lightweight
multi-path feature fusion network. <em>IETIP</em>, <em>16</em>(7),
1880–1896. (<a href="https://doi.org/10.1049/ipr2.12454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single infrared (IR) image super-resolution methods can help to reduce the cost and difficulty in manufacturing IR sensors for the imaging system. However, the deep learning-based image SR methods need to build a complex network and thus consume a lot of computational power, which limits the application of SR technology on devices with low computing resources in practice. To solve this problem, the authors present a lightweight multi-path feature fusion network (MFFN) for the single infrared (IR) image SR. A multi-path feature fusion block (MFFB) is developed to extract and fuse multiple and discriminative features in a recursive feedback way. Specifically, the multiple features are refined via the linear feature extraction branch, shared-source residual feature extraction branch, and channel attention branch in MFFB. Finally, the authors reconstruct the high-resolution IR images from the low-resolution counterpart based on the refined multiple features. The experimental results demonstrate that MFFN achieves high-quality single infrared image SR and shows superiority over previous methods for several scale factors (e.g. ×2, ×3, and ×4). MFFN has potential applications in the mobile infrared imaging system.},
  archive      = {J_IETIP},
  author       = {Fei Mo and Heng Wu and Shuo Qu and Shaojuan Luo and Lianglun Cheng},
  doi          = {10.1049/ipr2.12454},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1880-1896},
  shortjournal = {IET Image Process.},
  title        = {Single infrared image super-resolution based on lightweight multi-path feature fusion network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved synthetic aperture radar-scale invariant feature
transform algorithm for interferometric imaging radar altimeter image
registration. <em>IETIP</em>, <em>16</em>(7), 1866–1879. (<a
href="https://doi.org/10.1049/ipr2.12453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interferometric imaging radar altimeter (InIRA) integrating synthetic aperture radar (SAR) and synthetic aperture radar interferometry (InSAR) is a new type of radar altimeter. InIRA will be widely adopted in marine scientific research and surface water topography measurements. Owing to the geometric distortion of InIRA images, there are difficulties in feature point extraction and matching, making InIRA registration challenging. Here, an improved SAR-scale invariant feature transform (SIFT, SAR-SIFT) algorithm is proposed to address these difficulties. First, a modified Harris non-linear scale space is constructed using the Harris function and non-linear scale space, and extracted the feature points according to the SAR-SIFT algorithm. Second, the orbital parameters of InIRA are used to calculate the geographic coordinates of each feature point by Hermit interpolation. Subsequently, a geographic-coordinate-based method is proposed to match the feature points. The results show that the proposed algorithm gained more correct matches and improved the registration accuracy by 13% and 16.7% compared with the SIFT and SAR-SIFT algorithms, respectively, and was 18% more time efficient than the SAR-SIFT algorithm. The experiments demonstrate that the proposed algorithm outperforms the SAR-SIFT algorithm for InIRA image registration in terms of accuracy and efficiency.},
  archive      = {J_IETIP},
  author       = {Zhiyong Wang and Hao Li and Zihao Wang and Kaile Ye},
  doi          = {10.1049/ipr2.12453},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1866-1879},
  shortjournal = {IET Image Process.},
  title        = {An improved synthetic aperture radar-scale invariant feature transform algorithm for interferometric imaging radar altimeter image registration},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Salt and pepper noise removal method based on stationary
framelet transform with non-convex sparsity regularization.
<em>IETIP</em>, <em>16</em>(7), 1846–1865. (<a
href="https://doi.org/10.1049/ipr2.12451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salt and pepper noise occurs randomly and causes image degradation. Numerous denoising methods have been proposed to suppress this noise. However, existing methods have two main limitations. First, noise characteristics, such as noise location information and sparsity, are often described inaccurately or even ignored. Second, many existing methods separate the contaminated image into a recovered image and a noise part, leading to the recovery of an image with unsatisfactory smooth and detailed parts. In this study, the authors introduce a noise detection strategy to determine the position of the noise and employ a non-convex sparsity regularization depicted by quasi-norm to describe the sparsity of the noise, thereby addressing the first limitation. We adopt the morphological component analysis framework with stationary Framelet transform to decompose the processed image into the cartoon, texture, and noise parts to resolve the second limitation. Then, the proposed model is applied by using the alternating direction method of multipliers (ADMM). Finally, experiments are conducted to verify the proposed method and compare it with some current state-of-the-art denoising methods. The experimental results show that the proposed method can remove salt and pepper noise while preserving the details of the processed image and outperforming some state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Yingpin Chen and Yuming Huang and Lingzhi Wang and Huiying Huang and Jianhua Song and Chaoqun Yu and Yanping Xu},
  doi          = {10.1049/ipr2.12451},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1846-1865},
  shortjournal = {IET Image Process.},
  title        = {Salt and pepper noise removal method based on stationary framelet transform with non-convex sparsity regularization},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HessHist: A hessian-matrix weighted histogram for image
contrast enhancement. <em>IETIP</em>, <em>16</em>(7), 1831–1845. (<a
href="https://doi.org/10.1049/ipr2.12450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For image contrast enhancement operation, it is a keypoint to obtain more natural enhanced results and keep more details without distortion. In this paper, a novel image Hessian-matrix weighted histogram for image contrast enhancement is proposed, which can improve the contrast of smooth regions and simultaneously restrain the contrast of texture regions. In the proposed method, the multi-scale fractional-order Hessian-matrix is firstly utilized to detect and quantify the texture information of the input image, which explores the regions that should be contrasted or should be restrained. Then, the strong texture regions are suppressed by a designed suppress function. Finally, the information on unsuppressed regions and suppressed texture regions will be count by a histogram, which is termed as Hessian-matrix weighted Histogram (HessHist) in this paper. According to HessHist, the corresponding cumulative distribution function will realize the contrast enhancement operation on the input image. For real-time application, the integral images are introduced for fast computation of the HessHist. Experimental results show that the proposed HessHist-based image enhancement algorithm preserves more details of input image without distortion, and is competitive with state-of-the-art image enhancement algorithms in both subjective visual perception and objective evaluation metrics.},
  archive      = {J_IETIP},
  author       = {Junchao Fan and Xuyang Zong and Han Tang and Xiuli Bi and Bin Xiao and Weisheng Li},
  doi          = {10.1049/ipr2.12450},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1831-1845},
  shortjournal = {IET Image Process.},
  title        = {HessHist: A hessian-matrix weighted histogram for image contrast enhancement},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Superpixel segmentation algorithm based on local network
modularity increment. <em>IETIP</em>, <em>16</em>(7), 1822–1830. (<a
href="https://doi.org/10.1049/ipr2.12448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Superpixel segmentation is a kind of image preprocessing technology and a popular research direction in image processing. The purpose of superpixel segmentation is to reduce the complexity of image processing. The most widely applied Simple Linear Iterative Clustering (SLIC) superpixel segmentation algorithm has high operating efficiency. However, under-segmentation is prone to occur when the number of given superpixel regions is too small. In order to improve the segmentation accuracy, the superpixel segmentation algorithm based on local network modularity increment (LocalNet) from the perspective of network community detection is proposed here. The adjacency network is constructed according to the colour similarity of image pixels, the local community centre is found by the degree of network nodes, and the local network structure of the community is constructed. The modularity increment is employed as the boundary constraint to improve the segmentation accuracy of superpixel segmentation. Through the experimental comparison with the SLIC algorithm, its improved algorithm, and the algorithm proposed in recent years, the results show that our LocalNet algorithm significantly improves in segmentation accuracy Furthermore, the segmentation effect has obvious advantages under the premise that the segmentation speed is not much different from that of the other five algorithms.},
  archive      = {J_IETIP},
  author       = {Tianli Liu and Fang Dai and Wenyan Guo and Fengqun Zhao and Junfeng Wang and Xiaoxia Wang},
  doi          = {10.1049/ipr2.12448},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1822-1830},
  shortjournal = {IET Image Process.},
  title        = {Superpixel segmentation algorithm based on local network modularity increment},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Edge-aware image outpainting with attentional generative
adversarial networks. <em>IETIP</em>, <em>16</em>(7), 1807–1821. (<a
href="https://doi.org/10.1049/ipr2.12447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image outpainting aims at extending the field of view of an existing image. While image inpainting has achieved great success with the deep learning technology, image outpainting still receive less attention. The main challenge is how to generate high-quality extended images with clear texture and highly consistent semantic information. In order to solve the problem of the effect of invalid pixels on the generated image and the distance of effective pixels is too far. This paper proposes a two-stage image outpainting method (the EA method), which consists of an edge generation stage and an edge transformation stage. In this paper, the convolutional block attention module (CBAM) is introduced into the generation network to focus on spatial and channel feature and the improved VAE-GAN structure is used to generate the extended image for more realistic semantics. The EA method is evaluated on the CelebA, Pairs-streetview and homemade landscapes dataset, and show that the results contain high-quality textures as well as faithfully extend the semantics. The average PSNR, SSIM, FID index of the EA method on the three datasets is 22.7961, 0.7061, 6.8553 and show that it outperforms existing algorithms in both quantitative and qualitative analysis.},
  archive      = {J_IETIP},
  author       = {Xiaoming Li and Hengzhi Zhang and Lei Feng and Jing Hu and Rongguo Zhang and Qiang Qiao},
  doi          = {10.1049/ipr2.12447},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1807-1821},
  shortjournal = {IET Image Process.},
  title        = {Edge-aware image outpainting with attentional generative adversarial networks},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RISSNet: Retain low-light image details and improve the
structural similarity net. <em>IETIP</em>, <em>16</em>(7), 1793–1806.
(<a href="https://doi.org/10.1049/ipr2.12446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images taken under low light conditions cause people&#39;s sensory vision to fall into blind areas, and the essential properties of objects are hidden in the dark. There are many excellent algorithms that can adjust the brightness of high and low light region, but they still have room for improvement in realizing the function of denoising and similarity restoration of internal attributes of objects. Inspired by the work principle of human retina, the image is decomposed into two modules, firstly, the enhancement network which is guided by the Attention Mask module completed the enhancement by the U-net network structure. In this structural network, an RISS module is designed here to effectively reduce the loss of image feature information, so as to improve the structural similarity of reconstructed images. Full convolutional network is used to maintain the position relation of illumination feature information. This network is trained on LOL-dataset, and the experimental results prove that the network can recover images with different illuminations. In the later stage, the effectiveness of this method is verified by the subjective and objective comparison with other methods on different datasets. This method can not only realize the function of low-light image enhancement, but also the function of image similarity restoration and image denoising.},
  archive      = {J_IETIP},
  author       = {Junbo Zhao and Hongyang Chen and Shangyou Zeng and Chengxu Ma},
  doi          = {10.1049/ipr2.12446},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1793-1806},
  shortjournal = {IET Image Process.},
  title        = {RISSNet: Retain low-light image details and improve the structural similarity net},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Arbitrary style transfer with attentional networks via
unbalanced optimal transport. <em>IETIP</em>, <em>16</em>(7), 1778–1792.
(<a href="https://doi.org/10.1049/ipr2.12403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arbitrary style transfer aims to stylize the content image with the style image. The key problem of style transfer is how to balance the global content structure and the local style patterns. A promising method to solve this problem is the attentional style transfer method, where a learnable embedding of image features enables style patterns to be flexibly recombined with the content image, so local style patterns will be well preserved in the stylized image. However, current attentional style transfer methods cannot well preserve the global content structure. To solve this problem, a novel attentional style transfer network is proposed, that relies on Optimal Transport (OT) for computing the attention map. The proposed OT-based attention ensures the similarity between global distributions of the synthesized image and its corresponding style image. For the optimal transport computation, a regularized formulation is used, which not only allows an unbalanced optimal transport to address the deviational distributions but also improves the robustness of stylized results. The proposed method finds a well balance between the global content structure and local style patterns. Various experiments are conducted to demonstrate the superiority of the proposed method over state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Jie Li and Liwen Wu and Dan Xu and Shaowen Yao},
  doi          = {10.1049/ipr2.12403},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1778-1792},
  shortjournal = {IET Image Process.},
  title        = {Arbitrary style transfer with attentional networks via unbalanced optimal transport},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient coding-based grayscale image automatic
colorization method combined with attention mechanism. <em>IETIP</em>,
<em>16</em>(7), 1765–1777. (<a
href="https://doi.org/10.1049/ipr2.12452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of deep learning provides a new way for solving the colorization problem on the grayscale image. Excellent coding-based methods appear in the automatic image colorization task, avoiding the unsaturated colour effect problem of previous methods based on the L2 loss function. Traditional neural networks come with high computational costs and a large number of parameters. Considering the limitation of memory and computing resources and aiming at lightweight, a novel grey image automatic colorization network is proposed. The basic idea of coding-based methods is used, regarding the colorization task as a pixel-level classification problem, meanwhile redesign and improve the colour encoding and decoding process. This network architecture leverages a lightweight convolution to reduce the computation and combines an efficient attention model to form a residual block as the kernel of the backbone network. Furthermore, an efficient image self-attention mechanism placed at the end of the network is applied to enhance the ultimate colouring results. The method proposed in this paper can maintain the natural colouring effect and significantly reduce the computational amount and network model parameters.},
  archive      = {J_IETIP},
  author       = {Xujia Qin and Mengjia Li and Yuehui Liu and Hongbo Zheng and Jiazhou Chen and Meiyu Zhang},
  doi          = {10.1049/ipr2.12452},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1765-1777},
  shortjournal = {IET Image Process.},
  title        = {An efficient coding-based grayscale image automatic colorization method combined with attention mechanism},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A method of single-shot target detection with multi-scale
feature fusion and feature enhancement. <em>IETIP</em>, <em>16</em>(6),
1752–1763. (<a href="https://doi.org/10.1049/ipr2.12445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Single Shot MultiBox Detector (SSD) is one of the fastest detection algorithms. Although it has achieved good results in detection, it also has the problem of poor detection effect for small targets and occlusion between objects. Here, the authors propose a new target detection method called single-shot target detection with multi-scale feature fusion and feature enhancement. Here, the authors introduce multi-scale feature fusion module, feature enhancement module and efficient channel attention module, and integrate them into the detection module of the original SSD target detection algorithm to improve the ability of network feature extraction. Experimental results on pascal VOC 2007 datasets show that the proposed algorithm works well when the input size is 300 × 300, the detection speed reaches 41.7 frames per second ( FPS ) and the detection accuracy reaches 79.6%, which is 2.4% higher than the original SSD target detection algorithm. When the input size is 512 × 512, the detection accuracy is 81.9%, and the detection speed reaches 36.5 FPS , which is 3.2% higher than the original SSD target detection algorithm. According to the experimental results, our algorithm has a better performance when there are many objects in the image and there is occlusion.},
  archive      = {J_IETIP},
  author       = {Zhong Qu and Xue Shang and Shu-Fang Xia and Tu-Ming Yi and Dong-Yang Zhou},
  doi          = {10.1049/ipr2.12445},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1752-1763},
  shortjournal = {IET Image Process.},
  title        = {A method of single-shot target detection with multi-scale feature fusion and feature enhancement},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-scale network for remote sensing segmentation.
<em>IETIP</em>, <em>16</em>(6), 1742–1751. (<a
href="https://doi.org/10.1049/ipr2.12444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semantic segmentation of remote sensing images is a critical and challenging task. How to easily and reliably segment useful information from vast remote sensing images is a significant issue. Many methods based on convolutional neural networks have been widely explored to obtain more accurate segmentation from remote sensing images. However, due to the uniqueness of remote sensing images, such as the dramatic changes in the scale of the target object, the results are not satisfactory. To solve the problem, a special network is designed: (1) Create a new backbone network. Compared with ResNet50, the proposed method extracts features of varying sizes more effectively. (2) Reduce spatial information loss. Building a hybrid location module to compensate for the position loss caused by the down-sampling operation. (3) Models with high discriminant ability. In order to improve the discrimination ability of the model, a novel auxiliary loss function is designed to constrain the distance between inter-class and intra-class. The proposed algorithm is tested on remote sensing datasets (e.g., NWPU-45, DLRSD, and WHDLD). The experimental results show that this method obtains the best results and achieves state-of-the-art performance.},
  archive      = {J_IETIP},
  author       = {Gaihua Wang and Qianyu Zhai and Jinheng Lin},
  doi          = {10.1049/ipr2.12444},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1742-1751},
  shortjournal = {IET Image Process.},
  title        = {Multi-scale network for remote sensing segmentation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Perceptual redundancy model for compression of screen
content videos. <em>IETIP</em>, <em>16</em>(6), 1724–1741. (<a
href="https://doi.org/10.1049/ipr2.12443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Screen content video (SCV) consists primarily of text areas, computer graphics and other computer-generated content and possesses unique perceptual characteristics. To compress SCVs more effectively with less reduction in subjective quality, perceptual characteristics of SCVs are analyzed and a perceptual redundancy (PR) model for SCV compression is proposed, including spatial PR (SPR), temporal PR (TPR) and foveated PR (FPR) model. In SPR modeling, the SCV is divided into sharp edge (SE) areas and non-SE areas, then SPR is estimated separately. In TPR modeling, both inter-frame luminance adaptation effect and motion masking effect are taken into account. In FPR modeling, each frame of SCV is classified into abrupt frames, relative motion frames or static frames. Then fixation points of different kinds of frames are predicted using different methods, and FPR is modeled considering foveated masking effect and visual attention. Finally, the perceptual redundancy of SCV is estimated based on the product of SPR, TPR and FPR. It is experimentally demonstrated that compared to the state-of-the-art models, the authors&#39; model could obtain more accurate estimates of PR. Moreover, the model is incorporated into SCV compression with an adaptive perceptual quantizer. An average of 7.42% bits could be saved with less decline in subjective quality.},
  archive      = {J_IETIP},
  author       = {Junlin Li and Li Yu and Hongkui Wang},
  doi          = {10.1049/ipr2.12443},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1724-1741},
  shortjournal = {IET Image Process.},
  title        = {Perceptual redundancy model for compression of screen content videos},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Target tracking method based on interference detection.
<em>IETIP</em>, <em>16</em>(6), 1709–1723. (<a
href="https://doi.org/10.1049/ipr2.12442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering the problems of similarity interference, partial occlusions, and changes in scale during target tracking, a target tracking method based on interference detection is proposed, which is an improvement over the Siamese fully convolutional classification and regression neural network (SiamCAR) approach. Under the proposed framework, the marginal distribution of the feature maps is used to determine the presence or absence of interferents. When interference is present in a scene, a motion vector composed of the predicted value obtained through a Kalman filter is used as the basis for target prediction. Experiments on the benchmark LaSOT dataset show that the proposed algorithm based on SiamCAR, which introduces motion features, achieves the best performance in videos with similar object interference, partial occlusions, fast motion, and small target tracking, as compared with the classical SiamCAR and other excellent target tracking algorithms.},
  archive      = {J_IETIP},
  author       = {Xuyang Qin and Shibin Xuan and Li Wang and Yun Chen},
  doi          = {10.1049/ipr2.12442},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1709-1723},
  shortjournal = {IET Image Process.},
  title        = {Target tracking method based on interference detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time facial expression recognition based on iterative
transfer learning and efficient attention network. <em>IETIP</em>,
<em>16</em>(6), 1694–1708. (<a
href="https://doi.org/10.1049/ipr2.12441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time facial expression recognition is the basis for computers to understand human emotions and detect abnormalities in time. To effectively solve the problems of server overload and privacy information leakage, a real-time facial expression recognition method based on iterative transfer learning and efficient attention network (EAN) for edge resource-constrained scenes is proposed in this paper. Firstly, an EAN is designed with its parameter number and computation amount strictly limited by depth separable convolution and local channel attention mechanism. Then, the soft labels of facial expression data were obtained by EAN based on the idea of knowledge distillation, so as to provide more supervision information for the training process. Finally, an iterative transfer learning method of teacher-student (T-S) network was proposed; it refines the soft labels of the teacher network and further improves the recognition accuracy of the student network. The tests on the public datasets, FER2013 and RAF-DB, show that this method can significantly reduce the model complexity and achieve high recognition accuracy. Compared with other advanced methods, the proposed method strikes a good balance between complexity and accuracy, and well meets the real-time deployment requirements of facial expression recognition technology for edge resource-constrained scenes.},
  archive      = {J_IETIP},
  author       = {Yinghui Kong and Shuaitong Zhang and Ke Zhang and Qiang Ni and Jungong Han},
  doi          = {10.1049/ipr2.12441},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1694-1708},
  shortjournal = {IET Image Process.},
  title        = {Real-time facial expression recognition based on iterative transfer learning and efficient attention network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel cell structure-based disparity estimation for
unsupervised stereo matching. <em>IETIP</em>, <em>16</em>(6), 1678–1693.
(<a href="https://doi.org/10.1049/ipr2.12440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that preserving depth edges is an effective solution for achieving the accurate disparity map in stereo matching, but many state-of-the-art methods do not preserve depth edges well. In order to solve it efficiently, the cell structure containing irregular and regular shape regions is designed to preserve depth edges. Based on the well-designed cell structure, a novel disparity estimation method for stereo matching is proposed, in which a two-layer disparity optimization method is proposed to refine the disparity plane; it includes the front-parallel disparities computation and slanted-surfaces disparity plane refinement. In the framework of front-parallel disparities computation, a tree-based cost aggregation method is presented to make full use of the segmentation information of cells and then performing semi-global cost aggregation. In the framework of slanted-surfaces disparity plane refinement, a new probability model is proposed that employs Bayesian inference for refining disparities in textureless, weak texture and occluded regions. Experimental results show that higher accuracy could be achieved via the proposed method compared with some known state-of-the-art stereo methods on KITTI 2015 and Middlebury dataset, which are the standard benchmarks for testing the stereo matching methods. It can also be indicated that the proposed method can produce accurate disparity map and have good generalization performance.},
  archive      = {J_IETIP},
  author       = {Xianjing Cheng and Yong Zhao and Wenbang Yang and Zhijun Hu and Xiaomin Yu and Haoliang Zhao and Pengcheng Zeng},
  doi          = {10.1049/ipr2.12440},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1678-1693},
  shortjournal = {IET Image Process.},
  title        = {A novel cell structure-based disparity estimation for unsupervised stereo matching},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An algorithm to detect overlapping red blood cells for
sickle cell disease diagnosis. <em>IETIP</em>, <em>16</em>(6),
1669–1677. (<a href="https://doi.org/10.1049/ipr2.12439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Africa, Uganda is among the countries with a high number of babies (20,000 babies) born with sickle cell, contributing between 6.8% of the children born with sickle cell every year worldwide and approximately 4.5% of the children born with hemoglobinopathies worldwide. It is estimated that by 2050, sickle cell cases will increase by 30% if no intervention is put in place. To facilitate early detection of sickle cell anaemia, medical experts employ machine learning algorithms to detect sickle cell abnormality. Previous research revealed that algorithms for recognizing shape of a sickle cell from blood smear by fractional dimension, cannot detect sickle cells if applied on blood samples containing overlapping red blood cells. In this research, the authors developed an algorithm to detect overlapping red blood cells for sickle cell disease diagnosis. The algorithm uses canny edge and double threshold machine learning techniques and takes overlapping red blood cells images as inputs to detect if these cells are sickle cell anaemic. These images have a scale magnification of (200×, 400×, 650×) pixel taken using a microscope. The algorithm was tested on a total of 1000 digital images and the overall accuracy, sensitivity and specificity were 98.18%, 98.29% and 97.98% respectively.},
  archive      = {J_IETIP},
  author       = {Mabirizi Vicent and Kawuma Simon and Safari Yonasi},
  doi          = {10.1049/ipr2.12439},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1669-1677},
  shortjournal = {IET Image Process.},
  title        = {An algorithm to detect overlapping red blood cells for sickle cell disease diagnosis},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised anomaly detection via dual transformation-aware
embeddings. <em>IETIP</em>, <em>16</em>(6), 1657–1668. (<a
href="https://doi.org/10.1049/ipr2.12438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised anomaly detection refers to the discovery of unconventional images that are globally or locally different from the training set. Recently, reconstruction-based anomaly detection methods have made great progress. However, most of the existing methods take reconstructing the original image as the goal of latent feature learning. Due to lack of effective semantic guidance, latent features have intrinsic characteristics which retain redundant details of spatial structure. Such information is too general and cause over-expression problem. To solve this problem, in this paper, dual transformation-aware embeddings are coined which aims to achieve a stable model to learn high-level latent features in a self-supervised manner. To be more specific, the authors try to extract transformation-detectable feature embeddings for both structure and content views which explore the regular pattern under different transformations in normal situations. In addition, the relationship between the original feature and the transformed feature is established. Based on such relationship, the latent feature of generated image to predict transformation parameter is extracted. Then, a transformation-consistency regularization is proposed to constrain decoder to generate high-quality image with high-level consistency and achieve a more stable model. Experiments on MVTec-AD and CIFAR10 datasets prove the effectiveness and robustness of the proposed method.},
  archive      = {J_IETIP},
  author       = {Zhipeng Wang and Chunping Hou and Bangbang Ge and Yang Liu and Zhicheng Dong and Zhiqiang Wu},
  doi          = {10.1049/ipr2.12438},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1657-1668},
  shortjournal = {IET Image Process.},
  title        = {Unsupervised anomaly detection via dual transformation-aware embeddings},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inadequate dataset learning for major depressive disorder
MRI semantic classification. <em>IETIP</em>, <em>16</em>(6), 1648–1656.
(<a href="https://doi.org/10.1049/ipr2.12437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting patients with major depression (MDD) is currently a difficult task. Magnetic resonance imaging (MRI) data analysis may provide insight into individual patient responses, allowing for more customized treatment decisions. Due to the absence of brain MRI data for MDD patients, a transfer learning (TL) method developed is used using calculation criteria. Combining an Inception-v3 neural network with a typical pre-trained neural network, the move learning-based Inception-v3 was proposed for the classification of MDD MRI datasets. An experiment was performed on the classification of eight semantic emotions (defined by IMAPS). Compared to other methods, the proposed method performs high efficiency for 90–10% and 80–20% (positive and negative classes), normal (N), unnormal (UN), and average/total sets, and for 70–30%, accuracy (A) is 92.90%, area under the curve (AUC) is 94.23%, and average precision score (APS) is 95.75%. Individual patients&#39; responses to emotional stimulation can be predicted using the proposed methods, which can provide guidance in diagnosis and prognosis.},
  archive      = {J_IETIP},
  author       = {Jie Liu and Nilanjan Dey and Ruben González Crespo and Fuqian Shi and Chanjuan Liu},
  doi          = {10.1049/ipr2.12437},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1648-1656},
  shortjournal = {IET Image Process.},
  title        = {Inadequate dataset learning for major depressive disorder MRI semantic classification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A dynamic object filtering approach based on object
detection and geometric constraint between frames. <em>IETIP</em>,
<em>16</em>(6), 1636–1647. (<a
href="https://doi.org/10.1049/ipr2.12436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to eliminate the influence of moving targets in visual positioning, a dynamic object filtering approach based on object detection and inter-frame geometric constraints is proposed to filter the dynamic objects in monocular images. The object detection algorithm is firstly used to identify and locate objects in the single-frame image, and the object matching between frames is performed. Then, the depth estimation network and pose recovery network are trained jointly to output estimated depth along with transformation matrix of object centroids between frames respectively. Finally, the mapping centroid is obtained from the estimated depth and inter-frame transformation matrix. The joint constraint function is performed to complete the detection and filtering of dynamic objects. The experimental results show that the proposed dynamic object filtering approach not only can filter moving objects in sequence images accurately but also allows to reserve the dynamic objects that are temporarily in the stop state. The generalization ability of this approach is also verified in a real urban road scene and meets the requirements of follow-up research in visual positioning.},
  archive      = {J_IETIP},
  author       = {Jiansheng Wei and Shuguo Pan and Wang Gao and Tao Zhao},
  doi          = {10.1049/ipr2.12436},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1636-1647},
  shortjournal = {IET Image Process.},
  title        = {A dynamic object filtering approach based on object detection and geometric constraint between frames},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mirror invariant convolutional neural networks for image
classification. <em>IETIP</em>, <em>16</em>(6), 1626–1635. (<a
href="https://doi.org/10.1049/ipr2.12435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (DCNNs) have been developed rapidly and they perform well on both classification and object detection tasks. However, its strong performance makes people ignore to study the invariance of the DCNNs, such as mirror invariance. In fact, the ability of DCNNs in handling mirror-symmetrical images remains limited. In this paper, a mirror transformation convolutional layer is proposed, which transforms several feature maps to produce mirror-symmetrical feature maps based on the traditional convolutional layer. By combining with the mirror transformation convolutional layer, the DCNNs will have mirror invariance and the performance of neural networks can be improved on classification tasks. A dataset for driver&#39;s and passenger&#39;s seatbelt detection has been collected, which is used to verify the effectiveness of the proposed convolutional layer. In the experiments, one of the state-of-the-art DCNNs, GoogLeNet, is collaborated with the mirror transformation convolutional layer to form a mirror invariant networks (MINets). The experimental results show that the MINets can achieve better classification performance than the original GoogLeNet. MINets can also reduce the risk of over-fitting caused by applying data augmentations to the dataset.},
  archive      = {J_IETIP},
  author       = {Shufang Lu and Yan Li and Minqian Wang and Fei Gao},
  doi          = {10.1049/ipr2.12435},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1626-1635},
  shortjournal = {IET Image Process.},
  title        = {Mirror invariant convolutional neural networks for image classification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual perception and local features for
foreground-background segmentation. <em>IETIP</em>, <em>16</em>(6),
1613–1625. (<a href="https://doi.org/10.1049/ipr2.12434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional foreground-background segmentation models mainly depend on the low-level features of the image, while ignoring the visual effect. Combining visual perception and local features, a top-down segmentation model is proposed. This model regards foreground-background segmentation as a reasoning problem based on visual perception, and calculates the association between two-pixel blocks through Kullback–Leibler divergence, which solves the ill-posed problem of traditional single-pixel recognition. Meanwhile, local features are used to optimize the overall segmentation results in detail and improve the segmentation accuracy. The experimental results on the CMU-Cornell iCoseg database and the BSDS500 database show that visual perception and local features can help improve the segmentation performance to a certain extent.},
  archive      = {J_IETIP},
  author       = {Tong Peng and Kun He and Yao Su and Ziwei Hui},
  doi          = {10.1049/ipr2.12434},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1613-1625},
  shortjournal = {IET Image Process.},
  title        = {Visual perception and local features for foreground-background segmentation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Underwater image enhancement with latent consistency
learning-based color transfer. <em>IETIP</em>, <em>16</em>(6),
1594–1612. (<a href="https://doi.org/10.1049/ipr2.12433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the inevitable wavelength-dependent light absorption and forward/backward scattering, underwater images usually suffer severe color distortion and are hazy. It has become quite necessary to improve the visual quality of underwater images for both underwater observation and operation. Traditional enhancement methods and existing deep learning-based approaches to underwater image enhancement usually produce unsatisfactory results for photographs taken in complicated, wild underwater scenes. In such scenes, complex and diverse degradation-enhancement mappings are often difficult to model, especially since there are very limited samples available for learning. Inspired by the success of color-transfer techniques, it is found that clear template image-assisted color transfer is a promising strategy for underwater image enhancement, including not only color correction but also contrast and visibility improvement. Therefore, instead of directly learning the complex deep enhancement models, it is proposed to select proper color-transfer templates by learning the latent consistency between the templates and the raw underwater images. The proposed new enhancement strategy alleviates the problem caused by incomplete color-correction models and provides more stable enhancements by utilizing color transfer with consideration of global color distribution consistency and local visual contrast. Comprehensive experiments conducted on UIEB, RUIE, URPC and SQUID datasets demonstrate the good performance and great potential of the proposed new underwater image enhancement strategy.},
  archive      = {J_IETIP},
  author       = {Hua Yang and Fei Tian and Qi Qi and Q. M. Jonathan Wu and Kunqian Li},
  doi          = {10.1049/ipr2.12433},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1594-1612},
  shortjournal = {IET Image Process.},
  title        = {Underwater image enhancement with latent consistency learning-based color transfer},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast ship detection based on lightweight YOLOv5 network.
<em>IETIP</em>, <em>16</em>(6), 1585–1593. (<a
href="https://doi.org/10.1049/ipr2.12432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at a series of problems such as detection accuracy, calculation blocking, display delay, and so on in the ship detection of surveillance video, an improved YOLOv5 algorithm is proposed in this paper. First, to improve the detection performance, it is proposed to optimize the anchor box algorithm in the YOLOv5 network according to the ship target characteristics. Then, the t-SNE algorithm is used to reduce and visualize the data set label information and perform weighted analysis on the processed features for low-dimensional data. The mapped kernel k-means clustering algorithm adaptively selects a more appropriate anchor box and considers the detection performance of large and small ship targets. Secondly, to improve the problem of computational blocking and delay, the BN scaling factor γ is used to compress the YOLOv5 network, so that the model can be reduced without reducing the detection performance. The optimized YOLOv5 framework is trained on the self-integrated data set. The accuracy of the algorithm is increased by 2.34%, and the ship detection speed reaches 98 fps and 20 fps in the server environment and the low computing power version (Jetson nano), respectively.},
  archive      = {J_IETIP},
  author       = {Jia-Chun Zheng and Shi-Dan Sun and Shi-Jia Zhao},
  doi          = {10.1049/ipr2.12432},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1585-1593},
  shortjournal = {IET Image Process.},
  title        = {Fast ship detection based on lightweight YOLOv5 network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Infrared and visible image fusion based on multi-channel
convolutional neural network. <em>IETIP</em>, <em>16</em>(6), 1575–1584.
(<a href="https://doi.org/10.1049/ipr2.12431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the lack of labels in infrared and visible image fusion network, an infrared and visible image fusion model based on multi-channel unsupervised convolutional neural network (CNN) is proposed in this paper, in order to extract more detailed information through multi-channel inputs. In contrast to conventional unsupervised fusion network, the proposed network contains three channels for extracting infrared features, visible features and common features of infrared and visible images, respectively. The square loss function is used to train the network. Pairs of infrared and visible images are input to DenseNet to extract as more useful features as possible. A fusion module is designed to fuse the extracted features for testing. Experimental results show that the proposed method can preserve both the clear target of infrared and detailed information of visible images simultaneously. Experiments also demonstrate the superiority of the proposed method over the state-of-the-art methods in objective metrics.},
  archive      = {J_IETIP},
  author       = {Hongmei Wang and Wenbo An and Lin Li and Chenkai Li and Daming Zhou},
  doi          = {10.1049/ipr2.12431},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1575-1584},
  shortjournal = {IET Image Process.},
  title        = {Infrared and visible image fusion based on multi-channel convolutional neural network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MCRD-net: An unsupervised dense network with multi-scale
convolutional block attention for multi-focus image fusion.
<em>IETIP</em>, <em>16</em>(6), 1558–1574. (<a
href="https://doi.org/10.1049/ipr2.12430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-focus image fusion technology solves the problem of limited depth of field of the optical lens. It can extract different focus parts under the same target to synthesize a full-focus image. This paper proposes an unsupervised dense network for multi-focus image fusion. In the network, a multi-scale feature extraction module is employed to extract the spatial details of source images from different scales, and a convolutional block attention module is used to select the useful deep features, and a residual module is used to effectively optimize the performance of the network. By introducing these three modules, the proposed network can effectively extract the shallow and deep features of the source images. Besides, Gaussian-based Sum-Modified-Laplacian (GSML) is used to calculate the activity level of the feature map to generate a decision map. The performance of the proposed method is analyzed from two aspects: visual quality and objective metrics. Experimental results show that compared with nine image fusion methods, the performance of this algorithm is better.},
  archive      = {J_IETIP},
  author       = {Ding Zhou and Xin Jin and Qian Jiang and Li Cai and Shin-jye Lee and Shaowen Yao},
  doi          = {10.1049/ipr2.12430},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1558-1574},
  shortjournal = {IET Image Process.},
  title        = {MCRD-net: An unsupervised dense network with multi-scale convolutional block attention for multi-focus image fusion},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Novel and secure plaintext-related image encryption
algorithm based on compressive sensing and tent-sine system.
<em>IETIP</em>, <em>16</em>(6), 1544–1557. (<a
href="https://doi.org/10.1049/ipr2.12429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a secure plaintext-related image encryption scheme based on compressive sensing and a tent-sine system is proposed. First, the discrete wavelet transform (DWT) is used to transform the plain image to get a coefficient matrix. Second, several chaotic sequences generated by the tent-sine chaotic map are used to scramble the coefficient matrix and construct a measurement matrix. Afterward, compressive sensing is performed on the coefficient matrix to obtain a small-sized encrypted image. Finally, an image encryption scheme related to plaintext is designed. In particular, the proposed system uses the original image information to participate in the encryption process, ensuring the high sensitivity of the cryptosystem to minor differences in the plain image and good performance on resisting known/selected plaintext attacks. Furthermore, to convey the plaintext-related parameters to the receiver, the dimension of the ciphertext image is expanded, and the parameters are embedded into the ciphertext image. Simulation results and security analysis show that the proposed image encryption system has strong plaintext sensitivity and robustness for effectively resisting various typical attacks such as brute-force attacks, statistical attacks, and differential attacks.},
  archive      = {J_IETIP},
  author       = {Shufeng Huang and Linqing Huang and Shuting Cai and Xiaoming Xiong and Yuan Liu},
  doi          = {10.1049/ipr2.12429},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1544-1557},
  shortjournal = {IET Image Process.},
  title        = {Novel and secure plaintext-related image encryption algorithm based on compressive sensing and tent-sine system},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature-based no-reference video quality assessment using
extra trees. <em>IETIP</em>, <em>16</em>(6), 1531–1543. (<a
href="https://doi.org/10.1049/ipr2.12428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of social networks and improvements in the internet speed, the video data has become an ever-increasing portion of the global internet traffic. Besides the content, the quality of a video sequence is an important issue at the user end which is often affected by various factors such as compression. Therefore, monitoring the quality is crucial for the video content and service providers. A simple monitoring approach is to compare the raw video content (uncompressed) with the received data at the receiver. In most practical scenarios, however, the reference video sequence is not available. Consequently, it is desirable to have a general reference-less method for assessing the perceived quality of any given video sequence. In this paper, a no-reference video quality assessment technique based on video features is proposed. In particular, a long list of video features (21 sets of features, each consisting of 1 to 216 features) is considered and all possible combinations ( ) for training an Extra Trees regressor is examined. This choice of the regressor is wisely selected and is observed to perform better than other common regressors. The results reveal that the top 20 performing feature subsets all outperform the existing feature-based assessment methods in terms of the Pearson linear correlation coefficient (PLCC) or the Spearman rank order correlation coefficient (SROCC). Specially, the best performing regressor achieves on the test data over the KonVid-1k dataset. It is believed that the results of the comprehensive comparison could be potentially useful for other feature-based video-related problems. The source codes of the implementations are publicly available.},
  archive      = {J_IETIP},
  author       = {Hatef Otroshi-Shahreza and Arash Amini and Hamid Behroozi},
  doi          = {10.1049/ipr2.12428},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1531-1543},
  shortjournal = {IET Image Process.},
  title        = {Feature-based no-reference video quality assessment using extra trees},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The local ternary pattern encoder–decoder neural network for
dental image segmentation. <em>IETIP</em>, <em>16</em>(6), 1520–1530.
(<a href="https://doi.org/10.1049/ipr2.12416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in medical imaging analyses, especially the use of deep learning, are helping to identify, detect, classify, and quantify patterns in radiographs. At the centre of these advances is the ability to explore hierarchical feature representations learned from data. Deep learning is invaluably becoming the most sought out technique, leading to enhanced performances in the analysis of medical applications and systems. Deep learning techniques have achieved improved performance results in dental image segmentation. Segmentation of dental radiographs is a crucial step that helps dentists to diagnose dental caries. However, the performance of the deep networks used for these analyses are restrained by various challenging features found in dental carious lesions. Segmentation of dental images is often difficult due to the vast variety of types of topology, intricacies of medical structure and poor image quality caused by conditions such as low contrast, noise, irregular, and fuzzy border edges. These issues are exacerbated by low numbers of data images available for any particular analysis. A robust local ternary pattern encoder–decoder network (LTPEDN) is proposed to overcome dental image segmentation challenges and minimise the computational resources required. This new architecture is a modification of existing methods using an LTP. Images are preprocessed via augmentation and normalisation techniques to increase and prepare the datasets. Thereafter, the dataset input is sent to the LTPEDN for training and testing the model. Segmentation is performed using the non-learnable layers (the LTP layers) and the learnable layers (standard convolution layers), to extract the ROI of the teeth. The method was evaluated on an augmented dataset of 11, 000 dental images. It was trained on 8, 800 training set images and tested on 2, 200 testing set images. The new method is shown to be 94.32% accurate.},
  archive      = {J_IETIP},
  author       = {Omran Salih and Kevin Jan Duffy},
  doi          = {10.1049/ipr2.12416},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1520-1530},
  shortjournal = {IET Image Process.},
  title        = {The local ternary pattern encoder–decoder neural network for dental image segmentation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved cross entropy loss for noisy labels in vision leaf
disease classification. <em>IETIP</em>, <em>16</em>(6), 1511–1519. (<a
href="https://doi.org/10.1049/ipr2.12402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The predictive performance of supervised learning algorithms depends on the quality of labels. In a typical label collection process, multiple annotators provide subjective noisy estimates of the “truth” under the influence of their varying skill-levels and biases. Blindly treating these noisy labels as the ground truth limits the accuracy of learning algorithms in the presence of strong disagreement. This problem is critical for applications in domains where the annotation cost is high. Such problems are extremely serious in the domain of agricultural imaging in leaf disease classification. Due to the limitation of acquisition methods, noisy examples are often not only the normal mislabelled ones, but also the samples which contain more than one instance. To cope with the combination of the two noises, label smoothing is blended in the point of increasing entropy for uncertain labels, with Taylor cross entropy loss, which is proved to be efficient to solve the problem of artificial noisy labels on public datasets. And the proposed method is called smooth-Taylor cross entropy loss, which can deal with the real-world noises in vision leaf disease dataset. Extensive experimental results on cassava leaf disease dataset demonstrate that our proposed approach significantly outperforms the state-of-the-art counterparts.},
  archive      = {J_IETIP},
  author       = {Yipeng Chen and Ke Xu and Peng Zhou and Xiaojuan Ban and Di He},
  doi          = {10.1049/ipr2.12402},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1511-1519},
  shortjournal = {IET Image Process.},
  title        = {Improved cross entropy loss for noisy labels in vision leaf disease classification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CMC2R: Cross-modal collaborative contextual representation
for RGBT tracking. <em>IETIP</em>, <em>16</em>(5), 1500–1510. (<a
href="https://doi.org/10.1049/ipr2.12427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key challenge in RBGT tracking is how to fuse dual-modality information to build a robust RGB-T tracker. Motivated by CNN structure for local features, and visual transformer structure for global representations, the authors propose a two-stream hybrid structure, termed CMC 2 R, to take advantage of convolutional operations and self-attention mechanisms to lean the enhanced representation. CMC 2 R fuses local features and global representations under different resolutions through the transformer layer of the encoder block, and the two modalities are collaborated to get contextual information by the spatial and channel self-attention. The temporal association is performed with the track query, each track query models the entire track of an object, and updated frame-by-frame to build the long-range temporal relation. Experimental results show the effectiveness of the proposed method, and achieve the SOTAs performance.},
  archive      = {J_IETIP},
  author       = {Xiaohu Liu and Yichuang Luo and Keding Yan and Jianfei Chen and Zhiyong Lei},
  doi          = {10.1049/ipr2.12427},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1500-1510},
  shortjournal = {IET Image Process.},
  title        = {CMC2R: Cross-modal collaborative contextual representation for RGBT tracking},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reversible data hiding for JPEG images with a cascaded
structure. <em>IETIP</em>, <em>16</em>(5), 1486–1499. (<a
href="https://doi.org/10.1049/ipr2.12426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since Joint Photographic Experts Group is a typical image compression standard, so the reversible data hiding research on Joint Photographic Experts Group has received a lot of attention. There are two main research methods in Joint Photographic Experts Group-reversible data hiding: the method based on modifying DCT coefficients and the method based on variable length coding mapping. The disadvantage of the first one is that the visual loss is always inevitable during the DCT-coefficient modification process, while the second method often has insufficient embedding capacity and excessive file expansion. In order to solve the above problems, this paper proposes a cascaded reversible data hiding scheme, which builds a distortion minimisation scheme based on the DCT coefficients, and then cascades a file expansion suppression scheme based on the variable length coding mapping. For a particular case during the payload distribution, in this paper, all the secret information is embedded into DCT coefficients with a distortion minimisation scheme, and then the auxiliary information generated in the previous process is embedded in the variable length coding sequence with a scheme to optimise the file size expansion. The experimental results demonstrate the superiority of the proposed method in high-quality factor circumstances in terms of visual quality and file expansion.},
  archive      = {J_IETIP},
  author       = {Wuyue Zhan and Heng Yao},
  doi          = {10.1049/ipr2.12426},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1486-1499},
  shortjournal = {IET Image Process.},
  title        = {Reversible data hiding for JPEG images with a cascaded structure},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved tongue image segmentation algorithm based on
deeplabv3+ framework. <em>IETIP</em>, <em>16</em>(5), 1473–1485. (<a
href="https://doi.org/10.1049/ipr2.12425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tongue image segmentation is the key step of traditional Chinese medicine (TCM) intelligent tongue image analysis. The subsequent tongue image quality analysis is directly affected by the precision of segmentation. Deeplabv3+ network has become an excellent algorithm in the field of tongue images segmentation by virtue of its ability to extract multi-scale information and its codec structure. However, there are unclear edge segmentation of the tongue body and missegmentation of small areas in some tongue images. In view of the above phenomenon, an improved algorithm is proposed. Firstly, the network structure is optimized, so that the ability of the network to extract multi-scale information and low-level information is improved. Secondly, a loss function based on edge information is proposed, which makes the network pay more attention to the separation of tongue edges in the process of training. Finally, the segmentation results are post-processed by using the prior knowledge of tongue image, so as to eliminate the phenomenon of misjudgement. The experimental results show that the algorithm significantly improves the ambiguity of image segmentation, and the MIOU value is still increased to 99.13% when the MIOU value has reached 98.77%.},
  archive      = {J_IETIP},
  author       = {Xinfeng Zhang and Haonan Bian and Yiheng Cai and Keye Zhang and Hui Li},
  doi          = {10.1049/ipr2.12425},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1473-1485},
  shortjournal = {IET Image Process.},
  title        = {An improved tongue image segmentation algorithm based on deeplabv3+ framework},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving the performance of automotive vision-based
applications under rainy conditions. <em>IETIP</em>, <em>16</em>(5),
1457–1472. (<a href="https://doi.org/10.1049/ipr2.12424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Input images are the main source of information for vision-based algorithms. The presence of raindrops in input images degrades their quality and, consequently, reduces the quality of the target vision-based algorithm that consumes them. Many image restoration algorithms were proposed in the literature to remove rain presence in images to improve the input image quality. These algorithms, however, cannot remove all the raindrop presence and sometimes introduce undesirable side-effects, such as the blurring rain-occluded sections of the image and incorrectly de-raining areas in the image that are clear. It is hypothesized that a comparable performance improvement can be achieved by decreasing the sensitivity of vision-based algorithms to noisy input images, rather than denoising these images, through the process of de-raining. To test this hypothesis, the performance of state-of-the-art object detection and semantic segmentation models was evaluated, with de-rained image datasets used as input, and compared it to that performance of the same models, retrained with rained image sets. Results showed that the performance of the retrained models was better than that of the baseline detector with de-rained images used as input.},
  archive      = {J_IETIP},
  author       = {Yazan Hamzeh and Alireza Mohammadi and Samir A. Rawashdeh},
  doi          = {10.1049/ipr2.12424},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1457-1472},
  shortjournal = {IET Image Process.},
  title        = {Improving the performance of automotive vision-based applications under rainy conditions},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Segmentation of lung airways based on deep learning methods.
<em>IETIP</em>, <em>16</em>(5), 1444–1456. (<a
href="https://doi.org/10.1049/ipr2.12423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise segmentation of the lung airways is essential for a quantitative assessment of airway diseases. However, because of the complexity of the airway structure and the different thicknesses of the trachea at different positions, it is extremely difficult to segment the fine bronchial structure using chest computed tomography (CT). Traditional lung airway segmentation methods are generally based on the grayscale, geometric shape of the image, or the use of prior knowledge of anatomy. In recent years, deep learning techniques such as fully convolutional neural networks (FCNs) have achieved a great success in the field of image segmentation. Specifically, the symmetric encoder–decoder network represented by U-Net has achieved high accuracy in many medical image segmentation tasks. In the airway segmentation challenge task of the 4th International Symposium on Image Computing and Digital Medicine (ISICDM 2020), 9 of the 12 teams participating in the final round used the U-Net network or one of its other forms, obtaining good results for lung airway segmentation. Methods used to improve the segmentation accuracy include attention mechanisms and multiscale feature information fusion. This article provides a detailed description of the methods used by these 12 teams and analyses their results.},
  archive      = {J_IETIP},
  author       = {Wenjun Tan and Pan Liu and Xiaoshuo Li and Shaoxun Xu and Yufei Chen and Jinzhu Yang},
  doi          = {10.1049/ipr2.12423},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1444-1456},
  shortjournal = {IET Image Process.},
  title        = {Segmentation of lung airways based on deep learning methods},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A graph convolutional neural network model with fisher
vector encoding and channel-wise spatial-temporal aggregation for
skeleton-based action recognition. <em>IETIP</em>, <em>16</em>(5),
1433–1443. (<a href="https://doi.org/10.1049/ipr2.12422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition is an inspired yet challenging task in computer vision. Recently, the latest graph convolutional network (GCN), which generalises well-established convolutional neural networks to non-Euclidean structures, is proven to be highly successful for action recognition from body skeleton data. However, the GCN architecture has not been fully studied. In this work, a Fisher vector (FV) encoding based GCN architecture (FV-GCN) is proposed, which exceeds the limitations of existing GCN-based methods by combining the GCN model with FV encoding. A channel-wise spatial–temporal aggregation function to preserve spatial–temporal information in the whole action clip and integrate it into the FV-GCN architecture is also presented. Since FV is different from the GCN structure, this hybrid architecture that incorporates the advantages of both algorithms can discover complementary information of feature representation effectively. On two challenging human action datasets, kinetics, and NTU-RGBD, improved performance is demonstrated over the baseline method, and the FV-GCN is better or comparable to some state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Jun Tang and Yanjiang Wang and Sichao Fu and Baodi Liu and Weifeng Liu},
  doi          = {10.1049/ipr2.12422},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1433-1443},
  shortjournal = {IET Image Process.},
  title        = {A graph convolutional neural network model with fisher vector encoding and channel-wise spatial-temporal aggregation for skeleton-based action recognition},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual question answering with gated relation-aware
auxiliary. <em>IETIP</em>, <em>16</em>(5), 1424–1432. (<a
href="https://doi.org/10.1049/ipr2.12421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The great advances in computer vision and natural language processing make significant progress in visual question answering. In the visual question answering task, the visual representation is essential for understanding the image content. However, traditional methods rarely exploit the context information of the visual feature related to the question and the relation-aware information to capture valuable visual representation. Therefore, a gated relation-aware model is proposed to capture the enhanced visual representation for desiring answer prediction. The gated relation-aware module can learn relation-aware information between the visual feature and the context, and a certain object of an image, respectively. In addition, the proposed module can filter out the unnecessary relation-aware information through the gate guided by the question semantic representation. The results of the conducted experiments show that the gated relation-aware module makes a significant improvement on all answer categories.},
  archive      = {J_IETIP},
  author       = {Xiangjun Shao and Zhenglong Xiang and Yuanxiang Li},
  doi          = {10.1049/ipr2.12421},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1424-1432},
  shortjournal = {IET Image Process.},
  title        = {Visual question answering with gated relation-aware auxiliary},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Reinforcement learning cropping method based on
comprehensive feature and aesthetics assessment. <em>IETIP</em>,
<em>16</em>(5), 1415–1423. (<a
href="https://doi.org/10.1049/ipr2.12420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic image cropping can change the composition to improve the aesthetic quality of the images. Most of the existing automatic image cropping methods based on specific features need to generate a large number of candidate cropping windows. It is very time-consuming and can only produce a limited aspect ratio results. In the face of these situations, a reinforcement learning cropping method based on comprehensive feature and aesthetics assessment is proposed. It does not need to produce a large number of candidate windows. Its gradually cropping mode is more in line with the process of image cropping by human. What is more, the proposed method takes the image aesthetic assessment into consideration. Experimental results show that the proposed method improves the cropping efficiency and achieves excellent cropping effect on the open Flickr Cropping Dataset and CUHK Image Cropping Dataset. The proposed method can overcome the shortages of existing methods.},
  archive      = {J_IETIP},
  author       = {Yaqing Zhang and Xueming Li and Xuewei Li},
  doi          = {10.1049/ipr2.12420},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1415-1423},
  shortjournal = {IET Image Process.},
  title        = {Reinforcement learning cropping method based on comprehensive feature and aesthetics assessment},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Channel splitting attention network for low-light image
enhancement. <em>IETIP</em>, <em>16</em>(5), 1403–1414. (<a
href="https://doi.org/10.1049/ipr2.12418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light enhancement is a crucial task in computer vision because of the limited dynamic range of digital imaging devices in poor lighting conditions. Images taken under low-light conditions often suffer from insufficient brightness and severe noise. At present, many models based on convolutional neural networks have been proposed to enhance low-light images. However, most models treat the features on different channels equally, which is not conducive to models learning hierarchical features. Consequently, the method proposed a channel splitting attention network (CSAN) that divides the shallow features into two branches, the residual and dense branches, transmitting different information. Residual branching facilitates feature reuse, while dense branching promotes the exploration of new features. In addition, CSAN uses merge-and-run mappings to assist information integration between different branches and distinguishes the information contained in different branch features through an attention module designed in this paper. Multiple experiment results show that the method proposed is superior to state-of-the-art methods in qualitative and quantitative evaluation. Furthermore, CSAN can better suppress chromaticity aberration while enhancing low-light images.},
  archive      = {J_IETIP},
  author       = {Bibo Lu and Zebang Pang and Yanan Gu and Yanmei Zheng},
  doi          = {10.1049/ipr2.12418},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1403-1414},
  shortjournal = {IET Image Process.},
  title        = {Channel splitting attention network for low-light image enhancement},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-scale learning method with dilated convolutional
network for concrete surface cracks detection. <em>IETIP</em>,
<em>16</em>(5), 1389–1402. (<a
href="https://doi.org/10.1049/ipr2.12417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concrete surface cracks detection is an important task to ensure the safety of infrastructure. Because of the complexity of background and low contrast of concrete surface, it is difficult to detect the cracks on the concrete surface accurately. To tackle this problem, a multi-scale dilated convolutional method for concrete surface crack detection is proposed to improve the accuracy of detection. The proposed network is based on the encoder-decoder structure of U-Net. Cascade multi-scale dilated convolutions in the centre of the network is used to get larger receptive field without additional parameters. In the decoder stage, the feature fusion module is used to integrated the multi-scale and multi-level side network feature for the final prediction. A large crack dataset is collected as a training set and other three smaller datasets are used for evaluation. Extensive experiments have been conducted on these three crack datasets, which achieves optimal dataset scale ( ODS ) F-score over 0.84, optimal image scale ( OIS ) F-score over 0.85 and average precision ( AP ) over 0.86. This algorithm performs better than the current crack detection, edge detection and semantic segmentation methods.},
  archive      = {J_IETIP},
  author       = {Qiang Zhou and Zhong Qu and Fang-rong Ju},
  doi          = {10.1049/ipr2.12417},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1389-1402},
  shortjournal = {IET Image Process.},
  title        = {A multi-scale learning method with dilated convolutional network for concrete surface cracks detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BAM: Block attention mechanism for OCT image classification.
<em>IETIP</em>, <em>16</em>(5), 1376–1388. (<a
href="https://doi.org/10.1049/ipr2.12415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic retinopathy attracts considerable research interest due to the number of diabetic patients increasing rapidly in recent years. Diabetic retinopathy is a common symptom of retinopathy, which damages the patient&#39;s eyesight and even causes the patient to lose sight. The authors propose a novel attention mechanism named block attention mechanism to actively explore the role of attention mechanisms in recognizing retinopathy features. Specifically, the block attention mechanism contributions are as follows: (1) The relationship between the blocks in the entire feature map is explored, and the corresponding coefficients are assigned to different blocks to highlight the importance of blocks. (2) Furthermore, the relationship between the edge elements of the feature map and the edge elements is explored, and corresponding coefficients are assigned to the elements at different positions on the feature map to highlight the importance of the elements in the feature map. Experimental results show that the proposed framework outperforms the existing popular attention-based baselines on two public retina datasets, OCT2017 and SD-OCT, achieving a 99.64% and 96.54% accuracy rate, respectively.},
  archive      = {J_IETIP},
  author       = {Maidina Nabijiang and Xinjuan Wan and Shengsong Huang and Qi Liu and Bixia Wei and Jianing Zhu and Xiaodong Xie},
  doi          = {10.1049/ipr2.12415},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1376-1388},
  shortjournal = {IET Image Process.},
  title        = {BAM: Block attention mechanism for OCT image classification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust face recognition for occluded real-world images using
constrained probabilistic sparse network. <em>IETIP</em>,
<em>16</em>(5), 1359–1375. (<a
href="https://doi.org/10.1049/ipr2.12414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the occluded real-world face images across illumination, pose, expression, and resolution variations, a robust face recognition for occluded real-world images using constrained probabilistic sparse network is presented. A constrained probabilistic sparse representation network is constructed to obtain the features of all the training images from a global perspective, and the new network nodes are generated through the random combination of the training images. In the probabilistic sparse representation network, the probabilities of each class of the sparse subspace that the occluded test images individually belong to are defined and calculated. The final classifications of the test images are determined by the joint maximum probability of the network nodes. Meanwhile the second-order gradient constraint is the first introduced in the probabilistic sparse representation network. It is found that the constraint uses the adjacent pixels of the face images to obtain the local texture similarity, and further use the local texture similarity to distinguish the occlusion and non-occlusion parts. Thus the constraint can reduce the influence of the occlusion part on face recognition. Extensive experiments with the 12 existing methods on the five face databases demonstrate that the recognition rate of the proposed method is the best than the non-deep learning methods compared, and the proposed method can obtain nearly the same recognition rate with an advantage of a very less time consumption compared to the state-of-the-art deep learning methods.},
  archive      = {J_IETIP},
  author       = {Xiang Ma and Qinqin Ma and Qian Ma and Xiao Han},
  doi          = {10.1049/ipr2.12414},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1359-1375},
  shortjournal = {IET Image Process.},
  title        = {Robust face recognition for occluded real-world images using constrained probabilistic sparse network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving visual multi-object tracking algorithm via
integrating GM-PHD and correlation filter. <em>IETIP</em>,
<em>16</em>(5), 1349–1358. (<a
href="https://doi.org/10.1049/ipr2.12413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional visual multi-object tracking methods based on the Gaussian mixture probability hypothesis density filter are generally not well adapted for tracking the targets in the complex scenarios, where there are a large number of unknowable newborn objects and occluded objects, even some missing objects cannot be associated with their previous trajectories when they are redetected. An improved visual multi-object tracking algorithm is proposed by integrating an improved efficient convolution operator of the correlation filter and the Gaussian mixture probability hypothesis density filter. First, a similarity matrix based on the intersection-of-union is proposed for classifying the objects of survival objects, newborn objects, and then the improved efficient convolution operator method is employed to further identify whether the objects disappear or are missing. Moreover, the feature pyramid similarity is proposed to update the objects for enhancing the tracking accuracy. Finally, compared with some challenging methods on some challenging video sequences from publicly available MOT17 dataset, the proposed Gaussian mixture probability hypothesis density–feature pyramid similarity—efficient convolution operator* method has a good performance on detecting the newborn objects, occluded objects, blurring objects and re-identifying the missing objects with higher multiple object tracking accuracy.},
  archive      = {J_IETIP},
  author       = {Jinlong Yang and Peng Ni and Jiani Miao and Hongwei Ge},
  doi          = {10.1049/ipr2.12413},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1349-1358},
  shortjournal = {IET Image Process.},
  title        = {Improving visual multi-object tracking algorithm via integrating GM-PHD and correlation filter},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MAMask: Multi-feature aggregation instance segmentation with
pyramid attention mechanism. <em>IETIP</em>, <em>16</em>(5), 1341–1348.
(<a href="https://doi.org/10.1049/ipr2.12412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance segmentation is a fundamental yet challenging vision task. Recently, many instance segmentation methods have attempted to use attention mechanisms to improve model efficiency. However, these methods still ignore the problem of information loss in lateral connection of Feature Pyramid Networks (the supplement operation of low-resolution, semantically strong features in FPN). The paper presents an effective detection-based approach named MAMask, which is closely tied to the one-stage method, Fully Convolutional One-Stage Object Detection (FCOS). In particular, it adopts the multi-feature aggregation decoder with pyramid integrate attention (PIA) to instance segmentation. The pyramid integrate attention block can prevent the loss of important channel information by learning richer multi-scale representation. Meanwhile, it also brings significant improvements in performance for existing FPN-based frameworks at slight additional computational costs. The proposed MAMask achieves 37.3% in box AP on the COCO dataset. The method outperforms a few recent methods without longer training time. Compared with the current typical algorithms, the proposed method has achieved excellent performance.},
  archive      = {J_IETIP},
  author       = {Gaihua Wang and Jinheng Lin and Qianyu Zhai and Lei Cheng and Yingying Dai and Tianlun Zhang},
  doi          = {10.1049/ipr2.12412},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1341-1348},
  shortjournal = {IET Image Process.},
  title        = {MAMask: Multi-feature aggregation instance segmentation with pyramid attention mechanism},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved iteratively reweighted least squares algorithms for
sparse recovery problem. <em>IETIP</em>, <em>16</em>(5), 1324–1340. (<a
href="https://doi.org/10.1049/ipr2.12411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, some new algorithms based on the iteratively reweighted least squares (IRLS) method are proposed for sparse recovery problem. There are two important parameters in the IRLS method: a weighted parameter and a regularization parameter. On the one hand, in order to improve the performance of IRLS method, a new way is given to update the weight vector. On the other hand, for the regularization parameter, three new update methods are introduced to avoid the phenomenon that the regularization parameter drops too fast. Then, some improved iteratively reweighted least squares (IIRLS) algorithms are proposed, and their convergence and convergence rate are analyzed. The local convergence of our algorithms is superlinear and approaches a quadratic rate in special cases. Finally, a large number of algorithms are compared in solving the sparse recovery problem, including IRLS methods and IIRLS methods with different weighting parameters and regularization parameters, and certain iterative methods. The experimental results demonstrate that the proposed methods are efficient and promising.},
  archive      = {J_IETIP},
  author       = {Yufeng Liu and Zhibin Zhu and Benxin Zhang},
  doi          = {10.1049/ipr2.12411},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1324-1340},
  shortjournal = {IET Image Process.},
  title        = {Improved iteratively reweighted least squares algorithms for sparse recovery problem},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Focal learning on stranger for imbalanced image
segmentation. <em>IETIP</em>, <em>16</em>(5), 1305–1323. (<a
href="https://doi.org/10.1049/ipr2.12410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is an open issue to train effective deep network models on class imbalance datasets. In the widely used cost-sensitive imbalanced learning methods, the costs are based on the losses or class probabilities of samples. In this paper, it is discovered that these traditional cost-sensitive methods discard the clustering feature, and introduce the errors of annotations into costs, leading to sub-optimal models. It is further investigated that the feature magnitude of sample, which is computed before probability and loss, not only is independent of the annotation, but also represents the familiarity degree of model with the sample. These characteristics of feature magnitude are used to guide the training and inference of model. First, the concept of stranger is proposed, which is the sample with small feature magnitude value, and the idea of focal learning on strangers (FLS) is proposed. By adding the idea of FLS into two existing cost-sensitive methods, two novel losses are put forward: instance-level focal stranger loss (IFSL) and class-level focal stranger loss (CFSL). The losses can improve the aggregation features of samples within class, and reduce the negative influences of annotation errors on imbalanced learning. Second, considering the large difference of feature magnitude means between minority class and majority class in case of extreme class-imbalance dataset, a bias determination (BD) strategy is put forward to improve classification performance during inference. The methods are applied to the tasks of image semantic segmentation and salient-instance segmentation. The experimental results on four public semantic segmentation datasets demonstrate that IFSL can reduce the over-fitting of model, improve the classification accuracy of rare samples, and alleviate the reliance of performance on the annotation quality. The experimental results on two public salient-instance segmentation datasets show that CFSL makes the model have better scoring ability for salient object. Besides, the BD strategy can reduce the wrong classification caused by bias model. Therefore, the proposed methods can significantly advance image segmentation.},
  archive      = {J_IETIP},
  author       = {Yaochi Zhao and Shiguang Liu and Zhuhua Hu},
  doi          = {10.1049/ipr2.12410},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1305-1323},
  shortjournal = {IET Image Process.},
  title        = {Focal learning on stranger for imbalanced image segmentation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantically guided self-supervised monocular depth
estimation. <em>IETIP</em>, <em>16</em>(5), 1293–1304. (<a
href="https://doi.org/10.1049/ipr2.12409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth information plays an important role in the vision-related activities of robots and autonomous vehicles. An effective method to obtain 3D scene information is self-supervised monocular depth estimation, which utilizes large and diverse monocular video datasets during the training process without the need for ground-truth data. A novel multi-task learning strategy that uses semantic information to guide the monocular depth estimation method while maintaining self-supervision is proposed. An improved differential direct visual odometer (DDVO) combined with Pose-Net is applied for achieving better pose prediction. Minimum reprojection loss with auto-masking and semantic masking is used to remove the effects of low-texture areas and moving dynamic-class objects within scenes. Concurrently, the semantic masking is introduced into the DDVO pose predictor to filter moving objects and reduce the matching error between monocular sequence frames. In addition, PackNet is employed as the backbone of multi-task learning to further improve the accuracy of deep prediction. The proposed method produces state-of-the-art results for monocular depth estimation on the KITTI Eigen split benchmark, even outperforming supervised methods that have been trained using ground-truth depth.},
  archive      = {J_IETIP},
  author       = {Xiao Lu and Haoran Sun and Xiuling Wang and Zhiguo Zhang and Haixia Wang},
  doi          = {10.1049/ipr2.12409},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1293-1304},
  shortjournal = {IET Image Process.},
  title        = {Semantically guided self-supervised monocular depth estimation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding and modeling finger vascular pattern imaging.
<em>IETIP</em>, <em>16</em>(5), 1280–1292. (<a
href="https://doi.org/10.1049/ipr2.12408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, new insights in the near infrared imaging process used in finger-vein recognition by developing a physical model are presented. A realistic phantom finger that mimics the living human finger and also includes veins has been developed to validate this model. NIR phantom finger images show that the phantom can emulate the optical properties of a living human finger and can provide ground truth for the locations of the veins. Through physical modeling, it is particularly learned that—besides blood and soft tissue—bone also plays an important role in generating reliable NIR finger-vein images.},
  archive      = {J_IETIP},
  author       = {Pesigrihastamadya Normakristagaluh and Geert Jan Laanstra and Luuk Spreeuwers and Raymond Veldhuis},
  doi          = {10.1049/ipr2.12408},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1280-1292},
  shortjournal = {IET Image Process.},
  title        = {Understanding and modeling finger vascular pattern imaging},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic and context features integration for robust object
tracking. <em>IETIP</em>, <em>16</em>(5), 1268–1279. (<a
href="https://doi.org/10.1049/ipr2.12407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Siamese network-based object tracking learns features of a target object marked in the first frame and that of the object in subsequent frames simultaneously and then measures similarity between two features to recognize and locate the object. Owing to their efficiency and high accuracy, Siamese networks have attracted much attention recently. However, tracking accuracy decreases significantly when there are scale changes, occlusion, and pose variations due to the way that Siamese networks estimate feature similarity. To address this issue, the authors propose a tracking algorithm, named Semantic and context features integration for robust object tracking that integrates local and global features of the object. Local features provide context information for tracking parts of the object, while global features contain semantic information for tracking the object. The authors meticulously design local and global classification and regression heads and integrate them into one uniform framework to achieve integration tracking. This method effectively alleviates low accuracy in complex scenes such as scale changes, deformation, and occlusion. Numerous experiments demonstrate that this method achieves state-of-art (SOTA) performance with 45 FPS on a single RTX2060 Super GPU on public tracking datasets, including VOT2016, VOT2019, OTB100, GOT-10k, and LaSOT, and its effectiveness and efficiency is confirmed.},
  archive      = {J_IETIP},
  author       = {Jinzhen Yao and Jianlin Zhang and Zhixing Wang and Linsong Shao},
  doi          = {10.1049/ipr2.12407},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1268-1279},
  shortjournal = {IET Image Process.},
  title        = {Semantic and context features integration for robust object tracking},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Medical image segmentation using deep learning: A survey.
<em>IETIP</em>, <em>16</em>(5), 1243–1267. (<a
href="https://doi.org/10.1049/ipr2.12419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been widely used for medical image segmentation and a large number of papers has been presented recording the success of deep learning in the field. A comprehensive thematic survey on medical image segmentation using deep learning techniques is presented. This paper makes two original contributions. Firstly, compared to traditional surveys that directly divide literatures of deep learning on medical image segmentation into many groups and introduce literatures in detail for each group, we classify currently popular literatures according to a multi-level structure from coarse to fine. Secondly, this paper focuses on supervised and weakly supervised learning approaches, without including unsupervised approaches since they have been introduced in many old surveys and they are not popular currently. For supervised learning approaches, we analyse literatures in three aspects: the selection of backbone networks, the design of network blocks, and the improvement of loss functions. For weakly supervised learning approaches, we investigate literature according to data augmentation, transfer learning, and interactive segmentation, separately. Compared to existing surveys, this survey classifies the literatures very differently from before and is more convenient for readers to understand the relevant rationale and will guide them to think of appropriate improvements in medical image segmentation based on deep learning approaches.},
  archive      = {J_IETIP},
  author       = {Risheng Wang and Tao Lei and Ruixia Cui and Bingtao Zhang and Hongying Meng and Asoke K. Nandi},
  doi          = {10.1049/ipr2.12419},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1243-1267},
  shortjournal = {IET Image Process.},
  title        = {Medical image segmentation using deep learning: A survey},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MFAUNet: Multiscale feature attentive u-net for cardiac MRI
structural segmentation. <em>IETIP</em>, <em>16</em>(4), 1227–1242. (<a
href="https://doi.org/10.1049/ipr2.12406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate and robust automatic segmentation of cardiac structures in magnetic resonance imaging (MRI) is significant in calculating cardiac clinical functional indices, and diagnosing heart diseases. Most U-Net based methods use pooling, transposed convolution, and skip connection operations to integrate the multiscale features for improved segmentation in cardiac MRI. However, this architecture lacks adequate semantic connection between the channel and spatial information, and robustness in segmenting objects with significant shape variations. In this paper, a new multiscale feature attentive U-Net for cardiac MRI structural segmentation method is proposed. An attention mechanism is adopted after concatenating the multi-level features to aggregate different scale features and determine on which features to focus. Cascade and parallel dilated convolution is also employed in the decoder blocks and skip connection is employed to enhance the ability of sensing receptive fields for multiscale context information. Furthermore, deep supervision approach with a loss function that combines the dice and cross-entropy losses to reduce overfitting and ensure better prediction is introduced. The proposed method was evaluated on three public cardiac datasets. The experimental results indicate that the method achieved competitive segmentation performance with the three datasets, which verifies the robustness and generalisability of the proposed network. In comparison with conventional U-Net methods, the model leverages attention mechanism and dilated convolution block, which increases the semantic connection between the channel and the spatial information, and improves the robustness of the right ventricle segmentation performance. From the view of the Dice scores and segmentation results, the multiscale feature attentive U-Net method is one of effective methods in segmenting cardiac MRI structures.},
  archive      = {J_IETIP},
  author       = {Dapeng Li and Yanjun Peng and Yanfei Guo and Jindong Sun},
  doi          = {10.1049/ipr2.12406},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1227-1242},
  shortjournal = {IET Image Process.},
  title        = {MFAUNet: Multiscale feature attentive U-net for cardiac MRI structural segmentation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Selective part-based correlation filter tracking algorithm
with reinforcement learning. <em>IETIP</em>, <em>16</em>(4), 1208–1226.
(<a href="https://doi.org/10.1049/ipr2.12405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In visual object tracking methods, improving both the run time and the accuracy in the face of complex situations has always been an important issue. Many complex tracking algorithms, such as part-based algorithms, have better accuracy when facing occlusions, but they have much greater computational complexity. In response to the above problems, this paper proposes a selective part-based correlation filter (SPCF) tracking algorithm with a reinforcement learning to achieve more stable and efficient tracking of targets. First, according to the conditions of the response map of the correlation filter (CF), the entire tracking process is divided into three states: simple environments, complex environments, and harsh environments. Second, this paper uses reinforcement learning to determine the states of frames in different situations to improve the tracking effect of the algorithm. Third, the process of the online selection of states is transformed into a Markov decision process (MDP), where the policy learning of the MDP is achieved by reinforcement learning. Additionally, different strategies are used to track a target in different states: the overall filter is used to increase the speed in simple environments; part-based filters are used to improve the accuracy in complex environments; and in harsh environments where the target completely disappears, a redetection algorithm is used to find the target when it reappears. Finally, the performance of the tracking algorithm is verified on the VOT2018, OTB-2015, and LaSOT datasets.},
  archive      = {J_IETIP},
  author       = {Zhengzhi Lu and Guoan Yang and Deyang Liu and Junjie Yang and Yong Yang and Chuanbo Zhou},
  doi          = {10.1049/ipr2.12405},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1208-1226},
  shortjournal = {IET Image Process.},
  title        = {Selective part-based correlation filter tracking algorithm with reinforcement learning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A scale-sensitive heatmap representation for multi-person
pose estimation. <em>IETIP</em>, <em>16</em>(4), 1194–1207. (<a
href="https://doi.org/10.1049/ipr2.12404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-person pose estimation is a challenging vision task that can be seriously affected by keypoint scale variation. Existing heatmap-based approaches are devoted to reducing the effect by optimizing backbone architecture or loss functions, but the problem of an inaccurate heatmap representation with different keypoint scales still exists. A scale-sensitive heatmap algorithm is presented to generate reasonable spatial and contextual features for the network to predict more precise coordinates, by systematically considering the standard deviation, truncated radius, and shape of Gaussian kernels. Specifically, the scale-sensitive heatmap algorithm contains three parts: inter-person heatmap, limited-area heatmap, and shape-aware heatmap. The inter-person heatmap allocates different standard deviations for each human instance proportionally calculated by the keypoint-based method, the limited-area heatmap defines the truncated radius to limit the influence area of Gaussian kernels, and the shape-aware heatmap modifies the Gaussian kernels generated by some ellipse-shaped joints. Our scale-sensitive heatmap algorithm outperforms the baseline by a considerable margin on the COCO and CrowdPose benchmark datasets. The code and pretrained models are available at https://github.com/ducongju/Scale-sensitive-Heatmap .},
  archive      = {J_IETIP},
  author       = {Congju Du and Han Yu and Li Yu},
  doi          = {10.1049/ipr2.12404},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1194-1207},
  shortjournal = {IET Image Process.},
  title        = {A scale-sensitive heatmap representation for multi-person pose estimation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised image super-resolution with attention
CycleGAN. <em>IETIP</em>, <em>16</em>(4), 1181–1193. (<a
href="https://doi.org/10.1049/ipr2.12401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-Image Super-Resolution (SISR) has always been an important topic in the field of image processing, which attempts to improve the image resolution and is of great significance in practice. Recently, SISR has made substantial progress aided by deep learning (DL), which has demonstrated impressive potential in many low-level tasks. In the current DL-based SISR approaches, most of them are based on supervised learning. However, in the real world, only low-resolution (LR) images with unknown degradation are provided, which limit the application of current supervised models. To mitigate this problem, in this paper, a two-stage semi-supervised SISR method called SRAttentionGAN, is proposed. First, an upsampling network SRResNet, which is pre-trained in a supervised manner, is employed to scale the LR image to the desired size. Then, the upsampled results are fed into our improved unsupervised CycleGAN framework, which does not need paired samples, to obtain sharper and more realistic super-resolution (SR) images. Specifically, in the improved CycleGAN part, an attention-guided generator is proposed to perceive the discriminative semantic parts between the source and target images, to avoid the impact from low-level information. It also prevents the overall color tone from being changed. A multi-scale discriminator is also adopted to further rich texture details. The effectiveness of the proposed SRAttentionGAN experiments is validated using four benchmarks (Set5, Set14, Urban100, and BSDS100) in both quantitative and qualitative aspects. Compared with the state-of-the-arts, the results are visually promising and show competitive performance in perceptual metrics, Natural Image Quality Evaluator (NIQE) and Perception Index (PI), which have better agreement with the human visual perception.},
  archive      = {J_IETIP},
  author       = {Mingzheng Hou and Xudong He and Furong Dou and Xin Zhang and Zhaokang Guo and Ziliang Feng},
  doi          = {10.1049/ipr2.12401},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1181-1193},
  shortjournal = {IET Image Process.},
  title        = {Semi-supervised image super-resolution with attention CycleGAN},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic manifold boltzmann optimization based on
self-supervised learning for human motion estimation. <em>IETIP</em>,
<em>16</em>(4), 1162–1180. (<a
href="https://doi.org/10.1049/ipr2.12400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a challenge work to estimate the 3D human motion from image sequence. There are some problems, such as unsatisfactory estimation error, ambiguous matching and transient occlusion. Although the prior information of learning large-scale samples exists, these problems are still difficult to be solved. How to extract the feature of the high-dimensional (HD) sample of 3D human motion and find the desired one will become the key to solve these problems above. Some dimension reduction methods can extract the sample features and build the low-dimensional (LD) space to view their LD features, but how to search the relevant valid and desired LD samples remains the bottleneck problem, which can be used to reconstruct the 3D human motions denoted by the corresponding high-dimensional samples. Thus, a new method called dynamic manifold Boltzmann optimization (DMBO) is proposed to estimate the 3D human motion from multi-view images. DMBO can find the best matching 3D human motion model by the help of the self-supervised learning from Gaussian incremental dimension reduction model (GIDRM). DMBO can avoid the local optimum during searching and solve the problems above, so that the generation of the accurate 3D human motion corresponding to multi-view images can be achieved.},
  archive      = {J_IETIP},
  author       = {Wanyi Li and Yuqi Zeng and Yilin Wu and Qian Zhang and Guoming Chen and Yongchang Chen},
  doi          = {10.1049/ipr2.12400},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1162-1180},
  shortjournal = {IET Image Process.},
  title        = {Dynamic manifold boltzmann optimization based on self-supervised learning for human motion estimation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Structure-aware multiple salient region detection and
localization for autonomous robotic manipulation. <em>IETIP</em>,
<em>16</em>(4), 1135–1161. (<a
href="https://doi.org/10.1049/ipr2.12399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a multiple salient region detection and localization approach for unstructured industrial robot work environments with arbitrarily located and orientated objects. Different from the existing, the authors&#39; novel technique to detect multiple salient regions performs locally adaptive center-surround operations on proto-object partitions obtained through color consistency and spatial proximity analysis. The multi-scale center-surround operations are done by masks that are local structure-aware yielding regions with precise and accurate boundaries as required for robotic manipulation. First, experiments to evaluate the multiple salient region detection performance are carried out using four standard databases having images with multiple salient objects. Quantitative result analysis using F-measure, shuffled F-measure, shuffled AUC and MAE, and subjective result inspection suggests that the proposed approach is in general better at collectively detecting multiple salient regions than the state-of-the-art, including those based on deep learning. Then, real-life experiments involving robotic manipulation are carried out to demonstrate the utility of the multiple salient region detection method. For robotic manipulation, object localization is improved after salient region detection by employing a fast shadow detection algorithm proposed based on hue analysis, and recognition through existing matching techniques is applied only at the localized salient regions. The benefit of the novel multiple salient region detection approach in the robotic manipulation system is shown using localization and pose estimation accuracy, rates of detection and recognition, positional and angular errors, and processing speed.},
  archive      = {J_IETIP},
  author       = {Sudipta Bhuyan and Debashis Sen and Sankha Deb},
  doi          = {10.1049/ipr2.12399},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1135-1161},
  shortjournal = {IET Image Process.},
  title        = {Structure-aware multiple salient region detection and localization for autonomous robotic manipulation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A high-performance insulators location scheme based on
YOLOv4 deep learning network with GDIoU loss function. <em>IETIP</em>,
<em>16</em>(4), 1124–1134. (<a
href="https://doi.org/10.1049/ipr2.12392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a Gaussian Distance Intersection over Union (GDIoU) loss function-based YOLOv4 deep learning network to solve the problem of slow speed and low accuracy insulator location in power facilities health inspection. In the scheme, A GDIoU loss function is designed to accelerate the convergence speed of the YOLOv4 deep learning network; at the same time, the GDIoU loss is added as one part of the network propagation loss, and the insulator&#39;s location accuracy is accordingly improved. Moreover, a re-location scheme for tilt insulators correction is proposed to enhance the location accuracy of the insulators in different spatial angle states. Large amounts of field insulator images were gathered as training and testing samples to evaluate the performance of the proposed scheme. The experimental results have demonstrated that the GDIoU-based YOLOv4 deep learning network combined with the tilt correction scheme can improve the insulator location speed by three times compared with the peer schemes, and the average precision is increased by 7.37% compared with the naive YOLOv4 network. The performance of the proposed scheme meets the requirement of online insulator location adequately.},
  archive      = {J_IETIP},
  author       = {Bin Ma and Yongkang Fu and Chunpeng Wang and Jian Li and Yuli Wang},
  doi          = {10.1049/ipr2.12392},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1124-1134},
  shortjournal = {IET Image Process.},
  title        = {A high-performance insulators location scheme based on YOLOv4 deep learning network with GDIoU loss function},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel image quality assessment method and coefficient of
quality for digital solutions of colour blindness. <em>IETIP</em>,
<em>16</em>(4), 1111–1123. (<a
href="https://doi.org/10.1049/ipr2.12213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eyesight is one of the primary senses that human beings have. Reports show that colour blindness, a form of colour vision deficiency (CVD), affects about 8% of the male population and 0.5% of female population. The Assistive Technology Act of 2004 lays focus on technologies that help individuals with disabilities and deficiencies. With the rapid advancement in technologies, several assistive solutions are available for visually impaired or CVD patients. Such solutions involve simulation and compensation of conflicting colours to help the colour blind in the visual perception of colours. Given the increased usage of the web, post the pandemic, these solutions improve the quality of life for the colour blind. Defining the image quality assessment criteria for such digital solutions becomes imperative. The study proposes a novel method for image quality assessment of digital solutions aimed at assisting the colour blind users. The proposed coefficient of quality ( CQ ) would be useful to rank colour compensation and recolouring algorithms. Experiments were conducted with a novel questionnaire set designed for this quality measurement. The results affirm the efficiency of the assessment method proposed. This will also provide objective feedback to the researchers and experts in this area to improve their solutions for CVD patients.},
  archive      = {J_IETIP},
  author       = {Meenakshi S and Anshu Singla},
  doi          = {10.1049/ipr2.12213},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1111-1123},
  shortjournal = {IET Image Process.},
  title        = {A novel image quality assessment method and coefficient of quality for digital solutions of colour blindness},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-quality reversible data hiding scheme using sorting and
enhanced pairwise PEE. <em>IETIP</em>, <em>16</em>(4), 1096–1110. (<a
href="https://doi.org/10.1049/ipr2.12212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel reversible data hiding (RDH) technique using sorting and pairwise prediction error expansion (PEE) to improve embedding capacity (EC) while retaining the quality of cover image. The proposed scheme traverses alternate pixels of the cover image in a zig-zag order to construct two independent sets for sequential embedding. The pixels of each set are sorted in an increasing order of their rhombus mean followed by a two-pass data embedding by dividing the sets into 1 × 3 size blocks based on some pre-defined criteria. In pass 1, two prediction errors are calculated for the first and the last pixels using their rhombus means; and pairwise mapping is modified and exploited to embed the secret data in such a way that the value of the first pixel is either increased or remains unchanged, and the value of the last pixel is either decreased or remains unchanged. In pass-2, the middle pixel is utilised to predict the first and last pixels and the values of the first pixel and the last pixel are either decreased or remains unchanged and either increased or remains unchanged, respectively. In contrast to some existing recovery-based methods, the proposed pass-2 guarantees to complement the changes made in pass-1, thereby boosting the quality along with increased EC. The experimental results show that the proposed method achieves better embedding performance than the state-of-the-art RDH techniques. More specifically, the proposed method gets an increment by an average of 0.46 dB and 1.01 dB for embedding 10,000 bits and 20,000 bits respectively over its closest prior art.},
  archive      = {J_IETIP},
  author       = {Gurjinder Kaur and Samayveer Singh and Rajneesh Rani and Rajeev Kumar and Aruna Malik},
  doi          = {10.1049/ipr2.12212},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1096-1110},
  shortjournal = {IET Image Process.},
  title        = {High-quality reversible data hiding scheme using sorting and enhanced pairwise PEE},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CPC-GSCT: Visual quality assessment for coloured point cloud
based on geometric segmentation and colour transformation.
<em>IETIP</em>, <em>16</em>(4), 1083–1095. (<a
href="https://doi.org/10.1049/ipr2.12211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coloured point cloud (CPC) is one of the important representations of three-dimensional objects, which has been used in many fields. CPC may encounter geometric and colour distortion during its compression, simplification or other processing. Thus, the objective visual quality assessment of CPC is one of the urgent issues to be resolved in the CPC&#39;s applications. Aiming at this problem, this paper proposes a new full-reference visual quality assessment metric for CPC based on geometric segmentation and colour transformation (CPC-GSCT), which analyzes geometric distortion and colour distortion of CPC. First, considering the visual masking effect of CPC&#39;s geometric information, CPC is segmented into different regions and distributed with different weights to describe the influence of visual masking effect in CPC quality assessment. At the same time, a geometric combination feature vector is defined and extracted for measuring the CPC&#39;s geometric distortion. Then, considering the colour perception of human eyes, a colour combination feature vector is extracted to measure the CPC&#39;s colour distortion in HSV colour space. Finally, all the extracted geometric and colour features are constituted as a feature vector to predict the quality of CPC. Experimental results on three databases (IRPC, SJTU-PCQA and CPCD2.0) show that the proposed CPC-GSCT metric can achieve better performance in predicting the visual quality of CPC than relevant existing methods.},
  archive      = {J_IETIP},
  author       = {Lei Hua and Mei Yu and Zhouyan He and Renwei Tu and Gangyi Jiang},
  doi          = {10.1049/ipr2.12211},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1083-1095},
  shortjournal = {IET Image Process.},
  title        = {CPC-GSCT: Visual quality assessment for coloured point cloud based on geometric segmentation and colour transformation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhancement algorithm for high visibility of underwater
images. <em>IETIP</em>, <em>16</em>(4), 1067–1082. (<a
href="https://doi.org/10.1049/ipr2.12210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement has been attracting researchers in present day scenario for exploring the marine life. But underwater images suffer from various glitches like haziness, low contrast and faded colours due to absorption and scattering properties of light in water. To overcome these issues, the present paper proposes an enhancement method for underwater images. The proposed enhancement method comprises of three steps, i.e. automatic white balancing, dehazing and Rayleigh stretching in spatial domain. Automatic white balancing technique removes the colour cast from underwater images. Haze removal algorithm efficiently overcomes the haziness but it also reduces the local contrast and making the image appear dull. For improving the dehazing result and the visual quality simultaneously, the proposed method process the image by histogram stretching technique based on Rayleigh distribution. The proposed method corrects the colour cast and improves the contrast along with removing the haze from underwater images. Subjective and objective analysis on U 45 $U45$ dataset validates the efficiency of proposed method and the enhanced images achieve better visual quality as compared to the existing methods. High values of EMEE, EME, UIQM and UCIQE for different types of underwater images further proves the potential and efficacy of the proposed method.},
  archive      = {J_IETIP},
  author       = {Monika Mathur and Nidhi Goel},
  doi          = {10.1049/ipr2.12210},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1067-1082},
  shortjournal = {IET Image Process.},
  title        = {Enhancement algorithm for high visibility of underwater images},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Goal oriented image quality assessment. <em>IETIP</em>,
<em>16</em>(4), 1054–1066. (<a
href="https://doi.org/10.1049/ipr2.12209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The area of image quality assessment(IQA) is an active research area in image processing and computer vision. All IQA algorithms reported in literature are attempting to quantify only the visual quality of the images/videos. An interesting question to be answered is that given a goal (task) and an image (with good visual quality), is the image good to achieve the goal by the best possible algorithm. In an attempt to spur the research community to answer this question, a new paradigm of IQA, called goal oriented IQA (GO-IQA), is introduced. GO-IQA is defined as given an image and a goal, predicting how good the image is to achieve that goal by the best possible algorithm. The need for GO-IQA is that if GO-IQA score is less, then any arbitrary algorithm attempting to achieve the goal will eventually fail. In this paper, considering segmentation as goal, a GO-IQA algorithm is proposed to predict how good the image is to do accurate segmentation by the best possible segmentation algorithm. A support vector regression model has been trained with features related to image segmentation along with the accuracies of a class of well-known image segmentation algorithms. In the process, we have obtained encouraging results in terms of the Pearson linear correlation coefficient and mean squared error. The predictions given by the proposed model have been validated with the scores of state of the art segmentation algorithm.},
  archive      = {J_IETIP},
  author       = {Kiruthika S. and Masilamani V.},
  doi          = {10.1049/ipr2.12209},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1054-1066},
  shortjournal = {IET Image Process.},
  title        = {Goal oriented image quality assessment},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Research on fatigue detection based on visual features.
<em>IETIP</em>, <em>16</em>(4), 1044–1053. (<a
href="https://doi.org/10.1049/ipr2.12207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high incidence of traffic accidents brings immeasurable losses to life. In order to avoid such crises, researchers and automakers have used many methods to solve this problem. Among them, technology based on visual features is widely used in driver fatigue detection. As fatigue detection plays a vital role in the driving process, the high accuracy of fatigue monitoring is very important. This paper focuses on the method based on convolutional neural network to detect driver fatigue. First, in the face detection part, the Single-Shot Multi-Box Detector algorithm is used to improve the speed and accuracy of face detection to extract the eye and mouth regions; second, the VGG16 network is used to learn fatigue features, which is performed on the NTHU-Drowsy Driver Detection (NTHU-DDD) data set and the other two modified data sets Training test. The main result of this work is that the accuracy of fatigue monitoring is higher than other methods including the original method, with an accuracy rate of over 90%. And it has better generalization ability than the multi-physical feature fusion detection method. At the same time, we propose the fatigue detection method based on convolutional neural network to improve the advanced driver assistance system (ADAS) to make it more robust and reliable decision making.},
  archive      = {J_IETIP},
  author       = {Guangzhe Zhao and Yanqing He and Hanting Yang and Yong Tao},
  doi          = {10.1049/ipr2.12207},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1044-1053},
  shortjournal = {IET Image Process.},
  title        = {Research on fatigue detection based on visual features},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Compressed dual-channel neural network with application to
image-based smoke detection. <em>IETIP</em>, <em>16</em>(4), 1036–1043.
(<a href="https://doi.org/10.1049/ipr2.12205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective detection of smoke from visual scenes can play a vital role not only in industrial safety as an early warning system but also in forest fire prevention. However, it is difficult to detect smoke based on texture and color. Therefore, many researches have been conducted on this issue and derived detection methods based on convolutional neural networks (such as DNCNN and DCNN etc.). However, in the process of convolution, with the superposition of convolutions times, the parameters of the network increase gradually and thus cause a large computational burden, which brings about the problem of unsatisfactory operating efficiency. Thus, this paper mainly introduces the depthwise separable convolution into the state-of-the-art DCNN developed specifically for smoke detection, dubbed as the improved DCNN (IDCNN). Compared with standard convolution, by introducing the depthwise separable convolution, the convolution parameters and the corresponding calculation amount in the process of convolution can be greatly reduced, so that the network can deal with more data in a shorter time which improves operating efficiency. Experimental results demonstrate the effectiveness of IDCNN as compared with the state-of-the-art deep networks for smoke detection based on standard convolution in terms of parameter quantity and running speed.},
  archive      = {J_IETIP},
  author       = {Jiedong Zhang and Wenhui Xie and Hongyan Liu and Wenyi Dang and Anfeng Yu and Di Liu},
  doi          = {10.1049/ipr2.12205},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1036-1043},
  shortjournal = {IET Image Process.},
  title        = {Compressed dual-channel neural network with application to image-based smoke detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Angular-spatial analysis of factors affecting the
performance of light field reconstruction. <em>IETIP</em>,
<em>16</em>(4), 1027–1035. (<a
href="https://doi.org/10.1049/ipr2.12203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new VR multimedia format, Light Field (LF) has received more and more attention. LF can support users to move freely within a certain range during the experience. However, the limitation of LF acquisition technology has become the main obstacle to its large-scale application. Since the current hardware technology cannot produce a lens small enough to meet the needs of LF capture, the intensity of the collected LF image is usually insufficient. Although there are many software LF reconstruction algorithms have been proposed to make captured result denser, they all face the problem of poor generalization in the application process. In this work, it is attempted to locate the factors that affect the performance of LF reconstruction from both the angular and spatial domains. The analysis results show that it is affected not only by the edge and texture of the LF content in the spatial domain, but also by the adjacent disparity and the reflection characteristics in the angular domain. An indicator to quantify the impact of these factors on reconstruction performance is also proposed, which will be helpful to design a new generalized adaptive LF reconstruction algorithm in the future.},
  archive      = {J_IETIP},
  author       = {Xinjue Hu and Lin Zhang},
  doi          = {10.1049/ipr2.12203},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1027-1035},
  shortjournal = {IET Image Process.},
  title        = {Angular-spatial analysis of factors affecting the performance of light field reconstruction},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A blind contour-aware quality model for sonar images.
<em>IETIP</em>, <em>16</em>(4), 1017–1026. (<a
href="https://doi.org/10.1049/ipr2.12202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the penetration depth of visible light is limited in muddy and dark deep marine environment, sonar imaging, which is independent of natural light, has been attracting much attention in underwater detections. However, the visual quality of sonar image is inevitably damaged during the acquisition and the transmission in the sophisticated and dynamic underwater environment. The quality decrease can further bring negative impacts on oceanic information analysis. To measure the quality decrease of sonar images, this paper proposes a contour-aware model for sonar image quality assessment (CaMSIQA). The CaMSIQA metric is established upon the conclusion that the quality of sonar images can be defined as a measure of their utilities in real application scenarios. We exploit a contour information statistic (CIS) model that is critical to object recognition of sonar images here. Quantifying the changes of the CIS model between the unimpaired and test sonar images makes it possible to perceive the quality decrease of sonar images. Extensive experimental results have demonstrated the effectiveness of the proposed method compared with current mainstream image quality assessment (IQA) methods without full reference information.},
  archive      = {J_IETIP},
  author       = {Weiling Chen and Rongfu Lin and Rongxin Zhang and Yi Zhu},
  doi          = {10.1049/ipr2.12202},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1017-1026},
  shortjournal = {IET Image Process.},
  title        = {A blind contour-aware quality model for sonar images},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A photo-based quality assessment model for the estimation of
PM2.5 concentrations. <em>IETIP</em>, <em>16</em>(4), 1008–1016. (<a
href="https://doi.org/10.1049/ipr2.12201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid economic growth has caused severe environmental pollution, which has aroused great concern. This pollution affects public health and impairs visibility, therefore, it should be given greater consideration. In this paper, a photo-based PM2.5 concentration predictor is proposed based on the natural scene statistics without artificial assistance or extra information. Given that the quality of PM2.5 concentration images is determined by many factors, three types of influencing factors are analysed: the colourfulness, the structural degradation and the contrast. The first feature consists of the hue, saturation and colour descriptors, which measure the colourfulness of the PM2.5 concentration images. The second feature is determined based on the contrast can effectively portray the quality of PM2.5 concentration in the images. The third feature is extracted based on the natural scene statistics model, which measures the local and global structural degradation information and the naturalness of the PM2.5 concentration images. Finally, the three features are used to train a random forest model that can be used to predict the concentration of PM2.5. Experimental results illustrate that the performance of the proposed model is better than those of popular competitors on AQID.},
  archive      = {J_IETIP},
  author       = {Kezheng Sun and Lijuan Tang and Shuaifeng Huang and Jiansheng Qian},
  doi          = {10.1049/ipr2.12201},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1008-1016},
  shortjournal = {IET Image Process.},
  title        = {A photo-based quality assessment model for the estimation of PM2.5 concentrations},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robust photo-based PM2.5 monitoring method by combining
linear and non-linear learning. <em>IETIP</em>, <em>16</em>(4),
1000–1007. (<a href="https://doi.org/10.1049/ipr2.12200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Good health is pursued by people all over the world. However, the continual industrialisation has led to more and more atmospheric contamination, and PM 2.5 $_{2.5}$ has caused serious harm to our life safety and living environment. Without increasing the cost of sustainable industrial production, more and more attention has been paid to the related researches on improving PM 2.5 $_{2.5}$ monitoring, prevention and control level. Therefore, it is extremely urgent to establish a robust PM 2.5 $_{2.5}$ monitoring model that can adapt to a variety of scenarios, not only in local places like campuses but also in wide area like city. Existing work has proven that PM 2.5 $_{2.5}$ monitoring can be achieved by means of photos. But experiments show that the stated-of-the-art methods are far from ideal for PM 2.5 $_{2.5}$ monitoring when the author tested the performance in two public datasets. To solve the aforesaid issue, this paper ulteriorly proposes a novel photo-based PM 2.5 $_{2.5}$ monitoring model, which fuses the results of existing methods by firstly using the weighted average based on the least absolute shrinkage and selection operator regression for learning the basic linear component, secondly using the support vector regression for learning the non-linear residual component, and finally incorporating the above two outputs to infer the final PM 2.5 $_{2.5}$ concentration. The main contributions and innovations of this paper are embodied in: (1) the innovative use of image quality assessment model to extract 9 features for PM 2.5 $_{2.5}$ monitoring, (2) separately extract macro information and micro information from PM2.5 pictures, (3) two newly-established large-scaled datasets are applied to verify the effectiveness and robustness of the proposed PM 2.5 $_{2.5}$ monitoring model. Experiments show that on the latest PM 2.5 $_{2.5}$ datasets (local and wide), the proposed model has achieved high performance and demonstrated strong robustness.},
  archive      = {J_IETIP},
  author       = {Zhifang Xia},
  doi          = {10.1049/ipr2.12200},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1000-1007},
  shortjournal = {IET Image Process.},
  title        = {A robust photo-based PM2.5 monitoring method by combining linear and non-linear learning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Sonar image quality evaluation using deep neural network.
<em>IETIP</em>, <em>16</em>(4), 992–999. (<a
href="https://doi.org/10.1049/ipr2.12199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sonar technology plays an important role in the development of marine resources and military strategy. Due to the bad quality of underwater acoustics channels, the sonar images collected by sonar technology equipment are easily affected by various kinds of distortions. To obtain high-quality sonar images, the authors devise a novel dual-path deep neural network (DPDNN) to measure the quality of sonar images. In these two paths, the authors use a batch normalization layer to reduce the training time and use the skip operation to speed up the feature extraction . Based on the above two operations, the authors extract the microscopic and macroscopic structures of sonar images, respectively. Finally, a global average pooling layer and a fully connection layer are used to connect the above two paths. Experiments show that the authors&#39; DPDNN achieves significant improvements in prediction performance and efficiency. The source code will be published in the near future.},
  archive      = {J_IETIP},
  author       = {Huiqing Zhang and Shuo Li and Donghao Li and Zichen Wang and Qixiang Zhou and Qixin You},
  doi          = {10.1049/ipr2.12199},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {992-999},
  shortjournal = {IET Image Process.},
  title        = {Sonar image quality evaluation using deep neural network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MP2020: Visual quality assessment database for macro
photography images. <em>IETIP</em>, <em>16</em>(4), 985–991. (<a
href="https://doi.org/10.1049/ipr2.12198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of mobile phone camera technology, mobile phones can take a large number of macro photography images that previously could only be taken by professional cameras. Therefore, it is of great significance to study the quality of macro photography images. For this reason, a macro photography image visual quality evaluation database is established and it is named as MP2020. The database contains 100 reference images and 800 distorted images of four distortion types, including 200 distorted images of JPEG 2000, 200 distorted images of JPEG, 200 distorted images of white noise, and 200 distorted images of Gaussian blur. The DMOS values in the database were calculated from 48000 data which are provided by 60 subjects. Ten classical image quality assessment algorithms were tested on the MP2020 database. The experimental results show that the existing image quality assessment algorithms, which are widely used, are not applicable to the macro photography images. Therefore, MP2020 would contribute to the improvement of existing algorithms and the development of new algorithms. MP2020 has been uploaded to GitHub for download.},
  archive      = {J_IETIP},
  author       = {Qingbing Sang and Yujie Cao and Lixiong Liu and Cong Hu and Xiaojun Wu},
  doi          = {10.1049/ipr2.12198},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {985-991},
  shortjournal = {IET Image Process.},
  title        = {MP2020: Visual quality assessment database for macro photography images},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-dimensional DFT with sliding and hopping windows for
edge map generation of road images. <em>IETIP</em>, <em>16</em>(4),
972–984. (<a href="https://doi.org/10.1049/ipr2.12168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An edge map generation technique based on two-dimensional discrete Fourier transform with sliding and hopping windows is proposed for road images to detect the lanes and road markings. A 2 × $\times$ 2 sliding/hopping window DFT with bin indices ( k 1 , k 2 = 0 , 1 $k_1, k_2 = 0,1$ ) for horizontal edge detection and ( k 1 , k 2 = 1 , 0 $k_1, k_2 = 1,0$ ) for vertical edge detection has been proposed. The 2-D SDFT/HDFT-based edge detector has been proved to be more efficient for lane and road marking images in comparison with conventional edge detectors and cellular neural network based edge detectors. In the presence of noise and various signal to noise ratio conditions, the horizontal and vertical edges have been efficiently recovered with good Pratt figure of merit without applying any pre-processing, noise removal process, and post processing techniques. The PFOM was found to be quite stable with wide threshold range for various noise levels. The consistent performance of the proposed edge detector is proved with MSE and PSNR determination of detected edge images. Moreover, the proposed 2-D SDFT/HDFT-based edge detector performs well in developing edge maps for real-time road videos. The system-on-chip implementation of the 2-D SDFT/HDFT edge detector on Cyclone IV FPGA chip is also carried out for detecting the lane and road markings.},
  archive      = {J_IETIP},
  author       = {Amit Kr. Vishwakarma and N. Sukumar and P. Sumathi},
  doi          = {10.1049/ipr2.12168},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {972-984},
  shortjournal = {IET Image Process.},
  title        = {Two-dimensional DFT with sliding and hopping windows for edge map generation of road images},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust object detection under harsh autonomous-driving
environments. <em>IETIP</em>, <em>16</em>(4), 958–971. (<a
href="https://doi.org/10.1049/ipr2.12159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the autonomous driving environment, object instances in an image can be affected by various factors such as camera, driving state, weather, and system component. However, the deep learning-based vision systems are vulnerable to perturbation, which contains noise. Thus, robust object detection under harsh autonomous-driving environments is a more difficult than the generic situation. In this paper, it is found that not only the accuracy, but also the speed of the non-maximum suppression-based detector can be degraded under harsh environments. Therefore, object detection is handled under a harsh situation with adversarial mechanisms such as adversarial training and adversarial defence. Adversarial defence modules are designed to improve robustness in feature extraction level and define perturbations under a harsh environment for training object detectors to improve the robustness of the model&#39;s decision boundary. The proposed adversarial defence and training mechanisms improve the object detector in both accuracy and speed. The proposed method shows a 43.7% mean average precision for the COCO2015 dataset in generic object detection and 39.0% mean average precision for the BDD100K dataset in a driving environment. Furthermore, it achieves a real-time capability of 23 frames per second.},
  archive      = {J_IETIP},
  author       = {Youngjun Kim and Hyekyoung Hwang and Jitae Shin},
  doi          = {10.1049/ipr2.12159},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {958-971},
  shortjournal = {IET Image Process.},
  title        = {Robust object detection under harsh autonomous-driving environments},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A two-scaled fully convolutional learning network for road
detection. <em>IETIP</em>, <em>16</em>(4), 948–957. (<a
href="https://doi.org/10.1049/ipr2.12157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to detect road regions based on a two-scaled deep neural network. The information from different scales is helpful to boost the performance of deep learning models, and it is also a widely used strategy in various computer vision applications. In the two-scaled model, skip-architecture and fully convolutional layers are used to fuse the low-level details and high-level semantic information. It enables to detect the road areas by multi-scale feature maps from different reception fields. To avoid the redundancy of scale information and the loss of features caused by the pooling layer, the feature maps before the first pooling layer are adopted in our model. By the convolutional kernels, our model can balance the information of two scales automatically. The loss function is also improved, in which the intersection over union (IoU) term is taken into account to guide the model to learn more features on the whole road regions. Comprehensive experiments on three benchmark datasets demonstrate that this approach can reach state-of-the-art performance.},
  archive      = {J_IETIP},
  author       = {Dingding Yu and Xianliang Hu and Kewei Liang},
  doi          = {10.1049/ipr2.12157},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {948-957},
  shortjournal = {IET Image Process.},
  title        = {A two-scaled fully convolutional learning network for road detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kernel correlation filter tracking strategy based on
adaptive fusion response map. <em>IETIP</em>, <em>16</em>(4), 937–947.
(<a href="https://doi.org/10.1049/ipr2.12156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problem that the tracking performance of the traditional kernel correlation filter tracking algorithm is easy to be affected by illumination variation, occlusion and motion blur during tracking, an improved tracking strategy is proposed. A new Histogram of Hue Gradient (HHG) feature is designed, and the new HOG-HHG feature is obtained by connecting the HOG and the HHG in series. Two features, CN and HOG-HHG, are extracted respectively, and two kernel correlation filter classifiers are constructed base on the two features above to establish the corresponding response maps of the tracking scenes, respectively. The response maps are fused adaptively to improve the tracking robustness to the complex situations in the tracking process. The updating strategy of the target model is designed based on peak sidelobe ratio ( PSR ) and its difference, and the adaptive thresholds are used to improve the stability of the target model. Simulation results show that the proposed method has better tracking adaptability to the illumination variation, occlusion and motion blur. Both the precision and the success rate can be enhanced.},
  archive      = {J_IETIP},
  author       = {Chunbo Xiu and Yunfei Ma},
  doi          = {10.1049/ipr2.12156},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {937-947},
  shortjournal = {IET Image Process.},
  title        = {Kernel correlation filter tracking strategy based on adaptive fusion response map},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quaternion-based improved cuckoo algorithm for colour UAV
image edge detection. <em>IETIP</em>, <em>16</em>(3), 926–935. (<a
href="https://doi.org/10.1049/ipr2.12398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the extensive application of unmanned aerial vehicles (UAVs), there is an increasing demand for fast processing of coloured UAV images. The coloured UAV image pixels are usually represented by quaternion vectors with three bands of visible light corresponding to the three imaginary parts of the pure imaginary quaternion. Accordingly, the colour image edge points can be determined based on the quaternion polar coordinating the rotation principle. Here, a quaternion-based improved cuckoo algorithm is proposed to perform fast processing for UAVs images. In particular, a novel guiding equation is used to optimize the positions of the improved cuckoo algorithm before the Levi flight. Furthermore, a novel disturbance equation is used to obtain a varied location for the next location after the Levi flight. Comprehensive experiments are conducted to evaluate the performance of the proposed solution. The experimental results showed that the proposed method significantly reduces the image processing time and remarkably improves the quality.},
  archive      = {J_IETIP},
  author       = {Dujin Liu and Guolin Pu and Xiaoyan Wu},
  doi          = {10.1049/ipr2.12398},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {926-935},
  shortjournal = {IET Image Process.},
  title        = {Quaternion-based improved cuckoo algorithm for colour UAV image edge detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tomato leaf disease classification by exploiting transfer
learning and feature concatenation. <em>IETIP</em>, <em>16</em>(3),
913–925. (<a href="https://doi.org/10.1049/ipr2.12397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tomato is one of the most important vegetables worldwide. It is considered a mainstay of many countries’ economies. However, tomato crops are vulnerable to many diseases that lead to reducing or destroying production, and for this reason, early and accurate diagnosis of tomato diseases is very urgent. For this reason, many deep learning models have been developed to automate tomato leaf disease classification. Deep learning is far superior to traditional machine learning with loads of data, but traditional machine learning may outperform deep learning for limited training data. The authors propose a tomato leaf disease classification method by exploiting transfer learning and features concatenation. The authors extract features using pre-trained kernels (weights) from MobileNetV2 and NASNetMobile; then, they concatenate and reduce the dimensionality of these features using kernel principal component analysis. Following that, they feed these features into a conventional learning algorithm. The experimental results confirm the effectiveness of concatenated features for boosting the performance of classifiers. The authors have evaluated the three most popular traditional machine learning classifiers, random forest, support vector machine, and multinomial logistic regression; among them, multinomial logistic regression achieved the best performance with an average accuracy of 97%.},
  archive      = {J_IETIP},
  author       = {Mehdhar S. A. M. Al-gaashani and Fengjun Shang and Mohammed S. A. Muthanna and Mashael Khayyat and Ahmed A. Abd El-Latif},
  doi          = {10.1049/ipr2.12397},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {913-925},
  shortjournal = {IET Image Process.},
  title        = {Tomato leaf disease classification by exploiting transfer learning and feature concatenation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast outdoor hazy image dehazing based on saturation and
brightness. <em>IETIP</em>, <em>16</em>(3), 900–912. (<a
href="https://doi.org/10.1049/ipr2.12396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haze usually limits the visibility and reduces the contrast of outdoor images. The removal of haze in a single image has always been a challenging problem. Recently, many methods have been proposed to effectively remove haze in harsh conditions. However, these methods fail to balance the dehazing effect and time consumption. This paper proposes a method based on HSV colour space to restore the visibility of uniform scattering medium. It can prevent the atmospheric light and transmission from being miscalculated. This method uses the brightness component of haze image to estimate the global atmospheric light, which reduces the influence of luminescent objects on atmospheric light estimation. Then, this paper deduces the estimation model of the saturation of scene radiance based on the atmospheric scattering model. In the meantime, according to the advantages and problems of the model, the fast estimation of the transmission is realized by using the stretching function. Finally, this paper solves the parameters in the model by iterative method. Since this method can estimate different media transmission for each pixel, better results can be obtained. The simulation results show that the algorithm is superior to the existing algorithms in terms of dehazing effect and time consumption.},
  archive      = {J_IETIP},
  author       = {Daosong Hu and Yang Yang and Bo Li and Huiming Tang and Yu Xu},
  doi          = {10.1049/ipr2.12396},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {900-912},
  shortjournal = {IET Image Process.},
  title        = {Fast outdoor hazy image dehazing based on saturation and brightness},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reversible data hiding based on multi-predictor and adaptive
expansion. <em>IETIP</em>, <em>16</em>(3), 888–899. (<a
href="https://doi.org/10.1049/ipr2.12395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive embedding plays an important role in improving the embedding performance of reversible data hiding and it is usually realized by modifying the prediction-errors discriminately. By extending the skewed histogram shifting technique which uses a pair of extreme predictions to determine whether the target pixel should be predicted or not, this paper realizes another form of adaptive embedding, that is, adaptive prediction-error generation. Specifically, it is proposed to adaptively determine the pair of extreme predictions according to image content. Each pair of extreme predictions is first evaluated by the introduced distortion per embedding one bit. Then, an efficient mechanism to determine the best pair of extreme predictions for pixels with a given local complexity is designed. With such a mechanism solving the computational problem, it is also proposed to extend the context to obtain more pairs of extreme predictions such that more precise multi-predictor can be realized. With obtained errors, the best expansion bins are determined to achieve a more comprehensive self-adaption. Experimental results demonstrate that the proposed scheme achieves better capacity-distortion performance and outperforms a series of state-of-the-art schemes.},
  archive      = {J_IETIP},
  author       = {Wenguang He and Gangqiang Xiong and Yaomin Wang},
  doi          = {10.1049/ipr2.12395},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {888-899},
  shortjournal = {IET Image Process.},
  title        = {Reversible data hiding based on multi-predictor and adaptive expansion},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiscale residual fusion network for image denoising.
<em>IETIP</em>, <em>16</em>(3), 878–887. (<a
href="https://doi.org/10.1049/ipr2.12394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep-learning methods have been developed in recent years and have achieved dramatic improvements for image denoising. The existing deep-learning methods can be conducted using two major models: Encoder–decoder and high-resolution, where the high-resolution model has superior resolution ability for detail description and restoration. In this study, a high-resolution-based network called multiscale residual fusion network (MRF-Net) is proposed, which employed the spatial and contextual information of images. In detail, dilated convolution layers are used to enlarge the network&#39;s receptive field and learned sufficient features in a multiscale feature extracting module. The function of dilated convolution is reinterpreted here and it is viewed as a complex downsampling operation. Therefore, multiscale feature analysis could be performed in the proposed network by dilated convolution. Multilevel feature maps are sequentially obtained through a residual projection module, where considerable contextual and spatial information was collected from the multiscale features. In a residual fusion module, all maps were aggregated to generate a residual image effectively for noise removal. Experiments demonstrated that the MRF-Net outperformed several state-of-the-art model-based and deep-learning methods in both blind and non-blind image denoising tests. Meanwhile, ablation studies were executed to verify the denoising performance of each module. Moreover, this method exhibited high computational efficiency, thus demonstrating its practicability.},
  archive      = {J_IETIP},
  author       = {Cheng Yao and Yibin Tang and Jia Sun and Yuan Gao and Changping Zhu},
  doi          = {10.1049/ipr2.12394},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {878-887},
  shortjournal = {IET Image Process.},
  title        = {Multiscale residual fusion network for image denoising},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pavement crack detection using non-local theory and
iterative sampling. <em>IETIP</em>, <em>16</em>(3), 869–877. (<a
href="https://doi.org/10.1049/ipr2.12393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crack is a common form of road distress and a key study of an intelligent transportation system. However, automatic pavement crack detection is a very challenging task due to noisy texture background, intensity inhomogeneity, and topology complexity. In this paper, a new pavement crack detection algorithm to address these issues is proposed. First, non-local block matching strategy and local statistical mean are put together to generate the probability map of cracks, which has advantages on automatic threshold choosing and strong resistance to intensity inhomogeneity. Second, an iterative seed points sampling algorithm is proposed, which makes full use of the area and shape of connected regions where the seeds lie in, thus exploiting high reliable crack seeds for following curves extraction. Finally, a minimum spanning tree (MST) is adopted to connect points into crack curves and employ a crack growth method to find out the cracks, which is specified to deal with complex topology of cracks. For parameters, a robust and optimal parameters selection rule is obtained by data driven method. The algorithm is compared with other state-of-the-art algorithms on two datasets. The experiment result shows that the proposed method has a better detection performance on -measure score over other methods.},
  archive      = {J_IETIP},
  author       = {Zixian Wei and Tao SUN and Yuhao Wu and Liqing Zhou and Xiaoli Ruan},
  doi          = {10.1049/ipr2.12393},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {869-877},
  shortjournal = {IET Image Process.},
  title        = {Pavement crack detection using non-local theory and iterative sampling},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Metallic debossed characters industrial online
non-segmentation identification based on improved multi-scale image
fusion enhancement and deep neural network. <em>IETIP</em>,
<em>16</em>(3), 852–868. (<a
href="https://doi.org/10.1049/ipr2.12391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates accurate industry online scene text recognition techniques for metallic debossed characters (MDCs). As MDCs have low contrast with its background and easy to interfere with corrosion rust, oxidate skin or specular reflection and so forth, a multi-scale image fusion algorithm is first proposed to restore the MDCs’ shape and appearance information to enhance the sample&#39;s contrast by using the MDCs&#39; depth characteristic with the help of four-directional illumination. With this method, the three most important reflection-related picture quality evaluation standards, namely contrast, saturation, and exposure, can be enhanced simultaneously. Next, based on the fusion result, a U-shaped network based on VGG-16 network architecture is used for text localization. In order to improve network receptive fields, an improved post-processing output module that combines a text sequence score map for coarsening localization with a single-character score map for fine localization is proposed, so that the network&#39;s adaptability and accuracy for long texts are improved. Finally, a convolutional recurrent neural network is adopted to realize the recognition of sequence characters. The advantages and effectiveness of the proposed method are statistically analyzed with the data from a liquefied petroleum gas cylinder annual periodic inspection production line.},
  archive      = {J_IETIP},
  author       = {Zhong Xiang and Huaxiong Wu and Ding Zhou},
  doi          = {10.1049/ipr2.12391},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {852-868},
  shortjournal = {IET Image Process.},
  title        = {Metallic debossed characters industrial online non-segmentation identification based on improved multi-scale image fusion enhancement and deep neural network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AC-SDBSCAN: Toward concealed object detection of passive
terahertz images. <em>IETIP</em>, <em>16</em>(3), 839–851. (<a
href="https://doi.org/10.1049/ipr2.12390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The passive terahertz (THz) detection on objects concealed under clothing is quite challenging due to the heavy noise interference. In this paper, an active-contour-based self-adaptive DBSCAN (AC-SDBSCAN) detection algorithm is proposed. The core of AC-SDBSCAN algorithm lies in that the object contours are first extracted by AC method in a noise scenario and then the statistical features of the contours are used to motivate a SDBSCAN to complete clustering without initialization. Benefiting from the strong robustness of AC and DBSCAN to the noise, the concealed objects in the noisy THz images can be detected accurately. Extensive simulations are verified on four passive THz image datasets. The results indicate that the AC method in our solution can achieve over 87% accuracy for contour extraction of passive THz images, while the classical methods achieve less than 77%; in addition, the SDBSCAN can achieve over 90% clustering accuracy without manual initialization which is significantly superior to the conventional DBSCAN. Eventually, the proposed method completes the object detection of passive THz images with a maximum recall of 90.38% and a maximum precision of 94%.},
  archive      = {J_IETIP},
  author       = {Ya Liu and Fan Xu and Ziqi Pu and Xuyang Huang and Jun Chen and Shuning Shao},
  doi          = {10.1049/ipr2.12390},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {839-851},
  shortjournal = {IET Image Process.},
  title        = {AC-SDBSCAN: Toward concealed object detection of passive terahertz images},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved dark channel defogging algorithm based on the
HSI colour space. <em>IETIP</em>, <em>16</em>(3), 823–838. (<a
href="https://doi.org/10.1049/ipr2.12389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the traditional dark channel prior (DCP) algorithm is used for image defogging, it faces some problems such as unsatisfactory restoration of the target edge details, distortion in large-scale and high-brightness sky regions. To solve these problems, this paper proposes an improved dark channel defogging algorithm based on the HSI colour space, named the IDCP algorithm. First, an asymmetric mean filter window is used to obtain the modified dark channel image to improve the grey value mutation in the traditional dark channel image. Based on this work, an improved global atmospheric transmittance calculation method is proposed, which effectively improves the inapplicability of the dark channel prior in large-scale and high-brightness sky regions. Finally, the Laplacian operator is used to sharpen the image to enhance the target edge information. Simulation results show that the proposed algorithm can effectively solve the distortion phenomenon caused by the traditional DCP algorithm when handling large-scale and high-brightness sky regions, improve the target edge details, and restore the true colour of the image scene.},
  archive      = {J_IETIP},
  author       = {Yani Cui and Shuaiqing Zhi and Wenjin Liu and Jiaxian Deng and Jia Ren},
  doi          = {10.1049/ipr2.12389},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {823-838},
  shortjournal = {IET Image Process.},
  title        = {An improved dark channel defogging algorithm based on the HSI colour space},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Road crack detection network under noise based on feature
pyramid structure with feature enhancement (road crack detection under
noise). <em>IETIP</em>, <em>16</em>(3), 809–822. (<a
href="https://doi.org/10.1049/ipr2.12388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road crack detection is an important task for road safety and road maintenance. In the past, people made use of manual detection methods and tried to use computer vision to detect crack. The most prominent feature in recent years is the use of deep learning. However, there is no good deep learning method for road crack detection under noise. This challenge is faced bravely. First, a noise crack dataset is proposed, consisting of multiple noise crack images which is called NCD. Then, an adaptive bilateral filtering algorithm is developed, which can reduce the influence of noise. Finally, a new crack detection network with two new modules is designed. In the end, it is found that all the parts have promoting effects on crack detection under noise. Compared with other state-of-the-art methods, this method performs better, especially in road crack detection under noise. When evaluating the well-known crack500 test set, ODS F-measure of 0.628 is achieved. Besides, this method is also evaluated in another five datasets. Significantly, ODS F-measure of 0.545 is achieved, 4.0% higher than state-of-the-art on GAPs384.},
  archive      = {J_IETIP},
  author       = {Mingsi Sun and Hongwei Zhao and Jiao Li},
  doi          = {10.1049/ipr2.12388},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {809-822},
  shortjournal = {IET Image Process.},
  title        = {Road crack detection network under noise based on feature pyramid structure with feature enhancement (road crack detection under noise)},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An image cipher system based on networked chaotic map with
parameter q. <em>IETIP</em>, <em>16</em>(3), 797–808. (<a
href="https://doi.org/10.1049/ipr2.12386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper models a networked coupling chaotic system with a fractional-like local map based on network topology and fractional theory. The proposed model contains more system parameters and a more relaxed coupling relationship. The entropy analysis also shows that the proposed model is more sensitive to the changes in the coupling parameter. Based on this model, an effective image cipher system is designed using universal scrambling and diffusion based on DNA encoding. The proposed method utilizes the generated pseudo-random signals from the networked chaotic map. The secret keys can be generated in parallel and the security is also improved due to the rise of complexity of the new chaotic map. Analysis and simulations confirm that the new chaotic map as well as the DNA encoding technology improves the effectiveness and robustness of the proposed encryption algorithm.},
  archive      = {J_IETIP},
  author       = {Yu-Jie Sun and Hao Zhang and Zhen-Yu Li and Xing-Yuan Wang and Chun-Peng Wang},
  doi          = {10.1049/ipr2.12386},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {797-808},
  shortjournal = {IET Image Process.},
  title        = {An image cipher system based on networked chaotic map with parameter q},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep learning approach for the classification of TB from
NIH CXR dataset. <em>IETIP</em>, <em>16</em>(3), 787–796. (<a
href="https://doi.org/10.1049/ipr2.12385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, a novel customized deep learning model is proposed to detect Tuberculosis (TB) from chest X-rays (CXR). The model is utilized for three experimentations: (i) classification of CXR image as healthy or TB infected, (ii) sub-classification of infected images to TB specific manifestations, and (iii) classification of CXR image to thoracic disease manifestations. The National Institute of Health (NIH) CXR is used for experimentation. For the first two experimentations, the subset of the dataset is used containing only 10 TB specific manifestations, whereas, the entire NIH CXR dataset is used for the third experiment. The F1 score for binary classification of TB in experiment 1 is calculated as 0.92 which is higher than the average F1 score of the radiologists. The average accuracy for classifying TB specific manifestations in experiment 2 is recorded as 0.84. Finally, the average accuracy of the thoracic disease classification is recorded as 0.82 in experiment 3. The proposed system outperformed the existing approaches reporting higher AUC for each manifestation. Whereas, to the best of knowledge it is the first such attempt on NIH CXR dataset for TB and TB specific manifestation classification and the proposed system showed promising results.},
  archive      = {J_IETIP},
  author       = {S. Zainab Yousuf Zaidi and M. Usman Akram and Amina Jameel and Norah Saleh Alghamdi},
  doi          = {10.1049/ipr2.12385},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {787-796},
  shortjournal = {IET Image Process.},
  title        = {A deep learning approach for the classification of TB from NIH CXR dataset},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-stage progressive residual learning network for
multi-focus image fusion. <em>IETIP</em>, <em>16</em>(3), 772–786. (<a
href="https://doi.org/10.1049/ipr2.12383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boundary artifacts and color detail distortion are easily caused by the common multi-focus image fusion methods. In order to solve this problem, we propose a two-stage progressive residual learning network for multi-focus image fusion. The proposed network can progressively learn color information and detail features through end-to-end mapping. The whole network is composed of two sub-networks: the initial fusion block network and the enhanced fusion block network. First, the color information in the source image is fused by the initial fusion block network to generate the initial fusion image. Then on the basis of the initial fusion image, the detailed features of the source image are further fused by enhanced the fusion network to form the final fusion image. In order to solve the problem of lack of groundtruth when multi-focus image fusion is carried out with supervised method, the multi-focus image fusion problem is compared to the easy-to-solve image restoration problem. A synthetic dataset for network training is generated by ”degenerating” the VOC2012 dataset according to the set rules. After training, the method works well for fusion tasks without further processing. Experimental results show that the proposed method is superior to the existing methods in subjective visual perception and objective quantitative evaluation.},
  archive      = {J_IETIP},
  author       = {Haoran Wang and Zhen Hua and Jinjiang Li},
  doi          = {10.1049/ipr2.12383},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {772-786},
  shortjournal = {IET Image Process.},
  title        = {Two-stage progressive residual learning network for multi-focus image fusion},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiscale spectral-spatial cross-extraction network for
hyperspectral image classification. <em>IETIP</em>, <em>16</em>(3),
755–771. (<a href="https://doi.org/10.1049/ipr2.12382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNN) are becoming increasingly popular in modern remote sensing image classification tasks and have exhibited excellent results. For the existing CNN-based hyperspectral image (HSI) classification methods, most of which extract spatial or spectral features separately by convolution. But nearly all of these methods ignore the fact that the weighted summation of convolution may lead to appear new features in another dimension. To address this issue, a novel multiscale spectral-spatial cross-extraction network (MSSCEN) is proposed for HSI classification. Specifically, the proposed MSSCEN introduces spectral-spatial features cross extraction module (SSCEM), which fed extracted features from previous layer into spatial and spectral extraction branches separately again, so that the changes that occurred in the other domain after each convolution can be fully utilized. In addition, a new independent data augmentation module based on U-Net is designed to mitigate the problem of limited labelled samples. The paper conducts experiments on three classic hyperspectral datasets and the results demonstrate that the proposed method achieves the best classification accuracy than other state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Hongmin Gao and Hongyi Wu and Zhonghao Chen and Yunfei Zhang and Yiyan Zhang and Chenming Li},
  doi          = {10.1049/ipr2.12382},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {755-771},
  shortjournal = {IET Image Process.},
  title        = {Multiscale spectral-spatial cross-extraction network for hyperspectral image classification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single sample per person face recognition algorithm based on
the robust prototype dictionary and robust variation dictionary
construction. <em>IETIP</em>, <em>16</em>(3), 742–754. (<a
href="https://doi.org/10.1049/ipr2.12381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single sample per person (SSPP) face recognition uses only a single face image of each subject in the gallery set to recognize the probe sample. Since single sample cannot provide intra-class variation information, the matching accuracy of the gallery faces with the faces captured in unconstrained video is usually low. Recently, in order to improve the accuracy, the sparse representation-based classification (SRC) technology has been extended to generic learning method, which uses prototype and variation dictionary (P+V) model for face recognition. Because the inter-class scatter between atoms in prototype dictionary is not big enough, and the intra-class scatter in constructed variation dictionary (such as posture and expression) is not rich enough, the robustness of P+V model for SSPP face recognition is poor. To solve this problem, a robust prototype dictionary and robust variation dictionary construction (RPRV) method is proposed. First, a set of atoms is obtained by dictionary learning method using gallery images and generic images. Second, some effective atoms are selected by the proposed function index method. Finally, the robust prototype dictionary (RP) and the robust variation dictionary (RV) are represented linearly using these effective atoms, respectively. The face recognition is performed according to the proposed RP+RV model. Experiment results using public datasets show that the proposed RPRV has strong robustness for the face captured under the unconstrained environment. Comparison results show that the proposed RPRV method outperform state-of-the-art SRC-based methods for SSPP face recognition.},
  archive      = {J_IETIP},
  author       = {Shan Xue and Hai-Peng Ren},
  doi          = {10.1049/ipr2.12381},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {742-754},
  shortjournal = {IET Image Process.},
  title        = {Single sample per person face recognition algorithm based on the robust prototype dictionary and robust variation dictionary construction},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting robust unsupervised video person
re-identification. <em>IETIP</em>, <em>16</em>(3), 729–741. (<a
href="https://doi.org/10.1049/ipr2.12380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised video person re-identification (reID) methods usually depend on global-level features. Many supervised reID methods employed local-level features and achieved significant performance improvements. However, applying local-level features to unsupervised methods may introduce an unstable performance. To improve the performance stability for unsupervised video reID, this paper introduces a general scheme fusing part models and unsupervised learning. In this scheme, the global-level feature is divided into equal local-level feature. A local-aware module is employed to explore the potentials of local-level feature for unsupervised learning. A global-aware module is proposed to overcome the disadvantages of local-level features. Features from these two modules are fused to form a robust feature representation for each input image. This feature representation has the advantages of local-level feature without suffering from its disadvantages. Comprehensive experiments are conducted on three benchmarks, including PRID2011, iLIDS-VID, and DukeMTMC-VideoReID, and the results demonstrate that the proposed approach achieves state-of-the-art performance. Extensive ablation studies demonstrate the effectiveness and robustness of proposed scheme, local-aware module and global-aware module. The code and generated features are available at https://github.com/deropty/uPMnet .},
  archive      = {J_IETIP},
  author       = {Xianghao Zang and Ge Li and Wei Gao and Xiujun Shu},
  doi          = {10.1049/ipr2.12380},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {729-741},
  shortjournal = {IET Image Process.},
  title        = {Exploiting robust unsupervised video person re-identification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data augmentation and shadow image classification for shadow
detection. <em>IETIP</em>, <em>16</em>(3), 717–728. (<a
href="https://doi.org/10.1049/ipr2.12377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shadow detection is an important branch of computer vision. Recently, convolutional neural network (CNN)-based methods for shadow detection have achieved better performance than methods based on manually designed features. However, CNNs are extremely hungry for data and the training of CNN-based shadow detector requires time-consuming and expensive pixel-level annotations. To alleviate this problem in shadow detection, a method of data augmentation based on generative adversarial network (GAN), named ShadowGAN, has been proposed. Given a shadow mask and a shadow-free image, our ShadowGAN can generate shadow images with labels. To guide the training of ShadowGAN and get more realistic shadow images, L 1 ${{\cal L}_1}$ loss is further implemented to impose a restriction between real shadow images and generated shadow images. The effectiveness of ShadowGAN is demonstrated by training existing shadow detectors on enlarged dataset. In addition, to better make use of shadow-free images in shadow detection, shadow image classification task is added for the shadow detectors. Experiments show that this task can guide the feature extraction network to learn more robust shadow features. At last, these two methods are combined and a better performance of shadow detection is achieved.},
  archive      = {J_IETIP},
  author       = {Guoquan Li and Lingyun Wen and Zhengwen Huang and Ruiyang Xia and Yu Pang},
  doi          = {10.1049/ipr2.12377},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {717-728},
  shortjournal = {IET Image Process.},
  title        = {Data augmentation and shadow image classification for shadow detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inferior alveolar nerve canal segmentation by local features
based neural network model. <em>IETIP</em>, <em>16</em>(3), 703–716. (<a
href="https://doi.org/10.1049/ipr2.12375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of Inferior Alveolar Nerve Canal (IAC) plays major and crucial role in dental surgical procedures to avoid damage to IAC during the course of treatment. Exact visualization and detection of IAC is necessary for precise surgery planning to prevent IAC damage. The proposed method comprises of three stages namely, novel edge enhancement, candidate classification and candidate pixel clustering to detect the IAC. For better visualization of IAC, initially the edges of dental OPG images are enhanced using a novel structural filter. Candidate regions are selected from the enhanced image by the proposed Multi Hidden Layer Extreme Learning Machine Artificial Neural Network (MELMANN) model driven by combined regional features such as Histogram of Gradients (HOG), Local Binary Pattern (LBP) and Gray Level Co-occurrence Matrix (GLCM). Consequently the candidate region pixels are clustered by a Self-Organising Map-based Neural Network (SOM - NNC) along with active contour method to detect the IAC completely. Experimental results show that this method effectively delineated the IAC with the Dice coefficient of 0.854 ± 0.05.Therefore, the proposed method has high potential in clearly visualizing IAC to avoid neurological sensory disorders in oral and maxillofacial surgery and implantology and it provides better prediagnostic approach to the surgeons.},
  archive      = {J_IETIP},
  author       = {P. Uma Maheswari and A. Banumathi and G. Ulaganathan and R. Yoganandha},
  doi          = {10.1049/ipr2.12375},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {703-716},
  shortjournal = {IET Image Process.},
  title        = {Inferior alveolar nerve canal segmentation by local features based neural network model},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MAM: A multipath attention mechanism for image recognition.
<em>IETIP</em>, <em>16</em>(3), 691–702. (<a
href="https://doi.org/10.1049/ipr2.12370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention mechanism has shown excellent performance in many computer vision tasks, while the previous literature may not adequately consider different types of attention mechanisms or is individual elaborate designed for a certain network. In this paper, a general yet effective multipath attention mechanism (MAM) to explore the effect of visual attention for image recognition is proposed. In contrast with other attentions that leverage global pooling, the main advantage is that the MAM considers both the correlation of featuremaps and different scale structural information into account. The backbone representations are enhanced by adding MAM laterally along independent and separate dimensions, channel and spatial. Due to only a simple and unified calculation block is generated, MAM can be flexibly integrated into various CNNs within few parameters and trained together end-to-end. Furthermore, the topology structures of attention path arrangement are investigated using different connection schemes. Experimental results on several image recognition datasets show that the model outperforms various existing models. Finally, performance improvement through visualisation is intuitively discussed. The source code for the proposed attention module is publicly available.},
  archive      = {J_IETIP},
  author       = {Hao Zhang and Guoqin Peng and Zhichao Wu and Jian Gong and Dan Xu and Hongzhen Shi},
  doi          = {10.1049/ipr2.12370},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {691-702},
  shortjournal = {IET Image Process.},
  title        = {MAM: A multipath attention mechanism for image recognition},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An image enhancement algorithm of video surveillance scene
based on deep learning. <em>IETIP</em>, <em>16</em>(3), 681–690. (<a
href="https://doi.org/10.1049/ipr2.12286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Target enhancement is the most important task in a video surveillance system. In order to improve the accuracy and efficiency of target enhancement, and better deal with the subsequent recognition, tracking, behaviour understanding and other processing of targets, a deep learning-based image enhancement algorithm for video surveillance scenes is proposed. First, the super-resolution reconstruction of the image is carried out through the image super-resolution reconstruction method based on the hybrid deep convolutional network to improve the sharpness of the image. Then, for the reconstructed video surveillance scene image, the watershed image enhancement algorithm based on morphology and region merging is used to realize the enhancement of the video surveillance scene image. Deep learning algorithms can improve the accuracy of image enhancement through iterative calculations. Experimental results show that after image enhancement in daytime, night and noisy video surveillance scenes, the maximum enhancement difference rate is less than 0.5%, the cross-linking degree is close to 1, and the average image enhancement time is less than 1.3 s. It can realize image enhancement of video surveillance scenes and improve the image clarity of the video surveillance scene.},
  archive      = {J_IETIP},
  author       = {Wei-wei Shen and Lin Chen and Shuai Liu and Yu-Dong Zhang},
  doi          = {10.1049/ipr2.12286},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {681-690},
  shortjournal = {IET Image Process.},
  title        = {An image enhancement algorithm of video surveillance scene based on deep learning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ILBPSDNet: Based on improved local binary pattern shallow
deep convolutional neural network for character recognition.
<em>IETIP</em>, <em>16</em>(3), 669–680. (<a
href="https://doi.org/10.1049/ipr2.12226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an architecture based on the improved local binary pattern (LBP) shallow deep convolution neural network, which integrates hand-crafted feature pre-processing and the advantage of character learning in the supervised high-level function of CNN, in order to enhance its performance. This study introduced the information of scale space into the LBP to reduce the sensitivity to noise, and applied feature maps with two features, the maximum selection feature map (MLBP) and the first selection feature map (FLBP). The former selected the edge with the strongest intensity to reduce the influence of noise points, while the latter measured local binary features through the scale detection of an effective edge. In the network architecture design, according to the differences of input features, networks of different depths were used for learning, and the features learned by the two networks were adopted for classification. The experimental results show that, the ILBPSDNet proposed had certain recognition abilities in many character data sets, and the network parameters and computation were also reduced. Therefore, it has a significant effect in realizing the application of real-time character recognition. Finally, compared with other latest networks, its network performance could be maintained at a certain level.},
  archive      = {J_IETIP},
  author       = {Shih-Hsiung Lee and Wei-Fu Yu and Chu-Sing Yang},
  doi          = {10.1049/ipr2.12226},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {669-680},
  shortjournal = {IET Image Process.},
  title        = {ILBPSDNet: Based on improved local binary pattern shallow deep convolutional neural network for character recognition},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MSAR-DefogNet: Lightweight cloud removal network for high
resolution remote sensing images based on multi scale convolution.
<em>IETIP</em>, <em>16</em>(3), 659–668. (<a
href="https://doi.org/10.1049/ipr2.12224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High resolution remote sensing image cloud removal can bring a lot of convenience for human activities. However, the existing cloud removal algorithms have a variety of disadvantages. First of all, they have the disadvantages of long computing time and large consumption of computing resources. Secondly, the effect of recovery needs to be improved. In order to improve the above two points, a near real-time effective algorithm is proposed, namely MSAR-Defognet (multiple scale attention residual network using for cloud remove), which consumes less computing power and space and has superior cloud removal effect. On the one hand, several different large-scale filters are chosen to extract the weak information effectively, while can save the computing power and shorten the image processing time. On the other hand, the fine-grained convolution residual block with channel attention mechanism is used to enhance the network&#39;s ability to extract cloud features. In addition, a data set which is closer to the real cloud shape and has higher richness to train the cloud removal network, so that the parameters obtained by training have stronger robustness and can adaptively remove clouds with different thickness. Experiments show that, compared with other advanced network models, the network not only has the advantage of fast processing speed, but also has better image restoration effect in high-resolution remote sensing image restoration. It can meet the requirements of many hard real-time tasks, so that remote sensing images can play a greater value for human activities.},
  archive      = {J_IETIP},
  author       = {Ying Zhou and Weipeng Jing and Jian Wang and Guangsheng Chen and Rafal Scherer and Robertas Damaševičius},
  doi          = {10.1049/ipr2.12224},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {659-668},
  shortjournal = {IET Image Process.},
  title        = {MSAR-DefogNet: Lightweight cloud removal network for high resolution remote sensing images based on multi scale convolution},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identification of malnutrition and prediction of BMI from
facial images using real-time image processing and machine learning.
<em>IETIP</em>, <em>16</em>(3), 647–658. (<a
href="https://doi.org/10.1049/ipr2.12222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human faces contain useful information that can be used in the identification of age, gender, weight etc. Among these biometrics, body mass index (BMI) and body weight are good indicators of a healthy person. Motivated by the recent health science studies, this work investigates ways to identify malnutrition affected people and obese people by analyzing body weight and BMI from facial images by proposing a regression method based on the 50-layers Residual network architecture. For face detection, Multi-task Cascaded Convolutional Neural Networks have been employed. A system is created to evaluate BMI along with age and gender from human facial real-time images. Malnutrition and obesity are commonly determined with the help of BMI. In the previous works, height, weight, and BMI estimation through automatic means have predominantly focused on full-body images and videos of humans. The usage of facial images for estimating such traits have been given less importance. In order to facilitate the analysis, the dataset is cleaned along with metadata containing information about the persons height, weight, age, and gender. Gender-based analysis is performed for the prediction of BMI. Finally, an email containing the persons picture along with their details is sent to the concerned health officer.},
  archive      = {J_IETIP},
  author       = {Dhanamjayulu C and Nizhal U N and Praveen Kumar Reddy Maddikunta and Thippa Reddy Gadekallu and Celestine Iwendi and Chuliang Wei and Qin Xin},
  doi          = {10.1049/ipr2.12222},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {647-658},
  shortjournal = {IET Image Process.},
  title        = {Identification of malnutrition and prediction of BMI from facial images using real-time image processing and machine learning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time classification on oral ulcer images with residual
network and image enhancement. <em>IETIP</em>, <em>16</em>(3), 641–646.
(<a href="https://doi.org/10.1049/ipr2.12144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advances of deep learning research in the past few years, healthcare and smart medicines have been significantly developed. Inspired by the wide application of deep learning in medical image classification and disease diagnosis, this paper further proposes a variant of the Residual Network framework to classify the oral ulcer images in real-time. In particular, image pre-processing and enhancement techniques are used to enrich the datasets and reduce model overfitting. Besides, the transfer learning is further introduced into the residual blocks to improve the classification accuracy, with the later layers trained from the labeled datasets. To validate the performance of authors&#39; proposal, it is compared with other classic deep learning models with respect to the classification sensitivity, specificity, and accuracy. The experimental results show that authors&#39; approach outperforms those classic classification networks when the oral ulcers are classified and diagnosed in real-time.},
  archive      = {J_IETIP},
  author       = {Jianbin Guo and Haolin Wang and Xingsi Xue and Mengting Li and Zhongxiong Ma},
  doi          = {10.1049/ipr2.12144},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {641-646},
  shortjournal = {IET Image Process.},
  title        = {Real-time classification on oral ulcer images with residual network and image enhancement},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OsaMOT: Occlusion and scale-aware multi-object tracking
algorithm for low viewpoint. <em>IETIP</em>, <em>16</em>(2), 622–640.
(<a href="https://doi.org/10.1049/ipr2.12378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking (MOT), which uses the context information of image sequences to locate, maintain identities and generate trajectories of multiple targets in each frame, is key technology in the field of computer vision. To address the problems of occlusion and scale variation in low-viewpoint MOT, OsaMOT is proposed here. First, according to the global occlusion state of each frame, OsaMOT proposes the adaptive anti-occlusion feature to enhance the awareness and adaptability for occlusion. At the same time, OsaMOT uses the cascade screening mechanism to reduce the “virtual new target” phenomenon due to the dramatic change in target features caused by scale variation and occlusion. Finally, considering that the occluded templates will affect the tracking performance, OsaMOT proposes an adaptive anti-noise template update mechanism according to the partial occlusion state of the target, which improves the purity of the template library and further enhances the applicability to occlusion. The experimental results show that OsaMOT can weaken the influence of scale variation, partial occlusion, short-term full occlusion and long-term full occlusion in the low-viewpoint tracking scenes. Most evaluation indexes of OsaMOT under low-viewpoint tracking scenario are superior to those of some typical algorithms proposed in recent years, and the tracking robustness is improved.},
  archive      = {J_IETIP},
  author       = {Yingying Yue and Dan Xu and Kangjian He and Hongzhen Shi and Hao Zhang},
  doi          = {10.1049/ipr2.12378},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {622-640},
  shortjournal = {IET Image Process.},
  title        = {OsaMOT: Occlusion and scale-aware multi-object tracking algorithm for low viewpoint},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Yeast cell detection in color microscopic images using
ROC-optimized decoloring and segmentation. <em>IETIP</em>,
<em>16</em>(2), 606–621. (<a
href="https://doi.org/10.1049/ipr2.12376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the detection and evaluation of yeast cell microscopic images, decoloring and segmentation is crucial. Usually, a standard operation of color conversion to grayscale is used and subsequently, a thresholding algorithm is applied to yield the segmented cells. However, both types of these operations are controlled merely by fixed empirical parameters which are prone to inaccuracy. The authors illustrate a common problem of such a nonoptimal decoloring. They then develop a novel pipeline of algorithms for yeast cell color phantom generation. In this study, the authors extend Grundland&#39; s decoloring approach and couple it with an adaptive thresholding operation. For the generated phantom, the parameters of these operations are optimized using the ROC-based F 1-measure. It is showed that this measure is a better choice than the Precision and Recall measure alone. Computer experiments with various combinations of the operation coupling and different phantom backgrounds are accomplished, and the optimum values of all the controllable parameters are found. For the color phantom representing the most realistic conditions, the final fully optimized 4D combination of the improved Grundland&#39; s decoloring and locally adaptive thresholding ensures more efficient cell detection performance than parameterless or nonoptimized combinations.},
  archive      = {J_IETIP},
  author       = {Ivan Bajla and Michal Teplan},
  doi          = {10.1049/ipr2.12376},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {606-621},
  shortjournal = {IET Image Process.},
  title        = {Yeast cell detection in color microscopic images using ROC-optimized decoloring and segmentation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Glioma classification framework based on SE-ResNeXt network
and its optimization. <em>IETIP</em>, <em>16</em>(2), 596–605. (<a
href="https://doi.org/10.1049/ipr2.12374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the complexity of the glioma classification process, a classification framework based on SE-ResNeXt network is proposed to simplify the classification process of benign and malignant of gliomas. In addition, three optimization strategies are adopted to improve the accuracy. Firstly, the MultiStepLR strategy is used to adjust the learning rate dynamically in order to improve the learning ability of the network. Secondly, the one-hot label is optimized by the label smoothing strategy which can reduce the dependence of the network on the probability distribution of real labels and improve the prediction ability of the network. Finally, the transfer learning process is simplified by the transfer learning strategy on CE-MRI dataset, and the generalization ability of the network is improved. The experimental results show that the accuracy, sensitivity, specificity and AUC of the proposed method reach 97.45%, 98.35%, 94.93% and 0.9966 for the BraTS2017 dataset, 98.99%, 99.18%,98.33% and 0.9993 for the BraTS2019 dataset, respectively. Compared with the classical networks and other algorithms, the classification framework proposed in this paper has the best performance on glioma classification.},
  archive      = {J_IETIP},
  author       = {Jiang Linqi and Ning Chunyu and Li Jingyang},
  doi          = {10.1049/ipr2.12374},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {596-605},
  shortjournal = {IET Image Process.},
  title        = {Glioma classification framework based on SE-ResNeXt network and its optimization},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real time detection of driver fatigue based on CNN-LSTM.
<em>IETIP</em>, <em>16</em>(2), 576–595. (<a
href="https://doi.org/10.1049/ipr2.12373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fatigue driving is one of the main causes of traffic accidents. In order to solve this problem, a new Convolutional Neural Network and Long Short-Term Memory (CNN-LSTM) based real-time driver fatigue detection method is proposed. First of all, using simple linear clustering algorithm (SLIC), the driver&#39;s image is divided into super pixels of uniform size, which are used as input of CNN, and CNN is trained to automatically learn the features of eyes and mouth contained in the image, and then the location and area of eyes and mouth are obtained by using the trained CNN. On this basis, the eye feature parameter Perclos, mouth feature parameter MClosed and face orientation feature parameter Phdown are extracted, and the above feature parameters on the continuous time series and steering wheel angle feature parameter SA are taken as the input of LSTM, and the fatigue level is taken as the output to detect the fatigue state of the driver in real time. Experimental data shows that this method can not only overcome the influence of illumination, background, angle and individual differences, but also the accuracy of detection can reach 99.78%, and the average detection time is 16.94 ms/frame.},
  archive      = {J_IETIP},
  author       = {Ming-Zhou Liu and Xin Xu and Jing Hu and Qian-Nan Jiang},
  doi          = {10.1049/ipr2.12373},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {576-595},
  shortjournal = {IET Image Process.},
  title        = {Real time detection of driver fatigue based on CNN-LSTM},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Augmented global attention network for image
super-resolution. <em>IETIP</em>, <em>16</em>(2), 567–575. (<a
href="https://doi.org/10.1049/ipr2.12372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional networks dominate many machine vision fields. Nevertheless, a significant drawback of the convolution operation is that it only operates in the local region, so it lacks global information. Self-attention has become the latest technology for capturing long-range interactions, but it is mainly used for generative modeling and sequence modeling tasks. Using self-attention to tackle super-resolution as a substitute for convolution is considered. Therefore, augmented global attention convolution (AGAC) is proposed as an alternative to convolution to use self-attention for super-resolution. The proposed augmented global attention convolution can capture global context to produce more realistic super-resolution results. Due to the most existing works that have not exploited position information, a two-dimensional relative self-attention mechanism is proposed to enhance self-attention. To deal with the super-resolution task, the authors come up with an augmented global attention convolutional network (AGAN) to enhance the convolution operator with the self-attention mechanism through concatenating the convolution pattern map with the generated set of feature maps. Many experiments and analyses are conducted to demonstrate that the proposed model surpasses the advanced models with comparable parameters and performance.},
  archive      = {J_IETIP},
  author       = {Xiaobiao Du and Saibiao Jiang and Jie Liu},
  doi          = {10.1049/ipr2.12372},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {567-575},
  shortjournal = {IET Image Process.},
  title        = {Augmented global attention network for image super-resolution},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical broad learning system for hyperspectral image
classification. <em>IETIP</em>, <em>16</em>(2), 554–566. (<a
href="https://doi.org/10.1049/ipr2.12371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new spectral-spatial hyperspectral image (HSI) classification method called hierarchical broad learning system (HBLS) has been proposed in this paper. Specifically, it combines wavelet, broad learning system (BLS) and Gabor filters into a hierarchical structure. First of all, wavelet is used to reduce the observation noise of HSIs. Then BLS is adopted to acquire a set of pixelwise probability maps from the input data, and Gabor filters are used to explore spatial information by refining these probability maps. These two operations (BLS and Gabor filtering) are alternated to form a hierarchical architecture. And the discriminative spectral-spatial features can be extracted at each layer of the hierarchical architecture. Finally, the spectral-spatial features are fed into the standard BLS for classification. Experimental results on three widely used HSIs reveal that HBLS outperforms some state-of-the-art methods in terms of classification accuracy and sample complexity.},
  archive      = {J_IETIP},
  author       = {Guangrun Xiao and Yantao Wei and Huang Yao and Wei Deng and Jiazhen Xu and Donghui Pan},
  doi          = {10.1049/ipr2.12371},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {554-566},
  shortjournal = {IET Image Process.},
  title        = {Hierarchical broad learning system for hyperspectral image classification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LBP-based progressive feature aggregation network for
low-light image enhancement. <em>IETIP</em>, <em>16</em>(2), 535–553.
(<a href="https://doi.org/10.1049/ipr2.12369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At night or in other low-illumination environments, optical imaging devices cannot capture details and color information in images accurately because of the reduced number of photons captured and the low signal-to-noise ratio. Consequently, the image is very noisy with low contrast and inaccurate color information, which affects human visual perception and creates significant challenges in computer vision tasks. Low-light image enhancement has great research value because it aims to reduce image noise and improve image quality. In this study, we propose an LBP-based progressive feature aggregation network (P-FANet) for low-light image enhancement. The LBP feature has insensitivity to illumination, and it contains rich texture information. In the network, we input the LBP feature into each iteration of the network in an accompanying manner, which helps to restore some detailed information of the low-light image. First, we input the low-light image into the dual attention mechanism model to extract global features. Second, the extracted different features enter the feature aggregation module (FAM) for feature fusion. Third, we use the recurrent layer to share the features extracted at different stages, and use the residual layer to further extract deeper features. Finally, the enhanced image is output. The rationality of the method in this study has been verified through ablation experiments. Many experimental results show that the method in this study has greater advantages in subjective and objective evaluations compared with many other advanced methods.},
  archive      = {J_IETIP},
  author       = {Nana Yu and Jinjiang Li and Zhen Hua},
  doi          = {10.1049/ipr2.12369},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {535-553},
  shortjournal = {IET Image Process.},
  title        = {LBP-based progressive feature aggregation network for low-light image enhancement},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image quality enhancement using hybrid attention networks.
<em>IETIP</em>, <em>16</em>(2), 521–534. (<a
href="https://doi.org/10.1049/ipr2.12368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image quality enhancement aims to recover rich details from degraded images, which is applied into many fields, such as medical imaging, filming production and autonomous driving. Deep convolutional neural networks (CNNs) have enabled rapid development of image quality enhancement. However, most existing CNN-based methods lack versatility when targeting different subtasks in terms of the design of networks. Besides, they often fail to balance precise spatial representations and necessary contextual information. To deal with these problems, this paper proposes a novel unified framework for low-light image enhancement, image denoising and image super-resolution. The core of this architecture is a residual hybrid attention block (RHAB), which consists of several dynamic down-sampling modules (DDM) and hybrid attention up-sampling modules (HAUM). Specifically, multi-scale feature maps are fully interacted with each other with the help of nested subnetworks so that both high-resolution spatial details and high-level contextual information can be combined to improve the representation ability of the network. Further, a hybrid attention network (HAN) is proposed and evaluations on three separate subtasks demonstrate its good performance. Extensive experiments on the authors&#39; synthetic dataset, a more complex version, show that the authors&#39; method achieve better quantitative and visual results compared to other state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Jiachen Wang and Yingyun Yang and Yan Hua},
  doi          = {10.1049/ipr2.12368},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {521-534},
  shortjournal = {IET Image Process.},
  title        = {Image quality enhancement using hybrid attention networks},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Facial skin colour classification using machine learning and
hyperspectral imaging data. <em>IETIP</em>, <em>16</em>(2), 509–520. (<a
href="https://doi.org/10.1049/ipr2.12366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial skin colour is a primary indicator of human health, and skin care products aim to improve it. However, the beauty industry lacks accurate methods to automatically classify skin colour because it uses high granularity labels such as the Pantone scale, with up to 110 facial skin-colour types. In this study, the automatic classification of facial skin colour based on hyperspectral imaging and machine learning is investigated. An experiment is conducted using hyperspectral imaging to collect multi-dimensional big data on most of these skin-colour types. Owing to the multi-dimensionality of the data and the high granularity of skin-colour types, classifying the colour type accurately is challenging. Nevertheless, various machine-learning methods are applied and it is found that each had an advantage in categorizing a subset of colour types. Further, the features of the data are analysed and it is found that skin chromaticity and brightness could be classified separately to provide valuable information. Finally, to utilise this information and combine the advantages of various machine-learning methods, a two-stage integrated classifier is developed, which achieved 90.4% accuracy. This classifier can be used by the beauty industry to evaluate the effect of skincare products on facial skin colour.},
  archive      = {J_IETIP},
  author       = {Wanshan Zhu and Peng Sang and Yifan He},
  doi          = {10.1049/ipr2.12366},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {509-520},
  shortjournal = {IET Image Process.},
  title        = {Facial skin colour classification using machine learning and hyperspectral imaging data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CFNet: Context fusion network for multi-focus images.
<em>IETIP</em>, <em>16</em>(2), 499–508. (<a
href="https://doi.org/10.1049/ipr2.12363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-focus image fusion aims to generate a clear image by fusing multiple source images. Existing deep learning-based fusion methods often neglect the context information resulting in the loss of detail information. To address this issue, a context fusion network to merge multi-focus images, namely CFNet, is proposed. Specifically, a context fusion module is proposed to make full use of low-level pixels and high-level semantic features. Particularly, the pyramid fusion mechanism and cross-scale transfer strategy are adopted to ensure the visual and semantic consistency of the fused image. Meanwhile, to extract salient features more effectively, a spatial attention mechanism is introduced to enhance these features. Further, the pyramid loss is used to progressively refine the fused features at each scale. Experimental results show that the proposed method is superior to some existing methods in both qualitative and quantitative evaluation.},
  archive      = {J_IETIP},
  author       = {Kang Zhang and Zhiliang Wu and Xia Yuan and Chunxia Zhao},
  doi          = {10.1049/ipr2.12363},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {499-508},
  shortjournal = {IET Image Process.},
  title        = {CFNet: Context fusion network for multi-focus images},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image haze removal based on rolling deep learning and
retinex theory. <em>IETIP</em>, <em>16</em>(2), 485–498. (<a
href="https://doi.org/10.1049/ipr2.12362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multispectral remote sensing images are a very important data source, but its acquisition process is often affected by haze weather and other factors, resulting in the decline of image quality, blurred details and poor visual effect, which seriously affect their application and interpretation. To reduce the impact of haze on multispectral remote sensing image and improve the image clarity and the use value, a new haze removal method based on rolling deep learning and Retinex theory (RDLRT) was proposed here. It uses the rolling deep learning theory to realize the preliminary removal of image haze, and then calculates the peak signal-to-noise ratio (PSNR) value of the processed image, which is used to judge whether the obtained image is a stage effect image (SEI). Then, on the one hand, the SEI image is processed for colour saturation and brightness; on the other hand, the Retinex theory is used to further filter the haze of the SEI image. Finally, the results of the two processing are fused, and it is used as the restored image after removing the haze. A series of validation experiments were carried out with true multispectral remote sensing images, outdoor colour images and different methods, and good experimental results were obtained. The purpose of removing image haze and improving image quality is achieved. Through the comparison of these experiments, it is concluded that the RDLRT algorithm proposed in this paper can not only effectively remove haze in images, but also maintain good details and colour of the image, which has some popularization values for image pre-processing and restoration.},
  archive      = {J_IETIP},
  author       = {Shiqi Huang and Jie Xu and Zhigang Liu and Ke Sun and Ying Lu},
  doi          = {10.1049/ipr2.12362},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {485-498},
  shortjournal = {IET Image Process.},
  title        = {Image haze removal based on rolling deep learning and retinex theory},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LESC: Superpixel cut-based local expansion for accurate
stereo matching. <em>IETIP</em>, <em>16</em>(2), 470–484. (<a
href="https://doi.org/10.1049/ipr2.12361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid estimation of the accurate disparity between pixels is the goal of stereo matching. However, it is very difficult for the 3D labels-based methods due to huge search space of 3D labels, especially for high-resolution images. In this paper, a novel superpixel cut-based method is proposed, in an attempt to get the accurate disparity map efficiently, including the multi-layer superpixel optimization and iteractive local -expansion in parallel. As for the multi-layer superpixel optimization, feature point optimization is designed to get accurate candidate labels that are set for most pixels using non-local cost aggregation strategy and update per-pixel labels of the corresponding superpixels from the candidate label sets on the small-size superpixel layer, and then update the middle to large-size superpixel layers progressively using non-local cost aggregation strategy. In order to provide more prior information to identify weak texture and textureless regions in non-local cost aggregation, the weight combination of “intensity + gradient + binary image” is proposed for constructing an optimal minimum spanning tree (MST) to calculate the aggregated matching cost and obtain the labels of minimum aggregated matching cost. Moreover, the local patch surrounding the corresponding superpixels is designed to accelerate superpixel optimization in parallel, and a neighborhood structure is presented to optimize the algorithm in this study, including superpixel neighborhood and patch neighborhood. As for the iteractive local -expansion, three layers of patch structure corresponding to the superpixel neighborhood structure is proposed for optimizing the algorithm in this study. The experimental results show that higher accuracy could be achieved via the method in this study compared with some known state-of-the-art stereo methods on KITTI 2015 and Middlebury benchmark V3, which are the standard benchmarks for testing the stereo matching methods.},
  archive      = {J_IETIP},
  author       = {Xianjing Cheng and Yong Zhao and Wenbang Yang and Zhijun Hu and Xiaomin Yu and Haiwei Sang and Guiying Zhang},
  doi          = {10.1049/ipr2.12361},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {470-484},
  shortjournal = {IET Image Process.},
  title        = {LESC: Superpixel cut-based local expansion for accurate stereo matching},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Metric of choosing the optimal parameter setting for edge
aware filtering. <em>IETIP</em>, <em>16</em>(2), 453–469. (<a
href="https://doi.org/10.1049/ipr2.12360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the existing edge aware filters have a number of parameters. The optimal settings of these parameters guarantee the best performance but they depend on the input image. It would be difficult for inexperienced users to empirically get the optimal parameters. This paper proposes a new metric for choosing the optimal parameter settings of edge aware filtering, which is called metric of edge aware filtering (MEAF). MEAF evaluates the quality of filtered images from three aspects: the color distance, the distance of the prominent structure, and smoothness. The colour distance is calculated by rooted mean square error. The distance of the prominent structure is calculated by the proposed SSIM map masked by Sobel edges (MASKED-SSIM). In MASKED-SSIM, Sobel detector is used to detect the prominent structure from the input image and the calculation of structure distance is constrained on the prominent structures. Number of gradients and relative total variation are further defined to measure the smoothness of the filtered image. MEAF is an objective metric, which is specially designed for choosing the optimal parameters of edge aware filtering and can be used universally for arbitrary input image. Experiments on 12 state-of-the-art edge aware filters show the effectiveness of MEAF.},
  archive      = {J_IETIP},
  author       = {Hui Yin and Fei Zhou and Bin Li},
  doi          = {10.1049/ipr2.12360},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {453-469},
  shortjournal = {IET Image Process.},
  title        = {Metric of choosing the optimal parameter setting for edge aware filtering},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-world super-resolution of face-images from surveillance
cameras. <em>IETIP</em>, <em>16</em>(2), 442–452. (<a
href="https://doi.org/10.1049/ipr2.12359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing face image Super-Resolution (SR) methods assume that the Low-Resolution (LR) images were artificially downsampled from High-Resolution (HR) images with bicubic interpolation. This operation changes the natural image characteristics and reduces noise. Hence, SR methods trained on such data most often fail to produce good results when applied to real LR images. To solve this problem, a novel framework for the generation of realistic LR/HR training pairs is proposed. The framework estimates realistic blur kernels, noise distributions, and JPEG compression artifacts to generate LR images with similar image characteristics as the ones in the source domain. This allows to train an SR model using high-quality face images as Ground-Truth (GT). For better perceptual quality, a Generative Adversarial Network (GAN) based SR model is used, where the commonly used VGG-loss [1] is exchanged with LPIPS-loss [2]. Experimental results on both real and artificially corrupted face images show that our method results in more detailed reconstructions with less noise compared to the existing State-of-the-Art (SoTA) methods. In addition, it is shown that the traditional non-reference Image Quality Assessment (IQA) methods fail to capture this improvement and demonstrate that the more recent NIMA metric [3] correlates better with human perception via Mean Opinion Rank (MOR).},
  archive      = {J_IETIP},
  author       = {Andreas Aakerberg and Kamal Nasrollahi and Thomas B. Moeslund},
  doi          = {10.1049/ipr2.12359},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {442-452},
  shortjournal = {IET Image Process.},
  title        = {Real-world super-resolution of face-images from surveillance cameras},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A combination of feature extraction methods and deep
learning for brain tumour classification. <em>IETIP</em>,
<em>16</em>(2), 416–441. (<a
href="https://doi.org/10.1049/ipr2.12358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a method for categorizing tumour disease from magnetic resonance imaging images using a convolutional neural network. The proposed technique consists of three major phases, including feature extraction, feature selection, and combination. The authors considered the classification method using convolutional neural network without any pre-processing on input images as the original method. The original method is then improved in some sequential phases when convolutional neural network uses features generated by feature extraction methods. Many popular feature extraction methods in generating the input features of the network are examined. After examining the results, a set of feature extraction schemes with appropriate performance are selected for the next phase. Also, the authors assigned weight factors to each of the selected methods, according to their accuracy. By assigning these weight factors to the methods, the network&#39;s accuracy increased to an acceptable level compared to the first and second observations. An accuracy above 99.76% was achieved, which is a 2% improvement using these feature extraction methods compared to the original method. The authors also added a third class named ‘I do not know’ to increase the tumour detection problem&#39;s reliability in the last phase. The authors have successfully introduced a convolutional neural network architecture with high accuracy and reliability.},
  archive      = {J_IETIP},
  author       = {Masoumeh Siar and Mohammad Teshnehlab},
  doi          = {10.1049/ipr2.12358},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {416-441},
  shortjournal = {IET Image Process.},
  title        = {A combination of feature extraction methods and deep learning for brain tumour classification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new image processing strategy for surface crack
identification in building structures under non-uniform illumination.
<em>IETIP</em>, <em>16</em>(2), 407–415. (<a
href="https://doi.org/10.1049/ipr2.12357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crack detection in building structures is an important approach to evaluating the safety of these structures. However, in some cases under special circumstances, it is very difficult to identify all features of cracks by visual inspection. To tackle this problem and offer a higher potential for practical implementations, developing an automatic crack detection method seems essential. This paper establishes a novel algorithm for surface crack detection in building walls based on image processing techniques to enhance crack visibility. One of the most prevalent issues in surface crack detection from digital crack images is non-uniform illumination of background which makes narrow and tiny cracks indistinguishable. In this regard, an integrated approach, combining adaptive image threshold using local first-order statistics, contrast-limited adaptive histogram equalization, and noise filtering using non-linear diffusion filtering, is proposed to extract the whole skeleton of the crack from digital crack images with non-uniform illumination. Several realistic crack images, suffering from noise and uneven lighting of background, are processed through the proposed algorithm to demonstrate the robustness and high accuracy of the method.},
  archive      = {J_IETIP},
  author       = {Ahmad Mahdian Parrany and Mohsen Mirzaei},
  doi          = {10.1049/ipr2.12357},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {407-415},
  shortjournal = {IET Image Process.},
  title        = {A new image processing strategy for surface crack identification in building structures under non-uniform illumination},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Disentangled representation learning GANs for generalized
and stable font fusion network. <em>IETIP</em>, <em>16</em>(2), 393–406.
(<a href="https://doi.org/10.1049/ipr2.12355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic generation of calligraphy fonts has attracted broad attention of researchers. However, previous font generation research mainly focused on the known font style imitation based on image to image translation. For poor interpretability, it is hard for deep learning to create new fonts with various font styles and features according to human understanding. To address this issue, the font fusion network based on generative adversarial networks (GANs) and disentangled representation learning is proposed in this paper to generate brand new fonts. It separates font into two understandable disentangled features: stroke style and skeleton shape. According to personal preferences, various new fonts with multiple styles can be generated by fusing the stroke style and skeleton shape of different fonts. First, this task improves the interpretability of deep learning, and is more challenging than simply imitating font styles. Second, considering the robustness of the network, a fuzzy supervised learning skill is proposed to enhance the stability of the fusion of two fonts with considerable discrepancy. Finally, instead of retraining, the authors&#39; trained model can be quickly transferred to other font fusion samples. It improves the efficiency of the model. Qualitative and quantitative results demonstrate that the proposed method is capable of efficiently and stably generating the new font images with multiple styles. The source code and the implementation details of our model are available at https://github.com/Qinmengxi/Fontfusion .},
  archive      = {J_IETIP},
  author       = {Mengxi Qin and Ziying Zhang and Xiaoxue Zhou},
  doi          = {10.1049/ipr2.12355},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {393-406},
  shortjournal = {IET Image Process.},
  title        = {Disentangled representation learning GANs for generalized and stable font fusion network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting adversarial examples by additional evidence from
noise domain. <em>IETIP</em>, <em>16</em>(2), 378–392. (<a
href="https://doi.org/10.1049/ipr2.12354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks are widely adopted powerful tools for perceptual tasks. However, recent research indicated that they are easily fooled by adversarial examples, which are produced by adding imperceptible adversarial perturbations to clean examples. Here the steganalysis rich model (SRM) is utilized to generate noise feature maps, and they are combined with RGB images to discover the difference between adversarial examples and clean examples. In particular, a two-stream pseudo-siamese network that fuses the subtle difference in RGB images with the noise inconsistency in noise features is proposed. The proposed method has strong detection capability and transferability, and can be combined with any model without modifying its architecture or training procedure. The extensive empirical experiments show that, compared with the state-of-the-art detection methods, the proposed approach achieves excellent performance in distinguishing adversarial samples generated by popular attack methods on different real datasets. Moreover, this method has good generalization, it trained by a specific adversary can defend against other adversaries effectively.},
  archive      = {J_IETIP},
  author       = {Song Gao and Shui Yu and Liwen Wu and Shaowen Yao and Xiaowei Zhou},
  doi          = {10.1049/ipr2.12354},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {378-392},
  shortjournal = {IET Image Process.},
  title        = {Detecting adversarial examples by additional evidence from noise domain},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhanced gradient learning for deep neural networks.
<em>IETIP</em>, <em>16</em>(2), 365–377. (<a
href="https://doi.org/10.1049/ipr2.12353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have achieved great success in both computer vision and natural language processing tasks. How to improve the gradient flows is crucial in training very deep neural networks. To address this challenge, a gradient enhancement approach is proposed through constructing the short circuit neural connections. The proposed short circuit is a unidirectional neural connection that back propagates the sensitivities rather than gradients in neural networks from the deep layers to the shallow layers. Moreover, the short circuit is further formulated as a gradient truncation operation in its connecting layers, which can be plugged into the backbone models without introducing extra training parameters. Extensive experiments demonstrate that the deep neural networks, with the help of short circuit connection, gain a large margin of improvement over the baselines on both computer vision and natural language processing tasks. The work provides the promising solution to the low-resource scenarios, such as, intelligence transport systems of computer vision, question answering of natural language processing.},
  archive      = {J_IETIP},
  author       = {Ming Yan and Jianxi Yang and Cen Chen and Joey Tianyi Zhou and Yi Pan and Zeng Zeng},
  doi          = {10.1049/ipr2.12353},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {365-377},
  shortjournal = {IET Image Process.},
  title        = {Enhanced gradient learning for deep neural networks},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quality assessment for inspection images of power lines
based on spatial and sharpness evaluation. <em>IETIP</em>,
<em>16</em>(2), 356–364. (<a
href="https://doi.org/10.1049/ipr2.12352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital imaging and image-processing techniques have revolutionized the way of power line inspection in recent years. Massive images are captured and utilized for further processing to maintain the reliability, safety, and sustainability of power transmission. For power line inspection, the component region in the delivered images is required to be centered, large, and clear enough. In this paper, a component-oriented image quality assessment method is proposed to automatically predict image quality according to the demand of power line inspection. The proposed method considers two factors: spatial characteristic evaluation and sharpness evaluation. The spatial characteristic evaluation utilizes YOLOv3 to evaluate whether the component region is sufficiently centered and large, which enables the observer to quickly find the target. The sharpness evaluation employs ResNet to evaluate the clarity of component and makes the condition monitoring more accurately. For final quality assessment, a multi-stage filtering strategy is presented to aggregate these two factors and obtain high quality inspection images. The experimental results indicate that the high-quality images can be accurately identified to satisfy the requirements of power line inspection. The proposed quality assessment method enhances the efficiency for further data analysis of aerial images.},
  archive      = {J_IETIP},
  author       = {Xinyu Liu and Zhiheng Jin and Hao Jiang and Xiren Miao and Jing Chen and Zhicheng Lin},
  doi          = {10.1049/ipr2.12352},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {356-364},
  shortjournal = {IET Image Process.},
  title        = {Quality assessment for inspection images of power lines based on spatial and sharpness evaluation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient video hashing based on low-rank frames.
<em>IETIP</em>, <em>16</em>(2), 344–355. (<a
href="https://doi.org/10.1049/ipr2.12351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video hashing is a useful technology for diverse video applications, such as digital watermarking, copy detection and content authentication. This paper proposes a novel efficient video hashing based on low-rank frames. A key contribution is the low-rank frame calculation using the low-rank approximation of singular value decomposition (SVD). As the large singular values of SVD are stable to digital operations, video hash extraction using low-rank frames can provide good robustness. Since most energy is contained within the large singular values, low-rank frames also contribute to discrimination. Moreover, two-dimensional discrete wavelet transform (DWT) is applied to every low-rank frame and the mean of low-frequency DWT coefficients is selected as a hash element. Since these coefficients can represent input data approximately, hash discrimination is thus ensured. Experiments with 16,850 videos are carried out to test performances of the proposed algorithm. The results show that the proposed algorithm outperforms some well-known video hashing algorithms in computational time and classification about discrimination and robustness.},
  archive      = {J_IETIP},
  author       = {Zhenhai Chen and Zhenjun Tang and Xinpeng Zhang and Ronghai Sun and Xianquan Zhang},
  doi          = {10.1049/ipr2.12351},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {344-355},
  shortjournal = {IET Image Process.},
  title        = {Efficient video hashing based on low-rank frames},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A coarse-refine segmentation network for COVID-19 CT images.
<em>IETIP</em>, <em>16</em>(2), 333–343. (<a
href="https://doi.org/10.1049/ipr2.12278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid spread of the novel coronavirus disease 2019 (COVID-19) causes a significant impact on public health. It is critical to diagnose COVID-19 patients so that they can receive reasonable treatments quickly. The doctors can obtain a precise estimate of the infection&#39;s progression and decide more effective treatment options by segmenting the CT images of COVID-19 patients. However, it is challenging to segment infected regions in CT slices because the infected regions are multi-scale, and the boundary is not clear due to the low contrast between the infected area and the normal area. In this paper, a coarse-refine segmentation network is proposed to address these challenges. The coarse-refine architecture and hybrid loss is used to guide the model to predict the delicate structures with clear boundaries to address the problem of unclear boundaries. The atrous spatial pyramid pooling module in the network is added to improve the performance in detecting infected regions with different scales. Experimental results show that the model in the segmentation of COVID-19 CT images outperforms other familiar medical segmentation models, enabling the doctor to get a more accurate estimate on the progression of the infection and thus can provide more reasonable treatment options.},
  archive      = {J_IETIP},
  author       = {Ziwang Huang and Liang Li and Xiang Zhang and Ying Song and Jianwen Chen and Huiying Zhao and Yutian Chong and Hejun Wu and Yuedong Yang and Jun Shen and Yunfei Zha},
  doi          = {10.1049/ipr2.12278},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {333-343},
  shortjournal = {IET Image Process.},
  title        = {A coarse-refine segmentation network for COVID-19 CT images},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A thorough review of models, evaluation metrics, and
datasets on image captioning. <em>IETIP</em>, <em>16</em>(2), 311–332.
(<a href="https://doi.org/10.1049/ipr2.12367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning means generate descriptive sentences from a query image automatically. It has recently received widespread attention from the computer vision and natural language processing communities as an emerging visual task. Currently, both components have evolved considerably by exploiting object regions, attributes, attention mechanism methods, entity recognition with novelties, and training strategies. However, despite the impressive results, the research has not yet come to a conclusive answer. This survey aims to provide a comprehensive overview of image captioning methods, from technical architectures to benchmark datasets, evaluation metrics, and comparison of state-of-the-art methods. In particular, image captioning methods are divided into different categories based on the technique adopted. Representative methods in each class are summarized, and their advantages and limitations are discussed. Moreover, many related state-of-the-art studies were quantitatively compared to determine the recent trends and future directions in image captioning. The ultimate goal of this work is to serve as a tool for understanding the existing literature and highlighting future directions in the area of image captioning for Computer Vision and Natural Language Processing communities may benefit from.},
  archive      = {J_IETIP},
  author       = {Gaifang Luo and Lijun Cheng and Chao Jing and Can Zhao and Guozhu Song},
  doi          = {10.1049/ipr2.12367},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {311-332},
  shortjournal = {IET Image Process.},
  title        = {A thorough review of models, evaluation metrics, and datasets on image captioning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comprehensive review of machine learning (ML) in image
defogging: Taxonomy of concepts, scenes, feature extraction, and
classification techniques. <em>IETIP</em>, <em>16</em>(2), 289–310. (<a
href="https://doi.org/10.1049/ipr2.12365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured through a visual sensory system are degraded in a foggy scene, which negatively influences recognition, tracking, and detection of targets. Efficient tools are needed to detect, pre-process, and enhance foggy scenes. Machine learning (ML) has a significant role in image defogging domain for tackling adverse issues. Unfortunately, regardless of contributions that were made by ML, little attention has been attributed to this topic. This paper summarizes the role of ML methods and relevant aspects in the image defogging research area. Also, the basic terms and concepts are highlighted in image defogging topic. Feature extraction approaches with a summary of advantages and disadvantages are described. ML algorithms are also summarized that have been used for applications related to image defogging, that is, image denoising, image quality assessment, image segmentation, and foggy image classification. Open datasets are also discussed. Finally, the existing problems of the image defogging domain in general and, specifically related to ML which need to be further studied are discussed. To the best knowledge, this the first review paper which sheds a light on the role of ML and relevant aspects in the image defogging domain.},
  archive      = {J_IETIP},
  author       = {Zainab Hussein Arif and Moamin A. Mahmoud and Karrar Hameed Abdulkareem and Mazin Abed Mohammed and Mohammed Nasser Al-Mhiqani and Ammar Awad Mutlag and Robertas Damaševičius},
  doi          = {10.1049/ipr2.12365},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {289-310},
  shortjournal = {IET Image Process.},
  title        = {Comprehensive review of machine learning (ML) in image defogging: Taxonomy of concepts, scenes, feature extraction, and classification techniques},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Corrigendum. <em>IETIP</em>, <em>16</em>(1), 285–288. (<a
href="https://doi.org/10.1049/ipr2.12356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IETIP},
  doi          = {10.1049/ipr2.12356},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {285-288},
  shortjournal = {IET Image Process.},
  title        = {Corrigendum},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep coordinate attention network for single image
super-resolution. <em>IETIP</em>, <em>16</em>(1), 273–284. (<a
href="https://doi.org/10.1049/ipr2.12364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques and deep networks have recently been extensively studied and widely applied to single image super-resolution (SR). Among them, channel attention has garnered the most focus owing to its significant boost in the presentational power of a convolutional neural network. However, the original channel attention neglects the critical importance of the positional information, thus imposing performance limitations. Here, a novel perspective, namely, a coordinate attention mechanism, is explored to alleviate the aforementioned problem, and accordingly result in an enhanced SR performance. Specifically, a deep residual coordinate attention SR network (COSR) is proposed, which mainly incorporates the presented coordinate attention blocks into a deep nested residual structure. The coordinate attention captures the positional information by computing the average value vector from the two spatial directions, thus aggregating the features in different coordinates. The nested residual blocks pass low-frequency information from the top to the end through the skip connection lines, allowing convolution filters to concentrate more on high-frequency textures and edges, thereby reducing the difficulty of reconstruction. Extensive experiments demonstrate that our proposed COSR achieves a better performance and exceeds many state-of-the-art SR methods in terms of both quantitative metrics and visual quality.},
  archive      = {J_IETIP},
  author       = {Chao Xie and Hongyu Zhu and Yeqi Fei},
  doi          = {10.1049/ipr2.12364},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {273-284},
  shortjournal = {IET Image Process.},
  title        = {Deep coordinate attention network for single image super-resolution},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data gap decomposed by auxiliary modality for NIR-VIS
heterogeneous face recognition. <em>IETIP</em>, <em>16</em>(1), 261–272.
(<a href="https://doi.org/10.1049/ipr2.12350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the dark scene at night, the face images captured by ordinary visible light (VIS) are generally poor quality and very dim, while the near-infrared (NIR) can capture high definition and recognizable face images at night. The NIR-VIS Heterogeneous face recognition has become a hot research field, which helps to build an all-weather face recognition system. NIR-VIS HFR is sophisticated because of the large visual difference between NIR images and VIS images. In order to reduce the difficulty of such cross-modality invariant feature learning, this paper proposes a cross-modality data gap decomposed by auxiliary modality method (DGD) for NIR-VIS HFR. First, the brightness component (Y component) of VIS image YCbCr space is used as the auxiliary modality to decompose the cross-modality data gap. The lightness component retained the structural information of VIS image and was similar to the colour information of NIR modality; in this way, the huge gap between the NIR data and the VIS data is decomposed into two smaller gaps, thus reducing the difficulty of network learning. Second, the data of the three modalities are input into the weight sharing network and training under the combined guidance of cross-modality gap decomposition loss and intra-modality gap loss; in this way, the modality invariant features can be learned faster and better. Extensive experiments were conducted on two commonly used datasets CASIA NIR-VIS 2.0 and Oulu-CASIA NIR-VIS to evaluate DGD method. Experimental results indicate DGD method has competitive performance compared with the latest methods.},
  archive      = {J_IETIP},
  author       = {Rui Sun and Xiaoquan Shan and Han Zhang and Jun Gao},
  doi          = {10.1049/ipr2.12350},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {261-272},
  shortjournal = {IET Image Process.},
  title        = {Data gap decomposed by auxiliary modality for NIR-VIS heterogeneous face recognition},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple classifier system for remotely sensed data
clustering. <em>IETIP</em>, <em>16</em>(1), 252–260. (<a
href="https://doi.org/10.1049/ipr2.12349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Multiple Classifier System (or classifier ensemble) is the consensus of different clustering algorithms that can provide high accuracy for the best partition and thus overcome the constraints of conventional approaches based on single classifiers. The MCS is divided into two stages: Partition creation and partition combining. The potential benefits of this methodology in unsupervised land cover categorization utilizing synthetic, composite, and remotely sensed data are investigated in this paper. Four clustering algorithms are used for the MCS&#39;s first step, and according to the WB index, the best-unsupervised classification is obtained. In the second stage, relabeling and, voting approaches are then applied. The MCS&#39;s experimental results outperform the individual clustering outcomes in terms of accuracy.},
  archive      = {J_IETIP},
  author       = {Lamia Fatma Houbaba Chaouche Ramdane and Habib Mahi and Mostafa El Habib Daho and Mohammed El Amine Lazouni},
  doi          = {10.1049/ipr2.12349},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {252-260},
  shortjournal = {IET Image Process.},
  title        = {Multiple classifier system for remotely sensed data clustering},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). New flexible deterministic compressive measurement matrix
based on finite galois field. <em>IETIP</em>, <em>16</em>(1), 239–251.
(<a href="https://doi.org/10.1049/ipr2.12348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the deterministic construction of sensing matrices is a hot topic in compressed sensing. The coherence of the measurement matrix is an important research area in the design of deterministic compressed sensing. To solve this problem, this paper proposes a novel sparse deterministic measurement matrix, which basically accesses the optimal low coherence of the measurement matrix. Firstly, a class of sparse square matrix is constructed based on finite fields&#39; arithmetic. Then, the Hadamard matrix, or (discrete Fourier transform) DFT matrix, is nestled into the square matrix to construct an asymptotically optimal deterministic measurement matrix. That is, the relevant column vectors have orthogonal characteristics. Using this feature, the measurement matrix can be further optimized to reduce its mutual coherence, almost achieving the lower bound of the coherence (Welch bound). The two types of deterministic measurement matrices proposed are sparse with low mutual coherence and flexible measurement sizes. So, the proposed deterministic measurement matrices require less memory and time for the recovery as well as reducing the complexity due to their sparse structure. The simulation results show that compared with the existing (several typical) random matrices, the proposed method can reduce the mutual coherence and computational complexity of the measurement matrix.},
  archive      = {J_IETIP},
  author       = {Vahdat Kazemi and Ali Shahzadi and Hossein Khaleghi Bizaki},
  doi          = {10.1049/ipr2.12348},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {239-251},
  shortjournal = {IET Image Process.},
  title        = {New flexible deterministic compressive measurement matrix based on finite galois field},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PCAF-net: A liver segmentation network based on deep
learning. <em>IETIP</em>, <em>16</em>(1), 229–238. (<a
href="https://doi.org/10.1049/ipr2.12346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Liver cancer poses a great threat to people&#39;s health. Accurate liver segmentation is crucial to the diagnosis of liver cancer. In recent years, great achievements have been made in liver segmentation using a series of improved networks developed based on U-Net. U-Net uses a skip connection splices the feature map in the encoder and decoder. However, this method ignores the difference between the two feature maps, which limits the ability of the network to extract liver structures of different sizes. This study proposes a new network structure, the pyramid convolutional attention fusion network (PCAF-Net), for 2D liver segmentation. The PCAF mechanism was introduced to fuse the feature maps of different receptive field paths after attention mechanism processing to enhance the semantic expression ability of feature maps in skip connection and to improve the segmentation accuracy. The MICCAI 2017 LiTS dataset and CHAOS were used to validate the proposed method. The results of multi-index evaluation showed that the segmentation performance of the proposed network was superior to those of other networks. PCAF-Net achieved accurate liver segmentation, providing a reference for artificial intelligenceassisted clinical diagnosis of liver cancer.},
  archive      = {J_IETIP},
  author       = {Yanlin Wu and Guanglei Wang and Zhongyang Wang and Hongrui Wang},
  doi          = {10.1049/ipr2.12346},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {229-238},
  shortjournal = {IET Image Process.},
  title        = {PCAF-net: A liver segmentation network based on deep learning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-focus image fusion method based on multi-source
joint layering and convolutional sparse representation. <em>IETIP</em>,
<em>16</em>(1), 216–228. (<a
href="https://doi.org/10.1049/ipr2.12345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new Multi-Focus Image Fusion (MFIF) method based on multi-source joint layering and Convolutional Sparse Representation (CSR) is proposed. Based on the characteristics of multi-focus source images, a multi-source joint layering regularization model was designed to divide the sources into a common base-layer and respective focus detail-layers. This strategy can overcome the defects caused by source layering separately effectively. In detail-layer fusion, CSR was employed to extract and global features. It can avoid detail blur and high computational cost caused by image blocking in the conventional sparse representation model. The proposed detail-layer fusion rule combined the CSR coefficient maps pairwise with the window based select-max rule. In the experiments, the optimal layering parameter was selected by experiments at first, and then five recently proposed specific MFIF or general image fusion algorithms were contrasted with the proposed method by plenty of subjective and objective experimental comparisons. The experimental results demonstrated the superiority of the authors’ method.},
  archive      = {J_IETIP},
  author       = {Yanxiang Hu and Zhijie Chen and Bo Zhang and Lifeng Ma and Jiaqi Li},
  doi          = {10.1049/ipr2.12345},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {216-228},
  shortjournal = {IET Image Process.},
  title        = {A multi-focus image fusion method based on multi-source joint layering and convolutional sparse representation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heterogeneous face detection based on multi-task cascaded
convolutional neural network. <em>IETIP</em>, <em>16</em>(1), 207–215.
(<a href="https://doi.org/10.1049/ipr2.12344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial target detection is an important task in computer vision. Because heterogeneous face detection shows broad prospects, it has attracted extensive attention from the academic community. In recent years, with the rise of deep learning and its applications in computer vision, face detection technology has made great strides. This paper uses multi-task cascaded convolutional neural network (MTCNN) for heterogeneous face feature detection. This algorithm makes full use of the advantages of image pyramid, boundary regression, fully convolutional attention networks and non-maximum suppression. The main idea of this paper is to use candidate frame plus classifier for fast and efficient face detection. Specifically, the candidate window is generated by the proposal network (P-Net), and the high-precision candidate window is filtered and selected by the reduced network (R-Net), and the final bounding box and facial key points are generated by the output network (O-Net). In order to prove the effectiveness of this method in visible light, near-infrared and sketch face recognition scenes, it was verified in the datasets of CUFS, CUFSF and CASIA NIR-VIS 2.0. Experiments show that this method is effective for face images in heterogeneous face and is better than the latest algorithms.},
  archive      = {J_IETIP},
  author       = {XianBen Yang and Wei Zhang},
  doi          = {10.1049/ipr2.12344},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {207-215},
  shortjournal = {IET Image Process.},
  title        = {Heterogeneous face detection based on multi-task cascaded convolutional neural network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Disentangled and controllable sketch creation based on
disentangling the structure and color enhancement. <em>IETIP</em>,
<em>16</em>(1), 191–206. (<a
href="https://doi.org/10.1049/ipr2.12343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing sketch-based image processing methods include sketch recognition, sketch synthesis and sketch-based image retrieval. For sketch creation, a meaningful task is proposed namely disentangled and controllable sketch creation (DCSC) based on disentangling the structure and color enhancement. Specifically, as the first subtask, sketch structure enhancement (SSE) is used to enhance a non-professional sketch (NPS) and obtain a professional sketch (PS), which is a process denoted as NPS2PS. A data set named SketchMan is first provided, consisting of NPSs and PSs with various postures in different scenes. SSE is trained as a conditional image-to-image translation problem, and there are three models: direct sketch-to-sketch (SS), grayscale guided SS and contour guided SS. Multiple IOU metrics are proposed based on Corner Point Map (CPM), Straight Line Map (SLM) and Segmented Area Map (SAM). As the second subtask, sketch color enhancement (SCE) is trained as a two-stage framework containing a topology enhancement network (TE-Net) that maps a sketch to the corresponding grayscale domain and a color injection network (CI-Net) that injects the global color feature to the AdaIN residual blocks to perform adaptive sketch colorization. The TE-Net and CI-Net disentangle the topological and color features to perform more controllable and diverse SCE results. Experimental results demonstrate that our proposed methods are effective to address the challenging and meaningful DCSC task compared with other state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Nan Gao and Hui Ren and Jia Li and ZhiBin Su},
  doi          = {10.1049/ipr2.12343},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {191-206},
  shortjournal = {IET Image Process.},
  title        = {Disentangled and controllable sketch creation based on disentangling the structure and color enhancement},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ESA-CycleGAN: Edge feature and self-attention based
cycle-consistent generative adversarial network for style transfer.
<em>IETIP</em>, <em>16</em>(1), 176–190. (<a
href="https://doi.org/10.1049/ipr2.12342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, style transfer is used in a wide range of commercial applications, such as image beautification, film rendering etc. However, many existing methods of style transfer suffer from loss of details and poor overall visual effect. To address these problems, an edge feature and self-attention based cycle-consistent generative adversarial network (ESA-CycleGAN) is proposed. The model architecture consists of a generator, a discriminator, and an edge feature extraction network. Both the generator and the discriminator contain a self-attention module to capture global features of the image. The edge feature extraction network extracts the edge of the original image and feeds it into the network together with the original image, thereby allowing better processing of details. Besides, a perceptual loss term is added to optimize the network, resulting in better perceptual results. ESA-CycleGAN is applied on four datasets, respectively. The experimental results show that the authors’ computed final IS and FID values have good results compared to the results of several other existing models, indicating the superiority of the model in style transfer, which can better preserve the details of the original images with better image quality.},
  archive      = {J_IETIP},
  author       = {Li Wang and Lidan Wang and Shubai Chen},
  doi          = {10.1049/ipr2.12342},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {176-190},
  shortjournal = {IET Image Process.},
  title        = {ESA-CycleGAN: Edge feature and self-attention based cycle-consistent generative adversarial network for style transfer},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Underwater image enhancement via LBP-based attention
residual network. <em>IETIP</em>, <em>16</em>(1), 158–175. (<a
href="https://doi.org/10.1049/ipr2.12341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the influence of light absorption and scattering in underwater environments, underwater images exhibit color deviation, low contrast and detail blur, and other degradations. This paper proposes an underwater image enhancement method combining a residual convolution network, local binary pattern (LBP), and self-attention mechanism. The LBP operator processes the input underwater images. The LBP feature images and underwater images thus obtained constitute the network input. The network consists of three modules: a color correction module to remove the color deviation in underwater images, detail repair module to restore the integrity of details, and an LBP auxiliary enhancement module for global enhancement of image details. The correction and repair modules generate the correct color image and detailed supplement images, respectively. The final-result image is obtained by superpositioning the two generated images. The experimental results confirm that our method can reproduce the bright colors and complete details of the visual effect, showing a significant improvement over other advanced methods in quantitative evaluation.},
  archive      = {J_IETIP},
  author       = {ZhiXiong Huang and Jinjiang Li and Zhen Hua},
  doi          = {10.1049/ipr2.12341},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {158-175},
  shortjournal = {IET Image Process.},
  title        = {Underwater image enhancement via LBP-based attention residual network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trident-YOLO: Improving the precision and speed of mobile
device object detection. <em>IETIP</em>, <em>16</em>(1), 145–157. (<a
href="https://doi.org/10.1049/ipr2.12340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduce an efficient object detection network named Trident-You Only Look Once (YOLO), which is designed for mobile devices with limited computing power. The new architecture is improved based on YOLO v4-tiny. The authors redesign the network structure and propose a trident feature pyramid network (Trident-FPN), which can improve the precision and recall of lightweight object detection. Specifically, Trident-FPN increases the computational complexity by only a small amount of floating point operations per second (FLOPs) and obtains a multi-scale feature map of the model, which significantly lightweight object detection performance. To enlarge the receptive field of the network with the fewest FLOPs, this paper redesign the receptive field block (RFB) and spatial pyramid pooling (SPP) layer and propose tinier cross-stage partial RFBs and smaller cross-stage partial SPPs. This paper present extensive experiments, and Trident-YOLO shows strong performance compared to that of other popular models on the PASCAL VOC and MS COCO. On the MS COCO and PASCAL VOC 2007 test sets, the mean average precision (mAP) of Trident-YOLO improved by 4.5% and 5.0%, respectively. Trident-YOLO also reduce the network size by more than 54.4% compared to YOLO v4-tiny. With a 23.7% FLOP reduction, the FPS is improved by 1.9 on an Nvidia Jetson Xavier NX.},
  archive      = {J_IETIP},
  author       = {Guanbo Wang and Hongwei Ding, and Bo Li and Rencan Nie and Yifan Zhao},
  doi          = {10.1049/ipr2.12340},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {145-157},
  shortjournal = {IET Image Process.},
  title        = {Trident-YOLO: Improving the precision and speed of mobile device object detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coal gangue detection and recognition algorithm based on
deformable convolution YOLOv3. <em>IETIP</em>, <em>16</em>(1), 134–144.
(<a href="https://doi.org/10.1049/ipr2.12339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intelligentisation of coal mines is the only approach to the high-quality development of the coal industry. Detection, identification and sorting of coal gangue is an important part of the intelligentisation of coal mines. Focusing on various problems in coal gangue detecting and recognising algorithms, such as limited receptive field, slow convergence rate and low accuracy of small particle recognition, this paper proposes a coal gangue detection and recognition algorithm based on deformable convolution YOLOv3 (DCN-YOLOv3). To improve the accuracy of anchor frame positioning and enhance the diversity of the dataset, the deformed convolution YOLOv3 network model is established based on the detection algorithm YOLOv3, using deformable convolution, multiple k-means clustering results average method and data enhancement technology as means. The model was trained through the self-designed dataset, and the algorithm&#39;s correctness and accuracy for coal gangue recognition under different size and illumination conditions are verified. The test results showed that the algorithm effectively detects and recognises coal gangue, improves the accuracy and efficiency of detecting and recognising small-size coal and gangue and improves environmental robustness. Furthermore, compared with the traditional recognition algorithm, the network convergence speed of this algorithm is significantly improved, the mAP is increased to 99.45%, and the maximum FLOPs value is reduced by 61.4%. Accordingly, this research is considered to be of certain theoretical value and technical reference for identifying coal gangue.},
  archive      = {J_IETIP},
  author       = {De-yong Li and Guo-fa Wang and Yong Zhang and Shuang Wang},
  doi          = {10.1049/ipr2.12339},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {134-144},
  shortjournal = {IET Image Process.},
  title        = {Coal gangue detection and recognition algorithm based on deformable convolution YOLOv3},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Learning compact ConvNets through filter pruning based on
the saliency of a feature map. <em>IETIP</em>, <em>16</em>(1), 123–133.
(<a href="https://doi.org/10.1049/ipr2.12338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the performance increase of convolutional neural network (CNN), the disadvantages of CNN&#39;s high storage and high power consumption are followed. Among the methods mentioned in various literature, filter pruning is a crucial method for constructing lightweight networks. However, the current filter pruning method is still challenged by complicated processes and training inefficiency. This paper proposes an effective filter pruning method, which uses the saliency of the feature map (SFM), i.e. information entropy, as a theoretical guide for whether the filter is essential. The pruning principle use here is that the filter with a weak saliency feature map in the early stage will not significantly improve the final accuracy. Thus, one can efficiently prune the non-salient feature map with a smaller information entropy and the corresponding filter. Besides, an over-parameterized convolution method is employed to improve the pruned model&#39;s accuracy without increasing parameter at inference time. Experimental results show that without introducing any additional constraints, the effectiveness of this method in FLOPs and parameters reduction with similar accuracy has advanced the state-of-the-art. For example, on CIFAR-10, the pruned VGG-16 achieves only a small loss of 0.39% in Top-1 accuracy with a factor of 83.3% parameters, and 66.7% FLOPs reductions. On ImageNet-100, the pruned ResNet-50 achieves only a small accuracy degradation of 0.76% in Top-1 accuracy with a factor of 61.19% parameters, and 62.98% FLOPs reductions.},
  archive      = {J_IETIP},
  author       = {Zhoufeng Liu and Xiaohui Liu and Chunlei Li and Shumin Ding and Liang Liao},
  doi          = {10.1049/ipr2.12338},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {123-133},
  shortjournal = {IET Image Process.},
  title        = {Learning compact ConvNets through filter pruning based on the saliency of a feature map},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An optimal 3D convolutional neural network based lipreading
method. <em>IETIP</em>, <em>16</em>(1), 113–122. (<a
href="https://doi.org/10.1049/ipr2.12337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lipreading is a visual recognition of speech by using lip movement, which aims to recognise phrases and sentences spoken by a talking face without the audio. However, the existed models for lipreading suffer from slow training speed and insufficient performance. To accelerate the training speed of the model for lipreading, a batch group training algorithm is proposed, which groups all the data of different frames. In addition, a 3D-MouthNet-BLSTM-CTC architecture for lipreading is proposed to improve model performance. It bases on a 3D convolutional neural network, MouthNet, two Bi-LSTMs, and a CTC objective function. Experiment results in Oulu-VS2 and self-built dataset show that 96.2% accuracy rate is achieved on the Oulu-VS2 dataset, and 93.8% accuracy rate is achieved on the GRID dataset. This article is about lipreading research. It mainly uses deep learning methods to study lip-reading. A new network architecture and tests on public data sets are proposed to achieve the best results.},
  archive      = {J_IETIP},
  author       = {Lun He and Biyun Ding and Hao Wang and Tao Zhang},
  doi          = {10.1049/ipr2.12337},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {113-122},
  shortjournal = {IET Image Process.},
  title        = {An optimal 3D convolutional neural network based lipreading method},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ensemble cross-stage partial attention network for image
classification. <em>IETIP</em>, <em>16</em>(1), 102–112. (<a
href="https://doi.org/10.1049/ipr2.12335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel image classification architecture named ensemble cross-stage partial attention network based on the backbone network DarkNet53 of Yolov3 to improve the feature extraction capability and the interpretability of image classification. This network has multiple advantages, including light model parameters, fast classification speed, and high classification accuracy for small objects and complex images. Local network architectures of different cross-phases are added in the proposed network structure to reduce the calculation. Furthermore, channel and hybrid domain attention modules, which, respectively, fuse the branch feature with the extracted channel and spatial attention features, are designed for feature extraction of images. Experimental results confirm the improved performance of the proposed approach on the CIFAR-100, ImageNet, and UCMerced datasets. In addition, experiments on the MSCOCO dataset suggest the application of the proposed method to object detection with satisfactory accuracy.},
  archive      = {J_IETIP},
  author       = {Hai Lin and JunJie Yang},
  doi          = {10.1049/ipr2.12335},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {102-112},
  shortjournal = {IET Image Process.},
  title        = {Ensemble cross-stage partial attention network for image classification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Road infrared target detection with i-YOLO. <em>IETIP</em>,
<em>16</em>(1), 92–101. (<a
href="https://doi.org/10.1049/ipr2.12331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of road infrared targets is essential for autonomous driving. Different from RGB images, the acquisition of infrared images is unaffected by visible light. However, the signal-to-noise ratio still presents significant challenges. This study demonstrates an improved infrared target detection model for road infrared target detection. An advanced EfficientNet is incorporated to replace the conventional structure and enhance feature extraction. A Dilated-Residual U-Net is also introduced to reduce the noise of infrared images. Meanwhile, the k -means algorithm and data enhancement are implemented to improve the detection performance. The experimental results show that the mean average precision of the proposed model is observed to be 0.89 for the infrared road dataset with an average detection speed of 10.65 s −1 .},
  archive      = {J_IETIP},
  author       = {Mingyuan Sun and Haochun Zhang and Ziliang Huang and Yueqi Luo and Yiyi Li},
  doi          = {10.1049/ipr2.12331},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {92-101},
  shortjournal = {IET Image Process.},
  title        = {Road infrared target detection with I-YOLO},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hyperspectral image classification with deep 3D capsule
network and markov random field. <em>IETIP</em>, <em>16</em>(1), 79–91.
(<a href="https://doi.org/10.1049/ipr2.12330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the existing problems of capsule networks in deep feature extraction and spatial-spectral feature fusion of hyperspectral images, this paper proposes a hyperspectral image classification method that combines a deep residual 3D capsule network and Markov random field. Based on this method, the deep spatial-spectral features of hyperspectral images are extracted using the deep residual 3D convolutional structure, the vector capsules of the features are obtained by the initial capsule layer and mapped into probability capsules via the 3D dynamic routing mechanism to construct the classification probability map, and the spatial structure of the classification results is regularised by the Markov random field to further improve the classification accuracy and performance of the images. Two sets of benchmark hyperspectral images, namely Indian Pines and Pavia University data sets, were used to conduct comparative experiments and ablation study. The experimental results showed that, compared with the conventional convolutional neural network and existing capsule network models, the proposed method not only improves the classification accuracy of the images but also partly eliminates the category noise and affords a more regular classification probability map.},
  archive      = {J_IETIP},
  author       = {Xiong Tan and Zhixiang Xue and Xuchu Yu and Yifan Sun and Kuiliang Gao},
  doi          = {10.1049/ipr2.12330},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {79-91},
  shortjournal = {IET Image Process.},
  title        = {Hyperspectral image classification with deep 3D capsule network and markov random field},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image enhancement via texture protection retinex.
<em>IETIP</em>, <em>16</em>(1), 61–78. (<a
href="https://doi.org/10.1049/ipr2.12311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images obtained in dim light do not clearly represent the target scene, limiting information transmission on the image carrier. This study proposes a texture-preserving image enhancement method, i.e. ETPR. The proposed method draws the illumination map of a low light image by Max-RGB, and then applies the weighted median filter algorithm and Retinex to improve the illumination map further. Then, the enhanced image is changed from RGB mode to YCbCr mode and the texture of the luminance component Y is described. It is achieved by denoising the texture of the image to be enhanced, and then the final enhanced image is obtained by the method of sub-region image fusion denoising. The effectiveness of ETPR is established by comparing it with existing enhancement technologies using public data and real images.},
  archive      = {J_IETIP},
  author       = {Linlu Dong and Liangjun Zhao and Jun Wang},
  doi          = {10.1049/ipr2.12311},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {61-78},
  shortjournal = {IET Image Process.},
  title        = {Image enhancement via texture protection retinex},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Learning spatial self-attention information for visual
tracking. <em>IETIP</em>, <em>16</em>(1), 49–60. (<a
href="https://doi.org/10.1049/ipr2.12294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual object tracking has been a fundamental topic in computer vision and many convolutional neural network (CNN) based trackers proposed in recent years have achieved state-of-the-art performance, multi-domain convolutional neural network (MDNet) is one of the most representative CNN-based algorithms with high performance. In order to significantly improve the robustness and accuracy of the MDNet algorithm, a multi-domain convolutional neural network based on spatial self-attention information learning (SAMDNet) is proposed in this study. The authors use the spatial self-attention module for the spatial information learning of the model. The spatial self-attention module selectively aggregates the feature at each position by a weighted sum of the features at all positions. Under the control of self-learning parameters in this module, spatial attention information can be flexibly acquired. The authors also propose a novel interval loss term to solve the problem of different classes with the same semantics in the training data. Finally, an anomaly detection module is carefully designed for relocation after the algorithm completely lost the target. Extensive experiments on the object tracking benchmark (OTB) and the visual object tracking challenge (VOT) benchmarks show that the proposed tracker outperforming most of the state-of-art trackers.},
  archive      = {J_IETIP},
  author       = {Shengwu Li and Xuande Zhang and Jing Xiong and Chenjing Ning and Mingke Zhang},
  doi          = {10.1049/ipr2.12294},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {49-60},
  shortjournal = {IET Image Process.},
  title        = {Learning spatial self-attention information for visual tracking},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DDNet: 3D densely connected convolutional networks with
feature pyramids for nasopharyngeal carcinoma segmentation.
<em>IETIP</em>, <em>16</em>(1), 39–48. (<a
href="https://doi.org/10.1049/ipr2.12248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radiation therapy is the standard treatment for early stage Nasopharyngeal cancer (NPC). Thus, accurate delineation of target volumes at risk in NPC is important. While manual delineation is time-consuming and labour-intensive process and also leads to significant inter- and intra-practitioner variability. Thus, computer-aided segmentation algorithm is required. However, segmentation task is not trivial due to large variations (e.g., shape and size) of nasopharynx structure across subjects. Moreover, extreme foreground and background class imbalance in NPC segmentation remains challenge. In this paper, we propose a threedimensional densely connected convolutional neural network with multi-scale feature pyramids for NPC segmentation. We adapt the densely connected convolutional block into a new structure via adding feature pyramids. The concatenated pyramid feature carries multi-scale and hierarchical semantic information which is effective for segmenting different size of tumors and perceiving hierarchical context information. To address the foreground and background imbalance problem, we propose an enhanced version of focal loss. It prevents the large number of negative voxels far from boundaries from overwhelming the segmentation algorithm. We validated the proposed method on 120 clinical subjects. Experimental results demonstrate that our approach out-performed state-of-the-art methods and human experts.},
  archive      = {J_IETIP},
  author       = {Xiaojie Li and Mingxuan Tang and Feng Guo and Yuanxi Li and Kunling Cao and Qi Song and Xi Wu and Shanhui Sun and Jiliu Zhou},
  doi          = {10.1049/ipr2.12248},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {39-48},
  shortjournal = {IET Image Process.},
  title        = {DDNet: 3D densely connected convolutional networks with feature pyramids for nasopharyngeal carcinoma segmentation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CP-GAN: Meet the high requirements of diagnose report to
medical image by content preservation. <em>IETIP</em>, <em>16</em>(1),
29–38. (<a href="https://doi.org/10.1049/ipr2.12145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image generation from diagnostic report has important research significance for medical-aided diagnosis. This research can improve the diagnosis speed and accuracy of doctors and effectively save the storage resources of hospitals. The research has made significant progress in the field of natural images, but rarely used in medical images. Medical images have higher requirements for image quality. In this paper, a method based on attention mechanism and content preservation loss to improve image quality is proposed. The model consists of three stages, and each stage generates feature map of different scale combined with attention features as the input of the next stage to optimise semantic consistency between image and text. The last stage will generate an image, whose size is 256 × 256. And the content preservation loss can optimise the similarity between the generated image and the real image by low-level and high-level features to meet the high requirements of medical image for texture details. The content preservation loss consists of MSE loss, VGG loss and TV loss. The experiments on two datasets prove that the method can achieve excellent results.},
  archive      = {J_IETIP},
  author       = {Xianhua Zeng and Zhengyi Huang and Liming Xu and Yicai Xie},
  doi          = {10.1049/ipr2.12145},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {29-38},
  shortjournal = {IET Image Process.},
  title        = {CP-GAN: Meet the high requirements of diagnose report to medical image by content preservation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comprehensive survey: Image deraining and stereo-matching
task-driven performance analysis. <em>IETIP</em>, <em>16</em>(1), 11–28.
(<a href="https://doi.org/10.1049/ipr2.12347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deraining has been attracting a lot of attention from researchers, and various methods have been proposed, especially deep-networks are widely adopted in recent years. Their structures and learning become more and more complicated and diverse, making it difficult to analyze the contributions and improvements. In this paper, a comprehensive review for current rain removal methods is first provided to show their contributions. Specifically, they are reviewed in terms of handing rain streaks and rain mist. Second, besides evaluating their rain removal ability, they are also evaluated in terms of their impact on subsequent stereo-matching task. To this end, a new deraining dataset is first prepared, called Rain-Kitti2012 and Rain-Kitti2015. They are created by adding rain part to clean image-pairs in Kitti2012 and Kitti2015. By then, nine state-of-the-art deraining methods are evaluated with full-reference and no-reference image quality assessment metrics. Furthermore, the blurriness and distortion types introduced during deraining are measured. Finally, three learning-based stereo matching methods are compared, and they take the outputs of deraining methods as inputs. It is further discussed how derained images influence the accuracy of stereo matching, which can provide some insight for jointly handling rain removal and stereo matching. 1: A comprehensive review for the current rain removal methods is provided. They are categorized into rain-streak-oriented and rain-mist-oriented approaches in terms of degradation type, and are categorized into model-driven and data-driven approaches in terms of methodology. 2: A new image deraining dataset is introduced, which is the first dataset that can be used to perform stereo-matching-driven evaluation for deraining methods. The dataset is created by adding rain part to clean images in KITTI2012 and KITTI2015. 3: We evaluate 9 deep learning based deraining methods with full-reference and no- reference metrics. In addition, the types of distortions produced by these methods are discussed and measured quantitatively. And, the impact of 9 deraining methods on the subsequent stereo matching task is evaluated, which can provide some insight on how to design stereo matching task-driven deraining methods.},
  archive      = {J_IETIP},
  author       = {Shuangli Du and Yiguang Liu and Minghua Zhao and Zhenghao Shi and Zhenzhen You and Jie Li},
  doi          = {10.1049/ipr2.12347},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {11-28},
  shortjournal = {IET Image Process.},
  title        = {A comprehensive survey: Image deraining and stereo-matching task-driven performance analysis},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SAR image classification based on multi-feature fusion
decision convolutional neural network. <em>IETIP</em>, <em>16</em>(1),
1–10. (<a href="https://doi.org/10.1049/ipr2.12323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the success of convolutional neural network (CNN) in the field of image processing and image recognition, an improved fully convolutional neural network (FCNN) based on CNN is proposed. Although FCNN has better performance than CNN, the number of parameters of FCNN has also increased significantly compared to CNN. Consequently, this paper proposes a multi-feature fusion decision convolutional neural network (MFFD-CNN), which use two kinds down-sample methods in the pooling layer, and chooses the concatenate or add operation as the way that consolidates the feature of the convolutional layer with a stride of 2 and the max-pooling layer to execute fusion operations. Meanwhile replaces the traditional softmax loss with an additive margin Softmax (AM-Softmax) loss. The structure, which is based on the multi-target classification experiment results of the MSTAR data set, shows can not only obtain an average correct recognition rate 1.2% higher than FCNN, but also better stability without augmenting the training samples of MSTAR data.},
  archive      = {J_IETIP},
  author       = {Liang Guo},
  doi          = {10.1049/ipr2.12323},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-10},
  shortjournal = {IET Image Process.},
  title        = {SAR image classification based on multi-feature fusion decision convolutional neural network},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
