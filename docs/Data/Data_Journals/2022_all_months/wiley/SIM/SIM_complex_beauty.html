<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sim---350">SIM - 350</h2>
<ul>
<li><details>
<summary>
(2022). Bayesian meta-analytical methods to incorporate multiple
surrogate endpoints in drug development process. <em>SIM</em>,
<em>41</em>(30), 5877–5878. (<a
href="https://doi.org/10.1002/sim.9597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Sylwia Bujkiewicz},
  doi          = {10.1002/sim.9597},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5877-5878},
  shortjournal = {Stat. Med.},
  title        = {Bayesian meta-analytical methods to incorporate multiple surrogate endpoints in drug development process},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Review and evaluation of imputation methods for multivariate
longitudinal data with mixed-type incomplete variables. <em>SIM</em>,
<em>41</em>(30), 5844–5876. (<a
href="https://doi.org/10.1002/sim.9592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating relationships between multiple incomplete patient measurements requires methods to cope with missing values. Multiple imputation is one approach to address missing data by filling in plausible values for those that are missing. Multiple imputation procedures can be classified into two broad types: joint modeling (JM) and fully conditional specification (FCS). JM fits a multivariate distribution for the entire set of variables, but it may be complex to define and implement. FCS imputes missing data variable-by-variable from a set of conditional distributions. In many studies, FCS is easier to define and implement than JM, but it may be based on incompatible conditional models. Imputation methods based on multilevel modeling show improved operating characteristics when imputing longitudinal data, but they can be computationally intensive, especially when imputing multiple variables simultaneously. We review current MI methods for incomplete longitudinal data and their implementation on widely accessible software. Using simulated data from the National Health and Aging Trends Study, we compare their performance for monotone and intermittent missing data patterns. Our simulations demonstrate that in a longitudinal study with a limited number of repeated observations and time-varying variables, FCS-Standard is a computationally efficient imputation method that is accurate and precise for univariate single-level and multilevel regression models. When the analyses comprise multivariate multilevel models, FCS-LMM-latent is a statistically valid procedure with overall more accurate estimates, but it requires more intensive computations. Imputation methods based on generalized linear multilevel models can lead to biased subject-level variance estimates when the statistical analyses involve hierarchical models.},
  archive      = {J_SIM},
  author       = {Yi Cao and Heather Allore and Brent Vander Wyk and Roee Gutman},
  doi          = {10.1002/sim.9592},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5844-5876},
  shortjournal = {Stat. Med.},
  title        = {Review and evaluation of imputation methods for multivariate longitudinal data with mixed-type incomplete variables},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Overall assessment for selected markers from high-throughput
data. <em>SIM</em>, <em>41</em>(30), 5830–5843. (<a
href="https://doi.org/10.1002/sim.9596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reproducibility, a hallmark of science, is typically assessed in validation studies. We focus on high-throughput studies where a large number of biomarkers is measured in a training study, but only a subset of the most significant findings is selected and re-tested in a validation study. Our aim is to get the statistical measures of overall assessment for the selected markers, by integrating the information in both the training and validation studies. Naive statistical measures, such as the combined P $$ P $$ -value by conventional meta-analysis, that ignore the non-random selection are clearly biased, producing over-optimistic significance. We use the false-discovery rate (FDR) concept to develop a selection-adjusted FDR (sFDR) as an overall assessment measure. We describe the link between the overall assessment and other concepts such as replicability and meta-analysis. Some simulation studies and two real metabolomic datasets are considered to illustrate the application of sFDR in high-throughput data analyses.},
  archive      = {J_SIM},
  author       = {Woojoo Lee and Donghwan Lee and Yudi Pawitan},
  doi          = {10.1002/sim.9596},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5830-5843},
  shortjournal = {Stat. Med.},
  title        = {Overall assessment for selected markers from high-throughput data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sample size determination for the association between
longitudinal and time-to-event outcomes using the joint modeling
time-dependent slopes parameterization. <em>SIM</em>, <em>41</em>(30),
5810–5829. (<a href="https://doi.org/10.1002/sim.9595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given their improvements in bias reduction and efficiency, joint models (JMs) for longitudinal and time-to-event data offer great potential to clinical trials. However, for JM to become more widely used, there is a need for additional development of design considerations. To this end, Chen et al previously developed two closed-form sample size formulas in the JM setting. In this current work, we expand upon this framework by utilizing the time-dependent slopes parameterization, where the change in the longitudinal outcome influences the hazard, in addition to the current value of the longitudinal process. Our extended formula for the required number of events can be used when testing significance of the association between the longitudinal and time-to-event outcomes. We find that if the data indeed are generated such that not only the current value, but also the slope of the longitudinal outcome influence the hazard of the time-to-event process, it is advisable to use the current formula developed utilizing the time-dependent slopes parameterization. In this setting, our proposed formula will provide a more accurate estimate of power compared to the method by Chen et al. To illustrate our proposed method, we present power calculations of a biomarker qualification study for Hutchinson-Gilford progeria syndrome, an ultra-rare premature aging disease.},
  archive      = {J_SIM},
  author       = {Jessica LeClair and Joseph Massaro and Oleksandr Sverdlov and Leslie Gordon and Yorghos Tripodis},
  doi          = {10.1002/sim.9595},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5810-5829},
  shortjournal = {Stat. Med.},
  title        = {Sample size determination for the association between longitudinal and time-to-event outcomes using the joint modeling time-dependent slopes parameterization},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Practical implementation of the partial ordering continual
reassessment method in a phase i combination-schedule dose-finding
trial. <em>SIM</em>, <em>41</em>(30), 5789–5809. (<a
href="https://doi.org/10.1002/sim.9594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing medical interest in combining several agents and optimizing their dosing schedules in a single trial in order to optimize the treatment for patients. Evaluating at doses of several drugs and their scheduling in a single Phase I trial simultaneously possess a number of statistical challenges, and specialized methods to tackle these have been proposed in the literature. However, the uptake of these methods is slow and implementation examples of such advanced methods are still sparse to date. In this work, we share our experience of proposing a model-based partial ordering continual reassessment method (POCRM) design for three-dimensional dose-finding in an oncology trial. In the trial, doses of two agents and the dosing schedule of one of them can be escalated/de-escalated. We provide a step-by-step summary on how the POCRM design was implemented and communicated to the trial team. We proposed an approach to specify toxicity orderings and their a-priori probabilities, and developed a number of visualization tools to communicate the statistical properties of the design. The design evaluation included both a comprehensive simulation study and considerations of the individual trial behavior. The study is now enrolling patients. We hope that sharing our experience of the successful implementation of an advanced design in practice that went through evaluations of several health authorities will facilitate a better uptake of more efficient methods in practice.},
  archive      = {J_SIM},
  author       = {Pavel Mozgunov and Thomas Jaki and Ioannis Gounaris and Thomas Goddemeier and Anja Victor and Marianna Grinberg},
  doi          = {10.1002/sim.9594},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5789-5809},
  shortjournal = {Stat. Med.},
  title        = {Practical implementation of the partial ordering continual reassessment method in a phase i combination-schedule dose-finding trial},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dose finding studies for therapies with late-onset
toxicities: A comparison study of designs. <em>SIM</em>,
<em>41</em>(30), 5767–5788. (<a
href="https://doi.org/10.1002/sim.9593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An objective of phase I dose-finding trials is to find the maximum tolerated dose; the dose with a particular risk of toxicity. Frequently, this risk is assessed across the first cycle of therapy. However, in oncology, a course of treatment frequently consists of multiple cycles of therapy. In many cases, the overall risk of toxicity for a given treatment is not fully encapsulated by observations from the first cycle, and hence it is advantageous to include toxicity outcomes from later cycles in phase I trials. Extending the follow up period in a trial naturally extends the total length of the trial which is undesirable. We present a comparison of eight methods that incorporate late onset toxicities while not extensively extending the trial length. We conduct simulation studies over a number of scenarios and in two settings; the first setting with minimal stopping rules and the second setting with a full set of standard stopping rules expected in such a dose finding study. We find that the model-based approaches in general outperform the model-assisted approaches, with an interval censored approach and a modified version of the time-to-event continual reassessment method giving the most promising overall performance in terms of correct selections and trial length. Further recommendations are made for the implementation of such methods.},
  archive      = {J_SIM},
  author       = {Helen Barnett and Oliver Boix and Dimitris Kontos and Thomas Jaki},
  doi          = {10.1002/sim.9593},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5767-5788},
  shortjournal = {Stat. Med.},
  title        = {Dose finding studies for therapies with late-onset toxicities: A comparison study of designs},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Surv-CRM-12: A bayesian phase i/II survival CRM for
right-censored toxicity endpoints with competing disease progression.
<em>SIM</em>, <em>41</em>(29), 5753–5766. (<a
href="https://doi.org/10.1002/sim.9591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing interest in new classes of anti-cancer agents, such as molecularly-targeted therapies and immunotherapies with modes of action different from those of cytotoxic chemotherapies, has changed the dose-finding paradigm. In this setting, the observation of late-onset toxicity endpoints may be precluded by treatment and trial discontinuation due to disease progression, defining a competing event to toxicity. Trial designs where dose-finding is modeled in the framework of a survival competing risks model appear particularly well-suited. We aim to provide a phase I/II dose-finding design that allows dose-limiting toxicity (DLT) outcomes to be delayed or unobserved due to competing progression within the possibly long observation window. The proposed design named the Survival-continual reassessment method-12, uses survival models for right-censored DLT and progression endpoints. In this competing risks framework, cause-specific hazards for DLT and progression-free of DLT were considered, with model parameters estimated using Bayesian inference. It aims to identify the optimal dose (OD), by minimizing the cumulative incidence of disease progression, given an acceptable toxicity threshold. In a simulation study, design operating characteristics were evaluated and compared to the TITE-BOIN-ET design and a nonparametric benchmark approach. The performance of the proposed method was consistent with the complexity of scenarios as assessed by the nonparametric benchmark. We found that the proposed design presents satisfying operating characteristics in selecting the OD and safety.},
  archive      = {J_SIM},
  author       = {Anaïs Andrillon and Sylvie Chevret and Shing M. Lee and Lucie Biard},
  doi          = {10.1002/sim.9591},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5753-5766},
  shortjournal = {Stat. Med.},
  title        = {Surv-CRM-12: A bayesian phase I/II survival CRM for right-censored toxicity endpoints with competing disease progression},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A proper statistical inference framework to compare clinical
trial and real-world progression-free survival data. <em>SIM</em>,
<em>41</em>(29), 5738–5752. (<a
href="https://doi.org/10.1002/sim.9590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past decade has witnessed an increasing trend in utilizing external control data in clinical trials, especially in the form of synthetic control arms (SCA) derived from real-world or historical trial data. Including such data in clinical trial analysis can improve trial feasibility and efficiency, provided the issues caused by non-randomization and systematic differences are appropriately addressed. Current methodology development in this area focuses on establishing the comparability of patient baseline characteristics between arms, and more research is needed to ensure comparability of other elements such as endpoints. Motivated by the comparative analysis of SCA progression-free survival (PFS) and trial arm PFS, we aim to address another important but little discussed issue for external time-to-event (TTE) data that depend on disease assessment schedules (DAS). Since DAS are generally inconsistent across different data sources, we propose a proper statistical inference framework that harmonizes the DAS through data augmentation by multiple imputation. We demonstrate through extensive simulations that the proposed framework is unbiased in estimating median TTE and hazard ratio, well controls the type I error and achieves desirable power for log-rank test, while the unadjusted analysis can be biased and suffer from severe type I error inflation or power loss depending on the direction of the bias. Given the desirable performance, we recommend the proposed framework for comparative analysis using external DAS-based TTE data in clinical trials.},
  archive      = {J_SIM},
  author       = {Jian Zhu and Rui (Sammi) Tang},
  doi          = {10.1002/sim.9590},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5738-5752},
  shortjournal = {Stat. Med.},
  title        = {A proper statistical inference framework to compare clinical trial and real-world progression-free survival data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-threshold proportional hazards model and subgroup
identification. <em>SIM</em>, <em>41</em>(29), 5715–5737. (<a
href="https://doi.org/10.1002/sim.9589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel two-stage procedure for change point detection and parameter estimation in a multi-threshold proportional hazards model. In the first stage, we estimate the number of thresholds by formulating the threshold detection problem as a variable selection problem and applying the penalized partial likelihood approach. In the second stage, the change point locations are refined by a grid search and the standard inference for segment regression can then follow. The proposed model and estimation procedure could lend support to subgroup identification and personalized treatment recommendation in medical research. We establish the consistency of the threshold estimators and regression coefficient estimators under technical conditions. The finite sample performance of the method is demonstrated via simulation studies and two cancer data examples.},
  archive      = {J_SIM},
  author       = {Bing Wang and Jialiang Li and Xiaoguang Wang},
  doi          = {10.1002/sim.9589},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5715-5737},
  shortjournal = {Stat. Med.},
  title        = {Multi-threshold proportional hazards model and subgroup identification},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A flexible-hazards cure model with application to patients
with soft tissue sarcoma. <em>SIM</em>, <em>41</em>(29), 5698–5714. (<a
href="https://doi.org/10.1002/sim.9588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical research, it is often of great interest to have an accurate estimation of cure rates by different treatment options and for different patient groups. If the follow-up time is sufficiently long and the sample size is large, the proportion of cured patients will make the Kaplan-Meier estimator of survival function have a flat plateau at its tail, whose value indicates the overall cure rate. However, it may be difficult to estimate and compare the cure rates for all the subsets of interest in this way, due to the limit of sample sizes and curse of dimensionality. In the current literature, most regression models for estimating cure rates assume proportional hazards (PH) between different subgroups. It turns out that the estimation of cure rates for subgroups is highly sensitive to this assumption, so more flexible models are needed, especially when this PH assumption is clearly violated. We propose a new cure model to simultaneously incorporate both PH and non-PH scenarios for different covariates. We develop a stable and easily implementable iterative procedure for parameter estimation through maximization of the nonparametric likelihood function. The covariance matrix is estimated by adding perturbation weights to the estimation procedure. In simulation studies, the proposed method provides unbiased estimation for the regression coefficients, survival curves, and cure rates given covariates, while existing models are biased. Our model is applied to a study of stage III soft tissue sarcoma and provides trustworthy estimation of cure rates for different treatment and demographic groups.},
  archive      = {J_SIM},
  author       = {Can Xie and Xuelin Huang and Ruosha Li and Peter W. T. Pisters},
  doi          = {10.1002/sim.9588},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5698-5714},
  shortjournal = {Stat. Med.},
  title        = {A flexible-hazards cure model with application to patients with soft tissue sarcoma},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating mixture effects and cumulative spatial risk over
time simultaneously using a bayesian index low-rank kriging multiple
membership model. <em>SIM</em>, <em>41</em>(29), 5679–5697. (<a
href="https://doi.org/10.1002/sim.9587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exposome is an ideal in public health research that posits that individuals experience risk for adverse health outcomes from a wide variety of sources over their lifecourse. There have been increases in data collection in the various components of the exposome, but novel statistical methods are needed that capture multiple dimensions of risk at once. We introduce a Bayesian index low-rank kriging (LRK) multiple membership model (MMM) to simultaneously estimate the health effects of one or more groups of exposures, the relative importance of exposure components, and cumulative spatial risk over time using residential histories. The model employs an MMM to consider all residential locations for subjects weighted by duration and LRK to increase computational efficiency. We demonstrate the performance of the Bayesian index LRK-MMM through a simulation study, showing that the model accurately and consistently estimates the health effects of one or several group indices and has high power to identify a region of elevated spatial risk due to unmeasured environmental exposures. Finally, we apply our model to data from a multicenter case-control study of non-Hodgkin lymphoma (NHL), finding a significant positive association between one index of pesticides and risk for NHL in Iowa. Additionally, we find an area of significantly elevated spatial risk for NHL in Los Angeles. In conclusion, our Bayesian index LRK-MMM represents a step forward toward bringing the ideals of the exposome into practice for environmental risk analyzes.},
  archive      = {J_SIM},
  author       = {Joseph Boyle and Mary H. Ward and James R. Cerhan and Nat Rothman and David C. Wheeler},
  doi          = {10.1002/sim.9587},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5679-5697},
  shortjournal = {Stat. Med.},
  title        = {Estimating mixture effects and cumulative spatial risk over time simultaneously using a bayesian index low-rank kriging multiple membership model},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional probability and ratio-based approaches for
mapping the coverage of multi-dose vaccines. <em>SIM</em>,
<em>41</em>(29), 5662–5678. (<a
href="https://doi.org/10.1002/sim.9586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many vaccines are often administered in multiple doses to boost their effectiveness. In the case of childhood vaccines, the coverage maps of the doses and the differences between these often constitute an evidence base to guide investments in improving access to vaccination services and health system performance in low and middle-income countries. A major problem often encountered when mapping the coverage of multi-dose vaccines is the need to ensure that the coverage maps decrease monotonically with successive doses. That is, for doses i $$ i $$ and j $$ j $$ , i &lt; j ⇒ p i ⁡ ( s ) ≥ p j ⁡ ( s ) $$ i&lt;j\Rightarrow {p}_i\left(\boldsymbol{s}\right)\ge {p}_j\left(\boldsymbol{s}\right) $$ , where p i ⁡ ( s ) $$ {p}_i\left(\boldsymbol{s}\right) $$ is the coverage of dose i $$ i $$ at spatial location s $$ \boldsymbol{s} $$ . Here, we explore conditional probability (CP) and ratio-based (RB) approaches for mapping , embedded within a binomial geostatistical modeling framework, to address this problem. The fully Bayesian model is implemented using the INLA and SPDE approaches. Using a simulation study, we find that both approaches perform comparably for out-of-sample estimation under varying point-level sample size distributions. We apply the methodology to map the coverage of the three doses of diphtheria-tetanus-pertussis vaccine using data from the 2018 Nigeria Demographic and Health Survey. The coverage maps produced using both approaches are almost indistinguishable, although the CP approach yielded more precise estimates on average in this application. We also provide estimates of zero-dose children and the dropout rates between the doses. The methodology is straightforward to implement and can be applied to other vaccines and geographical contexts.},
  archive      = {J_SIM},
  author       = {Chigozie Edson Utazi and Justice Moses K. Aheto and Ho Man Theophilus Chan and Andrew J. Tatem and Sujit K. Sahu},
  doi          = {10.1002/sim.9586},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5662-5678},
  shortjournal = {Stat. Med.},
  title        = {Conditional probability and ratio-based approaches for mapping the coverage of multi-dose vaccines},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regression analysis for covariate-adaptive randomization: A
robust and efficient inference perspective. <em>SIM</em>,
<em>41</em>(29), 5645–5661. (<a
href="https://doi.org/10.1002/sim.9585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear regression is arguably the most fundamental statistical model; however, the validity of its use in randomized clinical trials, despite being common practice, has never been crystal clear, particularly when stratified or covariate-adaptive randomization is used. In this article, we investigate several of the most intuitive and commonly used regression models for estimating and inferring the treatment effect in randomized clinical trials. By allowing the regression model to be arbitrarily misspecified, we demonstrate that all these regression-based estimators robustly estimate the treatment effect, albeit with possibly different efficiency. We also propose consistent non-parametric variance estimators and compare their performances to those of the model-based variance estimators that are readily available in standard statistical software. Based on the results and taking into account both theoretical efficiency and practical feasibility, we make recommendations for the effective use of regression under various scenarios. For equal allocation, it suffices to use the regression adjustment for the stratum covariates and additional baseline covariates, if available, with the usual ordinary-least-squares variance estimator. For unequal allocation, regression with treatment-by-covariate interactions should be used, together with our proposed variance estimators. These recommendations apply to simple and stratified randomization, and minimization, among others. We hope this work helps to clarify and promote the usage of regression in randomized clinical trials.},
  archive      = {J_SIM},
  author       = {Wei Ma and Fuyi Tu and Hanzhong Liu},
  doi          = {10.1002/sim.9585},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5645-5661},
  shortjournal = {Stat. Med.},
  title        = {Regression analysis for covariate-adaptive randomization: A robust and efficient inference perspective},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blinded sample size re-estimation for comparing
over-dispersed count data incorporating follow-up lengths. <em>SIM</em>,
<em>41</em>(29), 5622–5644. (<a
href="https://doi.org/10.1002/sim.9584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blinded sample size re-estimation (BSSR) is an adaptive design to prevent the power reduction caused by misspecifications of the nuisance parameters in the sample size calculation of comparative clinical trials. However, conventional BSSR methods used for overdispersed count data may not recover the power as expected under the misspecification of the working variance function introduced by the specified analysis model. In this article, we propose a BSSR method that is robust to the misspecification of the working variance function. A weighted estimator of the dispersion parameter for the BSSR is derived, where the weights are introduced to incorporate the difference in the distribution of follow-up length between the interim analysis with BSSR and the final analysis. Simulation studies demonstrated the power of the proposed BSSR method was relatively stable under misspecifications of the working variance function. An application to a hypothetical randomized clinical trial of a treatment to reduce exacerbation rate in patients with chronic obstructive pulmonary disease is provided.},
  archive      = {J_SIM},
  author       = {Masataka Igeta and Shigeyuki Matsui},
  doi          = {10.1002/sim.9584},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5622-5644},
  shortjournal = {Stat. Med.},
  title        = {Blinded sample size re-estimation for comparing over-dispersed count data incorporating follow-up lengths},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A simulated maximum likelihood procedure for analyzing
imprecise trade-off thresholds between the benefits and harms of
medicines. <em>SIM</em>, <em>41</em>(29), 5612–5621. (<a
href="https://doi.org/10.1002/sim.9583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stated preference studies in which information on the willingness to trade-off between the benefits and harms of medicines is elicited from patients or other stakeholders are becoming increasingly mainstream. Such trade-offs can mathematically be represented by a weighted additive function, with the weights, whose ratios determine how much an individual is willing to trade-off between the treatment attributes, being the response vector for the statistical analysis. One way of eliciting trade-off information is through multi-dimensional thresholding (MDT), which is a bisection-based approach that results in increasingly tight bounds on the values of the weights ratios. While MDT is cognitively less demanding than other, more direct elicitation methods, its use complicates the statistical analysis as it results in weights data that are region censored. In this article, we present a simulated maximum likelihood (SML) procedure for fitting a Dirichlet population model directly to the region-censored weights data and perform a series of computational experiments to compare the proposed SML procedure to a naive approach in which a Dirichlet distribution is fitted to the centroids of the weights boundaries obtained with MDT. The results indicate that the SML procedure consistently outperformed the centroid-based approach, with the centroid-based approach requiring three bisection steps per trade-off to achieve a similar precision as the SML procedure with one bisection step per trade-off. Using the newly proposed SML procedure, MDT can be applied with smaller sample sizes or with fewer questions compared to the more naïve centroid-based approach that was applied in previous applications of MDT.},
  archive      = {J_SIM},
  author       = {Douwe Postmus and Francesco Pignatti and Hans L. Hillege and Tommi Tervonen},
  doi          = {10.1002/sim.9583},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5612-5621},
  shortjournal = {Stat. Med.},
  title        = {A simulated maximum likelihood procedure for analyzing imprecise trade-off thresholds between the benefits and harms of medicines},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bayesian multilevel time-varying framework for joint
modeling of hospitalization and survival in patients on dialysis.
<em>SIM</em>, <em>41</em>(29), 5597–5611. (<a
href="https://doi.org/10.1002/sim.9582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over 782 000 individuals in the United States have end-stage kidney disease with about 72% of patients on dialysis, a life-sustaining treatment. Dialysis patients experience high mortality and frequent hospitalizations, at about twice per year. These poor outcomes are exacerbated at key time periods, such as the fragile period after transition to dialysis. In order to study the time-varying effects of modifiable patient and dialysis facility risk factors on hospitalization and mortality, we propose a novel Bayesian multilevel time-varying joint model. Efficient estimation and inference is achieved within the Bayesian framework using Markov chain Monte Carlo, where multilevel (patient- and dialysis facility-level) varying coefficient functions are targeted via Bayesian P-splines. Applications to the United States Renal Data System, a national database which contains data on nearly all patients on dialysis in the United States, highlight significant time-varying effects of patient- and facility-level risk factors on hospitalization risk and mortality. Finite sample performance of the proposed methodology is studied through simulations.},
  archive      = {J_SIM},
  author       = {Esra Kürüm and Danh V. Nguyen and Sudipto Banerjee and Yihao Li and Connie M. Rhee and Damla Şentürk},
  doi          = {10.1002/sim.9582},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5597-5611},
  shortjournal = {Stat. Med.},
  title        = {A bayesian multilevel time-varying framework for joint modeling of hospitalization and survival in patients on dialysis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Some considerations on target estimands for health
technology assessment. <em>SIM</em>, <em>41</em>(28), 5592–5596. (<a
href="https://doi.org/10.1002/sim.9566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Antonio Remiro-Azócar},
  doi          = {10.1002/sim.9566},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5592-5596},
  shortjournal = {Stat. Med.},
  title        = {Some considerations on target estimands for health technology assessment},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comments on the debate between marginal and conditional
estimands. <em>SIM</em>, <em>41</em>(28), 5589–5591. (<a
href="https://doi.org/10.1002/sim.9558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Andrew J. Spieker},
  doi          = {10.1002/sim.9558},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5589-5591},
  shortjournal = {Stat. Med.},
  title        = {Comments on the debate between marginal and conditional estimands},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditions for success and margins of error: Estimation in
clinical trials. <em>SIM</em>, <em>41</em>(28), 5586–5588. (<a
href="https://doi.org/10.1002/sim.9497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Stephen Senn},
  doi          = {10.1002/sim.9497},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5586-5588},
  shortjournal = {Stat. Med.},
  title        = {Conditions for success and margins of error: Estimation in clinical trials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimands in heath technology assessment: A causal inference
perspective. <em>SIM</em>, <em>41</em>(28), 5577–5585. (<a
href="https://doi.org/10.1002/sim.9539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Kelly Van Lancker and Tat-Thang Vo and Mouna Akacha},
  doi          = {10.1002/sim.9539},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5577-5585},
  shortjournal = {Stat. Med.},
  title        = {Estimands in heath technology assessment: A causal inference perspective},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discussion of “target estimands for population-adjusted
indirect comparisons” by antonio remiro-azocar. <em>SIM</em>,
<em>41</em>(28), 5573–5576. (<a
href="https://doi.org/10.1002/sim.9533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Estelle Russek-Cohen},
  doi          = {10.1002/sim.9533},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5573-5576},
  shortjournal = {Stat. Med.},
  title        = {Discussion of “target estimands for population-adjusted indirect comparisons” by antonio remiro-azocar},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Commentary on “target estimands for population-adjusted
indirect comparisons.” <em>SIM</em>, <em>41</em>(28), 5570–5572. (<a
href="https://doi.org/10.1002/sim.9517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Anja Schiel},
  doi          = {10.1002/sim.9517},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5570-5572},
  shortjournal = {Stat. Med.},
  title        = {Commentary on “Target estimands for population-adjusted indirect comparisons”},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Target estimands for population-adjusted indirect
comparisons. <em>SIM</em>, <em>41</em>(28), 5558–5569. (<a
href="https://doi.org/10.1002/sim.9413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Antonio Remiro-Azócar},
  doi          = {10.1002/sim.9413},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5558-5569},
  shortjournal = {Stat. Med.},
  title        = {Target estimands for population-adjusted indirect comparisons},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Progression models for repeated measures: Estimating novel
treatment effects in progressive diseases. <em>SIM</em>,
<em>41</em>(28), 5537–5557. (<a
href="https://doi.org/10.1002/sim.9581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed models for repeated measures (MMRMs) are ubiquitous when analyzing outcomes of clinical trials. However, the linearity of the fixed-effect structure in these models largely restrict their use to estimating treatment effects that are defined as linear combinations of effects on the outcome scale. In some situations, alternative quantifications of treatment effects may be more appropriate. In progressive diseases, for example, one may want to estimate if a drug has cumulative effects resulting in increasing efficacy over time or whether it slows the time progression of disease. This article introduces a class of nonlinear mixed-effects models called progression models for repeated measures (PMRMs) that, based on a continuous-time extension of the categorical-time parametrization of MMRMs, enables estimation of novel types of treatment effects, including measures of slowing or delay of the time progression of disease. Compared to conventional estimates of treatment effects where the unit matches that of the outcome scale (eg, 2 points benefit on a cognitive scale), the time-based treatment effects can offer better interpretability and clinical meaningfulness (eg, 6 months delay in progression of cognitive decline). The PMRM class includes conventionally used MMRMs and related models for longitudinal data analysis, as well as variants of previously proposed disease progression models as special cases. The potential of the PMRM framework is illustrated using both simulated and historical data from clinical trials in Alzheimer&#39;s disease with different types of artificially simulated treatment effects. Compared to conventional models it is shown that PMRMs can offer substantially increased power to detect disease-modifying treatment effects where the benefit is increasing with treatment duration.},
  archive      = {J_SIM},
  author       = {Lars Lau Raket},
  doi          = {10.1002/sim.9581},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5537-5557},
  shortjournal = {Stat. Med.},
  title        = {Progression models for repeated measures: Estimating novel treatment effects in progressive diseases},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Group sequential methods for interim monitoring of
randomized clinical trials with time-lagged outcome. <em>SIM</em>,
<em>41</em>(28), 5517–5536. (<a
href="https://doi.org/10.1002/sim.9580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary analysis in two-arm clinical trials usually involves inference on a scalar treatment effect parameter; for example, depending on the outcome, the difference of treatment-specific means, risk difference, risk ratio, or odds ratio. Most clinical trials are monitored for the possibility of early stopping. Because ordinarily the outcome on any given subject can be ascertained only after some time lag, at the time of an interim analysis, among the subjects already enrolled, the outcome is known for only a subset and is effectively censored for those who have not been enrolled sufficiently long for it to be observed. Typically, the interim analysis is based only on the data from subjects for whom the outcome has been ascertained. A goal of an interim analysis is to stop the trial as soon as the evidence is strong enough to do so, suggesting that the analysis ideally should make the most efficient use of all available data, thus including information on censoring as well as other baseline and time-dependent covariates in a principled way. A general group sequential framework is proposed for clinical trials with a time-lagged outcome. Treatment effect estimators that take account of censoring and incorporate covariate information at an interim analysis are derived using semiparametric theory and are demonstrated to lead to stronger evidence for early stopping than standard approaches. The associated test statistics are shown to have the independent increments structure, so that standard software can be used to obtain stopping boundaries.},
  archive      = {J_SIM},
  author       = {Anastasios A. Tsiatis and Marie Davidian},
  doi          = {10.1002/sim.9580},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5517-5536},
  shortjournal = {Stat. Med.},
  title        = {Group sequential methods for interim monitoring of randomized clinical trials with time-lagged outcome},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Case studies in bias reduction and inference for electronic
health record data with selection bias and phenotype misclassification.
<em>SIM</em>, <em>41</em>(28), 5501–5516. (<a
href="https://doi.org/10.1002/sim.9579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records (EHR) are not designed for population-based research, but they provide easy and quick access to longitudinal health information for a large number of individuals. Many statistical methods have been proposed to account for selection bias, missing data, phenotyping errors, or other problems that arise in EHR data analysis. However, addressing multiple sources of bias simultaneously is challenging. We developed a methodological framework (R package, SAMBA ) for jointly handling both selection bias and phenotype misclassification in the EHR setting that leverages external data sources. These methods assume factors related to selection and misclassification are fully observed, but these factors may be poorly understood and partially observed in practice. As a follow-up to the methodological work, we demonstrate how to apply these methods for two real-world case studies, and we evaluate their performance. In both examples, we use individual patient-level data collected through the University of Michigan Health System and various external population-based data sources. In case study (a), we explore the impact of these methods on estimated associations between gender and cancer diagnosis. In case study (b), we compare corrected associations between previously identified genetic loci and age-related macular degeneration with gold standard external summary estimates. These case studies illustrate how to utilize diverse auxiliary information to achieve less biased inference in EHR-based research.},
  archive      = {J_SIM},
  author       = {Lauren J. Beesley and Bhramar Mukherjee},
  doi          = {10.1002/sim.9579},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5501-5516},
  shortjournal = {Stat. Med.},
  title        = {Case studies in bias reduction and inference for electronic health record data with selection bias and phenotype misclassification},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Rejoinder: Improving the performance of bayesian logistic
regression model with overdose control in oncology dose-finding studies.
<em>SIM</em>, <em>41</em>(27), 5497–5500. (<a
href="https://doi.org/10.1002/sim.9561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Hongtao Zhang and Alan Y. Chiang and Jixian Wang},
  doi          = {10.1002/sim.9561},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5497-5500},
  shortjournal = {Stat. Med.},
  title        = {Rejoinder: Improving the performance of bayesian logistic regression model with overdose control in oncology dose-finding studies},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). “Improving the performance of bayesian logistic regression
model with overdose control in oncology dose-finding studies” by hongtao
zhang, alan chiang, and jixian wang. <em>SIM</em>, <em>41</em>(27),
5494–5496. (<a href="https://doi.org/10.1002/sim.9494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Alexia Iasonos and Graham M. Wheeler},
  doi          = {10.1002/sim.9494},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5494-5496},
  shortjournal = {Stat. Med.},
  title        = {“Improving the performance of bayesian logistic regression model with overdose control in oncology dose-finding studies” by hongtao zhang, alan chiang, and jixian wang},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Explicit underdose control based on toxicity: Four points to
consider. <em>SIM</em>, <em>41</em>(27), 5491–5493. (<a
href="https://doi.org/10.1002/sim.9492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Paul H. Frankel and Elizabeth Garrett-Mayer and Mark D. Krailo},
  doi          = {10.1002/sim.9492},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5491-5493},
  shortjournal = {Stat. Med.},
  title        = {Explicit underdose control based on toxicity: Four points to consider},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Commentary on “improving the performance of bayesian
logistic regression model with overdose control in oncology dose-finding
studies.” <em>SIM</em>, <em>41</em>(27), 5484–5490. (<a
href="https://doi.org/10.1002/sim.9496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Ying Yuan and Yixuan Zhao},
  doi          = {10.1002/sim.9496},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5484-5490},
  shortjournal = {Stat. Med.},
  title        = {Commentary on “Improving the performance of bayesian logistic regression model with overdose control in oncology dose-finding studies”},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Improving the performance of bayesian logistic regression
model with overdose control in oncology dose-finding studies.
<em>SIM</em>, <em>41</em>(27), 5463–5483. (<a
href="https://doi.org/10.1002/sim.9402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An accurately identified maximum tolerated dose (MTD) serves as the cornerstone of successful subsequent phases in oncology drug development. Bayesian logistic regression model (BLRM) is a popular and versatile model-based dose-finding design. However, BLRM with original overdose control strategy has been reported to be safe but “excessively conservative.” In this article, we investigate the reason for conservativeness and point out that a major reason could be the lack of appropriate underdose control. We propose designs that balance overdose and underdose control to improve the performance over the original BLRM. Simulation results reveal that the new designs have better accuracy and treat more patients at MTD.},
  archive      = {J_SIM},
  author       = {Hongtao Zhang and Alan Y. Chiang and Jixian Wang},
  doi          = {10.1002/sim.9402},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5463-5483},
  shortjournal = {Stat. Med.},
  title        = {Improving the performance of bayesian logistic regression model with overdose control in oncology dose-finding studies},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust analysis of cancer heterogeneity for high-dimensional
data. <em>SIM</em>, <em>41</em>(27), 5448–5462. (<a
href="https://doi.org/10.1002/sim.9578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer heterogeneity plays an important role in the understanding of tumor etiology, progression, and response to treatment. To accommodate heterogeneity, cancer subgroup analysis has been extensively conducted. However, most of the existing studies share the limitation that they cannot accommodate heavy-tailed or contaminated outcomes and also high dimensional covariates, both of which are not uncommon in biomedical research. In this study, we propose a robust subgroup identification approach based on M-estimators together with concave and pairwise fusion penalties, which advances from existing studies by effectively accommodating high-dimensional data containing some outliers. The penalties are applied on both latent heterogeneity factors and covariates, where the estimation is expected to achieve subgroup identification and variable selection simultaneously, with the number of subgroups being apriori unknown. We innovatively develop an algorithm based on parallel computing strategy, with a significant advantage of capable of processing large-scale data. The convergence property of the proposed algorithm, oracle property of the penalized M-estimators, and selection consistency of the proposed BIC criterion are carefully established. Simulation and analysis of TCGA breast cancer data demonstrate that the proposed approach is promising to efficiently identify underlying subgroups in high-dimensional data.},
  archive      = {J_SIM},
  author       = {Chao Cheng and Xingdong Feng and Xiaoguang Li and Mengyun Wu},
  doi          = {10.1002/sim.9578},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5448-5462},
  shortjournal = {Stat. Med.},
  title        = {Robust analysis of cancer heterogeneity for high-dimensional data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic semiparametric transformation models for recurrent
event data with a terminal event. <em>SIM</em>, <em>41</em>(27),
5432–5447. (<a href="https://doi.org/10.1002/sim.9577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent event data with a terminal event commonly arise in many longitudinal follow-up studies. This article proposes a class of dynamic semiparametric transformation models for the marginal mean functions of the recurrent events with a terminal event, where some covariate effects may be time-varying. An estimation procedure is developed for the model parameters, and the asymptotic properties of the resulting estimators are established. In addition, relevant significance tests are suggested for examining whether or not covariate effects vary with time, and a model checking procedure is presented for assessing the adequacy of the proposed models. The finite sample performance of the proposed estimators is examined through simulation studies, and an application to a medical cost study of chronic heart failure patients is provided.},
  archive      = {J_SIM},
  author       = {Jin Jin and Xinyuan Song and Liuquan Sun},
  doi          = {10.1002/sim.9577},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5432-5447},
  shortjournal = {Stat. Med.},
  title        = {Dynamic semiparametric transformation models for recurrent event data with a terminal event},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal subsampling for parametric accelerated failure time
models with massive survival data. <em>SIM</em>, <em>41</em>(27),
5421–5431. (<a href="https://doi.org/10.1002/sim.9576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing availability of massive survival data, researchers need valid statistical inferences for survival modeling whose computation is not limited by computer memories. Existing works focus on relative risk models using the online updating and divide-and-conquer strategies. The subsampling strategy has not been available due to challenges in developing the asymptotic properties of the estimator under semiparametric models with censored data. This article tackles optimal subsampling algorithms to fast approximate the maximum likelihood estimator for parametric accelerate failure time models with massive survival data. We derive the asymptotic distributions of the subsampling estimator and the optimal sampling probabilities that minimize the asymptotic mean squared error of the estimator. A feasible two-step algorithm is proposed where the optimal sampling probabilities in the second step are estimated based on a pilot sample in the first step. The asymptotic properties of the two-step estimator are established. The performance of the estimator is validated in a simulation study. A real data analysis illustrates the usefulness of the methods.},
  archive      = {J_SIM},
  author       = {Zehan Yang and HaiYing Wang and Jun Yan},
  doi          = {10.1002/sim.9576},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5421-5431},
  shortjournal = {Stat. Med.},
  title        = {Optimal subsampling for parametric accelerated failure time models with massive survival data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the use of the likelihood ratio test methodology in
pharmacovigilance. <em>SIM</em>, <em>41</em>(27), 5395–5420. (<a
href="https://doi.org/10.1002/sim.9575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The safety of medical products due to adverse events (AE) from drugs, therapeutic biologics, and medical devices is a major public health concern worldwide. Likelihood ratio test (LRT) approaches to pharmacovigilance constitute a class of rigorous statistical tools that permit objective identification of AEs of a specific drug and/or a class of drugs cataloged in spontaneous reporting system databases. However, the existing LRT approaches encounter certain theoretical and computational challenges when an underlying Poisson model assumption is violated, including in cases of zero-inflated data. We briefly review existing LRT approaches and propose a novel class of (pseudo-) LRT methods to address these challenges. Our approach uses an alternative parametrization to formulate a unified framework with a common test statistic that can handle both Poisson and zero-inflated Poisson (ZIP) models. The proposed framework is computationally efficient, and it reveals deeper insights into the comparative behaviors of the Poisson and the ZIP models for handling AE data. Our extensive simulation studies document notably superior performances of the proposed methods over existing approaches particularly under zero-inflation, both in terms of statistical (eg, much better control of the nominal level and false discovery rate with substantially enhanced power) and computational ( 100-500-fold gains in average running times) performance metrics. An application of our method on the statin drug class from the FDA FAERS database reveals interesting insights on potential AEs. An R package, pvLRT , implementing our methods has been released in the public domain.},
  archive      = {J_SIM},
  author       = {Saptarshi Chakraborty and Anran Liu and Robert Ball and Marianthi Markatou},
  doi          = {10.1002/sim.9575},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5395-5420},
  shortjournal = {Stat. Med.},
  title        = {On the use of the likelihood ratio test methodology in pharmacovigilance},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contrast weighted learning for robust optimal treatment rule
estimation. <em>SIM</em>, <em>41</em>(27), 5379–5394. (<a
href="https://doi.org/10.1002/sim.9574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized medicine aims to tailor medical decisions based on patient-specific characteristics. Advances in data capturing techniques such as electronic health records dramatically increase the availability of comprehensive patient profiles, promoting the rapid development of optimal treatment rule (OTR) estimation methods. An archetypal OTR estimation approach is the outcome weighted learning, where OTR is determined under a weighted classification framework with clinical outcomes as the weights. Although outcome weighted learning has been extensively studied and extended, existing methods are susceptible to irregularities of outcome distributions such as outliers and heavy tails. Methods that involve modeling of the outcome are also sensitive to model misspecification. We propose a contrast weighted learning (CWL) framework that exploits the flexibility and robustness of contrast functions to enable robust OTR estimation for a wide range of clinical outcomes. The novel value function in CWL only depends on the pairwise contrast of clinical outcomes between patients irrespective of their distributional features and supports. The Fisher consistency and convergence rate of the estimated decision rule via CWL are established. We illustrate the superiority of the proposed method under finite samples using comprehensive simulation studies with ill-distributed continuous outcomes and ordinal outcomes. We apply the CWL method to two datasets from clinical trials on idiopathic pulmonary fibrosis and COVID-19 to demonstrate its real-world application.},
  archive      = {J_SIM},
  author       = {Xiaohan Guo and Ai Ni},
  doi          = {10.1002/sim.9574},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5379-5394},
  shortjournal = {Stat. Med.},
  title        = {Contrast weighted learning for robust optimal treatment rule estimation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DL 101: Basic introduction to deep learning with its
application in biomedical related fields. <em>SIM</em>, <em>41</em>(26),
5365–5378. (<a href="https://doi.org/10.1002/sim.9564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is a subfield of machine learning used to learn representations of data by successive layers. Remarkable achievements and breakthroughs have been made in image classification, speech recognition, et cetera, but the full capability of deep learning is still under exploration. As statistical researchers and practitioners, we are especially interested in leveraging and advancing deep learning techniques to address important and impactive problems in biomedical and other related fields. In this article, we provide a basic introduction to Feedforward Neural Networks (FNN) along with some intuitive explanations behind its strong functional representation. Guidance is provided on how to choose quite a few hyperparameters in neural networks for a specific problem. We further discuss several more advanced frameworks in deep learning. Some successful applications of deep learning in biomedical fields are also demonstrated. With this beginner&#39;s guide, we hope that interested readers can include deep learning in their toolbox to tackle future real-world questions and challenges.},
  archive      = {J_SIM},
  author       = {Tianyu Zhan},
  doi          = {10.1002/sim.9564},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5365-5378},
  shortjournal = {Stat. Med.},
  title        = {DL 101: Basic introduction to deep learning with its application in biomedical related fields},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Marginal semiparametric transformation models for clustered
multivariate competing risks data. <em>SIM</em>, <em>41</em>(26),
5349–5364. (<a href="https://doi.org/10.1002/sim.9573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate survival models are often used in studying multiple outcomes for right-censored data. However, the outcomes of interest often have competing risks, where standard multivariate survival models may lead to invalid inferences. For example, patients who had stem cell transplantation may experience multiple types of infections after transplant while reconstituting their immune system, where death without experiencing infections is a competing risk for infections. Such competing risks data often suffer from cluster effects due to a matched pair design or correlation within study centers. The cumulative incidence function (CIF) is widely used to summarize competing risks outcomes. Thus, it is often of interest to study direct covariate effects on the CIF. Most literature on clustered competing risks data analyses is limited to the univariate proportional subdistribution hazards model with inverse probability censoring weighting which requires correctly specifying the censoring distribution. We propose a marginal semiparametric transformation model for multivariate competing risks outcomes. The proposed model does not require modeling the censoring distribution, accommodates nonproportional subdistribution hazards structure, and provides a platform for joint inference of all causes and outcomes.},
  archive      = {J_SIM},
  author       = {Yizeng He and Soyoung Kim and Lu Mao and Kwang Woo Ahn},
  doi          = {10.1002/sim.9573},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5349-5364},
  shortjournal = {Stat. Med.},
  title        = {Marginal semiparametric transformation models for clustered multivariate competing risks data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analytical methods for correlated data arising from
multicenter hearing studies. <em>SIM</em>, <em>41</em>(26), 5335–5348.
(<a href="https://doi.org/10.1002/sim.9572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In epidemiological hearing studies, estimating the association between exposures and hearing loss using audiometrically-assessed hearing measurements is challenging due to the complex correlation structure in the clustered data, with clusters formed by the two ears of the same individual and the testing site and audiologist. We propose a linear mixed-effects model to take into account the multilevel correlation structures of the data. Both theoretically and in simulation studies, we compare single-ear linear regression models commonly used in published hearing loss studies with the proposed both-ears linear mixed models properly accounting for the multi-level correlations. Our findings include (1) when there are only participant-level covariates, the worse-ear linear regression models produce unbiased but typically less efficient estimators than the both-ear and average-ear approaches; (2) when there are ear-level confounders, the worse-ear method may lead to biased estimators and the average-ear method produces unbiased but typically less efficient estimators than the both-ear method; (3) the both-ear method may gain efficiency when additionally adjusting for testing sites and audiologists. As an illustrative example, we applied the single-ear and both-ear methods to assess aspirin-hearing association in the Nurses&#39; Health Study II.},
  archive      = {J_SIM},
  author       = {Yanghui Sheng and Ce Yang and Sharon Curhan and Gary Curhan and Molin Wang},
  doi          = {10.1002/sim.9572},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5335-5348},
  shortjournal = {Stat. Med.},
  title        = {Analytical methods for correlated data arising from multicenter hearing studies},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BOB: Bayesian optimal design for biosimilar trials with
co-primary endpoints. <em>SIM</em>, <em>41</em>(26), 5319–5334. (<a
href="https://doi.org/10.1002/sim.9571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For regulatory approval of a biosimilar product, extensive evaluations should be performed by rigorous clinical trials to establish the similarity between the reference product and the proposed biosimilar in terms of both efficacy and safety. Existing designs for biosimilar trials often use a single primary efficacy endpoint in trial monitoring, and then separately evaluate the safety of the biosimilar product in a secondary analysis at the trial completion. However, ignoring the safety endpoint and the correlation between safety and efficacy in trial monitoring may lead to a high false positive rate, or it may delay the termination of the trial when dissimilarity in safety is early detected. We propose a Bayesian optimal design for biosimilar trials by incorporating both safety and efficacy endpoints in a unified framework. Based on a Bayesian joint safety and efficacy model, we sequentially use a so-called Bayesian biosimilar probability to make go/no-go decisions. We calibrate the Bayesian design to maximize the statistical power while maintaining the frequentist type I error rate at the nominal level. We carry out extensive simulation studies to show that the design has desirable performance in terms of the false positive rate and the average sample size. We also apply the proposed design to a biosimilar trial evaluating a ranibizumab product.},
  archive      = {J_SIM},
  author       = {Xiaohan Chi and Zhangsheng Yu and Ruitao Lin},
  doi          = {10.1002/sim.9571},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5319-5334},
  shortjournal = {Stat. Med.},
  title        = {BOB: Bayesian optimal design for biosimilar trials with co-primary endpoints},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stratified proportional win-fractions regression analysis.
<em>SIM</em>, <em>41</em>(26), 5305–5318. (<a
href="https://doi.org/10.1002/sim.9570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently proposed proportional win-fractions (PW) model extends the two-sample win ratio analysis of prioritized composite endpoints to regression. Its proportionality assumption ensures that the covariate-specific win ratios are invariant to the follow-up time. However, this assumption is strong and may not be satisfied by every covariate in the model. We develop a stratified PW model that adjusts for certain prognostic factors without setting them as covariates, thus bypassing the proportionality requirement. We formulate the stratified model based on pairwise comparisons within each stratum, with a common win ratio across strata modeled as a multiplicative function of the covariates. Correspondingly, we construct an estimating function for the regression coefficients in the form of an incomplete -statistic consisting of within-stratum pairs. Two types of asymptotic variance estimators are developed depending on the number of strata relative to the sample size. This in particular allows valid inference even when the strata are extremely small, such as with matched pairs. Simulation studies in realistic settings show that the stratified model outperforms the unstratified version in robustness and efficiency. Finally, real data from a major cardiovascular trial are analyzed to illustrate the potential benefits of stratification. The proposed methods are implemented in the R package WR , publicly available on the Comprehensive R Archive Network (CRAN).},
  archive      = {J_SIM},
  author       = {Tuo Wang and Lu Mao},
  doi          = {10.1002/sim.9570},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5305-5318},
  shortjournal = {Stat. Med.},
  title        = {Stratified proportional win-fractions regression analysis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identifying surrogate markers in real-world comparative
effectiveness research. <em>SIM</em>, <em>41</em>(26), 5290–5304. (<a
href="https://doi.org/10.1002/sim.9569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In comparative effectiveness research (CER), leveraging short-term surrogates to infer treatment effects on long-term outcomes can guide policymakers evaluating new treatments. Numerous statistical procedures for identifying surrogates have been proposed for randomized clinical trials (RCTs), but no methods currently exist to evaluate the proportion of treatment effect (PTE) explained by surrogates in real-world data (RWD), which have become increasingly common. To address this knowledge gap, we propose inverse probability weighted (IPW) and doubly robust (DR) estimators of an optimal transformation of the surrogate and the corresponding PTE measure. We demonstrate that the proposed estimators are consistent and asymptotically normal, and the DR estimator is consistent when either the propensity score model or outcome regression model is correctly specified. Our proposed estimators are evaluated through extensive simulation studies. In two RWD settings, we show that our method can identify and validate surrogate markers for inflammatory bowel disease (IBD).},
  archive      = {J_SIM},
  author       = {Larry Han and Xuan Wang and Tianxi Cai},
  doi          = {10.1002/sim.9569},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5290-5304},
  shortjournal = {Stat. Med.},
  title        = {Identifying surrogate markers in real-world comparative effectiveness research},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bias correction based on weighted likelihood for conditional
estimation of subgroup effects in randomized clinical trials.
<em>SIM</em>, <em>41</em>(26), 5276–5289. (<a
href="https://doi.org/10.1002/sim.9567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, many confirmatory randomized clinical trials (RCTs) with predictive markers have taken the all-comers approach because of the difficulty in developing predictive markers that are biologically compelling enough to apply the enrichment approach to restrict the patient population to a marker-defined subgroup. However, such a RCT with weak marker credentials can conclude that the new treatment is efficacious only in the subgroup, especially when the primary analysis demonstrates some treatment efficacy in the subgroup, but the overall treatment efficacy is not significant under a control of study-wise alpha rate. In this article, we consider conditional estimation of subgroup treatment effects, given the negative result in testing the overall treatment efficacy in the trial. To address the problem of unstable estimation due to the truncation in the distribution of the test statistic on overall treatment efficacy, we propose a new approach based on a weighted likelihood for the truncated distribution. The weighted likelihood can be derived by invoking a randomized test with a smooth critical function for the overall test. Our approach allows for point and interval estimations of the conditional effects consistently based on the standard maximum likelihood inference. Numerical evaluations, including simulations and application to real clinical trials, and guidelines for implementing our methods with R-codes, are provided.},
  archive      = {J_SIM},
  author       = {Kiichiro Toyoizumi and Shigeyuki Matsui},
  doi          = {10.1002/sim.9567},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5276-5289},
  shortjournal = {Stat. Med.},
  title        = {Bias correction based on weighted likelihood for conditional estimation of subgroup effects in randomized clinical trials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On estimation and cross-validation of dynamic treatment
regimes with competing risks. <em>SIM</em>, <em>41</em>(26), 5258–5275.
(<a href="https://doi.org/10.1002/sim.9568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimal moment to start renal replacement therapy in a patient with acute kidney injury (AKI) remains a challenging problem in intensive care nephrology. Multiple randomized controlled trials have tried to answer this question, but these contrast only a limited number of treatment initiation strategies. In view of this, we use routinely collected observational data from the Ghent University Hospital intensive care units (ICUs) to investigate different prespecified timing strategies for renal replacement therapy initiation based on time-updated levels of serum potassium, pH, and fluid balance in critically ill patients with AKI with the aim to minimize 30-day ICU mortality. For this purpose, we apply statistical techniques for evaluating the impact of specific dynamic treatment regimes in the presence of ICU discharge as a competing event. We discuss two approaches, a nonparametric one — using an inverse probability weighted Aalen-Johansen estimator — and a semiparametric one — using dynamic-regime marginal structural models. Furthermore, we suggest an easy to implement cross-validation technique to assess the out-of-sample performance of the optimal dynamic treatment regime. Our work illustrates the potential of data-driven medical decision support based on routinely collected observational data.},
  archive      = {J_SIM},
  author       = {Paweł Morzywołek and Johan Steen and Wim Van Biesen and Johan Decruyenaere and Stijn Vansteelandt},
  doi          = {10.1002/sim.9568},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5258-5275},
  shortjournal = {Stat. Med.},
  title        = {On estimation and cross-validation of dynamic treatment regimes with competing risks},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predictive signature development based on maximizing the
area between receiver operating characteristic curves. <em>SIM</em>,
<em>41</em>(26), 5242–5257. (<a
href="https://doi.org/10.1002/sim.9565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Development of marker signatures to predict treatment benefits for a new therapeutic is an important scientific component in advancing the drug discovery and is an important first step toward the goal of precision medicine. In this article, we focus on developing an algorithm to search for optimal linear combination of markers that maximizes the area between two receiver operating characteristic curves of the new therapeutic and the control groups without assuming any parametric model. We further generalize the proposed algorithm for predictive signature development to maximize the difference of Harrel&#39;s C-index of the new therapeutic and the control groups when the outcome of interest is time-to-event. The performance of this proposed method is evaluated and compared to existing methods via simulations and real clinical trial data.},
  archive      = {J_SIM},
  author       = {Xin Huang and Lu Tian and Yan Sun and Saptarshi Chatterjee and Viswanath Devanarayan},
  doi          = {10.1002/sim.9565},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5242-5257},
  shortjournal = {Stat. Med.},
  title        = {Predictive signature development based on maximizing the area between receiver operating characteristic curves},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lq-based robust analytics on ultrahigh and high dimensional
data. <em>SIM</em>, <em>41</em>(26), 5220–5241. (<a
href="https://doi.org/10.1002/sim.9563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrahigh and high dimensional data are common in regression analysis for various fields, such as omics data, finance, and biological engineering. In addition to the problem of dimension, the data might also be contaminated. There are two main types of contamination: outliers and model misspecification. We develop an unique method that takes into account the ultrahigh or high dimensional issues and both types of contamination. In this article, we propose a framework for feature screening and selection based on the minimum Lq-likelihood estimation (MLqE), which accounts for the model misspecification contamination issue and has also been shown to be robust to outliers. In numerical analysis, we explore the robustness of this framework under different outliers and model misspecification scenarios. To examine the performance of this framework, we conduct real data analysis using the skin cutaneous melanoma data. When comparing with traditional screening and feature selection methods, the proposed method shows superiority in both variable identification effectiveness and parameter estimation accuracy.},
  archive      = {J_SIM},
  author       = {Jiachen Chen and Ruofan Bie and Yichen Qin and Yang Li and Shuangge Ma},
  doi          = {10.1002/sim.9563},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5220-5241},
  shortjournal = {Stat. Med.},
  title        = {Lq-based robust analytics on ultrahigh and high dimensional data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Network meta-analysis of rare events using penalized
likelihood regression. <em>SIM</em>, <em>41</em>(26), 5203–5219. (<a
href="https://doi.org/10.1002/sim.9562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network meta-analysis (NMA) of rare events has attracted little attention in the literature. Until recently, networks of interventions with rare events were analyzed using the inverse-variance NMA approach. However, when events are rare the normal approximations made by this model can be poor and effect estimates are potentially biased. Other methods for the synthesis of such data are the recent extension of the Mantel-Haenszel approach to NMA or the use of the noncentral hypergeometric distribution. In this article, we suggest a new common-effect NMA approach that can be applied even in networks of interventions with extremely low or even zero number of events without requiring study exclusion or arbitrary imputations. Our method is based on the implementation of the penalized likelihood function proposed by Firth for bias reduction of the maximum likelihood estimate to the logistic expression of the NMA model. A limitation of our method is that heterogeneity cannot be taken into account as an additive parameter as in most meta-analytical models. However, we account for heterogeneity by incorporating a multiplicative overdispersion term using a two-stage approach. We show through simulation that our method performs consistently well across all tested scenarios and most often results in smaller bias than other available methods. We also illustrate the use of our method through two clinical examples. We conclude that our “penalized likelihood NMA” approach is promising for the analysis of binary outcomes with rare events especially for networks with very few studies per comparison and very low control group risks.},
  archive      = {J_SIM},
  author       = {Theodoros Evrenoglou and Ian R. White and Sivem Afach and Dimitris Mavridis and Anna Chaimani},
  doi          = {10.1002/sim.9562},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5203-5219},
  shortjournal = {Stat. Med.},
  title        = {Network meta-analysis of rare events using penalized likelihood regression},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic modeling of the italians’ attitude towards covid-19.
<em>SIM</em>, <em>41</em>(26), 5189–5202. (<a
href="https://doi.org/10.1002/sim.9560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze repeated cross-sectional survey data collected by the Institute of Global Health Innovation, to characterize the perception and behavior of the Italian population during the Covid-19 pandemic, focusing on the period that spans from April 2020 to July 2021. To accomplish this goal, we propose a Bayesian dynamic latent-class regression model, that accounts for the effect of sampling bias including survey weights into the likelihood function. According to the proposed approach, attitudes towards covid-19 are described via ideal behaviors that are fixed over time, corresponding to different degrees of compliance with spread-preventive measures. The overall tendency toward a specific profile dynamically changes across survey waves via a latent Gaussian process regression, that adjusts for subject-specific covariates. We illustrate the evolution of Italians&#39; behaviors during the pandemic, providing insights on how the proportion of ideal behaviors has varied during the phases of the lockdown, while measuring the effect of age, sex, region and employment of the respondents on the attitude toward covid-19 .},
  archive      = {J_SIM},
  author       = {Emanuele Aliverti and Massimiliano Russo},
  doi          = {10.1002/sim.9560},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5189-5202},
  shortjournal = {Stat. Med.},
  title        = {Dynamic modeling of the italians&#39; attitude towards covid-19},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gaussian graphical models with applications to omics
analyses. <em>SIM</em>, <em>41</em>(25), 5150–5187. (<a
href="https://doi.org/10.1002/sim.9546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian graphical models (GGMs) provide a framework for modeling conditional dependencies in multivariate data. In this tutorial, we provide an overview of GGM theory and a demonstration of various GGM tools in R. The mathematical foundations of GGMs are introduced with the goal of enabling the researcher to draw practical conclusions by interpreting model results. Background literature is presented, emphasizing methods recently developed for high-dimensional applications such as genomics, proteomics, or metabolomics. The application of these methods is illustrated using a publicly available dataset of gene expression profiles from 578 participants with ovarian cancer in The Cancer Genome Atlas. Stand-alone code for the demonstration is available as an RMarkdown file at https://github.com/katehoffshutta/ggmTutorial .},
  archive      = {J_SIM},
  author       = {Katherine H. Shutta and Roberta De Vito and Denise M. Scholtens and Raji Balasubramanian},
  doi          = {10.1002/sim.9546},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5150-5187},
  shortjournal = {Stat. Med.},
  title        = {Gaussian graphical models with applications to omics analyses},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synthesizing studies for comparing different treatment
sequences in clinical trials. <em>SIM</em>, <em>41</em>(25), 5134–5149.
(<a href="https://doi.org/10.1002/sim.9559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With advances in cancer treatments and improved patient survival, more patients may go through multiple lines of treatment. It is of clinical importance to choose a sequence of effective treatments (eg, lines of treatment) for individual patients with the goal of optimizing their long-term clinical outcome (eg, survival). Several important issues arise in cancer studies. First, cancer clinical trials are usually conducted by each line of treatment. For a treatment sequence, we may have first line and second line treatment data from two different studies. Second, there is typically a treatment initiation period varying from patient to patient between progression of disease and the start of the second line treatment due to administrative reasons. Additionally, the choice of the second line treatment for patients with progression of disease may depend on their characteristics. We address all these issues and develop semiparametric methods under the potential outcome framework for the estimation of the overall survival probability for a treatment sequence and for comparing different treatment sequences. We establish the large sample properties of the proposed inferential procedures. Simulation studies and an application to a colorectal clinical trial are provided.},
  archive      = {J_SIM},
  author       = {Guoqing Diao and Haijun Ma and Donglin Zeng and Chunlei Ke and Joseph G. Ibrahim},
  doi          = {10.1002/sim.9559},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5134-5149},
  shortjournal = {Stat. Med.},
  title        = {Synthesizing studies for comparing different treatment sequences in clinical trials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online two-way estimation and inference via linear
mixed-effects models. <em>SIM</em>, <em>41</em>(25), 5113–5133. (<a
href="https://doi.org/10.1002/sim.9557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we tackle the estimation and inference problem of analyzing distributed streaming data that is collected continuously over multiple data sites. We propose an online two-way approach via linear mixed-effects models. We explicitly model the site-specific effects as random-effect terms, and tackle both between-site heterogeneity and within-site correlation. We develop an online updating procedure that does not need to re-access the previous data and can efficiently update the parameter estimate, when either new data sites, or new streams of sample observations of the existing data sites, become available. We derive the non-asymptotic error bound for our proposed online estimator, and show that it is asymptotically equivalent to the offline counterpart based on all the raw data. We compare with some key alternative solutions both analytically and numerically, and demonstrate the advantages of our proposal. We further illustrate our method with two data applications.},
  archive      = {J_SIM},
  author       = {Lan Luo and Lexin Li},
  doi          = {10.1002/sim.9557},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5113-5133},
  shortjournal = {Stat. Med.},
  title        = {Online two-way estimation and inference via linear mixed-effects models},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Noninferiority testing with censoring when the event rate is
low. <em>SIM</em>, <em>41</em>(25), 5102–5112. (<a
href="https://doi.org/10.1002/sim.9556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The PREDICT TB trial tests noninferiority of an abbreviated treatment regimen (arm A) vs a conventional treatment regimen (arm C). Treatment trials of drug-susceptible tuberculosis are expected to have low event rates (ie, relapse probabilities around 3-5%). We examine the question of what is the “best” way to test for noninferiority in a setting with low event rates. In a series of simulations supported by theoretical arguments, we examine operating characteristics of five tests, including normal approximation, exact, and simulation-based tests. Two of these tests are constructed from Kaplan–Meier based-estimators, which account for variable follow-up time (and those lost to follow-up). We evaluate the effect of loss to follow-up via simulations. We also examine the results of the five tests on a data set similar to PREDICT TB, the REMoxTB trial. We find that the normal approximation tests perform well, albeit with small type I error rate inflation. We also find that the Kaplan–Meier methods generally have larger power than the other tests, especially when there is between 10-30% loss to follow-up.},
  archive      = {J_SIM},
  author       = {Shannon K. Gallagher and Jing Wang and Keith Lumbard and Lori E. Dodd and Michael Proschan},
  doi          = {10.1002/sim.9556},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5102-5112},
  shortjournal = {Stat. Med.},
  title        = {Noninferiority testing with censoring when the event rate is low},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Communication-efficient estimation and inference for
high-dimensional quantile regression based on smoothed decorrelated
score. <em>SIM</em>, <em>41</em>(25), 5084–5101. (<a
href="https://doi.org/10.1002/sim.9555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed estimation based on different sources of observations has drawn attention in the modern statistical learning. In practice, due to the expensive cost or time-consuming process to collect data in some cases, the sample size on each local site can be small, but the dimension of covariates is large and may be far larger than the sample size on each site. In this article, we focus on the distributed estimation and inference for a preconceived low-dimensional parameter vector in the high-dimensional quantile regression model with small local sample size. Specifically, we consider that the data are inherently distributed and propose two communication-efficient estimators by generalizing the decorrelated score approach to conquer the slow convergence rate of nuisance parameter estimation and adopting the smoothing technique based on multiround algorithms. The risk bounds and limiting distributions of the proposed estimators are given. The finite sample performance of the proposed estimators is studied through simulations and an application to a gene expression dataset is also presented.},
  archive      = {J_SIM},
  author       = {Fengrui Di and Lei Wang and Heng Lian},
  doi          = {10.1002/sim.9555},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5084-5101},
  shortjournal = {Stat. Med.},
  title        = {Communication-efficient estimation and inference for high-dimensional quantile regression based on smoothed decorrelated score},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparison of treatments with ordinal responses in trials
with sequential monitoring and response-adaptive randomization.
<em>SIM</em>, <em>41</em>(25), 5061–5083. (<a
href="https://doi.org/10.1002/sim.9554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials, comparisons of treatments with ordinal responses are frequently conducted using the proportional odds model. However, the use of this model necessitates the adoption of the proportional odds assumption, which may not be appropriate. In particular, when responses are skewed, the use of the proportional odds model may result in a markedly inflated type I error rate. The latent Weibull distribution has recently been proposed to remedy this problem, and it has been demonstrated to be superior to the proportional odds model, especially when response-adaptive randomization is incorporated. However, there are several drawbacks associated with the latent Weibull model and the previously suggested response-adaptive treatment randomization scheme. In this paper, we propose the modified latent Weibull model to address these issues. Based on the modified latent Weibull model, the original response-adaptive design was also revised. In addition, the group sequential monitoring mechanism was included to enable interim analyses to be performed to determine, during a trial, whether a specific treatment is significantly more effective than another. If so, this will enable the trial to be terminated at a much earlier stage than a trial based on a fixed sample size. We performed a simulation study that clearly demonstrated the merits of our proposed framework. Furthermore, we redesigned a clinical study to further illustrate the advantages of our response-adaptive approach.},
  archive      = {J_SIM},
  author       = {Yian Yu and Cong Xu and Junjiang Zhong and Siu Hung Cheung},
  doi          = {10.1002/sim.9554},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5061-5083},
  shortjournal = {Stat. Med.},
  title        = {Comparison of treatments with ordinal responses in trials with sequential monitoring and response-adaptive randomization},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Classification model with weighted regularization to improve
the reproducibility of neuroimaging signature selection. <em>SIM</em>,
<em>41</em>(25), 5046–5060. (<a
href="https://doi.org/10.1002/sim.9553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) has been extensively applied in brain imaging studies to aid the diagnosis of psychiatric disorders and the selection of potential biomarkers. Due to the high dimensionality of imaging data and heterogeneous subtypes of psychiatric disorders, the reproducibility of ML results in brain imaging studies has drawn increasing attention. The reproducibility in brain imaging has been primarily examined in terms of prediction accuracy. However, achieving high prediction accuracy and discovering relevant features are two separate but related goals. An important yet under-investigated problem is the reproducibility of feature selection in brain imaging studies. We propose a new metric to quantify the reproducibility of neuroimaging feature selection via bootstrapping. We estimate the reproducibility index (R-index) for each feature as the reciprocal coefficient of variation of absolute mean difference across a larger number of bootstrap samples. We then integrate the R-index in regularized classification models as penalty weight. Reproducible features with a larger R-index are assigned smaller penalty weights and thus are more likely to be selected by our proposed models. Both simulated and multimodal neuroimaging data are used to examine the performance of our proposed models. Results show that our proposed R-index models are effective in separating informative features from noise features. Additionally, the proposed models yield similar or higher prediction accuracy than the standard regularized classification models while further reducing coefficient estimation error. Improvements achieved by the proposed models are essential to advance our understanding of the selected brain imaging features as well as their associations with psychiatric disorders.},
  archive      = {J_SIM},
  author       = {Xin Niu and Jiangtao Gou and Hansoo Chang and Michael Lowe and Fengqing (Zoe) Zhang},
  doi          = {10.1002/sim.9553},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5046-5060},
  shortjournal = {Stat. Med.},
  title        = {Classification model with weighted regularization to improve the reproducibility of neuroimaging signature selection},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single-stage, three-arm, adaptive test strategies for
non-inferiority trials with an unstable reference. <em>SIM</em>,
<em>41</em>(25), 5033–5045. (<a
href="https://doi.org/10.1002/sim.9552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For indications where only unstable reference treatments are available and use of placebo is ethically justified, three-arm “gold standard” designs with an experimental, reference and placebo arm are recommended for non-inferiority trials. In such designs, the demonstration of efficacy of the reference or experimental treatment is a requirement. They have the disadvantage that only little can be concluded from the trial if the reference fails to be efficacious. To overcome this, we investigate novel single-stage, adaptive test strategies where non-inferiority is tested only if the reference shows sufficient efficacy and otherwise δ $$ \delta $$ -superiority of the experimental treatment over placebo is tested. With a properly chosen superiority margin, -superiority indirectly shows non-inferiority. We optimize the sample size for several decision rules and find that the natural, data driven test strategy, which tests non-inferiority if the reference&#39;s efficacy test is significant, leads to the smallest overall and placebo sample sizes. We proof that under specific constraints on the sample sizes, this procedure controls the family-wise error rate. All optimal sample sizes are found to meet this constraint. We finally show how to account for a relevant placebo drop-out rate in an efficient way and apply the new test strategy to a real life data set.},
  archive      = {J_SIM},
  author       = {Werner Brannath and Martin Scharpenberg and Sylvia Schmidt},
  doi          = {10.1002/sim.9552},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5033-5045},
  shortjournal = {Stat. Med.},
  title        = {Single-stage, three-arm, adaptive test strategies for non-inferiority trials with an unstable reference},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Flexible propensity score estimation strategies for
clustered data in observational studies. <em>SIM</em>, <em>41</em>(25),
5016–5032. (<a href="https://doi.org/10.1002/sim.9551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing studies have suggested superior performance of nonparametric machine learning over logistic regression for propensity score estimation. However, it is unclear whether the advantages of nonparametric propensity score modeling are carried to settings where there is clustering of individuals, especially when there is unmeasured cluster-level confounding. In this work we examined the performance of logistic regression (all main effects), Bayesian additive regression trees and generalized boosted modeling for propensity score weighting in clustered settings, with the clustering being accounted for by including either cluster indicators or random intercepts. We simulated data for three hypothetical observational studies of varying sample and cluster sizes. Confounders were generated at both levels, including a cluster-level confounder that is unobserved in the analyses. A binary treatment and a continuous outcome were generated based on seven scenarios with varying relationships between the treatment and confounders (linear and additive, nonlinear/nonadditive, nonadditive with the unobserved cluster-level confounder). Results suggest that when the sample and cluster sizes are large, nonparametric propensity score estimation may provide better covariate balance, bias reduction, and 95% confidence interval coverage, regardless of the degree of nonlinearity or nonadditivity in the true propensity score model. When the sample or cluster sizes are small, however, nonparametric approaches may become more vulnerable to unmeasured cluster-level confounding and thus may not be a better alternative to multilevel logistic regression. We applied the methods to the National Longitudinal Study of Adolescent to Adult Health data, estimating the effect of team sports participation during adolescence on adulthood depressive symptoms.},
  archive      = {J_SIM},
  author       = {Ting-Hsuan Chang and Trang Quynh Nguyen and Youjin Lee and John W. Jackson and Elizabeth A. Stuart},
  doi          = {10.1002/sim.9551},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5016-5032},
  shortjournal = {Stat. Med.},
  title        = {Flexible propensity score estimation strategies for clustered data in observational studies},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Substantive model compatible multilevel multiple imputation:
A joint modeling approach. <em>SIM</em>, <em>41</em>(25), 5000–5015. (<a
href="https://doi.org/10.1002/sim.9549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Matteo Quartagno and James R. Carpenter},
  doi          = {10.1002/sim.9549},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5000-5015},
  shortjournal = {Stat. Med.},
  title        = {Substantive model compatible multilevel multiple imputation: A joint modeling approach},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A flexible approach for causal inference with multiple
treatments and clustered survival outcomes. <em>SIM</em>,
<em>41</em>(25), 4982–4999. (<a
href="https://doi.org/10.1002/sim.9548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When drawing causal inferences about the effects of multiple treatments on clustered survival outcomes using observational data, we need to address implications of the multilevel data structure, multiple treatments, censoring, and unmeasured confounding for causal analyses. Few off-the-shelf causal inference tools are available to simultaneously tackle these issues. We develop a flexible random-intercept accelerated failure time model, in which we use Bayesian additive regression trees to capture arbitrarily complex relationships between censored survival times and pre-treatment covariates and use the random intercepts to capture cluster-specific main effects. We develop an efficient Markov chain Monte Carlo algorithm to draw posterior inferences about the population survival effects of multiple treatments and examine the variability in cluster-level effects. We further propose an interpretable sensitivity analysis approach to evaluate the sensitivity of drawn causal inferences about treatment effect to the potential magnitude of departure from the causal assumption of no unmeasured confounding. Expansive simulations empirically validate and demonstrate good practical operating characteristics of our proposed methods. Applying the proposed methods to a dataset on older high-risk localized prostate cancer patients drawn from the National Cancer Database, we evaluate the comparative effects of three treatment approaches on patient survival, and assess the ramifications of potential unmeasured confounding. The methods developed in this work are readily available in the package .},
  archive      = {J_SIM},
  author       = {Liangyuan Hu and Jiayi Ji and Ronald D. Ennis and Joseph W. Hogan},
  doi          = {10.1002/sim.9548},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4982-4999},
  shortjournal = {Stat. Med.},
  title        = {A flexible approach for causal inference with multiple treatments and clustered survival outcomes},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Use of copula to model within-study association in bivariate
meta-analysis of binomial data at the aggregate level: A bayesian
approach and application to surrogate endpoint evaluation. <em>SIM</em>,
<em>41</em>(25), 4961–4981. (<a
href="https://doi.org/10.1002/sim.9547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bivariate meta-analysis provides a useful framework for combining information across related studies and has been utilized to combine evidence from clinical studies to evaluate treatment efficacy on two outcomes. It has also been used to investigate surrogacy patterns between treatment effects on the surrogate endpoint and the final outcome. Surrogate endpoints play an important role in drug development when they can be used to measure treatment effect early compared to the final outcome and to predict clinical benefit or harm. The standard bivariate meta-analytic approach models the observed treatment effects on the surrogate and the final outcome outcomes jointly, at both the within-study and between-studies levels, using a bivariate normal distribution. For binomial data, a normal approximation on log odds ratio scale can be used. However, this method may lead to biased results when the proportions of events are close to one or zero, affecting the validation of surrogate endpoints. In this article, we explore modeling the two outcomes on the original binomial scale. First, we present a method that uses independent binomial likelihoods to model the within-study variability avoiding to approximate the observed treatment effects. However, the method ignores the within-study association. To overcome this issue, we propose a method using a bivariate copula with binomial marginals, which allows the model to account for the within-study association. We applied the methods to an illustrative example in chronic myeloid leukemia to investigate the surrogate relationship between complete cytogenetic response and event-free-survival.},
  archive      = {J_SIM},
  author       = {Tasos Papanikos and John R. Thompson and Keith R. Abrams and Sylwia Bujkiewicz},
  doi          = {10.1002/sim.9547},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4961-4981},
  shortjournal = {Stat. Med.},
  title        = {Use of copula to model within-study association in bivariate meta-analysis of binomial data at the aggregate level: A bayesian approach and application to surrogate endpoint evaluation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast lasso-type safe screening for fine-gray competing risks
model with ultrahigh dimensional covariates. <em>SIM</em>,
<em>41</em>(24), 4941–4960. (<a
href="https://doi.org/10.1002/sim.9545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fine-Gray proportional sub-distribution hazards (PSH) model is among the most popular regression model for competing risks time-to-event data. This article develops a fast safe feature elimination method, named PSH-SAFE, for fitting the penalized Fine-Gray PSH model with a Lasso (or adaptive Lasso) penalty. Our PSH-SAFE procedure is straightforward to implement, fast, and scales well to ultrahigh dimensional data. We also show that as a feature screening procedure, PSH-SAFE is safe in a sense that the eliminated features are guaranteed to be inactive features in the original Lasso (or adaptive Lasso) estimator for the penalized PSH model. We evaluate the performance of the PSH-SAFE procedure in terms of computational efficiency, screening efficiency and safety, run-time, and prediction accuracy on multiple simulated datasets and a real bladder cancer data. Our empirical results show that the PSH-SAFE procedure possesses desirable screening efficiency and safety properties and can offer substantially improved computational efficiency as well as similar or better prediction performance in comparison to their baseline competitors.},
  archive      = {J_SIM},
  author       = {Hong Wang and Zhenyuan Shen and Zhelun Tan and Zhuan Zhang and Gang Li},
  doi          = {10.1002/sim.9545},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4941-4960},
  shortjournal = {Stat. Med.},
  title        = {Fast lasso-type safe screening for fine-gray competing risks model with ultrahigh dimensional covariates},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Causal discoveries for high dimensional mixed data.
<em>SIM</em>, <em>41</em>(24), 4924–4940. (<a
href="https://doi.org/10.1002/sim.9544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal relationships are of crucial importance for biological and medical research. Algorithms have been proposed for causal structure learning with graphical visualizations. While much of the literature focuses on biological studies where data often follow the same distribution, for example, the normal distribution for all variables, challenges emerge from epidemiological and clinical studies where data are often mixed with continuous, binary, and ordinal variables. We propose to use a mixed latent Gaussian copula model to estimate the underlying correlation structure via the rank correlation for mixed data. This correlation structure is then incorporated into a popular causal discovery algorithm, the PC algorithm, to identify causal structures. The proposed algorithm, called the latent-PC algorithm, is able to discover the true causal structure consistently under mild conditions in high dimensional settings. From simulation studies, the latent-PC algorithm delivers a competitive performance in terms of a similar or higher true positive rate and a similar or lower false positive rate, compared with other variants of the PC algorithm. In the high dimensional settings where the number of variables is more than the number of observations, the causal graphs identified by the latent-PC algorithm are closer to the true causal structures, compared to other competing algorithms. Further, we demonstrate the utility of the latent-PC algorithm in a real dataset for hepatocellular carcinoma. Causal structures for patient survival are visualized and connected with clinical interpretations in the literature.},
  archive      = {J_SIM},
  author       = {Zhanrui Cai and Dong Xi and Xuan Zhu and Runze Li},
  doi          = {10.1002/sim.9544},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4924-4940},
  shortjournal = {Stat. Med.},
  title        = {Causal discoveries for high dimensional mixed data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Doubly robust estimation of optimal dynamic treatment
regimes with multicategory treatments and survival outcomes.
<em>SIM</em>, <em>41</em>(24), 4903–4923. (<a
href="https://doi.org/10.1002/sim.9543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patients with chronic diseases, such as cancer or epilepsy, are often followed through multiple stages of clinical interventions. Dynamic treatment regimes (DTRs) are sequences of decision rules that assign treatments at each stage based on measured covariates for each patient. A DTR is said to be optimal if the expectation of the desirable clinical benefit reaches a maximum when applied to a population. When there are three or more options for treatments at each decision point and the clinical outcome of interest is a time-to-event variable, estimating an optimal DTR can be complicated. We propose a doubly robust method to estimate optimal DTRs with multicategory treatments and survival outcomes. A novel blip function is defined to measure the difference in expected outcomes among treatments, and a doubly robust weighted least squares algorithm is designed for parameter estimation. Simulations using various weight functions and scenarios support the advantages of the proposed method in estimating optimal DTRs over existing approaches. We further illustrate the practical value of our method by applying it to data from the Standard and New Antiepileptic Drugs study. In this analysis, the proposed method supports the use of the new drug lamotrigine over the standard option carbamazepine. When the actual treatments match the estimated optimal treatments, survival outcomes tend to be better. The newly developed method provides a practical approach for clinicians that is not limited to cases of binary treatment options.},
  archive      = {J_SIM},
  author       = {Zhang Zhang and Danhui Yi and Yiwei Fan},
  doi          = {10.1002/sim.9543},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4903-4923},
  shortjournal = {Stat. Med.},
  title        = {Doubly robust estimation of optimal dynamic treatment regimes with multicategory treatments and survival outcomes},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A function-based approach to model the measurement error in
wearable devices. <em>SIM</em>, <em>41</em>(24), 4886–4902. (<a
href="https://doi.org/10.1002/sim.9542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical activity (PA) is an important risk factor for many health outcomes. Wearable-devices such as accelerometers are increasingly used in biomedical studies to understand the associations between PA and health outcomes. Statistical analyses involving accelerometer data are challenging due to the following three characteristics: (i) high-dimensionality, (ii) temporal dependence, and (iii) measurement error. To address these challenges we treat accelerometer-based measures of PA as a single function-valued covariate prone to measurement error. Specifically, in order to determine the relationship between PA and a health outcome of interest, we propose a regression model with a functional covariate that accounts for measurement error. Using regression calibration, we develop a two-step estimation method for the model parameters and establish their consistency. A test is also proposed to test the significance of the estimated model parameters. Simulation studies are conducted to compare the proposed methods with existing alternative approaches under varying scenarios. Finally, the developed methods are used to assess the relationship between PA intensity and BMI obtained from the National Health and Nutrition Examination Survey data.},
  archive      = {J_SIM},
  author       = {Sneha Jadhav and Carmen D. Tekwe and Yuanyuan Luan},
  doi          = {10.1002/sim.9542},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4886-4902},
  shortjournal = {Stat. Med.},
  title        = {A function-based approach to model the measurement error in wearable devices},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and analysis of cluster randomized trials with
time-to-event outcomes under the additive hazards mixed model.
<em>SIM</em>, <em>41</em>(24), 4860–4885. (<a
href="https://doi.org/10.1002/sim.9541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A primary focus of current methods for cluster randomized trials (CRTs) has been for continuous, binary, and count outcomes, with relatively less attention given to right-censored, time-to-event outcomes. In this article, we detail considerations for sample size requirement and statistical inference in CRTs with time-to-event outcomes when the intervention effect parameter is specified through the additive hazards mixed model (AHMM), which includes a frailty term to explicitly account for the dependency between the failure times. First, we discuss improved inference for the treatment effect parameter via bias-corrected sandwich variance estimators and randomization-based test under AHMM, addressing potential small-sample biases in CRTs. Next, we derive a new sample size formula for AHMM analysis of CRTs accommodating both equal and unequal cluster sizes. When the cluster sizes vary, our sample size formula depends on the mean and coefficient of variation of cluster sizes, based on which we articulate the impact of cluster size variation in CRTs with time-to-event outcomes. Furthermore, we obtain the insight that the classical variance inflation factor for CRTs with a non-censored outcome can in fact apply to CRTs with a time-to-event outcome, providing that an appropriate definition of the intraclass correlation coefficient is considered under AHMM. Simulation studies are carried out to illustrate key design and analysis considerations in CRTs with a small to moderate number of clusters. The proposed sample size procedure and analytical methods are further illustrated using the context of the STrategies to Reduce Injuries and Develop Confidence in Elders CRT.},
  archive      = {J_SIM},
  author       = {Ondrej Blaha and Denise Esserman and Fan Li},
  doi          = {10.1002/sim.9541},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4860-4885},
  shortjournal = {Stat. Med.},
  title        = {Design and analysis of cluster randomized trials with time-to-event outcomes under the additive hazards mixed model},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Weighted generalized score test for comparing predictive
values in the presence of verification bias. <em>SIM</em>,
<em>41</em>(24), 4838–4859. (<a
href="https://doi.org/10.1002/sim.9540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Positive and negative predictive values of a diagnostic test are two important measures of test accuracy, which are more relevant in clinical settings than sensitivity and specificity. Statistical methods have been well-developed to compare the predictive values of two binary diagnostic tests when test results and disease status fully observed for all study patients. In practice, however, it is common that only a subset of study patients have the disease status verified due to ethical or cost considerations. Methods applied directly to the verified subjects may lead to biased results. A bias-corrected method has been developed to compare two predictive values in the presence of verification bias. However, the complexity of the existing method and the computational difficulty in implementing it has restricted its use. A simple and easily implemented statistical method is therefore needed. In this paper, we propose a weighted generalized score (WGS) test statistic for comparing two predictive values in the presence of verification bias. The proposed WGS test statistic is intuitive and simple to compute, only involving some minor modification of the WGS test statistic when disease status is verified for each study patient. Simulations demonstrate that the proposed WGS test statistic preserves type I error much better than the existing Wald statistic. The method is illustrated with data from a study of methods for the diagnosis of coronary artery disease.},
  archive      = {J_SIM},
  author       = {Yougui Wu},
  doi          = {10.1002/sim.9540},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4838-4859},
  shortjournal = {Stat. Med.},
  title        = {Weighted generalized score test for comparing predictive values in the presence of verification bias},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Calculating the power to examine treatment-covariate
interactions when planning an individual participant data meta-analysis
of randomized trials with a binary outcome. <em>SIM</em>,
<em>41</em>(24), 4822–4837. (<a
href="https://doi.org/10.1002/sim.9538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Before embarking on an individual participant data meta-analysis (IPDMA) project, researchers and funders need assurance it is worth their time and cost. This should include consideration of how many studies are promising their IPD and, given the characteristics of these studies, the power of an IPDMA including them. Here, we show how to estimate the power of a planned IPDMA of randomized trials to examine treatment-covariate interactions at the participant level (ie, treatment effect modifiers). We focus on a binary outcome with binary or continuous covariates, and propose a three-step approach, which assumes the true interaction size is common to all trials. In step one, the user must specify a minimally important interaction size and, for each trial separately (eg, as obtained from trial publications), the following aggregate data: the number of participants and events in control and treatment groups, the mean and SD for each continuous covariate, and the proportion of participants in each category for each binary covariate. This allows the variance of the interaction estimate to be calculated for each trial, using an analytic solution for Fisher&#39;s information matrix from a logistic regression model. Step 2 calculates the variance of the summary interaction estimate from the planned IPDMA (equal to the inverse of the sum of the inverse trial variances from step 1), and step 3 calculates the corresponding power based on a two-sided Wald test. Stata and R code are provided, and two examples given for illustration. Extension to allow for between-study heterogeneity is also considered.},
  archive      = {J_SIM},
  author       = {Richard D. Riley and Miriam Hattle and Gary S. Collins and Rebecca Whittle and Joie Ensor},
  doi          = {10.1002/sim.9538},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4822-4837},
  shortjournal = {Stat. Med.},
  title        = {Calculating the power to examine treatment-covariate interactions when planning an individual participant data meta-analysis of randomized trials with a binary outcome},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Paired serial limiting dilution assays. <em>SIM</em>,
<em>41</em>(24), 4809–4821. (<a
href="https://doi.org/10.1002/sim.9537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serial limiting dilution (SLD) assays are a widely used tool in many areas of public health research to measure the concentration of target entities. This concentration can be estimated via maximum likelihood. Asymptotic as well as exact inference methods have been proposed for hypothesis testing and confidence interval construction in this one-sample problem. However, in many scientific applications, it may be of interest to compare the concentration of target entities between a pair of samples and construct valid confidence intervals for the difference in concentrations. In this paper, an exact, computationally efficient inferential procedure is proposed for hypothesis testing and confidence interval construction in the two-sample SLD assay problem. The proposed exact method is compared to an approach based on asymptotic approximations in simulation studies. The methods are illustrated using data from the University of North Carolina HIV Cure Center.},
  archive      = {J_SIM},
  author       = {Xiudi Li and Susanne May and Ilana M. Trumble and Nancie M. Archin and Michael G. Hudgens},
  doi          = {10.1002/sim.9537},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4809-4821},
  shortjournal = {Stat. Med.},
  title        = {Paired serial limiting dilution assays},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric analysis of a generalized linear model with
multiple covariates subject to detection limits. <em>SIM</em>,
<em>41</em>(24), 4791–4808. (<a
href="https://doi.org/10.1002/sim.9536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies on the health effects of environmental mixtures face the challenge of limit of detection (LOD) in multiple correlated exposure measurements. Conventional approaches to deal with covariates subject to LOD, including complete-case analysis, substitution methods, and parametric modeling of covariate distribution, are feasible but may result in efficiency loss or bias. With a single covariate subject to LOD, a flexible semiparametric accelerated failure time (AFT) model to accommodate censored measurements has been proposed. We generalize this approach by considering a multivariate AFT model for the multiple correlated covariates subject to LOD and a generalized linear model for the outcome. A two-stage procedure based on semiparametric pseudo-likelihood is proposed for estimating the effects of these covariates on health outcome. Consistency and asymptotic normality of the estimators are derived for an arbitrary fixed dimension of covariates. Simulations studies demonstrate good large sample performance of the proposed methods vs conventional methods in realistic scenarios. We illustrate the practical utility of the proposed method with the LIFECODES birth cohort data, where we compare our approach to existing approaches in an analysis of multiple urinary trace metals in association with oxidative stress in pregnant women.},
  archive      = {J_SIM},
  author       = {Ling-Wan Chen and Jason P. Fine and Eric Bair and Victor S. Ritter and Thomas F. McElrath and David E. Cantonwine and John D. Meeker and Kelly K. Ferguson and Shanshan Zhao},
  doi          = {10.1002/sim.9536},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4791-4808},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric analysis of a generalized linear model with multiple covariates subject to detection limits},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unified estimation for cox regression model with nonmonotone
missing at random covariates. <em>SIM</em>, <em>41</em>(24), 4781–4790.
(<a href="https://doi.org/10.1002/sim.9512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates a unified estimator for Cox regression model (Cox, 1972) when covariate data are missing at random (Rubin, 1976). It extends the idea of using parametric working models (Zhao and Liu, 2021) to extract the partial information contained in the incomplete observations. The working models are flexible and convenient to deal with nonmonotone missing data patterns. It can also incorporate auxiliary variables into the analysis to reduce estimation bias and improve efficiency. The unified estimator is consistent and more efficient than the (weighted) complete case estimator. Similar to multiple imputation (MI) method (Rubin, 1987 and 1996), the proposed method is also based on standard (weighted) complete data analysis and can be easily implemented in standard software. Simulation studies comparing the unified estimator with the substantive model compatible modification of the fully conditional specification MI (SMC-FCS) estimator (Bartlett et al., 2015) in various settings indicate that the unified estimator is consistent and as efficient as SMC-FCS estimator. Data from a clinical trial in patients with early breast cancer are analyzed for illustration.},
  archive      = {J_SIM},
  author       = {David Luke Thiessen and Yang Zhao and Dongsheng Tu},
  doi          = {10.1002/sim.9512},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4781-4790},
  shortjournal = {Stat. Med.},
  title        = {Unified estimation for cox regression model with nonmonotone missing at random covariates},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accommodating population differences when validating risk
prediction models. <em>SIM</em>, <em>41</em>(24), 4756–4780. (<a
href="https://doi.org/10.1002/sim.9447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Validation of risk prediction models in independent data provides a more rigorous assessment of model performance than internal assessment, for example, done by cross-validation in the data used for model development. However, several differences between the populations that gave rise to the training and the validation data can lead to seemingly poor performance of a risk model. In this paper we formalize the notions of “similarity” or “relatedness” of the training and validation data, and define reproducibility and transportability. We address the impact of different distributions of model predictors and differences in verifying the disease status or outcome on measures of calibration, accuracy and discrimination of a model. When individual level information from both the training and validation data sets is available, we propose and study weighted versions of the validation metrics that adjust for differences in the risk factor distributions and in outcome verification between the training and validation data to provide a more comprehensive assessment of model performance. We provide conditions on the risk model and the populations that gave rise to the training and validation data that ensure a model&#39;s reproducibility or transportability, and show how to check these conditions using weighted and unweighted performance measures. We illustrate the method by developing and validating a model that predicts the risk of developing prostate cancer using data from two large prostate cancer screening trials.},
  archive      = {J_SIM},
  author       = {Ruth M. Pfeiffer and Yiyao Chen and Mitchell H. Gail and Donna P. Ankerst},
  doi          = {10.1002/sim.9447},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4756-4780},
  shortjournal = {Stat. Med.},
  title        = {Accommodating population differences when validating risk prediction models},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Designing a longitudinal clinical trial based on a composite
endpoint: Sample size, monitoring, and adaptation. <em>SIM</em>,
<em>41</em>(24), 4745–4755. (<a
href="https://doi.org/10.1002/sim.9416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal clinical trials are often designed to compare treatments on the basis of multiple outcomes. For example in the case of cardiac trials, the outcomes of interest include mortality as well as cardiac events and hospitalization. For a COVID-19 trial, the outcomes of interest include mortality, time on ventilator, and time in hospital. Earlier work by these authors proposed a non-parametric test based on a composite of multiple endpoints referred to as the Finkelstein-Schoenfeld ( FS ) test (Finkelstein and Schoenfeld. Stat Med . 1999;18(11):1341–1354.). More recently, an estimate of the treatment comparison based on multiple endpoints (related to the FS test) was proposed (Pocock et al. Eur Heart J . 2011;33(2):176–182.). This estimate, which summarized the ratio of the number of patients who fared better vs worse on the experimental arm was coined the win ratio . The aim of this article is to provide guidance in the design of a trial that will use the FS test or the win ratio. The issues that will be considered are the sample size, sequential monitoring, and adaptive designs.},
  archive      = {J_SIM},
  author       = {David A. Schoenfeld and Ritesh Ramchandani and Dianne M. Finkelstein},
  doi          = {10.1002/sim.9416},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4745-4755},
  shortjournal = {Stat. Med.},
  title        = {Designing a longitudinal clinical trial based on a composite endpoint: Sample size, monitoring, and adaptation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple imputation and test-wise deletion for causal
discovery with incomplete cohort data. <em>SIM</em>, <em>41</em>(23),
4716–4743. (<a href="https://doi.org/10.1002/sim.9535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal discovery algorithms estimate causal graphs from observational data. This can provide a valuable complement to analyses focusing on the causal relation between individual treatment-outcome pairs. Constraint-based causal discovery algorithms rely on conditional independence testing when building the graph. Until recently, these algorithms have been unable to handle missing values. In this article, we investigate two alternative solutions: test-wise deletion and multiple imputation. We establish necessary and sufficient conditions for the recoverability of causal structures under test-wise deletion, and argue that multiple imputation is more challenging in the context of causal discovery than for estimation. We conduct an extensive comparison by simulating from benchmark causal graphs: as one might expect, we find that test-wise deletion and multiple imputation both clearly outperform list-wise deletion and single imputation. Crucially, our results further suggest that multiple imputation is especially useful in settings with a small number of either Gaussian or discrete variables, but when the dataset contains a mix of both neither method is uniformly best. The methods we compare include random forest imputation and a hybrid procedure combining test-wise deletion and multiple imputation. An application to data from the IDEFICS cohort study on diet- and lifestyle-related diseases in European children serves as an illustrating example.},
  archive      = {J_SIM},
  author       = {Janine Witte and Ronja Foraita and Vanessa Didelez},
  doi          = {10.1002/sim.9535},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4716-4743},
  shortjournal = {Stat. Med.},
  title        = {Multiple imputation and test-wise deletion for causal discovery with incomplete cohort data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Classification of disease recurrence using transition
likelihoods with expectation-maximization algorithm. <em>SIM</em>,
<em>41</em>(23), 4697–4715. (<a
href="https://doi.org/10.1002/sim.9534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When an infectious disease recurs, it may be due to treatment failure or a new infection. Being able to distinguish and classify these two different outcomes is critical in effective disease control. A multi-state model based on Markov processes is a typical approach to estimating the transition probability between the disease states. However, it can perform poorly when the disease state is unknown. This article aims to demonstrate that the transition likelihoods of baseline covariates can distinguish one cause from another with high accuracy in infectious diseases such as malaria. A more general model for disease progression can be constructed to allow for additional disease outcomes. We start from a multinomial logit model to estimate the disease transition probabilities and then utilize the baseline covariate&#39;s transition information to provide a more accurate classification result. We apply the expectation-maximization (EM) algorithm to estimate unknown parameters, including the marginal probabilities of disease outcomes. A simulation study comparing our classifier to the existing two-stage method shows that our classifier has better accuracy, especially when the sample size is small. The proposed method is applied to determining relapse vs reinfection outcomes in two Plasmodium vivax treatment studies from Cambodia that used different genotyping approaches to demonstrate its practical use.},
  archive      = {J_SIM},
  author       = {Huijun Jiang and Quefeng Li and Jessica T. Lin and Feng-Chang Lin},
  doi          = {10.1002/sim.9534},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4697-4715},
  shortjournal = {Stat. Med.},
  title        = {Classification of disease recurrence using transition likelihoods with expectation-maximization algorithm},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Capturing the pool dilution effect in group testing
regression: A bayesian approach. <em>SIM</em>, <em>41</em>(23),
4682–4696. (<a href="https://doi.org/10.1002/sim.9532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group (pooled) testing is becoming a popular strategy for screening large populations for infectious diseases. This popularity is owed to the cost savings that can be realized through implementing group testing methods. These methods involve physically combining biomaterial (eg, saliva, blood, urine) collected on individuals into pooled specimens which are tested for an infection of interest. Through testing these pooled specimens, group testing methods reduce the cost of diagnosing all individuals under study by reducing the number of tests performed. Even though group testing offers substantial cost reductions, some practitioners are hesitant to adopt group testing methods due to the so-called dilution effect . The dilution effect describes the phenomenon in which biomaterial from negative individuals dilute the contributions from positive individuals to such a degree that a pool is incorrectly classified. Ignoring the dilution effect can reduce classification accuracy and lead to bias in parameter estimates and inaccurate inference. To circumvent these issues, we propose a Bayesian regression methodology which directly acknowledges the dilution effect while accommodating data that arises from any group testing protocol. As a part of our estimation strategy, we are able to identify pool specific optimal classification thresholds which are aimed at maximizing the classification accuracy of the group testing protocol being implemented. These two features working in concert effectively alleviate the primary concerns raised by practitioners regarding group testing. The performance of our methodology is illustrated via an extensive simulation study and by being applied to Hepatitis B data collected on Irish prisoners.},
  archive      = {J_SIM},
  author       = {Stella Self and Christopher McMahan and Stefani Mokalled},
  doi          = {10.1002/sim.9532},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4682-4696},
  shortjournal = {Stat. Med.},
  title        = {Capturing the pool dilution effect in group testing regression: A bayesian approach},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian analysis for partly linear cox model with
measurement error and time-varying covariate effect. <em>SIM</em>,
<em>41</em>(23), 4666–4681. (<a
href="https://doi.org/10.1002/sim.9531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Cox proportional hazards model is commonly used to estimate the association between time-to-event and covariates. Under the proportional hazards assumption, covariate effects are assumed to be constant in the follow-up period of study. When measurement error presents, common estimation methods that adjust for an error-contaminated covariate in the Cox proportional hazards model assume that the true function on the covariate is parametric and specified. We consider a semiparametric partly linear Cox model that allows the hazard to depend on an unspecified function of an error-contaminated covariate and an error-free covariate with time-varying effect, which simultaneously relaxes the assumption on the functional form of the error-contaminated covariate and allows for nonconstant effect of the error-free covariate. We take a Bayesian approach and approximate the unspecified function by a B-spline. Simulation studies are conducted to assess the finite sample performance of the proposed approach. The results demonstrate that our proposed method has favorable statistical performance. The proposed method is also illustrated by an application to data from the AIDS Clinical Trials Group Protocol 175.},
  archive      = {J_SIM},
  author       = {Anqi Pan and Xiao Song and Hanwen Huang},
  doi          = {10.1002/sim.9531},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4666-4681},
  shortjournal = {Stat. Med.},
  title        = {Bayesian analysis for partly linear cox model with measurement error and time-varying covariate effect},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bayesian modified ising model for identifying spatially
variable genes from spatial transcriptomics data. <em>SIM</em>,
<em>41</em>(23), 4647–4665. (<a
href="https://doi.org/10.1002/sim.9530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recent technology breakthrough in spatial molecular profiling (SMP) has enabled the comprehensive molecular characterizations of single cells while preserving spatial information. It provides new opportunities to delineate how cells from different origins form tissues with distinctive structures and functions. One immediate question in SMP data analysis is to identify genes whose expressions exhibit spatially correlated patterns, called spatially variable (SV) genes. Most current methods to identify SV genes are built upon the geostatistical model with Gaussian process to capture the spatial patterns. However, the Gaussian process models rely on ad hoc kernels that could limit the models&#39; ability to identify complex spatial patterns. In order to overcome this challenge and capture more types of spatial patterns, we introduce a Bayesian approach to identify SV genes via a modified Ising model. The key idea is to use the energy interaction parameter of the Ising model to characterize spatial expression patterns. We use auxiliary variable Markov chain Monte Carlo algorithms to sample from the posterior distribution with an intractable normalizing constant in the model. Simulation studies using both simulated and synthetic data showed that the energy-based modeling approach led to higher accuracy in detecting SV genes than those kernel-based methods. When applied to two real spatial transcriptomics (ST) datasets, the proposed method discovered novel spatial patterns that shed light on the biological mechanisms. In summary, the proposed method presents a new perspective for analyzing ST data.},
  archive      = {J_SIM},
  author       = {Xi Jiang and Guanghua Xiao and Qiwei Li},
  doi          = {10.1002/sim.9530},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4647-4665},
  shortjournal = {Stat. Med.},
  title        = {A bayesian modified ising model for identifying spatially variable genes from spatial transcriptomics data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling and forecasting of at home activity in older adults
using passive sensor technology. <em>SIM</em>, <em>41</em>(23),
4629–4646. (<a href="https://doi.org/10.1002/sim.9529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Life expectancy in the UK has increased since the 19th century. As of 2019, there are just under 12 million people in the UK aged 65 or over, with close to a quarter living by themselves. Thus, many families and carers are looking for new ways to improve the health and care of older people. Passive sensors such as infra-red motion and plug sensors have had success as a noninvasive way to help the older people. These provide a series of categorical sensor events throughout the day. Modeling this categorical dataset can help to understand and predict behavior. This article proposes a method to model the probability a sensor will trigger throughout the day for a household whilst accounting for the prior data and other sensors within the home. We present our results on a dataset from Howz, a company helping people to passively identify changes in their behavior over time.},
  archive      = {J_SIM},
  author       = {Jess Gillam and Rebecca Killick and Jack Heal and Ben Norwood},
  doi          = {10.1002/sim.9529},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4629-4646},
  shortjournal = {Stat. Med.},
  title        = {Modeling and forecasting of at home activity in older adults using passive sensor technology},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian criterion-based assessments of recurrent event
models with applications to commercial truck driver behavior studies.
<em>SIM</em>, <em>41</em>(23), 4607–4628. (<a
href="https://doi.org/10.1002/sim.9528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitype recurrent events are commonly observed in transportation studies, since commercial truck drivers may encounter different types of safety critical events (SCEs) and take different lengths of on-duty breaks in a driving shift. Bayesian nonhomogeneous Poisson process models are a flexible approach to jointly model the intensity functions of the multitype recurrent events. For evaluating and comparing these models, the deviance information criterion (DIC) and the logarithm of the pseudo-marginal likelihood (LPML) are studied and Monte Carlo methods are developed for computing these model assessment measures. We also propose a set of new concordance indices (C-indices) to evaluate various discrimination abilities of a Bayesian multitype recurrent event model. Specifically, the within-event C-index quantifies adequacy of a given model in fitting the recurrent event data for each type, the between-event C-index provides an assessment of the model fit between two types of recurrent events, and the overall C-index measures the model&#39;s discrimination ability among multiple types of recurrent events simultaneously. Moreover, we jointly model the incidence of SCEs and on-duty breaks with driving behaviors using a Bayesian Poisson process model with time-varying coefficients and time-dependent covariates. An in-depth analysis of a real dataset from the commercial truck driver naturalistic driving study is carried out to demonstrate the usefulness and applicability of the proposed methodology.},
  archive      = {J_SIM},
  author       = {Yiming Zhang and Ming-Hui Chen and Feng Guo},
  doi          = {10.1002/sim.9528},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4607-4628},
  shortjournal = {Stat. Med.},
  title        = {Bayesian criterion-based assessments of recurrent event models with applications to commercial truck driver behavior studies},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating cumulative spatial risk over time with low-rank
kriging multiple membership models. <em>SIM</em>, <em>41</em>(23),
4593–4606. (<a href="https://doi.org/10.1002/sim.9527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many health outcomes result from accumulated exposures to one or more environmental factors. Accordingly, spatial risk studies have begun to consider multiple residential locations of participants, acknowledging that participants move and thus are exposed to environmental factors in several places. However, novel methods are needed to estimate cumulative spatial risk for disease while accounting for other risk factors. To this end, we propose a Bayesian model (LRK-MMM) that embeds a multiple membership model (MMM) into a low-rank kriging (LRK) model in order to estimate cumulative spatial risk at the point level while allowing for multiple residential locations per subject. The LRK approach offers a more computationally efficient means to analyze spatial risk in case-control study data at the point level compared with a Bayesian generalized additive model, and as increased precision in spatial risk estimates by analyzing point locations instead of administrative areas. Through a simulation study, we demonstrate the efficacy of the model and its improvement upon an existing multiple membership model that uses area-level spatial random effects to estimate risk. The results show that our proposed method provides greater spatial sensitivity (improvements ranging from 0.12 to 0.54) and power (improvements ranging from 0.02 to 0.94) to detect regions of elevated risk for disease across a range of exposure scenarios. Finally, we apply our model to case-control data from the New England bladder cancer study to estimate cumulative spatial risk while adjusting for many covariates.},
  archive      = {J_SIM},
  author       = {Joseph Boyle and Mary H. Ward and Stella Koutros and Margaret R. Karagas and Molly Schwenn and Debra Silverman and David C. Wheeler},
  doi          = {10.1002/sim.9527},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4593-4606},
  shortjournal = {Stat. Med.},
  title        = {Estimating cumulative spatial risk over time with low-rank kriging multiple membership models},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Envelope-based partial partial least squares with
application to cytokine-based biomarker analysis for COVID-19.
<em>SIM</em>, <em>41</em>(23), 4578–4592. (<a
href="https://doi.org/10.1002/sim.9526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial least squares (PLS) regression is a popular alternative to ordinary least squares regression because of its superior prediction performance demonstrated in many cases. In various contemporary applications, the predictors include both continuous and categorical variables. A common practice in PLS regression is to treat the categorical variable as continuous. However, studies find that this practice may lead to biased estimates and invalid inferences (Schuberth et al., 2018). Based on a connection between the envelope model and PLS, we develop an envelope-based partial PLS estimator that considers the PLS regression on the conditional distributions of the response(s) and continuous predictors on the categorical predictors. Root-n consistency and asymptotic normality are established for this estimator. Numerical study shows that this approach can achieve more efficiency gains in estimation and produce better predictions. The method is applied for the identification of cytokine-based biomarkers for COVID-19 patients, which reveals the association between the cytokine-based biomarkers and patients&#39; clinical information including disease status at admission and demographical characteristics. The efficient estimation leads to a clear scientific interpretation of the results.},
  archive      = {J_SIM},
  author       = {Yeonhee Park and Zhihua Su and Dongjun Chung},
  doi          = {10.1002/sim.9526},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4578-4592},
  shortjournal = {Stat. Med.},
  title        = {Envelope-based partial partial least squares with application to cytokine-based biomarker analysis for COVID-19},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Targeted maximum likelihood estimation of causal effects
with interference: A simulation study. <em>SIM</em>, <em>41</em>(23),
4554–4577. (<a href="https://doi.org/10.1002/sim.9525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interference, the dependency of an individual&#39;s potential outcome on the exposure of other individuals, is a common occurrence in medicine and public health. Recently, targeted maximum likelihood estimation (TMLE) has been extended to settings of interference, including in the context of estimation of the mean of an outcome under a specified distribution of exposure, referred to as a policy. This paper summarizes how TMLE for independent data is extended to general interference (network-TMLE). An extensive simulation study is presented of network-TMLE, consisting of four data generating mechanisms (unit-treatment effect only, spillover effects only, unit-treatment and spillover effects, infection transmission) in networks of varying structures. Simulations show that network-TMLE performs well across scenarios with interference, but issues manifest when policies are not well-supported by the observed data, potentially leading to poor confidence interval coverage. Guidance for practical application, freely available software, and areas of future work are provided.},
  archive      = {J_SIM},
  author       = {Paul N. Zivich and Michael G. Hudgens and Maurice A. Brookhart and James Moody and David J. Weber and Allison E. Aiello},
  doi          = {10.1002/sim.9525},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4554-4577},
  shortjournal = {Stat. Med.},
  title        = {Targeted maximum likelihood estimation of causal effects with interference: A simulation study},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rank-based bayesian variable selection for genome-wide
transcriptomic analyses. <em>SIM</em>, <em>41</em>(23), 4532–4553. (<a
href="https://doi.org/10.1002/sim.9524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable selection is crucial in high-dimensional omics-based analyses, since it is biologically reasonable to assume only a subset of non-noisy features contributes to the data structures. However, the task is particularly hard in an unsupervised setting, and a priori ad hoc variable selection is still a very frequent approach, despite the evident drawbacks and lack of reproducibility. We propose a Bayesian variable selection approach for rank-based unsupervised transcriptomic analysis. Making use of data rankings instead of the actual continuous measurements increases the robustness of conclusions when compared to classical statistical methods, and embedding variable selection into the inferential tasks allows complete reproducibility. Specifically, we develop a novel extension of the Bayesian Mallows model for variable selection that allows for a full probabilistic analysis, leading to coherent quantification of uncertainties. Simulation studies demonstrate the versatility and robustness of the proposed method in a variety of scenarios, as well as its superiority with respect to several competitors when varying the data dimension or data generating process. We use the novel approach to analyze genome-wide RNAseq gene expression data from ovarian cancer patients: several genes that affect cancer development are correctly detected in a completely unsupervised fashion, showing the usefulness of the method in the context of signature discovery for cancer genomics. Moreover, the possibility to also perform uncertainty quantification plays a key role in the subsequent biological investigation.},
  archive      = {J_SIM},
  author       = {Emilie Eliseussen and Thomas Fleischer and Valeria Vitelli},
  doi          = {10.1002/sim.9524},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4532-4553},
  shortjournal = {Stat. Med.},
  title        = {Rank-based bayesian variable selection for genome-wide transcriptomic analyses},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A calibration approach to transportability and data-fusion
with observational data. <em>SIM</em>, <em>41</em>(23), 4511–4531. (<a
href="https://doi.org/10.1002/sim.9523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two important considerations in clinical research studies are proper evaluations of internal and external validity. While randomized clinical trials can overcome several threats to internal validity, they may be prone to poor external validity. Conversely, large prospective observational studies sampled from a broadly generalizable population may be externally valid, yet susceptible to threats to internal validity, particularly confounding. Thus, methods that address confounding and enhance transportability of study results across populations are essential for internally and externally valid causal inference, respectively. These issues persist for another problem closely related to transportability known as data-fusion. We develop a calibration method to generate balancing weights that address confounding and sampling bias, thereby enabling valid estimation of the target population average treatment effect. We compare the calibration approach to two additional doubly robust methods that estimate the effect of an intervention on an outcome within a second, possibly unrelated target population. The proposed methodologies can be extended to resolve data-fusion problems that seek to evaluate the effects of an intervention using data from two related studies sampled from different populations. A simulation study is conducted to demonstrate the advantages and similarities of the different techniques. We also test the performance of the calibration approach in a motivating real data example comparing whether the effect of biguanides vs sulfonylureas—the two most common oral diabetes medication classes for initial treatment—on all-cause mortality described in a historical cohort applies to a contemporary cohort of US Veterans with diabetes.},
  archive      = {J_SIM},
  author       = {Kevin P. Josey and Fan Yang and Debashis Ghosh and Sridharan Raghavan},
  doi          = {10.1002/sim.9523},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4511-4531},
  shortjournal = {Stat. Med.},
  title        = {A calibration approach to transportability and data-fusion with observational data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A note regarding alternative explanations for heterogeneity
in meta-analysis. <em>SIM</em>, <em>41</em>(22), 4501–4509. (<a
href="https://doi.org/10.1002/sim.9403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Stephen Senn and Susanne Schmitz and Anna Schritz and Artur Araujo},
  doi          = {10.1002/sim.9403},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4501-4509},
  shortjournal = {Stat. Med.},
  title        = {A note regarding alternative explanations for heterogeneity in meta-analysis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling multivariate age-related imaging variables with
dependencies. <em>SIM</em>, <em>41</em>(22), 4484–4500. (<a
href="https://doi.org/10.1002/sim.9522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroimaging techniques have been increasingly used to understand the neural biology of aging brains. The neuroimaging variables from distinct brain locations and modalities can exhibit age-related patterns that reflect localized neural decline. However, it is a challenge to identify the impacts of risk factors (eg, mental disorders) on multivariate imaging variables while simultaneously accounting for the dependence structure and nonlinear age trajectories using existing tools. We propose a mixed-effects model to address this challenge by building random effects based on the latent brain aging status. We develop computationally efficient algorithms to estimate the parameters of new random effects. The simulations show that our approach provides accurate parameter estimates, improves the inference efficiency, and reduces the root mean square error compared to existing methods. We further apply this method to the UK Biobank data to investigate the effects of tobacco smoking on the white matter integrity of the entire brain during aging and identify the adverse effects on white matter integrity with multiple fiber tracts.},
  archive      = {J_SIM},
  author       = {Hwiyoung Lee and Chixiang Chen and Peter Kochunov and Liyi Elliot Hong and Shuo Chen},
  doi          = {10.1002/sim.9522},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4484-4500},
  shortjournal = {Stat. Med.},
  title        = {Modeling multivariate age-related imaging variables with dependencies},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Defining r-squared measures for mixed-effects location scale
models. <em>SIM</em>, <em>41</em>(22), 4467–4483. (<a
href="https://doi.org/10.1002/sim.9521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ecological momentary assessment and other modern data collection technologies facilitate research on both within-subject and between-subject variability of health outcomes and behaviors. For such intensively measured longitudinal data, Hedeker et al extended the usual two-level mixed-effects model to a two-level mixed-effects location scale (MELS) model to accommodate covariates&#39; influence as well as random subject effects on both mean (location) and variability (scale) of the outcome. However, there is a lack of existing standardized effect size measures for the MELS model. To fill this gap, our study extends Rights and Sterba&#39;s framework of measures for multilevel models, which is based on model-implied variances, to MELS models. Our proposed framework applies to two different specifications of the random location effects, namely, through covariate-influenced random intercepts and through random intercepts combined with random slopes of observation-level covariates. We also provide an R function, R2MELS , that outputs summary tables and visualization for values of our measures. This framework is validated through a simulation study, and data from a health behaviors study and a depression study are used as examples to demonstrate this framework. These measures can help researchers provide greater interpretation of their findings using MELS models.},
  archive      = {J_SIM},
  author       = {Xingruo Zhang and Donald Hedeker},
  doi          = {10.1002/sim.9521},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4467-4483},
  shortjournal = {Stat. Med.},
  title        = {Defining R-squared measures for mixed-effects location scale models},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian unanchored additive models for component network
meta-analysis. <em>SIM</em>, <em>41</em>(22), 4444–4466. (<a
href="https://doi.org/10.1002/sim.9520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Component network meta-analysis (CNMA) models are an extension of standard network meta-analysis (NMA) models which account for the use of multicomponent treatments in the network. This article contributes innovatively to several statistical aspects of CNMA. First, by introducing a unified notation, we establish that currently available methods differ in the way they assume additivity, an important distinction that has been overlooked so far in the literature. In particular, one model uses a more restrictive form of additivity than the other which we term an anchored and unanchored model, respectively. We show that an anchored model can provide a poor fit to the data if it is misspecified. Second, given that Bayesian models are often preferred by practitioners, we develop two novel unanchored Bayesian CNMA models presented under the unified notation. An extensive simulation study examining bias, coverage probabilities, and treatment rankings confirms the favorable performance of the novel models. This is the first simulation study to compare the statistical properties of CNMA models in the literature. Finally, the use of our novel models is demonstrated on a real dataset, and the results of CNMA models on the dataset are compared.},
  archive      = {J_SIM},
  author       = {Augustine Wigle and Audrey Béliveau},
  doi          = {10.1002/sim.9520},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4444-4466},
  shortjournal = {Stat. Med.},
  title        = {Bayesian unanchored additive models for component network meta-analysis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bootstrap vs asymptotic variance estimation when using
propensity score weighting with continuous and binary outcomes.
<em>SIM</em>, <em>41</em>(22), 4426–4443. (<a
href="https://doi.org/10.1002/sim.9519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We used Monte Carlo simulations to compare the performance of asymptotic variance estimators to that of the bootstrap when estimating standard errors of differences in means, risk differences, and relative risks using propensity score weighting. We considered four different sets of weights: conventional inverse probability of treatment weights with the average treatment effect (ATE) as the target estimand, weights for estimating the average treatment effect in the treated (ATT), matching weights, and overlap weights. We considered sample sizes ranging from 250 to 10 000 and allowed the prevalence of treatment to range from 0.1 to 0.9. We found that, when using ATE weights and sample sizes were ≤ 1000, then the use of the bootstrap resulted in estimates of SE that were more accurate than the asymptotic estimates. A similar finding was observed when using ATT weights and sample sizes were ≤ 1000 and the prevalence of treatment was moderate to high. When using matching weights and overlap weights, both the asymptotic estimator and the bootstrap resulted in accurate estimates of SE across all sample sizes and prevalences of treatment. Even when using the bootstrap with ATE weights, empirical coverage rates of confidence intervals were suboptimal when sample sizes were low to moderate and the prevalence of treatment was either very low or very high. A similar finding was observed when using the bootstrap with ATT weights when sample sizes were low to moderate and the prevalence of treatment was very high.},
  archive      = {J_SIM},
  author       = {Peter C. Austin},
  doi          = {10.1002/sim.9519},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4426-4443},
  shortjournal = {Stat. Med.},
  title        = {Bootstrap vs asymptotic variance estimation when using propensity score weighting with continuous and binary outcomes},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive response-dependent two-phase designs: Some results
on robustness and efficiency. <em>SIM</em>, <em>41</em>(22), 4403–4425.
(<a href="https://doi.org/10.1002/sim.9516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large cohort studies now routinely involve biobanks in which biospecimens are stored for use in future biomarker studies. In such settings, two-phase response-dependent sampling designs involve subsampling individuals in the cohort, assaying their biospecimen to measure an expensive biomarker, and using this data to estimate key parameters of interest under budgetary constraints. When analyses are based on inverse probability weighted estimating functions, recent work has described adaptive two-phase designs in which a preliminary phase of subsampling based on a standard design facilitates approximation of an optimal selection model for a second subsampling phase. In this article, we refine the definition of an optimal subsampling scheme within the framework of adaptive two-phase designs, describe how adaptive two-phase designs can be used when analyses are based on likelihood or conditional likelihood, and consider the setting of a continuous biomarker where the nuisance covariate distribution is estimated nonparametrically at the design stage and analysis stage as required; efficiency and robustness issues are investigated. We also explore these methods for the surrogate variable problem and describe a generalization to accommodate multiple stages of phase II subsampling. A study involving individuals with psoriatic arthritis is considered for illustration, where the aim is to assess the association between the biomarker MMP-3 and the development of joint damage.},
  archive      = {J_SIM},
  author       = {Ce Yang and Liqun Diao and Richard J. Cook},
  doi          = {10.1002/sim.9516},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4403-4425},
  shortjournal = {Stat. Med.},
  title        = {Adaptive response-dependent two-phase designs: Some results on robustness and efficiency},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple imputation approaches for handling incomplete
three-level data with time-varying cluster-memberships. <em>SIM</em>,
<em>41</em>(22), 4385–4402. (<a
href="https://doi.org/10.1002/sim.9515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-level data arising from repeated measures on individuals clustered within higher-level units are common in medical research. A complexity arises when individuals change clusters over time, resulting in a cross-classified data structure. Missing values in these studies are commonly handled via multiple imputation (MI). If the three-level, cross-classified structure is modeled in the analysis, it also needs to be accommodated in the imputation model to ensure valid results. While incomplete three-level data can be handled using various approaches within MI, the performance of these in the cross-classified data setting remains unclear. We conducted simulations under a range of scenarios to compare these approaches in the context of an acute-effects cross-classified random effects substantive model, which models the time-varying cluster membership via simple additive random effects. The simulation study was based on a case study in a longitudinal cohort of students clustered within schools. We evaluated methods that ignore the time-varying cluster memberships by taking the first or most common cluster for each individual; pragmatic extensions of single- and two-level MI approaches within the joint modeling (JM) and the fully conditional specification (FCS) frameworks, using dummy indicators (DI) and/or imputing repeated measures in wide format to account for the cross-classified structure; and a three-level FCS MI approach developed specifically for cross-classified data. Results indicated that the FCS implementations performed well in terms of bias and precision while JM approaches performed poorly. Under both frameworks approaches using the DI extension should be used with caution in the presence of sparse data.},
  archive      = {J_SIM},
  author       = {Rushani Wijesuriya and Margarita Moreno-Betancur and John Carlin and Anurika Priyanjali De Silva and Katherine Jane Lee},
  doi          = {10.1002/sim.9515},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4385-4402},
  shortjournal = {Stat. Med.},
  title        = {Multiple imputation approaches for handling incomplete three-level data with time-varying cluster-memberships},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian local exchangeability design for phase II basket
trials. <em>SIM</em>, <em>41</em>(22), 4367–4384. (<a
href="https://doi.org/10.1002/sim.9514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an information borrowing strategy for the design and monitoring of phase II basket trials based on the local multisource exchangeability assumption between baskets (disease types). In our proposed local-MEM framework, information borrowing is only allowed to occur locally, that is, among baskets with similar response rate and the amount of information borrowing is determined by the level of similarity in response rate, whereas baskets not considered similar are not allowed to share information. We construct a two-stage design for phase II basket trials using the proposed strategy. The proposed method is compared to competing Bayesian methods and Simon&#39;s two-stage design in a variety of simulation scenarios. We demonstrate the proposed method is able to maintain the family-wise type I error rate at a reasonable level and has desirable basket-wise power compared to Simon&#39;s two-stage design. In addition, our method is computationally efficient compared to existing Bayesian methods in that the posterior profiles of interest can be derived explicitly without the need for sampling algorithms. R scripts to implement the proposed method are available at https://github.com/yilinyl/Bayesian-localMEM .},
  archive      = {J_SIM},
  author       = {Yilin Liu and Michael Kane and Denise Esserman and Ondrej Blaha and Daniel Zelterman and Wei Wei},
  doi          = {10.1002/sim.9514},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4367-4384},
  shortjournal = {Stat. Med.},
  title        = {Bayesian local exchangeability design for phase II basket trials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Controlled variable selection in weibull mixture cure models
for high-dimensional data. <em>SIM</em>, <em>41</em>(22), 4340–4366. (<a
href="https://doi.org/10.1002/sim.9513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical breakthroughs in recent years have led to cures for many diseases. The mixture cure model (MCM) is a type of survival model that is often used when a cured fraction exists. Many have sought to identify genomic features associated with a time-to-event outcome which requires variable selection strategies for high-dimensional spaces. Unfortunately, currently few variable selection methods exist for MCMs especially when there are more predictors than samples. This study develops high-dimensional penalized Weibull MCMs, which allow for identification of prognostic factors associated with both cure status and/or survival. We demonstrated how such models may be estimated using two different iterative algorithms. The model-X knockoffs method was combined with these algorithms to control the false discovery rate (FDR) in variable selection. Through extensive simulation studies, our penalized MCMs have been shown to outperform alternative methods on multiple metrics and achieve high statistical power with FDR being controlled. In an acute myeloid leukemia (AML) application with gene expression data, our proposed approach identified 14 genes associated with potential cure and 12 genes with time-to-relapse, which may help inform treatment decisions for AML patients.},
  archive      = {J_SIM},
  author       = {Han Fu and Deedra Nicolet and Krzysztof Mrózek and Richard M. Stone and Ann-Kathrin Eisfeld and John C. Byrd and Kellie J. Archer},
  doi          = {10.1002/sim.9513},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4340-4366},
  shortjournal = {Stat. Med.},
  title        = {Controlled variable selection in weibull mixture cure models for high-dimensional data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of stepped wedge cluster randomized trials in the
presence of a time-varying treatment effect. <em>SIM</em>,
<em>41</em>(22), 4311–4339. (<a
href="https://doi.org/10.1002/sim.9511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped wedge cluster randomized controlled trials are typically analyzed using models that assume the full effect of the treatment is achieved instantaneously. We provide an analytical framework for scenarios in which the treatment effect varies as a function of exposure time (time since the start of treatment) and define the “effect curve” as the magnitude of the treatment effect on the linear predictor scale as a function of exposure time. The “time-averaged treatment effect” (TATE) and “long-term treatment effect” (LTE) are summaries of this curve. We analytically derive the expectation of the estimator resulting from a model that assumes an immediate treatment effect and show that it can be expressed as a weighted sum of the time-specific treatment effects corresponding to the observed exposure times. Surprisingly, although the weights sum to one, some of the weights can be negative. This implies that may be severely misleading and can even converge to a value of the opposite sign of the true TATE or LTE. We describe several models, some of which make assumptions about the shape of the effect curve, that can be used to simultaneously estimate the entire effect curve, the TATE, and the LTE. We evaluate these models in a simulation study to examine the operating characteristics of the resulting estimators and apply them to two real datasets.},
  archive      = {J_SIM},
  author       = {Avi Kenny and Emily C. Voldal and Fan Xia and Patrick J. Heagerty and James P. Hughes},
  doi          = {10.1002/sim.9511},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4311-4339},
  shortjournal = {Stat. Med.},
  title        = {Analysis of stepped wedge cluster randomized trials in the presence of a time-varying treatment effect},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimands for factorial trials. <em>SIM</em>,
<em>41</em>(22), 4299–4310. (<a
href="https://doi.org/10.1002/sim.9510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factorial trials offer an efficient method to evaluate multiple interventions in a single trial, however the use of additional treatments can obscure research objectives, leading to inappropriate analytical methods and interpretation of results. We define a set of estimands for factorial trials, and describe a framework for applying these estimands, with the aim of clarifying trial objectives and ensuring appropriate primary and sensitivity analyses are chosen. This framework is intended for use in factorial trials where the intent is to conduct “two-trials-in-one” (ie, to separately evaluate the effects of treatments A and B), and is comprised of four steps: (i) specifying how additional treatment(s) (eg, treatment B) will be handled in the estimand, and how intercurrent events affecting the additional treatment(s) will be handled; (ii) designating the appropriate factorial estimator as the primary analysis strategy; (iii) evaluating the interaction to assess the plausibility of the assumptions underpinning the factorial estimator; and (iv) performing a sensitivity analysis using an appropriate multiarm estimator to evaluate to what extent departures from the underlying assumption of no interaction may affect results. We show that adjustment for other factors is necessary for noncollapsible effect measures (such as odds ratio), and through a trial re-analysis we find that failure to consider the estimand could lead to inappropriate interpretation of results. We conclude that careful use of the estimands framework clarifies research objectives and reduces the risk of misinterpretation of trial results, and should become a standard part of both the protocol and reporting of factorial trials.},
  archive      = {J_SIM},
  author       = {Brennan C. Kahan and Tim P. Morris and Beatriz Goulão and James Carpenter},
  doi          = {10.1002/sim.9510},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4299-4310},
  shortjournal = {Stat. Med.},
  title        = {Estimands for factorial trials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Augmented weighting estimators for the additive rates model
under multivariate recurrent event data with missing event type.
<em>SIM</em>, <em>41</em>(22), 4285–4298. (<a
href="https://doi.org/10.1002/sim.9509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate recurrent event data are frequently encountered in biomedical and epidemiological studies when subjects experience multiple types of recurrent events. In practice, the event type information may be missing due to a variety of reasons. In this article, we consider a semiparametric additive rates model for multivariate recurrent event data with missing event types. We develop the augmented inverse probability weighting technique to handle event types that are missing at random. The nonparametric kernel-assisted proposals for the missing mechanisms are studied. The resulting estimator is shown to be consistent and asymptotically normal. Extensive simulation studies and a real data application are provided to illustrate the validity and practical utility of the proposed method.},
  archive      = {J_SIM},
  author       = {Huijuan Ma and Weicai Pang and Liuquan Sun and Wei Xu},
  doi          = {10.1002/sim.9509},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4285-4298},
  shortjournal = {Stat. Med.},
  title        = {Augmented weighting estimators for the additive rates model under multivariate recurrent event data with missing event type},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized additive models to analyze nonlinear trends in
biomedical longitudinal data using r: Beyond repeated measures ANOVA and
linear mixed models. <em>SIM</em>, <em>41</em>(21), 4266–4283. (<a
href="https://doi.org/10.1002/sim.9505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical research, the outcome of longitudinal studies has been traditionally analyzed using the repeated measures analysis of variance (rm-ANOVA) or more recently, linear mixed models (LMEMs). Although LMEMs are less restrictive than rm-ANOVA as they can work with unbalanced data and non-constant correlation between observations, both methodologies assume a linear trend in the measured response. It is common in biomedical research that the true trend response is nonlinear and in these cases the linearity assumption of rm-ANOVA and LMEMs can lead to biased estimates and unreliable inference. In contrast, GAMs relax the linearity assumption of rm-ANOVA and LMEMs and allow the data to determine the fit of the model while also permitting incomplete observations and different correlation structures. Therefore, GAMs present an excellent choice to analyze longitudinal data with non-linear trends in the context of biomedical research. This paper summarizes the limitations of rm-ANOVA and LMEMs and uses simulated data to visually show how both methods produce biased estimates when used on data with non-linear trends. We present the basic theory of GAMs and using reported trends of oxygen saturation in tumors, we simulate example longitudinal data (2 treatment groups, 10 subjects per group, 5 repeated measures for each group) to demonstrate their implementation in R. We also show that GAMs are able to produce estimates with non-linear trends even when incomplete observations exist (with 40% of the simulated observations missing). To make this work reproducible, the code and data used in this paper are available at: https://github.com/aimundo/GAMs-biomedical-research .},
  archive      = {J_SIM},
  author       = {Ariel I. Mundo and John R. Tipton and Timothy J. Muldoon},
  doi          = {10.1002/sim.9505},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4266-4283},
  shortjournal = {Stat. Med.},
  title        = {Generalized additive models to analyze nonlinear trends in biomedical longitudinal data using r: Beyond repeated measures ANOVA and linear mixed models},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-based bayesian inference under computer assisted
balance-improving designs. <em>SIM</em>, <em>41</em>(21), 4245–4265. (<a
href="https://doi.org/10.1002/sim.9508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve covariate balance over a complete randomization, a number of methods have been proposed recently to utilize modern computational capabilities to find allocations with balance in observed covariates. Asymptotic inference on treatment effects based on these designs is more complicated than that under complete randomization, and this is why Fisher randomization tests often are suggested. This article suggests model-based Bayesian inference as a general method of inference in these designs, which can deal with complications such as arbitrary covariate balancing criteria and complex estimands. As an illustration, we focus on the case when the outcome is linearly related to the covariates and the estimand of interest is the Sample Average Treatment Effect (SATE). We use a large Monte Carlo simulation to compare the finite sample performance of the model-based Bayesian inference with that of two previous methods which are valid for asymptotic inference of SATE under Mahalanobis distance based rerandomization. We find that for experiments with small to moderate sample sizes, Bayesian inference is to be preferred to the previous methods. As a byproduct, we also find that regression adjustment together with small sample adjusted estimators of standard errors perform better than the previous methods.},
  archive      = {J_SIM},
  author       = {Junni L. Zhang and Per Johansson},
  doi          = {10.1002/sim.9508},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4245-4265},
  shortjournal = {Stat. Med.},
  title        = {Model-based bayesian inference under computer assisted balance-improving designs},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CAPITAL: Optimal subgroup identification via constrained
policy tree search. <em>SIM</em>, <em>41</em>(21), 4227–4244. (<a
href="https://doi.org/10.1002/sim.9507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized medicine, a paradigm of medicine tailored to a patient&#39;s characteristics, is an increasingly attractive field in health care. An important goal of personalized medicine is to identify a subgroup of patients, based on baseline covariates, that benefits more from the targeted treatment than other comparative treatments. Most of the current subgroup identification methods only focus on obtaining a subgroup with an enhanced treatment effect without paying attention to subgroup size. Yet, a clinically meaningful subgroup learning approach should identify the maximum number of patients who can benefit from the better treatment. In this article, we present an optimal subgroup selection rule (SSR) that maximizes the number of selected patients, and in the meantime, achieves the pre-specified clinically meaningful mean outcome, such as the average treatment effect. We derive two equivalent theoretical forms of the optimal SSR based on the contrast function that describes the treatment-covariates interaction in the outcome. We further propose a constrained policy tree search algorithm (CAPITAL) to find the optimal SSR within the interpretable decision tree class. The proposed method is flexible to handle multiple constraints that penalize the inclusion of patients with negative treatment effects, and to address time to event data using the restricted mean survival time as the clinically interesting mean outcome. Extensive simulations, comparison studies, and real data applications are conducted to demonstrate the validity and utility of our method.},
  archive      = {J_SIM},
  author       = {Hengrui Cai and Wenbin Lu and Rachel Marceau West and Devan V. Mehrotra and Lingkang Huang},
  doi          = {10.1002/sim.9507},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4227-4244},
  shortjournal = {Stat. Med.},
  title        = {CAPITAL: Optimal subgroup identification via constrained policy tree search},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using a mixed-effect model with a parameter-space of
heterogenous dimension to evaluate whether accountable care
organizations are associated with greater uniformity across constituent
practices. <em>SIM</em>, <em>41</em>(21), 4215–4226. (<a
href="https://doi.org/10.1002/sim.9506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accountable care organization (ACO) legislation was designed to improve patient outcomes by inducing greater coordination of care and adoption of best practices. Therefore, it is of interest to assess whether greater uniformity occurs among practices comprising an ACO post ACO formation. We develop a mixed-effect model with a difference-in-difference design to evaluate the effect of a patient receiving care from an ACO on patient outcomes and adapt this model to examine whether an ACO is associated with increased uniformity across its constituent practices. The task is complicated by the organizations within an ACO forming an additional layer in the multilevel model, due to medical practices and hospitals that form an ACOs being nested within the ACO, making the number of levels of the model variable and the dimension of the parameter space time-varying. We develop the model and a procedure for testing the hypothesis that ACO formation was associated with increased uniformity among its constituent practices. We apply our procedure to a cohort of medicare beneficiaries followed over 2009-2014. Although there is extensive heterogeneity of becoming an ACOs across practices, we find that the formation of an ACO appears to be associated with greater uniformity of patient outcomes among its constituent practices.},
  archive      = {J_SIM},
  author       = {Guanqing Chen and Valerie A. Lewis and Daniel J. Gottlieb and A. James O&#39;Malley},
  doi          = {10.1002/sim.9506},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4215-4226},
  shortjournal = {Stat. Med.},
  title        = {Using a mixed-effect model with a parameter-space of heterogenous dimension to evaluate whether accountable care organizations are associated with greater uniformity across constituent practices},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating the longitudinal trajectory of cognitive function
measurement using short-term data with different disease stages:
Application in alzheimer’s disease. <em>SIM</em>, <em>41</em>(21),
4200–4214. (<a href="https://doi.org/10.1002/sim.9504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disease (AD) is a chronic neurodegenerative disease characterized by a gradual decline in cognitive function over a few decades. The Mini-Mental State Examination (MMSE) is a widely used measure for evaluating global cognitive functioning. Characterizing the longitudinal trajectory of the MMSE in the population of interest is important to detect AD onset for preventive intervention. In this study, we formulate a new class of longitudinal trajectory modeling for MMSE from short-term individual data based on an ordinary differential equation. The proposed method models the relationship between individual decline speed of MMSE and the average MMSE using the fractional polynomial function model and subsequently estimates the longitudinal trajectory of MMSE by solving the ordinary differential equation for the estimated model. The appropriate model for trajectory estimation is selected based on the proposed criterion for quantifying the goodness of trajectory fit. The accuracy of the trajectory estimation of the proposed method was demonstrated via simulation studies. The proposed method was successfully applied to MMSE data from the Japanese Alzheimer&#39;s Disease Neuroimaging Initiative study.},
  archive      = {J_SIM},
  author       = {Akihiro Hirakawa and Hiroyuki Sato and Ryoichi Hanazawa and Keisuke Suzuki},
  doi          = {10.1002/sim.9504},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4200-4214},
  shortjournal = {Stat. Med.},
  title        = {Estimating the longitudinal trajectory of cognitive function measurement using short-term data with different disease stages: Application in alzheimer&#39;s disease},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trial emulation and survival analysis for disease incidence
registers: A case study on the causal effect of pre-emptive kidney
transplantation. <em>SIM</em>, <em>41</em>(21), 4176–4199. (<a
href="https://doi.org/10.1002/sim.9503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When drawing causal inference from observed data, failure time outcomes present additional challenges of censoring often combined with other missing data patterns. In this article, we follow incident cases of end-stage renal disease to examine the effect on all-cause mortality of starting treatment with transplant, so-called pre-emptive kidney transplantation, vs starting with dialysis possibly followed by delayed transplantation. The question is relatively simple: which start-off treatment is expected to bring the best survival for a target population? To address it, we emulate a target trial drawing on the long term Swedish Renal Registry, where a growing common set of baseline covariates was measured nationwide. Several lessons are learned which pertain to long term disease registers more generally. With characteristics of cases and versions of treatment evolving over time, informative censoring is already introduced in unadjusted Kaplan-Meier curves. This leads to misrepresented survival chances in observed treatment groups. The resulting biased treatment association may be aggravated upon implementing IPW for treatment. Aware of additional challenges, we further recall how similar studies to date have selected patients into treatment groups based on events occurring post treatment initiation. Our study reveals the dramatic impact of resulting immortal time bias combined with other typical features of long-term incident disease registers, including missing covariates during the early phases of the register. We discuss feasible ways of accommodating these features when targeting relevant estimands, and demonstrate how more than one causal question can be answered relying on the no unmeasured baseline confounders assumption.},
  archive      = {J_SIM},
  author       = {Camila Olarte Parra and Ingeborg Waernbaum and Staffan Schön and Els Goetghebeur},
  doi          = {10.1002/sim.9503},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4176-4199},
  shortjournal = {Stat. Med.},
  title        = {Trial emulation and survival analysis for disease incidence registers: A case study on the causal effect of pre-emptive kidney transplantation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic downscaling and daily nowcasting from influenza
surveillance data. <em>SIM</em>, <em>41</em>(21), 4159–4175. (<a
href="https://doi.org/10.1002/sim.9502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time trends from surveillance data are important to assess and develop preparedness for influenza outbreaks. The overwhelming testing demand and limited capacity of testing laboratories for viral positivity render daily confirmed case data inaccurate and delay its availability in preparedness. Using Bayesian dynamic downscaling models, we obtained posterior estimates for daily influenza incidences from weekly estimates of the Centers for Disease Control and Prevention and daily reported constitutional and respiratory complaints during emergency department (ED) visits obtained from the state health departments. Our model provides one-day and seven-day lead forecasts along with 95 prediction intervals. Our hybrid Markov Chain Monte Carlo and Kalman filter algorithms facilitate faster computation and enable us to update our estimates as new data become available. Our method is tested and validated using the State of Michigan data over the years 2009-2013. Reported constitutional and respiratory complaints at the EDs showed strong correlations of 0.81 and 0.68 respectively, with influenza rates. In general, our forecast model can be adapted to track an outbreak with only one respiratory virus as a causative agent.},
  archive      = {J_SIM},
  author       = {Rajib Paul and Dan Han and Elise DeDoncker and Diana Prieto},
  doi          = {10.1002/sim.9502},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4159-4175},
  shortjournal = {Stat. Med.},
  title        = {Dynamic downscaling and daily nowcasting from influenza surveillance data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identification and robust estimation of swapped direct and
indirect effects: Mediation analysis with unmeasured mediator-outcome
confounding and intermediate confounding. <em>SIM</em>, <em>41</em>(21),
4143–4158. (<a href="https://doi.org/10.1002/sim.9501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counterfactual-model-based mediation analysis can yield substantial insight into the causal mechanism through the assessment of natural direct effects (NDEs) and natural indirect effects (NIEs). However, the assumptions regarding unmeasured mediator-outcome confounding and intermediate mediator-outcome confounding that are required for the determination of NDEs and NIEs present practical challenges. To address this problem, we introduce an instrumental blocker, a novel quasi-instrumental variable, to relax both of these assumptions, and we define a swapped direct effect (SDE) and a swapped indirect effect (SIE) to assess the mediation. We show that the SDE and SIE are identical to the NDE and NIE, respectively, based on a causal interpretation. Moreover, the empirical expressions of the SDE and SIE are derived with and without an intermediate mediator-outcome confounder. Then, a multiply robust estimation method is derived to mitigate the model misspecification problem. We prove that the proposed estimator is consistent, asymptotically normal, and achieves the semiparametric efficiency bound. As an illustration, we apply the proposed method to genomic datasets of lung cancer to investigate the potential role of the epidermal growth factor receptor in the treatment of lung cancer.},
  archive      = {J_SIM},
  author       = {An-Shun Tai and Sheng-Hsuan Lin},
  doi          = {10.1002/sim.9501},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4143-4158},
  shortjournal = {Stat. Med.},
  title        = {Identification and robust estimation of swapped direct and indirect effects: Mediation analysis with unmeasured mediator-outcome confounding and intermediate confounding},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical tests for two-stage adaptive seamless design
using short- and long-term binary outcomes. <em>SIM</em>,
<em>41</em>(21), 4130–4142. (<a
href="https://doi.org/10.1002/sim.9500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adaptive seamless design combining phases II and III into a single trial has been shown growing interest for improving the efficiency of drug development, becoming the most frequent adaptive design type. It typically consists of two stages, the trial objectives being often different in each stage. The primary objectives are to select optimal experimental treatment group(s) in the first stage and compare the efficacy between the selected treatment and control groups in the second stage. In this article, we focus on a two-stage adaptive seamless design, for which treatment selection is based on the short-term binary endpoint and treatment comparison is based on the long-term binary endpoint. We thus propose an exact conditional test as a final analysis, based on the bivariate binomial distribution and given the selected treatment with the most promising short-term endpoint response rate from an interim analysis. Additionally, the mid- approach is incorporated to improve conservativeness for an exact test. Simulation studies were conducted to compare the proposed methods with a method based on the combination test. The proposed exact method controlled for type I error rate at the nominal level, regardless of the number of initial treatments or the correlation between short- and long-term endpoints. In terms of the treatment comparison power, the proposed methods are more powerful than that based on the combination test in the scenarios, with only one treatment being effective.},
  archive      = {J_SIM},
  author       = {Kenichi Takahashi and Ryota Ishii and Kazushi Maruo and Masahiko Gosho},
  doi          = {10.1002/sim.9500},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4130-4142},
  shortjournal = {Stat. Med.},
  title        = {Statistical tests for two-stage adaptive seamless design using short- and long-term binary outcomes},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transfer learning in high-dimensional semiparametric
graphical models with application to brain connectivity analysis.
<em>SIM</em>, <em>41</em>(21), 4112–4129. (<a
href="https://doi.org/10.1002/sim.9499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning has drawn growing attention with the target of improving statistical efficiency of one study (dataset) by digging up information from similar and related auxiliary studies (datasets). In this article, we consider transfer learning problem in estimating undirected semiparametric graphical model. We propose an algorithm called Trans-Copula-CLIME for estimating an undirected graphical model while uncovering information from similar auxiliary studies, characterizing the similarity between the target graph and each auxiliary graph by the sparsity of a divergence matrix. The proposed method relaxes the restrictive Gaussian distribution assumption, which deviates from reality for the fMRI dataset related to attention deficit hyperactivity disorder (ADHD) considered here. Nonparametric rank-based correlation coefficient estimators are utilized in the Trans-Copula-CLIME procedure to achieve robustness against normality. We establish the convergence rate of the Trans-Copula-CLIME estimator under some mild conditions, which demonstrates that if the similarity between the auxiliary studies and the target study is sufficiently high and the number of informative auxiliary samples is sufficiently large, the Trans-Copula-CLIME estimator shows great advantage over the existing non-transfer-learning ones. Simulation studies also show that Trans-Copula-CLIME estimator has better performance especially when data are not from Gaussian distribution. Finally, the proposed method is applied to infer functional brain connectivity pattern for ADHD patients in the target Beijing site by leveraging the fMRI datasets from some other sites.},
  archive      = {J_SIM},
  author       = {Yong He and Qiushi Li and Qinqin Hu and Lei Liu},
  doi          = {10.1002/sim.9499},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4112-4129},
  shortjournal = {Stat. Med.},
  title        = {Transfer learning in high-dimensional semiparametric graphical models with application to brain connectivity analysis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On a reparameterization of a flexible family of cure models.
<em>SIM</em>, <em>41</em>(21), 4091–4111. (<a
href="https://doi.org/10.1002/sim.9498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existence of items not susceptible to the event of interest is of both theoretical and practical importance. Although researchers may provide, for example, biological, medical, or sociological evidence for the presence of such items (cured), statistical models performing well under the existence or not of a cured proportion, frequently offer a necessary flexibility. This work introduces a new reparameterization of a flexible family of cure models, which not only includes among its special cases, the most studied cure models (such as the mixture, bounded cumulative hazard, and negative binomial cure model) but also classical survival models (ie, without cured items). One of the main properties of the proposed family, apart from its computationally tractable closed form, is that the case of zero cured proportion is not found at the boundary of the parameter space, as it typically happens to other families. A simulation study examines the (finite) performance of the suggested methodology, focusing to the estimation through EM algorithm and model discrimination, by the aid of the likelihood ratio test and Akaike information criterion; for illustrative purposes, analysis of two real life datasets (on recidivism and cutaneous melanoma) is also carried out.},
  archive      = {J_SIM},
  author       = {Fotios S. Milienos},
  doi          = {10.1002/sim.9498},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4091-4111},
  shortjournal = {Stat. Med.},
  title        = {On a reparameterization of a flexible family of cure models},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Restricted mean survival time regression model with
time-dependent covariates. <em>SIM</em>, <em>41</em>(21), 4081–4090. (<a
href="https://doi.org/10.1002/sim.9495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical or epidemiological follow-up studies, methods based on time scale indicators such as the restricted mean survival time (RMST) have been developed to some extent. Compared with traditional hazard rate indicator system methods, the RMST is easier to interpret and does not require the proportional hazard assumption. To date, regression models based on the RMST are indirect or direct models of the RMST and baseline covariates. However, time-dependent covariates are becoming increasingly common in follow-up studies. Based on the inverse probability of censoring weighting (IPCW) method, we developed a regression model of the RMST and time-dependent covariates. Through Monte Carlo simulation, we verified the estimation performance of the regression parameters of the proposed model. Compared with the time-dependent Cox model and the fixed (baseline) covariate RMST model, the time-dependent RMST model has a better prediction ability. Finally, an example of heart transplantation was used to verify the above conclusions.},
  archive      = {J_SIM},
  author       = {Chengfeng Zhang and Baoyi Huang and Hongji Wu and Hao Yuan and Yawen Hou and Zheng Chen},
  doi          = {10.1002/sim.9495},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4081-4090},
  shortjournal = {Stat. Med.},
  title        = {Restricted mean survival time regression model with time-dependent covariates},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sample size calculation for the andersen-gill model
comparing rates of recurrent events. <em>SIM</em>, <em>41</em>(20),
4079. (<a href="https://doi.org/10.1002/sim.9518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Yongqiang Tang and Ronan Fitzpatrick},
  doi          = {10.1002/sim.9518},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {4079},
  shortjournal = {Stat. Med.},
  title        = {Sample size calculation for the andersen-gill model comparing rates of recurrent events},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Markov neighborhood regression for statistical inference of
high-dimensional generalized linear models. <em>SIM</em>,
<em>41</em>(20), 4057–4078. (<a
href="https://doi.org/10.1002/sim.9493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional inference is one of fundamental problems in modern biomedical studies. However, the existing methods do not perform satisfactorily. Based on the Markov property of graphical models and the likelihood ratio test, this article provides a simple justification for the Markov neighborhood regression method such that it can be applied to statistical inference for high-dimensional generalized linear models with mixed features. The Markov neighborhood regression method is highly attractive in that it breaks the high-dimensional inference problems into a series of low-dimensional inference problems. The proposed method is applied to the cancer cell line encyclopedia data for identification of the genes and mutations that are sensitive to the response of anti-cancer drugs. The numerical results favor the Markov neighborhood regression method to the existing ones.},
  archive      = {J_SIM},
  author       = {Lizhe Sun and Faming Liang},
  doi          = {10.1002/sim.9493},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {4057-4078},
  shortjournal = {Stat. Med.},
  title        = {Markov neighborhood regression for statistical inference of high-dimensional generalized linear models},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep reinforcement learning for personalized treatment
recommendation. <em>SIM</em>, <em>41</em>(20), 4034–4056. (<a
href="https://doi.org/10.1002/sim.9491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In precision medicine, the ultimate goal is to recommend the most effective treatment to an individual patient based on patient-specific molecular and clinical profiles, possibly high-dimensional. To advance cancer treatment, large-scale screenings of cancer cell lines against chemical compounds have been performed to help better understand the relationship between genomic features and drug response; existing machine learning approaches use exclusively supervised learning, including penalized regression and recommender systems. However, it would be more efficient to apply reinforcement learning to sequentially learn as data accrue, including selecting the most promising therapy for a patient given individual molecular and clinical features and then collecting and learning from the corresponding data. In this article, we propose a novel personalized ranking system called Proximal Policy Optimization Ranking (PPORank), which ranks the drugs based on their predicted effects per cell line (or patient) in the framework of deep reinforcement learning (DRL). Modeled as a Markov decision process, the proposed method learns to recommend the most suitable drugs sequentially and continuously over time. As a proof-of-concept, we conduct experiments on two large-scale cancer cell line data sets in addition to simulated data. The results demonstrate that the proposed DRL-based PPORank outperforms the state-of-the-art competitors based on supervised learning. Taken together, we conclude that novel methods in the framework of DRL have great potential for precision medicine and should be further studied.},
  archive      = {J_SIM},
  author       = {Mingyang Liu and Xiaotong Shen and Wei Pan},
  doi          = {10.1002/sim.9491},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {4034-4056},
  shortjournal = {Stat. Med.},
  title        = {Deep reinforcement learning for personalized treatment recommendation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sample size calculation for randomized selection trials with
a time-to-event endpoint and a margin of practical equivalence.
<em>SIM</em>, <em>41</em>(20), 4022–4033. (<a
href="https://doi.org/10.1002/sim.9490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selection trials are used to compare potentially active experimental treatments without a control arm. While sample size calculation methods exist for binary endpoints, no such methods are available for time-to-event endpoints, even though these are ubiquitous in clinical trials. Recent selection trials have begun using progression-free survival as their primary endpoint, but have dichotomized it at a specific time point for sample size calculation and analysis. This changes the clinical question and may reduce power to detect a difference between the arms. In this article, we develop the theory for sample size calculation in selection trials where the time-to-event endpoint is assumed to follow an exponential or Weilbull distribution. We provide a free web application for sample size calculation, as well as an R package, that researchers can use in the design of their studies.},
  archive      = {J_SIM},
  author       = {Hakim-Moulay Dehbi and Andrew Embleton-Thirsk and Zachary Ryan McCaw},
  doi          = {10.1002/sim.9490},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {4022-4033},
  shortjournal = {Stat. Med.},
  title        = {Sample size calculation for randomized selection trials with a time-to-event endpoint and a margin of practical equivalence},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved nonparametric penalized maximum likelihood
estimation for arbitrarily censored survival data. <em>SIM</em>,
<em>41</em>(20), 4006–4021. (<a
href="https://doi.org/10.1002/sim.9489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonparametric maximum likelihood estimation encompasses a group of classic methods to estimate distribution-associated functions from potentially censored and truncated data, with extensive applications in survival analysis. These methods, including the Kaplan-Meier estimator and Turnbull&#39;s method, often result in overfitting, especially when the sample size is small. We propose an improvement to these methods by applying kernel smoothing to their raw estimates, based on a BIC-type loss function that balances the trade-off between optimizing model fit and controlling model complexity. In the context of a longitudinal study with repeated observations, we detail our proposed smoothing procedure and optimization algorithm. With extensive simulation studies over multiple realistic scenarios, we demonstrate that our smoothing-based procedure provides better overall accuracy in both survival function estimation and individual-level time-to-event prediction (imputation) by reducing overfitting. Our smoothing procedure decreases the bias (discrepancy between the estimated and true simulated survival function) using interval-censored data by up to 48% compared to the raw un-smoothed estimate, with similar improvements of up to 34% and 23% in within-sample and out-of-sample prediction, respectively. Our smoothing algorithm also demonstrates significant overall improvement across all three metrics when compared to a popular semiparametric B-splines estimation method. Finally, we apply our method to real data on censored breast cancer diagnosis, which similarly shows improvement when compared to empirical survival estimates from uncensored data. We provide an R package, SISE, for implementing our penalized likelihood method.},
  archive      = {J_SIM},
  author       = {Justin D. Tubbs and Lane G. Chen and Thuan-Quoc Thach and Pak C. Sham},
  doi          = {10.1002/sim.9489},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {4006-4021},
  shortjournal = {Stat. Med.},
  title        = {Improved nonparametric penalized maximum likelihood estimation for arbitrarily censored survival data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian network mediation analysis with application to the
brain functional connectome. <em>SIM</em>, <em>41</em>(20), 3991–4005.
(<a href="https://doi.org/10.1002/sim.9488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain functional connectome, the collection of interconnected neural circuits along functional networks, facilitates a cutting-edge understanding of brain functioning, and has a potential to play a mediating role within the effect pathway between an exposure and an outcome. While existing mediation analytic approaches are capable of providing insight into complex processes, they mainly focus on a univariate mediator or mediator vector, without considering network-variate mediators. To fill the methodological gap and accomplish this exciting and urgent application, in the article, we propose an integrative mediation analysis under a Bayesian paradigm with networks entailing the mediation effect. To parameterize the network measurements, we introduce individually specified stochastic block models with unknown block allocation, and naturally bridge effect elements through the latent network mediators induced by the connectivity weights across network modules. To enable the identification of truly active mediating components, we simultaneously impose a feature selection across network mediators. We show the superiority of our model in estimating different effect components and selecting active mediating network structures. As a practical illustration of this approach&#39;s application to network neuroscience, we characterize the relationship between a therapeutic intervention and opioid abstinence as mediated by brain functional sub-networks.},
  archive      = {J_SIM},
  author       = {Yize Zhao and Tianqi Chen and Jiachen Cai and Sarah Lichenstein and Marc N. Potenza and Sarah W. Yip},
  doi          = {10.1002/sim.9488},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3991-4005},
  shortjournal = {Stat. Med.},
  title        = {Bayesian network mediation analysis with application to the brain functional connectome},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extending the continual reassessment method to accommodate
step-up dosing in phase i trials. <em>SIM</em>, <em>41</em>(20),
3975–3990. (<a href="https://doi.org/10.1002/sim.9487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Continual Reassessment Method (CRM) was developed for Phase I trials to identify a maximum-tolerated dose of an agent using a design in which each participant is treated with a single administration of the agent. We propose an extension of the CRM in which participants receive multiple administrations of an agent using a so-called step-up dosing procedure in which participants receive one or more administrations of lower doses of the agent before they receive their penultimate dose. We use methods developed for the CRM to model the probability of DLT for each administration, which leads to the use of conditional probability models to model the joint probability of DLT across multiple administrations. We compare our approach to two existing methods that use time-to-event modeling methods for modeling the probability of DLT. We demonstrate through simulations that our approach has operating characteristics similar to existing methods, but due to its foundations in the CRM, ours is simpler to implement than existing approaches and is therefore more likely to be adopted in practice.},
  archive      = {J_SIM},
  author       = {Thomas M. Braun and Francois Mercier},
  doi          = {10.1002/sim.9487},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3975-3990},
  shortjournal = {Stat. Med.},
  title        = {Extending the continual reassessment method to accommodate step-up dosing in phase i trials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Net-benefit regression with censored cost-effectiveness data
from randomized or observational studies. <em>SIM</em>, <em>41</em>(20),
3958–3974. (<a href="https://doi.org/10.1002/sim.9486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cost-effectiveness analysis is an essential part of the evaluation of new medical interventions. While in many studies both costs and effectiveness (eg, survival time) are censored, standard survival analysis techniques are often invalid due to the induced dependent censoring problem. We propose methods for censored cost-effectiveness data using the net-benefit regression framework, which allow covariate-adjustment and subgroup identification when comparing two intervention groups. The methods provide a straightforward way to construct cost-effectiveness acceptability curves with censored data. We also propose a more efficient doubly robust estimator of average causal incremental net benefit, which increases the likelihood that the results will represent a valid inference in observational studies. Lastly, we conduct extensive numerical studies to examine the finite-sample performance of the proposed methods, and illustrate the proposed methods with a real data example using both survival time and quality-adjusted survival time as the measures of effectiveness.},
  archive      = {J_SIM},
  author       = {Shuai Chen and Jeffrey S. Hoch},
  doi          = {10.1002/sim.9486},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3958-3974},
  shortjournal = {Stat. Med.},
  title        = {Net-benefit regression with censored cost-effectiveness data from randomized or observational studies},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Competing risks predictions with different time scales under
the additive risk model. <em>SIM</em>, <em>41</em>(20), 3941–3957. (<a
href="https://doi.org/10.1002/sim.9485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the analysis for competing risks data, regression modeling of the cause-specific hazard functions has been usually conducted using the same time scale for all event types. However, when the true time scale is different for each event type, it would be appropriate to specify regression models for the cause-specific hazards on different time scales for different event types. Often, the proportional hazards model has been used for regression modeling of the cause-specific hazard functions. However, the proportionality assumption may not be appropriate in practice. In this article, we consider the additive risk model as an alternative to the proportional hazards model. We propose predictions of the cumulative incidence functions under the cause-specific additive risk models employing different time scales for different event types. We establish the consistency and asymptotic normality of the predicted cumulative incidence functions under the cause-specific additive risk models specified on different time scales using empirical processes and derive consistent variance estimators of the predicted cumulative incidence functions. Through simulation studies, we show that the proposed prediction methods perform well. We illustrate the methods using stage III breast cancer data obtained from the Surveillance, Epidemiology, and End Results (SEER) program of the National Cancer Institute.},
  archive      = {J_SIM},
  author       = {Minjung Lee and Jason P. Fine},
  doi          = {10.1002/sim.9485},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3941-3957},
  shortjournal = {Stat. Med.},
  title        = {Competing risks predictions with different time scales under the additive risk model},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A straightforward meta-analysis approach for oncology phase
i dose-finding studies. <em>SIM</em>, <em>41</em>(20), 3915–3940. (<a
href="https://doi.org/10.1002/sim.9484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase I early-phase clinical studies aim at investigating the safety and the underlying dose-toxicity relationship of a drug or combination. While little may still be known about the compound&#39;s properties, it is crucial to consider quantitative information available from any studies that may have been conducted previously on the same drug. A meta-analytic approach has the advantages of being able to properly account for between-study heterogeneity, and it may be readily extended to prediction or shrinkage applications. Here we propose a simple and robust two-stage approach for the estimation of maximum tolerated dose(s) utilizing penalized logistic regression and Bayesian random-effects meta-analysis methodology. Implementation is facilitated using standard R packages. The properties of the proposed methods are investigated in Monte Carlo simulations. The investigations are motivated and illustrated by two examples from oncology.},
  archive      = {J_SIM},
  author       = {Christian Röver and Moreno Ursino and Tim Friede and Sarah Zohar},
  doi          = {10.1002/sim.9484},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3915-3940},
  shortjournal = {Stat. Med.},
  title        = {A straightforward meta-analysis approach for oncology phase i dose-finding studies},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spike-and-slab least absolute shrinkage and selection
operator generalized additive models and scalable algorithms for
high-dimensional data analysis. <em>SIM</em>, <em>41</em>(20),
3899–3914. (<a href="https://doi.org/10.1002/sim.9483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are proposals that extend the classical generalized additive models (GAMs) to accommodate high-dimensional data ( p ≫ n $$ p\gg n $$ ) using group sparse regularization. However, the sparse regularization may induce excess shrinkage when estimating smooth functions, damaging predictive performance. Moreover, most of these GAMs consider an “all-in-all-out” approach for functional selection, rendering them difficult to answer if nonlinear effects are necessary. While some Bayesian models can address these shortcomings, using Markov chain Monte Carlo algorithms for model fitting creates a new challenge, scalability. Hence, we propose Bayesian hierarchical generalized additive models as a solution: we consider the smoothing penalty for proper shrinkage of curve interpolation via reparameterization. A novel two-part spike-and-slab LASSO prior for smooth functions is developed to address the sparsity of signals while providing extra flexibility to select the linear or nonlinear components of smooth functions. A scalable and deterministic algorithm, EM-Coordinate Descent, is implemented in an open-source R package BHAM. Simulation studies and metabolomics data analyses demonstrate improved predictive and computational performance against state-of-the-art models. Functional selection performance suggests trade-offs exist regarding the effect hierarchy assumption.},
  archive      = {J_SIM},
  author       = {Boyi Guo and Byron C. Jaeger and A. K. M. Fazlur Rahman and D. Leann Long and Nengjun Yi},
  doi          = {10.1002/sim.9483},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3899-3914},
  shortjournal = {Stat. Med.},
  title        = {Spike-and-slab least absolute shrinkage and selection operator generalized additive models and scalable algorithms for high-dimensional data analysis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian nonparametric inference for the overlap
coefficient: With an application to disease diagnosis. <em>SIM</em>,
<em>41</em>(20), 3879–3898. (<a
href="https://doi.org/10.1002/sim.9480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnostic tests play an important role in medical research and clinical practice. The ultimate goal of a diagnostic test is to distinguish between diseased and nondiseased individuals and before a test is routinely used in practice, it is a pivotal requirement that its ability to discriminate between these two states is thoroughly assessed. The overlap coefficient, which is defined as the proportion of overlap area between two probability density functions, has gained popularity as a summary measure of diagnostic accuracy. We propose two Bayesian nonparametric estimators, based on Dirichlet process mixtures, for estimating the overlap coefficient. We further introduce the covariate-specific overlap coefficient and develop a Bayesian nonparametric approach based on Dirichlet process mixtures of additive normal models for estimating it. A simulation study is conducted to assess the empirical performance of our proposed estimators. Two illustrations are provided: one concerned with the search for biomarkers of ovarian cancer and another one aimed to assess the age-specific accuracy of glucose as a biomarker of diabetes.},
  archive      = {J_SIM},
  author       = {Vanda Inácio and Javier E. Garrido Guillén},
  doi          = {10.1002/sim.9480},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3879-3898},
  shortjournal = {Stat. Med.},
  title        = {Bayesian nonparametric inference for the overlap coefficient: With an application to disease diagnosis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using principal stratification in analysis of clinical
trials. <em>SIM</em>, <em>41</em>(19), 3837–3877. (<a
href="https://doi.org/10.1002/sim.9439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ICH E9(R1) addendum (2019) proposed principal stratification (PS) as one of five strategies for dealing with intercurrent events. Therefore, understanding the strengths, limitations, and assumptions of PS is important for the broad community of clinical trialists. Many approaches have been developed under the general framework of PS in different areas of research, including experimental and observational studies. These diverse applications have utilized a diverse set of tools and assumptions. Thus, need exists to present these approaches in a unifying manner. The goal of this tutorial is threefold. First, we provide a coherent and unifying description of PS. Second, we emphasize that estimation of effects within PS relies on strong assumptions and we thoroughly examine the consequences of these assumptions to understand in which situations certain assumptions are reasonable. Finally, we provide an overview of a variety of key methods for PS analysis and use a real clinical trial example to illustrate them. Examples of code for implementation of some of these approaches are given in Supplemental Materials.},
  archive      = {J_SIM},
  author       = {Ilya Lipkovich and Bohdana Ratitch and Yongming Qu and Xiang Zhang and Mingyang Shan and Craig Mallinckrodt},
  doi          = {10.1002/sim.9439},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3837-3877},
  shortjournal = {Stat. Med.},
  title        = {Using principal stratification in analysis of clinical trials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluating effectiveness of public health intervention
strategies for mitigating COVID-19 pandemic. <em>SIM</em>,
<em>41</em>(19), 3820–3836. (<a
href="https://doi.org/10.1002/sim.9482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus disease 2019 (COVID-19) pandemic is an unprecedented global public health challenge. In the United States (US), state governments have implemented various non-pharmaceutical interventions (NPIs), such as physical distance closure (lockdown), stay-at-home order, mandatory facial mask in public in response to the rapid spread of COVID-19. To evaluate the effectiveness of these NPIs, we propose a nested case-control design with propensity score weighting under the quasi-experiment framework to estimate the average intervention effect on disease transmission across states. We further develop a method to test for factors that moderate intervention effect to assist precision public health intervention. Our method takes account of the underlying dynamics of disease transmission and balance state-level pre-intervention characteristics. We prove that our estimator provides causal intervention effect under assumptions. We apply this method to analyze US COVID-19 incidence cases to estimate the effects of six interventions. We show that lockdown has the largest effect on reducing transmission and reopening bars significantly increase transmission. States with a higher percentage of non-White population are at greater risk of increased associated with reopening bars.},
  archive      = {J_SIM},
  author       = {Shanghong Xie and Wenbo Wang and Qinxia Wang and Yuanjia Wang and Donglin Zeng},
  doi          = {10.1002/sim.9482},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3820-3836},
  shortjournal = {Stat. Med.},
  title        = {Evaluating effectiveness of public health intervention strategies for mitigating COVID-19 pandemic},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Similarity of competing risks models with constant
intensities in an application to clinical healthcare pathways involving
prostate cancer surgery. <em>SIM</em>, <em>41</em>(19), 3804–3819. (<a
href="https://doi.org/10.1002/sim.9481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent availability of routine medical data, especially in a university-clinical context, may enable the discovery of typical healthcare pathways, that is, typical temporal sequences of clinical interventions or hospital readmissions. However, such pathways are heterogeneous in a large provider such as a university hospital, and it is important to identify similar care pathways that can still be considered typical pathways. We understand the pathway as a temporal process with possible transitions from a single initial treatment state to hospital readmission of different types, which constitutes a competing risks setting. In this article, we propose a multi-state model-based approach to uncover pathway similarity between two groups of individuals. We describe a new bootstrap procedure for testing the similarity of constant transition intensities from two competing risk models. In a large simulation study, we investigate the performance of our similarity approach with respect to different sample sizes and different similarity thresholds. The studies are motivated by an application from urological clinical routine and we show how the results can be transferred to the application example.},
  archive      = {J_SIM},
  author       = {Nadine Binder and Kathrin Möllenhoff and August Sigle and Holger Dette},
  doi          = {10.1002/sim.9481},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3804-3819},
  shortjournal = {Stat. Med.},
  title        = {Similarity of competing risks models with constant intensities in an application to clinical healthcare pathways involving prostate cancer surgery},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian inference for discretely observed continuous time
multi-state models. <em>SIM</em>, <em>41</em>(19), 3789–3803. (<a
href="https://doi.org/10.1002/sim.9449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-state models are frequently applied to represent processes evolving through a discrete set of states. Important classes of multi-state models arise when transitions between states may depend on the time passed since entry into the current state or on the time elapsed from the start of the process. The former models are called semi-Markov while the latter are known as inhomogeneous Markov models. Inference for both the models presents computational difficulties when the process is only observed at discrete time points with no additional information about the state transitions. In fact, in both the cases, the likelihood function is not available in closed form. To obtain Bayesian inference under these two classes of models, we reconstruct the entire unobserved trajectories conditioned on the observed points via a Metropolis-Hastings algorithm. As proposal density we use that given by the nested Markov models whose conditioned trajectories can easily be drawn with the uniformization technique. The resulting inference is illustrated via simulation studies and the analysis of two benchmark datasets for multi-state models.},
  archive      = {J_SIM},
  author       = {Rosario Barone and Andrea Tancredi},
  doi          = {10.1002/sim.9449},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3789-3803},
  shortjournal = {Stat. Med.},
  title        = {Bayesian inference for discretely observed continuous time multi-state models},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncertainty in lung cancer stage for survival estimation via
set-valued classification. <em>SIM</em>, <em>41</em>(19), 3772–3788. (<a
href="https://doi.org/10.1002/sim.9448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The difficulty in identifying cancer stage in health care claims data has limited oncology quality of care and health outcomes research. We fit prediction algorithms for classifying lung cancer stage into three classes (stages I/II, stage III, and stage IV) using claims data, and then demonstrate a method for incorporating the classification uncertainty in survival estimation. Leveraging set-valued classification and split conformal inference, we show how a fixed algorithm developed in one cohort of data may be deployed in another, while rigorously accounting for uncertainty from the initial classification step. We demonstrate this process using SEER cancer registry data linked with Medicare claims data.},
  archive      = {J_SIM},
  author       = {Savannah Bergquist and Gabriel A. Brooks and Mary Beth Landrum and Nancy L. Keating and Sherri Rose},
  doi          = {10.1002/sim.9448},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3772-3788},
  shortjournal = {Stat. Med.},
  title        = {Uncertainty in lung cancer stage for survival estimation via set-valued classification},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A statistic with demonstrated insensitivity to unmeasured
bias for 2 × 2 × s tables in observational studies. <em>SIM</em>,
<em>41</em>(19), 3758–3771. (<a
href="https://doi.org/10.1002/sim.9446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Are weak associations between a treatment and a binary outcome always sensitive to small unmeasured biases in observational studies? This possibility is often discussed in epidemiology. The familiar Mantel-Haenszel test for a 2 × 2 × S $$ 2\times 2\times S $$ contingency table exaggerates sensitivity to unmeasured biases when the population odds ratios vary among the S $$ S $$ strata. A statistic built from several components, here from the S $$ S $$ strata, is said to have demonstrated insensitivity to bias if it uses only those components that provide indications of insensitivity to bias. Briefly, such a statistic is a d $$ d $$ -statistic. There are 2 S − 1 $$ {2}^S-1 $$ candidate statistics with S $$ S $$ strata, and a d $$ d $$ -statistic considers them all.  To have level α $$ \alpha $$ , a test based on a d $$ d $$ -statistic must pay a price for its double use of the data, but as the sample size increases, that price becomes small, while the gain may be large. The price is paid by conditioning on the limited information used to identify components that are insensitive to a bias of specified magnitude, basing the test result on the information that remains after conditioning. In large samples, the -statistic achieves the largest possible design sensitivity, so it does not exaggerate sensitivity to unmeasured bias. A simulation verifies that the large sample result has traction in samples of practical size. A study of sunlight as a cause of cataract is used to illustrate issues and methods. Several extensions of the method are discussed. An R package dstat2x2xk implements the method.},
  archive      = {J_SIM},
  author       = {Paul R. Rosenbaum},
  doi          = {10.1002/sim.9446},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3758-3771},
  shortjournal = {Stat. Med.},
  title        = {A statistic with demonstrated insensitivity to unmeasured bias for 2 × 2 × s tables in observational studies},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multilevel hybrid principal components analysis for
region-referenced functional electroencephalography data. <em>SIM</em>,
<em>41</em>(19), 3737–3757. (<a
href="https://doi.org/10.1002/sim.9445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography experiments produce region-referenced functional data representing brain signals in the time or the frequency domain collected across the scalp. The data typically also have a multilevel structure with high-dimensional observations collected across multiple experimental conditions or visits. Common analysis approaches reduce the data complexity by collapsing the functional and regional dimensions, where event-related potential (ERP) features or band power are targeted in a pre-specified scalp region. This practice can fail to portray more comprehensive differences in the entire ERP signal or the power spectral density (PSD) across the scalp. Building on the weak separability of the high-dimensional covariance process, the proposed multilevel hybrid principal components analysis (M-HPCA) utilizes dimension reduction tools from both vector and functional principal components analysis to decompose the total variation into between- and within-subject variance. The resulting model components are estimated in a mixed effects modeling framework via a computationally efficient minorization-maximization algorithm coupled with bootstrap. The diverse array of applications of M-HPCA is showcased with two studies of individuals with autism. While ERP responses to match vs mismatch conditions are compared in an audio odd-ball paradigm in the first study, short-term reliability of the PSD across visits is compared in the second. Finite sample properties of the proposed methodology are studied in extensive simulations.},
  archive      = {J_SIM},
  author       = {Emilie Campos and Aaron Wolfe Scheffler and Donatello Telesca and Catherine Sugar and Charlotte DiStefano and Shafali Jeste and April R. Levin and Adam Naples and Sara J. Webb and Frederick Shic and Geraldine Dawson and Susan Faja and James C. McPartland and Damla Şentürk},
  doi          = {10.1002/sim.9445},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3737-3757},
  shortjournal = {Stat. Med.},
  title        = {Multilevel hybrid principal components analysis for region-referenced functional electroencephalography data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Versatile tests for window mean survival time. <em>SIM</em>,
<em>41</em>(19), 3720–3736. (<a
href="https://doi.org/10.1002/sim.9444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Window mean survival time (WMST) evaluates the mean survival between a lower time horizon, τ 0 $$ {\tau}_0 $$ , and an upper time horizon, τ 1 $$ {\tau}_1 $$ . As a flexible extension of restricted mean survival time, specific clinically relevant windows of time can be assessed for survival difference accompanied by a communicable interpretation of estimates and tests. In its original application, WMST required the pre-specification of a window through the selection of appropriate window bounds, τ 0 $$ {\tau}_0 $$ and τ 1 $$ {\tau}_1 $$ . In the instance of severe window misspecification of τ 0 $$ {\tau}_0 $$ and τ 1 $$ {\tau}_1 $$ , the analysis may suffer from low power and a less meaningful interpretation. In this article, we introduce versatile tests whose procedures are based on the simultaneous use of multiple WMST test statistics that are asymptotically normal under the null hypothesis of no difference between two groups. Simulations are performed to examine the power of the tests in moderate sample sizes when the data are uncensored to heavily censored with a ramp-up enrollment period. The survival scenarios chosen for simulation are intended to imitate those which are commonly encountered in oncology, especially in trials involving immunotherapies. Implementation of the procedures is discussed in two real data examples for illustration. Functions for performing versatile WMST tests are provided in the survWMST package in R.},
  archive      = {J_SIM},
  author       = {Mitchell Paukner and Richard Chappell},
  doi          = {10.1002/sim.9444},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3720-3736},
  shortjournal = {Stat. Med.},
  title        = {Versatile tests for window mean survival time},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extending multivariate student’s-t semiparametric mixed
models for longitudinal data with censored responses and heavy tails.
<em>SIM</em>, <em>41</em>(19), 3696–3719. (<a
href="https://doi.org/10.1002/sim.9443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article extends the semiparametric mixed model for longitudinal censored data with Gaussian errors by considering the Student&#39;s t $$ t $$ -distribution. This model allows us to consider a flexible, functional dependence of an outcome variable over the covariates using nonparametric regression. Moreover, the proposed model takes into account the correlation between observations by using random effects. Penalized likelihood equations are applied to derive the maximum likelihood estimates that appear to be robust against outlying observations with respect to the Mahalanobis distance. We estimate nonparametric functions using smoothing splines under an EM-type algorithm framework. Finally, the proposed approach&#39;s performance is evaluated through extensive simulation studies and an application to two datasets from acquired immunodeficiency syndrome clinical trials.},
  archive      = {J_SIM},
  author       = {Thalita B. Mattos and Victor H. Lachos and Luis M. Castro and Larissa A. Matos},
  doi          = {10.1002/sim.9443},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3696-3719},
  shortjournal = {Stat. Med.},
  title        = {Extending multivariate student&#39;s-t semiparametric mixed models for longitudinal data with censored responses and heavy tails},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A classification for complex imbalanced data in disease
screening and early diagnosis. <em>SIM</em>, <em>41</em>(19), 3679–3695.
(<a href="https://doi.org/10.1002/sim.9442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced classification has drawn considerable attention in the statistics and machine learning literature. Typically, traditional classification methods often perform poorly when a severely skewed class distribution is observed, not to mention under a high-dimensional longitudinal data structure. Given the ubiquity of big data in modern health research, it is expected that imbalanced classification in disease diagnosis may encounter an additional level of difficulty that is imposed by such a complex data structure. In this article, we propose a nonparametric classification approach for imbalanced data in longitudinal and high-dimensional settings. Technically, the functional principal component analysis is first applied for feature extraction under the longitudinal structure. The univariate exponential loss function coupled with group LASSO penalty is then adopted into the classification procedure in high-dimensional settings. Along with a good improvement in imbalanced classification, our approach provides a meaningful feature selection for interpretation while enjoying a remarkably lower computational complexity. The proposed method is illustrated on the real data application of Alzheimer&#39;s disease early detection and its empirical performance in finite sample size is extensively evaluated by simulations.},
  archive      = {J_SIM},
  author       = {Yiming Li and Wei-Wen Hsu},
  doi          = {10.1002/sim.9442},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3679-3695},
  shortjournal = {Stat. Med.},
  title        = {A classification for complex imbalanced data in disease screening and early diagnosis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The polytomous discrimination index for prediction involving
multistate processes under intermittent observation. <em>SIM</em>,
<em>41</em>(19), 3661–3678. (<a
href="https://doi.org/10.1002/sim.9441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing importance of predictive modeling in health research comes the need for methods to rigorously assess predictive accuracy. We consider the problem of evaluating the accuracy of predictive models for nominal outcomes when outcome data are coarsened at random. We first consider the problem in the context of a multinomial response modeled by polytomous logistic regression. Attention is then directed to the motivating setting in which class membership corresponds to the state occupied in a multistate disease process at a time horizon of interest. Here, class (state) membership may be unknown at the time horizon since disease processes are under intermittent observation. We propose a novel extension to the polytomous discrimination index to address this and evaluate the predictive accuracy of an intensity-based model in the context of a study involving patients with arthritis from a registry at the University of Toronto Centre for Prognosis Studies in Rheumatic Diseases.},
  archive      = {J_SIM},
  author       = {Shu Jiang and Richard J. Cook},
  doi          = {10.1002/sim.9441},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3661-3678},
  shortjournal = {Stat. Med.},
  title        = {The polytomous discrimination index for prediction involving multistate processes under intermittent observation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multivariate partial linear varying coefficients model for
gene-environment interactions with multiple longitudinal traits.
<em>SIM</em>, <em>41</em>(19), 3643–3660. (<a
href="https://doi.org/10.1002/sim.9440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlated phenotypes often share common genetic determinants. Thus, a multi-trait analysis can potentially increase association power and help in understanding pleiotropic effect. When multiple traits are jointly measured over time, the correlation information between multivariate longitudinal responses can help to gain power in association analysis, and the longitudinal traits can provide insights on the dynamic gene effect over time. In this work, we propose a multivariate partially linear varying coefficients model to identify genetic variants with their effects potentially modified by environmental factors. We derive a testing framework to jointly test the association of genetic factors and illustrated with a bivariate phenotypic trait, while taking the time varying genetic effects into account. We extend the quadratic inference functions to deal with the longitudinal correlations and used penalized splines for the approximation of nonparametric coefficient functions. Theoretical results such as consistency and asymptotic normality of the estimates are established. The performance of the testing procedure is evaluated through Monte Carlo simulation studies. The utility of the method is demonstrated with a real data set from the Twin Study of Hormones and Behavior across the menstrual cycle project, in which single nucleotide polymorphisms associated with emotional eating behavior are identified.},
  archive      = {J_SIM},
  author       = {Honglang Wang and Jingyi Zhang and Kelly L. Klump and Sybil Alexandra Burt and Yuehua Cui},
  doi          = {10.1002/sim.9440},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3643-3660},
  shortjournal = {Stat. Med.},
  title        = {Multivariate partial linear varying coefficients model for gene-environment interactions with multiple longitudinal traits},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The batched stepped wedge design: A design robust to delays
in cluster recruitment. <em>SIM</em>, <em>41</em>(18), 3627–3641. (<a
href="https://doi.org/10.1002/sim.9438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped wedge designs are an increasingly popular variant of longitudinal cluster randomized trial designs, and roll out interventions across clusters in a randomized, but step-wise fashion. In the standard stepped wedge design, assumptions regarding the effect of time on outcomes may require that all clusters start and end trial participation at the same time. This would require ethics approvals and data collection procedures to be in place in all clusters before a stepped wedge trial can start in any cluster. Hence, although stepped wedge designs are useful for testing the impacts of many cluster-based interventions on outcomes, there can be lengthy delays before a trial can commence. In this article, we introduce “batched” stepped wedge designs. Batched stepped wedge designs allow clusters to commence the study in batches, instead of all at once, allowing for staggered cluster recruitment. Like the stepped wedge, the batched stepped wedge rolls out the intervention to all clusters in a randomized and step-wise fashion: a series of self-contained stepped wedge designs. Provided that separate period effects are included for each batch, software for standard stepped wedge sample size calculations can be used. With this time parameterization, in many situations including when linear models are assumed, sample size calculations reduce to the setting of a single stepped wedge design with multiple clusters per sequence. In these situations, sample size calculations will not depend on the delays between the commencement of batches. Hence, the power of batched stepped wedge designs is robust to unexpected delays between batches.},
  archive      = {J_SIM},
  author       = {Jessica Kasza and Rhys Bowden and Richard Hooper and Andrew B. Forbes},
  doi          = {10.1002/sim.9438},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3627-3641},
  shortjournal = {Stat. Med.},
  title        = {The batched stepped wedge design: A design robust to delays in cluster recruitment},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Propensity score methods for observational studies with
clustered data: A review. <em>SIM</em>, <em>41</em>(18), 3612–3626. (<a
href="https://doi.org/10.1002/sim.9437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Propensity score methods are a popular approach to mitigating confounding bias when estimating causal effects in observational studies. When study units are clustered (eg, patients nested within health systems), additional challenges arise such as accounting for unmeasured confounding at multiple levels and dependence between units within the same cluster. While clustered observational data are widely used to draw causal inferences in many fields, including medicine and healthcare, extensions of propensity score methods to clustered settings are still a relatively new area of research. This article presents a framework for estimating causal effects using propensity scores when study units are nested within clusters and are nonrandomly assigned to treatment conditions. We emphasize the need for investigators to examine the nature of the clustering, among other properties, of the observational data at hand in order to guide their choice of causal estimands and the corresponding propensity score approach.},
  archive      = {J_SIM},
  author       = {Ting-Hsuan Chang and Elizabeth A. Stuart},
  doi          = {10.1002/sim.9437},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3612-3626},
  shortjournal = {Stat. Med.},
  title        = {Propensity score methods for observational studies with clustered data: A review},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real time monitoring and prediction of time to endpoint
maturation in clinical trials. <em>SIM</em>, <em>41</em>(18), 3596–3611.
(<a href="https://doi.org/10.1002/sim.9436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials, almost all key milestone dates can be defined in terms of time to endpoint maturation (TTEM). The real time monitoring and accurate prediction of TTEM have a significant impact on clinical trial planning and execution and can bring significant value to clinical trial practitioners. TTEM is defined as the time to achieve or observe a certain number or percentage of some endpoint of interest. It is a combination of time to site initiation, time to subject enrollment after site initiation and time to event of interest after subject enrollment. To better predict TTEM during the trial, the future site initiation and subject enrollment have to be taken into account while predicting the number of events. In this article, we propose a novel simulation-based framework combining time to site initiation, time to subject enrollment and time to event in order to predict TTEM. A nonhomogeneous Poisson process with a quadratic time-varying rate function is used to model site initiation and subject enrollment and more advanced time to event models had been explored and integrated on top of them, such as Weibull, piecewise exponential, and model averaging which is equivalent to a Bayesian model selection strategy. To evaluate the predictive performance of the proposed methodology, we conducted extensive simulations and applied the methodology to 14 randomly selected real oncology phase 2 and phase 3 studies in both solid tumor and hematology with a total 31 study-endpoint combinations. The predictive performance of the proposed methodology was then compared with popular and commonly available commercial software, for example, East (Cytel, Cambridge, MA, USA). From both simulation and real data, the proposed methodology can significantly improve the prediction accuracy by up to 54% compared to the commonly available method.},
  archive      = {J_SIM},
  author       = {Li Wang and Yang Liu and Xiaotian Chen and Erik Pulkstenis},
  doi          = {10.1002/sim.9436},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3596-3611},
  shortjournal = {Stat. Med.},
  title        = {Real time monitoring and prediction of time to endpoint maturation in clinical trials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian model of disease progression in
mucopolysaccaridosis IIIA. <em>SIM</em>, <em>41</em>(18), 3579–3595. (<a
href="https://doi.org/10.1002/sim.9435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mucopolysaccaridosis IIIA (MPS IIIA) is a rare genetic disease that afflicts children and leads to neurocognitive degeneration. We develop a Bayesian disease progression model (DPM) of MPS IIIA that characterizes the pattern of cognitive growth and decline in this disease. The DPM is a repeated measures model that incorporates a nonlinear developmental trajectory and shape-invariant random effects. This approach quantifies the pattern of cognitive development in MPS IIIA and addresses differences in biological age, length of follow-up, and clinical outcomes across natural history subjects. The DPM can be used in clinical trials to estimate the percent slowing in disease progression for treatment relative to natural history. Simulations demonstrate that the DPM provides substantial improvements in power relative to alternative analyses.},
  archive      = {J_SIM},
  author       = {Joe Marion and Juan Ruiz and Benjamin R. Saville},
  doi          = {10.1002/sim.9435},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3579-3595},
  shortjournal = {Stat. Med.},
  title        = {Bayesian model of disease progression in mucopolysaccaridosis IIIA},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-reporting and screening: Data with right-censored,
left-censored, and complete observations. <em>SIM</em>, <em>41</em>(18),
3561–3578. (<a href="https://doi.org/10.1002/sim.9434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider survival data that combine three types of observations: uncensored, right-censored, and left-censored. Such data arises from screening a medical condition, in situations where self-detection arises naturally. Our goal is to estimate the failure-time distribution, based on these three observation types. We propose a novel methodology for distribution estimation using both semiparametric and nonparametric techniques. We then evaluate the performance of these estimators via simulated data. Finally, as a case study, we estimate the patience of patients who arrive at an emergency department and wait for treatment. Three categories of patients are observed: those who leave the system and announce it, and thus their patience time is observed; those who get service and thus their patience time is right-censored by the waiting time; and those who leave the system without announcing it. For this third category, the patients&#39; absence is revealed only when they are called to service, which is after they have already left; formally, their patience time is left-censored. Other applications of our proposed methodology are discussed.},
  archive      = {J_SIM},
  author       = {Jonathan Yefenof and Yair Goldberg and Jennifer Wiler and Avishai Mandelbaum and Ya&#39;acov Ritov},
  doi          = {10.1002/sim.9434},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3561-3578},
  shortjournal = {Stat. Med.},
  title        = {Self-reporting and screening: Data with right-censored, left-censored, and complete observations},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic prediction with time-dependent marker in survival
analysis using supervised functional principal component analysis.
<em>SIM</em>, <em>41</em>(18), 3547–3560. (<a
href="https://doi.org/10.1002/sim.9433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-varying biomarkers reflect important information on disease progression over time. Dynamic prediction for event occurrence on a real-time basis, utilizing time-varying information, is crucial in making accurate clinical decisions. Functional principal component analysis (FPCA) has been widely adopted in the literature for extracting features from time-varying biomarker trajectories. However, feature extraction via FPCA is conducted independent of the time-to-event response, which may not produce optimal results when the goal lies in prediction. With this consideration, we propose a novel supervised FPCA, where the functional principal components are determined to optimize the association between the time-varying biomarker and time-to-event outcome. The proposed framework also accommodates irregularly spaced and sparse longitudinal data. Our method is empirically shown to retain better discrimination and calibration performance than the unsupervised FPCA method in simulation studies. Application of the proposed method is also illustrated in the Alzheimer&#39;s Disease Neuroimaging Initiative database.},
  archive      = {J_SIM},
  author       = {Haolun Shi and Shu Jiang and Jiguo Cao},
  doi          = {10.1002/sim.9433},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3547-3560},
  shortjournal = {Stat. Med.},
  title        = {Dynamic prediction with time-dependent marker in survival analysis using supervised functional principal component analysis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On optimal biomarker cutoffs accounting for
misclassification costs in diagnostic trilemmas with applications to
pancreatic cancer. <em>SIM</em>, <em>41</em>(18), 3527–3546. (<a
href="https://doi.org/10.1002/sim.9432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pancreatic ductal adenocarcinoma (PDAC) is the most deadly cancer and currently there is strong clinical interest in novel biomarkers that contribute to its early detection. Assessing appropriately the accuracy of such biomarkers is a crucial issue and often one needs to take into account that many assays include biospecimens of individuals coming from three groups: healthy, chronic pancreatitis, and PDAC. The ROC surface is an appropriate tool for assessing the overall accuracy of a marker employed under such trichotomous settings. A decision/classification rule is often based on the so-called Youden index and its three-dimensional generalization. However, both the clinical and the statistical literature have not paid the necessary attention to the underlying false classification (FC) rates that are of equal or even greater importance. In this article we provide a framework to make inferences around all classification rates as well as comparisons. We explore the trinormal model, flexible models based on power transformations, and robust non-parametric alternatives. We provide a full framework for the construction of confidence intervals, regions, and spaces for joint inferences or for clinically meaningful points of interest. We further discuss the implications of costs related to different FCs. We evaluate our approaches through extensive simulations and illustrate them using data from a recent PDAC study conducted at the MD Anderson Cancer Center.},
  archive      = {J_SIM},
  author       = {Leonidas E. Bantis and John V. Tsimikas},
  doi          = {10.1002/sim.9432},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3527-3546},
  shortjournal = {Stat. Med.},
  title        = {On optimal biomarker cutoffs accounting for misclassification costs in diagnostic trilemmas with applications to pancreatic cancer},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The impact of methodological choices when developing
predictive models using urinary metabolite data. <em>SIM</em>,
<em>41</em>(18), 3511–3526. (<a
href="https://doi.org/10.1002/sim.9431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continuous evolution of metabolomics over the past two decades has stimulated the search for metabolic biomarkers of many diseases. Metabolomic data measured from urinary samples can provide rich information of the biological events triggered by organ rejection in pediatric kidney transplant recipients. With additional validation, metabolic markers can be used to build clinically useful diagnostic tools. However, there are many methodological steps ranging from data processing to modeling that can influence the performance of the resulting metabolomic classifiers. In this study we focus on the comparison of various classification methods that can handle the complex structure of metabolomic data, including regularized classifiers, partial least squares discriminant analysis, and nonlinear classification models. We also examine the effectiveness of a physiological normalization technique widely used in the clinical and biochemical literature but not extensively analyzed and compared in urine metabolomic studies. While the main objective of this work is to interrogate metabolomic data of pediatric kidney transplant recipients to improve the diagnosis of T cell-mediated rejection (TCMR), we also analyze three independent datasets from other disease conditions to investigate the generalizability of our findings.},
  archive      = {J_SIM},
  author       = {Nikolas Krstic and Kevin Multani and David S. Wishart and Tom Blydt-Hansen and Gabriela V. Cohen Freue},
  doi          = {10.1002/sim.9431},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3511-3526},
  shortjournal = {Stat. Med.},
  title        = {The impact of methodological choices when developing predictive models using urinary metabolite data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Differential expression of single-cell RNA-seq data using
tweedie models. <em>SIM</em>, <em>41</em>(18), 3492–3510. (<a
href="https://doi.org/10.1002/sim.9430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of computational methods and software to identify differentially expressed features in single-cell RNA-sequencing (scRNA-seq) has been shown to be influenced by several factors, including the choice of the normalization method used and the choice of the experimental platform (or library preparation protocol) to profile gene expression in individual cells. Currently, it is up to the practitioner to choose the most appropriate differential expression (DE) method out of over 100 DE tools available to date, each relying on their own assumptions to model scRNA-seq expression features. To model the technological variability in cross-platform scRNA-seq data, here we propose to use Tweedie generalized linear models that can flexibly capture a large dynamic range of observed scRNA-seq expression profiles across experimental platforms induced by platform- and gene-specific statistical properties such as heavy tails, sparsity, and gene expression distributions. We also propose a zero-inflated Tweedie model that allows zero probability mass to exceed a traditional Tweedie distribution to model zero-inflated scRNA-seq data with excessive zero counts. Using both synthetic and published plate- and droplet-based scRNA-seq datasets, we perform a systematic benchmark evaluation of more than 10 representative DE methods and demonstrate that our method (Tweedieverse) outperforms the state-of-the-art DE approaches across experimental platforms in terms of statistical power and false discovery rate control. Our open-source software (R/Bioconductor package) is available at https://github.com/himelmallick/Tweedieverse .},
  archive      = {J_SIM},
  author       = {Himel Mallick and Suvo Chatterjee and Shrabanti Chowdhury and Saptarshi Chatterjee and Ali Rahnavard and Stephanie C. Hicks},
  doi          = {10.1002/sim.9430},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3492-3510},
  shortjournal = {Stat. Med.},
  title        = {Differential expression of single-cell RNA-seq data using tweedie models},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analyzing cohort studies with interval-censored data: A new
model-based linear rank-type test. <em>SIM</em>, <em>41</em>(18),
3479–3491. (<a href="https://doi.org/10.1002/sim.9429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To compare two or more survival distributions with interval-censored data, various nonparametric tests have been proposed. Some are based on the G ρ $$ {G}^{\rho } $$ -family introduced by Harrington and Fleming (1991) that allows flexibility for situations in which the hazard ratio decreases monotonically to unity. However, it is unclear how to choose the appropriate value of the parameter ρ $$ \rho $$ . In this work, we propose a novel linear rank-type test for analyzing interval-censored data that derived from a proportional reversed hazard model. We show its relationship with decreasing hazard ratio. This test statistic provides an alternative to the G ρ $$ {G}^{\rho } $$ -based test statistics by bypassing the choice of the ρ $$ \rho $$ parameter. Simulation results show its good behavior. Two studies on breast cancer and drug users illustrate its practical uses and highlight findings that would have been overlooked if other tests had been used. The test is easy to implement with standard software and can be used for a wide range of situations with interval-censored data to test the equality of survival distributions between two or more independent groups.},
  archive      = {J_SIM},
  author       = {Rodolphe Jantzen and Pascale Tubert-Bitter and Philippe Broët},
  doi          = {10.1002/sim.9429},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3479-3491},
  shortjournal = {Stat. Med.},
  title        = {Analyzing cohort studies with interval-censored data: A new model-based linear rank-type test},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accounting for publication bias using a bivariate trim and
fill meta-analysis procedure. <em>SIM</em>, <em>41</em>(18), 3466–3478.
(<a href="https://doi.org/10.1002/sim.9428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In research synthesis, publication bias (PB) refers to the phenomenon that the publication of a study is associated with the direction and statistical significance of its results. Consequently, it may lead to biased (commonly optimistic) estimates of treatment effects. Visualization tools such as funnel plots have been widely used to investigate PB in univariate meta-analyses. The trim and fill procedure is a nonparametric method to identify and adjust for PB. It is popular among applied scientists due to its simplicity. However, most visualization tools and PB correction methods focus on univariate outcomes. For a meta-analysis with multiple outcomes, the conventional univariate trim and fill method can only account for different outcomes separately and thus may lead to inconsistent conclusions. In this article, we propose a bivariate trim and fill procedure to simultaneously account for PB in the presence of two outcomes that are possibly associated. Based on a recently developed galaxy plot for bivariate meta-analysis, the proposed procedure uses a data-driven imputation algorithm to detect and adjust PB. The method relies on the symmetry of the galaxy plot and assumes that some studies are suppressed based on a linear combination of outcomes. The method projects bivariate outcomes along a particular direction, uses the univariate trim and fill method to estimate the number of trimmed and filled studies, and yields consistent conclusions about PB. The proposed approach is validated using simulated data and is applied to a meta-analysis of the efficacy and safety of antidepressant drugs.},
  archive      = {J_SIM},
  author       = {Chongliang Luo and Arielle Marks-Anglin and Rui Duan and Lifeng Lin and Chuan Hong and Haitao Chu and Yong Chen},
  doi          = {10.1002/sim.9428},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3466-3478},
  shortjournal = {Stat. Med.},
  title        = {Accounting for publication bias using a bivariate trim and fill meta-analysis procedure},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward evaluation of disseminated effects of medications for
opioid use disorder within provider-based clusters using
routinely-collected health data. <em>SIM</em>, <em>41</em>(18),
3449–3465. (<a href="https://doi.org/10.1002/sim.9427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Routinely-collected health data can be employed to emulate a target trial when randomized trial data are not available. Patients within provider-based clusters likely exert and share influence on each other&#39;s treatment preferences and subsequent health outcomes and this is known as dissemination or spillover. Extending a framework to replicate an idealized two-stage randomized trial using routinely-collected health data, an evaluation of disseminated effects within provider-based clusters is possible. In this article, we propose a novel application of causal inference methods for dissemination to retrospective cohort studies in administrative claims data and evaluate the impact of the normality of the random effects distribution for the cluster-level propensity score on estimation of the causal parameters. An extensive simulation study was conducted to study the robustness of the methods under different distributions of the random effects. We applied these methods to evaluate baseline prescription for medications for opioid use disorder among a cohort of patients diagnosed with opioid use disorder and adjust for baseline confounders using information obtained from an administrative claims database. We discuss future research directions in this setting to better address unmeasured confounding in the presence of disseminated effects.},
  archive      = {J_SIM},
  author       = {Ashley Buchanan and Tianyu Sun and Jing Wu and Hilary Aroke and Jeffrey Bratberg and Josiah Rich and Stephen Kogut and Joseph Hogan},
  doi          = {10.1002/sim.9427},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3449-3465},
  shortjournal = {Stat. Med.},
  title        = {Toward evaluation of disseminated effects of medications for opioid use disorder within provider-based clusters using routinely-collected health data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-matched learning to construct treatment decision rules
from electronic health records. <em>SIM</em>, <em>41</em>(17),
3434–3447. (<a href="https://doi.org/10.1002/sim.9426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records (EHRs) collected from large-scale health systems provide rich subject-specific information on a broad patient population at a lower cost compared to randomized controlled trials. Thus, EHRs may serve as a complementary resource to provide real-world data to construct individualized treatment rules (ITRs) and achieve precision medicine. However, in the absence of randomization, inferring treatment rules from EHR data may suffer from unmeasured confounding. In this article, we propose a self-matched learning method inspired by the self-controlled case series (SCCS) design to mitigate this challenge. We alleviate unmeasured time-invariant confounding between patients by matching different periods of treatments within the same patient (self-controlled matching) to infer the optimal ITRs. The proposed method constructs a within-subject matched value function for optimizing ITRs and bears similarity to the SCCS design. We examine assumptions that ensure Fisher consistency, and show that our method requires weaker assumptions on unmeasured confounding than alternative methods. Through extensive simulation studies, we demonstrate that self-matched learning has comparable performance to other existing methods when there are no unmeasured confounders, but performs markedly better when unobserved time-invariant confounders are present, which is often the case for EHRs. Sensitivity analyses show that the proposed method is robust under different scenarios. Finally, we apply self-matched learning to estimate the optimal ITRs from type 2 diabetes patient EHRs, which shows our estimated decision rules lead to greater advantages in reducing patients&#39; diabetes-related complications.},
  archive      = {J_SIM},
  author       = {Tianchen Xu and Yuan Chen and Donglin Zeng and Yuanjia Wang},
  doi          = {10.1002/sim.9426},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3434-3447},
  shortjournal = {Stat. Med.},
  title        = {Self-matched learning to construct treatment decision rules from electronic health records},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling the underlying biological processes in alzheimer’s
disease using a multivariate competing risk joint model. <em>SIM</em>,
<em>41</em>(17), 3421–3433. (<a
href="https://doi.org/10.1002/sim.9425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many clinical trials repeatedly measure several longitudinal outcomes on patients. Patient follow-up can discontinue due to an outcome-dependent event, such as clinical diagnosis, death, or dropout. Joint modeling is a popular choice for the analysis of this type of data. Using example data from a prodromal Alzheimer&#39;s disease trial, we propose a new type of multivariate joint model in which longitudinal brain imaging outcomes and memory impairment ratings are allowed to be associated both with time to open-label medication and dropout, and where the brain imaging outcomes may also directly affect the memory impairment ratings. Existing joint models for multivariate longitudinal outcomes account for the correlation between the longitudinal outcomes through the random effects, often by assuming a multivariate normal distribution. However, for these models, it is difficult to interpret how the longitudinal outcomes affect each other. We model the dependence between the longitudinal outcomes differently so that a first longitudinal outcome affects a second one. Specifically, for each longitudinal outcome, we use a linear mixed-effects model to estimate its trajectory, where, for the second longitudinal outcome, we include the linear predictor of the first outcome as a time-varying covariate. This facilitates an easy and direct interpretation of the association between the longitudinal outcomes and provides a framework for latent mediation analysis to understand the underlying biological processes. For the trial considered here, we found that part of the intervention effect is mediated through hippocampal brain atrophy. The proposed joint models are fitted using a Bayesian framework via MCMC simulation.},
  archive      = {J_SIM},
  author       = {Floor M. van Oudenhoven and Sophie H. N. Swinkels and Tobias Hartmann and Dimitris Rizopoulos},
  doi          = {10.1002/sim.9425},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3421-3433},
  shortjournal = {Stat. Med.},
  title        = {Modeling the underlying biological processes in alzheimer&#39;s disease using a multivariate competing risk joint model},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Penalized weighted proportional hazards model for robust
variable selection and outlier detection. <em>SIM</em>, <em>41</em>(17),
3398–3420. (<a href="https://doi.org/10.1002/sim.9424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying exceptional responders or nonresponders is an area of increased research interest in precision medicine as these patients may have different biological or molecular features and therefore may respond differently to therapies. Our motivation stems from a real example from a clinical trial where we are interested in characterizing exceptional prostate cancer responders. We investigate the outlier detection and robust regression problem in the sparse proportional hazards model for censored survival outcomes. The main idea is to model the irregularity of each observation by assigning an individual weight to the hazard function. By applying a LASSO-type penalty on both the model parameters and the log transformation of the weight vector, our proposed method is able to perform variable selection and outlier detection simultaneously. The optimization problem can be transformed to a typical penalized maximum partial likelihood problem and thus it is easy to implement. We further extend the proposed method to deal with the potential outlier masking problem caused by censored outcomes. The performance of the proposed estimator is demonstrated with extensive simulation studies and real data analyses in low-dimensional and high-dimensional settings.},
  archive      = {J_SIM},
  author       = {Bin Luo and Xiaoli Gao and Susan Halabi},
  doi          = {10.1002/sim.9424},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3398-3420},
  shortjournal = {Stat. Med.},
  title        = {Penalized weighted proportional hazards model for robust variable selection and outlier detection},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Orthogonal array composite designs for drug combination
experiments with applications for tuberculosis. <em>SIM</em>,
<em>41</em>(17), 3380–3397. (<a
href="https://doi.org/10.1002/sim.9423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this article is to provide an overview of the orthogonal array composite design (OACD) methodology, illustrate the various advantages, and provide a real-world application. An OACD combines a two-level factorial design with a three-level orthogonal array and it can be used as an alternative to existing composite designs for building response surface models. We compare the D $$ D $$ -efficiencies of OACDs relative to the commonly used central composite design (CCD) when there are a few missing observations and demonstrate that OACDs are more robust to missing observations for two scenarios. The first scenario assumes one missing observation either from one factorial point or one additional point. The second scenario assumes two missing observations either from two factorial points or from two additional points, or from one factorial point and one additional point. Furthermore, we compare OACDs and CCDs in terms of -optimality for precise predictions. Lastly, a real-world application of an OACD for a tuberculosis drug combination study is provided.},
  archive      = {J_SIM},
  author       = {Jose Luna and Jessica Jaynes and Hongquan Xu and Weng Kee Wong},
  doi          = {10.1002/sim.9423},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3380-3397},
  shortjournal = {Stat. Med.},
  title        = {Orthogonal array composite designs for drug combination experiments with applications for tuberculosis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robust bayesian bias-adjusted random effects model for
consideration of uncertainty about bias terms in evidence synthesis.
<em>SIM</em>, <em>41</em>(17), 3365–3379. (<a
href="https://doi.org/10.1002/sim.9422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analysis is a statistical method used in evidence synthesis for combining, analyzing and summarizing studies that have the same target endpoint and aims to derive a pooled quantitative estimate using fixed and random effects models or network models. Differences among included studies depend on variations in target populations (ie, heterogeneity) and variations in study quality due to study design and execution (ie, bias). The risk of bias is usually assessed qualitatively using critical appraisal, and quantitative bias analysis can be used to evaluate the influence of bias on the quantity of interest. We propose a way to consider ignorance or ambiguity in how to quantify bias terms in a bias analysis by characterizing bias with imprecision (as bounds on probability) and use robust Bayesian analysis to estimate the overall effect. Robust Bayesian analysis is here seen as Bayesian updating performed over a set of coherent probability distributions, where the set emerges from a set of bias terms. We show how the set of bias terms can be specified based on judgments on the relative magnitude of biases (ie, low, unclear, and high risk of bias) in one or several domains of the Cochrane&#39;s risk of bias table. For illustration, we apply a robust Bayesian bias-adjusted random effects model to an already published meta-analysis on the effect of Rituximab for rheumatoid arthritis from the Cochrane Database of Systematic Reviews.},
  archive      = {J_SIM},
  author       = {Ivette Raices Cruz and Matthias C. M. Troffaes and Johan Lindström and Ullrika Sahlin},
  doi          = {10.1002/sim.9422},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3365-3379},
  shortjournal = {Stat. Med.},
  title        = {A robust bayesian bias-adjusted random effects model for consideration of uncertainty about bias terms in evidence synthesis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fixed-effects inference and tests of correlation for
longitudinal functional data. <em>SIM</em>, <em>41</em>(17), 3349–3364.
(<a href="https://doi.org/10.1002/sim.9421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an inferential framework for fixed effects in longitudinal functional models and introduce tests for the correlation structures induced by the longitudinal sampling procedure. The framework provides a natural extension of standard longitudinal correlation models for scalar observations to functional observations. Using simulation studies, we compare fixed effects estimation under correctly and incorrectly specified correlation structures and also test the longitudinal correlation structure. Finally, we apply the proposed methods to a longitudinal functional dataset on physical activity. The computer code for the proposed method is available at https://github.com/rli20ST758/FILF .},
  archive      = {J_SIM},
  author       = {Ruonan Li and Luo Xiao and Ekaterina Smirnova and Erjia Cui and Andrew Leroux and Ciprian M. Crainiceanu},
  doi          = {10.1002/sim.9421},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3349-3364},
  shortjournal = {Stat. Med.},
  title        = {Fixed-effects inference and tests of correlation for longitudinal functional data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Surveillance for endemic infectious disease outbreaks:
Adaptive sampling using profile likelihood estimation. <em>SIM</em>,
<em>41</em>(17), 3336–3348. (<a
href="https://doi.org/10.1002/sim.9420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outbreaks of an endemic infectious disease can occur when the disease is introduced into a highly susceptible subpopulation or when the disease enters a network of connected individuals. For example, significant HIV outbreaks among people who inject drugs have occurred in at least half a dozen US states in recent years. This motivates the current study: how can limited testing resources be allocated across geographic regions to rapidly detect outbreaks of an endemic infectious disease? We develop an adaptive sampling algorithm that uses profile likelihood to estimate the distribution of the number of positive tests that would occur for each location in a future time period if that location were sampled. Sampling is performed in the location with the highest estimated probability of triggering an outbreak alarm in the next time period. The alarm function is determined by a semiparametric likelihood ratio test. We compare the profile likelihood sampling (PLS) method numerically to uniform random sampling (URS) and Thompson sampling (TS). TS was worse than URS when the outbreak occurred in a location with lower initial prevalence than other locations. PLS had lower time to outbreak detection than TS in some but not all scenarios, but was always better than URS even when the outbreak occurred in a location with a lower initial prevalence than other locations. PLS provides an effective and reliable method for rapidly detecting endemic disease outbreaks that is robust to this uncertainty.},
  archive      = {J_SIM},
  author       = {Michael Fairley and Isabelle J. Rao and Margaret L. Brandeau and Gary L. Qian and Gregg S. Gonsalves},
  doi          = {10.1002/sim.9420},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3336-3348},
  shortjournal = {Stat. Med.},
  title        = {Surveillance for endemic infectious disease outbreaks: Adaptive sampling using profile likelihood estimation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Calculating power for the finkelstein and schoenfeld test
statistic for a composite endpoint with two components. <em>SIM</em>,
<em>41</em>(17), 3321–3335. (<a
href="https://doi.org/10.1002/sim.9419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Finkelstein and Schoenfeld (FS) test is a popular generalized pairwise comparison approach to analyze prioritized composite endpoints (eg, components are assessed in order of clinical importance). Power and sample size estimation for the FS test, however, are generally done via simulation studies. This simulation approach can be extremely computationally burdensome, compounded by increasing number of composite endpoints and with increasing sample size. Here we propose an analytical solution to calculate power and sample size for commonly encountered two-component hierarchical composite endpoints. The power formulas are derived assuming underlying distributions in each of the component outcomes on the population level, which provide a computationally efficient and practical alternative to the standard simulation approach. Monte Carlo simulation results demonstrate that performance of the proposed power formulas are consistent with that of the simulation approach, and have generally desirable objective properties including robustness to mis-specified distributional assumptions. We demonstrate the application of the proposed formulas by calculating power and sample size for the Transthyretin Amyloidosis Cardiomyopathy Clinical Trial.},
  archive      = {J_SIM},
  author       = {Thomas J. Zhou and Michael P. LaValley and Kerrie P. Nelson and Howard J. Cabral and Joseph M. Massaro},
  doi          = {10.1002/sim.9419},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3321-3335},
  shortjournal = {Stat. Med.},
  title        = {Calculating power for the finkelstein and schoenfeld test statistic for a composite endpoint with two components},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The number needed to treat adjusted for explanatory
variables in regression and survival analysis: Theory and application.
<em>SIM</em>, <em>41</em>(17), 3299–3320. (<a
href="https://doi.org/10.1002/sim.9418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number needed to treat (NNT) is an efficacy index commonly used in randomized clinical trials. The NNT is the average number of treated patients for each undesirable patient outcome, for example, death, prevented by the treatment. We introduce a systematic theoretically-based framework to model and estimate the conditional and the harmonic mean NNT in the presence of explanatory variables, in various models with dichotomous and nondichotomous outcomes. The conditional NNT is illustrated in a series of four primary examples; logistic regression, linear regression, Kaplan-Meier estimation, and Cox regression models. Also, we establish and prove mathematically the exact relationship between the conditional and the harmonic mean NNT in the presence of explanatory variables. We introduce four different methods to calculate asymptotically-correct confidence intervals for both indices. Finally, we implemented a simulation study to provide numerical demonstrations of the aforementioned theoretical results and the four examples. Numerical analysis showed that the parametric estimators of the NNT with nonparametric bootstrap-based confidence intervals outperformed other examined combinations in most settings. An R package and a web application have been developed and made available online to calculate the conditional and the harmonic mean NNTs with their corresponding confidence intervals.},
  archive      = {J_SIM},
  author       = {Valentin Vancak and Yair Goldberg and Stephen Z. Levine},
  doi          = {10.1002/sim.9418},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3299-3320},
  shortjournal = {Stat. Med.},
  title        = {The number needed to treat adjusted for explanatory variables in regression and survival analysis: Theory and application},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variable selection in semiparametric regression models for
longitudinal data with informative observation times. <em>SIM</em>,
<em>41</em>(17), 3281–3298. (<a
href="https://doi.org/10.1002/sim.9417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common issue in longitudinal studies is that subjects&#39; visits are irregular and may depend on observed outcome values which is known as longitudinal data with informative observation times (follow-up). Semiparametric regression modeling for this type of data has received much attention as it provides more flexibility in studying the association between regression factors and a longitudinal outcome. An important problem here is how to select relevant variables and estimate their coefficients in semiparametric regression models when the number of covariates at baseline is large. The current penalization procedures in semiparametric regression models for longitudinal data do not account for informative observation times. We propose a variable selection procedure that is suitable for the estimation methods based on pseudo-score functions. We investigate the asymptotic properties of penalized estimators and conduct simulation studies to illustrate the theoretical results. We also use the procedure for variable selection in semiparametric regression models for the STAR*D dataset from a multistage randomized clinical trial for treating major depressive disorder.},
  archive      = {J_SIM},
  author       = {Omidali Aghababaei Jazi and Eleanor Pullenayegum},
  doi          = {10.1002/sim.9417},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3281-3298},
  shortjournal = {Stat. Med.},
  title        = {Variable selection in semiparametric regression models for longitudinal data with informative observation times},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Penalized likelihood estimation of a mixture cure cox model
with partly interval censoring—an application to thin melanoma.
<em>SIM</em>, <em>41</em>(17), 3260–3280. (<a
href="https://doi.org/10.1002/sim.9415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-to-event data in medical studies may involve some patients who are cured and will never experience the event of interest. In practice, those cured patients are right censored. However, when data contain a cured fraction, standard survival methods such as Cox proportional hazards models can produce biased results and therefore misleading interpretations. In addition, for some outcomes, the exact time of an event is not known; instead an interval of time in which the event occurred is recorded. This article proposes a new computational approach that can deal with both the cured fraction issues and the interval censoring challenge. To do so, we extend the traditional mixture cure Cox model to accommodate data with partly interval censoring for the observed event times. The traditional method for estimation of the model parameters is based on the expectation-maximization (EM) algorithm, where the log-likelihood is maximized through an indirect complete data log-likelihood function. We propose in this article an alternative algorithm that directly optimizes the log-likelihood function. Extensive Monte Carlo simulations are conducted to demonstrate the performance of the new method over the EM algorithm. The main advantage of the new algorithm is the generation of asymptotic variance matrices for all the estimated parameters. The new method is applied to a thin melanoma dataset to predict melanoma recurrence. Various inferences, including survival and hazard function plots with point-wise confidence intervals, are presented. An R package is now available at Github and will be uploaded to R CRAN.},
  archive      = {J_SIM},
  author       = {Annabel Webb and Jun Ma and Serigne N. Lô},
  doi          = {10.1002/sim.9415},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3260-3280},
  shortjournal = {Stat. Med.},
  title        = {Penalized likelihood estimation of a mixture cure cox model with partly interval censoring—An application to thin melanoma},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust structured heterogeneity analysis approach for
high-dimensional data. <em>SIM</em>, <em>41</em>(17), 3229–3259. (<a
href="https://doi.org/10.1002/sim.9414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Revealing relationships between genes and disease phenotypes is a critical problem in biomedical studies. This problem has been challenged by the heterogeneity of diseases. Patients of a perceived same disease may form multiple subgroups, and different subgroups have distinct sets of important genes. It is hence imperative to discover the latent subgroups and reveal the subgroup-specific important genes. Some heterogeneity analysis methods have been proposed in the recent literature. Despite considerable successes, most of the existing studies are still limited as they cannot accommodate data contamination and ignore the interconnections among genes. Aiming at these shortages, we develop a robust structured heterogeneity analysis approach to identify subgroups, select important genes as well as estimate their effects on the phenotype of interest. Possible data contamination is accommodated by employing the Huber loss function. A sparse overlapping group lasso penalty is imposed to conduct regularization estimation and gene identification, while taking into account the possibly overlapping cluster structure of genes. This approach takes an iterative strategy in the similar spirit of K-means clustering. Simulations demonstrate that the proposed approach outperforms alternatives in revealing the heterogeneity and selecting important genes for each subgroup. The analysis of Cancer Cell Line Encyclopedia data leads to biologically meaningful findings with improved prediction and grouping stability.},
  archive      = {J_SIM},
  author       = {Yifan Sun and Ziye Luo and Xinyan Fan},
  doi          = {10.1002/sim.9414},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3229-3259},
  shortjournal = {Stat. Med.},
  title        = {Robust structured heterogeneity analysis approach for high-dimensional data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Translating questions to estimands in randomized clinical
trials with intercurrent events. <em>SIM</em>, <em>41</em>(16),
3211–3228. (<a href="https://doi.org/10.1002/sim.9398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intercurrent (post-treatment) events occur frequently in randomized trials, and investigators often express interest in treatment effects that suitably take account of these events. Contrasts that naively condition on intercurrent events do not have a straight-forward causal interpretation, and the practical relevance of other commonly used approaches is debated. In this work, we discuss how to formulate and choose an estimand, beyond the marginal intention-to-treat effect, from the point of view of a decision maker and drug developer. In particular, we argue that careful articulation of a practically useful research question should either reflect decision making at this point in time or future drug development. Indeed, a substantially interesting estimand is simply a formalization of the (plain English) description of a research question. A common feature of estimands that are practically useful is that they correspond to possibly hypothetical but well-defined interventions in identifiable (sub)populations. To illustrate our points, we consider five examples that were recently used to motivate consideration of principal stratum estimands in clinical trials. In all of these examples, we propose alternative causal estimands, such as conditional effects, sequential regime effects, and separable effects, that correspond to explicit research questions of substantial interest.},
  archive      = {J_SIM},
  author       = {Mats J. Stensrud and Oliver Dukes},
  doi          = {10.1002/sim.9398},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3211-3228},
  shortjournal = {Stat. Med.},
  title        = {Translating questions to estimands in randomized clinical trials with intercurrent events},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sample size methods for evaluation of predictive biomarkers.
<em>SIM</em>, <em>41</em>(16), 3199–3210. (<a
href="https://doi.org/10.1002/sim.9412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Treatment selection biomarkers are those that can be useful in guiding choice of therapy. Just as new therapies require evaluation in appropriately designed clinical trials to determine their benefit, therapy selection biomarkers require evaluation in appropriately designed studies. These studies may be prospective clinical trials or retrospective studies based on specimens stored from a completed clinical trial. Ideally, patient treatment assignments should be randomized, and consideration should be given to an appropriate sample size—either for prospective planning of a new study or access to a sufficient number of stored specimens. Here, we develop a novel sample size method for estimation of a confidence interval of specified average width, for an intuitively appealing previously proposed parameter that reflects the expected benefit of using biomarker-guided therapy relative to a standard-of-care therapy. The estimation approach combines Monte Carlo and regression to result in a procedure that performs well over a range of scenarios. Although derived under a specific Cox proportional hazards regression model, robustness to model violations is demonstrated by evaluation under accelerated failure time and cure models. The sample size method produces adequate or conservative sample size estimates under a range of scenarios. Computer code in R and C++, and applications for Mac and Windows are made available for implementation of the sample size estimation procedure. The method is applied to a real data setting and results discussed.},
  archive      = {J_SIM},
  author       = {Kevin K. Dobbin and Lisa M. McShane},
  doi          = {10.1002/sim.9412},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3199-3210},
  shortjournal = {Stat. Med.},
  title        = {Sample size methods for evaluation of predictive biomarkers},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distribution-free model selection for longitudinal
zero-inflated count data with missing responses and covariates.
<em>SIM</em>, <em>41</em>(16), 3180–3198. (<a
href="https://doi.org/10.1002/sim.9411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many medical and social science studies, count responses with excess zeros are very common and often the primary outcome of interest. Such count responses are usually generated under some clustered correlation structures due to longitudinal observations of subjects. To model such longitudinal count data with excess zeros, the zero-inflated binomial (ZIB) models for bounded outcomes, and the zero-inflated negative binomial (ZINB) and zero-inflated poisson (ZIP) models for unbounded outcomes all are popular methods. To alleviate the effects of deviations from model assumptions, a semiparametric (or, distribution-free) weighted generalized estimating equations has been proposed to estimate model parameters when data are subject to missingness. In this article, we further explore important covariates for the response variable. Without assumptions on the data distribution, a model selection criterion based on the expected weighted quadratic loss is proposed to select an appropriate subset of covariates, especially when count responses have excess zeros and data are subject to nonmonotone missingness in both responses and covariates. To understand the selection effects of the percentages of excess zeros and missingness, we design various scenarios for covariate selection in the mean model via simulation studies and a real data example regarding the study of cardiovascular disease is also presented for illustration.},
  archive      = {J_SIM},
  author       = {Chun-Shu Chen and Chung-Wei Shen},
  doi          = {10.1002/sim.9411},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3180-3198},
  shortjournal = {Stat. Med.},
  title        = {Distribution-free model selection for longitudinal zero-inflated count data with missing responses and covariates},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Subgroup-specific dose finding for phase i-II trials using
bayesian clustering. <em>SIM</em>, <em>41</em>(16), 3164–3179. (<a
href="https://doi.org/10.1002/sim.9410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most models and algorithms for dose-finding clinical trials, it is assumed that the trial participants are homogeneous—the optimal dose is the same for all those who qualify for the trial. However, if there are heterogeneous populations who may benefit from the same treatment, it is inefficient to conduct dose-finding separately for each group, and assuming homogeneity across all subpopulations may lead to identification of the incorrect dose for some (or all) subgroups. To accommodate heterogeneity in dose-finding trials when both efficacy and toxicity outcomes must be used to identify the optimal dose (as in immunotherapeutic oncology treatments), we utilize an adaptive Bayesian clustering method which borrows strength among similar subgroups and clusters truly homogeneous subgroups. Unlike methodology already described in the literature, our proposed methodology does not require the assumption of exchangeability between subgroups or a priori ordering of subgroups, but does allow for specification of different subgroup-specific priors if prior information is available. We provide a comparison of operating characteristics between our method and Bayesian hierarchical models for subgroups in a variety of relevant scenarios. After simulation studies with four a priori subgroups, we observed that our method and the hierarchical models both outperform separate subgroup-specific models when all subgroups have the same dose-efficacy and dose-toxicity curves. However, our method outperforms hierarchical models when one subgroup has a different dose-efficacy or dose-toxicity curve from the other three subgroups.},
  archive      = {J_SIM},
  author       = {Alexandra Curtis and Brian Smith and Andrew G. Chapple},
  doi          = {10.1002/sim.9410},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3164-3179},
  shortjournal = {Stat. Med.},
  title        = {Subgroup-specific dose finding for phase I-II trials using bayesian clustering},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Weighted McNemar’s test for the comparison of two screening
tests in the presence of verification bias. <em>SIM</em>,
<em>41</em>(16), 3149–3163. (<a
href="https://doi.org/10.1002/sim.9409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical methods have been well-developed for comparing two binary screening tests in the presence of verification bias. However, the complexity of existing methods and the computational difficulty in implementing them have restricted their use. A simple and easily implemented statistical method is therefore needed. In this paper, we propose a weighted McNemar&#39;s test statistic for comparing two sensitivities(specificities). The proposed test statistics are intuitive and simple to compute, only involving some minor modification of a McNemar&#39;s test statistic using the estimated verification probabilities for discordant pairs. Simulations demonstrate that the proposed weighted McNemar&#39;s test statistics preserve type I error as well as or better than the existing statistics. Furthermore, unlike the existing methods, the proposed weighted McNemar&#39;s test statistics can still be applied even when none of the accordant pairs are verified.},
  archive      = {J_SIM},
  author       = {Yougui Wu},
  doi          = {10.1002/sim.9409},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3149-3163},
  shortjournal = {Stat. Med.},
  title        = {Weighted McNemar&#39;s test for the comparison of two screening tests in the presence of verification bias},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian inference for asymptomatic COVID-19 infection
rates. <em>SIM</em>, <em>41</em>(16), 3131–3148. (<a
href="https://doi.org/10.1002/sim.9408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To strengthen inferences meta-analyses are commonly used to summarize information from a set of independent studies. In some cases, though, the data may not satisfy the assumptions underlying the meta-analysis. Using three Bayesian methods that have a more general structure than the common meta-analytic ones, we can show the extent and nature of the pooling that is justified statistically. In this article, we reanalyze data from several reviews whose objective is to make inference about the COVID-19 asymptomatic infection rate. When it is unlikely that all of the true effect sizes come from a single source researchers should be cautious about pooling the data from all of the studies. Our findings and methodology are applicable to other COVID-19 outcome variables, and more generally.},
  archive      = {J_SIM},
  author       = {Dexter Cahoy and Joseph Sedransk},
  doi          = {10.1002/sim.9408},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3131-3148},
  shortjournal = {Stat. Med.},
  title        = {Bayesian inference for asymptomatic COVID-19 infection rates},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Twenty years since joinpoint 1.0: Two major enhancements,
their justification, and impact. <em>SIM</em>, <em>41</em>(16),
3102–3130. (<a href="https://doi.org/10.1002/sim.9407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since its release of Version 1.0 in 1998, Joinpoint software developed for cancer trend analysis by a team at the US National Cancer Institute has received a considerable attention in the trend analysis community and it became one of most widely used software for trend analysis. The paper published in Statistics in Medicine in 2000 (a previous study) describes the permutation test procedure to select the number of joinpoints, and Joinpoint Version 1.0 implemented the permutation procedure as the default model selection method and employed parametric methods for the asymptotic inference of the model parameters. Since then, various updates and extensions have been made in Joinpoint software. In this paper, we review basic features of Joinpoint, summarize important updates of Joinpoint software since its first release in 1998, and provide more information on two major enhancements. More specifically, these enhancements overcome prior limitations in both the accuracy and computational efficiency of previously used methods. The enhancements include: (i) data driven model selection methods which are generally more accurate under a broad range of data settings and more computationally efficient than the permutation test and (ii) the use of the empirical quantile method for construction of confidence intervals for the slope parameters and the location of the joinpoints, which generally provides more accurate coverage than the prior parametric methods used. We show the impact of these changes in cancer trend analysis published by the US National Cancer Institute.},
  archive      = {J_SIM},
  author       = {Hyune-Ju Kim and Huann-Sheng Chen and Jeffrey Byrne and Bill Wheeler and Eric J. Feuer},
  doi          = {10.1002/sim.9407},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3102-3130},
  shortjournal = {Stat. Med.},
  title        = {Twenty years since joinpoint 1.0: Two major enhancements, their justification, and impact},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How large should the next study be? Predictive power and
sample size requirements for replication studies. <em>SIM</em>,
<em>41</em>(16), 3090–3101. (<a
href="https://doi.org/10.1002/sim.9406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We use information derived from over 40K trials in the Cochrane Collaboration database of systematic reviews (CDSR) to compute the replication probability, or predictive power of an experiment given its observed (two-sided) P $$ P $$ -value. We find that an exact replication of a marginally significant result with P = . 05 $$ P=.05 $$ has less than 30% chance of again reaching significance. Moreover, the replication of a result with P = . 005 $$ P=.005 $$ still has only 50% chance of significance. We also compute the probability that the direction (sign) of the estimated effect is correct, which is closely related to the type S error of Gelman and Tuerlinckx. We find that if an estimated effect has P = . 05 $$ P=.05 $$ , there is a 93% probability that its sign is correct. If P = . 005 $$ P=.005 $$ , then that probability is 99%. Finally, we compute the required sample size for a replication study to achieve some specified power conditional on the -value of the original study. We find that the replication of a result with requires a sample size more than 16 times larger than the original study to achieve 80% power, while requires at least 3.5 times larger sample size. These findings confirm that failure to replicate the statistical significance of a trial does not necessarily indicate that the original result was a fluke.},
  archive      = {J_SIM},
  author       = {Erik W. van Zwet and Steven N. Goodman},
  doi          = {10.1002/sim.9406},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3090-3101},
  shortjournal = {Stat. Med.},
  title        = {How large should the next study be? predictive power and sample size requirements for replication studies},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation of vaccine efficacy for variants that emerge
after the placebo group is vaccinated. <em>SIM</em>, <em>41</em>(16),
3076–3089. (<a href="https://doi.org/10.1002/sim.9405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SARS-CoV-2 continues to evolve and the vaccine efficacy against variants is challenging to estimate. It is now common in phase III vaccine trials to provide vaccine to those randomized to placebo once efficacy has been demonstrated, precluding a direct assessment of placebo controlled vaccine efficacy after placebo vaccination. In this work, we extend methods developed for estimating vaccine efficacy post placebo vaccination to allow variant specific time varying vaccine efficacy, where time is measured since vaccination. The key idea is to infer counterfactual strain specific placebo case counts by using surveillance data that provide the proportions of the different strains. This blending of clinical trial and observational data allows estimation of strain-specific time varying vaccine efficacy, or sieve effects, including for strains that emerge after placebo vaccination. The key requirements are that the surveillance strain distribution accurately reflects the strain distribution for a placebo group throughout follow-up after placebo group vaccination, and that at least one strain is present before and after placebo vaccination. For illustration, we develop a Poisson approach for an idealized design under a rare disease assumption and then use a proportional hazards model to address staggered entry, staggered crossover, and smoothly varying strain specific vaccine efficacy. We evaluate these methods by theoretical work and simulations, and demonstrate that useful estimation of the efficacy profile is possible for strains that emerge after vaccination of the placebo group. An important principle is to incorporate sensitivity analyses to guard against misspecification of the strain distribution.},
  archive      = {J_SIM},
  author       = {Dean Follmann and Michael Fay and Craig Magaret},
  doi          = {10.1002/sim.9405},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3076-3089},
  shortjournal = {Stat. Med.},
  title        = {Estimation of vaccine efficacy for variants that emerge after the placebo group is vaccinated},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical multivariate directed acyclic graph
autoregressive models for spatial diseases mapping. <em>SIM</em>,
<em>41</em>(16), 3057–3075. (<a
href="https://doi.org/10.1002/sim.9404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disease mapping is an important statistical tool used by epidemiologists to assess geographic variation in disease rates and identify lurking environmental risk factors from spatial patterns. Such maps rely upon spatial models for regionally aggregated data, where neighboring regions tend to exhibit similar outcomes than those farther apart. We contribute to the literature on multivariate disease mapping, which deals with measurements on multiple (two or more) diseases in each region. We aim to disentangle associations among the multiple diseases from spatial autocorrelation in each disease. We develop multivariate directed acyclic graphical autoregression models to accommodate spatial and inter-disease dependence. The hierarchical construction imparts flexibility and richness, interpretability of spatial autocorrelation and inter-disease relationships, and computational ease, but depends upon the order in which the cancers are modeled. To obviate this, we demonstrate how Bayesian model selection and averaging across orders are easily achieved using bridge sampling. We compare our method with a competitor using simulation studies and present an application to multiple cancer mapping using data from the Surveillance, Epidemiology, and End Results program.},
  archive      = {J_SIM},
  author       = {Leiwen Gao and Abhirup Datta and Sudipto Banerjee},
  doi          = {10.1002/sim.9404},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3057-3075},
  shortjournal = {Stat. Med.},
  title        = {Hierarchical multivariate directed acyclic graph autoregressive models for spatial diseases mapping},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Handling parametric assumptions in principal causal effect
estimation using gaussian mixtures. <em>SIM</em>, <em>41</em>(16),
3039–3056. (<a href="https://doi.org/10.1002/sim.9401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the latent stratum membership, principal stratification models with continuous outcomes naturally fit in the parametric estimation framework of Gaussian mixtures. However, with models that are not nonparametrically identified, relying on parametric mixture modeling has been mostly discouraged as a way of identifying principal effects. This study revisits this rather deserted use of parametric mixture modeling, which may open up various possibilities in principal stratification modeling. The main problem with using the parametric mixture modeling approach is that it is hard to assess the quality of principal effect estimates given its reliance on parametric conditions. As a way of assessing the estimation quality in this situation, this study proposes that we use parametric mixture modeling in two different ways, with and without the assurance of nonparametric identification. The key identifying assumption employed in this study is the moving exclusion restriction, a flexible version of the standard exclusion restriction assumption. This assumption is used as a temporary vehicle to help assess the quality of principal effect estimates obtained relying on parametric mixture modeling. The study presents promising results, showing the possibility of using parametric mixture modeling as an accessible tool for causal inference.},
  archive      = {J_SIM},
  author       = {Booil Jo},
  doi          = {10.1002/sim.9401},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3039-3056},
  shortjournal = {Stat. Med.},
  title        = {Handling parametric assumptions in principal causal effect estimation using gaussian mixtures},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel regression method for the analysis of multireader
multicase-free-response receiver operating characteristics studies.
<em>SIM</em>, <em>41</em>(16), 3022–3038. (<a
href="https://doi.org/10.1002/sim.9400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In diagnostic radiology, the multireader multicase (MRMC) design and the free-response receiver operating characteristics (FROC) method are often used in combination. The cross-correlated data generated by the MRMC-FROC study leads to difficulties in the corresponding analysis, and the need to include covariates in the model further complicates the subsequent analysis. In this paper, we propose a regression approach based on three new measures with good interpretability. The correlation structure of the original test results is taken directly into account in the estimation procedure. The proposed method also allows the inclusion of continuous or discrete covariates. Consistent and asymptotically normal estimators are derived for the new measures. Simulation studies are conducted to evaluate the performance of the proposed approach. The simulation results show that the regression approach performs well under a wide range of scenarios. We also apply the proposed regression approach to a diagnostic study of computer-aided diagnosis in lung cancer.},
  archive      = {J_SIM},
  author       = {Xueqing Liu and Jiarui Sun and Xiao-Hua Zhou},
  doi          = {10.1002/sim.9400},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3022-3038},
  shortjournal = {Stat. Med.},
  title        = {A novel regression method for the analysis of multireader multicase-free-response receiver operating characteristics studies},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regression modeling of restricted mean survival time for
left-truncated right-censored data. <em>SIM</em>, <em>41</em>(16),
3003–3021. (<a href="https://doi.org/10.1002/sim.9399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The restricted mean survival time (RMST) is a clinically meaningful summary measure in studies with survival outcomes. Statistical methods have been developed for regression analysis of RMST to investigate impacts of covariates on RMST, which is a useful alternative to the Cox regression analysis. However, existing methods for regression modeling of RMST are not applicable to left-truncated right-censored data that arise frequently in prevalent cohort studies, for which the sampling bias due to left truncation and informative censoring induced by the prevalent sampling scheme must be properly addressed. The pseudo-observation (PO) approach has been used in regression modeling of RMST for right-censored data and competing-risks data. For left-truncated right-censored data, we propose to directly model RMST as a function of baseline covariates based on POs under general censoring mechanisms. We adjust for the potential covariate-dependent censoring or dependent censoring by the inverse probability of censoring weighting method. We establish large sample properties of the proposed estimators and assess their finite sample performances by simulation studies under various scenarios. We apply the proposed methods to a prevalent cohort of women diagnosed with stage IV breast cancer identified from surveillance, epidemiology, and end results-medicare linked database.},
  archive      = {J_SIM},
  author       = {Rong Rong and Jing Ning and Hong Zhu},
  doi          = {10.1002/sim.9399},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3003-3021},
  shortjournal = {Stat. Med.},
  title        = {Regression modeling of restricted mean survival time for left-truncated right-censored data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A distribution-free procedure for testing versatile
alternative in medical multisample comparison studies. <em>SIM</em>,
<em>41</em>(16), 2978–3002. (<a
href="https://doi.org/10.1002/sim.9397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a test for multisample comparison studies that can be applied without strict assumptions, especially when the underlying population distributions are far from normal. The new test can detect differences not only in location or scale but also in shape parameters among parent population distributions. We are motivated by numerous medical studies, where the variables are not normally distributed and may present in the various groups more complex differences than simple differences in a particular aspect of underlying distributions, such as location or scale. In these situations, traditional ANOVA and Kruskal-Wallis tests are unreliable since the underlying assumptions are not valid. The proposed procedure also allows the researcher to determine which aspects are more responsible for a significant result. This is an important practical advantage over procedures that test for general differences among the distribution functions but cannot identify which aspects lead to significant results. The asymptotic distribution of the test statistic is analyzed along with its small sample behavior against several competing tests. The practical advantages of the proposed procedure are illustrated with a multisample comparison study of a biomarker for liver damage in patients with hepatitis C.},
  archive      = {J_SIM},
  author       = {Amitava Mukherjee and Wolfgang Kössler and Marco Marozzi},
  doi          = {10.1002/sim.9397},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {2978-3002},
  shortjournal = {Stat. Med.},
  title        = {A distribution-free procedure for testing versatile alternative in medical multisample comparison studies},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Utility based approach in individualized optimal dose
selection using machine learning methods. <em>SIM</em>, <em>41</em>(16),
2957–2977. (<a href="https://doi.org/10.1002/sim.9396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal in personalized medicine is to individualize treatment using patient characteristics and improve health outcomes. Selection of optimal dose must balance the effect of dose on both treatment efficacy and toxicity outcomes. We consider a setting with one binary efficacy and one binary toxicity outcome. The goal is to find the optimal dose for each patient using clinical features and biomarkers from available dataset. We propose to use flexible machine learning methods such as random forest and Gaussian process models to build models for efficacy and toxicity depending on dose and biomarkers. A copula is used to model the joint distribution of the two outcomes and the estimates are constrained to have non-decreasing dose-efficacy and dose-toxicity relationships. Numerical utilities are elicited from clinicians for each potential bivariate outcome. For each patient, the optimal dose is chosen to maximize the posterior mean of the utility function. We also propose alternative approaches to optimal dose selection by adding additional toxicity based constraints and an approach taking into account the uncertainty in the estimation of the utility function. The proposed methods are evaluated in a simulation study to compare expected utility outcomes under various estimated optimal dose rules. Gaussian process models tended to have better performance than random forest. Enforcing monotonicity during modeling provided small benefits. Whether and how, correlation between efficacy and toxicity, was modeled, had little effect on performance. The proposed methods are illustrated with a study of patients with liver cancer treated with stereotactic body radiation therapy.},
  archive      = {J_SIM},
  author       = {Pin Li and Jeremy M. G. Taylor and Philip S. Boonstra and Theodore S. Lawrence and Matthew J. Schipper},
  doi          = {10.1002/sim.9396},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {2957-2977},
  shortjournal = {Stat. Med.},
  title        = {Utility based approach in individualized optimal dose selection using machine learning methods},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reclaiming independence in spatial-clustering datasets: A
series of data-driven spatial weights matrices. <em>SIM</em>,
<em>41</em>(15), 2939–2956. (<a
href="https://doi.org/10.1002/sim.9395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most spatial models include a spatial weights matrix ( W ) derived from the first law of geography to adjust the spatial dependence to fulfill the independence assumption. In various fields such as epidemiological and environmental studies, the spatial dependence often shows clustering (or geographic discontinuity) due to natural or social factors. In such cases, adjustment using the first-law-of-geography-based W might be inappropriate and leads to inaccuracy estimations and loss of statistical power. In this work, we propose a series of data-driven W s (DDWs) built following the spatial pattern identified by the scan statistic, which can be easily carried out using existing tools such as SaTScan software. The DDWs take both the clustering (or discontinuous) and the intuitive first-law-of-geographic-based spatial dependence into consideration. Aiming at two common purposes in epidemiology studies (ie, estimating the effect value of explanatory variable X and estimating the risk of each spatial unit in disease mapping), the common spatial autoregressive models and the Leroux-prior-based conditional autoregressive (CAR) models were selected to evaluate performance of DDWs, respectively. Both simulation and case studies show that our DDWs achieve considerably better performance than the classic W in datasets with clustering (or discontinuous) spatial dependence. Furthermore, the latest published density-based spatial clustering models, aiming at dealing with such clustering (or discontinuity) spatial dependence in disease mapping, were also compared as references. The DDWs, incorporated into the CAR models, still show considerable advantage, especially in the datasets for common diseases.},
  archive      = {J_SIM},
  author       = {Wei Wang and Xiong Xiao and Jian Qian and Shiqi Chen and Fang Liao and Fei Yin and Tao Zhang and Xiaosong Li and Yue Ma},
  doi          = {10.1002/sim.9395},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2939-2956},
  shortjournal = {Stat. Med.},
  title        = {Reclaiming independence in spatial-clustering datasets: A series of data-driven spatial weights matrices},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inclusion of unexposed clusters improves the precision of
fixed effects analysis of stepped-wedge cluster randomized trials.
<em>SIM</em>, <em>41</em>(15), 2923–2938. (<a
href="https://doi.org/10.1002/sim.9394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped-wedge cluster randomized trials (SW-CRTs) are typically analyzed using mixed effects models. The fixed effects model is a useful alternative that controls for all time-invariant cluster-level confounders and has proper control of type I error when the number of clusters is small. In principle, all clusters in SW-CRTs are designed to eventually receive the intervention, but in real-world research, some trials can end with unexposed clusters (clusters that never received the intervention), such as when a trial is terminated early based on interim analysis results. Typically, unexposed clusters are expected to contribute no information to the fixed effects intervention effect estimator and are excluded from fixed effects analyses. In this article we mathematically prove that inclusion of unexposed clusters improves the precision of the fixed effects least squares dummy variable (LSDV) intervention effect estimator, re-analyze data from a recent SW-CRT of a novel palliative care intervention containing an unexposed cluster, and evaluate the methods by simulation. We found that including unexposed clusters improves the precision of the fixed effects LSDV intervention effect estimator in both real and simulated datasets. Our simulations also reveal an increase in power and decrease in root mean square error. These improvements are present even if the assumptions of constant residual variance and period effects are violated. In the case that a SW-CRT concludes with unexposed clusters, these unexposed clusters can be included in the fixed effects LSDV analysis to improve precision, power, and root mean square error.},
  archive      = {J_SIM},
  author       = {Kenneth Menglin Lee and Xiangmei Ma and Grace Meijuan Yang and Yin Bun Cheung},
  doi          = {10.1002/sim.9394},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2923-2938},
  shortjournal = {Stat. Med.},
  title        = {Inclusion of unexposed clusters improves the precision of fixed effects analysis of stepped-wedge cluster randomized trials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian sample size determination for diagnostic accuracy
studies. <em>SIM</em>, <em>41</em>(15), 2908–2922. (<a
href="https://doi.org/10.1002/sim.9393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of a new diagnostic test ideally follows a sequence of stages which, among other aims, evaluate technical performance. This includes an analytical validity study, a diagnostic accuracy study, and an interventional clinical utility study. In this article, we propose a novel Bayesian approach to sample size determination for the diagnostic accuracy study, which takes advantage of information available from the analytical validity stage. We utilize assurance to calculate the required sample size based on the target width of a posterior probability interval and can choose to use or disregard the data from the analytical validity study when subsequently inferring measures of test accuracy. Sensitivity analyses are performed to assess the robustness of the proposed sample size to the choice of prior, and prior-data conflict is evaluated by comparing the data to the prior predictive distributions. We illustrate the proposed approach using a motivating real-life application involving a diagnostic test for ventilator associated pneumonia. Finally, we compare the properties of the approach against commonly used alternatives. The results show that, when suitable prior information is available, the assurance-based approach can reduce the required sample size when compared to alternative approaches.},
  archive      = {J_SIM},
  author       = {Kevin J. Wilson and S. Faye Williamson and A. Joy Allen and Cameron J. Williams and Thomas P. Hellyer and B. Clare Lendrem},
  doi          = {10.1002/sim.9393},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2908-2922},
  shortjournal = {Stat. Med.},
  title        = {Bayesian sample size determination for diagnostic accuracy studies},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning for the dynamic prediction of multivariate
longitudinal and survival data. <em>SIM</em>, <em>41</em>(15),
2894–2907. (<a href="https://doi.org/10.1002/sim.9392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The joint model for longitudinal and survival data improves time-to-event predictions by including longitudinal outcome variables in addition to baseline covariates. However, in practice, joint models may be limited by parametric assumptions in both the longitudinal and survival submodels. In addition, computational difficulties arise when considering multiple longitudinal outcomes due to the large number of random effects to be integrated out in the full likelihood. In this article, we discuss several recent machine learning methods for incorporating multivariate longitudinal data for time-to-event prediction. The presented methods use functional data analysis or convolutional neural networks to model the longitudinal data, both of which scale well to multiple longitudinal outcomes. In addition, we propose a novel architecture based on the transformer neural network, named TransformerJM, which jointly models longitudinal and time-to-event data. The prognostic abilities of each model are assessed and compared through both simulation and real data analysis on Alzheimer&#39;s disease datasets. Specifically, the models were evaluated based on their ability to dynamically update predictions as new longitudinal data becomes available. We showed that TransformerJM improves upon the predictive performance of existing methods across different scenarios.},
  archive      = {J_SIM},
  author       = {Jeffrey Lin and Sheng Luo},
  doi          = {10.1002/sim.9392},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2894-2907},
  shortjournal = {Stat. Med.},
  title        = {Deep learning for the dynamic prediction of multivariate longitudinal and survival data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient estimation of indirect effects in case-control
studies using a unified likelihood framework. <em>SIM</em>,
<em>41</em>(15), 2879–2893. (<a
href="https://doi.org/10.1002/sim.9390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation models are a set of statistical techniques that investigate the mechanisms that produce an observed relationship between an exposure variable and an outcome variable in order to deduce the extent to which the relationship is influenced by intermediate mediator variables. For a case-control study, the most common mediation analysis strategy employs a counterfactual framework that permits estimation of indirect and direct effects on the odds ratio scale for dichotomous outcomes, assuming either binary or continuous mediators. While this framework has become an important tool for mediation analysis, we demonstrate that we can embed this approach in a unified likelihood framework for mediation analysis in case-control studies that leverages more features of the data (in particular, the relationship between exposure and mediator) to improve efficiency of indirect effect estimates. One important feature of our likelihood approach is that it naturally incorporates cases within the exposure-mediator model to improve efficiency. Our approach does not require knowledge of disease prevalence and can model confounders and exposure-mediator interactions, and is straightforward to implement in standard statistical software. We illustrate our approach using both simulated data and real data from a case-control genetic study of lung cancer.},
  archive      = {J_SIM},
  author       = {Glen A. Satten and Sarah W. Curtis and Claudia Solis-Lemus and Elizabeth J. Leslie and Michael P. Epstein},
  doi          = {10.1002/sim.9390},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2879-2893},
  shortjournal = {Stat. Med.},
  title        = {Efficient estimation of indirect effects in case-control studies using a unified likelihood framework},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Omnibus testing approach for gene-based gene-gene
interaction. <em>SIM</em>, <em>41</em>(15), 2854–2878. (<a
href="https://doi.org/10.1002/sim.9389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genetic interaction is considered as one of the main heritable component of complex traits. With the emergence of genome-wide association studies (GWAS), a collection of statistical methods dedicated to the identification of interaction at the SNP level have been proposed. More recently, gene-based gene-gene interaction testing has emerged as an attractive alternative as they confer advantage in both statistical power and biological interpretation. Most of the gene-based interaction methods rely on a multidimensional modeling of the interaction, thus facing a lack of robustness against the huge space of interaction patterns. In this paper, we study a global testing approaches to address the issue of gene-based gene-gene interaction. Based on a logistic regression modeling framework, all SNP-SNP interaction tests are combined to produce a gene-level test for interaction. We propose an omnibus test that takes advantage of (1) the heterogeneity between existing global tests and (2) the complementarity between allele-based and genotype-based coding of SNPs. Through an extensive simulation study, it is demonstrated that the proposed omnibus test has the ability to detect with high power the most common interaction genetic models with one causal pair as well as more complex genetic models where more than one causal pair is involved. On the other hand, the flexibility of the proposed approach is shown to be robust and improves power compared to single global tests in replication studies. Furthermore, the application of our procedure to real datasets confirms the adaptability of our approach to replicate various gene-gene interactions.},
  archive      = {J_SIM},
  author       = {Florian Hébert and David Causeur and Mathieu Emily},
  doi          = {10.1002/sim.9389},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2854-2878},
  shortjournal = {Stat. Med.},
  title        = {Omnibus testing approach for gene-based gene-gene interaction},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving large-scale estimation and inference for profiling
health care providers. <em>SIM</em>, <em>41</em>(15), 2840–2853. (<a
href="https://doi.org/10.1002/sim.9387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provider profiling has been recognized as a useful tool in monitoring health care quality, facilitating inter-provider care coordination, and improving medical cost-effectiveness. Existing methods often use generalized linear models with fixed provider effects, especially when profiling dialysis facilities. As the number of providers under evaluation escalates, the computational burden becomes formidable even for specially designed workstations. To address this challenge, we introduce a serial blockwise inversion Newton algorithm exploiting the block structure of the information matrix. A shared-memory divide-and-conquer algorithm is proposed to further boost computational efficiency. In addition to the computational challenge, the current literature lacks an appropriate inferential approach to detecting providers with outlying performance especially when small providers with extreme outcomes are present. In this context, traditional score and Wald tests relying on large-sample distributions of the test statistics lead to inaccurate approximations of the small-sample properties. In light of the inferential issue, we develop an exact test of provider effects using exact finite-sample distributions, with the Poisson-binomial distribution as a special case when the outcome is binary. Simulation analyses demonstrate improved estimation and inference over existing methods. The proposed methods are applied to profiling dialysis facilities based on emergency department encounters using a dialysis patient database from the Centers for Medicare &amp; Medicaid Services.},
  archive      = {J_SIM},
  author       = {Wenbo Wu and Yuan Yang and Jian Kang and Kevin He},
  doi          = {10.1002/sim.9387},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2840-2853},
  shortjournal = {Stat. Med.},
  title        = {Improving large-scale estimation and inference for profiling health care providers},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiply robust subgroup analysis based on a single-index
threshold linear marginal model for longitudinal data with dropouts.
<em>SIM</em>, <em>41</em>(15), 2822–2839. (<a
href="https://doi.org/10.1002/sim.9386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying subpopulations that may be sensitive to the specific treatment is an important step toward precision medicine. On the other hand, longitudinal data with dropouts is common in medical research, and subgroup analysis for this data type is still limited. In this paper, we consider a single-index threshold linear marginal model, which can be used simultaneously to identify subgroups with differential treatment effects based on linear combination of the selected biomarkers, estimate the treatment effects in different subgroups based on regression coefficients, and test the significance of the difference in treatment effects based on treatment-subgroup interaction. The regression parameters are estimated by solving a penalized smoothed generalized estimating equation and the selection bias caused by missingness is corrected by a multiply robust weighting matrix, which allows multiple missingness models to be taken account into estimation. The proposed estimator remains consistent when any model for missingness is correctly specified. Under regularity conditions, the asymptotic normality of the estimator is established. Simulation studies confirm the desirable finite-sample performance of the proposed method. As an application, we analyze the data from a clinical trial on pancreatic cancer.},
  archive      = {J_SIM},
  author       = {Kecheng Wei and Huichen Zhu and Guoyou Qin and Zhongyi Zhu and Dongsheng Tu},
  doi          = {10.1002/sim.9386},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2822-2839},
  shortjournal = {Stat. Med.},
  title        = {Multiply robust subgroup analysis based on a single-index threshold linear marginal model for longitudinal data with dropouts},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Jointly modeling of sleep variables that are objectively
measured by wrist actigraphy. <em>SIM</em>, <em>41</em>(15), 2804–2821.
(<a href="https://doi.org/10.1002/sim.9385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently developed actigraphy devices have made it possible for continuous and objective monitoring of sleep over multiple nights. Sleep variables captured by wrist actigraphy devices include sleep onset, sleep end, total sleep time, wake time after sleep onset, number of awakenings, etc. Currently available statistical methods to analyze such actigraphy data have limitations. First, averages over multiple nights are used to summarize sleep activities, ignoring variability over multiple nights from the same subject. Second, sleep variables are often analyzed independently. However, sleep variables tend to be correlated with each other. For example, how long a subject sleeps at night can be correlated with how long and how frequent he/she wakes up during that night. It is important to understand these inter-relationships. We therefore propose a joint mixed effect model on total sleep time, number of awakenings, and wake time. We develop an estimating procedure based upon a sequence of generalized linear mixed effects models, which can be implemented using existing software. The use of these models not only avoids computational intensity and instability that may occur by directly applying a numerical algorithm on a complicated joint likelihood function, but also provides additional insights on sleep activities. We demonstrated in simulation studies that the proposed estimating procedure performed well in estimating both fixed and random effects&#39; parameters. We applied the proposed model to data from the Women&#39;s Interagency HIV Sleep Study to examine the association of employment status and age with overall sleep quality assessed by several actigraphy measured sleep variables.},
  archive      = {J_SIM},
  author       = {Xiaonan Xue and Simin Hua and Kathleen Weber and Qibin Qi and Robert Kaplan and Deborah R. Gustafson and Anjali Sharma and Audrey French and Helen J. Burgess},
  doi          = {10.1002/sim.9385},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2804-2821},
  shortjournal = {Stat. Med.},
  title        = {Jointly modeling of sleep variables that are objectively measured by wrist actigraphy},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Negative binomial factor regression with application to
microbiome data analysis. <em>SIM</em>, <em>41</em>(15), 2786–2803. (<a
href="https://doi.org/10.1002/sim.9384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human microbiome provides essential physiological functions and helps maintain host homeostasis via the formation of intricate ecological host-microbiome relationships. While it is well established that the lifestyle of the host, dietary preferences, demographic background, and health status can influence microbial community composition and dynamics, robust generalizable associations between specific host-associated factors and specific microbial taxa have remained largely elusive. Here, we propose factor regression models that allow the estimation of structured parsimonious associations between host-related features and amplicon-derived microbial taxa. To account for the overdispersed nature of the amplicon sequencing count data, we propose negative binomial reduced rank regression (NB-RRR) and negative binomial co-sparse factor regression (NB-FAR). While NB-RRR encodes the underlying dependency among the microbial abundances as outcomes and the host-associated features as predictors through a rank-constrained coefficient matrix, NB-FAR uses a sparse singular value decomposition of the coefficient matrix. The latter approach avoids the notoriously difficult joint parameter estimation by extracting sparse unit-rank components of the coefficient matrix sequentially, effectively delivering interpretable bi-clusters of taxa and host-associated factors. To solve the nonconvex optimization problems associated with these factor regression models, we present a novel iterative block-wise majorization procedure. Extensive simulation studies and an application to the microbial abundance data from the American Gut Project (AGP) demonstrate the efficacy of the proposed procedure. In the AGP data, we identify several factors that strongly link dietary habits and host life style to specific microbial families.},
  archive      = {J_SIM},
  author       = {Aditya K. Mishra and Christian L. Müller},
  doi          = {10.1002/sim.9384},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2786-2803},
  shortjournal = {Stat. Med.},
  title        = {Negative binomial factor regression with application to microbiome data analysis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixed-type multivariate response regression with covariance
estimation. <em>SIM</em>, <em>41</em>(15), 2768–2785. (<a
href="https://doi.org/10.1002/sim.9383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new method for multivariate response regression and covariance estimation when elements of the response vector are of mixed types, for example some continuous and some discrete. Our method is based on a model which assumes the observable mixed-type response vector is connected to a latent multivariate normal response linear regression through a link function. We explore the properties of this model and show its parameters are identifiable under reasonable conditions. We impose no parametric restrictions on the covariance of the latent normal other than positive definiteness, thereby avoiding assumptions about unobservable variables which can be difficult to verify in practice. To accommodate this generality, we propose a novel algorithm for approximate maximum likelihood estimation that works “off-the-shelf” with many different combinations of response types, and which scales well in the dimension of the response vector. Our method typically gives better predictions and parameter estimates than fitting separate models for the different response types and allows for approximate likelihood ratio testing of relevant hypotheses such as independence of responses. The usefulness of the proposed method is illustrated in simulations; and one biomedical and one genomic data example.},
  archive      = {J_SIM},
  author       = {Karl Oskar Ekvall and Aaron J. Molstad},
  doi          = {10.1002/sim.9383},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2768-2785},
  shortjournal = {Stat. Med.},
  title        = {Mixed-type multivariate response regression with covariance estimation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tracking the transmission dynamics of COVID-19 with a
time-varying coefficient state-space model. <em>SIM</em>,
<em>41</em>(15), 2745–2767. (<a
href="https://doi.org/10.1002/sim.9382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spread of COVID-19 has been greatly impacted by regulatory policies and behavior patterns that vary across counties, states, and countries. Population-level dynamics of COVID-19 can generally be described using a set of ordinary differential equations, but these deterministic equations are insufficient for modeling the observed case rates, which can vary due to local testing and case reporting policies and nonhomogeneous behavior among individuals. To assess the impact of population mobility on the spread of COVID-19, we have developed a novel Bayesian time-varying coefficient state-space model for infectious disease transmission. The foundation of this model is a time-varying coefficient compartment model to recapitulate the dynamics among susceptible, exposed, undetected infectious, detected infectious, undetected removed, hospitalized, detected recovered, and detected deceased individuals. The infectiousness and detection parameters are modeled to vary by time, and the infectiousness component in the model incorporates information on multiple sources of population mobility. Along with this compartment model, a multiplicative process model is introduced to allow for deviation from the deterministic dynamics. We apply this model to observed COVID-19 cases and deaths in several U.S. states and Colorado counties. We find that population mobility measures are highly correlated with transmission rates and can explain complicated temporal variation in infectiousness in these regions. Additionally, the inferred connections between mobility and epidemiological parameters, varying across locations, have revealed the heterogeneous effects of different policies on the dynamics of COVID-19.},
  archive      = {J_SIM},
  author       = {Joshua P. Keller and Tianjian Zhou and Andee Kaplan and G. Brooke Anderson and Wen Zhou},
  doi          = {10.1002/sim.9382},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2745-2767},
  shortjournal = {Stat. Med.},
  title        = {Tracking the transmission dynamics of COVID-19 with a time-varying coefficient state-space model},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A generalized weighted combination test of treatment effect
for clinical trials with a sequential parallel comparison design and
binary endpoint. <em>SIM</em>, <em>41</em>(15), 2725–2744. (<a
href="https://doi.org/10.1002/sim.9381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the issue of a large placebo effect in certain therapeutic areas, rather than the application of the traditional gold standard parallel group placebo-controlled design, different versions of the sequential parallel comparison design have been advocated. In general, the design consists of two consecutive stages and three treatment groups. Stage 1 placebo nonresponders potentially form a prespecified patient subgroup for formal between-treatment comparison at the final analysis. In this research, a version of the design is considered for a binary endpoint. To fully utilize all available data, a generalized weighted combination test is proposed in case placebo has a relatively small effect for some of the study endpoints. The weighted combination of the test based on stage 1 data and the test based on stage 2 data of stage 1 placebo nonresponders suggested in the literature uses only a part of the study data and is a special case of this generalized weighted combination test. A multiple imputation approach is outlined for handling missing not at random data. Simulation is conducted to evaluate the performances of the methods and a data example is employed to illustrate the applications of the methods.},
  archive      = {J_SIM},
  author       = {Hui Quan and Xiaofei Chen and Junxiang Luo and Xun Chen},
  doi          = {10.1002/sim.9381},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2725-2744},
  shortjournal = {Stat. Med.},
  title        = {A generalized weighted combination test of treatment effect for clinical trials with a sequential parallel comparison design and binary endpoint},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric multiple inflation count model with
application to a smoking cessation study. <em>SIM</em>, <em>41</em>(15),
2711–2724. (<a href="https://doi.org/10.1002/sim.9380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Count data are observed by practitioners across various fields. Often, a substantially large proportion of one or some values causes extra variation and may lead to a particular case of mixed structured data. In these cases, a standard count model may lead to poor inference of the parameters involved because of its inability to account for extra variation. Furthermore, we hypothesize a possible nonlinear relationship of a continuous covariate with the logarithm of the mean count and with the probability of belonging to an inflated category. We propose a semiparametric multiple inflation Poisson (MIP) model that considers the two nonlinear link functions. We develop a sieve maximum likelihood estimator (sMLE) for the regression parameters of interest. We establish the asymptotic behavior of the sMLE. Simulations are conducted to evaluate the performance of the proposed sieve MIP (sMIP). Then, we illustrate the methodology on data from a smoking cessation study. Finally, some remarks and opportunities for future research conclude the article.},
  archive      = {J_SIM},
  author       = {Ujjwal Das and Ranojoy Basu},
  doi          = {10.1002/sim.9380},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2711-2724},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric multiple inflation count model with application to a smoking cessation study},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantiles based personalized treatment selection for
multivariate outcomes and multiple treatments. <em>SIM</em>,
<em>41</em>(15), 2695–2710. (<a
href="https://doi.org/10.1002/sim.9377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a method for individualized treatment selection when there are correlated multiple responses for the K treatment ( K ≥ 2 ) scenario. Here we use ranks of quantiles of outcome variables for each treatment conditional on patient-specific scores constructed from collected covariate measurements. Our method covers any number of treatments and outcome variables using any number of quantiles and it can be applied for a broad set of models. We propose a rank aggregation technique for combining several lists of ranks where both these lists and elements within each list can be correlated. The method has the flexibility to incorporate patient and clinician preferences into the optimal treatment decision on an individual case basis. A simulation study demonstrates the performance of the proposed method in finite samples. We also present illustrations using two different datasets from diabetes and HIV-1 clinical trials to show the applicability of the proposed procedure for real data.},
  archive      = {J_SIM},
  author       = {Karunarathna B. Kulasekera and Chathura Siriwardhana},
  doi          = {10.1002/sim.9377},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2695-2710},
  shortjournal = {Stat. Med.},
  title        = {Quantiles based personalized treatment selection for multivariate outcomes and multiple treatments},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CORRECTION. <em>SIM</em>, <em>41</em>(14), 2693. (<a
href="https://doi.org/10.1002/sim.9391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  doi          = {10.1002/sim.9391},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2693},
  shortjournal = {Stat. Med.},
  title        = {CORRECTION},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Sample size formula for a win ratio endpoint. <em>SIM</em>,
<em>41</em>(14), 2691–2692. (<a
href="https://doi.org/10.1002/sim.9388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Ron Xiaolong Yu and Jitendra Ganju},
  doi          = {10.1002/sim.9388},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2691-2692},
  shortjournal = {Stat. Med.},
  title        = {Sample size formula for a win ratio endpoint},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comments on “sample size formula for a win ratio endpoint”
by r. X. Yu and j. ganju. <em>SIM</em>, <em>41</em>(14), 2688–2690. (<a
href="https://doi.org/10.1002/sim.9379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Samvel B. Gasparyan and Elaine K. Kowalewski and Gary G. Koch},
  doi          = {10.1002/sim.9379},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2688-2690},
  shortjournal = {Stat. Med.},
  title        = {Comments on “Sample size formula for a win ratio endpoint” by r. x. yu and j. ganju},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian semi-parametric modeling of covariance matrices for
multivariate longitudinal data. <em>SIM</em>, <em>41</em>(14),
2665–2687. (<a href="https://doi.org/10.1002/sim.9376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article develops marginal models for multivariate longitudinal responses. Overall, the model consists of five regression submodels, one for the mean and four for the covariance matrix, with the latter resulting by considering various matrix decompositions. The decompositions that we employ are intuitive, easy to understand, and they do not rely on any assumptions such as the presence of an ordering among the multivariate responses. The regression submodels are semi-parametric, with unknown functions represented by basis function expansions. We use spike-slap priors for the regression coefficients to achieve variable selection and function regularization, and to obtain parameter estimates that account for model uncertainty. An efficient Markov chain Monte Carlo algorithm for posterior sampling is developed. The simulation study presented investigates the gains that one may have when considering multivariate longitudinal analyses instead of univariate ones, and whether these gains can counteract the negative effects of missing data. We apply the methods on a highly unbalanced longitudinal dataset with four responses observed over a period of 20 years.},
  archive      = {J_SIM},
  author       = {Georgios Papageorgiou},
  doi          = {10.1002/sim.9376},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2665-2687},
  shortjournal = {Stat. Med.},
  title        = {Bayesian semi-parametric modeling of covariance matrices for multivariate longitudinal data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finite-sample adjustments in variance estimators for
clustered competing risks regression. <em>SIM</em>, <em>41</em>(14),
2645–2664. (<a href="https://doi.org/10.1002/sim.9375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The marginal Fine-Gray proportional subdistribution hazards model is a popular approach to directly study the association between covariates and the cumulative incidence function with clustered competing risks data, which often arise in multicenter randomized trials or multilevel observational studies. To account for the within-cluster correlations between failure times, the uncertainty of the regression parameters estimators is quantified by the robust sandwich variance estimator, which may have unsatisfactory performance with a limited number of clusters. To overcome this limitation, we propose four bias-corrected variance estimators to reduce the negative bias of the usual sandwich variance estimator, extending the bias-correction techniques from generalized estimating equations with noncensored exponential family outcomes to clustered competing risks outcomes. We further compare their finite-sample operating characteristics through simulations and two real data examples. In particular, we found the Mancl and DeRouen (MD) type sandwich variance estimator generally has the smallest bias. Furthermore, with a small number of clusters, the Wald -confidence interval with the MD sandwich variance estimator carries close to nominal coverage for the cluster-level effect parameter. The -confidence intervals based on the sandwich variance estimator with any one of the three types of multiplicative bias correction or the -confidence interval with the Morel, Bokossa and Neerchal (MBN) type sandwich variance estimator have close to nominal coverage for the individual-level effect parameter. Finally, we develop a user-friendly R package crrcbcv implementing the proposed sandwich variance estimators to assist practical applications.},
  archive      = {J_SIM},
  author       = {Xinyuan Chen and Fan Li},
  doi          = {10.1002/sim.9375},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2645-2664},
  shortjournal = {Stat. Med.},
  title        = {Finite-sample adjustments in variance estimators for clustered competing risks regression},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sample size calculation for the augmented logrank test in
randomized clinical trials. <em>SIM</em>, <em>41</em>(14), 2627–2644.
(<a href="https://doi.org/10.1002/sim.9374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In randomized clinical trials, incorporating baseline covariates can improve the power in hypothesis testing for treatment effects. For survival endpoints, the Cox proportional hazards model with baseline covariates as explanatory variables can improve the standard logrank test in power. Although this has long been recognized, this adjustment is not commonly used as the primary analysis and instead the logrank test followed by the estimation of the hazard ratio between treatment groups is often used. By projecting the score function for the Cox proportional hazards model onto a space of covariates, the logrank test can be more powerful. We derive a power formula for this augmented logrank test under the same setting as the widely used power formula for the logrank test and propose a simple strategy for sizing randomized clinical trials utilizing historical data of the control treatment. Through numerical studies, the proposed procedure was found to have the potential to reduce the sample size substantially as compared to the standard logrank test. A concern to utilize historical data is that those might not reflect well the data structure of the study to design and then the sample size calculated might not be accurate. Since our power formula is applicable to datasets pooled across the treatment arms, the validity of the power calculation at the design stage can be checked in blind reviews.},
  archive      = {J_SIM},
  author       = {Satoshi Hattori and Sho Komukai and Tim Friede},
  doi          = {10.1002/sim.9374},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2627-2644},
  shortjournal = {Stat. Med.},
  title        = {Sample size calculation for the augmented logrank test in randomized clinical trials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Laplacian-p-splines for bayesian inference in the mixture
cure model. <em>SIM</em>, <em>41</em>(14), 2602–2626. (<a
href="https://doi.org/10.1002/sim.9373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mixture cure model for analyzing survival data is characterized by the assumption that the population under study is divided into a group of subjects who will experience the event of interest over some finite time horizon and another group of cured subjects who will never experience the event irrespective of the duration of follow-up. When using the Bayesian paradigm for inference in survival models with a cure fraction, it is common practice to rely on Markov chain Monte Carlo (MCMC) methods to sample from posterior distributions. Although computationally feasible, the iterative nature of MCMC often implies long sampling times to explore the target space with chains that may suffer from slow convergence and poor mixing. Furthermore, extra efforts have to be invested in diagnostic checks to monitor the reliability of the generated posterior samples. A sampling-free strategy for fast and flexible Bayesian inference in the mixture cure model is suggested in this article by combining Laplace approximations and penalized B-splines. A logistic regression model is assumed for the cure proportion and a Cox proportional hazards model with a P-spline approximated baseline hazard is used to specify the conditional survival function of susceptible subjects. Laplace approximations to the posterior conditional latent vector are based on analytical formulas for the gradient and Hessian of the log-likelihood, resulting in a substantial speed-up in approximating posterior distributions. The spline specification yields smooth estimates of survival curves and functions of latent variables together with their associated credible interval are estimated in seconds. A fully stochastic algorithm based on a Metropolis-Langevin-within-Gibbs sampler is also suggested as an alternative to the proposed Laplacian-P-splines mixture cure (LPSMC) methodology. The statistical performance and computational efficiency of LPSMC is assessed in a simulation study. Results show that LPSMC is an appealing alternative to MCMC for approximate Bayesian inference in standard mixture cure models. Finally, the novel LPSMC approach is illustrated on three applications involving real survival data.},
  archive      = {J_SIM},
  author       = {Oswaldo Gressani and Christel Faes and Niel Hens},
  doi          = {10.1002/sim.9373},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2602-2626},
  shortjournal = {Stat. Med.},
  title        = {Laplacian-P-splines for bayesian inference in the mixture cure model},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian models for aggregate and individual patient data
component network meta-analysis. <em>SIM</em>, <em>41</em>(14),
2586–2601. (<a href="https://doi.org/10.1002/sim.9372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network meta-analysis can synthesize evidence from studies comparing multiple treatments for the same disease. Sometimes the treatments of a network are complex interventions, comprising several independent components in different combinations. A component network meta-analysis (CNMA) can be used to analyze such data and can in principle disentangle the individual effect of each component. However, components may interact with each other, either synergistically or antagonistically. Deciding which interactions, if any, to include in a CNMA model may be difficult, especially for large networks with many components. In this article, we present two Bayesian CNMA models that can be used to identify prominent interactions between components. Our models utilize Bayesian variable selection methods, namely the stochastic search variable selection and the Bayesian LASSO, and can benefit from the inclusion of prior information about important interactions. Moreover, we extend these models to combine data from studies providing aggregate information and studies providing individual patient data (IPD). We illustrate our models in practice using three real datasets, from studies in panic disorder, depression, and multiple myeloma. Finally, we describe methods for developing web-applications that can utilize results from an IPD-CNMA, to allow for personalized estimates of relative treatment effects given a patient&#39;s characteristics.},
  archive      = {J_SIM},
  author       = {Orestis Efthimiou and Michael Seo and Eirini Karyotaki and Pim Cuijpers and Toshi A. Furukawa and Guido Schwarzer and Gerta Rücker and Dimitris Mavridis},
  doi          = {10.1002/sim.9372},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2586-2601},
  shortjournal = {Stat. Med.},
  title        = {Bayesian models for aggregate and individual patient data component network meta-analysis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Marginal, conditional, and pseudo likelihood ratio
approaches for biomarker combination to predict a binary disease
outcome. <em>SIM</em>, <em>41</em>(14), 2574–2585. (<a
href="https://doi.org/10.1002/sim.9371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a common practice in public health research that multiple biomarkers are collected to diagnose or predict a disease outcome. A natural question is how to combine multiple biomarkers to improve the diagnostic accuracy. It has been shown by Neyman-Pearson lemma that the likelihood ratio statistic achieves the optimal AUC in theory. However, practical difficulty often lies in the estimation of the multivariate density functions. We propose three novel methods for the biomarker combination, with the idea of breaking down the joint densities to a series of univariate densities. The marginal likelihood ratio approach only assumes the marginal distribution of each biomarker. While the conditional likelihood ratio (CLR) and pseudo likelihood ratio (PLR) approaches assume the conditional distributions of a marker given others, and hence make use of the correlation structure to estimate the combination rules. The proposed methods make it much easier to assume and validate the univariate distributions of a biomarker than making multivariate distributional assumptions. Extensive simulation studies demonstrate that the CLR and the PLR approaches outperform many existing methods, and are therefore recommended for practical use. The proposed methods are motivated by and applied to a biomarker study to diagnose childhood autism/autism spectrum disorder.},
  archive      = {J_SIM},
  author       = {Danping Liu and Yongli Han and Aiyi Liu},
  doi          = {10.1002/sim.9371},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2574-2585},
  shortjournal = {Stat. Med.},
  title        = {Marginal, conditional, and pseudo likelihood ratio approaches for biomarker combination to predict a binary disease outcome},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Association testing for binary trees—a markov branching
process approach. <em>SIM</em>, <em>41</em>(14), 2557–2573. (<a
href="https://doi.org/10.1002/sim.9370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new approach to test associations between binary trees and covariates. In this approach, binary-tree structured data are treated as sample paths of binary fission Markov branching processes (bMBP). We propose a generalized linear regression model and developed inference procedures for association testing, including variable selection and estimation of covariate effects. Simulation studies show that these procedures are able to accurately identify covariates that are associated with the binary tree structure by impacting the rate parameter of the bMBP. The problem of association testing on binary trees is motivated by modeling hierarchical clustering dendrograms of pixel intensities in biomedical images. By using semi-synthetic data generated from a real brain-tumor image, our simulation studies show that the bMBP model is able to capture the characteristics of dendrogram trees in brain-tumor images. Our final analysis of the glioblastoma multiforme brain-tumor data from The Cancer Imaging Archive identified multiple clinical and genetic variables that are potentially associated with brain-tumor heterogeneity.},
  archive      = {J_SIM},
  author       = {Xiaowei Wu and Hongxiao Zhu},
  doi          = {10.1002/sim.9370},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2557-2573},
  shortjournal = {Stat. Med.},
  title        = {Association testing for binary trees—A markov branching process approach},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical power and sample size requirements to detect an
intervention by time interaction in four-level longitudinal cluster
randomized trials. <em>SIM</em>, <em>41</em>(14), 2542–2556. (<a
href="https://doi.org/10.1002/sim.9369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster/group randomized controlled trials (CRTs) have a long history in the study of health sciences. CRT is a special type of intervention trial in which a complete group is randomly assigned to a study condition (or intervention). It is typically performed when individual randomization is difficult/impossible without substantial risk of contamination across study arms or prohibitive from the cost or group dynamics point of view. In this article, the aim is to design and analyze four-level longitudinal cluster randomized trials. The main interest here is to study the difference between treatment groups over time for such a four-level hierarchical data structure. This work is motivated by a real-life study for education based HIV prevention. Such trials are not only popular for administrative convenience, ethical considerations, subject compliance, but also help to reduce contamination bias. A random intercept mixed effects linear regression including a time by intervention interaction is used for modeling. Closed form expression of the power function to detect the interaction effect is determined. Sample size equations depend on correlation among schools but not on correlations among classes or students while, the power function depends on the product of number of units at different levels. Optimal allocation of units under a fixed cost by minimizing the expected standardized variance is also determined and are shown to be independent of correlations among units in any level. Results of detailed simulation studies find the theoretical power estimates based on the derived formulae close to the empirical estimates.},
  archive      = {J_SIM},
  author       = {Samiran Ghosh and Siuli Mukhopadhyay and Priyanka Majumder and Bo Wang},
  doi          = {10.1002/sim.9369},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2542-2556},
  shortjournal = {Stat. Med.},
  title        = {Statistical power and sample size requirements to detect an intervention by time interaction in four-level longitudinal cluster randomized trials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonparametric bounds in two-sample summary-data mendelian
randomization: Some cautionary tales for practice. <em>SIM</em>,
<em>41</em>(14), 2523–2541. (<a
href="https://doi.org/10.1002/sim.9368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, in genetic epidemiology, Mendelian randomization (MR) has become a popular approach to estimate causal exposure effects by using single nucleotide polymorphisms from genome-wide association studies (GWAS) as instruments. The most popular type of MR study, a two-sample summary-data MR study, relies on having summary statistics from two independent GWAS and using parametric methods for estimation. However, little is understood about using a nonparametric bound-based analysis, a popular approach in traditional instrumental variables frameworks, to study causal effects in two-sample MR. In this article, we explore using a nonparametric, bound-based analysis in two-sample MR studies, focusing primarily on implications for practice. We also propose a framework to assess how likely one can obtain more informative bounds if we used a different MR design, notably a one-sample MR design. We conclude by demonstrating our findings through two real data analyses concerning the causal effect of smoking on lung cancer and the causal effect of high cholesterol on heart attacks. Overall, our results suggest that while a bound-based analysis may be appealing due to its nonparametric nature, it is far more conservative in two-sample settings than in one-sample settings to get informative bounds on the causal exposure effect.},
  archive      = {J_SIM},
  author       = {Ralph Møller Trane and Hyunseung Kang},
  doi          = {10.1002/sim.9368},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2523-2541},
  shortjournal = {Stat. Med.},
  title        = {Nonparametric bounds in two-sample summary-data mendelian randomization: Some cautionary tales for practice},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A mixture distribution approach for assessing genetic impact
from twin study. <em>SIM</em>, <em>41</em>(14), 2513–2522. (<a
href="https://doi.org/10.1002/sim.9367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is challenging to evaluate the genetic impacts on a biologic feature and separate them from environmental impacts. This is usually achieved through twin studies by assessing the collective genetic impact defined by the differential correlation in monozygotic twins vs dizygotic twins. Since the underlying order in a twin, determined by latent genetic factors, is unknown, the observed twin data are unordered. Conventional methods for correlation are not appropriate. To handle the missing order, we model twin data by a mixture bivariate distribution and estimate under two likelihood functions: the likelihood over the monozygotic and dizygotic twins separately, and the likelihood over the two twin types combined. Both likelihood estimators are consistent. More importantly, the combined likelihood overcomes the drawback of mixture distribution estimation, namely, the slow convergence. It yields correlation coefficient estimator of root- n consistency and allows effective statistical inference on the collective genetic impact. The method is demonstrated by a twin study on immune traits.},
  archive      = {J_SIM},
  author       = {Zonghui Hu and Pengfei Li and Dean Follmann and Jing Qin},
  doi          = {10.1002/sim.9367},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2513-2522},
  shortjournal = {Stat. Med.},
  title        = {A mixture distribution approach for assessing genetic impact from twin study},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-assisted analyses of longitudinal, ordinal outcomes
with absorbing states. <em>SIM</em>, <em>41</em>(14), 2497–2512. (<a
href="https://doi.org/10.1002/sim.9366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies of critically ill, hospitalized patients often follow participants and characterize daily health status using an ordinal outcome variable. Statistically, longitudinal proportional odds models are a natural choice in these settings since such models can parsimoniously summarize differences across patient groups and over time. However, when one or more of the outcome states is absorbing, the proportional odds assumption for the follow-up time parameter will likely be violated, and more flexible longitudinal models are needed. Motivated by the VIOLET Study (Ginde et al), a parallel-arm, randomized clinical trial of Vitamin in critically ill patients, we discuss and contrast several treatment effect estimands based on time-dependent odds ratio parameters, and we detail contemporary modeling approaches. In VIOLET, the outcome is a four-level ordinal variable where the lowest ”not alive” state is absorbing and the highest ”at-home” state is nearly absorbing. We discuss flexible extensions of the proportional odds model for longitudinal data that can be used for either model-based inference, where the odds ratio estimator is taken directly from the model fit, or for model-assisted inferences, where heterogeneity across cumulative log odds dichotomizations is modeled and results are summarized to obtain an overall odds ratio estimator. We focus on direct estimation of cumulative probability model (CPM) parameters using likelihood-based analysis procedures that naturally handle absorbing states. We illustrate the modeling procedures, the relative precision of model-based and model-assisted estimators, and the possible differences in the values for which the estimators are consistent through simulations and analysis of the VIOLET Study data.},
  archive      = {J_SIM},
  author       = {Jonathan S. Schildcrout and Frank E. Harrell and Patrick J. Heagerty and Sebastien Haneuse and Chiara Di Gravio and Shawn P. Garbett and Paul J. Rathouz and Bryan E. Shepherd},
  doi          = {10.1002/sim.9366},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2497-2512},
  shortjournal = {Stat. Med.},
  title        = {Model-assisted analyses of longitudinal, ordinal outcomes with absorbing states},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating misclassification errors in the reporting of
maternal mortality in national civil registration vital statistics
systems: A bayesian hierarchical bivariate random walk model to estimate
sensitivity and specificity for multiple countries and years with
missing data. <em>SIM</em>, <em>41</em>(14), 2483–2496. (<a
href="https://doi.org/10.1002/sim.9335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Civil registration vital statistics (CRVS) systems provide data on maternal mortality that can be used for monitoring trends and to inform policies and programs. However, CRVS maternal mortality data may be subject to substantial reporting errors due to misclassification of maternal deaths. Information on misclassification is available for selected countries and periods only. We developed a Bayesian hierarchical bivariate random walk model to estimate sensitivity and specificity for multiple populations and years and used the model to estimate misclassification errors in the reporting of maternal mortality in CRVS systems. The proposed Bayesian misclassification (BMis) model captures differences in sensitivity and specificity across populations and over time, allows for extrapolations to periods with missing data, and includes an exact likelihood function for data provided in aggregated form. Validation exercises using maternal mortality data suggest that BMis is reasonably well calibrated and improves upon the CRVS-adjustment approach used until 2018 by the UN Maternal Mortality Inter-Agency Group (UN-MMEIG) to account for bias in CRVS data resulting from misclassification error. Since 2019, BMis is used by the UN-MMEIG to account for misclassification errors when estimating maternal mortality using CRVS data.},
  archive      = {J_SIM},
  author       = {Emily Peterson and Doris Chou and Ann-Beth Moller and Alison Gemmill and Lale Say and Leontine Alkema},
  doi          = {10.1002/sim.9335},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2483-2496},
  shortjournal = {Stat. Med.},
  title        = {Estimating misclassification errors in the reporting of maternal mortality in national civil registration vital statistics systems: A bayesian hierarchical bivariate random walk model to estimate sensitivity and specificity for multiple countries and years with missing data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The required size of cluster randomized trials of
nonpharmaceutical interventions in epidemic settings. <em>SIM</em>,
<em>41</em>(13), 2466–2482. (<a
href="https://doi.org/10.1002/sim.9365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To control the SARS-CoV-2 pandemic and future pathogen outbreaks requires an understanding of which nonpharmaceutical interventions are effective at reducing transmission. Observational studies, however, are subject to biases that could erroneously suggest an impact on transmission, even when there is no true effect. Cluster randomized trials permit valid hypothesis tests of the effect of interventions on community transmission. While such trials could be completed in a relatively short period of time, they might require large sample sizes to achieve adequate power. However, the sample sizes required for such tests in outbreak settings are largely undeveloped, leaving unanswered the question of whether these designs are practical. We develop approximate sample size formulae and simulation-based sample size methods for cluster randomized trials in infectious disease outbreaks. We highlight key relationships between characteristics of transmission and the enrolled communities and the required sample sizes, describe settings where trials powered to detect a meaningful true effect size may be feasible, and provide recommendations for investigators in planning such trials. The approximate formulae and simulation banks may be used by investigators to quickly assess the feasibility of a trial, followed by more detailed methods to more precisely size the trial. For example, we show that community-scale trials requiring 220 clusters with 100 tested individuals per cluster are powered to identify interventions that reduce transmission by 40% in one generation interval, using parameters identified for SARS-CoV-2 transmission. For more modest treatment effects, or when transmission is extremely overdispersed, however, much larger sample sizes are required.},
  archive      = {J_SIM},
  author       = {Justin K. Sheen and Johannes Haushofer and C. Jessica E. Metcalf and Lee Kennedy-Shaffer},
  doi          = {10.1002/sim.9365},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2466-2482},
  shortjournal = {Stat. Med.},
  title        = {The required size of cluster randomized trials of nonpharmaceutical interventions in epidemic settings},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Assessing complier average causal effects from longitudinal
trials with multiple endpoints and treatment noncompliance: An
application to a study of arthritis health journal. <em>SIM</em>,
<em>41</em>(13), 2448–2465. (<a
href="https://doi.org/10.1002/sim.9364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Treatment noncompliance often occurs in longitudinal randomized controlled trials (RCTs) on human subjects, and can greatly complicate treatment effect assessment. The complier average causal effect (CACE) informs the intervention efficacy for the subpopulation who would comply regardless of assigned treatment and has been considered as patient-oriented treatment effects of interest in the presence of noncompliance. Real-world RCTs evaluating multifaceted interventions often employ multiple study endpoints to measure treatment success. In such trials, limited sample sizes, low compliance rates, and small to moderate effect sizes on individual endpoints can significantly reduce the power to detect CACE when these correlated endpoints are analyzed separately. To overcome the challenge, we develop a multivariate longitudinal potential outcome model with stratification on latent compliance types to efficiently assess multivariate CACEs (MCACE) by combining information across multiple endpoints and visits. Evaluation using simulation data shows a significant increase in the estimation efficiency with the MCACE model, including up to 50% reduction in standard errors (SEs) of CACE estimates and 1-fold increase in the power to detect CACE. Finally, we apply the proposed MCACE model to an RCT on Arthritis Health Journal online tool. Results show that the MCACE analysis detects significant and beneficial intervention effects on two of the six endpoints while estimating CACEs for these endpoints separately fail to detect treatment effect on any endpoint.},
  archive      = {J_SIM},
  author       = {Lulu Guo and Yi Qian and Hui Xie},
  doi          = {10.1002/sim.9364},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2448-2465},
  shortjournal = {Stat. Med.},
  title        = {Assessing complier average causal effects from longitudinal trials with multiple endpoints and treatment noncompliance: An application to a study of arthritis health journal},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A two-way flexible generalized gamma transformation cure
rate model. <em>SIM</em>, <em>41</em>(13), 2427–2447. (<a
href="https://doi.org/10.1002/sim.9363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a two-way flexible cure rate model. The first flexibility is provided by considering a family of Box-Cox transformation cure models that include the commonly used cure models as special cases. The second flexibility is provided by proposing the wider class of generalized gamma distributions to model the associated lifetime. The advantage of this two-way flexibility is that it allows us to carry out tests of hypotheses to select an adequate cure model (within the family of Box-Cox transformation cure models) and a suitable lifetime distribution (within the wider class of generalized gamma distributions) that jointly provides the best fit to a given data. First, we study the maximum likelihood estimation of the generalized gamma Box-Cox transformation (GGBCT) model parameters. Then, we use the flexibility of our proposed model to carry out power studies to demonstrate the power of likelihood ratio test in rejecting mis-specified models. Furthermore, we study the bias and efficiency of the estimators of the cure rates under model mis-specification. Our findings strongly suggest the importance of selecting a correct lifetime distribution and a correct cure rate model, which can be achieved through the proposed two-way flexible model. Finally, we illustrate the applicability of our proposed model using a data from a breast cancer study and show that our model provides a better fit than the existing semiparametric Box-Cox transformation cure model with piecewise exponential approximation to the lifetime distribution.},
  archive      = {J_SIM},
  author       = {Pei Wang and Suvra Pal},
  doi          = {10.1002/sim.9363},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2427-2447},
  shortjournal = {Stat. Med.},
  title        = {A two-way flexible generalized gamma transformation cure rate model},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Testing a global null hypothesis using ensemble machine
learning methods. <em>SIM</em>, <em>41</em>(13), 2417–2426. (<a
href="https://doi.org/10.1002/sim.9362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing a global null hypothesis that there are no significant predictors for a binary outcome of interest among a large set of biomarker measurements is an important task in biomedical studies. We seek to improve the power of such testing methods by leveraging ensemble machine learning methods. Ensemble machine learning methods such as random forest, bagging, and adaptive boosting model the relationship between the outcome and the predictor nonparametrically, while stacking combines the strength of multiple learners. We demonstrate the power of the proposed testing methods through Monte Carlo studies and show the use of the methods by applying them to the immunologic biomarkers dataset from the RV144 HIV vaccine efficacy trial.},
  archive      = {J_SIM},
  author       = {Sunwoo Han and Youyi Fong and Ying Huang},
  doi          = {10.1002/sim.9362},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2417-2426},
  shortjournal = {Stat. Med.},
  title        = {Testing a global null hypothesis using ensemble machine learning methods},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved estimation in negative binomial regression.
<em>SIM</em>, <em>41</em>(13), 2403–2416. (<a
href="https://doi.org/10.1002/sim.9361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Negative binomial regression is commonly employed to analyze overdispersed count data. With small to moderate sample sizes, the maximum likelihood estimator of the dispersion parameter may be subject to a significant bias, that in turn affects inference on mean parameters. This article proposes inference for negative binomial regression based on adjustments of the score function aimed at mean or median bias reduction. The resulting estimating equations generalize those available for improved inference in generalized linear models and can be solved using a suitable extension of iterative weighted least squares. Simulation studies confirm the good properties of the new methods, which are also found to solve in many cases numerical problems of maximum likelihood estimation. The methods are illustrated and evaluated using two case studies: an Ames salmonella assay data set and data on epileptic seizures. Inference based on adjusted scores turns out to generally improve on maximum likelihood, and even on explicit bias correction, with median bias reduction being overall preferable.},
  archive      = {J_SIM},
  author       = {Euloge Clovis Kenne Pagui and Alessandra Salvan and Nicola Sartori},
  doi          = {10.1002/sim.9361},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2403-2416},
  shortjournal = {Stat. Med.},
  title        = {Improved estimation in negative binomial regression},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Group sequential design for randomized trials using “first
hitting time” model. <em>SIM</em>, <em>41</em>(13), 2375–2402. (<a
href="https://doi.org/10.1002/sim.9360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group sequential design (GSD) has become a popular choice in recent clinical trials as it improves trial efficiency by providing options for early termination. The implementation of traditional tests for survival analysis (eg, the log-rank test and the Cox proportional hazard (PH) model) in the GSD setting has been widely discussed. The PH assumption is required for conventional (sequential) design, it is, however, often violated in practice. As an alternative, some generalized tests have been proposed (eg, the Max-Combo test) and their efficacies have been established. In this article, we explore the application of a more flexible, “first hitting time” based threshold regression (TR) model to GSD. TR assumes that subjects&#39; health status is a latent (unobservable) process, and the clinical event of interest occurs when the latent health process hits a pre-specified boundary. The simulation results supported our findings that, in most cases, this comparable new method can successfully control type I error while providing higher early stopping opportunities in the sequential design, even when non-proportional hazard presents.},
  archive      = {J_SIM},
  author       = {Yiming Chen and John Lawrence and Mei-Ling Ting Lee},
  doi          = {10.1002/sim.9360},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2375-2402},
  shortjournal = {Stat. Med.},
  title        = {Group sequential design for randomized trials using “first hitting time” model},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A semi-parametric bayesian model for semi-continuous
longitudinal data. <em>SIM</em>, <em>41</em>(13), 2354–2374. (<a
href="https://doi.org/10.1002/sim.9359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-continuous data present challenges in both model fitting and interpretation. Parametric distributions may be inappropriate for extreme long right tails of the data. Mean effects of covariates, susceptible to extreme values, may fail to capture relevant information for most of the sample. We propose a two-component semi-parametric Bayesian mixture model, with the discrete component captured by a probability mass (typically at zero) and the continuous component of the density modeled by a mixture of B-spline densities that can be flexibly fit to any data distribution. The model includes random effects of subjects to allow for application to longitudinal data. We specify prior distributions on parameters and perform model inference using a Markov chain Monte Carlo (MCMC) Gibbs-sampling algorithm programmed in R. Statistical inference can be made for multiple quantiles of the covariate effects simultaneously providing a comprehensive view. Various MCMC sampling techniques are used to facilitate convergence. We demonstrate the performance and the interpretability of the model via simulations and analyses on the National Consortium on Alcohol and Neurodevelopment in Adolescence study (NCANDA) data on alcohol binge drinking.},
  archive      = {J_SIM},
  author       = {Junting Ren and Susan Tapert and Chun Chieh Fan and Wesley K. Thompson},
  doi          = {10.1002/sim.9359},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2354-2374},
  shortjournal = {Stat. Med.},
  title        = {A semi-parametric bayesian model for semi-continuous longitudinal data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multivariate parametric empirical bayes screening approach
for early detection of hepatocellular carcinoma using multiple
longitudinal biomarkers. <em>SIM</em>, <em>41</em>(13), 2338–2353. (<a
href="https://doi.org/10.1002/sim.9358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The early detection of hepatocellular carcinoma (HCC) is critical to improving outcomes since advanced HCC has limited treatment options. Current guidelines recommend HCC ultrasound surveillance every 6 months in high-risk patients however the sensitivity for detecting early stage HCC in clinical practice is poor. Blood-based biomarkers are a promising direction since they are more easily standardized and less resource intensive. Combining of multiple biomarkers is more likely to achieve the sensitivity required for a clinically useful screening algorithm and the longitudinal trajectory of biomarkers contains valuable information that should be utilized. We propose a multivariate parametric empirical Bayes (mPEB) screening approach that defines personalized thresholds for each patient at each screening visit to identify significant deviations that trigger additional testing with more sensitive imaging. The Hepatitis C Antiviral Long-term Treatment against Cirrhosis (HALT-C) trial provides a valuable source of data to study HCC screening algorithms. We study the performance of the mPEB algorithm applied to serum -fetoprotein, a widely used HCC surveillance biomarker, and des- carboxy prothrombin, an HCC risk biomarker that is FDA approved but not used in practice in the United States. Using cross-validation, we found that the mPEB algorithm demonstrated moderate but improved sensitivity compared to alternative screening approaches. Future research will validate the clinical utility of the approach in larger cohort studies with additional biomarkers.},
  archive      = {J_SIM},
  author       = {Nabihah Tayob and Anna S. F. Lok and Ziding Feng},
  doi          = {10.1002/sim.9358},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2338-2353},
  shortjournal = {Stat. Med.},
  title        = {A multivariate parametric empirical bayes screening approach for early detection of hepatocellular carcinoma using multiple longitudinal biomarkers},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extending the susceptible-exposed-infected-removed (SEIR)
model to handle the false negative rate and symptom-based administration
of COVID-19 diagnostic tests: SEIR-fansy. <em>SIM</em>, <em>41</em>(13),
2317–2337. (<a href="https://doi.org/10.1002/sim.9357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {False negative rates of severe acute respiratory coronavirus 2 diagnostic tests, together with selection bias due to prioritized testing can result in inaccurate modeling of COVID-19 transmission dynamics based on reported “case” counts. We propose an extension of the widely used Susceptible-Exposed-Infected-Removed (SEIR) model that accounts for misclassification error and selection bias, and derive an analytic expression for the basic reproduction number as a function of false negative rates of the diagnostic tests and selection probabilities for getting tested. Analyzing data from the first two waves of the pandemic in India, we show that correcting for misclassification and selection leads to more accurate prediction in a test sample. We provide estimates of undetected infections and deaths between April 1, 2020 and August 31, 2021. At the end of the first wave in India, the estimated under-reporting factor for cases was at 11.1 (95% CI: 10.7,11.5) and for deaths at 3.58 (95% CI: 3.5,3.66) as of February 1, 2021, while they change to 19.2 (95% CI: 17.9, 19.9) and 4.55 (95% CI: 4.32, 4.68) as of July 1, 2021. Equivalently, 9.0% (95% CI: 8.7%, 9.3%) and 5.2% (95% CI: 5.0%, 5.6%) of total estimated infections were reported on these two dates, while 27.9% (95% CI: 27.3%, 28.6%) and 22% (95% CI: 21.4%, 23.1%) of estimated total deaths were reported. Extensive simulation studies demonstrate the effect of misclassification and selection on estimation of and prediction of future infections. A R-package SEIRfansy is developed for broader dissemination.},
  archive      = {J_SIM},
  author       = {Ritwik Bhaduri and Ritoban Kundu and Soumik Purkayastha and Michael Kleinsasser and Lauren J. Beesley and Bhramar Mukherjee and Jyotishka Datta},
  doi          = {10.1002/sim.9357},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2317-2337},
  shortjournal = {Stat. Med.},
  title        = {Extending the susceptible-exposed-infected-removed (SEIR) model to handle the false negative rate and symptom-based administration of COVID-19 diagnostic tests: SEIR-fansy},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sample size estimation using a latent variable model for
mixed outcome co-primary, multiple primary and composite endpoints.
<em>SIM</em>, <em>41</em>(13), 2303–2316. (<a
href="https://doi.org/10.1002/sim.9356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed outcome endpoints that combine multiple continuous and discrete components are often employed as primary outcome measures in clinical trials. These may be in the form of co-primary endpoints, which conclude effectiveness overall if an effect occurs in all of the components, or multiple primary endpoints, which require an effect in at least one of the components. Alternatively, they may be combined to form composite endpoints, which reduce the outcomes to a one-dimensional endpoint. There are many advantages to joint modeling the individual outcomes, however in order to do this in practice we require techniques for sample size estimation. In this article we show how the latent variable model can be used to estimate the joint endpoints and propose hypotheses, power calculations and sample size estimation methods for each. We illustrate the techniques using a numerical example based on a four-dimensional endpoint and find that the sample size required for the co-primary endpoint is larger than that required for the individual endpoint with the smallest effect size. Conversely, the sample size required in the multiple primary case is similar to that needed for the outcome with the largest effect size. We show that the empirical power is achieved for each endpoint and that the FWER can be sufficiently controlled using a Bonferroni correction if the correlations between endpoints are less than 0.5. Otherwise, less conservative adjustments may be needed. We further illustrate empirically the efficiency gains that may be achieved in the composite endpoint setting.},
  archive      = {J_SIM},
  author       = {Martina E. McMenamin and Jessica K. Barrett and Anna Berglind and James M. S. Wason},
  doi          = {10.1002/sim.9356},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2303-2316},
  shortjournal = {Stat. Med.},
  title        = {Sample size estimation using a latent variable model for mixed outcome co-primary, multiple primary and composite endpoints},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian two-stage design for phase II oncology trials with
binary endpoint. <em>SIM</em>, <em>41</em>(12), 2291–2301. (<a
href="https://doi.org/10.1002/sim.9355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In phase II oncology trials, two-stage design allowing early stopping for futility and/or efficacy is frequently used. However, this design based on frequentist statistical approaches could not guarantee a high posterior probability of attending the pre-specified clinically interesting rate from a Bayesian perspective. Here, we proposed a new Bayesian design enabling early terminating for efficacy as well as futility. In addition to the clinically uninteresting and interesting response rate, a prior distribution of response rate, the minimum posterior threshold probabilities and the lengths of the highest posterior density intervals were specified in the design. Finally, we defined the feasible design with the highest total effective predictive probability. We studied the properties of the proposed design and applied it to an oncology trial as an example. The proposed design ensured that the observed response rate fell within prespecified levels of posterior probability. The proposed design provides an alternative design to single-arm two-stage trials.},
  archive      = {J_SIM},
  author       = {Lichang Chen and Jianhong Pan and Yanpeng Wu and Jingxian Wang and Fangyao Chen and Jun Zhao and Pingyan Chen},
  doi          = {10.1002/sim.9355},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2291-2301},
  shortjournal = {Stat. Med.},
  title        = {Bayesian two-stage design for phase II oncology trials with binary endpoint},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bayesian hierarchical model for individual participant
data meta-analysis of demand curves. <em>SIM</em>, <em>41</em>(12),
2276–2290. (<a href="https://doi.org/10.1002/sim.9354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individual participant data meta-analysis is a frequently used method to combine and contrast data from multiple independent studies. Bayesian hierarchical models are increasingly used to appropriately take into account potential heterogeneity between studies. In this paper, we propose a Bayesian hierarchical model for individual participant data generated from the Cigarette Purchase Task (CPT). Data from the CPT details how demand for cigarettes varies as a function of price, which is usually described as an exponential demand curve. As opposed to the conventional random-effects meta-analysis methods, Bayesian hierarchical models are able to estimate both the study-specific and population-level parameters simultaneously without relying on the normality assumptions. We applied the proposed model to a meta-analysis with baseline CPT data from six studies and compared the results from the proposed model and a two-step conventional random-effects meta-analysis approach. We conducted extensive simulation studies to investigate the performance of the proposed approach and discussed the benefits of using the Bayesian hierarchical model for individual participant data meta-analysis of demand curves.},
  archive      = {J_SIM},
  author       = {Shengwei Zhang and Haitao Chu and Warren K. Bickel and Chap T. Le and Tracy T. Smith and Janet L. Thomas and Eric C. Donny and Dorothy K. Hatsukami and Xianghua Luo},
  doi          = {10.1002/sim.9354},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2276-2290},
  shortjournal = {Stat. Med.},
  title        = {A bayesian hierarchical model for individual participant data meta-analysis of demand curves},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pooling random forest and functional data analysis for
biomedical signals supervised classification: Theory and application to
electrocardiogram data. <em>SIM</em>, <em>41</em>(12), 2247–2275. (<a
href="https://doi.org/10.1002/sim.9353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific progress has contributed to creating many devices to gather vast amounts of biomedical data over time. The goal of these devices is generally to monitor people&#39;s health conditions, diagnose, and prevent patients&#39; diseases, for example, to discover cardiovascular disorders or predict epileptic seizures. A common way of investigating these data is classification, but these instruments generate signals often characterized by high dimensionality. Learning from these data is definitely a challenging task due to many issues, for example, the trade-off between complexity and accuracy and the course of dimensionality. This study proposes a supervised classification method based on the joint use of functional data analysis, classification trees, and random forest to deal with massive biomedical data recorded over time. For this purpose, this research suggests different original tools to extract features and train functional classifiers, interpret the classification rules, assess leaves&#39; quality and composition, avoid the classical drawbacks due to the COD, and improve the accuracy of the functional classifiers. Focusing on ECG data as a possible example, the final purpose of this study is to offer an original approach to identify and classify patients at risk using different types of biomedical signals. The results confirm that this line of research is exciting; indeed, the interpretative tools show evidence to be very useful for understanding classification rules. Furthermore, the performance of the proposed functional classifier, in terms of accuracy, is excellent because the latter breaks the previous classification record regarding a well-known ECG dataset.},
  archive      = {J_SIM},
  author       = {Fabrizio Maturo and Rosanna Verde},
  doi          = {10.1002/sim.9353},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2247-2275},
  shortjournal = {Stat. Med.},
  title        = {Pooling random forest and functional data analysis for biomedical signals supervised classification: Theory and application to electrocardiogram data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation of the proportion of treatment effect explained
by a high-dimensional surrogate. <em>SIM</em>, <em>41</em>(12),
2227–2246. (<a href="https://doi.org/10.1002/sim.9352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical studies examining the effectiveness of a treatment with respect to some primary outcome often require long-term follow-up of patients and/or costly or burdensome measurements of the primary outcome of interest. Identifying a surrogate marker for the primary outcome of interest may allow one to evaluate a treatment effect with less follow-up time, less cost, or less burden. While much clinical and statistical work has focused on identifying and validating surrogate markers, available approaches tend to focus on settings in which only a single surrogate marker is of interest. Limited work has been done to accommodate the high-dimensional surrogate marker setting where the number of potential surrogates is greater than the sample size. In this article, we develop methods to estimate the proportion of treatment effect explained by high-dimensional surrogates. We study the asymptotic properties of our proposed estimator, propose inference procedures, and examine finite sample performance via a simulation study. We illustrate our proposed methods using data from a randomized study comparing a novel whey-based oral nutrition supplement with a standard supplement with respect to change in body fat percentage over 12 weeks, where the surrogate markers of interest are gene expression probesets.},
  archive      = {J_SIM},
  author       = {Ruixuan Rachel Zhou and Sihai Dave Zhao and Layla Parast},
  doi          = {10.1002/sim.9352},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2227-2246},
  shortjournal = {Stat. Med.},
  title        = {Estimation of the proportion of treatment effect explained by a high-dimensional surrogate},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inferring medication adherence from time-varying health
measures. <em>SIM</em>, <em>41</em>(12), 2205–2226. (<a
href="https://doi.org/10.1002/sim.9351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medication adherence is a problem of widespread concern in clinical care. Poor adherence is a particular problem for patients with chronic diseases requiring long-term medication because poor adherence can result in less successful treatment outcomes and even preventable deaths. Existing methods to collect information about patient adherence are resource-intensive or do not successfully detect low-adherers with high accuracy. Acknowledging that health measures recorded at clinic visits are more reliably recorded than a patient&#39;s adherence, we have developed an approach to infer medication adherence rates based on longitudinally recorded health measures that are likely impacted by time-varying adherence behaviors. Our framework permits the inclusion of baseline health characteristics and socio-demographic data. We employ a modular inferential approach. First, we fit a two-component model on a training set of patients who have detailed adherence data obtained from electronic medication monitoring. One model component predicts adherence behaviors only from baseline health and socio-demographic information, and the other predicts longitudinal health measures given the adherence and baseline health measures. Posterior draws of relevant model parameters are simulated from this model using Markov chain Monte Carlo methods. Second, we develop an approach to infer medication adherence from the time-varying health measures using a sequential Monte Carlo algorithm applied to a new set of patients for whom no adherence data are available. We apply and evaluate the method on a cohort of hypertensive patients, using baseline health comorbidities, socio-demographic measures, and blood pressure measured over time to infer patients&#39; adherence to antihypertensive medication.},
  archive      = {J_SIM},
  author       = {Kristen B. Hunter and Mark E. Glickman and Luis F. Campos},
  doi          = {10.1002/sim.9351},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2205-2226},
  shortjournal = {Stat. Med.},
  title        = {Inferring medication adherence from time-varying health measures},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sample size calculation for cluster randomized trials with
zero-inflated count outcomes. <em>SIM</em>, <em>41</em>(12), 2191–2204.
(<a href="https://doi.org/10.1002/sim.9350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster randomized trials (CRT) have been widely employed in medical and public health research. Many clinical count outcomes, such as the number of falls in nursing homes, exhibit excessive zero values. In the presence of zero inflation, traditional power analysis methods for count data based on Poisson or negative binomial distribution may be inadequate. In this study, we present a sample size method for CRTs with zero-inflated count outcomes. It is developed based on GEE regression directly modeling the marginal mean of a zero-inflated Poisson outcome, which avoids the challenge of testing two intervention effects under traditional modeling approaches. A closed-form sample size formula is derived which properly accounts for zero inflation, ICCs due to clustering, unbalanced randomization, and variability in cluster size. Robust approaches, including t -distribution-based approximation and Jackknife re-sampling variance estimator, are employed to enhance trial properties under small sample sizes. Extensive simulations are conducted to evaluate the performance of the proposed method. An application example is presented in a real clinical trial setting.},
  archive      = {J_SIM},
  author       = {Zhengyang Zhou and Dateng Li and Song Zhang},
  doi          = {10.1002/sim.9350},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2191-2204},
  shortjournal = {Stat. Med.},
  title        = {Sample size calculation for cluster randomized trials with zero-inflated count outcomes},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Assessing treatment benefit in the presence of placebo
response using the sequential parallel comparison design. <em>SIM</em>,
<em>41</em>(12), 2166–2190. (<a
href="https://doi.org/10.1002/sim.9349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials, placebo response is considered a beneficial effect arising from multiple factors, including the patient&#39;s expectations for the treatment. Its presence makes the classical parallel study design suboptimal and can bias the inference. The sequential parallel comparison design (SPCD), a two-stage design where the first stage is a classical parallel study design, followed by another parallel design among placebo subjects from the first stage, was proposed to address the shortcomings of the classical design. In SPCD, in lieu of treatment effect, a weighted average of the mean treatment difference in Stage I among all randomized subjects and the mean treatment difference in Stage II among placebo non-responders was proposed as the efficacy measure. However, by linking two possibly different populations, this weighted average lacks interpretability, and the choice of weight remains controversial. In this work, under the principal stratification framework, we propose a causal estimand for the treatment effect under each of three clinically important principal strata: Always Responders, Never Responders, and Drug-only Responders. To make the stratum treatment effect identifiable, we introduce a set of assumptions and two sensitivity parameters. By further considering the strata as latent characteristics, the sensitivity parameters can be estimated. An extensive simulation study is conducted to evaluate the operating characteristics of the proposed method. Finally, we apply our method on the ADAPT-A study data to assess the benefit of low-dose aripiprazole adjunctive to antidepressant therapy treatment.},
  archive      = {J_SIM},
  author       = {Xiaoyan Liu and Chanmin Kim and Zifei Han and Pilar Lim and Satrajit Roychoudhury and Maurizio Fava and Gheorghe Doros},
  doi          = {10.1002/sim.9349},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2166-2190},
  shortjournal = {Stat. Med.},
  title        = {Assessing treatment benefit in the presence of placebo response using the sequential parallel comparison design},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluating the robustness of targeted maximum likelihood
estimators via realistic simulations in nutrition intervention trials.
<em>SIM</em>, <em>41</em>(12), 2132–2165. (<a
href="https://doi.org/10.1002/sim.9348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several recently developed methods have the potential to harness machine learning in the pursuit of target quantities inspired by causal inference, including inverse weighting, doubly robust estimating equations and substitution estimators like targeted maximum likelihood estimation. There are even more recent augmentations of these procedures that can increase robustness, by adding a layer of cross-validation (cross-validated targeted maximum likelihood estimation and double machine learning, as applied to substitution and estimating equation approaches, respectively). While these methods have been evaluated individually on simulated and experimental data sets, a comprehensive analysis of their performance across real data based simulations have yet to be conducted. In this work, we benchmark multiple widely used methods for estimation of the average treatment effect using ten different nutrition intervention studies data. A nonparametric regression method, undersmoothed highly adaptive lasso, is used to generate the simulated distribution which preserves important features from the observed data and reproduces a set of true target parameters. For each simulated data, we apply the methods above to estimate the average treatment effects as well as their standard errors and resulting confidence intervals. Based on the analytic results, a general recommendation is put forth for use of the cross-validated variants of both substitution and estimating equation estimators. We conclude that the additional layer of cross-validation helps in avoiding unintentional over-fitting of nuisance parameter functionals and leads to more robust inferences.},
  archive      = {J_SIM},
  author       = {Haodong Li and Sonali Rosete and Jeremy Coyle and Rachael V. Phillips and Nima S. Hejazi and Ivana Malenica and Benjamin F. Arnold and Jade Benjamin-Chung and Andrew Mertens and John M. Colford and Mark J. van der Laan and Alan E. Hubbard},
  doi          = {10.1002/sim.9348},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2132-2165},
  shortjournal = {Stat. Med.},
  title        = {Evaluating the robustness of targeted maximum likelihood estimators via realistic simulations in nutrition intervention trials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shared decision making of burdensome surveillance tests
using personalized schedules and their burden and benefit. <em>SIM</em>,
<em>41</em>(12), 2115–2131. (<a
href="https://doi.org/10.1002/sim.9347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benchmark surveillance tests for detecting disease progression (eg, biopsies, endoscopies) in early-stage chronic noncommunicable diseases (eg, cancer, lung diseases) are usually burdensome. For detecting progression timely, patients undergo invasive tests planned in a fixed one-size-fits-all manner (eg, annually). We aim to present personalized test schedules based on the risk of disease progression, that optimize the burden (the number of tests) and the benefit (shorter time delay in detecting progression is better) better than fixed schedules, and enable shared decision making. Our motivation comes from the problem of scheduling biopsies in prostate cancer surveillance. Using joint models for time-to-event and longitudinal data, we consolidate patients&#39; longitudinal data (eg, biomarkers) and results of previous tests, into individualized future cumulative-risk of progression. We then create personalized schedules by planning tests on future visits where the predicted cumulative-risk is above a threshold (eg, 5% risk). We update personalized schedules with data gathered over follow-up. To find the optimal risk threshold, we minimize a utility function of the expected number of tests (burden) and expected time delay in detecting progression (shorter is beneficial) for different thresholds. We estimate these two in a patient-specific manner for following any schedule, by utilizing a patient&#39;s predicted risk profile. Patients/doctors can employ these quantities to compare personalized and fixed schedules objectively and make a shared decision of a test schedule.},
  archive      = {J_SIM},
  author       = {Anirudh Tomer and Daan Nieboer and Monique J. Roobol and Ewout W. Steyerberg and Dimitris Rizopoulos},
  doi          = {10.1002/sim.9347},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2115-2131},
  shortjournal = {Stat. Med.},
  title        = {Shared decision making of burdensome surveillance tests using personalized schedules and their burden and benefit},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Network meta-analysis and random walks. <em>SIM</em>,
<em>41</em>(12), 2091–2114. (<a
href="https://doi.org/10.1002/sim.9346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network meta-analysis (NMA) is a central tool for evidence synthesis in clinical research. The results of an NMA depend critically on the quality of evidence being pooled. In assessing the validity of an NMA, it is therefore important to know the proportion contributions of each direct treatment comparison to each network treatment effect. The construction of proportion contributions is based on the observation that each row of the hat matrix represents a so-called “evidence flow network” for each treatment comparison. However, the existing algorithm used to calculate these values is associated with ambiguity according to the selection of paths. In this article, we present a novel analogy between NMA and random walks. We use this analogy to derive closed-form expressions for the proportion contributions. A random walk on a graph is a stochastic process that describes a succession of random “hops” between vertices which are connected by an edge. The weight of an edge relates to the probability that the walker moves along that edge. We use the graph representation of NMA to construct the transition matrix for a random walk on the network of evidence. We show that the net number of times a walker crosses each edge of the network is related to the evidence flow network. By then defining a random walk on the directed evidence flow network, we derive analytically the matrix of proportion contributions. The random-walk approach has none of the associated ambiguity of the existing algorithm.},
  archive      = {J_SIM},
  author       = {Annabel L. Davies and Theodoros Papakonstantinou and Adriani Nikolakopoulou and Gerta Rücker and Tobias Galla},
  doi          = {10.1002/sim.9346},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2091-2114},
  shortjournal = {Stat. Med.},
  title        = {Network meta-analysis and random walks},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Critical issues concerning the assessment sets in agreement
assessment. <em>SIM</em>, <em>41</em>(11), 2069–2089. (<a
href="https://doi.org/10.1002/sim.9345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since Bland and Altman&#39;s pioneering work, the assessment set has been an important constituent of agreement assessment. This article explores critical issues concerning the assessment set. First, we point out the fundamental differences between assessment approaches based on point estimation and those based on hypothesis testing related to the assessment set, and further present how the related disciplines choose between the two. Second, we argue that an assessment set with minimum volume should be preferred, as in the comparison of confidence intervals. Finally, we discuss assessment sets for the assessment of multiple methods, and propose two methods for the construction of assessment sets. Numerical examples further establish the applicability of these approaches.},
  archive      = {J_SIM},
  author       = {Chu-Lan Kao and Lin-An Chen and Chi-An Hu},
  doi          = {10.1002/sim.9345},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2069-2089},
  shortjournal = {Stat. Med.},
  title        = {Critical issues concerning the assessment sets in agreement assessment},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inference about ratios of age-standardized rates with
sampling errors in the population denominators for estimating both
rates. <em>SIM</em>, <em>41</em>(11), 2052–2068. (<a
href="https://doi.org/10.1002/sim.9344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A rate ratio (RR) is an important metric for comparing cancer risks among different subpopulations. Inference for RR becomes complicated when populations used for calculating age-standardized cancer rates involve sampling errors, a situation that arises increasingly often when sample surveys must be used to obtain the population data. We compare a few strategies of estimating the standardized RR and propose bias-corrected ratio estimators as well as the corresponding variance estimators and confidence intervals that simultaneously consider the sampling error in estimating populations and the traditional Poisson error in the occurrence of cancer case or death. Performance of the proposed methods is evaluated empirically based on simulation studies. An application to immigration disparities in cancer mortality among Hispanic Americans is discussed. Our simulation studies show that a bias-corrected RR estimator performs the best in reducing the bias without increasing the coefficient of variation; the proposed variance estimators for the RR estimators and associated confidence intervals are fairly accurate. Finding of our application study are both interesting and consistent with the common sense as well as the results of our simulation studies.},
  archive      = {J_SIM},
  author       = {Jiming Jiang and Yuanyuan Li and Thuan Nguyen and Mandi Yu},
  doi          = {10.1002/sim.9344},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2052-2068},
  shortjournal = {Stat. Med.},
  title        = {Inference about ratios of age-standardized rates with sampling errors in the population denominators for estimating both rates},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonparametric regression with right-censored covariate via
conditional density function. <em>SIM</em>, <em>41</em>(11), 2025–2051.
(<a href="https://doi.org/10.1002/sim.9343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Censoring often occurs in data collection. This article, considers nonparametric regression when the covariate is censored under general settings. In contrast to censoring in the response variable in survival analysis, regression with censored covariates is more challenging but less studied in the literature, especially for dependent censoring. We propose to estimate the regression function using conditional hazard rates. The asymptotic normality of our proposed estimator is established. Both theoretical results and simulation studies demonstrate that the proposed method is more efficient than the estimation based on complete observations and other methods, especially when the censoring rate is high. We illustrate the usefulness of the proposed method using a data set from the Framingham heart study and a data set from a randomized placebo-controlled clinical trial of the drug D-penicillamine.},
  archive      = {J_SIM},
  author       = {Hui Jiang and Lei Huang and Yingcun Xia},
  doi          = {10.1002/sim.9343},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2025-2051},
  shortjournal = {Stat. Med.},
  title        = {Nonparametric regression with right-censored covariate via conditional density function},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Depthgram: Visualizing outliers in high-dimensional
functional data with application to fMRI data exploration. <em>SIM</em>,
<em>41</em>(11), 2005–2024. (<a
href="https://doi.org/10.1002/sim.9342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional magnetic resonance imaging (fMRI) is a non-invasive technique that facilitates the study of brain activity by measuring changes in blood flow. Brain activity signals can be recorded during the alternate performance of given tasks, that is, task fMRI (tfMRI), or during resting-state, that is, resting-state fMRI (rsfMRI), as a measure of baseline brain activity. This contributes to the understanding of how the human brain is organized in functionally distinct subdivisions. fMRI experiments from high-resolution scans provide hundred of thousands of longitudinal signals for each individual, corresponding to brain activity measurements over each voxel of the brain along the duration of the experiment. In this context, we propose novel visualization techniques for high-dimensional functional data relying on depth-based notions that enable computationally efficient 2-dim representations of fMRI data, which elucidate sample composition, outlier presence, and individual variability. We believe that this previous step is crucial to any inferential approach willing to identify neuroscientific patterns across individuals, tasks, and brain regions. We present the proposed technique via an extensive simulation study, and demonstrate its application on a motor and language tfMRI experiment.},
  archive      = {J_SIM},
  author       = {Yasser Alemán-Gómez and Ana Arribas-Gil and Manuel Desco and Antonio Elías and Juan Romo},
  doi          = {10.1002/sim.9342},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2005-2024},
  shortjournal = {Stat. Med.},
  title        = {Depthgram: Visualizing outliers in high-dimensional functional data with application to fMRI data exploration},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robustness of κ-type coefficients for clinical agreement.
<em>SIM</em>, <em>41</em>(11), 1986–2004. (<a
href="https://doi.org/10.1002/sim.9341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The degree of inter-rater agreement is usually assessed through κ -type coefficients and the extent of agreement is then characterized by comparing the value of the adopted coefficient against a benchmark scale. Through two motivating examples, it is displayed the different behavior of some κ -type coefficients due to asymmetric distribution of marginal frequencies over categories. In order to investigate the robustness of four κ -type coefficients for nominal and ordinal classifications and of an inferential benchmarking procedure that, differently from straightforward benchmarking, does not neglect the influence of the experimental conditions, an extensive Monte Carlo simulation study has been conducted. The robustness has been investigated for several scenarios, differing for sample size, rating scale dimension, number of raters, frequency distribution of rater classifications, pattern of agreement across raters. Simulation results reveal an higher paradoxical behavior of Fleiss kappa and Conger kappa with ordinal rather than nominal classifications; the coefficients robustness improves with increasing sample size and number of raters for both nominal and ordinal classifications whereas robustness improves with rating scale dimension only for nominal classifications. By identifying the scenarios (ie, minimum sample size, number of raters, rating scale dimension) with acceptable robustness, this study provides guidelines about the design of robust agreement studies.},
  archive      = {J_SIM},
  author       = {Amalia Vanacore and Maria Sole Pellegrino},
  doi          = {10.1002/sim.9341},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1986-2004},
  shortjournal = {Stat. Med.},
  title        = {Robustness of κ-type coefficients for clinical agreement},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hypothesis test for causal mediation of time-to-event
mediator and outcome. <em>SIM</em>, <em>41</em>(11), 1971–1985. (<a
href="https://doi.org/10.1002/sim.9340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hepatitis B has been a well-documented risk factor of liver cancer and mortality. To what extent hepatitis B affects mortality through increasing liver cancer incidence is of research interest and remains to be studied. We formulate the research question as a hypothesis testing problem of causal mediation where both the mediator and the outcome are time-to-event variables. The problem is closely related to semicompeting risks because time to the intermediate event may be censored by an occurrence of the outcome. We propose two hypothesis testing methods: a weighted log-rank test (WLR) and an intersection-union test (IUT). A test statistic of the WLR is constructed by adapting a nonparametric estimator of the mediation effect; however, the test may be conservative regarding its Type I Error rate. To address this, we further propose the IUT, the test statistic of which is constructed under the composite null hypothesis. Asymptotic properties of the two tests are studied, showing that the IUT is a size test with better statistical power than the WLR. The theoretical properties are supported by extensive simulation studies under finite samples. Applying the proposed methods to the motivating hepatitis study, both WLR and IUT provided strong evidence that hepatitis B had a significant mediation effect on mortality via liver cancer incidence.},
  archive      = {J_SIM},
  author       = {Yen-Tsung Huang},
  doi          = {10.1002/sim.9340},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1971-1985},
  shortjournal = {Stat. Med.},
  title        = {Hypothesis test for causal mediation of time-to-event mediator and outcome},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal adaptive promising zone designs. <em>SIM</em>,
<em>41</em>(11), 1950–1970. (<a
href="https://doi.org/10.1002/sim.9339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop optimal decision rules for sample size re-estimation in two-stage adaptive group sequential clinical trials. It is usual for the initial sample size specification of such trials to be adequate to detect a realistic treatment effect δ a with good power, but not sufficient to detect the smallest clinically meaningful treatment effect δ min . Moreover it is difficult for the sponsors of such trials to make the up-front commitment needed to adequately power a study to detect δ min . It is easier to justify increasing the sample size if the interim data enter a so-called “promising zone” that ensures with high probability that the trial will succeed. We have considered promising zone designs that optimize unconditional power and promising zone designs that optimize conditional power and have discussed the tension that exists between these two objectives. Where there is reluctance to base the sample size re-estimation rule on the parameter we propose a Bayesian option whereby a prior distribution is assigned to the unknown treatment effect , which is then integrated out of the objective function with respect to its posterior distribution at the interim analysis.},
  archive      = {J_SIM},
  author       = {Cyrus Mehta and Apurva Bhingare and Lingyun Liu and Pralay Senchaudhuri},
  doi          = {10.1002/sim.9339},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1950-1970},
  shortjournal = {Stat. Med.},
  title        = {Optimal adaptive promising zone designs},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Split and combine simulation extrapolation algorithm to
correct geocoding coarsening of built environment exposures.
<em>SIM</em>, <em>41</em>(11), 1932–1949. (<a
href="https://doi.org/10.1002/sim.9338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A major challenge in studies relating built environment features to health is measurement error in exposure due to geocoding errors. Faulty geocodes in built environment data introduce errors to exposure assessments that may induce bias in the corresponding health effect estimates. In this study, we examine the distribution of the measurement error in measures constructed from point-referenced exposures, quantify the extent of bias in exposure effect estimates due to geocode coarsening, and extend the simulation extrapolation (SIMEX) method to correct the bias. The motivating example focuses on the association between children&#39;s body mass index and exposure to the junk food environment, represented by the number of junk food outlets within a buffer area near their schools. We show, algebraically and through simulation studies, that coarsening of food outlet coordinates results in exposure measurement errors that have heterogeneous variance and nonzero mean, and that the resulting bias in the health effect can be away from the null. The proposed SC-SIMEX procedure accommodates the nonstandard measurement error distribution, without requiring external data, and provides the best bias correction compared to other SIMEX approaches.},
  archive      = {J_SIM},
  author       = {Jung Y. Won and Emma V. Sanchez-Vaznaugh and Yuqi Zhai and Brisa N. Sánchez},
  doi          = {10.1002/sim.9338},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1932-1949},
  shortjournal = {Stat. Med.},
  title        = {Split and combine simulation extrapolation algorithm to correct geocoding coarsening of built environment exposures},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TITE-BOIN12: A bayesian phase i/II trial design to find the
optimal biological dose with late-onset toxicity and efficacy.
<em>SIM</em>, <em>41</em>(11), 1918–1931. (<a
href="https://doi.org/10.1002/sim.9337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of immunotherapies and targeted therapies, the focus of early phase clinical trials has shifted from finding the maximum tolerated dose to identifying the optimal biological dose (OBD), which maximizes the toxicity-efficacy trade-off. One major impediment to using adaptive designs to find OBD is that efficacy or/and toxicity are often late-onset, hampering the designs&#39; real-time decision rules for treating new patients. To address this issue, we propose the model-assisted TITE-BOIN12 design to find OBD with late-onset toxicity and efficacy. As an extension of the BOIN12 design, the TITE-BOIN12 design also uses utility to quantify the toxicity-efficacy trade-off. We consider two approaches, Bayesian data augmentation and an approximated likelihood method, to enable real-time decision making when some patients&#39; toxicity and efficacy outcomes are pending. Extensive simulations show that, compared to some existing designs, TITE-BOIN12 significantly shortens the trial duration while having comparable or higher accuracy to identify OBD and a lower risk of overdosing patients. To facilitate the use of the TITE-BOIN12 design, we develop a user-friendly software freely available at http://www.trialdesign.org .},
  archive      = {J_SIM},
  author       = {Yanhong Zhou and Ruitao Lin and J. Jack Lee and Daniel Li and Li Wang and Ruobing Li and Ying Yuan},
  doi          = {10.1002/sim.9337},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1918-1931},
  shortjournal = {Stat. Med.},
  title        = {TITE-BOIN12: A bayesian phase I/II trial design to find the optimal biological dose with late-onset toxicity and efficacy},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Landmarking 2.0: Bridging the gap between joint models and
landmarking. <em>SIM</em>, <em>41</em>(11), 1901–1917. (<a
href="https://doi.org/10.1002/sim.9336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of dynamic prediction with time-dependent covariates, given by biomarkers, repeatedly measured over time, has received much attention over the last decades. Two contrasting approaches have become in widespread use. The first is joint modeling, which attempts to jointly model the longitudinal markers and the event time. The second is landmarking, a more pragmatic approach that avoids modeling the marker process. Landmarking has been shown to be less efficient than correctly specified joint models in simulation studies, when data are generated from the joint model. When the mean model is misspecified, however, simulation has shown that joint models may be inferior to landmarking. The objective of this article is to develop methods that improve the predictive accuracy of landmarking, while retaining its relative simplicity and robustness. We start by fitting a working longitudinal model for the biomarker, including a temporal correlation structure. Based on that model, we derive a predictable time-dependent process representing the expected value of the biomarker after the landmark time, and we fit a time-dependent Cox model based on the predictable time-dependent covariate. Dynamic predictions based on this approach for new patients can be obtained by first deriving the expected values of the biomarker, given the measured values before the landmark time point, and then calculating the predicted probabilities based on the time-dependent Cox model. We illustrate the approach in predicting overall survival in liver cirrhosis patients based on prothrombin index.},
  archive      = {J_SIM},
  author       = {Hein Putter and Hans C. van Houwelingen},
  doi          = {10.1002/sim.9336},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1901-1917},
  shortjournal = {Stat. Med.},
  title        = {Landmarking 2.0: Bridging the gap between joint models and landmarking},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Assessing reproducibility of high-throughput experiments in
the case of missing data. <em>SIM</em>, <em>41</em>(10), 1884–1899. (<a
href="https://doi.org/10.1002/sim.9334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-throughput experiments are an essential part of modern biological and biomedical research. The outcomes of high-throughput biological experiments often have a lot of missing observations due to signals below detection levels. For example, most single-cell RNA-seq (scRNA-seq) protocols experience high levels of dropout due to the small amount of starting material, leading to a majority of reported expression levels being zero. Though missing data contain information about reproducibility, they are often excluded in the reproducibility assessment, potentially generating misleading assessments. In this article, we develop a regression model to assess how the reproducibility of high-throughput experiments is affected by the choices of operational factors (eg, platform or sequencing depth) when a large number of measurements are missing. Using a latent variable approach, we extend correspondence curve regression, a recently proposed method for assessing the effects of operational factors to reproducibility, to incorporate missing values. Using simulations, we show that our method is more accurate in detecting differences in reproducibility than existing measures of reproducibility. We illustrate the usefulness of our method using a single-cell RNA-seq dataset collected on HCT116 cells. We compare the reproducibility of different library preparation platforms and study the effect of sequencing depth on reproducibility, thereby determining the cost-effective sequencing depth that is required to achieve sufficient reproducibility.},
  archive      = {J_SIM},
  author       = {Roopali Singh and Feipeng Zhang and Qunhua Li},
  doi          = {10.1002/sim.9334},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1884-1899},
  shortjournal = {Stat. Med.},
  title        = {Assessing reproducibility of high-throughput experiments in the case of missing data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constrained randomization and statistical inference for
multi-arm parallel cluster randomized controlled trials. <em>SIM</em>,
<em>41</em>(10), 1862–1883. (<a
href="https://doi.org/10.1002/sim.9333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A practical limitation of cluster randomized controlled trials (cRCTs) is that the number of available clusters may be small, resulting in an increased risk of baseline imbalance under simple randomization. Constrained randomization overcomes this issue by restricting the allocation to a subset of randomization schemes where sufficient overall covariate balance across comparison arms is achieved. However, for multi-arm cRCTs, several design and analysis issues pertaining to constrained randomization have not been fully investigated. Motivated by an ongoing multi-arm cRCT, we elaborate the method of constrained randomization and provide a comprehensive evaluation of the statistical properties of model-based and randomization-based tests under both simple and constrained randomization designs in multi-arm cRCTs, with varying combinations of design and analysis-based covariate adjustment strategies. In particular, as randomization-based tests have not been extensively studied in multi-arm cRCTs, we additionally develop most-powerful randomization tests under the linear mixed model framework for our comparisons. Our results indicate that under constrained randomization, both model-based and randomization-based analyses could gain power while preserving nominal type I error rate, given proper analysis-based adjustment for the baseline covariates. Randomization-based analyses, however, are more robust against violations of distributional assumptions. The choice of balance metrics and candidate set sizes and their implications on the testing of the pairwise and global hypotheses are also discussed. Finally, we caution against the design and analysis of multi-arm cRCTs with an extremely small number of clusters, due to insufficient degrees of freedom and the tendency to obtain an overly restricted randomization space.},
  archive      = {J_SIM},
  author       = {Yunji Zhou and Elizabeth L. Turner and Ryan A. Simmons and Fan Li},
  doi          = {10.1002/sim.9333},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1862-1883},
  shortjournal = {Stat. Med.},
  title        = {Constrained randomization and statistical inference for multi-arm parallel cluster randomized controlled trials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Common scale minimal sufficient balance: An improved method
for covariate-adaptive randomization based on the wilcoxon-mann-whitney
odds ratio statistic. <em>SIM</em>, <em>41</em>(10), 1846–1861. (<a
href="https://doi.org/10.1002/sim.9332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimal sufficient balance (MSB) is a recently suggested method for adaptively controlling covariate imbalance in randomized controlled trials in a manner which reduces the impact on randomness of allocation over other approaches by only intervening when the imbalance is sufficiently significant. Despite its improvements, the approach is unable to consider the relative clinical importance or magnitude of imbalance in each covariate weight, and ignores any imbalance which is not statistically significant, even when these imbalances may collectively justify intervention. We propose the common scale MSB (CS-MSB) method which addresses these limitations, and present simulation studies comparing our proposed method to MSB. We demonstrate that CS-MSB requires less intervention than MSB to achieve the same level of covariate balance, and does not adversely impact either statistical power or Type-I error.},
  archive      = {J_SIM},
  author       = {Hannah Johns and Dominic Italiano and Bruce Campbell and Leonid Churilov},
  doi          = {10.1002/sim.9332},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1846-1861},
  shortjournal = {Stat. Med.},
  title        = {Common scale minimal sufficient balance: An improved method for covariate-adaptive randomization based on the wilcoxon-mann-whitney odds ratio statistic},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient penalized estimation approach for
semiparametric linear transformation models with interval-censored data.
<em>SIM</em>, <em>41</em>(10), 1829–1845. (<a
href="https://doi.org/10.1002/sim.9331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider efficient estimation of flexible transformation models with interval-censored data. To reduce the dimension of semiparametric models, the unknown monotone function is approximated via a monotone B -spline. A penalization technique is used to provide computationally efficient estimation of all parameters. To accomplish model fitting and inference, an easy to implement nested iterative expectation-maximization (EM) algorithm is developed for estimation, and a simple variance-covariance estimation approach is proposed which makes large-sample inference for the regression parameters possible. Theoretically, we show that the estimator of the unknown monotone increasing function achieves the optimal rate of convergence, and the estimators of the regression parameters are asymptotically normal and efficient under the appropriate selection of the order of the smoothing parameter and the knots of the spline space. The proposed penalized procedure is assessed through extensive numerical experiments and implemented in R package PenIC . The proposed methodology is further illustrated via a signal tandmobiel study.},
  archive      = {J_SIM},
  author       = {Minggen Lu and Yan Liu and Chin-Shang Li and Jianguo Sun},
  doi          = {10.1002/sim.9331},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1829-1845},
  shortjournal = {Stat. Med.},
  title        = {An efficient penalized estimation approach for semiparametric linear transformation models with interval-censored data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating the health effects of environmental mixtures
using principal stratification. <em>SIM</em>, <em>41</em>(10),
1815–1828. (<a href="https://doi.org/10.1002/sim.9330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The control of ambient air quality in the United States has been a major public health success since the passing of the Clean Air Act, with particulate matter (PM) reductions resulting in an estimated 160 000 premature deaths prevented in 2010 alone. Currently, public policy is oriented around lowering the levels of individual pollutants and this focus has driven the nature of much epidemiological research. Recently, attention has been given to viewing air pollution as a complex mixture and to developing a multi-pollutant approach to controlling ambient concentrations. We present a statistical approach for estimating the health impacts of complex environmental mixtures using a mixture-altering contrast, which is any comparison, intervention, policy, or natural experiment that changes a mixture&#39;s composition. We combine the notion of mixture-altering contrasts with sliced inverse regression, propensity score matching, and principal stratification to assess the health effects of different air pollution chemical mixtures. We demonstrate the application of this approach in an analysis of the health effects of wildfire PM air pollution in the Western US.},
  archive      = {J_SIM},
  author       = {Roger D. Peng and Jia C. Liu and Meredith C. McCormack and Loretta J. Mickley and Michelle L. Bell},
  doi          = {10.1002/sim.9330},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1815-1828},
  shortjournal = {Stat. Med.},
  title        = {Estimating the health effects of environmental mixtures using principal stratification},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust inference on effects attributable to mediators: A
controlled-direct-effect-based approach for causal effect decomposition
with multiple mediators. <em>SIM</em>, <em>41</em>(10), 1797–1814. (<a
href="https://doi.org/10.1002/sim.9329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effect decomposition is a critical technique for mechanism investigation in settings with multiple causally ordered mediators. Causal mediation analysis is a standard method for effect decomposition, but the assumptions required for the identification process are extremely strong. Moreover, mediation analysis focuses on addressing mediating mechanisms rather than interacting mechanisms. Mediation and interaction for mediators both contribute to the occurrence of disease, and therefore unifying mediation and interaction in effect decomposition is important to causal mechanism investigation. By extending the framework of controlled direct effects, this study proposes the effect attributable to mediators (EAM) as a novel measure for effect decomposition. For policymaking, EAM represents how much an effect can be eliminated by setting mediators to certain values. From the perspective of mechanism investigation, EAM contains information about how much a particular mediator or set of mediators is involved in the causal mechanism through mediation, interaction, or both. EAM is more appropriate than the conventional path-specific effect for application in clinical or medical studies. The assumptions of EAM for identification are considerably weaker than those of causal mediation analysis. We develop a semiparametric estimator of EAM with robustness to model misspecification. The asymptotic property is fully realized. We applied EAM to assess the magnitude of the effect of hepatitis C virus infection on mortality, which was eliminated by controlling alanine aminotransferase and treating hepatocellular carcinoma.},
  archive      = {J_SIM},
  author       = {An-Shun Tai and Yi-Juan Du and Sheng-Hsuan Lin},
  doi          = {10.1002/sim.9329},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1797-1814},
  shortjournal = {Stat. Med.},
  title        = {Robust inference on effects attributable to mediators: A controlled-direct-effect-based approach for causal effect decomposition with multiple mediators},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A shared-parameter location-scale mixed model to link the
responsivity in self-initiated event reports and the event-contingent
ecological momentary assessments. <em>SIM</em>, <em>41</em>(10),
1780–1796. (<a href="https://doi.org/10.1002/sim.9328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the issue of (non-) responsivity of self-initiated assessments in Ecological Momentary Assessment (EMA) or other mobile health (mHealth) studies, where subjects are instructed to self-initiate reports when experiencing defined events, for example, smoking. Since such reports are self-initiated, the frequency and determinants of nonresponse to these event reports is usually unknown, however it may be suspected that nonresponse of such self-initiated reports is not random. In this case, existing methods for missing data may be insufficient in the modeling of these observed self-initiated reports. In certain EMA studies, random prompts, distinct from the self-initiated reports, may be converted to event reports. For example, such a conversion can occur if during a random prompt a subject is assessed about the event (eg, smoking) and it is determined that the subject is engaging in the event at the time of the prompt. Such converted prompts can provide some information about the subject&#39;s non-responsivity of event reporting. Furthermore, such non-responsivity can be associated with the primary longitudinal EMA outcome (eg, mood) in which case a joint modeling of the non-responsivity and the mood outcome is possible. Here, we propose a shared-parameter location-scale model to link the primary outcome model for mood and a model for subjects&#39; non-responsivity by shared random effects which characterize a subject&#39;s mood level, mood change pattern, and mood variability. Via simulations and real data analysis, our proposed model is shown to be more informative, have better coverage of parameters, and provide better fit to the data than more conventional models.},
  archive      = {J_SIM},
  author       = {Qianheng Ma and Robin J. Mermelstein and Donald Hedeker},
  doi          = {10.1002/sim.9328},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1780-1796},
  shortjournal = {Stat. Med.},
  title        = {A shared-parameter location-scale mixed model to link the responsivity in self-initiated event reports and the event-contingent ecological momentary assessments},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comparison of estimation methods adjusting for selection
bias in adaptive enrichment designs with time-to-event endpoints.
<em>SIM</em>, <em>41</em>(10), 1767–1779. (<a
href="https://doi.org/10.1002/sim.9327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive enrichment designs in clinical trials have been developed to enhance drug developments. They permit, at interim analyses during the trial, to select the sub-populations that benefits the most from the treatment. Because of this selection, the naive maximum likelihood estimation of the treatment effect, commonly used in classical randomized controlled trials, is biased. In the literature, several methods have been proposed to obtain a better estimation of the treatments&#39; effects in such contexts. To date, most of the works have focused on normally distributed endpoints, and some estimators have been proposed for time-to-event endpoints but they have not all been compared side-by-side. In this work, we conduct an extensive simulation study, inspired by a real case-study in heart failure, to compare the maximum-likelihood estimator (MLE) with an unbiased estimator, shrinkage estimators, and bias-adjusted estimators for the estimation of the treatment effect with time-to-event data. The performances of the estimators are evaluated in terms of bias, variance, and mean squared error. Based on the results, along with the MLE, we recommend to provide the unbiased estimator and the single-iteration bias-adjusted estimator: the former completely eradicates the selection bias, but is highly variable with respect to a naive estimator; the latter is less biased than the MLE estimator and only slightly more variable.},
  archive      = {J_SIM},
  author       = {Fulvio Di Stefano and Matthieu Pannaux and Anne Correges and Stephanie Galtier and Veronique Robert and Gaelle Saint-Hilary},
  doi          = {10.1002/sim.9327},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1767-1779},
  shortjournal = {Stat. Med.},
  title        = {A comparison of estimation methods adjusting for selection bias in adaptive enrichment designs with time-to-event endpoints},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model misspecification in stepped wedge trials: Random
effects for time or treatment. <em>SIM</em>, <em>41</em>(10), 1751–1766.
(<a href="https://doi.org/10.1002/sim.9326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed models are commonly used to analyze stepped wedge trials (SWTs) to account for clustering and repeated measures on clusters. One critical issue researchers face is whether to include a random time effect or a random treatment effect. When the wrong model is chosen, inference on the treatment effect may be invalid. We explore asymptotic and finite-sample convergence of variance component estimates when the model is misspecified and how misspecification affects the estimated variance of the treatment effect. For asymptotic results, we rely on analytical solutions rather than simulation studies, which allow us to succinctly describe the convergence of misspecified estimates, even though there are multiple roots for each misspecified model. We found that both direction and magnitude of the bias associated with model-based standard errors depends on the study design and magnitude of the true variance components. We identify some scenarios in which choosing the wrong random effect has a large impact on model-based inference. However, many trends depend on trial design and assumptions about the true correlation structure, so we provide tools for researchers to investigate specific scenarios of interest. We use data from an SWT on disinvesting from weekend services in hospital wards to demonstrate how these results can be applied as a sensitivity analysis, which quantifies the impact of misspecification under a variety of settings and directly compares the potential consequences of different modeling choices. Our results will provide guidance for prespecified model choices and supplement sensitivity analyses to inform confidence in the validity of results.},
  archive      = {J_SIM},
  author       = {Emily C. Voldal and Fan Xia and Avi Kenny and Patrick J. Heagerty and James P. Hughes},
  doi          = {10.1002/sim.9326},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1751-1766},
  shortjournal = {Stat. Med.},
  title        = {Model misspecification in stepped wedge trials: Random effects for time or treatment},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A modified self-controlled case series method for
event-dependent exposures and high event-related mortality, with
application to COVID-19 vaccine safety. <em>SIM</em>, <em>41</em>(10),
1735–1750. (<a href="https://doi.org/10.1002/sim.9325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a modified self-controlled case series (SCCS) method to handle both event-dependent exposures and high event-related mortality. This development is motivated by an epidemiological study undertaken in France to quantify potential risks of cardiovascular events associated with COVID-19 vaccines. Event-dependence of vaccinations, and high event-related mortality, are likely to arise in other SCCS studies of COVID-19 vaccine safety. Using this case study and simulations to broaden its scope, we explore these features and the biases they may generate, implement the modified SCCS model, illustrate some of the properties of this model, and develop a new test for presence of a dose effect. The model we propose has wider application, notably when the event of interest is death.},
  archive      = {J_SIM},
  author       = {Yonas Ghebremichael-Weldeselassie and Marie Joëlle Jabagi and Jérémie Botton and Marion Bertrand and Bérangère Baricault and Jérôme Drouin and Alain Weill and Mahmoud Zureik and Rosemary Dray-Spira and Paddy Farrington},
  doi          = {10.1002/sim.9325},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1735-1750},
  shortjournal = {Stat. Med.},
  title        = {A modified self-controlled case series method for event-dependent exposures and high event-related mortality, with application to COVID-19 vaccine safety},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correction. <em>SIM</em>, <em>41</em>(9), 1733–1734. (<a
href="https://doi.org/10.1002/sim.9378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Stephen Lake},
  doi          = {10.1002/sim.9378},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1733-1734},
  shortjournal = {Stat. Med.},
  title        = {Correction},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comments on “online estimation of the case fatality rate
using a run-off triangle data approach: An application to the korean
MERS outbreak in 2015” by sungim lee and johan lim published in
statistics in medicine (vol. 38, 2644-2679, 2019). <em>SIM</em>,
<em>41</em>(9), 1728–1732. (<a
href="https://doi.org/10.1002/sim.9123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Byungwon Kim and Seonghong Kim and Sungkyu Jung and Woncheol Jang and Johan Lim},
  doi          = {10.1002/sim.9123},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1728-1732},
  shortjournal = {Stat. Med.},
  title        = {Comments on “Online estimation of the case fatality rate using a run-off triangle data approach: An application to the korean MERS outbreak in 2015” by sungim lee and johan lim published in statistics in medicine (vol. 38, 2644-2679, 2019)},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correcting for partial verification bias in diagnostic
accuracy studies: A tutorial using r. <em>SIM</em>, <em>41</em>(9),
1709–1727. (<a href="https://doi.org/10.1002/sim.9311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnostic tests play a crucial role in medical care. Thus any new diagnostic tests must undergo a thorough evaluation. New diagnostic tests are evaluated in comparison with the respective gold standard tests. The performance of binary diagnostic tests is quantified by accuracy measures, with sensitivity and specificity being the most important measures. In any diagnostic accuracy study, the estimates of these measures are often biased owing to selective verification of the patients, which is referred to as partial verification bias. Several methods for correcting partial verification bias are available depending on the scale of the index test, target outcome, and missing data mechanism. However, these are not easily accessible to the researchers due to the complexity of the methods. This article aims to provide a brief overview of the methods available to correct for partial verification bias involving a binary diagnostic test and provide a practical tutorial on how to implement the methods using the statistical programming language R.},
  archive      = {J_SIM},
  author       = {Wan Nor Arifin and Umi Kalsom Yusof},
  doi          = {10.1002/sim.9311},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1709-1727},
  shortjournal = {Stat. Med.},
  title        = {Correcting for partial verification bias in diagnostic accuracy studies: A tutorial using r},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian set of best dynamic treatment regimes: Construction
and sample size calculation for SMARTs with binary outcomes.
<em>SIM</em>, <em>41</em>(9), 1688–1708. (<a
href="https://doi.org/10.1002/sim.9323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential, multiple assignment, randomized trials (SMARTs) compare sequences of treatment decision rules called dynamic treatment regimes (DTRs). In particular, the Adaptive Treatment for Alcohol and Cocaine Dependence (ENGAGE) SMART aimed to determine the best DTRs for patients with a substance use disorder. While many authors have focused on a single pairwise comparison, addressing the main goal involves comparisons of &gt;2 DTRs. For complex comparisons, there is a paucity of methods for binary outcomes. We fill this gap by extending the multiple comparisons with the best (MCB) methodology to the Bayesian binary outcome setting. The set of best is constructed based on simultaneous credible intervals. A substantial challenge for power analysis is the correlation between outcome estimators for distinct DTRs embedded in SMARTs due to overlapping subjects. We address this using Robins&#39; G-computation formula to take a weighted average of parameter draws obtained via simulation from the parameter posteriors. We use non-informative priors and work with the exact distribution of parameters avoiding unnecessary normality assumptions and specification of the correlation matrix of DTR outcome summary statistics. We conduct simulation studies for both the construction of a set of optimal DTRs using the Bayesian MCB procedure and the sample size calculation for two common SMART designs. We illustrate our method on the ENGAGE SMART. The R package SMARTbayesR for power calculations is freely available on the Comprehensive R Archive Network (CRAN) repository. An RShiny app is available at https://wilart.shinyapps.io/shinysmartbayesr/ .},
  archive      = {J_SIM},
  author       = {William J. Artman and Brent A. Johnson and Kevin G. Lynch and James R. McKay and Ashkan Ertefaie},
  doi          = {10.1002/sim.9323},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1688-1708},
  shortjournal = {Stat. Med.},
  title        = {Bayesian set of best dynamic treatment regimes: Construction and sample size calculation for SMARTs with binary outcomes},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust nonparametric integrative analysis to decipher
heterogeneity and commonality across subgroups using sparse boosting.
<em>SIM</em>, <em>41</em>(9), 1658–1687. (<a
href="https://doi.org/10.1002/sim.9322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many biomedical problems, data are often heterogeneous, with samples spanning multiple patient subgroups, where different subgroups may have different disease subtypes, stages, or other medical contexts. These subgroups may be related, but they are also expected to have differences with respect to the underlying biology. The heterogeneous data presents a precious opportunity to explore the heterogeneities and commonalities between related subgroups. Unfortunately, effective statistical analysis methods are still lacking. Recently, several novel methods based on integrative analysis have been proposed to tackle this challenging problem. Despite promising results, the existing studies are still limited by ignoring data contamination and making strict assumptions of linear effects of covariates on response. As such, we develop a robust nonparametric integrative analysis approach to identify heterogeneity and commonality, as well as select important covariates and estimate covariate effects. Possible data contamination is accommodated by adopting the Cauchy loss function, and a nonparametric model is built to accommodate nonlinear effects. The proposed approach is based on a sparse boosting technique. The advantages of the proposed approach are demonstrated in extensive simulations. The analysis of The Cancer Genome Atlas data on glioblastoma multiforme and lung adenocarcinoma shows that the proposed approach makes biologically meaningful findings with satisfactory prediction.},
  archive      = {J_SIM},
  author       = {Zihan Li and Ziye Luo and Yifan Sun},
  doi          = {10.1002/sim.9322},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1658-1687},
  shortjournal = {Stat. Med.},
  title        = {Robust nonparametric integrative analysis to decipher heterogeneity and commonality across subgroups using sparse boosting},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-step hypothesis testing to detect gene-environment
interactions in a genome-wide scan with a survival endpoint.
<em>SIM</em>, <em>41</em>(9), 1644–1657. (<a
href="https://doi.org/10.1002/sim.9319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defined by their genetic profile, individuals may exhibit differential clinical outcomes due to an environmental exposure. Identifying subgroups based on specific exposure-modifying genes can lead to targeted interventions and focused studies. Genome-wide interaction scans (GWIS) can be performed to identify such genes, but these scans typically suffer from low power due to the large multiple testing burden. We provide a novel framework for powerful two-step hypothesis tests for GWIS with a time-to-event endpoint under the Cox proportional hazards model. In the Cox regression setting, we develop an approach that prioritizes genes for Step-2 testing based on a carefully constructed Step-1 screening procedure. Simulation results demonstrate this two-step approach can lead to substantially higher power for identifying gene-environment ( ) interactions compared to the standard GWIS while preserving the family wise error rate over a range of scenarios. In a taxane-anthracycline chemotherapy study for breast cancer patients, the two-step approach identifies several gene expression by treatment interactions that would not be detected using the standard GWIS.},
  archive      = {J_SIM},
  author       = {Eric S. Kawaguchi and Gang Li and Juan Pablo Lewinger and W. James Gauderman},
  doi          = {10.1002/sim.9319},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1644-1657},
  shortjournal = {Stat. Med.},
  title        = {Two-step hypothesis testing to detect gene-environment interactions in a genome-wide scan with a survival endpoint},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preserving data privacy when using multi-site data to
estimate individualized treatment rules. <em>SIM</em>, <em>41</em>(9),
1627–1643. (<a href="https://doi.org/10.1002/sim.9318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision medicine is a rapidly expanding area of health research wherein patient level information is used to inform treatment decisions. A statistical framework helps to formalize the individualization of treatment decisions that characterize personalized management plans. Numerous methods have been proposed to estimate individualized treatment rules that optimize expected patient outcomes, many of which have desirable properties such as robustness to model misspecification. However, while individual data are essential in this context, there may be concerns about data confidentiality, particularly in multi-center studies where data are shared externally. To address this issue, we compared two approaches to privacy preservation: (i) data pooling, which is a covariate microaggregation technique and (ii) distributed regression. These approaches were combined with the doubly robust yet user-friendly method of dynamic weighted ordinary least squares to estimate individualized treatment rules. In simulations, we extensively evaluated the performance of the methods in estimating the parameters of the decision rule under different assumptions. The results demonstrate that double robustness is not maintained in data pooling setting and that this can result in bias, whereas the distributed regression provides good performance. We illustrate the methods via an analysis of optimal Warfarin dosing using data from the International Warfarin Consortium.},
  archive      = {J_SIM},
  author       = {Coraline Danieli and Erica E. M. Moodie},
  doi          = {10.1002/sim.9318},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1627-1643},
  shortjournal = {Stat. Med.},
  title        = {Preserving data privacy when using multi-site data to estimate individualized treatment rules},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An order restricted multi-arm multi-stage clinical trial
design. <em>SIM</em>, <em>41</em>(9), 1613–1626. (<a
href="https://doi.org/10.1002/sim.9314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One family of designs that can noticeably improve efficiency in later stages of drug development are multi-arm multi-stage (MAMS) designs. They allow several arms to be studied concurrently and gain efficiency by dropping poorly performing treatment arms during the trial as well as by allowing to stop early for benefit. Conventional MAMS designs were developed for the setting, in which treatment arms are independent and hence can be inefficient when an order in the effects of the arms can be assumed (eg, when considering different treatment durations or different doses). In this work, we extend the MAMS framework to incorporate the order of treatment effects when no parametric dose-response or duration-response model is assumed. The design can identify all promising treatments with high probability. We show that the design provides strong control of the family-wise error rate and illustrate the design in a study of symptomatic asthma. Via simulations we show that the inclusion of the ordering information leads to better decision-making compared to a fixed sample and a MAMS design. Specifically, in the considered settings, reductions in sample size of around 15% were achieved in comparison to a conventional MAMS design.},
  archive      = {J_SIM},
  author       = {Alessandra Serra and Pavel Mozgunov and Thomas Jaki},
  doi          = {10.1002/sim.9314},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1613-1626},
  shortjournal = {Stat. Med.},
  title        = {An order restricted multi-arm multi-stage clinical trial design},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of composite endpoints with component-wise
censoring in the presence of differential visit schedules. <em>SIM</em>,
<em>41</em>(9), 1599–1612. (<a
href="https://doi.org/10.1002/sim.9312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composite endpoints are very common in clinical research, such as recurrence-free survival in oncology research, defined as the earliest of either death or disease recurrence. Because of the way data are collected in such studies, component-wise censoring is common, where, for example, recurrence is an interval-censored event and death is a right-censored event. However, a common way to analyze such component-wise censored composite endpoints is to treat them as right-censored, with the date at which the non-fatal event was detected serving as the date the event occurred. This approach is known to introduce upward bias when the Kaplan-Meier estimator is applied, but has more complex impact on semi-parametric regression approaches. In this article we compare the performance of the Cox model estimators for right-censored data and the Cox model estimators for interval-censored data in the context of component-wise censored data where the visit process differs across levels of a covariate of interest, a common scenario in observational data. We additionally examine estimators of the cause-specific hazard when applied to the individual components of such component-wise censored composite endpoints. We found that when visit schedules differed according to levels of a covariate of interest, the Cox model estimators for right-censored data and the estimators for cause-specific hazards were increasingly biased as the frequency of visits decreased. The Cox model estimator for interval-censored data with censoring at the last disease-free date is recommended for use in the presence of differential visit schedules.},
  archive      = {J_SIM},
  author       = {Anne A. Eaton and Emily C. Zabor},
  doi          = {10.1002/sim.9312},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1599-1612},
  shortjournal = {Stat. Med.},
  title        = {Analysis of composite endpoints with component-wise censoring in the presence of differential visit schedules},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IDNetwork: A deep illness-death network based on multi-state
event history process for disease prognostication. <em>SIM</em>,
<em>41</em>(9), 1573–1598. (<a
href="https://doi.org/10.1002/sim.9310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-state models can capture the different patterns of disease evolution. In particular, the illness-death model is used to follow disease progression from a healthy state to an intermediate state of the disease and to a death-related final state. We aim to use those models in order to adapt treatment decisions according to the evolution of the disease. In state-of-the art methods, the risks of transition between the states are modeled via (semi-) Markov processes and transition-specific Cox proportional hazard (P.H.) models. The Cox P.H. model assumes that each variable makes a linear contribution to the model, but the relationship between covariates and risks can be more complex in clinical situations. To address this challenge, we propose a neural network architecture called illness-death network (IDNetwork) that relaxes the linear Cox P.H. assumption within an illness-death process. IDNetwork employs a multi-task architecture and uses a set of fully connected subnetworks in order to learn the probabilities of transition. Through simulations, we explore different configurations of the architecture and demonstrate the added value of our model. IDNetwork significantly improves the predictive performance compared to state-of-the-art methods on a simulated data set, on two clinical trials for patients with colon cancer and on a real-world data set in breast cancer.},
  archive      = {J_SIM},
  author       = {Aziliz Cottin and Nicolas Pecuchet and Marine Zulian and Agathe Guilloux and Sandrine Katsahian},
  doi          = {10.1002/sim.9310},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1573-1598},
  shortjournal = {Stat. Med.},
  title        = {IDNetwork: A deep illness-death network based on multi-state event history process for disease prognostication},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient integration of aggregate data and individual
participant data in one-way mixed models. <em>SIM</em>, <em>41</em>(9),
1555–1572. (<a href="https://doi.org/10.1002/sim.9307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Often both aggregate data (AD) studies and individual participant data (IPD) studies are available for specific treatments. Combining these two sources of data could improve the overall meta-analytic estimates of treatment effects. Moreover, often for some studies with AD, the associated IPD maybe available, albeit at some extra effort or cost to the analyst. We propose a method for combining treatment effects across trials when the response is from the exponential family of distribution and hence a generalized linear model structure can be used. We consider the case when treatment effects are fixed and common across studies. Using the proposed combination method, we study the relative efficiency of analyzing all IPD studies vs combining various percentages of AD and IPD studies. For many different models, design constraints under which the AD estimators are the IPD estimators, and hence fully efficient, are known. For such models, we advocate a selection procedure that chooses AD studies over IPD studies in a manner that force least departure from design constraints and hence ensures an efficient combined AD and IPD estimator.},
  archive      = {J_SIM},
  author       = {Neha Agarwala and Junyong Park and Anindya Roy},
  doi          = {10.1002/sim.9307},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1555-1572},
  shortjournal = {Stat. Med.},
  title        = {Efficient integration of aggregate data and individual participant data in one-way mixed models},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effect modification in anchored indirect treatment
comparison: Comments on “matching-adjusted indirect comparisons:
Application to time-to-event data.” <em>SIM</em>, <em>41</em>(8),
1541–1553. (<a href="https://doi.org/10.1002/sim.9286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Antonio Remiro-Azócar and Anna Heath and Gianluca Baio},
  doi          = {10.1002/sim.9286},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1541-1553},
  shortjournal = {Stat. Med.},
  title        = {Effect modification in anchored indirect treatment comparison: comments on “Matching-adjusted indirect comparisons: application to time-to-event data”},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sequential modeling for a class of reference-based
imputation methods in clinical trials with quantitative or binary
outcomes. <em>SIM</em>, <em>41</em>(8), 1525–1540. (<a
href="https://doi.org/10.1002/sim.9303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider treatment effect estimation in a randomized clinical trial with longitudinally measured quantitative or categorical outcomes. To handle missing data, we usually assume missing at random and then conduct sensitivity analysis for missing not at random. In the literature, several reference-based imputation methods, including “jump to reference” (J2R) and “copy reference” (CR), have been commonly used for conducting sensitivity analysis. J2R assumes the mean effect profile of patients who discontinue the investigative treatment jumps to that of the patients in the reference group after discontinuation, while CR assumes the conditional mean effect profile given the status prior to the time of discontinuation copies that of the patients in the reference group after discontinuation. In this article, we propose a novel, wide class of reference-based imputation methods for conducting sensitivity analysis, which includes J2R and CR as two extreme ends. The framework is motivated by the thought that the investigative treatment may have no, partially, or fully carried-over effect after deviation from the assigned treatment (eg, discontinue the assigned treatment or change to a different treatment). Further, we show that the proposed reference-based imputation methods can be implemented through sequential modeling. This property ensures that the methods can be applied to clinical trials with either quantitative or categorical outcomes. We use both causal-inference arguments and numerical examples to demonstrate the performance of the proposed methods.},
  archive      = {J_SIM},
  author       = {Yixin Fang and Man Jin},
  doi          = {10.1002/sim.9303},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1525-1540},
  shortjournal = {Stat. Med.},
  title        = {Sequential modeling for a class of reference-based imputation methods in clinical trials with quantitative or binary outcomes},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Causal inference methods for vaccine sieve analysis with
effect modification. <em>SIM</em>, <em>41</em>(8), 1513–1524. (<a
href="https://doi.org/10.1002/sim.9302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The protective effects of vaccines may vary depending on individual characteristics, such as age. Traditionally, such effect modification has been examined with subgroup analyses or inclusion of cross-product terms in regression frameworks. However, in many vaccine settings, effect modification may also depend on the infecting pathogen&#39;s characteristics, which are measured postrandomization. Sieve analysis examines whether such effects are present by combining pathogen genetic sequence information with individual-level data and can generate new hypotheses on the pathways whereby vaccines provide protection. In this article, we develop a causal framework for evaluating effect modification in the context of sieve analysis. Our approach can be used to assess the magnitude of sieve effects and, in particular, whether these effects are modified by individual-level characteristics. Our method accounts for difficulties occurring in real-world data analysis, such as competing risks, nonrandomized treatments, and differential dropout. Our approach also integrates modern machine learning techniques. We demonstrate the validity and efficiency of our approach in simulation studies and apply the methodology to a malaria vaccine study.},
  archive      = {J_SIM},
  author       = {Guandong Yang and Laura B. Balzer and David Benkeser},
  doi          = {10.1002/sim.9302},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1513-1524},
  shortjournal = {Stat. Med.},
  title        = {Causal inference methods for vaccine sieve analysis with effect modification},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Power analysis for stepped wedge trials with multiple
interventions. <em>SIM</em>, <em>41</em>(8), 1498–1512. (<a
href="https://doi.org/10.1002/sim.9301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped wedge design (SWD) trials are cluster randomized trials that feature staggered, unidirectional cross-over between treatment conditions. Existing literature on power for SWDs focuses primarily on designs with two conditions, typically a control and an intervention condition. However, SWDs with more than one treatment condition are being proposed and conducted. We present a linear mixed model for SWDs with two or more interventions, including both multiarm and factorial designs. We derive standard errors of the intervention effect coefficients, and present power calculation methods. We consider both repeated cross-sectional and cohort designs. Design features, with a focus on treatment allocations, are examined to determine their impact on power.},
  archive      = {J_SIM},
  author       = {Phillip Sundin and Catherine M. Crespi},
  doi          = {10.1002/sim.9301},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1498-1512},
  shortjournal = {Stat. Med.},
  title        = {Power analysis for stepped wedge trials with multiple interventions},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal sampling for design-based estimators of regression
models. <em>SIM</em>, <em>41</em>(8), 1482–1497. (<a
href="https://doi.org/10.1002/sim.9300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-phase designs measure variables of interest on a subcohort where the outcome and covariates are readily available or cheap to collect on all individuals in the cohort. Given limited resource availability, it is of interest to find an optimal design that includes more informative individuals in the final sample. We explore the optimal designs and efficiencies for analyses by design-based estimators. Generalized raking is an efficient class of design-based estimators, and they improve on the inverse-probability weighted (IPW) estimator by adjusting weights based on the auxiliary information. We derive a closed-form solution of the optimal design for estimating regression coefficients from generalized raking estimators. We compare it with the optimal design for analysis via the IPW estimator and other two-phase designs in measurement-error settings. We consider general two-phase designs where the outcome variable and variables of interest can be continuous or discrete. Our results show that the optimal designs for analyses by the two classes of design-based estimators can be very different. The optimal design for analysis via the IPW estimator is optimal for IPW estimation and typically gives near-optimal efficiency for generalized raking estimation, though we show there is potential improvement in some settings.},
  archive      = {J_SIM},
  author       = {Tong Chen and Thomas Lumley},
  doi          = {10.1002/sim.9300},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1482-1497},
  shortjournal = {Stat. Med.},
  title        = {Optimal sampling for design-based estimators of regression models},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sensitivity to missing not at random dropout in clinical
trials: Use and interpretation of the trimmed means estimator.
<em>SIM</em>, <em>41</em>(8), 1462–1481. (<a
href="https://doi.org/10.1002/sim.9299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outcome values in randomized controlled trials (RCTs) may be missing not at random (MNAR), if patients with extreme outcome values are more likely to drop out (eg, due to perceived ineffectiveness of treatment, or adverse effects). In such scenarios, estimates from complete case analysis (CCA) and multiple imputation (MI) will be biased. We investigate the use of the trimmed means (TM) estimator for the case of univariable missingness in one continuous outcome. The TM estimator operates by setting missing values to the most extreme value, and then “trimming” away equal fractions of both groups, estimating the treatment effect using the remaining data. The TM estimator relies on two assumptions, which we term the “strong MNAR” and “location shift” assumptions. We derive formulae for the TM estimator bias resulting from the violation of these assumptions for normally distributed outcomes. We propose an adjusted TM estimator, which relaxes the location shift assumption and detail how our bias formulae can be used to establish the direction of bias of CCA and TM estimates, to inform sensitivity analyses. The TM approach is illustrated in a sensitivity analysis of the CoBalT RCT of cognitive behavioral therapy (CBT) in 469 individuals with 46 months follow-up. Results were consistent with a beneficial CBT treatment effect, with MI estimates closer to the null and TM estimates further from the null than the CCA estimate. We propose using the TM estimator as a sensitivity analysis for data where extreme outcome value dropout is plausible.},
  archive      = {J_SIM},
  author       = {Audinga-Dea Hazewinkel and Jack Bowden and Kaitlin H. Wade and Tom Palmer and Nicola J. Wiles and Kate Tilling},
  doi          = {10.1002/sim.9299},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1462-1481},
  shortjournal = {Stat. Med.},
  title        = {Sensitivity to missing not at random dropout in clinical trials: Use and interpretation of the trimmed means estimator},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical considerations for cross-sectional HIV incidence
estimation based on recency test. <em>SIM</em>, <em>41</em>(8),
1446–1461. (<a href="https://doi.org/10.1002/sim.9296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal cohorts to determine the incidence of HIV infection are logistically challenging, so researchers have sought alternative strategies. Recency test methods use biomarker profiles of HIV-infected subjects in a cross-sectional sample to infer whether they are “recently” infected and to estimate incidence in the population. Two main estimators have been used in practice: one that assumes a recency test is perfectly specific, and another that allows for false-recent results. To date, these commonly used estimators have not been rigorously studied with respect to their assumptions and statistical properties. In this article, we present a theoretical framework with which to understand these estimators and interrogate their assumptions, and perform a simulation study and data analysis to assess the performance of these estimators under realistic HIV epidemiological dynamics. We find that the snapshot estimator and the adjusted estimator perform well when their corresponding assumptions hold. When assumptions on constant incidence and recency test characteristics fail to hold, the adjusted estimator is more robust than the snapshot estimator. We conclude with recommendations for the use of these estimators in practice and a discussion of future methodological developments to improve HIV incidence estimation via recency test.},
  archive      = {J_SIM},
  author       = {Fei Gao and Marlena Bannick},
  doi          = {10.1002/sim.9296},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1446-1461},
  shortjournal = {Stat. Med.},
  title        = {Statistical considerations for cross-sectional HIV incidence estimation based on recency test},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Practical recommendations on double score matching for
estimating causal effects. <em>SIM</em>, <em>41</em>(8), 1421–1445. (<a
href="https://doi.org/10.1002/sim.9289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike in randomized clinical trials (RCTs), confounding control is critical for estimating the causal effects from observational studies due to the lack of treatment randomization. Under the unconfoundedness assumption, matching methods are popular because they can be used to emulate an RCT that is hidden in the observational study. To ensure the key assumption hold, the effort is often made to collect a large number of possible confounders, rendering dimension reduction imperative in matching. Three matching schemes based on the propensity score (PSM), prognostic score (PGM), and double score (DSM, ie, the collection of the first two scores) have been proposed in the literature. However, a comprehensive comparison is lacking among the three matching schemes and has not made inroads into the best practices including variable selection, choice of caliper, and replacement. In this article, we explore the statistical and numerical properties of PSM, PGM, and DSM via extensive simulations. Our study supports that DSM performs favorably with, if not better than, the two single score matching in terms of bias and variance. In particular, DSM is doubly robust in the sense that the matching estimator is consistent requiring either the propensity score model or the prognostic score model is correctly specified. Variable selection on the propensity score model and matching with replacement is suggested for DSM, and we illustrate the recommendations with comprehensive simulation studies. An R package is available at https://github.com/Yunshu7/dsmatch .},
  archive      = {J_SIM},
  author       = {Yunshu Zhang and Shu Yang and Wenyu Ye and Douglas E. Faries and Ilya Lipkovich and Zbigniew Kadziola},
  doi          = {10.1002/sim.9289},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1421-1445},
  shortjournal = {Stat. Med.},
  title        = {Practical recommendations on double score matching for estimating causal effects},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ranked set sampling in finite populations with bivariate
responses: An application to an osteoporosis study. <em>SIM</em>,
<em>41</em>(8), 1397–1420. (<a
href="https://doi.org/10.1002/sim.9285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of the research on rank-based sampling designs in finite populations has been concerned with univariate situations. In this article, we study design-based estimation using a bivariate ranked set sampling (BIRSS) for finite populations when we have bivariate response variables. We derive the first and second-order inclusion probabilities associated with a BIRSS design. We show that the size of a BIRSS sample is random and propose using a conditional Poisson sampling (CPS) design to rectify this problem. We then use calculated inclusion probabilities to obtain design-based estimators of correlation coefficients between the bone mineral density (BMD) levels at the baseline and followup of a longitudinal BMD study in the province of Manitoba in Canada. We also study the problem of estimating the parameters of a regression model between the followup BMD and easy to obtain auxiliary information from the underlying population. Finally, we study the problem of classifying patients as those with or without osteoporosis using BIRSS and various CPS designs. We show that BIRSS designs are very flexible and can be used to obtain more efficient design-based estimators in sample surveys when dealing with response variables that are hard to measure or expensive to obtain.},
  archive      = {J_SIM},
  author       = {Masoud Azimian and Mohammad Moradi and Mohammad Jafari Jozani and William D. Leslie},
  doi          = {10.1002/sim.9285},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1397-1420},
  shortjournal = {Stat. Med.},
  title        = {Ranked set sampling in finite populations with bivariate responses: An application to an osteoporosis study},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accounting for unequal cluster sizes in designing cluster
randomized trials to detect treatment effect heterogeneity.
<em>SIM</em>, <em>41</em>(8), 1376–1396. (<a
href="https://doi.org/10.1002/sim.9283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unequal cluster sizes are common in cluster randomized trials (CRTs). While there are a number of previous investigations studying the impact of unequal cluster sizes on the power for testing the average treatment effect in CRTs, little is known about the impact of unequal cluster sizes on the power for testing the heterogeneous treatment effect (HTE) in CRTs. In this work, we expand the sample size procedures for studying HTE in CRTs to accommodate cluster size variation under the linear mixed model framework. Through analytical derivation and graphical exploration, we show that the sample size for the HTE with an individual-level effect modifier is less affected by unequal cluster sizes than with a cluster-level effect modifier. The impact of cluster size variability jointly depends on the mean and coefficient of variation of cluster sizes, covariate intraclass correlation coefficient (ICC) and the conditional outcome ICC. In addition, we demonstrate that the HTE-motivated analysis of covariance framework can be used for analyzing the average treatment effect, and offer a more efficient sample size procedure for studying the average treatment effect adjusting for the effect modifier. We use simulations to confirm the accuracy of the proposed sample size procedures for both the average treatment effect and HTE in CRTs. Extensions to multivariate effect modifiers are provided and our procedure is illustrated in the context of the Strategies to Reduce Injuries and Develop Confidence in Elders trial.},
  archive      = {J_SIM},
  author       = {Guangyu Tong and Denise Esserman and Fan Li},
  doi          = {10.1002/sim.9283},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1376-1396},
  shortjournal = {Stat. Med.},
  title        = {Accounting for unequal cluster sizes in designing cluster randomized trials to detect treatment effect heterogeneity},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Determination of the number of observers needed to evaluate
a subjective test and its application in two PD-l1 studies.
<em>SIM</em>, <em>41</em>(8), 1361–1375. (<a
href="https://doi.org/10.1002/sim.9282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In pathological studies, subjective assays, especially companion diagnostic tests, can dramatically affect treatment of cancer. Binary diagnostic test results (ie, positive vs negative) may vary between pathologists or observers who read the tumor slides. Some tests have clearly defined criteria resulting in highly concordant outcomes, even with minimal training. Other tests are more challenging. Observers may achieve poor concordance even with training. While there are many statistically rigorous methods for measuring concordance between observers, we are unaware of a method that can identify how many observers are needed to determine whether a test can reach an acceptable concordance, if at all. Here we introduce a statistical approach to the assessment of test performance when the test is read by multiple observers, as would occur in the real world. By plotting the number of observers against the estimated overall agreement proportion, we can obtain a curve that plateaus to the average observer concordance. Diagnostic tests that are well-defined and easily judged show high concordance and plateau with few interobserver comparisons. More challenging tests do not plateau until many interobserver comparisons are made, and typically reach a lower plateau or even 0. We further propose a statistical test of whether the overall agreement proportion will drop to 0 with a large number of pathologists. The proposed analytical framework can be used to evaluate the difficulty in the interpretation of pathological test criteria and platforms, and to determine how pathology-based subjective tests will perform in the real world. The method could also be used outside of pathology, where concordance of a diagnosis or decision point relies on the subjective application of multiple criteria. We apply this method in two recent PD-L1 studies to test whether the curve of overall agreement proportion will converge to 0 and determine the minimal sufficient number of observers required to estimate the concordance plateau of their reads.},
  archive      = {J_SIM},
  author       = {Gang Han and Michael J. Schell and Emily S. Reisenbichler and Bohong Guo and David L. Rimm},
  doi          = {10.1002/sim.9282},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1361-1375},
  shortjournal = {Stat. Med.},
  title        = {Determination of the number of observers needed to evaluate a subjective test and its application in two PD-l1 studies},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Risk prediction models for discrete ordinal outcomes:
Calibration and the impact of the proportional odds assumption.
<em>SIM</em>, <em>41</em>(8), 1334–1360. (<a
href="https://doi.org/10.1002/sim.9281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calibration is a vital aspect of the performance of risk prediction models, but research in the context of ordinal outcomes is scarce. This study compared calibration measures for risk models predicting a discrete ordinal outcome, and investigated the impact of the proportional odds assumption on calibration and overfitting. We studied the multinomial, cumulative, adjacent category, continuation ratio, and stereotype logit/logistic models. To assess calibration, we investigated calibration intercepts and slopes, calibration plots, and the estimated calibration index. Using large sample simulations, we studied the performance of models for risk estimation under various conditions, assuming that the true model has either a multinomial logistic form or a cumulative logit proportional odds form. Small sample simulations were used to compare the tendency for overfitting between models. As a case study, we developed models to diagnose the degree of coronary artery disease (five categories) in symptomatic patients. When the true model was multinomial logistic, proportional odds models often yielded poor risk estimates, with calibration slopes deviating considerably from unity even on large model development datasets. The stereotype logistic model improved the calibration slope, but still provided biased risk estimates for individual patients. When the true model had a cumulative logit proportional odds form, multinomial logistic regression provided biased risk estimates, although these biases were modest. Nonproportional odds models require more parameters to be estimated from the data, and hence suffered more from overfitting. Despite larger sample size requirements, we generally recommend multinomial logistic regression for risk prediction modeling of discrete ordinal outcomes.},
  archive      = {J_SIM},
  author       = {Michael Edlinger and Maarten van Smeden and Hannes F Alber and Maria Wanitschek and Ben Van Calster},
  doi          = {10.1002/sim.9281},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1334-1360},
  shortjournal = {Stat. Med.},
  title        = {Risk prediction models for discrete ordinal outcomes: Calibration and the impact of the proportional odds assumption},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A puzzle of proportions: Two popular bayesian tests can
yield dramatically different conclusions. <em>SIM</em>, <em>41</em>(8),
1319–1333. (<a href="https://doi.org/10.1002/sim.9278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing the equality of two proportions is a common procedure in science, especially in medicine and public health. In these domains, it is crucial to be able to quantify evidence for the absence of a treatment effect. Bayesian hypothesis testing by means of the Bayes factor provides one avenue to do so, requiring the specification of prior distributions for parameters. The most popular analysis approach views the comparison of proportions from a contingency table perspective, assigning prior distributions directly to the two proportions. Another, less popular approach views the problem from a logistic regression perspective, assigning prior distributions to logit-transformed parameters. Reanalyzing 39 null results from the New England Journal of Medicine with both approaches, we find that they can lead to markedly different conclusions, especially when the observed proportions are at the extremes (ie, very low or very high). We explain these stark differences and provide recommendations for researchers interested in testing the equality of two proportions and users of Bayes factors more generally. The test that assigns prior distributions to logit-transformed parameters creates prior dependence between the two proportions and yields weaker evidence when the observations are at the extremes. When comparing two proportions, we argue that this test should become the new default.},
  archive      = {J_SIM},
  author       = {Fabian Dablander and Karoline Huth and Quentin F. Gronau and Alexander Etz and Eric-Jan Wagenmakers},
  doi          = {10.1002/sim.9278},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1319-1333},
  shortjournal = {Stat. Med.},
  title        = {A puzzle of proportions: Two popular bayesian tests can yield dramatically different conclusions},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comment on “a depth-based global envelope test for comparing
two groups of functions with applications to biomedical data” by s.
Lopez-pintado and k. qian. <em>SIM</em>, <em>41</em>(7), 1316–1317. (<a
href="https://doi.org/10.1002/sim.8990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This is a note describing nonparametric methodology of functional tests in the functional general linear models, which is more rich than the methodology presented in the commented paper.},
  archive      = {J_SIM},
  author       = {Tomáš Mrkvička and Mari Myllymäki},
  doi          = {10.1002/sim.8990},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1316-1317},
  shortjournal = {Stat. Med.},
  title        = {Comment on “A depth-based global envelope test for comparing two groups of functions with applications to biomedical data” by s. lopez-pintado and k. qian},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Re: Spline-based accelerated failure time model.
<em>SIM</em>, <em>41</em>(7), 1314–1315. (<a
href="https://doi.org/10.1002/sim.8964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Mark Clements and Benjamin Christoffersen and Patrick Royston and Michael Crowther},
  doi          = {10.1002/sim.8964},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1314-1315},
  shortjournal = {Stat. Med.},
  title        = {Re: Spline-based accelerated failure time model},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation and inference for multikink expectile regression
with longitudinal data. <em>SIM</em>, <em>41</em>(7), 1296–1313. (<a
href="https://doi.org/10.1002/sim.9277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate parameter estimation, kink points testing and statistical inference for a longitudinal multikink expectile regression model. The estimators for the kink locations and regression coefficients are obtained by using a bootstrap restarting iterative algorithm to avoid local minima. A backward selection procedure based on a modified BIC is applied to estimate the number of kink points. We theoretically demonstrate the number selection consistency of kink points and the asymptotic normality of all estimators. In particular, the estimators of kink locations are shown to achieve root- n consistency. A weighted cumulative sum type statistic is proposed to test the existence of kink effects at a given expectile, and its limiting distributions are derived under both the null and the local alternative hypotheses. The traditional Wald-type and cluster bootstrap confidence intervals for kink locations are also constructed. Simulation studies show that the proposed estimators and test have desirable finite sample performance in both homoscedastic and heteroscedastic errors. Two applications to the Nation Growth, Lung and Health Study and Capital Bike sharing dataset in Washington D.C. are also presented. The R codes for simulation studies and the real data are available at https://github.com/wangleink/MKER .},
  archive      = {J_SIM},
  author       = {Dongyu Li and Lei Wang and Weihua Zhao},
  doi          = {10.1002/sim.9277},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1296-1313},
  shortjournal = {Stat. Med.},
  title        = {Estimation and inference for multikink expectile regression with longitudinal data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Minimum sample size calculations for external validation of
a clinical prediction model with a time-to-event outcome. <em>SIM</em>,
<em>41</em>(7), 1280–1295. (<a
href="https://doi.org/10.1002/sim.9275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous articles in Statistics in Medicine describe how to calculate the sample size required for external validation of prediction models with continuous and binary outcomes. The minimum sample size criteria aim to ensure precise estimation of key measures of a model&#39;s predictive performance, including measures of calibration, discrimination, and net benefit. Here, we extend the sample size guidance to prediction models with a time-to-event (survival) outcome, to cover external validation in datasets containing censoring. A simulation-based framework is proposed, which calculates the sample size required to target a particular confidence interval width for the calibration slope measuring the agreement between predicted risks (from the model) and observed risks (derived using pseudo-observations to account for censoring) on the log cumulative hazard scale. Precise estimation of calibration curves, discrimination, and net-benefit can also be checked in this framework. The process requires assumptions about the validation population in terms of the (i) distribution of the model&#39;s linear predictor and (ii) event and censoring distributions. Existing information can inform this; in particular, the linear predictor distribution can be approximated using the C-index or Royston&#39;s D statistic from the model development article, together with the overall event risk. We demonstrate how the approach can be used to calculate the sample size required to validate a prediction model for recurrent venous thromboembolism. Ideally the sample size should ensure precise calibration across the entire range of predicted risks, but must at least ensure adequate precision in regions important for clinical decision-making. Stata and R code are provided.},
  archive      = {J_SIM},
  author       = {Richard D. Riley and Gary S. Collins and Joie Ensor and Lucinda Archer and Sarah Booth and Sarwar I. Mozumder and Mark J. Rutherford and Maarten van Smeden and Paul C. Lambert and Kym I. E. Snell},
  doi          = {10.1002/sim.9275},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1280-1295},
  shortjournal = {Stat. Med.},
  title        = {Minimum sample size calculations for external validation of a clinical prediction model with a time-to-event outcome},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian transformation models with partly interval-censored
data. <em>SIM</em>, <em>41</em>(7), 1263–1279. (<a
href="https://doi.org/10.1002/sim.9271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many scientific fields, partly interval-censored data, which consist of exactly observed and interval-censored observations on the failure time of interest, appear frequently. However, methodological developments in the analysis of partly interval-censored data are relatively limited and have mainly focused on additive or proportional hazards models. The general linear transformation model provides a highly flexible modeling framework that includes several familiar survival models as special cases. Despite such nice features, the inference procedure for this class of models has not been developed for partly interval-censored data. We propose a fully Bayesian approach coped with efficient Markov chain Monte Carlo methods to fill this gap. A four-stage data augmentation procedure is introduced to tackle the challenges presented by the complex model and data structure. The proposed method is easy to implement and computationally attractive. The empirical performance of the proposed method is evaluated through two simulation studies, and the model is then applied to a dental health study.},
  archive      = {J_SIM},
  author       = {Chunjie Wang and Jingjing Jiang and Xinyuan Song},
  doi          = {10.1002/sim.9271},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1263-1279},
  shortjournal = {Stat. Med.},
  title        = {Bayesian transformation models with partly interval-censored data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature selection and classification over the network with
missing node observations. <em>SIM</em>, <em>41</em>(7), 1242–1262. (<a
href="https://doi.org/10.1002/sim.9267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Jointly analyzing transcriptomic data and the existing biological networks can yield more robust and informative feature selection results, as well as better understanding of the biological mechanisms. Selecting and classifying node features over genome-scale networks has become increasingly important in genomic biology and genomic medicine. Existing methods have some critical drawbacks. The first is they do not allow flexible modeling of different subtypes of selected nodes. The second is they ignore nodes with missing values, very likely to increase bias in estimation. To address these limitations, we propose a general modeling framework for Bayesian node classification (BNC) with missing values. A new prior model is developed for the class indicators incorporating the network structure. For posterior computation, we resort to the Swendsen-Wang algorithm for efficiently updating class indicators. BNC can naturally handle missing values in the Bayesian modeling framework, which improves the node classification accuracy and reduces the bias in estimating gene effects. We demonstrate the advantages of our methods via extensive simulation studies and the analysis of the cutaneous melanoma dataset from The Cancer Genome Atlas.},
  archive      = {J_SIM},
  author       = {Zhuxuan Jin and Jian Kang and Tianwei Yu},
  doi          = {10.1002/sim.9267},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1242-1262},
  shortjournal = {Stat. Med.},
  title        = {Feature selection and classification over the network with missing node observations},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Event-specific win ratios for inference with terminal and
non-terminal events. <em>SIM</em>, <em>41</em>(7), 1225–1241. (<a
href="https://doi.org/10.1002/sim.9266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For semi-competing risks data involving a non-terminal event and a terminal event we derive the asymptotic distributions of the event-specific win ratios under proportional hazards (PH) assumptions for the relevant cause-specific hazard functions of the non-terminal and terminal event, respectively. The win ratios converge to the respective hazard ratios under the PH assumptions and therefore are censoring-free, whether or not the censoring distributions in the two treatment arms are the same. With the asymptotic bivariate normal distributions of the win ratios, confidence intervals and testing procedures are obtained. Through extensive simulation studies and data analysis, we identified proper transformations of the win ratios that yield good control of the type one error rate for various testing procedures while maintaining competitive power. The confidence intervals also have good coverage probabilities. Furthermore, a test for the PH assumptions and a test of equal hazard ratios are developed. The new procedures are illustrated in the clinical trial Aldosterone Antagonist Therapy for Adults With Heart Failure and Preserved Systolic Function, which evaluated the effects of spironolactone in patients with heart failure and a preserved left ventricular ejection fraction.},
  archive      = {J_SIM},
  author       = {Song Yang and James Troendle and Daewoo Pak and Eric Leifer},
  doi          = {10.1002/sim.9266},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1225-1241},
  shortjournal = {Stat. Med.},
  title        = {Event-specific win ratios for inference with terminal and non-terminal events},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BIPSE: A biomarker-based phase i/II design for immunotherapy
trials with progression-free survival endpoint. <em>SIM</em>,
<em>41</em>(7), 1205–1224. (<a
href="https://doi.org/10.1002/sim.9265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bayesian biomarker-based phase I/II design (BIPSE) is presented for immunotherapy trials with a progression-free survival (PFS) endpoint. The objective is to identify the subgroup-specific optimal dose, defined as the dose with the best risk-benefit tradeoff in each biomarker subgroup. We jointly model the immune response, toxicity outcome, and PFS with information borrowing across subgroups. A plateau model is used to describe the marginal distribution of the immune response. Conditional on the immune response, we model toxicity using probit regression and model PFS using the mixture cure rate model. During the trial, based on the accumulating data, we continuously update model estimates and adaptively randomize patients to doses with high desirability within each subgroup. Simulation studies show that the BIPSE design has desirable operating characteristics in selecting the subgroup-specific optimal doses and allocating patients to those optimal doses, and outperforms conventional designs.},
  archive      = {J_SIM},
  author       = {Beibei Guo and Yong Zang},
  doi          = {10.1002/sim.9265},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1205-1224},
  shortjournal = {Stat. Med.},
  title        = {BIPSE: A biomarker-based phase I/II design for immunotherapy trials with progression-free survival endpoint},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A semiparametric risk score for physical activity.
<em>SIM</em>, <em>41</em>(7), 1191–1204. (<a
href="https://doi.org/10.1002/sim.9262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a generalized partially additive model to build a single semiparametric risk scoring system for physical activity across multiple populations. A score comprised of distinct and objective physical activity measures is a new concept that offers challenges due to the nonlinear relationship between physical behaviors and various health outcomes. We overcome these challenges by modeling each score component as a smooth term, an extension of generalized partially linear single-index models. We use penalized splines and propose two inferential methods, one using profile likelihood and a nonparametric bootstrap, the other using a full Bayesian model, to solve additional computational problems. Both methods exhibit similar and accurate performance in simulations. These models are applied to the National Health and Nutrition Examination Survey and quantify nonlinear and interpretable shapes of score components for all-cause mortality.},
  archive      = {J_SIM},
  author       = {Erjia Cui and E. Christi Thompson and Raymond J. Carroll and David Ruppert},
  doi          = {10.1002/sim.9262},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1191-1204},
  shortjournal = {Stat. Med.},
  title        = {A semiparametric risk score for physical activity},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple imputation confidence intervals for the mean of the
discrete distributions for incomplete data. <em>SIM</em>,
<em>41</em>(7), 1172–1190. (<a
href="https://doi.org/10.1002/sim.9254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confidence intervals for the mean of discrete exponential families are widely used in many applications. Since missing data are commonly encountered, the interval estimation for incomplete data is an important problem. The performances of the existing multiple imputation confidence intervals are unsatisfactory. We propose modified multiple imputation confidence intervals to improve the existing confidence intervals for the mean of the discrete exponential families with quadratic variance functions. A simulation study shows that the coverage probabilities of the modified confidence intervals are closer to the nominal level than the existing confidence intervals when the true mean is near the boundaries of the parameter space. These confidence intervals are also illustrated with real data examples.},
  archive      = {J_SIM},
  author       = {Chung-Han Lee and Hsiuying Wang},
  doi          = {10.1002/sim.9254},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1172-1190},
  shortjournal = {Stat. Med.},
  title        = {Multiple imputation confidence intervals for the mean of the discrete distributions for incomplete data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal adaptive allocation using deep reinforcement
learning in a dose-response study. <em>SIM</em>, <em>41</em>(7),
1157–1171. (<a href="https://doi.org/10.1002/sim.9247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of the dose-response curve for efficacy and subsequent selection of an appropriate dose in phase II trials are important processes in drug development. Various methods have been investigated to estimate dose-response curves. Generally, these methods are used with equal allocation of subjects for simplicity; nevertheless, they may not fully optimize performance metrics because of nonoptimal allocation. Optimal allocation methods, which include adaptive allocation methods, have been proposed to overcome the limitations of equal allocation. However, they rely on asymptotics, and thus sometimes cannot efficiently optimize the performance metric with the sample size in an actual clinical trial. The purpose of this study is to construct an adaptive allocation rule that directly optimizes a performance metric, such as power, accuracy of model selection, accuracy of the estimated target dose, or mean absolute error over the estimated dose-response curve. We demonstrate that deep reinforcement learning with an appropriately defined state and reward can be used to construct such an adaptive allocation rule. The simulation study shows that the proposed method can successfully improve the performance metric to be optimized when compared with the equal allocation, D-optimal, and TD-optimal methods. In particular, when the mean absolute error was set to the metric to be optimized, it is possible to construct a rule that is superior for many metrics.},
  archive      = {J_SIM},
  author       = {Kentaro Matsuura and Junya Honda and Imad El Hanafi and Takashi Sozu and Kentaro Sakamaki},
  doi          = {10.1002/sim.9247},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1157-1171},
  shortjournal = {Stat. Med.},
  title        = {Optimal adaptive allocation using deep reinforcement learning in a dose-response study},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weighted generalized estimating equations and unified
estimation for longitudinal data with nonmonotone missing data patterns.
<em>SIM</em>, <em>41</em>(7), 1148–1156. (<a
href="https://doi.org/10.1002/sim.9246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data are a major complication in longitudinal data analysis. Weighted generalized estimating equations (WGEEs, Robins et al, J Am Stat Assoc 1995;90:106-121) were developed to deal with missing response data. They have been extended for data with both missing responses and missing covariates (Chen et al, J Am Stat Assoc 2010;105:336-353). However, it may introduce more variability in dealing with the correlation structure of the responses. We propose new WGEEs for missing at random data where both response and (time-dependent) covariates may have values missing in nonmonotone missing data patterns. We also explain how to improve the estimation efficiency of WGEEs using a unified approach (Zhao and Liu, AStA Adv Stat Anal 2021;105(1):87-101). The proposed unified estimator is consistent and more efficient than the regular WGEE estimator. It is computationally simple and can be directly implemented in standard software. Simulation studies for both continuous response and binary response data are provided to examine the performance of the proposed estimators. A clinical trial example investigating the quality of life of women with early-stage breast cancer and the associated factors is analyzed.},
  archive      = {J_SIM},
  author       = {Meng Liu and Yang Zhao},
  doi          = {10.1002/sim.9246},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1148-1156},
  shortjournal = {Stat. Med.},
  title        = {Weighted generalized estimating equations and unified estimation for longitudinal data with nonmonotone missing data patterns},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Limitations of clinical trial sample size estimate by
subtraction of two measurements. <em>SIM</em>, <em>41</em>(7),
1137–1147. (<a href="https://doi.org/10.1002/sim.9244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In planning randomized clinical trials (RCTs) for diseases such as Alzheimer&#39;s disease (AD), researchers frequently rely on the use of existing data obtained from only two time points to estimate sample size via the subtraction of baseline from follow-up measurements in each subject. However, the inadequacy of this method has not been reported. The aim of this study is to discuss the limitation of sample size estimation based on the subtraction of available data from only two time points for RCTs. Mathematical equations are derived to demonstrate the condition under which the obtained data pairs with variable time intervals could be used to adequately estimate sample size. The MRI-based hippocampal volume measurements from the Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI) and Monte Carlo simulations (MCS) were used to illustrate the existing bias and variability of estimates. MCS results support the theoretically derived condition under which the subtraction approach may work. MCS also show the systematically under- or over-estimated sample sizes by up to 32.27 bias. Not used properly, such subtraction approach outputs the same sample size regardless of trial durations partly due to the way measurement errors are handled. Estimating sample size by subtracting two measurements should be treated with caution. Such estimates can be biased, the magnitude of which depends on the planned RCT duration. To estimate sample sizes, we recommend using more than two measurements and more comprehensive approaches such as linear mixed effect models.},
  archive      = {J_SIM},
  author       = {Kewei Chen and Xiaojuan Guo and Rong Pan and Chengjie Xiong and Danielle J. Harvey and Yinghua Chen and Li Yao and Yi Su and Eric M. Reiman},
  doi          = {10.1002/sim.9244},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1137-1147},
  shortjournal = {Stat. Med.},
  title        = {Limitations of clinical trial sample size estimate by subtraction of two measurements},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Combining information to estimate adherence in studies of
pre-exposure prophylaxis for HIV prevention: Application to HPTN 067.
<em>SIM</em>, <em>41</em>(6), 1120–1136. (<a
href="https://doi.org/10.1002/sim.9321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In trials of oral HIV pre-exposure prophylaxis (PrEP), multiple approaches have been used to measure adherence, including self-report, pill counts, electronic dose monitoring devices, and biological measures such as drug levels in plasma, peripheral blood mononuclear cells, hair, and/or dried blood spots. No one of these measures is ideal and each has strengths and weaknesses. However, accurate estimates of adherence to oral PrEP are important as drug efficacy is closely tied to adherence, and secondary analyses of trial data within identified adherent/non-adherent subgroups may yield important insights into real-world drug effectiveness. We develop a statistical approach to combining multiple measures of adherence and show in simulated data that the proposed method provides a more accurate measure of true adherence than self-report. We then apply the method to estimate adherence in the ADAPT study (HPTN 067) in South African women.},
  archive      = {J_SIM},
  author       = {James P. Hughes and Brian D. Williamson and Chloe Krakauer and Gordon Chau and Brayan Ortiz and Jon Wakefield and Craig Hendrix and K. Rivet Amico and Timothy H. Holtz and Linda-Gail Bekker and Robert Grant},
  doi          = {10.1002/sim.9321},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1120-1136},
  shortjournal = {Stat. Med.},
  title        = {Combining information to estimate adherence in studies of pre-exposure prophylaxis for HIV prevention: Application to HPTN 067},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Profile-likelihood bayesian model averaging for two-sample
summary data mendelian randomization in the presence of horizontal
pleiotropy. <em>SIM</em>, <em>41</em>(6), 1100–1119. (<a
href="https://doi.org/10.1002/sim.9320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-sample summary data Mendelian randomization is a popular method for assessing causality in epidemiology, by using genetic variants as instrumental variables. If genes exert pleiotropic effects on the outcome not entirely through the exposure of interest, this can lead to heterogeneous and (potentially) biased estimates of causal effect. We investigate the use of Bayesian model averaging to preferentially search the space of models with the highest posterior likelihood. We develop a Metropolis-Hasting algorithm to perform the search using the recently developed MR-RAPS as the basis for defining a posterior distribution that efficiently accounts for pleiotropic and weak instrument bias. We demonstrate how our general modeling approach can be extended from a standard one-component causal model to a two-component model, which allows a large proportion of SNPs to violate the InSIDE assumption. We use Monte Carlo simulations to illustrate our methods and compare it to several related approaches. We finish by applying our approach to investigate the causal role of cholesterol on the development age-related macular degeneration.},
  archive      = {J_SIM},
  author       = {Chin Yang Shapland and Qingyuan Zhao and Jack Bowden},
  doi          = {10.1002/sim.9320},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1100-1119},
  shortjournal = {Stat. Med.},
  title        = {Profile-likelihood bayesian model averaging for two-sample summary data mendelian randomization in the presence of horizontal pleiotropy},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Response adaptive intervention allocation in stepped-wedge
cluster randomized trials. <em>SIM</em>, <em>41</em>(6), 1081–1099. (<a
href="https://doi.org/10.1002/sim.9317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Michael J. Grayling and James M. S. Wason and Sofía S. Villar},
  doi          = {10.1002/sim.9317},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1081-1099},
  shortjournal = {Stat. Med.},
  title        = {Response adaptive intervention allocation in stepped-wedge cluster randomized trials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A nonparametric bayesian method for dose finding in drug
combinations cancer trials. <em>SIM</em>, <em>41</em>(6), 1059–1080. (<a
href="https://doi.org/10.1002/sim.9316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an adaptive design for early-phase drug-combination cancer trials with the goal of estimating the maximum tolerated dose (MTD). A nonparametric Bayesian model, using beta priors truncated to the set of partially ordered dose combinations, is used to describe the probability of dose limiting toxicity (DLT). Dose allocation between successive cohorts of patients is estimated using a modified continual reassessment scheme. The updated probabilities of DLT are calculated with a Gibbs sampler that employs a weighting mechanism to calibrate the influence of data vs the prior. At the end of the trial, we recommend one or more dose combinations as the MTD based on our proposed algorithm. We apply our method to a Phase I clinical trial of CB-839 and Gemcitabine that motivated this nonparametric design. The design operating characteristics indicate that our method is comparable with existing methods.},
  archive      = {J_SIM},
  author       = {Zahra S. Razaee and Galen Cook-Wiens and Mourad Tighiouart},
  doi          = {10.1002/sim.9316},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1059-1080},
  shortjournal = {Stat. Med.},
  title        = {A nonparametric bayesian method for dose finding in drug combinations cancer trials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple imputation for longitudinal data using bayesian
lasso imputation model. <em>SIM</em>, <em>41</em>(6), 1042–1058. (<a
href="https://doi.org/10.1002/sim.9315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple imputation is a promising approach to handle missing data and is widely used in analysis of longitudinal clinical studies. A key consideration in the implementation of multiple imputation is to obtain accurate imputed values by specifying an imputation model that incorporates auxiliary variables potentially associated with missing variables. The use of informative auxiliary variables is known to be beneficial to make the missing at random assumption more plausible and help to reduce uncertainty of the imputations; however, it is not straightforward to pre-specify them in many cases. We propose a data-driven specification of the imputation model using Bayesian lasso in the context of longitudinal clinical study, and develop a built-in function of the Bayesian lasso imputation model which is performed within the framework of multiple imputation using chained equations. A simulation study suggested that the Bayesian lasso imputation model worked well in a variety of longitudinal study settings, providing unbiased treatment effect estimates with well-controlled type I error rates and coverage probabilities of the confidence interval; in contrast, ignorance of the informative auxiliary variables led to serious bias and inflation of type I error rate. Moreover, the Bayesian lasso imputation model offered higher statistical powers compared with conventional imputation methods. In our simulation study, the gains in statistical power were remarkable when the sample size was small relative to the number of auxiliary variables. An illustration through a real example also suggested that the Bayesian lasso imputation model could give smaller standard errors of the treatment effect estimate.},
  archive      = {J_SIM},
  author       = {Yusuke Yamaguchi and Satoshi Yoshida and Toshihiro Misumi and Kazushi Maruo},
  doi          = {10.1002/sim.9315},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1042-1058},
  shortjournal = {Stat. Med.},
  title        = {Multiple imputation for longitudinal data using bayesian lasso imputation model},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional or unconditional logistic regression for
frequency matched case-control design? <em>SIM</em>, <em>41</em>(6),
1023–1041. (<a href="https://doi.org/10.1002/sim.9313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequency matching is commonly used in epidemiological case control studies to balance the distributions of the matching factors between the case and control groups and to improve the efficiency of case-control designs. Applied researchers have held a common opinion that unconditional logistic regression should be used to analyze frequency matched designs and conditional logistic regression is unnecessary. However, the justification of this view is unclear. To compare the performances of ULR and CLR in terms of simplicity, unbiasedness, and efficiency in a more intuitive way, we viewed frequency matching from the perspective of weighted sampling and derived the outcome models describing how the exposure and matching factors are associated with the outcome in the matched data separately in two scenarios: (1) only categorical variables are used for matching; (2) continuous variables are categorized for matching. In either scenario the derived outcome model is a logit model with stratum-specific intercepts. Correctly specified unconditional logistic regression can be more efficient than conditional logistic regression, particularly when continuous matching factors are used, whereas conditional logistic regression is a more practical approach because it is less dependent on modeling choices.},
  archive      = {J_SIM},
  author       = {Fei Wan},
  doi          = {10.1002/sim.9313},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1023-1041},
  shortjournal = {Stat. Med.},
  title        = {Conditional or unconditional logistic regression for frequency matched case-control design?},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian hierarchical finite mixture of regression for
histopathological imaging-based cancer data analysis. <em>SIM</em>,
<em>41</em>(6), 1009–1022. (<a
href="https://doi.org/10.1002/sim.9309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is heterogeneous, and for seemingly similar cancer patients, the associations between an outcome/phenotype and covariates can be different. To describe such differences, finite mixture of regression (FMR) and other modeling techniques have been developed. “Classic” FMR analysis has usually been based on clinical, demographic, and molecular variables. More recently, histopathological imaging data—which is a byproduct of biopsy and enjoys broader data availability and higher cost-effectiveness—has been increasingly used in cancer modeling, although it is noted that its application to cancer FMR analysis still remains limited. In this article, we further advance cancer FMR analysis based on histopathological imaging data. Significantly advancing from the existing analyses under heterogeneity and homogeneity, our goal is to simultaneously use two types of histopathological imaging features, which are extracted based on domain-specific biomedical knowledge and using automated signal processing software, respectively. A significant modeling/methodological advancement is that, to reflect the “increased resolution” of the second type of imaging features over the first type, we impose a hierarchy in the mixture structures. An effective and flexible Bayesian approach is proposed. Simulation shows its competitiveness over several alternatives. The TCGA lung cancer data is analyzed, and interesting heterogeneous structures different from using the alternatives are found. Overall, this study provides a new venue for FMR analysis for cancer and other complex diseases.},
  archive      = {J_SIM},
  author       = {Yunju Im and Yuan Huang and Jian Huang and Shuangge Ma},
  doi          = {10.1002/sim.9309},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1009-1022},
  shortjournal = {Stat. Med.},
  title        = {Bayesian hierarchical finite mixture of regression for histopathological imaging-based cancer data analysis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparing the accuracy of screening tests with verification
of disease status restricted to test positives. <em>SIM</em>,
<em>41</em>(6), 994–1008. (<a
href="https://doi.org/10.1002/sim.9306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a fundamental component of health care, disease screening is of highly importance. Oftentimes, two screening tests for a specific disease are compared in order to determine an optimal screening policy, for example, the digital rectal examination (DRE) and serum prostate specific antigen (PSA) level for screening prostate cancer. Ideally, if a gold standard test is given to each individual being screened to establish their true disease status, the difference in accuracy measures between two tests can be evaluated. In practice, however, it is common that only individuals who test positive on at least one screening test are to receive gold standard tests, which are often invasive and cannot be applied to those with negative results on both tests due to ethical reasons. Under such circumstances, estimates of the differences in accuracy measures between two tests cannot be determined, thus the inference problem within this framework is challenging. In this article, using sensitivity and specificity as measures of test accuracy, we show that their difference between two tests is interval-identified, as bounded by estimable sharp bounds. Here, we develop the asymptotic normality for the estimators of the bounds and construct confidence intervals for the difference by utilizing the method for solving inference problem for partially identified parameters. The performance of constructed confidence intervals for the difference and their sharp bounds are evaluated via simulation studies. We also apply the proposed method to the prostate cancer example to compare the accuracy of DRE and PSA.},
  archive      = {J_SIM},
  author       = {Lu Wang and Xiao-Hua Zhou},
  doi          = {10.1002/sim.9306},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {994-1008},
  shortjournal = {Stat. Med.},
  title        = {Comparing the accuracy of screening tests with verification of disease status restricted to test positives},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximate likelihood-based estimation method of
multiple-type pathogen interactions: An application to longitudinal
pneumococcal carriage data. <em>SIM</em>, <em>41</em>(6), 981–993. (<a
href="https://doi.org/10.1002/sim.9305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the serotypes of Streptococcus pneumoniae are known to compete during colonization in human hosts, our knowledge of how competition occurs is still incomplete. New insights of pneumococcal between-type competition could be generated from carriage data obtained by molecular-based detection methods, which record more complete sets of serotypes involved in co-carriage than when detection is done by culture. Here, we develop a Bayesian estimation method for inferring between-type interactions from longitudinal data recording the presence/absence of the types at discrete observation times. It allows inference from data containing co-carriage of two or more serotypes, which is often the case when pneumococcal presence is determined by molecular-based methods. The computational burden posed by the increased number of types detected in co-carriage is addressed by approximating the likelihood under a multi-state model with the likelihood of only those trajectories with minimum number of acquisition and clearance events between observation times. The proposed method&#39;s performance was validated on simulated data. The estimates of the interaction parameters of acquisition and clearance were unbiased in settings with short sampling intervals between observation times. With less frequent sampling, the estimates of the interaction parameters became more biased, but their ratio, which summarizes the total interaction, remained unbiased. Confounding due to unobserved heterogeneity in exposure could be corrected by including individual-level random effects. In an application to empirical data about pneumococcal carriage in infants, we found new evidence for between-serotype competition in clearance, although the effect size was small.},
  archive      = {J_SIM},
  author       = {Irene Man and Johannes A. Bogaards and Kishan Makwana and Krzysztof Trzciński and Kari Auranen},
  doi          = {10.1002/sim.9305},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {981-993},
  shortjournal = {Stat. Med.},
  title        = {Approximate likelihood-based estimation method of multiple-type pathogen interactions: An application to longitudinal pneumococcal carriage data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). B-value and empirical equivalence bound: A new procedure of
hypothesis testing. <em>SIM</em>, <em>41</em>(6), 964–980. (<a
href="https://doi.org/10.1002/sim.9298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a two-stage procedure for hypothesis testing, where the first stage is conventional hypothesis testing and the second is an equivalence testing procedure using an introduced empirical equivalence bound (EEB). In 2016, the American Statistical Association released a policy statement on P -values to clarify the proper use and interpretation in response to the criticism of reproducibility and replicability in scientific findings. A recent solution to improve reproducibility and transparency in statistical hypothesis testing is to integrate P -values (or confidence intervals) with practical or scientific significance. Similar ideas have been proposed via the equivalence test, where the goal is to infer equality under a presumption (null) of inequality of parameters. However, the definition of scientific significance/equivalence can sometimes be ill-justified and subjective. To circumvent this drawback, we introduce the B -value and the EEB, which are both estimated from the data. Performing a second-stage equivalence test, our procedure offers an opportunity to improve the reproducibility of findings across studies.},
  archive      = {J_SIM},
  author       = {Yi Zhao and Brian S. Caffo and Joshua B. Ewen},
  doi          = {10.1002/sim.9298},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {964-980},
  shortjournal = {Stat. Med.},
  title        = {B-value and empirical equivalence bound: A new procedure of hypothesis testing},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Sample size formula for a win ratio endpoint. <em>SIM</em>,
<em>41</em>(6), 950–963. (<a
href="https://doi.org/10.1002/sim.9297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The win ratio composite endpoint, which organizes the components of the composite hierarchically, is becoming popular in late-stage clinical trials. The method involves comparing data in a pair-wise manner starting with the endpoint highest in priority (eg, cardiovascular death). If the comparison is a tie, the endpoint next highest in priority (eg, hospitalizations for heart failure) is compared, and so on. Its sample size is usually calculated through complex simulations because there does not exist in the literature a simple sample size formula. This article provides a formula that depends on the probability that a randomly selected patient from one group does better than a randomly selected patient from another group, and on the probability of a tie. We compare the published 95% confidence intervals, which require patient-level data, with that calculated from the formula, requiring only summary-level data, for 17 composite or single win ratio endpoints. The two sets of results are similar. Simulations show the sample size formula performs well. The formula provides important insights. It shows when adding an endpoint to the hierarchy can increase power even if the added endpoint has low power by itself. It provides relevant information to modify an on-going blinded trial if necessary. The formula allows a non-specialist to quickly determine the size of the trial with a win ratio endpoint whose use is expected to increase over time.},
  archive      = {J_SIM},
  author       = {Ron Xiaolong Yu and Jitendra Ganju},
  doi          = {10.1002/sim.9297},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {950-963},
  shortjournal = {Stat. Med.},
  title        = {Sample size formula for a win ratio endpoint},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable algorithms for semiparametric accelerated failure
time models in high dimensions. <em>SIM</em>, <em>41</em>(6), 933–949.
(<a href="https://doi.org/10.1002/sim.9264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semiparametric accelerated failure time (AFT) models are a useful alternative to Cox proportional hazards models, especially when the assumption of constant hazard ratios is untenable. However, rank-based criteria for fitting AFT models are often nondifferentiable, which poses a computational challenge in high-dimensional settings. In this article, we propose a new alternating direction method of multipliers algorithm for fitting semiparametric AFT models by minimizing a penalized rank-based loss function. Our algorithm scales well in both the number of subjects and number of predictors, and can easily accommodate a wide range of popular penalties. To improve the selection of tuning parameters, we propose a new criterion which avoids some common problems in cross-validation with censored responses. Through extensive simulation studies, we show that our algorithm and software is much faster than existing methods (which can only be applied to special cases), and we show that estimators which minimize a penalized rank-based criterion often outperform alternative estimators which minimize penalized weighted least squares criteria. Application to nine cancer datasets further demonstrates that rank-based estimators of semiparametric AFT models are competitive with estimators assuming proportional hazards in high-dimensional settings, whereas weighted least squares estimators are often not. A software package implementing the algorithm, along with a set of auxiliary functions, is available for download at github.com/ajmolstad/penAFT .},
  archive      = {J_SIM},
  author       = {Piotr M. Suder and Aaron J. Molstad},
  doi          = {10.1002/sim.9264},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {933-949},
  shortjournal = {Stat. Med.},
  title        = {Scalable algorithms for semiparametric accelerated failure time models in high dimensions},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CORRECTION. <em>SIM</em>, <em>41</em>(5), 932. (<a
href="https://doi.org/10.1002/sim.9263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Man Lai Tang and Hon Keung Tony Ng},
  doi          = {10.1002/sim.9263},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {932},
  shortjournal = {Stat. Med.},
  title        = {CORRECTION},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximate profile likelihood estimation for cox regression
with covariate measurement error. <em>SIM</em>, <em>41</em>(5), 910–931.
(<a href="https://doi.org/10.1002/sim.9324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In nutritional epidemiology, measurement error in covariates is a well-known problem since dietary intakes are usually assessed through self-reporting. In this article, we consider an additive error model in which error variables are highly correlated, and propose a new method called approximate profile likelihood estimation (APLE) for covariates measured with error in the Cox regression. Asymptotic normality of this estimator is established under regularity conditions, and simulation studies are conducted to examine the finite sample performance of the proposed estimator empirically. Moreover, the popular correction method called regression calibration is shown to be a special case of APLE. We then apply APLE to deal with measurement error in some nutrients of interest in the EPIC-InterAct Study under a sensitivity analysis framework.},
  archive      = {J_SIM},
  author       = {Zhiqiang Cao and Man Yu Wong},
  doi          = {10.1002/sim.9324},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {910-931},
  shortjournal = {Stat. Med.},
  title        = {Approximate profile likelihood estimation for cox regression with covariate measurement error},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multiple testing framework for diagnostic accuracy studies
with co-primary endpoints. <em>SIM</em>, <em>41</em>(5), 891–909. (<a
href="https://doi.org/10.1002/sim.9308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Major advances have been made regarding the utilization of machine learning techniques for disease diagnosis and prognosis based on complex and high-dimensional data. Despite all justified enthusiasm, overoptimistic assessments of predictive performance are still common in this area. However, predictive models and medical devices based on such models should undergo a throughout evaluation before being implemented into clinical practice. In this work, we propose a multiple testing framework for (comparative) phase III diagnostic accuracy studies with sensitivity and specificity as co-primary endpoints. Our approach challenges the frequent recommendation to strictly separate model selection and evaluation, that is, to only assess a single diagnostic model in the evaluation study. We show that our parametric simultaneous test procedure asymptotically allows strong control of the family-wise error rate. A multiplicity correction is also available for point and interval estimates. Moreover, we demonstrate in an extensive simulation study that our multiple testing strategy on average leads to a better final diagnostic model and increased statistical power. To plan such studies, we propose a Bayesian approach to determine the optimal number of models to evaluate simultaneously. For this purpose, our algorithm optimizes the expected final model performance given previous (hold-out) data from the model development phase. We conclude that an assessment of multiple promising diagnostic models in the same evaluation study has several advantages when suitable adjustments for multiple comparisons are employed.},
  archive      = {J_SIM},
  author       = {Max Westphal and Antonia Zapf and Werner Brannath},
  doi          = {10.1002/sim.9308},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {891-909},
  shortjournal = {Stat. Med.},
  title        = {A multiple testing framework for diagnostic accuracy studies with co-primary endpoints},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional power and friends: The why and how of
(un)planned, unblinded sample size recalculations in confirmatory
trials. <em>SIM</em>, <em>41</em>(5), 877–890. (<a
href="https://doi.org/10.1002/sim.9288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adapting the final sample size of a trial to the evidence accruing during the trial is a natural way to address planning uncertainty. Since the sample size is usually determined by an argument based on the power of the trial, an interim analysis raises the question of how the final sample size should be determined conditional on the accrued information. To this end, we first review and compare common approaches to estimating conditional power, which is often used in heuristic sample size recalculation rules. We then discuss the connection of heuristic sample size recalculation and optimal two-stage designs, demonstrating that the latter is the superior approach in a fully preplanned setting. Hence, unplanned design adaptations should only be conducted as reaction to trial-external new evidence, operational needs to violate the originally chosen design, or post hoc changes in the optimality criterion but not as a reaction to trial-internal data. We are able to show that commonly discussed sample size recalculation rules lead to paradoxical adaptations where an initially planned optimal design is not invariant under the adaptation rule even if the planning assumptions do not change. Finally, we propose two alternative ways of reacting to newly emerging trial-external evidence in ways that are consistent with the originally planned design to avoid such inconsistencies.},
  archive      = {J_SIM},
  author       = {Kevin Kunzmann and Michael J. Grayling and Kim May Lee and David S. Robertson and Kaspar Rufibach and James M. S. Wason},
  doi          = {10.1002/sim.9288},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {877-890},
  shortjournal = {Stat. Med.},
  title        = {Conditional power and friends: The why and how of (un)planned, unblinded sample size recalculations in confirmatory trials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian kernel machine regression-causal mediation
analysis. <em>SIM</em>, <em>41</em>(5), 860–876. (<a
href="https://doi.org/10.1002/sim.9255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Greater understanding of the pathways through which an environmental mixture operates is important to design effective interventions. We present new methodology to estimate natural direct and indirect effects and controlled direct effects of a complex mixture exposure on an outcome through a mediator variable. We implement Bayesian Kernel Machine Regression (BKMR) to allow for all possible interactions and nonlinear effects of (1) the co-exposures on the mediator, (2) the co-exposures and mediator on the outcome, and (3) selected covariates on the mediator and/or outcome. From the posterior predictive distributions of the mediator and outcome, we simulate counterfactuals to obtain posterior samples, estimates, and credible intervals of the mediation effects. Our simulation study demonstrates that when the exposure-mediator and exposure-mediator-outcome relationships are complex, BKMR-Causal Mediation Analysis performs better than current mediation methods. We applied our methodology to quantify the contribution of birth length as a mediator between in utero co-exposure to arsenic, manganese, and lead, and children&#39;s neurodevelopmental scores, in a prospective birth cohort in Bangladesh. Among younger children, we found a negative (adverse) association between the metal mixture and neurodevelopment. We also found evidence that birth length mediates the effect of exposure to the metal mixture on neurodevelopment for younger children. If birth length were fixed to its 75 t h percentile value, the harmful effect of the metal mixture on neurodevelopment is attenuated, suggesting nutritional interventions to help increase fetal growth, and thus birth length, could potentially block the harmful effect of the metal mixture on neurodevelopment.},
  archive      = {J_SIM},
  author       = {Katrina L. Devick and Jennifer F. Bobb and Maitreyi Mazumdar and Birgit Claus Henn and David C. Bellinger and David C. Christiani and Robert O. Wright and Paige L. Williams and Brent A. Coull and Linda Valeri},
  doi          = {10.1002/sim.9255},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {860-876},
  shortjournal = {Stat. Med.},
  title        = {Bayesian kernel machine regression-causal mediation analysis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical advising: Professional development opportunities
for the biostatistician. <em>SIM</em>, <em>41</em>(5), 847–859. (<a
href="https://doi.org/10.1002/sim.9290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Marissa LeBlanc and Corina S. Rueegg and Nural Bekiroğlu and Tonya M. Esterhuizen and Morten W. Fagerland and Ragnhild S. Falk and Kathrine F. Frøslie and Erika Graf and Georg Heinze and Ulrike Held and René Holst and Theis Lange and Madhu Mazumdar and Ida H. Myrberg and Martin Posch and Jamie C. Sergeant and Werner Vach and Eric A. Vance and Harald Weedon-Fekjær and Manuela Zucknick},
  doi          = {10.1002/sim.9290},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {847-859},
  shortjournal = {Stat. Med.},
  title        = {Statistical advising: Professional development opportunities for the biostatistician},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two decades of statistical education collaboration in the
global south: Lessons learned from an ethiopian project and the way
forward. <em>SIM</em>, <em>41</em>(5), 845–846. (<a
href="https://doi.org/10.1002/sim.9293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Luc Duchateau and Paul Janssen},
  doi          = {10.1002/sim.9293},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {845-846},
  shortjournal = {Stat. Med.},
  title        = {Two decades of statistical education collaboration in the global south: Lessons learned from an ethiopian project and the way forward},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Training and capacity building in medical statistics in
sub-saharan africa: Impact of the london school of hygiene &amp;
tropical medicine MSc in medical statistics, 1969 to 2021. <em>SIM</em>,
<em>41</em>(5), 838–844. (<a
href="https://doi.org/10.1002/sim.9304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since its inception in 1969, the MSc in medical statistics program has placed a high priority on training students from Africa. In this article, we review how the program has shaped, and in turn been shaped by, two substantial capacity building initiatives: (a) a fellowship program, funded by the UK Medical Research Council, and run through the International Statistical Epidemiology Group at the LSHTM, and (b) the Sub-Saharan capacity building in Biostatistics (SSACAB) initiative, administered through the Developing Excellence in Leadership, Training and Science in Africa (DELTAS) program of the African Academy of Sciences. We reflect on the impact of both initiatives, and the implications for future work in this area.},
  archive      = {J_SIM},
  author       = {James R. Carpenter and Jim Todd and Kathy Baisley and John Bradley and Nazarius Mbona Tumwesigye and Patrick Musonda and Tobias Chirwa},
  doi          = {10.1002/sim.9304},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {838-844},
  shortjournal = {Stat. Med.},
  title        = {Training and capacity building in medical statistics in sub-saharan africa: Impact of the london school of hygiene &amp; tropical medicine MSc in medical statistics, 1969 to 2021},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Commentary on training and education in medical statistics,
in celebration of 40 years of statistics in medicine and 50 years of the
MSc medical statistics at LSHTM. <em>SIM</em>, <em>41</em>(5), 835–837.
(<a href="https://doi.org/10.1002/sim.9292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Deborah Ashby},
  doi          = {10.1002/sim.9292},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {835-837},
  shortjournal = {Stat. Med.},
  title        = {Commentary on training and education in medical statistics, in celebration of 40 years of statistics in medicine and 50 years of the MSc medical statistics at LSHTM},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Peter armitage speaking on the occasion of the 50th
anniversary of the m.sc. In medical statistics, LSHTM (for the symposium
on april 11-12, 2019). <em>SIM</em>, <em>41</em>(5), 833–834. (<a
href="https://doi.org/10.1002/sim.9291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Peter Armitage},
  doi          = {10.1002/sim.9291},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {833-834},
  shortjournal = {Stat. Med.},
  title        = {Peter armitage speaking on the occasion of the 50th anniversary of the M.Sc. in medical statistics, LSHTM (for the symposium on april 11-12, 2019)},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Foreword. <em>SIM</em>, <em>41</em>(5), 832. (<a
href="https://doi.org/10.1002/sim.9294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  doi          = {10.1002/sim.9294},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {832},
  shortjournal = {Stat. Med.},
  title        = {Foreword},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SIM 40th anniversary. <em>SIM</em>, <em>41</em>(5), 831. (<a
href="https://doi.org/10.1002/sim.9295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  doi          = {10.1002/sim.9295},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {831},
  shortjournal = {Stat. Med.},
  title        = {SIM 40th anniversary},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). M&amp;m: A maximum duration design with the maxcombo test
for a group sequential trial of an immunotherapy with a random delayed
treatment effect. <em>SIM</em>, <em>41</em>(4), 815–830. (<a
href="https://doi.org/10.1002/sim.9251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A random delayed treatment effect is expected in a confirmatory clinical trial for an immunotherapy due to the individual heterogeneity of physiological conditions. For this reason, the delay time will be assumed to follow a continuous distribution that is difficult to estimate accurately based on the early-phase data, which hinders the specification of the most powerful weighted log-rank test. Therefore, we propose a simulation-based maximum duration design with a robustly powerful Maxcombo test for a group sequential trial for the immunotherapy with the random delayed treatment effect. The design obtains the group sequential boundaries by a simulation procedure and determines the required maximum sample size using a one-dimensional search in which another simulation procedure is used to calculate empirical power. The simulation researches proved the accuracy of the group sequential boundaries and their robustness against the misspecified maximum sample sizes for large samples and revealed their moderate sensitivity against the misspecified survival distributions under the null hypothesis of no difference. The studies investigated whether the type I error rate would inflate under the “inferior” null hypothesis and evaluated the robustness against different distributions of the delay time in terms of the empirical power among the Maxcombo tests and component weighted log-rank tests.},
  archive      = {J_SIM},
  author       = {Bosheng Li and Liwen Su and Yuqing Ye and Fangrong Yan},
  doi          = {10.1002/sim.9251},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {815-830},
  shortjournal = {Stat. Med.},
  title        = {M&amp;M: A maximum duration design with the maxcombo test for a group sequential trial of an immunotherapy with a random delayed treatment effect},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Complex survival trial design by the product integration
method. <em>SIM</em>, <em>41</em>(4), 798–814. (<a
href="https://doi.org/10.1002/sim.9256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonproportional hazards (NPHs) are often observed in survival trials such as the immunotherapy cancer trials. Under NPH, the classical log-rank test can be inefficient, and the estimated hazards ratio from the Cox model is difficult to interpret. The weighted log-rank test, and the tests for comparing the restricted mean survival time or the milestone survival become increasingly popular in handling NPH. The sample size calculation for these tests may require high-dimensional numerical integration. We present a sample size determination method for survival trials via product integration on the basis of a continuous-time multistate Markov model. The main challenge of the method lies in the design of the multistate model under a complex NPH pattern, and this is illustrated for NPH induced by delayed effect with individual heterogeneity in the lag duration, cure fractions, and treatment switching due to disease progression or noncompliance. Numerical examples are presented to demonstrate the accuracy of the proposed method. We obtain the following findings. The powers of the tests for milestone survival and RMST depend on both the trial duration and milestone timepoint, and may not increase as the milestone timepoint increases. If the milestone timepoint is appropriately chosen, the RMST test can be more powerful than the conventional log-rank test in the presence of diminishing treatment effect or in the proportional hazards cure model. In general, the RMST test yields lower power than a proper Fleming-Harrington weighted log-rank test.},
  archive      = {J_SIM},
  author       = {Yongqiang Tang},
  doi          = {10.1002/sim.9256},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {798-814},
  shortjournal = {Stat. Med.},
  title        = {Complex survival trial design by the product integration method},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cancer immunotherapy trial design with random delayed
treatment effect and cure rate. <em>SIM</em>, <em>41</em>(4), 786–797.
(<a href="https://doi.org/10.1002/sim.9258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immunotherapies are increasingly used for treating patients with advanced-stage cancers. However, cancer immunotherapy trials often present delayed treatment effects and long-term survivors which result nonproportional hazard models and challenge the immunotherapy trial designs. In this article, we proposed a general random delayed cure rate model for designing cancer immunotherapy trials. A sample size formula is derived for a weighted log-rank test. The accuracy of sample size estimation is assessed and compared with the existing methods via simulation studies. The sensitivities for misspecifying the random delay time are also studied through simulations.},
  archive      = {J_SIM},
  author       = {Jianrong Wu and Jing Wei},
  doi          = {10.1002/sim.9258},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {786-797},
  shortjournal = {Stat. Med.},
  title        = {Cancer immunotherapy trial design with random delayed treatment effect and cure rate},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A doubly robust method to handle missing multilevel outcome
data with application to the china health and nutrition survey.
<em>SIM</em>, <em>41</em>(4), 769–785. (<a
href="https://doi.org/10.1002/sim.9260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data are common in longitudinal cohort studies and can lead to bias, particularly in studies with informative missingness. Many common methods for handling informatively missing data in survey samples require correctly specifying a model for missingness. Although doubly robust methods exist to provide unbiased regression coefficients in the presence of missing outcome data, these methods do not account for correlation due to clustering inherent in longitudinal or cluster-sampled studies. In this work, we developed a doubly robust method to estimate the regression of an outcome on a predictor in the presence of missing multilevel data on the outcome, which results in consistent estimation of regression coefficients assuming correct specification of either (1) the probability of missingness or (2) the outcome model. This method involves specification of separate hierarchical models for missingness and for the outcome, conditional on observed auxiliary variables and cluster-specific random effects, to account for correlation among observations. We showed this proposed estimator is doubly robust and derived its asymptotic distribution, conducted simulation studies to compare the method to an existing doubly robust method developed for independent data, and applied the method to data from the China Health and Nutrition Survey, an ongoing multilevel longitudinal cohort study.},
  archive      = {J_SIM},
  author       = {Nicole M. Butera and Donglin Zeng and Annie Green Howard and Penny Gordon-Larsen and Jianwen Cai},
  doi          = {10.1002/sim.9260},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {769-785},
  shortjournal = {Stat. Med.},
  title        = {A doubly robust method to handle missing multilevel outcome data with application to the china health and nutrition survey},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting outcomes of phase III oncology trials with
bayesian mediation modeling of tumor response. <em>SIM</em>,
<em>41</em>(4), 751–768. (<a
href="https://doi.org/10.1002/sim.9268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pivotal cancer trials often fail to yield evidence in support of new therapies thought to offer promising alternatives to standards-of-care. Conducting randomized controlled trials in oncology tends to be considerably more expensive than studies of other diseases with comparable sample size. Moreover, phase III trial design often takes place with a paucity of survival data for experimental therapies. Experts have explained the failures on the basis of design flaws which produce studies with unrealistic expectations. This article presents a framework for predicting outcomes of phase III oncology trials using Bayesian mediation models. Predictions, which arise from interim analyses, derive from multivariate modeling of the relationships among treatment, tumor response, and their conjoint effects on survival. Acting as a safeguard against inaccurate pre-trial design assumptions, the methodology may better facilitate rapid closure of negative studies. Additionally the models can be used to inform re-estimations of sample size for under-powered trials that demonstrate survival benefit via tumor response mediation. The methods are applied to predict the outcomes of two colorectal cancer studies. Simulation is used to evaluate and compare models in the absence versus presence of reliable surrogate markers of survival.},
  archive      = {J_SIM},
  author       = {Jie Zhou and Xun Jiang and Hong Amy Xia and Peng Wei and Brian P. Hobbs},
  doi          = {10.1002/sim.9268},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {751-768},
  shortjournal = {Stat. Med.},
  title        = {Predicting outcomes of phase III oncology trials with bayesian mediation modeling of tumor response},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A semiparametric gumbel regression model for analyzing
longitudinal data with non-normal tails. <em>SIM</em>, <em>41</em>(4),
736–750. (<a href="https://doi.org/10.1002/sim.9248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abnormal longitudinal values in biomarkers can be a sign of abnormal status or signal development of a disease. Identifying new biomarkers for early and efficient disease detection is crucial for disease prevention. Compared to the majority of the healthy general population, abnormal values are located within the tails of the biomarker distribution. Thus, parametric regression models that accommodate abnormal values in biomarkers can better detect the association between biomarkers and disease. In this article, we propose semiparametric Gumbel regression models for (1) longitudinal continuous biomarker outcomes, (2) flexibly modeling the time-effect on the outcome, and (3) accounting for the measurement error in biomarker measurements. We adopted the EM algorithm in combination with a two-dimensional grid search to estimate regression parameters and a function of time-effect. We proposed an efficient asymptotic variance estimator for regression parameter estimates. The proposed estimator is asymptotically unbiased in both theory and simulation studies. We applied the proposed model and two other models to investigate associations between fasting blood glucose biomarkers and potential risk factors from a diabetes ancillary study to the Atherosclerosis Risk in Communities (ARIC) study. The real data application was illustrated by fitting the proposed regression model and graphically evaluating the goodness-of-fit value.},
  archive      = {J_SIM},
  author       = {Noorie Hyun and David J. Couper and Donglin Zeng},
  doi          = {10.1002/sim.9248},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {736-750},
  shortjournal = {Stat. Med.},
  title        = {A semiparametric gumbel regression model for analyzing longitudinal data with non-normal tails},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Risk controlled decision trees and random forests for
precision medicine. <em>SIM</em>, <em>41</em>(4), 719–735. (<a
href="https://doi.org/10.1002/sim.9253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical methods generating individualized treatment rules (ITRs) often focus on maximizing expected benefit, but these rules may expose patients to excess risk. For instance, aggressive treatment of type 2 diabetes (T2D) with insulin therapies may result in an ITR which controls blood glucose levels but increases rates of hypoglycemia, diminishing the appeal of the ITR. This work proposes two methods to identify risk-controlled ITRs (rcITR), a class of ITR which maximizes a benefit while controlling risk at a prespecified threshold. A novel penalized recursive partitioning algorithm is developed which optimizes an unconstrained, penalized value function. The final rule is a risk-controlled decision tree (rcDT) that is easily interpretable. A natural extension of the rcDT model, risk controlled random forests (rcRF), is also proposed. Simulation studies demonstrate the robustness of rcRF modeling. Three variable importance measures are proposed to further guide clinical decision-making. Both rcDT and rcRF procedures can be applied to data from randomized controlled trials or observational studies. An extensive simulation study interrogates the performance of the proposed methods. A data analysis of the DURABLE diabetes trial in which two therapeutics were compared is additionally presented. An R package implements the proposed methods ( https://github.com/kdoub5ha/rcITR ).},
  archive      = {J_SIM},
  author       = {Kevin Doubleday and Jin Zhou and Hua Zhou and Haoda Fu},
  doi          = {10.1002/sim.9253},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {719-735},
  shortjournal = {Stat. Med.},
  title        = {Risk controlled decision trees and random forests for precision medicine},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A group-sequential randomized trial design utilizing
supplemental trial data. <em>SIM</em>, <em>41</em>(4), 698–718. (<a
href="https://doi.org/10.1002/sim.9249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Definitive clinical trials are resource intensive, often requiring a large number of participants over several years. One approach to improve the efficiency of clinical trials is to incorporate historical information into the primary trial analysis. This approach has tremendous potential in the areas of pediatric or rare disease trials, where achieving reasonable power is difficult. In this article, we introduce a novel Bayesian group-sequential trial design based on Multisource Exchangeability Models, which allows for dynamic borrowing of historical information at the interim analyses. Our approach achieves synergy between group sequential and adaptive borrowing methodology to attain improved power and reduced sample size. We explore the frequentist operating characteristics of our design through simulation and compare our method to a traditional group-sequential design. Our method achieves earlier stopping of the primary study while increasing power under the alternative hypothesis but has a potential for type I error inflation under some null scenarios. We discuss the issues of decision boundary determination, power and sample size calculations, and the issue of information accrual. We present our method for a continuous and binary outcome, as well as in a linear regression setting.},
  archive      = {J_SIM},
  author       = {Ales Kotalik and David M. Vock and Brian P. Hobbs and Joseph S. Koopmeiners},
  doi          = {10.1002/sim.9249},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {698-718},
  shortjournal = {Stat. Med.},
  title        = {A group-sequential randomized trial design utilizing supplemental trial data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian regularization for a nonstationary gaussian linear
mixed effects model. <em>SIM</em>, <em>41</em>(4), 681–697. (<a
href="https://doi.org/10.1002/sim.9279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In omics experiments, estimation and variable selection can involve thousands of proteins/genes observed from a relatively small number of subjects. Many regression regularization procedures have been developed for estimation and variable selection in such high-dimensional problems. However, approaches have predominantly focused on linear regression models that ignore correlation arising from long sequences of repeated measurements on the outcome. Our work is motivated by the need to identify proteomic biomarkers that improve the prediction of rapid lung-function decline for individuals with cystic fibrosis (CF) lung disease. We extend four Bayesian penalized regression approaches for a Gaussian linear mixed effects model with nonstationary covariance structure to account for the complicated structure of longitudinal lung function data while simultaneously estimating unknown parameters and selecting important protein isoforms to improve predictive performance. Different types of shrinkage priors are evaluated to induce variable selection in a fully Bayesian framework. The approaches are studied with simulations. We apply the proposed method to real proteomics and lung-function outcome data from our motivating CF study, identifying a set of relevant clinical/demographic predictors and a proteomic biomarker for rapid decline of lung function. We also illustrate the methods on CD4 yeast cell-cycle genomic data, confirming that the proposed method identifies transcription factors that have been highlighted in the literature for their importance as cell cycle transcription factors.},
  archive      = {J_SIM},
  author       = {Emrah Gecili and Siva Sivaganesan and Ozgur Asar and John P. Clancy and Assem Ziady and Rhonda D. Szczesniak},
  doi          = {10.1002/sim.9279},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {681-697},
  shortjournal = {Stat. Med.},
  title        = {Bayesian regularization for a nonstationary gaussian linear mixed effects model},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RCRdiff: A fully integrated bayesian method for differential
expression analysis using raw NanoString nCounter data. <em>SIM</em>,
<em>41</em>(4), 665–680. (<a
href="https://doi.org/10.1002/sim.9250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The medium-throughput mRNA abundance platform NanoString nCounter has gained great popularity in the past decade, due to its high sensitivity and technical reproducibility as well as remarkable applicability to ubiquitous formalin fixed paraffin embedded (FFPE) tissue samples. Based on RCRnorm developed for normalizing NanoString nCounter data and Bayesian LASSO for variable selection, we propose a fully integrated Bayesian method, called RCRdiff, to detect differentially expressed (DE) genes between different groups of tissue samples (eg, normal and cancer). Unlike existing methods that often require normalization performed beforehand, RCRdiff directly handles raw read counts and jointly models the behaviors of different types of internal controls along with DE and non-DE gene patterns. Doing so would avoid efficiency loss caused by ignoring estimation uncertainty from the normalization step in a sequential approach and thus can offer more reliable statistical inference. We also propose clustering-based strategies for DE gene selection, which do not require any external dataset and are free of any arbitrary cutoff. Empirical evidence of the attractiveness of RCRdiff is demonstrated via extensive simulation and data examples.},
  archive      = {J_SIM},
  author       = {Can Xu and Xinlei Wang and Johan Lim and Guanghua Xiao and Yang Xie},
  doi          = {10.1002/sim.9250},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {665-680},
  shortjournal = {Stat. Med.},
  title        = {RCRdiff: A fully integrated bayesian method for differential expression analysis using raw NanoString nCounter data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sample size calculation in hierarchical 2×2 factorial trials
with unequal cluster sizes. <em>SIM</em>, <em>41</em>(4), 645–664. (<a
href="https://doi.org/10.1002/sim.9284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a suicide prevention trial with hierarchical treatment allocation (cluster-level and individual-level treatments), we address the sample size requirements for testing the treatment effects as well as their interaction. We assume a linear mixed model, within which two types of treatment effect estimands (controlled effect and marginal effect) are defined. For each null hypothesis corresponding to an estimand, we derive sample size formulas based on large-sample z -approximation, and provide finite-sample modifications based on a t -approximation. We relax the equal cluster size assumption and express the sample size formulas as functions of the mean and coefficient of variation of cluster sizes. We show that the sample size requirement for testing the controlled effect of the cluster-level treatment is more sensitive to cluster size variability than that for testing the controlled effect of the individual-level treatment; the same observation holds for testing the marginal effects. In addition, we show that the sample size for testing the interaction effect is proportional to that for testing the controlled or the marginal effect of the individual-level treatment. We conduct extensive simulations to validate the proposed sample size formulas, and find the empirical power agrees well with the predicted power for each test. Furthermore, the t -approximations often provide better control of type I error rate with a small number of clusters. Finally, we illustrate our sample size formulas to design the motivating suicide prevention factorial trial. The proposed methods are implemented in the R package H2x2Factorial .},
  archive      = {J_SIM},
  author       = {Zizhong Tian and Denise Esserman and Guangyu Tong and Ondrej Blaha and James Dziura and Peter Peduzzi and Fan Li},
  doi          = {10.1002/sim.9284},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {645-664},
  shortjournal = {Stat. Med.},
  title        = {Sample size calculation in hierarchical 2×2 factorial trials with unequal cluster sizes},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Methods to assess evidence consistency in dose-response
model based network meta-analysis. <em>SIM</em>, <em>41</em>(4),
625–644. (<a href="https://doi.org/10.1002/sim.9270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network meta-analysis (NMA) simultaneously estimates multiple relative treatment effects based on evidence that forms a network of treatment comparisons. Heterogeneity in treatment definitions, such as dose, can lead to a violation of the consistency assumption that underpins NMA. Model-based NMA (MBNMA) methods have been proposed that allow functional dose-response relationships to be estimated within an NMA, which avoids lumping different doses together and thereby reduces the likelihood of inconsistency. Dose-response MBNMA relies on appropriate specification of the dose-response relationship as well as consistency of relative effects. In this article we describe methods to check for inconsistency in dose-response MBNMA models. Global and local (node-splitting) tests for inconsistency are described that account for studies with ≥3 arms that are typical in dose-finding trials. We show that consistency needs to be assessed with respect to the choice of dose-response function. We illustrate the methods using a network comparing biologics for moderate-to-severe psoriasis. By comparing results from an Emax and an exponential dose-response function we show that failure to correctly characterise the dose-response can introduce apparent inconsistency. The number of comparisons for which node-splitting is possible is also shown to be dependent on the complexity of the selected dose-response function. We highlight that the nature of dose-finding studies, which typically compare multiple doses of the same agent, provide limited scope to assess inconsistency, but these study designs help guard against inconsistency in the first place. We demonstrate the importance of assessing consistency to obtain robust relative effects to inform drug-development and policy decisions.},
  archive      = {J_SIM},
  author       = {Hugo Pedder and Sofia Dias and Martin Boucher and Meg Bennetts and David Mawdsley and Nicky J. Welton},
  doi          = {10.1002/sim.9270},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {625-644},
  shortjournal = {Stat. Med.},
  title        = {Methods to assess evidence consistency in dose-response model based network meta-analysis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using fractional polynomials and restricted cubic splines to
model non-proportional hazards or time-varying covariate effects in the
cox regression model. <em>SIM</em>, <em>41</em>(3), 612–624. (<a
href="https://doi.org/10.1002/sim.9259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Cox proportional hazards model is used extensively in clinical and epidemiological research. A key assumption of this model is that of proportional hazards. A variable satisfies the proportional hazards assumption if the effect of that variable on the hazard function is constant over time. When the proportional hazards assumption is violated for a given variable, a common approach is to modify the model so that the regression coefficient associated with the given variable is assumed to be a linear function of time (or of log-time), rather than being constant or fixed. However, this is an unnecessarily restrictive assumption. We describe two different methods to allow a regression coefficient, and thus the hazard ratio, in a Cox model to vary as a flexible function of time. These methods use either fractional polynomials or restricted cubic splines to model the log-hazard ratio as a function of time. We illustrate the utility of these methods using data on 12 705 patients who presented to a hospital emergency department with a primary diagnosis of heart failure. We used a Cox model to assess the association between elevated cardiac troponin at presentation and the hazard of death after adjustment for an extensive set of covariates. SAS code for implementing the restricted cubic spline approach is provided, while an existing Stata function allows for the use of fractional polynomials.},
  archive      = {J_SIM},
  author       = {Peter C. Austin and Jiming Fang and Douglas S. Lee},
  doi          = {10.1002/sim.9259},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {612-624},
  shortjournal = {Stat. Med.},
  title        = {Using fractional polynomials and restricted cubic splines to model non-proportional hazards or time-varying covariate effects in the cox regression model},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian multivariate network meta-analysis model for the
difference in restricted mean survival times. <em>SIM</em>,
<em>41</em>(3), 595–611. (<a
href="https://doi.org/10.1002/sim.9276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network meta-analysis (NMA) is essential for clinical decision-making. NMA enables inference for all pair-wise comparisons between interventions available for the same indication, by using both direct evidence and indirect evidence. In randomized trials with time-to event outcome data, such as lung cancer data, conventional NMA methods rely on the hazard ratio and the proportional hazards assumption, and ignore the varying follow-up durations across trials. We introduce a novel multivariate NMA model for the difference in restricted mean survival times (RMST). Our model synthesizes all the available evidence from multiple time points simultaneously and borrows information across time points through within-study covariance and between-study covariance for the differences in RMST. We propose an estimator of the within-study covariance and we then assume it to be known. We estimate the model under the Bayesian framework. We evaluated our model by conducting a simulation study. Our multiple-time-point model yields lower mean squared error over the conventional single-time-point model at all time points, especially when the availability of evidence decreases. We illustrated the model on a network of randomized trials of second-line treatments of advanced non-small-cell lung cancer. Our multiple-time-point model yielded increased precision and detected evidence of benefit at earlier time points as compared to the single-time-point model. Our model has the advantage of providing clinically interpretable measures of treatment effects.},
  archive      = {J_SIM},
  author       = {Xiaoyu Tang and Ludovic Trinquart},
  doi          = {10.1002/sim.9276},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {595-611},
  shortjournal = {Stat. Med.},
  title        = {Bayesian multivariate network meta-analysis model for the difference in restricted mean survival times},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multivariate log-contrast regression with sub-compositional
predictors: Testing the association between preterm infants’ gut
microbiome and neurobehavioral outcomes. <em>SIM</em>, <em>41</em>(3),
580–594. (<a href="https://doi.org/10.1002/sim.9273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To link a clinical outcome with compositional predictors in microbiome analysis, the linear log-contrast model is a popular choice, and the inference procedure for assessing the significance of each covariate is also available. However, with the existence of multiple potentially interrelated outcomes and the information of the taxonomic hierarchy of bacteria, a multivariate analysis method that considers the group structure of compositional covariates and an accompanying group inference method are still lacking. Motivated by a study for identifying the microbes in the gut microbiome of preterm infants that impact their later neurobehavioral outcomes, we formulate a constrained integrative multi-view regression. The neurobehavioral scores form multivariate responses, the log-transformed sub-compositional microbiome data form multi-view feature matrices, and a set of linear constraints on their corresponding sub-coefficient matrices ensures the sub-compositional nature. We assume all the sub-coefficient matrices are possible of low-rank to enable joint selection and inference of sub-compositions/views. We propose a scaled composite nuclear norm penalization approach for model estimation and develop a hypothesis testing procedure through de-biasing to assess the significance of different views. Simulation studies confirm the effectiveness of the proposed procedure. We apply the method to the preterm infant study, and the identified microbes are mostly consistent with existing studies and biological understandings.},
  archive      = {J_SIM},
  author       = {Xiaokang Liu and Xiaomei Cong and Gen Li and Kendra Maas and Kun Chen},
  doi          = {10.1002/sim.9273},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {580-594},
  shortjournal = {Stat. Med.},
  title        = {Multivariate log-contrast regression with sub-compositional predictors: Testing the association between preterm infants&#39; gut microbiome and neurobehavioral outcomes},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving main analysis by borrowing information from
auxiliary data. <em>SIM</em>, <em>41</em>(3), 567–579. (<a
href="https://doi.org/10.1002/sim.9252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many clinical and observational studies, auxiliary data from the same subjects, such as repeated measurements or surrogate variables, will be collected in addition to the data of main interest. Not directly related to the main study, these auxiliary data in practice are rarely incorporated into the main analysis, though they may carry extra information that can help improve the estimation in the main analysis. Under the setting where part of or all subjects have auxiliary data available, we propose an effective weighting approach to borrow the auxiliary information by building a working model for the auxiliary data, where improvement of estimation precision over the main analysis is guaranteed regardless of the specification of the working model. An information index is also constructed to assess how well the selected working model works to improve the main analysis. Both theoretical and numerical studies show the excellent and robust performance of the proposed method in comparison to estimation without using the auxiliary data. Finally, we utilize the Atherosclerosis Risk in Communities study for illustration.},
  archive      = {J_SIM},
  author       = {Chixiang Chen and Peisong Han and Fan He},
  doi          = {10.1002/sim.9252},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {567-579},
  shortjournal = {Stat. Med.},
  title        = {Improving main analysis by borrowing information from auxiliary data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Marginal indirect standardization using latent clustering on
multiple hospitals. <em>SIM</em>, <em>41</em>(3), 554–566. (<a
href="https://doi.org/10.1002/sim.9272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A method was introduced in 2018 of performing indirect standardization for hospital profiling when only the marginal distributions of confounding variables are observed for the index hospital but the full joint covariate distribution is available for the reference hospitals (Wang et al, J Am Stat Assoc 2018; 114:662-630). The method constructs a synthetic comparison hospital using a weighted combination of reference hospitals, with weights assumed to follow a Dirichlet distribution with equal concentration parameters. In this article, we propose a novel method that improves upon the approach in a previous study (Wang et al, J Am Stat Assoc 2018; 114:662-630), by assuming the existence of latent classes among reference hospitals to allow for unequal Dirichlet concentration parameters. The latent class memberships, and thus the hospital weights, are informed by hospital-level characteristics. Our new method results in less biased point estimates and narrower uncertainty intervals for the standardized incidence ratio compared with the existing approach. We show the superiority of our novel methods in an application to a study on prevalence of high-radiation computed tomography exams, as well as in a simulation of the same medical context.},
  archive      = {J_SIM},
  author       = {Yifei Wang and Daniel J. Tancredi and Diana L. Miglioretti},
  doi          = {10.1002/sim.9272},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {554-566},
  shortjournal = {Stat. Med.},
  title        = {Marginal indirect standardization using latent clustering on multiple hospitals},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional gaussian graphical model for estimating
personalized disease symptom networks. <em>SIM</em>, <em>41</em>(3),
543–553. (<a href="https://doi.org/10.1002/sim.9274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The co-occurrence of symptoms may result from the direct interactions between these symptoms and the symptoms can be treated as a system. In addition, subject-specific risk factors (eg, genetic variants, age) can also exert external influence on the system. In this work, we develop a covariate-dependent conditional Gaussian graphical model to obtain personalized symptom networks. The strengths of network connections are modeled as a function of covariates to capture the heterogeneity among individuals and subgroups of individuals. We assess the performance of our proposed method by simulation studies and an application to a large natural history study of Huntington&#39;s disease to investigate the networks of symptoms in multiple clinical domains (motor, cognitive, psychiatric) and identify important brain imaging biomarkers that are associated with the connections. We show that the symptoms in the same clinical domain interact more often with each other than cross domains and the psychiatric subnetwork is the densest network. We validate the findings using the subjects&#39; symptom measurements at follow-up visits.},
  archive      = {J_SIM},
  author       = {Shanghong Xie and Erin McDonnell and Yuanjia Wang},
  doi          = {10.1002/sim.9274},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {543-553},
  shortjournal = {Stat. Med.},
  title        = {Conditional gaussian graphical model for estimating personalized disease symptom networks},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A conditional autoregressive model for genetic association
analysis accounting for genetic heterogeneity. <em>SIM</em>,
<em>41</em>(3), 517–542. (<a
href="https://doi.org/10.1002/sim.9257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Converging evidence from genetic studies and population genetics theory suggest that complex diseases are characterized by remarkable genetic heterogeneity, and individual rare mutations with different effects could collectively play an important role in human diseases. Many existing statistical models for association analysis assume homogeneous effects of genetic variants across all individuals, and could be subject to power loss in the presence of genetic heterogeneity. To consider possible heterogeneous genetic effects among individuals, we propose a conditional autoregressive model. In the proposed method, the genetic effect is considered as a random effect and a score test is developed to test the variance component of genetic random effect. Through simulations, we compare the type I error and power performance of the proposed method with those of the generalized genetic random field and the sequence kernel association test methods under different disease scenarios. We find that our method outperforms the other two methods when (i) the rare variants have the major contribution to the disease, or (ii) the genetic effects vary in different individuals or subgroups of individuals. Finally, we illustrate the new method by applying it to the whole genome sequencing data from the Alzheimer&#39;s Disease Neuroimaging Initiative.},
  archive      = {J_SIM},
  author       = {Xiaoxi Shen and Yalu Wen and Yuehua Cui and Qing Lu},
  doi          = {10.1002/sim.9257},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {517-542},
  shortjournal = {Stat. Med.},
  title        = {A conditional autoregressive model for genetic association analysis accounting for genetic heterogeneity},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A penalization approach to random-effects meta-analysis.
<em>SIM</em>, <em>41</em>(3), 500–516. (<a
href="https://doi.org/10.1002/sim.9261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Systematic reviews and meta-analyses are principal tools to synthesize evidence from multiple independent sources in many research fields. The assessment of heterogeneity among collected studies is a critical step when performing a meta-analysis, given its influence on model selection and conclusions about treatment effects. A common-effect (CE) model is conventionally used when the studies are deemed homogeneous, while a random-effects (RE) model is used for heterogeneous studies. However, both models have limitations. For example, the CE model produces excessively conservative confidence intervals with low coverage probabilities when the collected studies have heterogeneous treatment effects. The RE model, on the other hand, assigns higher weights to small studies compared to the CE model. In the presence of small-study effects or publication bias, the over-weighted small studies from a RE model can lead to substantially biased overall treatment effect estimates. In addition, outlying studies may exaggerate between-study heterogeneity. This article introduces penalization methods as a compromise between the CE and RE models. The proposed methods are motivated by the penalized likelihood approach, which is widely used in the current literature to control model complexity and reduce variances of parameter estimates. We compare the existing and proposed methods with simulated data and several case studies to illustrate the benefits of the penalization methods.},
  archive      = {J_SIM},
  author       = {Yipeng Wang and Lifeng Lin and Christopher G. Thompson and Haitao Chu},
  doi          = {10.1002/sim.9261},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {500-516},
  shortjournal = {Stat. Med.},
  title        = {A penalization approach to random-effects meta-analysis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian spatial models for voxel-wise prostate cancer
classification using multi-parametric magnetic resonance imaging data.
<em>SIM</em>, <em>41</em>(3), 483–499. (<a
href="https://doi.org/10.1002/sim.9245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-parametric magnetic resonance imaging (mpMRI) has been playing an increasingly important role in the detection of prostate cancer (PCa). Various computer-aided detection algorithms were proposed for automated PCa detection by combining information in multiple mpMRI parameters. However, there are specific features of mpMRI, including between-voxel correlation within each prostate and heterogeneity across patients, that have not been fully explored but could potentially improve PCa detection if leveraged appropriately. This article proposes novel Bayesian approaches for voxel-wise PCa classification that accounts for spatial correlation and between-patient heterogeneity in the mpMRI data. Modeling the spatial correlation is challenging due to the extreme high dimensionality of the data, and we propose three scalable approaches based on Nearest Neighbor Gaussian Process (NNGP), reduced-rank approximation, and a conditional autoregressive (CAR) model that approximates a Gaussian Process with the Matérn covariance, respectively. Our simulation study shows that properly modeling the spatial correlation and between-patient heterogeneity can substantially improve PCa classification. Application to in vivo data illustrates that classification is improved by all three spatial modeling approaches considered, while modeling the between-patient heterogeneity does not further improve our classifiers. Among the proposed models, the NNGP-based model is recommended given its high classification accuracy and computational efficiency.},
  archive      = {J_SIM},
  author       = {Jin Jin and Lin Zhang and Ethan Leng and Gregory J. Metzger and Joseph S. Koopmeiners},
  doi          = {10.1002/sim.9245},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {483-499},
  shortjournal = {Stat. Med.},
  title        = {Bayesian spatial models for voxel-wise prostate cancer classification using multi-parametric magnetic resonance imaging data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An extension of the mixed-effects growth model that
considers between-person differences in the within-subject variance and
the autocorrelation. <em>SIM</em>, <em>41</em>(3), 471–482. (<a
href="https://doi.org/10.1002/sim.9280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experience sampling methods have led to a significant increase in the availability of intensive longitudinal data. Typically, this type of data is analyzed with a mixed-effects model that allows to examine hypotheses concerning between-person differences in the mean structure by including multiple random effects per individual (eg, random intercept and random slopes). Here, we describe an extension of this model that—in addition to the random effects for the mean structure—also includes a random effect for the within-subject variance and a random effect for the autocorrelation. After the description of the model, we show how its parameters can be efficiently estimated using a marginal maximum likelihood (ML) approach. We then illustrate the model using a real data example. We also present the results of a small simulation study in which we compare the ML approach with a Bayesian estimation approach.},
  archive      = {J_SIM},
  author       = {Steffen Nestler},
  doi          = {10.1002/sim.9280},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {471-482},
  shortjournal = {Stat. Med.},
  title        = {An extension of the mixed-effects growth model that considers between-person differences in the within-subject variance and the autocorrelation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correlated geometric models of order k and its application
to intensive care unit and leprosy data. <em>SIM</em>, <em>41</em>(3),
449–470. (<a href="https://doi.org/10.1002/sim.9287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geometric models are used to analyse the discrete time until the occurrence of an event of interest (success or consecutive successes). In two real data sets, named leprosy and intensive care unit (ICU), the events correspond, respectively, to abandoning the clinical treatment of leprosy, where abandonment corresponds to four consecutive patient absences from treatment, and the patient&#39;s discharge from the ICU. The distribution proposed in this article, called the correlated geometric distribution of order k (or correlated k -order geometric distribution), consists of including a correlation parameter in the geometric distribution of order k , thus considering the dependence between patient responses until the occurrence of the event. This model proves to be a better option for real data analysis where the effect of individual correlation is considered. The model is applied to real leprosy data to estimate the treatment abandonment probability. Bayesian methods are used to determine the parameter estimators of the models and to evaluate regression models. The covariates are related to the probability of the event by an appropriate link function chosen by Bayesian selection criteria. A diagnostic analysis evaluates the models fit by posterior randomized quantile residuals and influential observations by -divergence measures. This methodology is illustrated by simulation studies and real ICU admission data analysis. Studies show a good fit of the proposed model. Real data analyses also find that the probabilities of the event of interest can be overestimated or underestimated when modeled without considering the effect of dependency on the model.},
  archive      = {J_SIM},
  author       = {Roberta de Souza and Carlos Alberto Ribeiro Diniz},
  doi          = {10.1002/sim.9287},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {449-470},
  shortjournal = {Stat. Med.},
  title        = {Correlated geometric models of order k and its application to intensive care unit and leprosy data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extending hui-walter framework to correlated outcomes with
application to diagnosis tests of an eye disease among premature
infants. <em>SIM</em>, <em>41</em>(3), 433–448. (<a
href="https://doi.org/10.1002/sim.9269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnostic accuracy, a measure of diagnostic tests for correctly identifying patients with or without a target disease, plays an important role in evidence-based medicine. Diagnostic accuracy of a new test ideally should be evaluated by comparing to a gold standard; however, in many medical applications it may be invasive, costly, or even unethical to obtain a gold standard for particular diseases. When the accuracy of a new candidate test under evaluation is assessed by comparison to an imperfect reference test, bias is expected to occur and result in either overestimates or underestimates of its true accuracy. In addition, diagnostic test studies often involve repeated measurements of the same patient, such as the paired eyes or multiple teeth, and generally lead to correlated and clustered data. Using the conventional statistical methods to estimate diagnostic accuracy can be biased by ignoring the within-cluster correlations. Despite numerous statistical approaches have been proposed to tackle this problem, the methodology to deal with correlated and clustered data in the absence of a gold standard is limited. In this article, we propose a method based on the composite likelihood function to derive simple and intuitive closed-form solutions for estimates of diagnostic accuracy, in terms of sensitivity and specificity. Through simulation studies, we illustrate the relative advantages of the proposed method over the existing methods that simply treat an imperfect reference test as a gold standard in correlated and clustered data. Compared with the existing methods, the proposed method can reduce not only substantial bias, but also the computational burden. Moreover, to demonstrate the utility of this approach, we apply the proposed method to the study of National-Eye-Institute-funded Telemedicine Approaches to Evaluating of Acute-Phase Retinopathy of Prematurity (e-ROP), for estimating accuracies of both the ophthalmologist examination and the image evaluation.},
  archive      = {J_SIM},
  author       = {Yu-Lun Liu and Gui-Shuang Ying and Graham E. Quinn and Xiao-Hua Zhou and Yong Chen},
  doi          = {10.1002/sim.9269},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {433-448},
  shortjournal = {Stat. Med.},
  title        = {Extending hui-walter framework to correlated outcomes with application to diagnosis tests of an eye disease among premature infants},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Introduction to computational causal inference using
reproducible stata, r, and python code: A tutorial. <em>SIM</em>,
<em>41</em>(2), 407–432. (<a
href="https://doi.org/10.1002/sim.9234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main purpose of many medical studies is to estimate the effects of a treatment or exposure on an outcome. However, it is not always possible to randomize the study participants to a particular treatment, therefore observational study designs may be used. There are major challenges with observational studies; one of which is confounding. Controlling for confounding is commonly performed by direct adjustment of measured confounders; although, sometimes this approach is suboptimal due to modeling assumptions and misspecification. Recent advances in the field of causal inference have dealt with confounding by building on classical standardization methods. However, these recent advances have progressed quickly with a relative paucity of computational-oriented applied tutorials contributing to some confusion in the use of these methods among applied researchers. In this tutorial, we show the computational implementation of different causal inference estimators from a historical perspective where new estimators were developed to overcome the limitations of the previous estimators (ie, nonparametric and parametric g-formula, inverse probability weighting, double-robust, and data-adaptive estimators). We illustrate the implementation of different methods using an empirical example from the Connors study based on intensive care medicine, and most importantly, we provide reproducible and commented code in Stata, R, and Python for researchers to adapt in their own observational study. The code can be accessed at https://github.com/migariane/Tutorial_Computational_Causal_Inference_Estimators .},
  archive      = {J_SIM},
  author       = {Matthew J. Smith and Mohammad A. Mansournia and Camille Maringe and Paul N. Zivich and Stephen R. Cole and Clémence Leyrat and Aurélien Belot and Bernard Rachet and Miguel A. Luque-Fernandez},
  doi          = {10.1002/sim.9234},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {407-432},
  shortjournal = {Stat. Med.},
  title        = {Introduction to computational causal inference using reproducible stata, r, and python code: A tutorial},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional information and inference in response-adaptive
allocation designs. <em>SIM</em>, <em>41</em>(2), 390–406. (<a
href="https://doi.org/10.1002/sim.9243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Response-adaptive allocation designs refer to a class of designs where the probability an observation is assigned to a treatment is changed throughout an experiment based on the accrued responses. Such procedures result in random treatment sample sizes. Most of the current literature considers unconditional inference procedures in the analysis of response-adaptive allocation designs. The focus of this article is inference conditional on the observed treatment sample sizes. The inverse of information is a description of the large sample variance of the parameter estimates. A simple form for the conditional information relative to unconditional information is derived. It is found that conditional information can be greater than unconditional information. A conditional bootstrap procedure is developed that, in the majority of cases examined, resulted in narrower confidence intervals than relevant unconditional procedures.},
  archive      = {J_SIM},
  author       = {Adam Lane},
  doi          = {10.1002/sim.9243},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {390-406},
  shortjournal = {Stat. Med.},
  title        = {Conditional information and inference in response-adaptive allocation designs},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bayesian phase i/II platform design for co-developing drug
combination therapies for multiple indications. <em>SIM</em>,
<em>41</em>(2), 374–389. (<a
href="https://doi.org/10.1002/sim.9242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing trend to combine a new targeted or immunotherapy agent with the cancer-specific standard of care to treat different types of cancers. We propose a master-protocol-based, Bayesian phase I/II platform design to co-develop combination (BPCC) therapies in multiple indications. Under the BPCC design, only a single master protocol is needed, and the combined drug is evaluated in different indications in a concurrent or staggered fashion. For each indication, we jointly model dose-toxicity and -efficacy relationships and employ Bayesian hierarchical models to borrow information across them for more efficient indication-specific decision-making. To account for the characteristic of targeted or immunotherapy agents that their efficacy may not monotonically increase with the dose, and often plateau at high doses, we use the utility to quantify the risk-benefit tradeoff of the treatment. At each interim, we update the toxicity and efficacy model, as well as the estimate of the utility, based on the observed data across indications to inform the indication-specific decision of dose escalation and de-escalation and identify the optimal biological dose for each indication. Simulation study shows that the BPCC design has desirable operating characteristics, and that it provides an efficient approach to accelerate the development of combination therapies.},
  archive      = {J_SIM},
  author       = {Rongji Mu and Jin Xu and Rui (Sammi) Tang and Scott Kopetz and Ying Yuan},
  doi          = {10.1002/sim.9242},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {374-389},
  shortjournal = {Stat. Med.},
  title        = {A bayesian phase I/II platform design for co-developing drug combination therapies for multiple indications},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A joint model for multivariate longitudinal and survival
data to discover the conversion to alzheimer’s disease. <em>SIM</em>,
<em>41</em>(2), 356–373. (<a
href="https://doi.org/10.1002/sim.9241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disease (AD) is an incurable and progressive disease that starts from mild cognitive impairment and deteriorates over time. Examining the effects of patients&#39; longitudinal cognitive decline on time to conversion to AD and obtaining a reliable diagnostic model are therefore critical to the evaluation of AD prognosis and early treatment. Previous studies either assess patients&#39; cognitive impairment through a single cognitive test or assume it changes linearly across time, thereby leading to an incomplete measure of cognitive decline or overlooking the subtle trajectory pattern of patients&#39; cognitive impairment. This study develops a new joint model to address these shortcomings. First, a dynamic factor analysis model is adopted to characterize cognitive impairment through multiple cognitive measures in a comprehensive manner. Second, a spline-based random coefficient model is proposed to reveal possibly nonlinear trajectories of patients&#39; cognitive decline. Finally, a proportional hazard model is considered to examine the effects of time-invariant markers and time-variant cognitive impairment on AD hazards. A Bayesian approach coupled with spline approximation techniques and MCMC methods is developed to conduct statistical inference. The application of the proposed method to the Alzheimer&#39;s Disease Neuroimaging Initiative study provides new insights into the prevention of AD and shows a high prediction capacity of the proposed method.},
  archive      = {J_SIM},
  author       = {Kai Kang and Deng Pan and Xinyuan Song},
  doi          = {10.1002/sim.9241},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {356-373},
  shortjournal = {Stat. Med.},
  title        = {A joint model for multivariate longitudinal and survival data to discover the conversion to alzheimer&#39;s disease},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Penalized poisson model for network meta-analysis of
individual patient time-to-event data. <em>SIM</em>, <em>41</em>(2),
340–355. (<a href="https://doi.org/10.1002/sim.9240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network meta-analysis (NMA) allows the combination of direct and indirect evidence from a set of randomized clinical trials. Performing NMA using individual patient data (IPD) is considered as a “gold standard” approach as it provides several advantages over NMA based on aggregate data. For example, it allows to perform advanced modeling of covariates or covariate-treatment interactions. An important issue in IPD NMA is the selection of influential parameters among terms that account for inconsistency, covariates, covariate-by-treatment interactions or nonproportionality of treatments effect for time to event data. This issue has not been deeply studied in the literature yet and in particular not for time-to-event data. A major difficulty is to jointly account for between-trial heterogeneity which could have a major influence on the selection process. The use of penalized generalized mixed effect model is a solution, but existing implementations have several shortcomings and an important computational cost that precludes their use for complex IPD NMA. In this article, we propose a penalized Poisson regression model to perform IPD NMA of time-to-event data. It is based only on fixed effect parameters which improve its computational cost over the use of random effects. It could be easily implemented using existing penalized regression package. Computer code is shared for implementation. The methods were applied on simulated data to illustrate the importance to take into account between trial heterogeneity during the selection procedure. Finally, it was applied to an IPD NMA of overall survival of chemotherapy and radiotherapy in nasopharyngeal carcinoma.},
  archive      = {J_SIM},
  author       = {Edouard Ollier and Pierre Blanchard and Gwénaël Le Teuff and Stefan Michiels},
  doi          = {10.1002/sim.9240},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {340-355},
  shortjournal = {Stat. Med.},
  title        = {Penalized poisson model for network meta-analysis of individual patient time-to-event data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A causal data fusion method for the general exposure and
outcome. <em>SIM</em>, <em>41</em>(2), 328–339. (<a
href="https://doi.org/10.1002/sim.9239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of the big data era, the need to combine multiple individual data sets to draw causal effects arises naturally in many medical and biological applications. Especially each data set cannot measure enough confounders to infer the causal effect of an exposure on an outcome. In this article, we extend the method proposed by a previous study to causal data fusion of more than two data sets without external validation and to a more general (continuous or discrete) exposure and outcome. Theoretically, we obtain the condition for identifiability of exposure effects using multiple individual data sources for the continuous or discrete exposure and outcome. The simulation results show that our proposed causal data fusion method has unbiased causal effect estimate and higher precision than traditional regression, meta-analysis and statistical matching methods. We further apply our method to study the causal effect of BMI on glucose level in individuals with diabetes by combining two data sets. Our method is essential for causal data fusion and provides important insights into the ongoing discourse on the empirical analysis of merging multiple individual data sources.},
  archive      = {J_SIM},
  author       = {Hongkai Li and Jinzhu Jia and Ran Yan and Fuzhong Xue and Zhi Geng},
  doi          = {10.1002/sim.9239},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {328-339},
  shortjournal = {Stat. Med.},
  title        = {A causal data fusion method for the general exposure and outcome},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal diagnostic test allocation strategy during the
COVID-19 pandemic and beyond. <em>SIM</em>, <em>41</em>(2), 310–327. (<a
href="https://doi.org/10.1002/sim.9238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Timely diagnostic testing for active SARS-CoV-2 viral infections is key to controlling the spread of the virus and preventing severe disease. A central public health challenge is defining test allocation strategies with limited resources. In this paper, we provide a mathematical framework for defining an optimal strategy for allocating viral diagnostic tests. The framework accounts for imperfect test results, selective testing in certain high-risk patient populations, practical constraints in terms of budget and/or total number of available tests, and the purpose of testing. Our method is not only useful for detecting infections, but can also be used for long-time surveillance to detect new outbreaks. In our proposed approach, tests can be allocated across population strata defined by symptom severity and other patient characteristics, allowing the test allocation plan to prioritize higher risk patient populations. We illustrate our framework using historical data from the initial wave of the COVID-19 outbreak in New York City. We extend our proposed method to address the challenge of allocating two different types of diagnostic tests with different costs and accuracy, for example, the RT-PCR and the rapid antigen test (RAT), under budget constraints. We show how this latter framework can be useful to reopening of college campuses where university administrators are challenged with finite resources for community surveillance. We provide a R Shiny web application allowing users to explore test allocation strategies across a variety of pandemic scenarios. This work can serve as a useful tool for guiding public health decision-making at a community level and adapting testing plans to different stages of an epidemic. The conceptual framework has broader relevance beyond the current COVID-19 pandemic.},
  archive      = {J_SIM},
  author       = {Jiacong Du and Lauren J Beesley and Seunggeun Lee and Xiang Zhou and Walter Dempsey and Bhramar Mukherjee},
  doi          = {10.1002/sim.9238},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {310-327},
  shortjournal = {Stat. Med.},
  title        = {Optimal diagnostic test allocation strategy during the COVID-19 pandemic and beyond},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constrained hierarchical bayesian model for latent subgroups
in basket trials with two classifiers. <em>SIM</em>, <em>41</em>(2),
298–309. (<a href="https://doi.org/10.1002/sim.9237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The basket trial in oncology is a novel clinical trial design that enables the simultaneous assessment of one treatment in multiple cancer types. In addition to the usual basket classifier of the cancer types, many recent basket trials further contain other classifiers like biomarkers that potentially affect the clinical outcomes. In other words, the treatment effects in those baskets are often categorized by not only the cancer types but also the levels of other classifiers. Therefore, the assumption of exchangeability is often violated when some baskets are more sensitive to the targeted treatment, whereas others are less. In this article, we propose a constrained hierarchical Bayesian model for latent subgroups (CHBM-LS) to deal with potential heterogeneity of treatment effects due to both the cancer type (first classifier) and another classifier (second classifier) in basket trials. Different baskets defined by multiple cancer types and multiple levels of the second classifier are aggregated into subgroups using a latent subgroup modeling approach. Within each latent subgroup, the treatment effects are similar and approximately exchangeable to borrow information. The CHBM-LS approach evaluates the treatment effect for each basket while allowing adaptive information borrowing across the baskets by identifying latent subgroups. The simulation study shows that the CHBM-LS approach outperforms other approaches with higher statistical power and better-controlled type I error rates under various scenarios with heterogeneous treatment effects across baskets.},
  archive      = {J_SIM},
  author       = {Kentaro Takeda and Shufang Liu and Alan Rong},
  doi          = {10.1002/sim.9237},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {298-309},
  shortjournal = {Stat. Med.},
  title        = {Constrained hierarchical bayesian model for latent subgroups in basket trials with two classifiers},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). New methods for multiple testing in permutation inference
for the general linear model. <em>SIM</em>, <em>41</em>(2), 276–297. (<a
href="https://doi.org/10.1002/sim.9236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Permutation methods are commonly used to test the significance of regressors of interest in general linear models (GLMs) for functional (image) data sets, in particular for neuroimaging applications as they rely on mild assumptions. Permutation inference for GLMs typically consists of three parts: choosing a relevant test statistic, computing pointwise permutation tests, and applying a multiple testing correction. We propose new multiple testing methods as an alternative to the commonly used maximum value of test statistics across the image. The new methods improve power and robustness against inhomogeneity of the test statistic across its domain. The methods rely on sorting the permuted functional test statistics based on pointwise rank measures; still, they can be implemented even for large data. The performance of the methods is demonstrated through a designed simulation experiment and an example of brain imaging data. We developed the R package GET, which can be used for the computation of the proposed procedures.},
  archive      = {J_SIM},
  author       = {Tomáš Mrkvička and Mari Myllymäki and Mikko Kuronen and Naveen Naidu Narisetty},
  doi          = {10.1002/sim.9236},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {276-297},
  shortjournal = {Stat. Med.},
  title        = {New methods for multiple testing in permutation inference for the general linear model},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Response-adaptive treatment allocation for clinical studies
with recurrent event and terminal event data. <em>SIM</em>,
<em>41</em>(2), 258–275. (<a
href="https://doi.org/10.1002/sim.9235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In long-term clinical studies, recurrent event data are frequently collected to contrast the efficacy of two different treatments. However, the recurrent event process can be stopped by a terminal event, such as death. For analyzing recurrent event and terminal event data, joint frailty modeling has recently received considerable attention because it makes it possible to study the joint evolution over time of both recurrent and terminal event processes and gives consistent and efficient parameters. For a two-arm clinical trial design based on these data sets, there has been limited research on investigating the balanced design, let alone adaptive treatment allocation. Although equal sample size allocation obtained for both treatments is intuitively first adopted in a trial design, if one treatment is expected to be superior, it may be desirable to allocate more subjects to the effective treatment. In this article, we calculate the required sample size based on restricted randomization and then propose a target response-adaptive randomization procedure for recurrent and terminal event outcomes based on the joint frailty model. A randomization procedure, the doubly adaptive biased coin design that targets some optimal allocations, is implemented. The proposed adaptive treatment allocation schemes have been shown to be capable of reducing the number of trial participants who receive inferior treatment while simultaneously reaching an optimal target, as well as retaining a comparable test power as compared to a restricted randomization design. Finally, two clinical studies, the COAPT trial and the A-HeFT trial, are used to illustrate the advantages of adopting the proposed procedure.},
  archive      = {J_SIM},
  author       = {Pei-Fang Su},
  doi          = {10.1002/sim.9235},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {258-275},
  shortjournal = {Stat. Med.},
  title        = {Response-adaptive treatment allocation for clinical studies with recurrent event and terminal event data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation of standard deviations and inverse-variance
weights from an observed range. <em>SIM</em>, <em>41</em>(2), 242–257.
(<a href="https://doi.org/10.1002/sim.9233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A variety of methods have been proposed to estimate a standard deviation, when only a sample range has been observed or reported. This problem occurs in the interpretation of individual clinical studies that are incompletely reported, and also in their incorporation into meta-analyses. The methods differ with respect to their focus being either on the standard deviation in the underlying population or on the particular sample in hand, a distinction that has not been widely recognized. In this article, we contrast and compare various estimators of these two quantities with respect to bias and mean squared error, for normally distributed data. We show that unbiased estimators are available for either quantity, and recommend our preferred methods. We also propose a Taylor series method to obtain inverse-variance weights, for samples where only the sample range is available; this method yields very little bias, even for quite small samples. In contrast, the naïve approach of simply taking the inverse of an estimated variance is shown to be substantially biased, and can place unduly large weight on small samples, such as small clinical trials in a meta-analysis. Accordingly, this naïve (but commonly used) method is not recommended.},
  archive      = {J_SIM},
  author       = {Stephen D. Walter and Jan Rychtář and Dewey Taylor and Narayanaswamy Balakrishnan},
  doi          = {10.1002/sim.9233},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {242-257},
  shortjournal = {Stat. Med.},
  title        = {Estimation of standard deviations and inverse-variance weights from an observed range},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weighted least-squares regression with competing risks data.
<em>SIM</em>, <em>41</em>(2), 227–241. (<a
href="https://doi.org/10.1002/sim.9232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semiparametric accelerated failure time (AFT) model linearly relates the logarithm of the failure time to a set of covariates, while leaving the error distribution unspecified. This model has been widely investigated in survival literature due to its simple interpretation and relationship with linear models. However, there has been much less focus on developing AFT-type linear regression methods for analyzing competing risks data, in which patients can potentially experience one of multiple failure causes. In this article, we propose a simple least-squares (LS) linear regression model for a cause-specific subdistribution function, where the conventional LS equation is modified to account for data incompleteness under competing risks. The proposed estimators are shown to be consistent and asymptotically normal with consistent estimation of the variance-covariance matrix. We further extend the proposed methodology to risk prediction and analysis under clustered competing risks scenario. Simulation studies suggest that the proposed method provides rapid and valid statistical inferences and predictions. Application of our method to two oncology datasets demonstrate its utility in routine clinical data analysis.},
  archive      = {J_SIM},
  author       = {Sangbum Choi and Taehwa Choi and Hyunsoon Cho and Dipankar Bandyopadhyay},
  doi          = {10.1002/sim.9232},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {227-241},
  shortjournal = {Stat. Med.},
  title        = {Weighted least-squares regression with competing risks data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple imputation procedures for estimating causal effects
with multiple treatments with application to the comparison of
healthcare providers. <em>SIM</em>, <em>41</em>(1), 208–226. (<a
href="https://doi.org/10.1002/sim.9231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Choosing between multiple healthcare providers requires us to simultaneously compare the expected outcomes under each provider. This comparison is complex because the composition of patients treated by each provider may differ. Similar issues arise when simultaneously comparing the adverse effects of interventions using non-randomized data. To simultaneously estimate the effects of multiple providers/interventions we propose procedures that explicitly impute the set of potential outcomes for each subject. The procedures are based on different specifications of the generalized additive models (GAM) and the Bayesian additive regression trees (BART). We compare the performance of the proposed procedures to previously proposed matching and weighting procedures using an extensive simulation study for continuous outcomes. Our simulations show that when the distributions of the covariates across treatment groups have adequate overlap, the multiple imputation procedures based on separate BART or GAM models in each treatment group are generally superior to weighting based methods and have similar and sometimes better performance than matching on the logit of the generalized propensity score. Another advantage of these multiple imputation procedures is the ability to provide point and interval estimates to a wide range of causal effect estimands. We apply the proposed procedures to comparing multiple nursing homes in Massachusetts for readmission outcomes. The proposed approach can be applied to other causal effects applications with multiple treatments.},
  archive      = {J_SIM},
  author       = {Gabriella C. Silva and Roee Gutman},
  doi          = {10.1002/sim.9231},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {208-226},
  shortjournal = {Stat. Med.},
  title        = {Multiple imputation procedures for estimating causal effects with multiple treatments with application to the comparison of healthcare providers},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). MOVER confidence intervals for a difference or ratio effect
parameter under stratified sampling. <em>SIM</em>, <em>41</em>(1),
194–207. (<a href="https://doi.org/10.1002/sim.9230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stratification is commonly employed in clinical trials to reduce the chance covariate imbalances and increase the precision of the treatment effect estimate. We propose a general framework for constructing the confidence interval (CI) for a difference or ratio effect parameter under stratified sampling by the method of variance estimates recovery (MOVER). We consider the additive variance and additive CI approaches for the difference, in which either the CI for the weighted difference, or the CI for the weighted effect in each group, or the variance for the weighted difference is calculated as the weighted sum of the corresponding stratum-specific statistics. The CI for the ratio is derived by the Fieller and log-ratio methods. The weights can be random quantities under the assumption of a constant effect across strata, but this assumption is not needed for fixed weights. These methods can be easily applied to different endpoints in that they require only the point estimate, CI, and variance estimate for the measure of interest in each group across strata. The methods are illustrated with two real examples. In one example, we derive the MOVER CIs for the risk difference and risk ratio for binary outcomes. In the other example, we compare the restricted mean survival time and milestone survival in stratified analysis of time-to-event outcomes. Simulations show that the proposed MOVER CIs generally outperform the standard large sample CIs, and that the additive CI approach performs better than the additive variance approach. Sample SAS code is provided in the Supplementary Material.},
  archive      = {J_SIM},
  author       = {Yongqiang Tang},
  doi          = {10.1002/sim.9230},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {194-207},
  shortjournal = {Stat. Med.},
  title        = {MOVER confidence intervals for a difference or ratio effect parameter under stratified sampling},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Set-regression with applications to subgroup analysis.
<em>SIM</em>, <em>41</em>(1), 180–193. (<a
href="https://doi.org/10.1002/sim.9229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression is a commonly used statistical model. It is the conditional mean of the response given covariates μ ⁡ ( x ) = E ⁡ ( Y | X = x ) . However, in some practical problems, the interest is the conditional mean of the response given the covariates belonging to some set A . Notably, in precision medicine and subgroup analysis in clinical trials, the aim is to identify subjects who benefit the most from the treatment, or identify an optimal set in the covariate space which manifests treatment favoritism if a subject&#39;s covariates fall in this set and the subject is classified to the favorable treatment subgroup. Existing methods for subgroup analysis achieve this indirectly by using classical regression. This motivates us to develop a new type of regression: set-regression , defined as μ ⁡ ( A ) = E ⁡ ( Y | X ∈ A ) which directly addresses the subgroup analysis problem. This extends not only the classical regression model but also improves recursive partitioning and support vector machine approaches, and is particularly suitable for objectives involving optimization of the regression over sets, such as subgroup analysis. We show that the new versatile set-regression identifies the subgroup with increased accuracy. It is easy to use. Simulation studies also show superior performance of the proposed method in finite samples.},
  archive      = {J_SIM},
  author       = {Ao Yuan and Lida Wang and Ming T. Tan},
  doi          = {10.1002/sim.9229},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {180-193},
  shortjournal = {Stat. Med.},
  title        = {Set-regression with applications to subgroup analysis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Measurement errors in control risk regression: A comparison
of correction techniques. <em>SIM</em>, <em>41</em>(1), 163–179. (<a
href="https://doi.org/10.1002/sim.9228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Control risk regression is a diffuse approach for meta-analysis about the effectiveness of a treatment, relating the measure of risk with which the outcome occurs in the treated group to that in the control group. The severity of illness is a source of between-study heterogeneity that can be difficult to measure. It can be approximated by the rate of events in the control group. Since the estimate is a surrogate for the underlying risk, it is prone to measurement error. Correction methods are necessary to provide reliable inference. This article illustrates the extent of measurement error effects under different scenarios, including departures from the classical normality assumption for the control risk distribution. The performance of different measurement error corrections is examined. Attention will be paid to likelihood-based structural methods assuming a distribution for the control risk measure and to functional methods avoiding the assumption, namely, a simulation-based method and two score function methods. Advantages and limits of the approaches are evaluated through simulation. In case of large heterogeneity, structural approaches are preferable to score methods, while score methods perform better for small heterogeneity and small sample size. The simulation-based approach has a satisfactory behavior whichever the examined scenario, with no convergence issues. The methods are applied to a meta-analysis about the association between diabetes and risk of Parkinson disease. The study intends to make researchers aware of the measurement error problem occurring in control risk regression and lead them to the use of appropriate correction techniques to prevent fallacious conclusions.},
  archive      = {J_SIM},
  author       = {Annamaria Guolo},
  doi          = {10.1002/sim.9228},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {163-179},
  shortjournal = {Stat. Med.},
  title        = {Measurement errors in control risk regression: A comparison of correction techniques},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identification of geographic clusters for temporal
heterogeneity with application to dengue surveillance. <em>SIM</em>,
<em>41</em>(1), 146–162. (<a
href="https://doi.org/10.1002/sim.9227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying transmission of hot spots with temporal trends is important for reducing infectious disease propagation. Cluster analysis is a particularly useful tool to explore underlying stochastic processes between observations by grouping items into categories by their similarity. In a study of epidemic propagation, clustering geographic regions that have similar time series could help researchers track diffusion routes from a common source of an infectious disease. In this article, we propose a two-stage scan statistic to classify regions into various geographic clusters by their temporal heterogeneity. The proposed scan statistic is more flexible than traditional methods in that contiguous and nonproximate regions with similar temporal patterns can be identified simultaneously. A simulation study and data analysis for a dengue fever infection are also presented for illustration.},
  archive      = {J_SIM},
  author       = {Pei-Sheng Lin},
  doi          = {10.1002/sim.9227},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {146-162},
  shortjournal = {Stat. Med.},
  title        = {Identification of geographic clusters for temporal heterogeneity with application to dengue surveillance},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ratio estimators of intervention effects on event rates in
cluster randomized trials. <em>SIM</em>, <em>41</em>(1), 128–145. (<a
href="https://doi.org/10.1002/sim.9226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider five asymptotically unbiased estimators of intervention effects on event rates in non-matched and matched-pair cluster randomized trials, including ratio of mean counts ( r 1 ) , ratio of mean cluster-level event rates ( r 2 ) , ratio of event rates ( r 3 ) , double ratio of counts ( r 4 ) , and double ratio of event rates ( r 5 ) . In the absence of an indirect effect, they all estimate the direct effect of the intervention. Otherwise, r 1 , r 2 , and r 3 estimate the total effect, which comprises the direct and indirect effects, whereas r 4 and r 5 estimate the direct effect only. We derive the conditions under which each estimator is more precise or powerful than its alternatives. To control bias in studies with a small number of clusters, we propose a set of approximately unbiased estimators. We evaluate their properties by simulation and apply the methods to a trial of seasonal malaria chemoprevention. The approximately unbiased estimators are practically unbiased and their confidence intervals usually have coverage probability close to the nominal level; the asymptotically unbiased estimators perform well when the number of clusters is approximately 32 or more per trial arm. Despite its simplicity, performs comparably with and in trials with a large but realistic number of clusters. When the variability of baseline event rate is large and there is no indirect effect, and tend to offer higher power than , and . We discuss the implications of these findings to the planning and analysis of cluster randomized trials.},
  archive      = {J_SIM},
  author       = {Xiangmei Ma and Paul Milligan and Kwok Fai Lam and Yin Bun Cheung},
  doi          = {10.1002/sim.9226},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {128-145},
  shortjournal = {Stat. Med.},
  title        = {Ratio estimators of intervention effects on event rates in cluster randomized trials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian consensus clustering for multivariate longitudinal
data. <em>SIM</em>, <em>41</em>(1), 108–127. (<a
href="https://doi.org/10.1002/sim.9225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical and epidemiological studies, there is a growing interest in studying the heterogeneity among patients based on longitudinal characteristics to identify subtypes of the study population. Compared to clustering a single longitudinal marker, simultaneously clustering multiple longitudinal markers allow additional information to be incorporated into the clustering process, which reveals co-existing longitudinal patterns and generates deeper biological insight. In the current study, we propose a Bayesian consensus clustering (BCC) model for multivariate longitudinal data. Instead of arriving at a single overall clustering, the proposed model allows each marker to follow marker-specific local clustering and these local clusterings are aggregated to find a global (consensus) clustering. To estimate the posterior distribution of model parameters, a Gibbs sampling algorithm is proposed. We apply our proposed model to the primary biliary cirrhosis study to identify patient subtypes that may be associated with their prognosis. We also perform simulation studies to compare the clustering performance between the proposed model and existing models under several scenarios. The results demonstrate that the proposed BCC model serves as a useful tool for clustering multivariate longitudinal data.},
  archive      = {J_SIM},
  author       = {Zihang Lu and Wendy Lou},
  doi          = {10.1002/sim.9225},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {108-127},
  shortjournal = {Stat. Med.},
  title        = {Bayesian consensus clustering for multivariate longitudinal data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Use of likelihood estimates for variances for the design and
evaluation of multiregional clinical trials with heterogeneous
variances. <em>SIM</em>, <em>41</em>(1), 87–107. (<a
href="https://doi.org/10.1002/sim.9224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Globalized drug development studies, such as multiregional clinical trials (MRCTs), have attracted much attention due to their ability to expedite drug development and shorten the time lag of drug release. While observing the overall effect of a new drug, the region-specific effects to support drug registration in constituent regions can also be evaluated. Several challenges arise in conducting MRCTs, such as the heterogeneity in the variability of the primary endpoint across regions. However, most of the existing statistical methods assume a common variability, which may not be valid in practice due to differences across regions (eg, diversities in ethnicity or disparities in medical culture/practice). We present a statistical method for the design and evaluation of MRCTs to consider the heterogeneous variability across regions. We assessed the overall sample size requirement and addressed the region-specific sample size determination to establish the consistency of treatment effects between the specific region and the entire group. We demonstrate the proposed approach with numerical examples.},
  archive      = {J_SIM},
  author       = {Yuh-Jenn Wu and Yu-Chieh Cheng and Chieh Chiang and Li-Hsueh Cheng and Ching-Ti Liu and Chin-Fu Hsiao},
  doi          = {10.1002/sim.9224},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {87-107},
  shortjournal = {Stat. Med.},
  title        = {Use of likelihood estimates for variances for the design and evaluation of multiregional clinical trials with heterogeneous variances},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Propensity score methods for merging observational and
experimental datasets. <em>SIM</em>, <em>41</em>(1), 65–86. (<a
href="https://doi.org/10.1002/sim.9223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider how to merge a limited amount of data from a randomized controlled trial (RCT) into a much larger set of data from an observational data base (ODB), to estimate an average causal treatment effect. Our methods are based on stratification. The strata are defined in terms of effect moderators as well as propensity scores estimated in the ODB. Data from the RCT are placed into the strata they would have occupied, had they been in the ODB instead. We assume that treatment differences are comparable in the two data sources. Our first “spiked-in” method simply inserts the RCT data into their corresponding ODB strata. We also consider a data-driven convex combination of the ODB and RCT treatment effect estimates within each stratum. Using the delta method and simulations, we identify a bias problem with the spiked-in estimator that is ameliorated by the convex combination estimator. We apply our methods to data from the Women&#39;s Health Initiative, a study of thousands of postmenopausal women which has both observational and experimental data on hormone therapy (HT). Using half of the RCT to define a gold standard, we find that a version of the spiked-in estimator yields lower-MSE estimates of the causal impact of HT on coronary heart disease than would be achieved using either a small RCT or the observational component on its own.},
  archive      = {J_SIM},
  author       = {Evan T. R. Rosenman and Art B. Owen and Mike Baiocchi and Hailey R. Banack},
  doi          = {10.1002/sim.9223},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {65-86},
  shortjournal = {Stat. Med.},
  title        = {Propensity score methods for merging observational and experimental datasets},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint inference about the AUC and youden index for paired
biomarkers. <em>SIM</em>, <em>41</em>(1), 37–64. (<a
href="https://doi.org/10.1002/sim.9222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is common to compare biomarkers&#39; diagnostic or prognostic performance using some summary ROC measures such as the area under the ROC curve (AUC) or the Youden index. We propose to compare two paired biomarkers using both the AUC and the Youden index since the two indices describe different aspects of the ROC curve. This comparison can be made by estimating the joint confidence region (an elliptical area) of the differences of the paired AUCs and the Youden indices. Furthermore, for deciding if one marker is better than the other in terms of both the A ⁢ U ⁢ C and the Youden index ( J ), we can test H 0 : A ⁢ U ⁢ C a ≤ A ⁢ U ⁢ C b or J a ≤ J b against H a : A ⁢ U ⁢ C a &gt; A ⁢ U ⁢ C b and J a &gt; J b using the paired differences. The construction of such a joint hypothesis is an example of the multivariate order-restricted hypotheses. For such a hypothesis, we propose and compare three testing procedures: (1) the intersection-union test ( ); (2) the conditional test; and (3) the joint test. The performance of the proposed inference methods was evaluated and compared through simulations. The simulation results demonstrate that the proposed joint confidence region maintains the desired confidence level, and all three tests maintain the type I error under the null. Furthermore, among the three proposed testing methods, the conditional test is the preferred approach with markedly larger power consistently than the other two competing methods.},
  archive      = {J_SIM},
  author       = {Jingjing Yin and Hani Samawi and Lili Tian},
  doi          = {10.1002/sim.9222},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {37-64},
  shortjournal = {Stat. Med.},
  title        = {Joint inference about the AUC and youden index for paired biomarkers},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian semiparametric joint modeling of longitudinal
explanatory variables of mixed types and a binary outcome. <em>SIM</em>,
<em>41</em>(1), 17–36. (<a
href="https://doi.org/10.1002/sim.9221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many prospective biomedical studies collect longitudinal clinical and lifestyle data that are both continuous and discrete. In some studies, there is interest in the association between a binary outcome and the values of these longitudinal measurements at a specific time point. A common problem in these studies is inconsistency in timing of measurements and missing follow-ups which can lead to few measurements at the time of interest. Some methods have been developed to address this problem, but are only applicable to continuous measurements. To address this limitation, we propose a new class of joint models for a binary outcome and longitudinal explanatory variables of mixed types. The longitudinal model uses a latent normal random variable construction with regression splines to model time-dependent trends in mean with a Dirichlet Process prior assigned to random effects to relax distribution assumptions. We also standardize timing of the explanatory variables by relating the binary outcome to imputed longitudinal values at a set time point. The proposed model is evaluated through simulation studies and applied to data from a cancer survivor study of participants in the Women&#39;s Health Initiative.},
  archive      = {J_SIM},
  author       = {Woobeen Lim and Michael L. Pennell and Michelle J. Naughton and Electra D. Paskett},
  doi          = {10.1002/sim.9221},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {17-36},
  shortjournal = {Stat. Med.},
  title        = {Bayesian semiparametric joint modeling of longitudinal explanatory variables of mixed types and a binary outcome},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A simulation study of disaggregation regression for spatial
disease mapping. <em>SIM</em>, <em>41</em>(1), 1–16. (<a
href="https://doi.org/10.1002/sim.9220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disaggregation regression has become an important tool in spatial disease mapping for making fine-scale predictions of disease risk from aggregated response data. By including high resolution covariate information and modeling the data generating process on a fine scale, it is hoped that these models can accurately learn the relationships between covariates and response at a fine spatial scale. However, validating these high resolution predictions can be a challenge, as often there is no data observed at this spatial scale. In this study, disaggregation regression was performed on simulated data in various settings and the resulting fine-scale predictions are compared to the simulated ground truth. Performance was investigated with varying numbers of data points, sizes of aggregated areas and levels of model misspecification. The effectiveness of cross validation on the aggregate level as a measure of fine-scale predictive performance was also investigated. Predictive performance improved as the number of observations increased and as the size of the aggregated areas decreased. When the model was well-specified, fine-scale predictions were accurate even with small numbers of observations and large aggregated areas. Under model misspecification predictive performance was significantly worse for large aggregated areas but remained high when response data was aggregated over smaller regions. Cross-validation correlation on the aggregate level was a moderately good predictor of fine-scale predictive performance. While these simulations are unlikely to capture the nuances of real-life response data, this study gives insight into the effectiveness of disaggregation regression in different contexts.},
  archive      = {J_SIM},
  author       = {Rohan Arambepola and Tim C. D. Lucas and Anita K. Nandi and Peter W. Gething and Ewan Cameron},
  doi          = {10.1002/sim.9220},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Stat. Med.},
  title        = {A simulation study of disaggregation regression for spatial disease mapping},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
