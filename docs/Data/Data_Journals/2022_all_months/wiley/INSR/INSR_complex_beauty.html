<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>INSR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="insr---45">INSR - 45</h2>
<ul>
<li><details>
<summary>
(2022). A joint normal-binary (probit) model. <em>INSR</em>,
<em>90</em>(S1), S37–S51. (<a
href="https://doi.org/10.1111/insr.12532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical research, often hierarchical binary and continuous responses need to be jointly modelled. In joint generalised linear mixed models, this can be done with correlated random effects, which allows examining the association structure between the various responses and the evolution of this association over time. In addition, the effect of covariates on all outcomes can be assessed simultaneously. Still, investigating this association is often limited to examining the correlations between the responses on an underlying scale. In addition, the interpretation of this hierarchical model is conditional on the subject-specific random effects. This paper extends this approach and shows how manifest correlations can be computed, that is, the associations between the observed responses. Further, a marginal model is formulated, in which the interpretation is no longer conditional on the random effects. In addition, prediction intervals are derived of one subvector of responses conditional on the other. These methods are applied in a case study of the lung function and allergic bronchopulmonary aspergillosis in patients with cystic fibrosis.},
  archive      = {J_INSR},
  author       = {Margaux Delporte and Steffen Fieuws and Geert Molenberghs and Geert Verbeke and Simeon Situma Wanyama and Elpis Hatziagorou and Christiane De Boeck},
  doi          = {10.1111/insr.12532},
  journal      = {International Statistical Review},
  month        = {11},
  number       = {S1},
  pages        = {S37-S51},
  shortjournal = {Int. Stat. Rev.},
  title        = {A joint normal-binary (probit) model},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Global seasonal and pandemic patterns in influenza: An
application of longitudinal study designs. <em>INSR</em>,
<em>90</em>(S1), S82–S95. (<a
href="https://doi.org/10.1111/insr.12529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The confluence of growing analytic capacities and global surveillance systems for seasonal infections has created new opportunities to further develop statistical methodology and advance the understanding of the global disease dynamics. We developed a framework to characterise the seasonality of infectious diseases for publicly available global health surveillance data. Specifically, we aimed to estimate the seasonal characteristics and their uncertainty using mixed effects models with harmonic components and the δ-method and develop multi-panel visualisations to present complex interplay of seasonal peaks across geographic locations. We compiled a set of 2 422 weekly time series of 14 reported outcomes for 173 Member States from the World Health Organization&#39;s (WHO) international influenza virological surveillance system, FluNet, from 02 January 1995 through 20 June 2021. We produced an analecta of data visualisations to describe global travelling waves of influenza while addressing issues of data completeness and credibility. Our results offer directions for further improvements in data collection, reporting, analysis and development of statistical methodology and predictive approaches.},
  archive      = {J_INSR},
  author       = {Elena N. Naumova and Ryan B. Simpson and Bingjie Zhou and Meghan A. Hartwick},
  doi          = {10.1111/insr.12529},
  journal      = {International Statistical Review},
  month        = {10},
  number       = {S1},
  pages        = {S82-S95},
  shortjournal = {Int. Stat. Rev.},
  title        = {Global seasonal and pandemic patterns in influenza: An application of longitudinal study designs},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synergy of biostatistics and epidemiology in air pollution
health effects studies. <em>INSR</em>, <em>90</em>(S1), S67–S81. (<a
href="https://doi.org/10.1111/insr.12525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extraordinary advances in quantifying the health effects of ambient air pollution over the last five decades have led to dramatic improvement in air quality in the United States. This work has been possible through innovative epidemiologic study designs coupled with advanced statistical analytic methods. This paper presents a historical perspective on the coordinated developments of epidemiologic designs and statistical methods for air pollution health effects studies at the Harvard School of Public Health.},
  archive      = {J_INSR},
  author       = {Douglas W. Dockery},
  doi          = {10.1111/insr.12525},
  journal      = {International Statistical Review},
  month        = {10},
  number       = {S1},
  pages        = {S67-S81},
  shortjournal = {Int. Stat. Rev.},
  title        = {Synergy of biostatistics and epidemiology in air pollution health effects studies},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A legacy of EM algorithms. <em>INSR</em>, <em>90</em>(S1),
S52–S66. (<a href="https://doi.org/10.1111/insr.12526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nan Laird has an enormous and growing impact on computational statistics. Her paper with Dempster and Rubin on the expectation-maximisation (EM) algorithm is the second most cited paper in statistics. Her papers and book on longitudinal modelling are nearly as impressive. In this brief survey, we revisit the derivation of some of her most useful algorithms from the perspective of the minorisation-maximisation (MM) principle. The MM principle generalises the EM principle and frees it from the shackles of missing data and conditional expectations. Instead, the focus shifts to the construction of surrogate functions via standard mathematical inequalities. The MM principle can deliver a classical EM algorithm with less fuss or an entirely new algorithm with a faster rate of convergence. In any case, the MM principle enriches our understanding of the EM principle and suggests new algorithms of considerable potential in high-dimensional settings where standard algorithms such as Newton&#39;s method and Fisher scoring falter.},
  archive      = {J_INSR},
  author       = {Kenneth Lange and Hua Zhou},
  doi          = {10.1111/insr.12526},
  journal      = {International Statistical Review},
  month        = {10},
  number       = {S1},
  pages        = {S52-S66},
  shortjournal = {Int. Stat. Rev.},
  title        = {A legacy of EM algorithms},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Likelihood-based inference for the finite population mean
with post-stratification information under non-ignorable non-response.
<em>INSR</em>, <em>90</em>(S1), S17–S36. (<a
href="https://doi.org/10.1111/insr.12527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe models and likelihood-based estimation of the finite population mean for a survey subject to unit non-response, when post-stratification information is available from external sources. A feature of the models is that they do not require the assumption that the data are missing at random (MAR). As a result, the proposed models provide estimates under weaker assumptions than those required in the absence of post-stratification information, thus allowing more robust inferences. In particular, we describe models for estimation of the finite population mean of a survey outcome with categorical covariates and externally observed categorical post-stratifiers. We compare inferences from the proposed method with existing design-based estimators via simulations. We apply our methods to school-level data from California Department of Education to estimate the mean academic performance index (API) score in years 1999 and 2000. We end with a discussion.},
  archive      = {J_INSR},
  author       = {Sahar Z. Zangeneh and Roderick J. Little},
  doi          = {10.1111/insr.12527},
  journal      = {International Statistical Review},
  month        = {10},
  number       = {S1},
  pages        = {S17-S36},
  shortjournal = {Int. Stat. Rev.},
  title        = {Likelihood-based inference for the finite population mean with post-stratification information under non-ignorable non-response},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical analysis of longitudinal studies. <em>INSR</em>,
<em>90</em>(S1), S2–S16. (<a
href="https://doi.org/10.1111/insr.12523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal studies play a prominent role in research on growth, change and/or decline in individuals, and in characterising the environmental and social factors which influence change. The essential feature of a longitudinal study is taking repeated measures of an outcome on the same set of individuals at multiple timepoints, thereby allowing investigators to characterise within subject changes during the measurement period. This paper provides an overview of how the basic design features and analysis of longitudinal studies are related to other study designs, including longitudinal clinical trials as well as repeated measures studies. I summarise the use of the linear mixed model as described in Laird and Ware for the analysis of a broad class of designs and present some applications in health and medicine.},
  archive      = {J_INSR},
  author       = {Nan M. Laird},
  doi          = {10.1111/insr.12523},
  journal      = {International Statistical Review},
  month        = {10},
  number       = {S1},
  pages        = {S2-S16},
  shortjournal = {Int. Stat. Rev.},
  title        = {Statistical analysis of longitudinal studies},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Foreword. <em>INSR</em>, <em>90</em>(S1), S1. (<a
href="https://doi.org/10.1111/insr.12520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_INSR},
  doi          = {10.1111/insr.12520},
  journal      = {International Statistical Review},
  month        = {10},
  number       = {S1},
  pages        = {S1},
  shortjournal = {Int. Stat. Rev.},
  title        = {Foreword},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). <em>INSR</em>, <em>90</em>(3), 626–627. (<a
href="https://doi.org/10.1111/insr.12531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_INSR},
  author       = {Shuangzhe Liu},
  doi          = {10.1111/insr.12531},
  journal      = {International Statistical Review},
  month        = {12},
  number       = {3},
  pages        = {626-627},
  shortjournal = {Int. Stat. Rev.},
  title        = {Data visualization for social and policy research: a step-by-step approach using r and python jose manuel magallanes reyes cambridge university press, 2022, 292 pages, $105, hardback ISBN: 978-1-108-49433-5},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). <em>INSR</em>, <em>90</em>(3), 622–625. (<a
href="https://doi.org/10.1111/insr.12530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_INSR},
  author       = {Gijs Dekkers},
  doi          = {10.1111/insr.12530},
  journal      = {International Statistical Review},
  month        = {12},
  number       = {3},
  pages        = {622-625},
  shortjournal = {Int. Stat. Rev.},
  title        = {Thinking clearly with data: a guide to quantitative reasoning and analysis ethan bueno de mesquita and anthony fowler princeton university press, 2021, 400 pages, $95.00/£74.00, hardback ISBN: 978-0-691-21436-8},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonparametric testing of the dependence structure among
points–marks–covariates in spatial point patterns. <em>INSR</em>,
<em>90</em>(3), 592–621. (<a
href="https://doi.org/10.1111/insr.12503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate testing of the hypothesis of independence between a covariate and the marks in a marked point process. It would be rather straightforward if the (unmarked) point process were independent of the covariate and the marks. In practice, however, such an assumption is questionable and possible dependence between the point process and the covariate or the marks may lead to incorrect conclusions. Therefore, we propose to investigate the complete dependence structure in the triangle points–marks–covariates together. We take advantage of the recent development of the nonparametric random shift methods, namely, the new variance correction approach, and propose tests of the null hypothesis of independence between the marks and the covariate and between the points and the covariate. We present a detailed simulation study showing the performance of the methods and provide two theorems establishing the appropriate form of the correction factors for the variance correction. Finally, we illustrate the use of the proposed methods in two real applications.},
  archive      = {J_INSR},
  author       = {Jiří Dvořák and Tomáš Mrkvička and Jorge Mateu and Jonatan A. González},
  doi          = {10.1111/insr.12503},
  journal      = {International Statistical Review},
  month        = {12},
  number       = {3},
  pages        = {592-621},
  shortjournal = {Int. Stat. Rev.},
  title        = {Nonparametric testing of the dependence structure among Points–Marks–Covariates in spatial point patterns},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Priors in bayesian deep learning: A review. <em>INSR</em>,
<em>90</em>(3), 563–591. (<a
href="https://doi.org/10.1111/insr.12502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the choice of prior is one of the most critical parts of the Bayesian inference workflow, recent Bayesian deep learning models have often fallen back on vague priors, such as standard Gaussians. In this review, we highlight the importance of prior choices for Bayesian deep learning and present an overview of different priors that have been proposed for (deep) Gaussian processes, variational autoencoders and Bayesian neural networks. We also outline different methods of learning priors for these models from data. We hope to motivate practitioners in Bayesian deep learning to think more carefully about the prior specification for their models and to provide them with some inspiration in this regard.},
  archive      = {J_INSR},
  author       = {Vincent Fortuin},
  doi          = {10.1111/insr.12502},
  journal      = {International Statistical Review},
  month        = {12},
  number       = {3},
  pages        = {563-591},
  shortjournal = {Int. Stat. Rev.},
  title        = {Priors in bayesian deep learning: A review},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Should we condition on the number of points when modelling
spatial point patterns? <em>INSR</em>, <em>90</em>(3), 551–562. (<a
href="https://doi.org/10.1111/insr.12501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss the practice of directly or indirectly assuming a model for the number of points when modelling spatial point patterns even though it is rarely possible to validate such a model in practice because most point pattern data consist of only one pattern. We therefore explore the possibility to condition on the number of points instead when fitting and validating spatial point process models. In a simulation study with different popular spatial point process models, we consider model validation using global envelope tests based on functional summary statistics. We find that conditioning on the number of points will for some functional summary statistics lead to more narrow envelopes and thus stronger tests and that it can also be useful for correcting for some conservativeness in the tests when testing composite hypothesis. However, for other functional summary statistics, it makes little or no difference to condition on the number of points. When estimating parameters in popular spatial point process models, we conclude that for mathematical and computational reasons, it is convenient to assume a distribution for the number of points.},
  archive      = {J_INSR},
  author       = {Jesper Møller and Ninna Vihrs},
  doi          = {10.1111/insr.12501},
  journal      = {International Statistical Review},
  month        = {12},
  number       = {3},
  pages        = {551-562},
  shortjournal = {Int. Stat. Rev.},
  title        = {Should we condition on the number of points when modelling spatial point patterns?},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rethinking the effective sample size. <em>INSR</em>,
<em>90</em>(3), 525–550. (<a
href="https://doi.org/10.1111/insr.12500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effective sample size (ESS) is widely used in sample-based simulation methods for assessing the quality of a Monte Carlo approximation of a given distribution and of related integrals. In this paper, we revisit the approximation of the ESS in the specific context of importance sampling. The derivation of this approximation, that we will denote as ESS ^ , is partially available in a 1992 foundational technical report of Augustine Kong. This approximation has been widely used in the last 25 years due to its simplicity as a practical rule of thumb in a wide variety of importance sampling methods. However, we show that the multiple assumptions and approximations in the derivation of ESS ^ make it difficult to be considered even as a reasonable approximation of the ESS. We extend the discussion of the ESS ^ in the multiple importance sampling setting, we display numerical examples and we discuss several avenues for developing alternative metrics. This paper does not cover the use of ESS for Markov chain Monte Carlo algorithms.},
  archive      = {J_INSR},
  author       = {Víctor Elvira and Luca Martino and Christian P. Robert},
  doi          = {10.1111/insr.12500},
  journal      = {International Statistical Review},
  month        = {12},
  number       = {3},
  pages        = {525-550},
  shortjournal = {Int. Stat. Rev.},
  title        = {Rethinking the effective sample size},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Elaboration models with symmetric information divergence.
<em>INSR</em>, <em>90</em>(3), 499–524. (<a
href="https://doi.org/10.1111/insr.12499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various statistical methodologies embed a probability distribution in a more flexible family of distributions. The latter is called elaboration model , which is constructed by choice or a formal procedure and evaluated by asymmetric measures such as the likelihood ratio and Kullback–Leibler information. The use of asymmetric measures can be problematic for this purpose. This paper introduces two formal procedures, referred to as link functions, that embed any baseline distribution with a continuous density on the real line into model elaborations. Conditions are given for the link functions to render symmetric Kullback–Leibler divergence, Rényi divergence and phi-divergence family. The first link function elaborates quantiles of the baseline probability distribution. This approach produces continuous counterparts of the binary probability models. Examples include the Cauchy, probit, logit, Laplace and Student&#39;s links. The second link function elaborates the baseline survival function. Examples include the proportional odds and change point links. The logistic distribution is characterised as the one that satisfies the conditions for both links. An application demonstrates advantages of symmetric divergence measures for assessing the efficacy of covariates.},
  archive      = {J_INSR},
  author       = {Majid Asadi and Karthik Devarajan and Nader Ebrahimi and Ehsan Soofi and Lauren Spirko-Burns},
  doi          = {10.1111/insr.12499},
  journal      = {International Statistical Review},
  month        = {12},
  number       = {3},
  pages        = {499-524},
  shortjournal = {Int. Stat. Rev.},
  title        = {Elaboration models with symmetric information divergence},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Some solutions inspired by survey sampling theory to build
effective clinical trials. <em>INSR</em>, <em>90</em>(3), 481–498. (<a
href="https://doi.org/10.1111/insr.12498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The organisation of a design of experiments, for example, for the realisation of a clinical trial, is crucial. It is often desirable to balance designs so that the means of the covariates are approximately the same in the test and control groups. In survey sampling theory, balanced sampling and calibration are two techniques that improve the precision of estimates. In this paper, we show the links between the two areas. We begin by assessing the gain in precision between a balanced design and a simple random sampling for the least squares estimators and the estimator by differences. We compare rerandomisation techniques and the cube method in order to balance the design. We propose a new method, particularly efficient, which combines the cube method with multivariate matching. A set of simulations is carried out in order to evaluate the different methods. The interest of the calibration is shown even if the design is almost balanced. It is thus shown that tools used by survey statisticians can be useful for experimental designs and clinical trials.},
  archive      = {J_INSR},
  author       = {Yves Tillé},
  doi          = {10.1111/insr.12498},
  journal      = {International Statistical Review},
  month        = {12},
  number       = {3},
  pages        = {481-498},
  shortjournal = {Int. Stat. Rev.},
  title        = {Some solutions inspired by survey sampling theory to build effective clinical trials},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bias, fairness and accountability with artificial
intelligence and machine learning algorithms. <em>INSR</em>,
<em>90</em>(3), 468–480. (<a
href="https://doi.org/10.1111/insr.12492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of artificial intelligence (AI) and machine learning algorithms has led to opportunities as well as challenges in their use. In this overview paper, we begin with a discussion of bias and fairness issues that arise with the use of AI techniques, with a focus on supervised machine learning algorithms. We then describe the types and sources of data bias and discuss the nature of algorithmic unfairness. In addition, we provide a review of fairness metrics in the literature, discuss their limitations, and describe de-biasing (or mitigation) techniques in the model life cycle.},
  archive      = {J_INSR},
  author       = {Nengfeng Zhou and Zach Zhang and Vijayan N. Nair and Harsh Singhal and Jie Chen},
  doi          = {10.1111/insr.12492},
  journal      = {International Statistical Review},
  month        = {12},
  number       = {3},
  pages        = {468-480},
  shortjournal = {Int. Stat. Rev.},
  title        = {Bias, fairness and accountability with artificial intelligence and machine learning algorithms},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Temporal models for demographic and global health outcomes
in multiple populations: Introducing a new framework to review and
standardise documentation of model assumptions and facilitate model
comparison. <em>INSR</em>, <em>90</em>(3), 437–467. (<a
href="https://doi.org/10.1111/insr.12491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is growing interest in producing estimates of demographic and global health indicators in populations with limited data. Statistical models are needed to combine data from multiple data sources into estimates and projections with uncertainty. Diverse modelling approaches have been applied to this problem, making comparisons between models difficult. We propose a model class, Temporal Models for Multiple Populations (TMMPs), to facilitate both documentation of model assumptions in a standardised way and comparison across models. The class makes a distinction between the process model, which describes latent trends in the indicator interest, and the data model, which describes the data generating process of the observed data. We provide a general notation for the process model that encompasses many popular temporal modelling techniques, and we show how existing models for a variety of indicators can be written using this notation. We end with a discussion of outstanding questions and future directions.},
  archive      = {J_INSR},
  author       = {Herbert Susmann and Monica Alexander and Leontine Alkema},
  doi          = {10.1111/insr.12491},
  journal      = {International Statistical Review},
  month        = {12},
  number       = {3},
  pages        = {437-467},
  shortjournal = {Int. Stat. Rev.},
  title        = {Temporal models for demographic and global health outcomes in multiple populations: Introducing a new framework to review and standardise documentation of model assumptions and facilitate model comparison},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Survey weighting after imperfect linkage to an
administrative file. <em>INSR</em>, <em>90</em>(3), 419–436. (<a
href="https://doi.org/10.1111/insr.12490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an instrumental variable regression estimator of population totals using a sample, a set of links between the sample units and records on an administrative file, and a set of calibration totals calculated from the administrative file. This paper proposes a survey-weighted estimator of a population total that is valid when the survey non-response mechanism is non-ignorable and false negatives occur in the administrative-survey linkage. False negatives lead to measurement error in the administrative variables that are available on the survey and will lead to biased estimates if not taken into account. We show the benefit of the proposed approach in a simulation and in a case study.},
  archive      = {J_INSR},
  author       = {James Chipperfield},
  doi          = {10.1111/insr.12490},
  journal      = {International Statistical Review},
  month        = {12},
  number       = {3},
  pages        = {419-436},
  shortjournal = {Int. Stat. Rev.},
  title        = {Survey weighting after imperfect linkage to an administrative file},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). <em>INSR</em>, <em>90</em>(2), 415–417. (<a
href="https://doi.org/10.1111/insr.12516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_INSR},
  author       = {Reijo Sund},
  doi          = {10.1111/insr.12516},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {415-417},
  shortjournal = {Int. Stat. Rev.},
  title        = {Administrative records for survey methodology edited by asaph young chun, michael d. larsen, gabriele durrant, jerome p. reiter john wiley and sons, 2021, 384 pages, $128.95 (hardcover) ISBN: 978-1-1192-7204-5},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). <em>INSR</em>, <em>90</em>(2), 414–415. (<a
href="https://doi.org/10.1111/insr.12515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_INSR},
  author       = {Krzysztof Podgórski},
  doi          = {10.1111/insr.12515},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {414-415},
  shortjournal = {Int. Stat. Rev.},
  title        = {Wavelets from a statistical perspective maarten jansen chapman and Hall/CRC, 2022, xix + 325 pages, $96 (hardcover) ISBN: 978-1-032-20067-5 (hardcover)},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Game data science magy seif el-nasr, truong-huy d. Nguyen,
alessandro canossa, anders drachen oxford university press, 2022, xvi +
416 pages, <span
class="math inline">105(<em>h</em><em>a</em><em>r</em><em>d</em><em>b</em><em>a</em><em>c</em><em>k</em>)/</span><!-- -->55
(paperback) ISBN-10: 019289787X, ISBN-13: 978-0192897879 (hardback),
978–0192897886 (paperback). <em>INSR</em>, <em>90</em>(2), 412–414. (<a
href="https://doi.org/10.1111/insr.12514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_INSR},
  author       = {Shuangzhe Liu},
  doi          = {10.1111/insr.12514},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {412-414},
  shortjournal = {Int. Stat. Rev.},
  title        = {Game data science magy seif el-nasr, truong-huy d. nguyen, alessandro canossa, anders drachen oxford university press, 2022, xvi + 416 pages, $105 (hardback)/$55 (paperback) ISBN-10: 019289787X, ISBN-13: 978-0192897879 (hardback), 978–0192897886 (paperback)},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). <em>INSR</em>, <em>90</em>(2), 411–412. (<a
href="https://doi.org/10.1111/insr.12513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_INSR},
  author       = {Fabrizio Durante},
  doi          = {10.1111/insr.12513},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {411-412},
  shortjournal = {Int. Stat. Rev.},
  title        = {Extreme value theory with applications to natural hazards: from statistical theory to industrial practice edited by nicolas bousquet and pietro bernardara springer cham, 2021, xxii + 481 pages, $199.99 ISBN: 978-3-030-74941-5},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unlucky number 13? Manipulating evidence subject to
snooping. <em>INSR</em>, <em>90</em>(2), 397–410. (<a
href="https://doi.org/10.1111/insr.12488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Questionable research practices have generated considerable recent interest throughout and beyond the scientific community. We subsume such practices involving secret data snooping that influences subsequent statistical inference under the term MESSing (manipulating evidence subject to snooping) and discuss, illustrate and quantify the possibly dramatic effects of several forms of MESSing using an empirical and a simple theoretical example. The empirical example uses numbers from the most popular German lottery, which seem to suggest that 13 is an unlucky number.},
  archive      = {J_INSR},
  author       = {Uwe Hassler and Marc-Oliver Pohle},
  doi          = {10.1111/insr.12488},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {397-410},
  shortjournal = {Int. Stat. Rev.},
  title        = {Unlucky number 13? manipulating evidence subject to snooping},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). White noise test from ordinal patterns in the
entropy–complexity plane. <em>INSR</em>, <em>90</em>(2), 374–396. (<a
href="https://doi.org/10.1111/insr.12487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article serves two purposes. Firstly, it surveys the Bandt and Pompe methodology for the statistical community, stressing topics that are open for research. Secondly, it contributes towards a better understanding of the statistical properties of that approach for time series analysis. The Bandt and Pompe methodology consists of computing information theory descriptors from the histogram of ordinal patterns. Such descriptors lie in a 2D manifold: the entropy–complexity plane. This article provides the first proposal of a test in the entropy–complexity plane for the white noise hypothesis. Our test is based on true white noise sequences obtained from physical devices. The proposed methodology provides consistent results: It assesses sequences of true random samples as random (adequate test size), rejects correlated and contaminated sequences (sound test power) and captures the randomness of generators previously analysed in the literature.},
  archive      = {J_INSR},
  author       = {Eduarda T. C. Chagas and Marcelo Queiroz-Oliveira and Osvaldo A. Rosso and Heitor S. Ramos and Cristopher G. S. Freitas and Alejandro C. Frery},
  doi          = {10.1111/insr.12487},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {374-396},
  shortjournal = {Int. Stat. Rev.},
  title        = {White noise test from ordinal patterns in the Entropy–Complexity plane},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nudging a pseudo-science towards a science—the role of
statistics in a rainfall enhancement trial in oman. <em>INSR</em>,
<em>90</em>(2), 346–373. (<a
href="https://doi.org/10.1111/insr.12486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although cloud seeding is a commonly used and plausible method for rainfall enhancement, its practical efficacy has not been established for seeding of convective clouds with hygroscopic materials. Other methods of rainfall enhancement are viewed as much less plausible. Thus, although increased electrical charge has been shown to enhance precipitation in cloud chamber experiments, exactly how ionisation of clouds can increase rainfall in the open atmosphere remains conjectural. A trial of the efficacy of ionisation for rainfall enhancement in the Hajar Mountains of Oman was carried out over 2013–2018. This paper provides some background to this non-mainstream approach to increasing rainfall, showing how statistical modelling of rainfall data might be used to nudge rainfall enhancement via ionisation towards a more scientifically acceptable status. Analysis of the data collected in the trial shows that ionisation led to a statistically significant enhancement in positive rainfall in gauges located up to 70 km downwind of the ionisers. A headline analysis specified prior to commencement of the trial resulted in an estimate of 16.23% enhancement relative to rainfall that would have fallen without any ionisation, while a more sophisticated after the event analysis increased this estimate to 17.64%.},
  archive      = {J_INSR},
  author       = {Ray Chambers and Stephen Beare and Scott Peak and Mohammed Al-Kalbani},
  doi          = {10.1111/insr.12486},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {346-373},
  shortjournal = {Int. Stat. Rev.},
  title        = {Nudging a pseudo-science towards a Science—The role of statistics in a rainfall enhancement trial in oman},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constructing flexible, identifiable and interpretable
statistical models for binary data. <em>INSR</em>, <em>90</em>(2),
328–345. (<a href="https://doi.org/10.1111/insr.12485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary regression models are ubiquitous in virtually every scientific field. Frequently, traditional generalised linear models fail to capture the variability in the probability surface that gives rise to the binary observations, and remedial methods are required. This has generated a substantial literature composed of binary regression models motivated by various applications. We describe an organisation of generalisations to traditional binary regression methods based on the familiar three-part structure of generalised linear models (random component, systematic component and link function). This perspective facilitates both the comparison of existing approaches and the development of flexible models with interpretable parameters that capture application-specific data-generating mechanisms. We use our proposed organisational structure to discuss concerns with certain existing models for binary data based on quantile regression. We then use the framework to develop and compare several binary regression models tailored to occupancy data for European red squirrels ( Sciurus vulgaris ).},
  archive      = {J_INSR},
  author       = {Henry R. Scharf and Xinyi Lu and Perry J. Williams and Mevin B. Hooten},
  doi          = {10.1111/insr.12485},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {328-345},
  shortjournal = {Int. Stat. Rev.},
  title        = {Constructing flexible, identifiable and interpretable statistical models for binary data},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparser ordinal regression models based on parametric and
additive location-shift approaches. <em>INSR</em>, <em>90</em>(2),
306–327. (<a href="https://doi.org/10.1111/insr.12484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The potential of location-shift models to find adequate models between the proportional odds model and the non-proportional odds model is investigated. It is demonstrated that these models are very useful in ordinal modelling. While proportional odds models are often too simple, non-proportional odds models are typically unnecessary complicated and seem widely dispensable. In addition, the class of location-shift models is extended to allow for smooth effects. The additive location-shift model contains two functions for each explanatory variable, one for the location and one for dispersion. It is much sparser than hard-to-handle additive models with category-specific covariate functions but more flexible than common vector generalised additive models. An R package is provided that is able to fit parametric and additive location-shift models.},
  archive      = {J_INSR},
  author       = {Gerhard Tutz and Moritz Berger},
  doi          = {10.1111/insr.12484},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {306-327},
  shortjournal = {Int. Stat. Rev.},
  title        = {Sparser ordinal regression models based on parametric and additive location-shift approaches},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating monotonic hazard ratio functions of time.
<em>INSR</em>, <em>90</em>(2), 285–305. (<a
href="https://doi.org/10.1111/insr.12483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In non-proportional hazards models, the hazard ratio for a unit increase in covariate value is not constant but varies over time. Existing approaches to estimating time-varying log hazard ratio include various spline approximations and maximum penalised partial likelihood. We consider improvements to these methods under the plausible assumption that the hazard ratio changes from its short-term to long-term value in a monotonic fashion. A monotone B-spline estimate based on equidistant knots with the last few coefficients constrained to be equal works reasonably well. We also propose a constrained maximum penalised partial likelihood approach with the constraints removed through re-parameterisation. A novel feature of the proposed method is that it is based on a log product of spacings penalty rather than the usual roughness penalty, which makes selection of smoothing parameter easier. The utility of the proposed methods is demonstrated using a real data example and simulations.},
  archive      = {J_INSR},
  author       = {Anthony Y.C. Kuk},
  doi          = {10.1111/insr.12483},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {285-305},
  shortjournal = {Int. Stat. Rev.},
  title        = {Estimating monotonic hazard ratio functions of time},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A review of seasonal adjustment diagnostics. <em>INSR</em>,
<em>90</em>(2), 259–284. (<a
href="https://doi.org/10.1111/insr.12482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seasonal adjustment methods are used to process and publish thousands of time series across the world each month, and judgement of the adequacy relies heavily upon seasonal adjustment diagnostics. This paper discusses tests for the adequacy of a seasonal adjustment, first reviewing the broader background on tests for seasonality and then proceeding to four tests that are appropriate for stationary forms of seasonality. We contrast time-domain and frequency-domain approaches, focusing upon four diagnostics available in the seasonal adjustment literature (and software packages) and applying the methods to a large collection of public use time series. Each of the four tests is designed around a distinct formulation of seasonality, and hence, empirical performances differ; we compare and contrast the methods and include discussion on how diagnostic results can be used to improve faulty seasonal adjustments. Directions for future research, involving inverse partial autocorrelations and polyspectra, are also discussed, and the methodologies are supported by theoretical and simulation results.},
  archive      = {J_INSR},
  author       = {Tucker McElroy and Anindya Roy},
  doi          = {10.1111/insr.12482},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {259-284},
  shortjournal = {Int. Stat. Rev.},
  title        = {A review of seasonal adjustment diagnostics},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Directions old and new: Palaeomagnetism and fisher (1953)
meet modern statistics. <em>INSR</em>, <em>90</em>(2), 237–258. (<a
href="https://doi.org/10.1111/insr.12481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most modern articles in the palaeomagnetism literature are based on statistics developed by Fisher&#39;s 1953 paper ‘Dispersion on a sphere’, which assumes independent and identically distributed (iid) spherical data. However, palaeomagnetic sample designs are usually hierarchical, where specimens are collected within sites and the data are then combined across sites to calculate an overall mean direction for a geological formation. The specimens within sites are typically more similar than specimens between different sites, and so the iid assumptions fail. This article has three principal goals. The first is to review, contrast and compare both the statistics and geophysics literature on the topic of analysis methods for clustered data on spheres. The second is to present a new hierarchical parametric model, which avoids the unrealistic assumption of rotational symmetry in Fisher&#39;s 1953 paper ‘Dispersion on a sphere’ and may be broadly useful in the analysis of many palaeomagnetic datasets. To help develop the model, we use publicly available data as a case study collected from the Golan Heights volcanic plateau. The third goal is to explore different methods for constructing confidence regions for the overall mean direction based on clustered data. Two bootstrap confidence regions that we propose perform well and will be especially useful to geophysics practitioners.},
  archive      = {J_INSR},
  author       = {Janice L. Scealy and David Heslop and Jia Liu and Andrew T. A. Wood},
  doi          = {10.1111/insr.12481},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {237-258},
  shortjournal = {Int. Stat. Rev.},
  title        = {Directions old and new: Palaeomagnetism and fisher (1953) meet modern statistics},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modelling excess zeros in count data: A new perspective on
modelling approaches. <em>INSR</em>, <em>90</em>(2), 216–236. (<a
href="https://doi.org/10.1111/insr.12479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the analysis of count data in which the observed frequency of zero counts is unusually large, typically with respect to the Poisson distribution. We focus on two alternative modelling approaches: over-dispersion (OD) models and zero-inflation (ZI) models, both of which can be seen as generalisations of the Poisson distribution; we refer to these as implicit and explicit ZI models, respectively. Although sometimes seen as competing approaches, they can be complementary; OD is a consequence of ZI modelling, and ZI is a by-product of OD modelling. The central objective in such analyses is often concerned with inference on the effect of covariates on the mean, in light of the apparent excess of zeros in the counts. Typically, the modelling of the excess zeros per se is a secondary objective, and there are choices to be made between, and within, the OD and ZI approaches. The contribution of this paper is primarily conceptual. We contrast, descriptively, the impact on zeros of the two approaches. We further offer a novel descriptive characterisation of alternative ZI models, including the classic hurdle and mixture models, by providing a unifying theoretical framework for their comparison. This in turn leads to a novel and technically simpler ZI model. We develop the underlying theory for univariate counts and touch on its implication for multivariate count data.},
  archive      = {J_INSR},
  author       = {John Haslett and Andrew C. Parnell and John Hinde and Rafael de Andrade Moral},
  doi          = {10.1111/insr.12479},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {216-236},
  shortjournal = {Int. Stat. Rev.},
  title        = {Modelling excess zeros in count data: A new perspective on modelling approaches},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variable selection for interval-censored failure time data.
<em>INSR</em>, <em>90</em>(2), 193–215. (<a
href="https://doi.org/10.1111/insr.12480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable selection for interval-censored failure time data has recently attracted a great deal of attention along with the analysis of interval-censored data in both method developments and practical applications. Interval-censored data are a general type of time-to-event or failure time data where the failure time of interest is known or observed only to lie in an interval instead of being observed exactly. They often occur in different forms and in many fields, including demographical studies, epidemiological studies, medical or public health researches and social sciences. This paper will discuss and review the existing methods for variable selection based on interval-censored data, a relatively new but important topic, along with some directions or issues that need more research.},
  archive      = {J_INSR},
  author       = {Mingyue Du and Jianguo Sun},
  doi          = {10.1111/insr.12480},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {193-215},
  shortjournal = {Int. Stat. Rev.},
  title        = {Variable selection for interval-censored failure time data},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). <em>INSR</em>, <em>90</em>(1), 191–192. (<a
href="https://doi.org/10.1111/insr.12497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_INSR},
  author       = {Krzysztof Podgórski},
  doi          = {10.1111/insr.12497},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {191-192},
  shortjournal = {Int. Stat. Rev.},
  title        = {Practical smoothing: the joys of P-splines paul h. c. eilers and brian d. marx cambridge university press, 2021, xii + 199 pages, $59.99, hardcover ISBN: 978-1-1084-8295-0},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). <em>INSR</em>, <em>90</em>(1), 189–191. (<a
href="https://doi.org/10.1111/insr.12496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_INSR},
  author       = {Kelly McConville},
  doi          = {10.1111/insr.12496},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {189-191},
  shortjournal = {Int. Stat. Rev.},
  title        = {Communicating with data: the art of writing for data science deborah nolan and sara stoudt oxford university press, 2021, vii + 331 pages, $45.95, paperback ISBN: 978-0-1988-6275-8},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). <em>INSR</em>, <em>90</em>(1), 187–188. (<a
href="https://doi.org/10.1111/insr.12495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_INSR},
  author       = {John H. Maindonald},
  doi          = {10.1111/insr.12495},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {187-188},
  shortjournal = {Int. Stat. Rev.},
  title        = {Replication and evidence factors in observational studies paul r. rosenbaum chapman &amp; Hall/CRC, 2021, xviii + 254 pages, $120, hardback ISBN: 978-036748-388-3},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). <em>INSR</em>, <em>90</em>(1), 185–186. (<a
href="https://doi.org/10.1111/insr.12494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_INSR},
  author       = {Debashis Ghosh},
  doi          = {10.1111/insr.12494},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {185-186},
  shortjournal = {Int. Stat. Rev.},
  title        = {Fundamentals of causal inference with r babette a. brumback chapman &amp; Hall/CRC, 2021, xi + 236 pages, $69.95, hardcover ISBN: 978-0-3677-0505-3},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). <em>INSR</em>, <em>90</em>(1), 184–185. (<a
href="https://doi.org/10.1111/insr.12493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_INSR},
  author       = {Daniel Fischer},
  doi          = {10.1111/insr.12493},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {184-185},
  shortjournal = {Int. Stat. Rev.},
  title        = {Population genomics with r emmanuel paradis chapman &amp; Hall/CRC, 2020, 394 pages, $120, hardback ISBN: 978-1-1386-0818-4},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Assessing bayesian semi-parametric log-linear models: An
application to disclosure risk estimation. <em>INSR</em>,
<em>90</em>(1), 165–183. (<a
href="https://doi.org/10.1111/insr.12471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method for identifying models with good predictive performance in the family of Bayesian log-linear mixed models with Dirichlet process random effects for count data. Their wide applicability makes the assessment of model performance crucial in many fields, including disclosure risk estimation, which is the focus of the present work. Rather than assessing models on the whole contingency table, we target the specific objective of the analysis and propose a two-stage model selection procedure aimed at limiting a form of bias arising in the process of model selection. Our proposal combines two different criteria: at the first stage, a path in the model search space is identified through a strongly penalized log-likelihood; at the second, a small number of semi-parametric models is evaluated through a context-dependent score-based information criterion. Tested on a variety of contingency tables, our method proves to be able to identify models with good predictive performance in a few steps, even in the presence of large tables with many sampling and structural zeros. We carefully discuss the proposed method in the context of the literature on model assessment and contextualize the illustrative application in the recent debate on statistical disclosure limitation. Finally, we provide examples of further applications in different research areas.},
  archive      = {J_INSR},
  author       = {Cinzia Carota and Maurizio Filippone and Silvia Polettini},
  doi          = {10.1111/insr.12471},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {165-183},
  shortjournal = {Int. Stat. Rev.},
  title        = {Assessing bayesian semi-parametric log-linear models: An application to disclosure risk estimation},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient and robust propensity-score-based methods for
population inference using epidemiologic cohorts. <em>INSR</em>,
<em>90</em>(1), 146–164. (<a
href="https://doi.org/10.1111/insr.12470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most epidemiologic cohorts are composed of volunteers who do not represent the general population. To improve population inference from cohorts, propensity-score (PS)-based matching methods, such as PS-based kernel weighting (KW) method, utilise probability survey samples as external references to develop PSs for membership in the cohort versus survey. We identify a strong exchangeability assumption (SEA) that underlies existing PS-based matching methods whose failure invalidates inferences, even if the propensity model is correctly specified. Herein, we develop a framework of propensity estimation and relax the SEA to a weak exchangeability assumption (WEA) for matching methods. To recover efficiency, we propose a scaled KW (KW.S) matching method by scaling survey weights in propensity estimation. We prove consistency of KW.S estimators of means/prevalences under WEA and provide consistent finite population variance estimators. In simulations, the KW.S estimators had smallest mean squared error (MSE). Our data example showed the KW estimates requiring the SEA had large bias, whereas the proposed KW.S estimates had the smallest MSE.},
  archive      = {J_INSR},
  author       = {Lingxiao Wang and Barry I. Graubard and Hormuzd A. Katki and Yan Li},
  doi          = {10.1111/insr.12470},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {146-164},
  shortjournal = {Int. Stat. Rev.},
  title        = {Efficient and robust propensity-score-based methods for population inference using epidemiologic cohorts},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A critical review of LASSO and its derivatives for variable
selection under dependence among covariates. <em>INSR</em>,
<em>90</em>(1), 118–145. (<a
href="https://doi.org/10.1111/insr.12469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The limitations of the well-known LASSO regression as a variable selector are tested when there exists dependence structures among covariates. We analyse both the classic situation with n ≥ p and the high dimensional framework with p &gt; n . Known restrictive properties of this methodology to guarantee optimality, as well as inconveniences in practice, are analysed and tested by means of an extensive simulation study. Examples of these drawbacks are showed making use of different dependence scenarios. In order to search for improvements, a broad comparison with LASSO derivatives and alternatives is carried out. Eventually, we give some guidance about what procedures work best in terms of the considered data nature.},
  archive      = {J_INSR},
  author       = {Laura Freijeiro-González and Manuel Febrero-Bande and Wenceslao González-Manteiga},
  doi          = {10.1111/insr.12469},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {118-145},
  shortjournal = {Int. Stat. Rev.},
  title        = {A critical review of LASSO and its derivatives for variable selection under dependence among covariates},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effect of covariate omission in randomised controlled
trials: A review and simulation study. <em>INSR</em>, <em>90</em>(1),
100–117. (<a href="https://doi.org/10.1111/insr.12468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In randomised controlled trials (RCTs), the allocated treatment group and covariates are expected to be independent. In the framework of the normal model, omitting covariates affects precision of the treatment effect estimator but does not yield bias in the estimator. However, when covariates are omitted, the treatment effect estimator has a bias in non-normal models such as logistic and Cox models even in RCTs. Additionally, covariate omission in the accelerated failure time model and Poisson model causes a bias in the standard error (SE) estimator and yields an inflation of Type-I error rate for the treatment effect. In this study, we reviewed the literature regarding the effect of covariate omission from the aspect of the bias and precision of treatment effect estimator, bias of SE estimator, and Type-I error rate for these models assumed in RCTs. Furthermore, we conducted a simulation study in a wide variety of scenarios to evaluate the effect of covariate omission. Our literature review and simulation study provide a simple guide for consideration of covariate adjustment in RCTs. We recommend that all important and measurement covariates be included in analysis models to reduce bias for the treatment effect and SE estimators and control the Type-I error rate.},
  archive      = {J_INSR},
  author       = {Ryota Ishii and Kazushi Maruo and Masahiko Gosho},
  doi          = {10.1111/insr.12468},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {100-117},
  shortjournal = {Int. Stat. Rev.},
  title        = {Effect of covariate omission in randomised controlled trials: A review and simulation study},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian models applied to cyber security anomaly detection
problems. <em>INSR</em>, <em>90</em>(1), 78–99. (<a
href="https://doi.org/10.1111/insr.12466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber security is an important concern for all individuals, organisations and governments globally. Cyber attacks have become more sophisticated, frequent and dangerous than ever, and traditional anomaly detection methods have been proved to be less effective when dealing with these new classes of cyber threats. In order to address this, both classical and Bayesian models offer a valid and innovative alternative to the traditional signature-based methods, motivating the increasing interest in statistical research that it has been observed in recent years. In this review, we provide a description of some typical cyber security challenges, typical types of data and statistical methods, paying special attention to Bayesian approaches for these problems.},
  archive      = {J_INSR},
  author       = {José A. Perusquía and Jim E. Griffin and Cristiano Villa},
  doi          = {10.1111/insr.12466},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {78-99},
  shortjournal = {Int. Stat. Rev.},
  title        = {Bayesian models applied to cyber security anomaly detection problems},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On testing for homogeneity with zero-inflated models through
the lens of model misspecification. <em>INSR</em>, <em>90</em>(1),
62–77. (<a href="https://doi.org/10.1111/insr.12462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applications of two-component mixture models such as the popular zero-inflated model for discrete-valued data, it is customary for the data analyst to evaluate the inherent heterogeneity in view of observed data. To this end, the score test, acclaimed for its simplicity, is routinely performed. It has long been recognised that this test may behave erratically under model misspecification, but the implications of this behaviour remain poorly understood for popular two-component mixture models. For the special case of zero-inflated count models, we use data simulations and theoretical arguments to evaluate this behaviour and discuss its implications in settings where the working model is restrictive with regard to the true data-generating mechanism. We enrich this discussion with an analysis of count data in HIV research, where a one-component model is shown to fit the data reasonably well despite apparent extra zeros. These results suggest that a rejection of homogeneity does not imply that the underlying mixture model is appropriate. Rather, such a rejection simply implies that the mixture model should be carefully interpreted in the light of potential model misspecifications, and further evaluated against other competing models.},
  archive      = {J_INSR},
  author       = {Wei-Wen Hsu and Nadeesha R. Mawella and David Todem},
  doi          = {10.1111/insr.12462},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {62-77},
  shortjournal = {Int. Stat. Rev.},
  title        = {On testing for homogeneity with zero-inflated models through the lens of model misspecification},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient test for homogeneity of mean directions on the
hyper-sphere. <em>INSR</em>, <em>90</em>(1), 41–61. (<a
href="https://doi.org/10.1111/insr.12461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper aims to develop a universally implementable efficient test for testing homogeneity of mean directions of several independent hyper-spherical populations. Conventional tests are valid only under highly concentrated and/or large-size groups. Focusing on the popular Langevin distribution on a d -hyper-sphere, the present work extends the very recent results for the circular case. The hurdle of the nuisance non-location-scale concentration parameter κ is overcome through a variant of the integrated likelihood ratio test (ILRT), yielding a simple and elegant test statistic. Analytically, second-order accurate asymptotic chi-squared distribution of ILRT is established. Extensive simulation study demonstrates that ILRT uniformly outperforms its peers, notably under highly dispersed groups, which is precisely the target parametric region, and is robust under a large class of alternate distributions. Five real-life data analyses from diverse disciplines, including the emerging field of vectorcardiography and a novel application to compositional data analysis in the context of drug development, illustrate applications of the findings.},
  archive      = {J_INSR},
  author       = {Hemangi V. Kulkarni and Ashis SenGupta},
  doi          = {10.1111/insr.12461},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {41-61},
  shortjournal = {Int. Stat. Rev.},
  title        = {An efficient test for homogeneity of mean directions on the hyper-sphere},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An interview with john m. abowd. <em>INSR</em>,
<em>90</em>(1), 1–40. (<a
href="https://doi.org/10.1111/insr.12489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {John M. Abowd is the Chief Scientist and Associate Director for Research and Methodology, US Census Bureau. He completed his AB in Economics at Notre Dame in 1973 and his PhD in Economics at University of Chicago in 1977 under Arnold Zellner. During his academic career, John has held faculty positions at Princeton, the University of Chicago, and, since 1987 at Cornell University where he is the Edmund Ezra Day Professor Emeritus of Economics, Statistics and Data Science. John was trained as a statistician and labor economist, and his economic research has focused on the rigorous empirical evaluation of labor market institutions. In the late 1990s, he began working with the Census Bureau on projects that would end up leveraging administrative and survey records into official statistical products. Through that work, he has developed a research agenda focused on issues necessary to generate those products, including data privacy, synthetic data, total error analysis, data linkage, and missing data problems, among others.},
  archive      = {J_INSR},
  author       = {Ian Schmutte and Lars Vilhuber},
  doi          = {10.1111/insr.12489},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {1-40},
  shortjournal = {Int. Stat. Rev.},
  title        = {An interview with john m. abowd},
  volume       = {90},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
