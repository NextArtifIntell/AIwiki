<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JASA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jasa---182">JASA - 182</h2>
<ul>
<li><details>
<summary>
(2022). Sampling: Design and analysis, 3rd ed. <em>JASA</em>,
<em>117</em>(540), 2287–2288. (<a
href="https://doi.org/10.1080/01621459.2022.2139708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {S. Lynne Stokes},
  doi          = {10.1080/01621459.2022.2139708},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2287-2288},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Sampling: Design and analysis, 3rd ed.},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric regression with r. <em>JASA</em>,
<em>117</em>(540), 2283–2287. (<a
href="https://doi.org/10.1080/01621459.2022.2139707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Zixiao Wang and Yi Feng and Lin Liu},
  doi          = {10.1080/01621459.2022.2139707},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2283-2287},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Semiparametric regression with r},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inference on multi-level partial correlations based on
multi-subject time series data. <em>JASA</em>, <em>117</em>(540),
2268–2282. (<a
href="https://doi.org/10.1080/01621459.2021.1917417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial correlations are commonly used to analyze the conditional dependence among variables. In this work, we propose a hierarchical model to study both the subject- and population-level partial correlations based on multi-subject time-series data. Multiple testing procedures adaptive to temporally dependent data with false discovery proportion control are proposed to identify the nonzero partial correlations in both the subject and population levels. A computationally feasible algorithm is developed. Theoretical results and simulation studies demonstrate the good properties of the proposed procedures. We illustrate the application of the proposed methods in a real example of brain connectivity on fMRI data from normal healthy persons and patients with Parkinson’s disease. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yumou Qiu and Xiao-Hua Zhou},
  doi          = {10.1080/01621459.2021.1917417},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2268-2282},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inference on multi-level partial correlations based on multi-subject time series data},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling time-varying random objects and dynamic networks.
<em>JASA</em>, <em>117</em>(540), 2252–2267. (<a
href="https://doi.org/10.1080/01621459.2021.1917416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Samples of dynamic or time-varying networks and other random object data such as time-varying probability distributions are increasingly encountered in modern data analysis. Common methods for time-varying data such as functional data analysis are infeasible when observations are time courses of networks or other complex non-Euclidean random objects that are elements of general metric spaces. In such spaces, only pairwise distances between the data objects are available and a strong limitation is that one cannot carry out arithmetic operations due to the lack of an algebraic structure. We combat this complexity by a generalized notion of mean trajectory taking values in the object space. For this, we adopt pointwise Fréchet means and then construct pointwise distance trajectories between the individual time courses and the estimated Fréchet mean trajectory, thus representing the time-varying objects and networks by functional data. Functional principal component analysis of these distance trajectories can reveal interesting features of dynamic networks and object time courses and is useful for downstream analysis. Our approach also makes it possible to study the empirical dynamics of time-varying objects, including dynamic regression to the mean or explosive behavior over time. We demonstrate desirable asymptotic properties of sample based estimators for suitable population targets under mild assumptions. The utility of the proposed methodology is illustrated with dynamic networks, time-varying distribution data and longitudinal growth data.},
  archive      = {J_JASA},
  author       = {Paromita Dubey and Hans-Georg Müller},
  doi          = {10.1080/01621459.2021.1917416},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2252-2267},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Modeling time-varying random objects and dynamic networks},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised linear regression. <em>JASA</em>,
<em>117</em>(540), 2238–2251. (<a
href="https://doi.org/10.1080/01621459.2021.1915320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a regression problem where for some part of the data we observe both the label variable ( Y ) and the predictors ( X X X ), while for other part of the data only the predictors are given. Such a problem arises, for example, when observations of the label variable are costly and may require a skilled human agent. When the conditional expectation E [ Y | X ] E [ Y | X ] E[Y|X] is not exactly linear, one can consider the best linear approximation to the conditional expectation, which can be estimated consistently by the least-square estimates (LSE). The latter depends only on the labeled data. We suggest improved alternative estimates to the LSE that use also the unlabeled data. Our estimation method can be easily implemented and has simply described asymptotic properties. The new estimates asymptotically dominate the usual standard procedures under certain non-linearity condition of E [ Y | X ] ; otherwise, they are asymptotically equivalent. The performance of the new estimator for small sample size is investigated in an extensive simulation study. A real data example of inferring homeless population is used to illustrate the new methodology.},
  archive      = {J_JASA},
  author       = {David Azriel and Lawrence D. Brown and Michael Sklar and Richard Berk and Andreas Buja and Linda Zhao},
  doi          = {10.1080/01621459.2021.1915320},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2238-2251},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Semi-supervised linear regression},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single-index thresholding in quantile regression.
<em>JASA</em>, <em>117</em>(540), 2222–2237. (<a
href="https://doi.org/10.1080/01621459.2021.1915319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Threshold regression models are useful for identifying subgroups with heterogeneous parameters. The conventional threshold regression models split the sample based on a single and observed threshold variable, which enforces the threshold point to be equal for all subgroups of the population. In this article, we consider a more flexible single-index threshold model in the quantile regression setup, in which the sample is split based on a linear combination of predictors. We propose a new estimator by smoothing the indicator function in thresholding, which enables Gaussian approximation for statistical inference and allows characterizing the limiting distribution when the quantile process is interested. We further construct a mixed-bootstrap inference method with faster computation and a procedure for testing the constancy of the threshold parameters across quantiles. Finally, we demonstrate the value of the proposed methods via simulation studies, as well as through the application to an executive compensation data.},
  archive      = {J_JASA},
  author       = {Yingying Zhang and Huixia Judy Wang and Zhongyi Zhu},
  doi          = {10.1080/01621459.2021.1915319},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2222-2237},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Single-index thresholding in quantile regression},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integrative factor regression and its inference for
multimodal data analysis. <em>JASA</em>, <em>117</em>(540), 2207–2221.
(<a href="https://doi.org/10.1080/01621459.2021.1914635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal data, where different types of data are collected from the same subjects, are fast emerging in a large variety of scientific applications. Factor analysis is commonly used in integrative analysis of multimodal data, and is particularly useful to overcome the curse of high dimensionality and high correlations. However, there is little work on statistical inference for factor analysis-based supervised modeling of multimodal data. In this article, we consider an integrative linear regression model that is built upon the latent factors extracted from multimodal data. We address three important questions: how to infer the significance of one data modality given the other modalities in the model; how to infer the significance of a combination of variables from one modality or across different modalities; and how to quantify the contribution, measured by the goodness of fit, of one data modality given the others. When answering each question, we explicitly characterize both the benefit and the extra cost of factor analysis. Those questions, to our knowledge, have not yet been addressed despite wide use of factor analysis in integrative multimodal analysis, and our proposal bridges an important gap. We study the empirical performance of our methods through simulations, and further illustrate with a multimodal neuroimaging analysis.},
  archive      = {J_JASA},
  author       = {Quefeng Li and Lexin Li},
  doi          = {10.1080/01621459.2021.1914635},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2207-2221},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Integrative factor regression and its inference for multimodal data analysis},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Testing independence under biased sampling. <em>JASA</em>,
<em>117</em>(540), 2194–2206. (<a
href="https://doi.org/10.1080/01621459.2021.1912758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing for dependence between pairs of random variables is a fundamental problem in statistics. In some applications, data are subject to selection bias that can create spurious dependence. An important example is truncation models, in which observed pairs are restricted to a specific subset of the X-Y plane. Standard tests for independence are not suitable in such cases, and alternative tests that take the selection bias into account are required. Here, we generalize the notion of quasi-independence with respect to the sampling mechanism, and study the problem of detecting any deviations from it. We develop two tests statistics motivated by the classic Hoeffding’s statistic, and use two approaches to compute their distribution under the null: (i) a bootstrap-based approach, and (ii) a permutation-test with nonuniform probability of permutations. We also handle an important application to the case of censoring with truncation, by estimating the biased sampling mechanism from the data. We prove the validity of the tests, and show, using simulations, that they improve power compared to competing methods for important special cases. The tests are applied to four datasets, two that are subject to truncation, with and without censoring, and two to bias mechanisms related to length bias.},
  archive      = {J_JASA},
  author       = {Yaniv Tenzer and Micha Mandel and Or Zuk},
  doi          = {10.1080/01621459.2021.1912758},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2194-2206},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Testing independence under biased sampling},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gradient-based markov chain monte carlo for bayesian
inference with non-differentiable priors. <em>JASA</em>,
<em>117</em>(540), 2182–2193. (<a
href="https://doi.org/10.1080/01621459.2021.1909600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of nondifferentiable priors in Bayesian statistics has become increasingly popular, in particular in Bayesian imaging analysis. Current state-of-the-art methods are approximate in the sense that they replace the posterior with a smooth approximation via Moreau-Yosida envelopes, and apply gradient-based discretized diffusions to sample from the resulting distribution. We characterize the error of the Moreau-Yosida approximation and propose a novel implementation using underdamped Langevin dynamics. In misson-critical cases, however, replacing the posterior with an approximation may not be a viable option. Instead, we show that piecewise-deterministic Markov processes (PDMP) can be used for exact posterior inference from distributions satisfying almost everywhere differentiability. Furthermore, in contrast with diffusion-based methods, the suggested PDMP-based samplers place no assumptions on the prior shape, nor require access to a computationally cheap proximal operator, and consequently have a much broader scope of application. Through detailed numerical examples, including a nondifferentiable circular distribution and a nonconvex genomics model, we elucidate the relative strengths of these sampling methods on problems of moderate to high dimensions, underlining the benefits of PDMP-based methods when accurate sampling is decisive. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jacob Vorstrup Goldman and Torben Sell and Sumeetpal Sidhu Singh},
  doi          = {10.1080/01621459.2021.1909600},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2182-2193},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Gradient-based markov chain monte carlo for bayesian inference with non-differentiable priors},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized bayes quantification learning under dataset
shift. <em>JASA</em>, <em>117</em>(540), 2163–2181. (<a
href="https://doi.org/10.1080/01621459.2021.1909599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantification learning is the task of prevalence estimation for a test population using predictions from a classifier trained on a different population. Quantification methods assume that the sensitivities and specificities of the classifier are either perfect or transportable from the training to the test population. These assumptions are inappropriate in the presence of dataset shift, when the misclassification rates in the training population are not representative of those for the test population. Quantification under dataset shift has been addressed only for single-class (categorical) predictions and assuming perfect knowledge of the true labels on a small subset of the test population. We propose generalized Bayes quantification learning (GBQL) that uses the entire compositional predictions from probabilistic classifiers and allows for uncertainty in true class labels for the limited labeled test data. Instead of positing a full model, we use a model-free Bayesian estimating equation approach to compositional data using Kullback–Leibler loss-functions based only on a first-moment assumption. The idea will be useful in Bayesian compositional data analysis in general as it is robust to different generating mechanisms for compositional data and allows 0’s and 1’s in the compositional outputs thereby including categorical outputs as a special case. We show how our method yields existing quantification approaches as special cases. Extension to an ensemble GBQL that uses predictions from multiple classifiers yielding inference robust to inclusion of a poor classifier is discussed. We outline a fast and efficient Gibbs sampler using a rounding and coarsening approximation to the loss functions. We establish posterior consistency, asymptotic normality and valid coverage of interval estimates from GBQL, which to our knowledge are the first theoretical results for a quantification approach in the presence of local labeled data. We also establish finite sample posterior concentration rate. Empirical performance of GBQL is demonstrated through simulations and analysis of real data with evident dataset shift. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jacob Fiksel and Abhirup Datta and Agbessi Amouzou and Scott Zeger},
  doi          = {10.1080/01621459.2021.1909599},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2163-2181},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Generalized bayes quantification learning under dataset shift},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting abrupt changes in the presence of local
fluctuations and autocorrelated noise. <em>JASA</em>, <em>117</em>(540),
2147–2162. (<a
href="https://doi.org/10.1080/01621459.2021.1909598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While there are a plethora of algorithms for detecting changes in mean in univariate time-series, almost all struggle in real applications where there is autocorrelated noise or where the mean fluctuates locally between the abrupt changes that one wishes to detect. In these cases, default implementations, which are often based on assumptions of a constant mean between changes and independent noise, can lead to substantial over-estimation of the number of changes. We propose a principled approach to detect such abrupt changes that models local fluctuations as a random walk process and autocorrelated noise via an AR(1) process. We then estimate the number and location of changepoints by minimizing a penalized cost based on this model. We develop a novel and efficient dynamic programming algorithm, DeCAFS, that can solve this minimization problem; despite the additional challenge of dependence across segments, due to the autocorrelated noise, which makes existing algorithms inapplicable. Theory and empirical results show that our approach has greater power at detecting abrupt changes than existing approaches. We apply our method to measuring gene expression levels in bacteria. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Gaetano Romano and Guillem Rigaill and Vincent Runge and Paul Fearnhead},
  doi          = {10.1080/01621459.2021.1909598},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2147-2162},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Detecting abrupt changes in the presence of local fluctuations and autocorrelated noise},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design-based ratio estimators and central limit theorems for
clustered, blocked RCTs. <em>JASA</em>, <em>117</em>(540), 2135–2146.
(<a href="https://doi.org/10.1080/01621459.2021.1906685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops design-based ratio estimators for clustered, blocked randomized controlled trials (RCTs), with an application to a federally funded, school-based RCT testing the effects of behavioral health interventions. We consider finite population weighted least-square estimators for average treatment effects (ATEs), allowing for general weighting schemes and covariates. We consider models with block-by-treatment status interactions as well as restricted models with block indicators only. We prove new finite population central limit theorems for each block specification. We also discuss simple variance estimators that share features with commonly used cluster-robust standard error estimators. Simulations show that the design-based ATE estimator yields nominal rejection rates with standard errors near true ones, even with few clusters.},
  archive      = {J_JASA},
  author       = {Peter Z. Schochet and Nicole E. Pashley and Luke W. Miratrix and Tim Kautz},
  doi          = {10.1080/01621459.2021.1906685},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2135-2146},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Design-based ratio estimators and central limit theorems for clustered, blocked RCTs},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A doubly enhanced EM algorithm for model-based tensor
clustering. <em>JASA</em>, <em>117</em>(540), 2120–2134. (<a
href="https://doi.org/10.1080/01621459.2021.1904959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern scientific studies often collect datasets in the form of tensors. These datasets call for innovative statistical analysis methods. In particular, there is a pressing need for tensor clustering methods to understand the heterogeneity in the data. We propose a tensor normal mixture model approach to enable probabilistic interpretation and computational tractability. Our statistical model leverages the tensor covariance structure to reduce the number of parameters for parsimonious modeling, and at the same time explicitly exploits the correlations for better variable selection and clustering. We propose a doubly enhanced expectation–maximization (DEEM) algorithm to perform clustering under this model. Both the expectation-step and the maximization-step are carefully tailored for tensor data in order to maximize statistical accuracy and minimize computational costs in high dimensions. Theoretical studies confirm that DEEM achieves consistent clustering even when the dimension of each mode of the tensors grows at an exponential rate of the sample size. Numerical studies demonstrate favorable performance of DEEM in comparison to existing methods.},
  archive      = {J_JASA},
  author       = {Qing Mai and Xin Zhang and Yuqing Pan and Kai Deng},
  doi          = {10.1080/01621459.2021.1904959},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2120-2134},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A doubly enhanced EM algorithm for model-based tensor clustering},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Individual data protected integrative regression analysis of
high-dimensional heterogeneous data. <em>JASA</em>, <em>117</em>(540),
2105–2119. (<a
href="https://doi.org/10.1080/01621459.2021.1904958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evidence-based decision making often relies on meta-analyzing multiple studies, which enables more precise estimation and investigation of generalizability. Integrative analysis of multiple heterogeneous studies is, however, highly challenging in the ultra high-dimensional setting. The challenge is even more pronounced when the individual-level data cannot be shared across studies, known as DataSHIELD contraint. Under sparse regression models that are assumed to be similar yet not identical across studies, we propose in this paper a novel integrative estimation procedure for data-Shielding High-dimensional Integrative Regression (SHIR). SHIR protects individual data through summary-statistics-based integrating procedure, accommodates between-study heterogeneity in both the covariate distribution and model parameters, and attains consistent variable selection. Theoretically, SHIR is statistically more efficient than the existing distributed approaches that integrate debiased LASSO estimators from the local sites. Furthermore, the estimation error incurred by aggregating derived data is negligible compared to the statistical minimax rate and SHIR is shown to be asymptotically equivalent in estimation to the ideal estimator obtained by sharing all data. The finite-sample performance of our method is studied and compared with existing approaches via extensive simulation settings. We further illustrate the utility of SHIR to derive phenotyping algorithms for coronary artery disease using electronic health records data from multiple chronic disease cohorts.},
  archive      = {J_JASA},
  author       = {Tianxi Cai and Molei Liu and Yin Xia},
  doi          = {10.1080/01621459.2021.1904958},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2105-2119},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Individual data protected integrative regression analysis of high-dimensional heterogeneous data},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonparametric fusion learning for multiparameters:
Synthesize inferences from diverse sources using data depth and
confidence distribution. <em>JASA</em>, <em>117</em>(540), 2086–2104.
(<a href="https://doi.org/10.1080/01621459.2021.1902817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fusion learning refers to synthesizing inferences from multiple sources or studies to make a more effective inference and prediction than from any individual source or study alone. Most existing methods for synthesizing inferences rely on parametric model assumptions, such as normality, which often do not hold in practice. We propose a general nonparametric fusion learning framework for synthesizing inferences for multiparameters from different studies. The main tool underlying the proposed framework is the new notion of depth confidence distribution (depth-CD) , which is developed by combining data depth and confidence distribution. Broadly speaking, a depth-CD is a data-driven nonparametric summary distribution of the available inferential information for a target parameter. We show that a depth-CD is a powerful inferential tool and, moreover, is an omnibus form of confidence regions, whose contours of level sets shrink toward the true parameter value. The proposed fusion learning approach combines depth-CD s from the individual studies, with each depth-CD constructed by nonparametric bootstrap and data depth. The approach is shown to be efficient , general and robust . Specifically, it achieves high-order accuracy and Bahadur efficiency under suitably chosen combining elements. It allows the model or inference structure to be different among individual studies. And, it readily adapts to heterogeneous studies with a broad range of complex and irregular settings. This last property enables the approach to use indirect evidence from incomplete studies to gain efficiency for the overall inference. We develop the theoretical support for the proposed approach, and we also illustrate the approach in making combined inference for the common mean vector and correlation coefficient from several studies. The numerical results from simulated studies show the approach to be less biased and more efficient than the traditional approaches in nonnormal settings. The advantages of the approach are also demonstrated in a Federal Aviation Administration study of aircraft landing performance. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Dungang Liu and Regina Y. Liu and Min-ge Xie},
  doi          = {10.1080/01621459.2021.1902817},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2086-2104},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric fusion learning for multiparameters: Synthesize inferences from diverse sources using data depth and confidence distribution},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Network functional varying coefficient model. <em>JASA</em>,
<em>117</em>(540), 2074–2085. (<a
href="https://doi.org/10.1080/01621459.2021.1901718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider functional responses with network dependence observed for each individual at irregular time points. To model both the interindividual dependence and within-individual dynamic correlation, we propose a network functional varying coefficient (NFVC) model. The response of each individual is characterized by a linear combination of responses from its connected nodes and its exogenous covariates. All the model coefficients are allowed to be time dependent. The NFVC model adds to the richness of both the classical network autoregression model and the functional regression models. To overcome the complexity caused by the network interdependence, we devise a special nonparametric least-squares-type estimator, which is feasible when the responses are observed at irregular time points for different individuals. The estimator takes advantage of the sparsity of the network structure to reduce the computational burden. To further conduct the functional principal component analysis, a novel within-individual covariance function estimation method is proposed and studied. Theoretical properties of our estimators, which involve techniques related to empirical processes, nonparametrics, functional data analysis and various concentration inequalities, are analyzed. We analyze a social network dataset to illustrate the powerfulness of the proposed procedure.},
  archive      = {J_JASA},
  author       = {Xuening Zhu and Zhanrui Cai and Yanyuan Ma},
  doi          = {10.1080/01621459.2021.1901718},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2074-2085},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Network functional varying coefficient model},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical network models for exchangeable structured
interaction processes. <em>JASA</em>, <em>117</em>(540), 2056–2073. (<a
href="https://doi.org/10.1080/01621459.2021.1896526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network data often arises via a series of structured interactions among a population of constituent elements. E-mail exchanges, for example, have a single sender followed by potentially multiple receivers. Scientific articles, on the other hand, may have multiple subject areas and multiple authors. We introduce a statistical model, termed the Pitman-Yor hierarchical vertex components model (PY-HVCM), that is well suited for structured interaction data. The proposed PY-HVCM effectively models complex relational data by partial pooling of local information via a latent, shared population-level distribution. The PY-HCVM is a canonical example of hierarchical vertex components models —a subfamily of models for exchangeable structured interaction-labeled networks , that is, networks invariant to interaction relabeling. Theoretical analysis and supporting simulations provide clear model interpretation, and establish global sparsity and power law degree distribution. A computationally tractable Gibbs sampling algorithm is derived for inferring sparsity and power law properties of complex networks. We demonstrate the model on both the Enron e-mail dataset and an ArXiv dataset, showing goodness of fit of the model via posterior predictive validation.},
  archive      = {J_JASA},
  author       = {Walter Dempsey and Brandon Oselio and Alfred Hero},
  doi          = {10.1080/01621459.2021.1896526},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2056-2073},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Hierarchical network models for exchangeable structured interaction processes},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Risk projection for time-to-event outcome leveraging summary
statistics with source individual-level data. <em>JASA</em>,
<em>117</em>(540), 2043–2055. (<a
href="https://doi.org/10.1080/01621459.2021.1895810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting risks of chronic diseases has become increasingly important in clinical practice. When a prediction model is developed in a cohort, there is a great interest to apply the model to other cohorts. Due to potential discrepancy in baseline disease incidences between different cohorts and shifts in patient composition, the risk predicted by the model built in the source cohort often under- or over-estimates the risk in a new cohort. In this article, we assume the relative risks of predictors are the same between the two cohorts, and propose a novel weighted estimating equation approach to recalibrating the projected risk for the targeted population through updating the baseline risk. The recalibration leverages the knowledge about survival probabilities for the disease of interest and competing events, and summary information of risk factors from the target population. We establish the consistency and asymptotic normality of the proposed estimators. Extensive simulation demonstrate that the proposed estimators are robust, even if the risk factor distributions differ between the source and target populations, and gain efficiency if they are the same, as long as the information from the target is precise. The method is illustrated with a recalibration of colorectal cancer prediction model.},
  archive      = {J_JASA},
  author       = {Jiayin Zheng and Yingye Zheng and Li Hsu},
  doi          = {10.1080/01621459.2021.1895810},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2043-2055},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Risk projection for time-to-event outcome leveraging summary statistics with source individual-level data},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymptotics for EBLUPs: Nested error regression models.
<em>JASA</em>, <em>117</em>(540), 2028–2042. (<a
href="https://doi.org/10.1080/01621459.2021.1895178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we derive the asymptotic distribution of estimated best linear unbiased predictors (EBLUPs) of the random effects in a nested error regression model. Under very mild conditions which do not require the assumption of normality, we show that asymptotically the distribution of the EBLUPs as both the number of clusters and the cluster sizes diverge to infinity is the convolution of the true distribution of the random effects and a normal distribution. This result yields very simple asymptotic approximations to and estimators of the prediction mean squared error of EBLUPs, and then asymptotic prediction intervals for the unobserved random effects. We also derive a higher order approximation to the asymptotic mean squared error and provide a detailed theoretical and empirical comparison with the well-known analytical prediction mean squared error approximations and estimators proposed by Kackar and Harville and Prasad and Rao. We show that our simple estimator of the predictor mean squared errors of EBLUPs works very well in practice when both the number of clusters and the cluster sizes are sufficiently large. Finally, we illustrate the use of the asymptotic prediction intervals with data on radon measurements of houses in Massachusetts and Arizona.},
  archive      = {J_JASA},
  author       = {Ziyang Lyu and A.H. Welsh},
  doi          = {10.1080/01621459.2021.1895178},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2028-2042},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Asymptotics for EBLUPs: Nested error regression models},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Testing mediation effects using logic of boolean matrices.
<em>JASA</em>, <em>117</em>(540), 2014–2027. (<a
href="https://doi.org/10.1080/01621459.2021.1895177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A central question in high-dimensional mediation analysis is to infer the significance of individual mediators. The main challenge is that the total number of potential paths that go through any mediator is super-exponential in the number of mediators. Most existing mediation inference solutions either explicitly impose that the mediators are conditionally independent given the exposure, or ignore any potential directed paths among the mediators. In this article, we propose a novel hypothesis testing procedure to evaluate individual mediation effects, while taking into account potential interactions among the mediators. Our proposal thus fills a crucial gap, and greatly extends the scope of existing mediation tests. Our key idea is to construct the test statistic using the logic of Boolean matrices, which enables us to establish the proper limiting distribution under the null hypothesis. We further employ screening, data splitting, and decorrelated estimation to reduce the bias and increase the power of the test. We show that our test can control both the size and false discovery rate asymptotically, and the power of the test approaches one, while allowing the number of mediators to diverge to infinity with the sample size. We demonstrate the efficacy of the method through simulations and a neuroimaging study of Alzheimer’s disease. A Python implementation of the proposed procedure is available at https://github.com/callmespring/LOGAN .},
  archive      = {J_JASA},
  author       = {Chengchun Shi and Lexin Li},
  doi          = {10.1080/01621459.2021.1895177},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {2014-2027},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Testing mediation effects using logic of boolean matrices},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust two-step wavelet-based inference for time series
models. <em>JASA</em>, <em>117</em>(540), 1996–2013. (<a
href="https://doi.org/10.1080/01621459.2021.1895176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent time series models such as (the independent sum of) ARMA( p , q ) models with additional stochastic processes are increasingly used for data analysis in biology, ecology, engineering, and economics. Inference on and/or prediction from these models can be highly challenging: (i) the data may contain outliers that can adversely affect the estimation procedure; (ii) the computational complexity can become prohibitive when the time series are extremely large; (iii) model selection adds another layer of (computational) complexity; and (iv) solutions that address (i), (ii), and (iii) simultaneously do not exist in practice. This paper aims at jointly addressing these challenges by proposing a general framework for robust two-step estimation based on a bounded influence M-estimator of the wavelet variance. We first develop the conditions for the joint asymptotic normality of the latter estimator thereby providing the necessary tools to perform (direct) inference for scale-based analysis of signals. Taking advantage of the model-independent weights of this first-step estimator, we then develop the asymptotic properties of two-step robust estimators using the framework of the generalized method of wavelet moments (GMWM). Simulation studies illustrate the good finite sample performance of the robust GMWM estimator and applied examples highlight the practical relevance of the proposed approach.},
  archive      = {J_JASA},
  author       = {Stéphane Guerrier and Roberto Molinari and Maria-Pia Victoria-Feser and Haotian Xu},
  doi          = {10.1080/01621459.2021.1895176},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1996-2013},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust two-step wavelet-based inference for time series models},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consistent sparse deep learning: Theory and computation.
<em>JASA</em>, <em>117</em>(540), 1981–1995. (<a
href="https://doi.org/10.1080/01621459.2021.1895175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been the engine powering many successes of data science. However, the deep neural network (DNN), as the basic model of deep learning, is often excessively over-parameterized, causing many difficulties in training, prediction and interpretation. We propose a frequentist-like method for learning sparse DNNs and justify its consistency under the Bayesian framework: the proposed method could learn a sparse DNN with at most O ( n / log ( n ) ) O ( n / log ( n ) ) O(n/ log (n)) connections and nice theoretical guarantees such as posterior consistency, variable selection consistency and asymptotically optimal generalization bounds. In particular, we establish posterior consistency for the sparse DNN with a mixture Gaussian prior, show that the structure of the sparse DNN can be consistently determined using a Laplace approximation-based marginal posterior inclusion probability approach, and use Bayesian evidence to elicit sparse DNNs learned by an optimization method such as stochastic gradient descent in multiple runs with different initializations. The proposed method is computationally more efficient than standard Bayesian methods for large-scale sparse DNNs. The numerical results indicate that the proposed method can perform very well for large-scale network compression and high-dimensional nonlinear variable selection, both advancing interpretable machine learning.},
  archive      = {J_JASA},
  author       = {Yan Sun and Qifan Song and Faming Liang},
  doi          = {10.1080/01621459.2021.1895175},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1981-1995},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Consistent sparse deep learning: Theory and computation},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian framework for simultaneous registration and
estimation of noisy, sparse, and fragmented functional data.
<em>JASA</em>, <em>117</em>(540), 1964–1980. (<a
href="https://doi.org/10.1080/01621459.2021.1893179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applications, smooth processes generate data that are recorded under a variety of observational regimes, including dense sampling and sparse or fragmented observations that are often contaminated with error. The statistical goal of registering and estimating the individual underlying functions from discrete observations has thus far been mainly approached sequentially without formal uncertainty propagation, or in an application-specific manner by pooling information across subjects. We propose a unified Bayesian framework for simultaneous registration and estimation, which is flexible enough to accommodate inference on individual functions under general observational regimes. Our ability to do this relies on the specification of strongly informative prior models over the amplitude component of function variability using two strategies: a data-driven approach that defines an empirical basis for the amplitude subspace based on training data, and a shape-restricted approach when the relative location and number of extrema is well-understood. The proposed methods build on the elastic functional data analysis framework to separately model amplitude and phase variability inherent in functional data. We emphasize the importance of uncertainty quantification and visualization of these two components as they provide complementary information about the estimated functions. We validate the proposed framework using multiple simulation studies and real applications.},
  archive      = {J_JASA},
  author       = {James Matuk and Karthik Bharath and Oksana Chkrebtii and Sebastian Kurtek},
  doi          = {10.1080/01621459.2021.1893179},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1964-1980},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian framework for simultaneous registration and estimation of noisy, sparse, and fragmented functional data},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inference of breakpoints in high-dimensional time series.
<em>JASA</em>, <em>117</em>(540), 1951–1963. (<a
href="https://doi.org/10.1080/01621459.2021.1893178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For multiple change-points detection of high-dimensional time series, we provide asymptotic theory concerning the consistency and the asymptotic distribution of the breakpoint statistics and estimated break sizes. The theory backs up a simple two-step procedure for detecting and estimating multiple change-points. The proposed two-step procedure involves the maximum of a MOSUM (moving sum) type statistics in the first step and a CUSUM (cumulative sum) refinement step on an aggregated time series in the second step. Thus, for a fixed time-point, we can capture both the biggest break across different coordinates and aggregating simultaneous breaks over multiple coordinates. Extending the existing high-dimensional Gaussian approximation theorem to dependent data with jumps, the theory allows us to characterize the size and power of our multiple change-point test asymptotically. Moreover, we can make inferences on the breakpoints estimates when the break sizes are small. Our theoretical setup incorporates both weak temporal and strong or weak cross-sectional dependence and is suitable for heavy-tailed innovations. A robust long-run covariance matrix estimation is proposed, which can be of independent interest. An application on detecting structural changes of the U.S. unemployment rate is considered to illustrate the usefulness of our method.},
  archive      = {J_JASA},
  author       = {Likai Chen and Weining Wang and Wei Biao Wu},
  doi          = {10.1080/01621459.2021.1893178},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1951-1963},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inference of breakpoints in high-dimensional time series},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Moderate-dimensional inferences on quadratic functionals in
ordinary least squares. <em>JASA</em>, <em>117</em>(540), 1931–1950. (<a
href="https://doi.org/10.1080/01621459.2021.1893177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical inferences for quadratic functionals of linear regression parameter have found wide applications including signal detection, global testing, inferences of error variance and fraction of variance explained. Classical theory based on ordinary least squares estimator works perfectly in the low-dimensional regime, but fails when the parameter dimension p n grows proportionally to the sample size n . In some cases, its performance is not satisfactory even when n ≥ 5 p n n ≥ 5 p n n≥5pn . The main contribution of this article is to develop dimension-adaptive inferences for quadratic functionals when lim n → ∞ p n / n = τ ∈ [ 0 , 1 ) . We propose a bias-and-variance-corrected test statistic and demonstrate that its theoretical validity (such as consistency and asymptotic normality) is adaptive to both low dimension with τ = 0 and moderate dimension with τ ∈ ( 0 , 1 ) . Our general theory holds, in particular, without Gaussian design/error or structural parameter assumption, and applies to a broad class of quadratic functionals covering all aforementioned applications. As a by-product, we find that the classical fixed-dimensional results continue to hold if and only if the signal-to-noise ratio is large enough, say when p n diverges but slower than n . Extensive numerical results demonstrate the satisfactory performance of the proposed methodology even when p n ≥ 0.9 n in some extreme cases. The mathematical arguments are based on the random matrix theory and leave-one-observation-out method.},
  archive      = {J_JASA},
  author       = {Xiao Guo and Guang Cheng},
  doi          = {10.1080/01621459.2021.1893177},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1931-1950},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Moderate-dimensional inferences on quadratic functionals in ordinary least squares},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A versatile estimation procedure without estimating the
nonignorable missingness mechanism. <em>JASA</em>, <em>117</em>(540),
1916–1930. (<a
href="https://doi.org/10.1080/01621459.2021.1893176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the estimation problem in a regression setting where the outcome variable is subject to nonignorable missingness and identifiability is ensured by the shadow variable approach. We propose a versatile estimation procedure where modeling of missingness mechanism is completely bypassed. We show that our estimator is easy to implement and we derive the asymptotic theory of the proposed estimator. We also investigate some alternative estimators under different scenarios. Comprehensive simulation studies are conducted to demonstrate the finite sample performance of the method. We apply the estimator to a children’s mental health study to illustrate its usefulness.},
  archive      = {J_JASA},
  author       = {Jiwei Zhao and Yanyuan Ma},
  doi          = {10.1080/01621459.2021.1893176},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1916-1930},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A versatile estimation procedure without estimating the nonignorable missingness mechanism},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preliminary multiple-test estimation, with applications to
k-sample covariance estimation. <em>JASA</em>, <em>117</em>(540),
1904–1915. (<a
href="https://doi.org/10.1080/01621459.2021.1892703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multisample covariance estimation—that is, estimation of the covariance matrices associated with k distinct populations—is a classical problem in multivariate statistics. A common solution is to base estimation on the outcome of a test that these covariance matrices show some given pattern. Such a preliminary test may, for example, investigate whether or not the various covariance matrices are equal to each other (test of homogeneity), or whether or not they have common eigenvectors (test of common principal components), etc. Since it is usually unclear what the possible pattern might be, it is natural to consider a collection of such patterns, leading to a collection of preliminary tests, and to base estimation on the outcome of such a multiple testing rule. In the present work, we therefore study preliminary test estimation based on multiple tests. Since this is of interest also outside k -sample covariance estimation, we do so in a very general framework where it is only assumed that the sequence of models at hand is locally asymptotically normal. In this general setup, we define the proposed estimators and derive their asymptotic properties. We come back to k -sample covariance estimation to illustrate the asymptotic and finite-sample behaviors of our estimators. Finally, we treat a real data example that allows us to show their practical relevance in a supervised classification framework.},
  archive      = {J_JASA},
  author       = {Davy Paindaveine and Joséa Rasoafaraniaina and Thomas Verdebout},
  doi          = {10.1080/01621459.2021.1892703},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1904-1915},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Preliminary multiple-test estimation, with applications to k-sample covariance estimation},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On characterizations and tests of benford’s law.
<em>JASA</em>, <em>117</em>(540), 1887–1903. (<a
href="https://doi.org/10.1080/01621459.2021.1891927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benford’s law defines a probability distribution for patterns of significant digits in real numbers. When the law is expected to hold for genuine observations, deviation from it can be taken as evidence of possible data manipulation. We derive results on a transform of the significand function that provide motivation for new tests of conformance to Benford’s law exploiting its sum-invariance characterization. We also study the connection between sum invariance of the first digit and the corresponding marginal probability distribution. We approximate the exact distribution of the new test statistics through a computationally efficient Monte Carlo algorithm. We investigate the power of our tests under different alternatives and we point out relevant situations in which they are clearly preferable to the available procedures. Finally, we show the application potential of our approach in the context of fraud detection in international trade.},
  archive      = {J_JASA},
  author       = {Lucio Barabesi and Andrea Cerasa and Andrea Cerioli and Domenico Perrotta},
  doi          = {10.1080/01621459.2021.1891927},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1887-1903},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {On characterizations and tests of benford’s law},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast, optimal, and targeted predictions using parameterized
decision analysis. <em>JASA</em>, <em>117</em>(540), 1875–1886. (<a
href="https://doi.org/10.1080/01621459.2021.1891926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction is critical for decision-making under uncertainty and lends validity to statistical inference. With targeted prediction , the goal is to optimize predictions for specific decision tasks of interest, which we represent via functionals. Although classical decision analysis extracts predictions from a Bayesian model, these predictions are often difficult to interpret and slow to compute. Instead, we design a class of parameterized actions for Bayesian decision analysis that produce optimal, scalable, and simple targeted predictions. For a wide variety of action parameterizations and loss functions—including linear actions with sparsity constraints for targeted variable selection—we derive a convenient representation of the optimal targeted prediction that yields efficient and interpretable solutions. Customized out-of-sample predictive metrics are developed to evaluate and compare among targeted predictors. Through careful use of the posterior predictive distribution, we introduce a procedure that identifies a set of near-optimal, or acceptable targeted predictors, which provide unique insights into the features and level of complexity needed for accurate targeted prediction. Simulations demonstrate excellent prediction, estimation, and variable selection capabilities. Targeted predictions are constructed for physical activity (PA) data from the National Health and Nutrition Examination Survey to better predict and understand the characteristics of intraday PA. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Daniel R. Kowal},
  doi          = {10.1080/01621459.2021.1891926},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1875-1886},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Fast, optimal, and targeted predictions using parameterized decision analysis},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). First-order newton-type estimator for distributed estimation
and inference. <em>JASA</em>, <em>117</em>(540), 1858–1874. (<a
href="https://doi.org/10.1080/01621459.2021.1891925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies distributed estimation and inference for a general statistical problem with a convex loss that could be nondifferentiable. For the purpose of efficient computation, we restrict ourselves to stochastic first-order optimization, which enjoys low per-iteration complexity. To motivate the proposed method, we first investigate the theoretical properties of a straightforward divide-and-conquer stochastic gradient descent approach. Our theory shows that there is a restriction on the number of machines and this restriction becomes more stringent when the dimension p is large. To overcome this limitation, this article proposes a new multi-round distributed estimation procedure that approximates the Newton step only using stochastic subgradient. The key component in our method is the proposal of a computationally efficient estimator of Σ − 1 w , where Σ is the population Hessian matrix and w is any given vector. Instead of estimating Σ (or Σ − 1 ) that usually requires the second-order differentiability of the loss, the proposed first-order Newton-type estimator (FONE) directly estimates the vector of interest Σ − 1 w as a whole and is applicable to nondifferentiable losses. Our estimator also facilitates the inference for the empirical risk minimizer. It turns out that the key term in the limiting covariance has the form of Σ − 1 w , which can be estimated by FONE.},
  archive      = {J_JASA},
  author       = {Xi Chen and Weidong Liu and Yichen Zhang},
  doi          = {10.1080/01621459.2021.1891925},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1858-1874},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {First-order newton-type estimator for distributed estimation and inference},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variable selection in the presence of factors: A model
selection perspective. <em>JASA</em>, <em>117</em>(540), 1847–1857. (<a
href="https://doi.org/10.1080/01621459.2021.1889565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of a Gaussian multiple regression model, we address the problem of variable selection when in the list of potential predictors there are factors, that is, categorical variables. We adopt a model selection perspective, that is, we approach the problem by constructing a class of models, each corresponding to a particular selection of active variables. The methodology is Bayesian and proceeds by computing the posterior probability of each of these models. We highlight the fact that the set of competing models depends on the dummy variable representation of the factors, an issue already documented by Fernández et al. in a particular example but that has not received any attention since then. We construct methodology that circumvents this problem and that presents very competitive frequentist behavior when compared with recently proposed techniques. Additionally, it is fully automatic, in that it does not require the specification of any tuning parameters.},
  archive      = {J_JASA},
  author       = {Gonzalo García-Donato and Rui Paulo},
  doi          = {10.1080/01621459.2021.1889565},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1847-1857},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Variable selection in the presence of factors: A model selection perspective},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inference for high-dimensional linear mixed-effects models:
A quasi-likelihood approach. <em>JASA</em>, <em>117</em>(540),
1835–1846. (<a
href="https://doi.org/10.1080/01621459.2021.1888740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear mixed-effects models are widely used in analyzing clustered or repeated measures data. We propose a quasi-likelihood approach for estimation and inference of the unknown parameters in linear mixed-effects models with high-dimensional fixed effects. The proposed method is applicable to general settings where the dimension of the random effects and the cluster sizes are possibly large. Regarding the fixed effects, we provide rate optimal estimators and valid inference procedures that do not rely on the structural information of the variance components. We also study the estimation of variance components with high-dimensional fixed effects in general settings. The algorithms are easy to implement and computationally fast. The proposed methods are assessed in various simulation settings and are applied to a real study regarding the associations between body mass index and genetic polymorphic markers in a heterogeneous stock mice population.},
  archive      = {J_JASA},
  author       = {Sai Li and T. Tony Cai and Hongzhe Li},
  doi          = {10.1080/01621459.2021.1888740},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1835-1846},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inference for high-dimensional linear mixed-effects models: A quasi-likelihood approach},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-component mixture model in the presence of covariates.
<em>JASA</em>, <em>117</em>(540), 1820–1834. (<a
href="https://doi.org/10.1080/01621459.2021.1888739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study a generalization of the two-groups model in the presence of covariates—a problem that has recently received much attention in the statistical literature due to its applicability in multiple hypotheses testing problems. The model we consider allows for infinite dimensional parameters and offers flexibility in modeling the dependence of the response on the covariates. We discuss the identifiability issues arising in this model and systematically study several estimation strategies. We propose a tuning parameter-free nonparametric maximum likelihood method, implementable via the expectation-maximization algorithm, to estimate the unknown parameters. Further, we derive the rate of convergence of the proposed estimators—in particular we show that the finite sample Hellinger risk for every ‘approximate’ nonparametric maximum likelihood estimator achieves a near-parametric rate (up to logarithmic multiplicative factors). In addition, we propose and theoretically study two ‘marginal’ methods that are more scalable and easily implementable. We demonstrate the efficacy of our procedures through extensive simulation studies and relevant data analyses—one arising from neuroscience and the other from astronomy. We also outline the application of our methods to multiple testing. The companion R package NPMLEmix implements all the procedures proposed in this article.},
  archive      = {J_JASA},
  author       = {Nabarun Deb and Sujayam Saha and Adityanand Guntuboyina and Bodhisattva Sen},
  doi          = {10.1080/01621459.2021.1888739},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1820-1834},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Two-component mixture model in the presence of covariates},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Directed community detection with network embedding.
<em>JASA</em>, <em>117</em>(540), 1809–1819. (<a
href="https://doi.org/10.1080/01621459.2021.1887742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection in network data aims at grouping similar nodes sharing certain characteristics together. Most existing methods focus on detecting communities in undirected networks, where similarity between nodes is measured by their node features and whether they are connected. In this article, we propose a novel method to conduct network embedding and community detection simultaneously in a directed network. The network embedding model introduces two sets of vectors to represent the out- and in-nodes separately, and thus allows the same nodes belong to different out- and in-communities. The community detection formulation equips the negative log-likelihood with a novel regularization term to encourage community structure among the nodes representations, and thus achieves better performance by jointly estimating the nodes embeddings and their community structures. To tackle the resultant optimization task, an efficient alternative updating scheme is developed. More importantly, the asymptotic properties of the proposed method are established in terms of both network embedding and community detection, which are also supported by numerical experiments on some simulated and real examples.},
  archive      = {J_JASA},
  author       = {Jingnan Zhang and Xin He and Junhui Wang},
  doi          = {10.1080/01621459.2021.1887742},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1809-1819},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Directed community detection with network embedding},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep compositional spatial models. <em>JASA</em>,
<em>117</em>(540), 1787–1808. (<a
href="https://doi.org/10.1080/01621459.2021.1887741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial processes with nonstationary and anisotropic covariance structure are often used when modeling, analyzing, and predicting complex environmental phenomena. Such processes may often be expressed as ones that have stationary and isotropic covariance structure on a warped spatial domain. However, the warping function is generally difficult to fit and not constrained to be injective, often resulting in “space-folding.” Here, we propose modeling an injective warping function through a composition of multiple elemental injective functions in a deep-learning framework. We consider two cases; first, when these functions are known up to some weights that need to be estimated, and, second, when the weights in each layer are random. Inspired by recent methodological and technological advances in deep learning and deep Gaussian processes, we employ approximate Bayesian methods to make inference with these models using graphics processing units. Through simulation studies in one and two dimensions we show that the deep compositional spatial models are quick to fit, and are able to provide better predictions and uncertainty quantification than other deep stochastic models of similar complexity. We also show their remarkable capacity to model nonstationary, anisotropic spatial data using radiances from the MODIS instrument aboard the Aqua satellite.},
  archive      = {J_JASA},
  author       = {Andrew Zammit-Mangion and Tin Lok James Ng and Quan Vu and Maurizio Filippone},
  doi          = {10.1080/01621459.2021.1887741},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1787-1808},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Deep compositional spatial models},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed estimation for principal component analysis: An
enlarged eigenspace analysis. <em>JASA</em>, <em>117</em>(540),
1775–1786. (<a
href="https://doi.org/10.1080/01621459.2021.1886937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing size of modern datasets brings many challenges to the existing statistical estimation approaches, which calls for new distributed methodologies. This article studies distributed estimation for a fundamental statistical machine learning problem, principal component analysis (PCA). Despite the massive literature on top eigenvector estimation, much less is presented for the top- L -dim ( L &gt; 1) eigenspace estimation, especially in a distributed manner. We propose a novel multi-round algorithm for constructing top- L -dim eigenspace for distributed data. Our algorithm takes advantage of shift-and-invert preconditioning and convex optimization. Our estimator is communication-efficient and achieves a fast convergence rate. In contrast to the existing divide-and-conquer algorithm, our approach has no restriction on the number of machines. Theoretically, the traditional Davis–Kahan theorem requires the explicit eigengap assumption to estimate the top- L -dim eigenspace. To abandon this eigengap assumption, we consider a new route in our analysis: instead of exactly identifying the top- L -dim eigenspace, we show that our estimator is able to cover the targeted top- L -dim population eigenspace. Our distributed algorithm can be applied to a wide range of statistical problems based on PCA, such as principal component regression and single index model. Finally, we provide simulation studies to demonstrate the performance of the proposed distributed estimator.},
  archive      = {J_JASA},
  author       = {Xi Chen and Jason D. Lee and He Li and Yun Yang},
  doi          = {10.1080/01621459.2021.1886937},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1775-1786},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Distributed estimation for principal component analysis: An enlarged eigenspace analysis},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the hauck–donner effect in wald tests: Detection, tipping
points, and parameter space characterization. <em>JASA</em>,
<em>117</em>(540), 1763–1774. (<a
href="https://doi.org/10.1080/01621459.2021.1886936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Wald test remains ubiquitous in statistical practice despite shortcomings such as its inaccuracy in small samples and lack of invariance under reparameterization. This article develops on another but lesser-known shortcoming called the Hauck–Donner effect (HDE) whereby a Wald test statistic is no longer monotone increasing as a function of increasing distance between the parameter estimate and the null value. Resulting in an upward biased p-value and loss of power, the aberration can lead to very damaging consequences such as in variable selection. The HDE afflicts many types of regression models and corresponds to estimates near the boundary of the parameter space. This article presents several new results, and its main contributions are to (i) propose a very general test for detecting the HDE in the class of vector generalized linear models (VGLMs), regardless of the underlying cause; (ii) fundamentally characterize the HDE by pairwise ratios of Wald and Rao score and likelihood ratio test statistics for 1-parameter distributions with large samples; (iii) show that the parameter space may be partitioned into an interior encased by at least 5 HDE severity measures (faint, weak, moderate, strong, extreme); (iv) prove that a necessary condition for the HDE in a 2 by 2 table is a log odds ratio of at least 2; (v) give some practical guidelines about HDE-free hypothesis testing. Overall, practical post-fit tests can now be conducted potentially to any model estimated by iteratively reweighted least squares, especially the GLM and VGLM classes, the latter which encompasses many popular regression models.},
  archive      = {J_JASA},
  author       = {Thomas W. Yee},
  doi          = {10.1080/01621459.2021.1886936},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1763-1774},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {On the Hauck–Donner effect in wald tests: Detection, tipping points, and parameter space characterization},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive inference for change points in high-dimensional
data. <em>JASA</em>, <em>117</em>(540), 1751–1762. (<a
href="https://doi.org/10.1080/01621459.2021.1884562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a class of test statistics for a change point in the mean of high-dimensional independent data. Our test integrates the U-statistic based approach in a recent work by Wang et al. and the L q -norm based high-dimensional test in a recent work by He et al., and inherits several appealing features such as being tuning parameter free and asymptotic independence for test statistics corresponding to even q ’s. A simple combination of test statistics corresponding to several different q ’s leads to a test with adaptive power property, that is, it can be powerful against both sparse and dense alternatives. On the estimation front, we obtain the convergence rate of the maximizer of our test statistic standardized by sample size when there is one change-point in mean and q = 2, and propose to combine our tests with a wild binary segmentation algorithm to estimate the change-point number and locations when there are multiple change-points. Numerical comparisons using both simulated and real data demonstrate the advantage of our adaptive test and its corresponding estimation method.},
  archive      = {J_JASA},
  author       = {Yangfan Zhang and Runmin Wang and Xiaofeng Shao},
  doi          = {10.1080/01621459.2021.1884562},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1751-1762},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Adaptive inference for change points in high-dimensional data},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Linear hypothesis testing in linear models with
high-dimensional responses. <em>JASA</em>, <em>117</em>(540), 1738–1750.
(<a href="https://doi.org/10.1080/01621459.2021.1884561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a new projection test for linear hypotheses on regression coefficient matrices in linear models with high-dimensional responses. We systematically study the theoretical properties of the proposed test. We first derive the optimal projection matrix for any given projection dimension to achieve the best power and provide an upper bound for the optimal dimension of projection matrix. We further provide insights into how to construct the optimal projection matrix. One- and two-sample mean problems can be formulated as special cases of linear hypotheses studied in this article. We both theoretically and empirically demonstrate that the proposed test can outperform the existing ones for one- and two-sample mean problems. We conduct Monte Carlo simulation to examine the finite sample performance and illustrate the proposed test by a real data example.},
  archive      = {J_JASA},
  author       = {Changcheng Li Runze Li},
  doi          = {10.1080/01621459.2021.1884561},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1738-1750},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Linear hypothesis testing in linear models with high-dimensional responses},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inference in experiments with matched pairs. <em>JASA</em>,
<em>117</em>(540), 1726–1737. (<a
href="https://doi.org/10.1080/01621459.2021.1883437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies inference for the average treatment effect in randomized controlled trials where treatment status is determined according to a “matched pairs” design. By a “matched pairs” design, we mean that units are sampled iid from the population of interest, paired according to observed, baseline covariates and finally, within each pair, one unit is selected at random for treatment. This type of design is used routinely throughout the sciences, but fundamental questions about its implications for inference about the average treatment effect remain. The main requirement underlying our analysis is that pairs are formed so that units within pairs are suitably “close” in terms of the baseline covariates, and we develop novel results to ensure that pairs are formed in a way that satisfies this condition. Under this assumption, we show that, for the problem of testing the null hypothesis that the average treatment effect equals a prespecified value in such settings, the commonly used two-sample t -test and “matched pairs” t -test are conservative in the sense that these tests have limiting rejection probability under the null hypothesis no greater than and typically strictly less than the nominal level. We show, however, that a simple adjustment to the standard errors of these tests leads to a test that is asymptotically exact in the sense that its limiting rejection probability under the null hypothesis equals the nominal level. We also study the behavior of randomization tests that arise naturally in these types of settings. When implemented appropriately, we show that this approach also leads to a test that is asymptotically exact in the sense described previously, but additionally has finite-sample rejection probability no greater than the nominal level for certain distributions satisfying the null hypothesis. A simulation study and empirical application confirm the practical relevance of our theoretical results.},
  archive      = {J_JASA},
  author       = {Yuehao Bai and Joseph P. Romano and Azeem M. Shaikh},
  doi          = {10.1080/01621459.2021.1883437},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1726-1737},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inference in experiments with matched pairs},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kernel ordinary differential equations. <em>JASA</em>,
<em>117</em>(540), 1711–1725. (<a
href="https://doi.org/10.1080/01621459.2021.1882466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinary differential equation (ODE) is widely used in modeling biological and physical processes in science. In this article, we propose a new reproducing kernel-based approach for estimation and inference of ODE given noisy observations. We do not assume the functional forms in ODE to be known, or restrict them to be linear or additive, and we allow pairwise interactions. We perform sparse estimation to select individual functionals, and construct confidence intervals for the estimated signal trajectories. We establish the estimation optimality and selection consistency of kernel ODE under both the low-dimensional and high-dimensional settings, where the number of unknown functionals can be smaller or larger than the sample size. Our proposal builds upon the smoothing spline analysis of variance (SS-ANOVA) framework, but tackles several important problems that are not yet fully addressed, and thus extends the scope of existing SS-ANOVA as well. We demonstrate the efficacy of our method through numerous ODE examples.},
  archive      = {J_JASA},
  author       = {Xiaowu Dai and Lexin Li},
  doi          = {10.1080/01621459.2021.1882466},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1711-1725},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Kernel ordinary differential equations},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neuronized priors for bayesian sparse linear regression.
<em>JASA</em>, <em>117</em>(540), 1695–1710. (<a
href="https://doi.org/10.1080/01621459.2021.1876710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Bayesian variable selection methods have been intensively studied, their routine use in practice has not caught up with their non-Bayesian counterparts such as Lasso, likely due to difficulties in both computations and flexibilities of prior choices. To ease these challenges, we propose the neuronized priors to unify and extend some popular shrinkage priors, such as Laplace, Cauchy, horseshoe, and spike-and-slab priors. A neuronized prior can be written as the product of a Gaussian weight variable and a scale variable transformed from Gaussian via an activation function. Compared with classic spike-and-slab priors, the neuronized priors achieve the same explicit variable selection without employing any latent indicator variables, which results in both more efficient and flexible posterior sampling and more effective posterior modal estimation. Theoretically, we provide specific conditions on the neuronized formulation to achieve the optimal posterior contraction rate, and show that a broadly applicable MCMC algorithm achieves an exponentially fast convergence rate under the neuronized formulation. We also examine various simulated and real data examples and demonstrate that using the neuronization representation is computationally more or comparably efficient than its standard counterpart in all well-known cases. An R package NPrior is provided for using neuronized priors in Bayesian linear regression.},
  archive      = {J_JASA},
  author       = {Minsuk Shin and Jun S. Liu},
  doi          = {10.1080/01621459.2021.1876710},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1695-1710},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Neuronized priors for bayesian sparse linear regression},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A regression modeling approach to structured shrinkage
estimation. <em>JASA</em>, <em>117</em>(540), 1684–1694. (<a
href="https://doi.org/10.1080/01621459.2021.1875838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problems involving the simultaneous estimation of multiple parameters arise in many areas of theoretical and applied statistics. A canonical example is the estimation of a vector of normal means. Frequently, structural information about relationships between the parameters of interest is available. For example, in a gene expression denoising problem, genes with similar functions may have similar expression levels. Despite its importance, structural information has not been well-studied in the simultaneous estimation literature, perhaps in part because it poses challenges to the usual geometric or empirical Bayes shrinkage estimation paradigms. This article proposes that some of these challenges can be resolved by adopting an alternate paradigm, based on regression modeling. This approach can naturally incorporate structural information and also motivates new shrinkage estimation and inference procedures. As an illustration, this regression paradigm is used to develop a class of estimators with asymptotic risk optimality properties that perform well in simulations and in denoising gene expression data from a single cell RNA-sequencing experiment.},
  archive      = {J_JASA},
  author       = {Sihai Dave Zhao and William Biscarri},
  doi          = {10.1080/01621459.2021.1875838},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1684-1694},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A regression modeling approach to structured shrinkage estimation},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heterogeneous mediation analysis on epigenomic PTSD and
traumatic stress in a predominantly african american cohort.
<em>JASA</em>, <em>117</em>(540), 1669–1683. (<a
href="https://doi.org/10.1080/01621459.2022.2089572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DNA methylation (DNAm) has been suggested to play a critical role in post-traumatic stress disorder (PTSD), through mediating the relationship between trauma and PTSD. However, this underlying mechanism of PTSD for African Americans still remains unknown. To fill this gap, in this article, we investigate how DNAm mediates the effects of traumatic experiences on PTSD symptoms in the Detroit Neighborhood Health Study (DNHS) (2008–2013) which involves primarily African Americans adults. To achieve this, we develop a new mediation analysis approach for high-dimensional potential DNAm mediators. A key novelty of our method is that we consider heterogeneity in mediation effects across subpopulations. Specifically, mediators in different subpopulations could have opposite effects on the outcome, and thus could be difficult to identify under a traditional homogeneous model framework. In contrast, the proposed method can estimate heterogeneous mediation effects and identifies subpopulations in which individuals share similar effects. Simulation studies demonstrate that the proposed method outperforms existing methods for both homogeneous and heterogeneous data. We also present our mediation analysis results of a dataset with 125 participants and more than 450 , 000 CpG sites from the DNHS study. The proposed method finds three subgroups of subjects and identifies DNAm mediators corresponding to genes such as HSP90AA1 and NFATC1 which have been linked to PTSD symptoms in literature. Our finding could be useful in future finer-grained investigation of PTSD mechanism and in the development of new treatments for PTSD. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Fei Xue and Xiwei Tang and Grace Kim and Karestan C. Koenen and Chantel L. Martin and Sandro Galea and Derek Wildman and Monica Uddin and Annie Qu},
  doi          = {10.1080/01621459.2022.2089572},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1669-1683},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Heterogeneous mediation analysis on epigenomic PTSD and traumatic stress in a predominantly african american cohort},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mapping the genetic-imaging-clinical pathway with
applications to alzheimer’s disease. <em>JASA</em>, <em>117</em>(540),
1656–1668. (<a
href="https://doi.org/10.1080/01621459.2022.2087658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease is a progressive form of dementia that results in problems with memory, thinking, and behavior. It often starts with abnormal aggregation and deposition of β amyloid and tau, followed by neuronal damage such as atrophy of the hippocampi, leading to Alzheimer’s disease (AD). The aim of this article is to map the genetic-imaging-clinical pathway for AD in order to delineate the genetically-regulated brain changes that drive disease progression based on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset. We develop a novel two-step approach to delineate the association between high-dimensional 2D hippocampal surface exposures and the Alzheimer’s Disease Assessment Scale (ADAS) cognitive score, while taking into account the ultra-high dimensional clinical and genetic covariates at baseline. Analysis results suggest that the radial distance of each pixel of both hippocampi is negatively associated with the severity of behavioral deficits conditional on observed clinical and genetic covariates. These associations are stronger in Cornu Ammonis region 1 (CA1) and subiculum subregions compared to Cornu Ammonis region 2 (CA2) and Cornu Ammonis region 3 (CA3) subregions. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Dengdeng Yu and Linbo Wang and Dehan Kong and Hongtu Zhu},
  doi          = {10.1080/01621459.2022.2087658},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1656-1668},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Mapping the genetic-imaging-clinical pathway with applications to alzheimer’s disease},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Causal structural learning on MPHIA individual dataset.
<em>JASA</em>, <em>117</em>(540), 1642–1655. (<a
href="https://doi.org/10.1080/01621459.2022.2077209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Population-based HIV Impact Assessment (PHIA) is an ongoing project that conducts nationally representative HIV-focused surveys for measuring national and regional progress toward UNAIDS’ 90-90-90 targets, the primary strategy to end the HIV epidemic. We believe the PHIA survey offers a unique opportunity to better understand the key factors that drive the HIV epidemics in the most affected countries in sub-Saharan Africa. In this article, we propose a novel causal structural learning algorithm to discover important covariates and potential causal pathways for 90-90-90 targets. Existing constraint-based causal structural learning algorithms are quite aggressive in edge removal. The proposed algorithm preserves more information about important features and potential causal pathways. It is applied to the Malawi PHIA (MPHIA) dataset and leads to interesting results. For example, it discovers age and condom usage to be important for female HIV awareness; the number of sexual partners to be important for male HIV awareness; and knowing the travel time to HIV care facilities leads to a higher chance of being treated for both females and males. We further compare and validate the proposed algorithm using BIC and using Monte Carlo simulations, and show that the proposed algorithm achieves improvement in true positive rates in important feature discovery over existing algorithms. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Le Bao and Changcheng Li and Runze Li and Songshan Yang},
  doi          = {10.1080/01621459.2022.2077209},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1642-1655},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Causal structural learning on MPHIA individual dataset},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Functional mixed effects clustering with application to
longitudinal urologic chronic pelvic pain syndrome symptom data.
<em>JASA</em>, <em>117</em>(540), 1631–1641. (<a
href="https://doi.org/10.1080/01621459.2022.2066536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By clustering patients with the urologic chronic pelvic pain syndromes (UCPPS) into homogeneous subgroups and associating these subgroups with baseline covariates and other clinical outcomes, we provide opportunities to investigate different potential elements of pathogenesis, which may also guide us in selection of appropriate therapeutic targets. Motivated by the longitudinal urologic symptom data with extensive subject heterogeneity and differential variability of trajectories, we propose a functional clustering procedure where each subgroup is modeled by a functional mixed effects model, and the posterior probability is used to iteratively classify each subject into different subgroups. The classification takes into account both group-average trajectories and between-subject variabilities. We develop an equivalent state-space model for efficient computation. We also propose a cross-validation based Kullback–Leibler information criterion to choose the optimal number of subgroups. The performance of the proposed method is assessed through a simulation study. We apply our methods to longitudinal bi-weekly measures of a primary urological urinary symptoms score from a UCPPS longitudinal cohort study, and identify four subgroups ranging from moderate decline, mild decline, stable and mild increasing. The resulting clusters are also associated with the one-year changes in several clinically important outcomes, and are also related to several clinically relevant baseline predictors, such as sleep disturbance score, physical quality of life and painful urgency. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Wensheng Guo and Mengying You and Jialin Yi and Michel A. Pontari and J. Richard Landis},
  doi          = {10.1080/01621459.2022.2066536},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1631-1641},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Functional mixed effects clustering with application to longitudinal urologic chronic pelvic pain syndrome symptom data},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Modeling and learning from variation and covariation.
<em>JASA</em>, <em>117</em>(540), 1627–1630. (<a
href="https://doi.org/10.1080/01621459.2022.2117703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Blakeley B. McShane and Ulf Böckenholt and Karsten T. Hansen},
  doi          = {10.1080/01621459.2022.2117703},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1627-1630},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Modeling and learning from variation and covariation},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The potential of factor analysis for replication,
generalization, and integration. <em>JASA</em>, <em>117</em>(540),
1622–1626. (<a
href="https://doi.org/10.1080/01621459.2022.2096618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Paul De Boeck and Michael L. DeKay and Menglin Xu},
  doi          = {10.1080/01621459.2022.2096618},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1622-1626},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {The potential of factor analysis for replication, generalization, and integration},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Variation and covariation in large-scale replication
projects: An evaluation of replicability. <em>JASA</em>,
<em>117</em>(540), 1605–1621. (<a
href="https://doi.org/10.1080/01621459.2022.2054816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last decade, large-scale replication projects across the biomedical and social sciences have reported relatively low replication rates. In these large-scale replication projects, replication has typically been evaluated based on a single replication study of some original study and dichotomously as successful or failed. However, evaluations of replicability that are based on a single study and are dichotomous are inadequate, and evaluations of replicability should instead be based on multiple studies, be continuous, and be multi-faceted. Further, such evaluations are in fact possible due to two characteristics shared by many large-scale replication projects. In this article, we provide such an evaluation for two prominent large-scale replication projects, one which replicated a phenomenon from cognitive psychology and another which replicated 13 phenomena from social psychology and behavioral economics. Our results indicate a very high degree of replicability in the former and a medium to low degree of replicability in the latter. They also suggest an unidentified covariate in each, namely ocular dominance in the former and political ideology in the latter, that is theoretically pertinent. We conclude by discussing evaluations of replicability at large, recommendations for future large-scale replication projects, and design-based model generalization. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Blakeley B. McShane and Ulf Böckenholt and Karsten T. Hansen},
  doi          = {10.1080/01621459.2022.2054816},
  journal      = {Journal of the American Statistical Association},
  number       = {540},
  pages        = {1605-1621},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Variation and covariation in large-scale replication projects: An evaluation of replicability},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Foundations of statistics for data scientists: With r and
python. <em>JASA</em>, <em>117</em>(539), 1603–1604. (<a
href="https://doi.org/10.1080/01621459.2022.2104726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Nicholas J. Horton},
  doi          = {10.1080/01621459.2022.2104726},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1603-1604},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Foundations of statistics for data scientists: With r and python},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A history of data visualization and graphic communication,.
<em>JASA</em>, <em>117</em>(539), 1601–1603. (<a
href="https://doi.org/10.1080/01621459.2022.2098134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Hyunwoo Park},
  doi          = {10.1080/01621459.2022.2098134},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1601-1603},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A history of data visualization and graphic communication,},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An invitation to sequential monte carlo samplers.
<em>JASA</em>, <em>117</em>(539), 1587–1600. (<a
href="https://doi.org/10.1080/01621459.2022.2087659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statisticians often use Monte Carlo methods to approximate probability distributions, primarily with Markov chain Monte Carlo and importance sampling. Sequential Monte Carlo samplers are a class of algorithms that combine both techniques to approximate distributions of interest and their normalizing constants. These samplers originate from particle filtering for state space models and have become general and scalable sampling techniques. This article describes sequential Monte Carlo samplers and their possible implementations, arguing that they remain under-used in statistics, despite their ability to perform sequential inference and to leverage parallel processing resources among other potential benefits. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Chenguang Dai and Jeremy Heng and Pierre E. Jacob and Nick Whiteley},
  doi          = {10.1080/01621459.2022.2087659},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1587-1600},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {An invitation to sequential monte carlo samplers},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extremile regression. <em>JASA</em>, <em>117</em>(539),
1579–1586. (<a
href="https://doi.org/10.1080/01621459.2021.1875837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression extremiles define a least squares analogue of regression quantiles. They are determined by weighted expectations rather than tail probabilities. Of special interest is their intuitive meaning in terms of expected minima and maxima. Their use appears naturally in risk management where, in contrast to quantiles, they fulfill the coherency axiom and take the severity of tail losses into account. In addition, they are comonotonically additive and belong to both the families of spectral risk measures and concave distortion risk measures. This article provides the first detailed study exploring implications of the extremile terminology in a general setting of presence of covariates. We rely on local linear (least squares) check function minimization for estimating conditional extremiles and deriving the asymptotic normality of their estimators. We also extend extremile regression far into the tails of heavy-tailed distributions. Extrapolated estimators are constructed and their asymptotic theory is developed. Some applications to real data are provided.},
  archive      = {J_JASA},
  author       = {Abdelaati Daouia and Irène Gijbels and Gilles Stupfler},
  doi          = {10.1080/01621459.2021.1875837},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1579-1586},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Extremile regression},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-dimensional spatial quantile function-on-scalar
regression. <em>JASA</em>, <em>117</em>(539), 1563–1578. (<a
href="https://doi.org/10.1080/01621459.2020.1870984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a novel spatial quantile function-on-scalar regression model, which studies the conditional spatial distribution of a high-dimensional functional response given scalar predictors. With the strength of both quantile regression and copula modeling, we are able to explicitly characterize the conditional distribution of the functional or image response on the whole spatial domain. Our method provides a comprehensive understanding of the effect of scalar covariates on functional responses across different quantile levels and also gives a practical way to generate new images for given covariate values. Theoretically, we establish the minimax rates of convergence for estimating coefficient functions under both fixed and random designs. We further develop an efficient primal-dual algorithm to handle high-dimensional image data. Simulations and real data analysis are conducted to examine the finite-sample performance.},
  archive      = {J_JASA},
  author       = {Zhengwu Zhang and Xiao Wang and Linglong Kong and Hongtu Zhu},
  doi          = {10.1080/01621459.2020.1870984},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1563-1578},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {High-dimensional spatial quantile function-on-scalar regression},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonparametric tests of the causal null with nondiscrete
exposures. <em>JASA</em>, <em>117</em>(539), 1551–1562. (<a
href="https://doi.org/10.1080/01621459.2020.1865168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many scientific studies, it is of interest to determine whether an exposure has a causal effect on an outcome. In observational studies, this is a challenging task due to the presence of confounding variables that affect both the exposure and the outcome. Many methods have been developed to test for the presence of a causal effect when all such confounding variables are observed and when the exposure of interest is discrete. In this article, we propose a class of nonparametric tests of the null hypothesis that there is no average causal effect of an arbitrary univariate exposure on an outcome in the presence of observed confounding. Our tests apply to discrete, continuous, and mixed discrete-continuous exposures. We demonstrate that our proposed tests are doubly robust consistent, that they have correct asymptotic Type I error if both nuisance parameters involved in the problem are estimated at fast enough rates, and that they have power to detect local alternatives approaching the null at the rate n − 1 / 2 . We study the performance of our tests in numerical studies, and use them to test for the presence of a causal effect of BMI on immune response in early phase vaccine trials. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ted Westling},
  doi          = {10.1080/01621459.2020.1865168},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1551-1562},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric tests of the causal null with nondiscrete exposures},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sensitivity analysis via the proportion of unmeasured
confounding. <em>JASA</em>, <em>117</em>(539), 1540–1550. (<a
href="https://doi.org/10.1080/01621459.2020.1864382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In observational studies, identification of ATEs is generally achieved by assuming that the correct set of confounders has been measured and properly included in the relevant models. Because this assumption is both strong and untestable, a sensitivity analysis should be performed. Common approaches include modeling the bias directly or varying the propensity scores to probe the effects of a potential unmeasured confounder. In this article, we take a novel approach whereby the sensitivity parameter is the “proportion of unmeasured confounding”: the proportion of units for whom the treatment is not as good as randomized even after conditioning on the observed covariates. We consider different assumptions on the probability of a unit being unconfounded. In each case, we derive sharp bounds on the average treatment effect as a function of the sensitivity parameter and propose nonparametric estimators that allow flexible covariate adjustment. We also introduce a one-number summary of a study’s robustness to the number of confounded units. Finally, we explore finite-sample properties via simulation, and apply the methods to an observational database used to assess the effects of right heart catheterization. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Matteo Bonvini and Edward H. Kennedy},
  doi          = {10.1080/01621459.2020.1864382},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1540-1550},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Sensitivity analysis via the proportion of unmeasured confounding},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Survival regression models with dependent bayesian
nonparametric priors. <em>JASA</em>, <em>117</em>(539), 1530–1539. (<a
href="https://doi.org/10.1080/01621459.2020.1864381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel Bayesian nonparametric model for regression in survival analysis. Our model builds on the classical neutral to the right model of Doksum and on the Cox proportional hazards model of Kim and Lee. The use of a vector of dependent Bayesian nonparametric priors allows us to efficiently model the hazard as a function of covariates while allowing nonproportionality. The model can be seen as having competing latent risks. We characterize the posterior of the underlying dependent vector of completely random measures and study the asymptotic behavior of the model. We show how an MCMC scheme can provide Bayesian inference for posterior means and credible intervals. The method is illustrated using simulated and real data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Alan Riva-Palacio and Fabrizio Leisen and Jim Griffin},
  doi          = {10.1080/01621459.2020.1864381},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1530-1539},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Survival regression models with dependent bayesian nonparametric priors},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Covariate information number for feature screening in
ultrahigh-dimensional supervised problems. <em>JASA</em>,
<em>117</em>(539), 1516–1529. (<a
href="https://doi.org/10.1080/01621459.2020.1864380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary high-throughput experimental and surveying techniques give rise to ultrahigh-dimensional supervised problems with sparse signals; that is, a limited number of observations ( n ), each with a very large number of covariates ( p ≫ n ) ( p ≫ n ) (p≫n) , only a small share of which is truly associated with the response. In these settings, major concerns on computational burden, algorithmic stability, and statistical accuracy call for substantially reducing the feature space by eliminating redundant covariates before the use of any sophisticated statistical analysis. Along the lines of Pearson’s correlation coefficient-based sure independence screening and other model- and correlation-based feature screening methods, we propose a model-free procedure called covariate information number-sure independence screening (CIS). CIS uses a marginal utility connected to the notion of the traditional Fisher information, possesses the sure screening property, and is applicable to any type of response (features) with continuous features (response). Simulations and an application to transcriptomic data on rats reveal the comparative strengths of CIS over some popular feature screening methods. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Debmalya Nandy and Francesca Chiaromonte and Runze Li},
  doi          = {10.1080/01621459.2020.1864380},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1516-1529},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Covariate information number for feature screening in ultrahigh-dimensional supervised problems},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric multinomial logistic regression for
multivariate point pattern data. <em>JASA</em>, <em>117</em>(539),
1500–1515. (<a
href="https://doi.org/10.1080/01621459.2020.1863812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new method for analysis of multivariate point pattern data observed in a heterogeneous environment and with complex intensity functions. We suggest semiparametric models for the intensity functions that depend on an unspecified factor common to all types of points. This is for example well suited for analyzing spatial covariate effects on events such as street crime activities that occur in a complex urban environment. A multinomial conditional composite likelihood function is introduced for estimation of intensity function regression parameters and the asymptotic joint distribution of the resulting estimators is derived under mild conditions. Crucially, the asymptotic covariance matrix depends on ratios of cross pair correlation functions of the multivariate point process. To make valid statistical inference without restrictive assumptions, we construct consistent nonparametric estimators for these ratios. Finally, we construct standardized residual plots, predictive probability plots, and semiparametric intensity plots to validate and to visualize the findings of the model. The effectiveness of the proposed methodology is demonstrated through extensive simulation studies and an application to analyzing the effects of socio-economic and demographical variables on occurrences of street crimes in Washington DC. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Kristian Bjørn Hessellund and Ganggang Xu and Yongtao Guan and Rasmus Waagepetersen},
  doi          = {10.1080/01621459.2020.1863812},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1500-1515},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Semiparametric multinomial logistic regression for multivariate point pattern data},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical modeling for spatio-temporal data from
stochastic convection-diffusion processes. <em>JASA</em>,
<em>117</em>(539), 1482–1499. (<a
href="https://doi.org/10.1080/01621459.2020.1863223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a physical-statistical modeling approach for spatio-temporal data arising from a class of stochastic convection-diffusion processes. Such processes are widely found in scientific and engineering applications where fundamental physics imposes critical constraints on how data can be modeled and how models should be interpreted. The idea of spectrum decomposition is employed to approximate a physical spatio-temporal process by the linear combination of spatial basis functions and a multivariate random process of spectral coefficients. Unlike existing approaches assuming spatially and temporally invariant convection-diffusion, this article considers a more general scenario with spatially varying convection-diffusion and nonzero-mean source-sink. As a result, the temporal dynamics of spectral coefficients is coupled with each other, which can be interpreted as the nonlinear energy redistribution across multiple scales from the perspective of physics. Because of the spatially varying convection-diffusion, the space-time covariance is nonstationary in space. The theoretical results are integrated into a hierarchical dynamical spatio-temporal model. The connection is established between the proposed model and the existing models based on integro-difference equations. Computational efficiency and scalability are also investigated to make the proposed approach practical. The advantages of the proposed methodology are demonstrated by numerical examples, a case study, and comprehensive comparison studies. Computer code is available on GitHub. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xiao Liu and Kyongmin Yeo and Siyuan Lu},
  doi          = {10.1080/01621459.2020.1863223},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1482-1499},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical modeling for spatio-temporal data from stochastic convection-diffusion processes},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating malaria vaccine efficacy in the absence of a gold
standard case definition: Mendelian factorial design. <em>JASA</em>,
<em>117</em>(539), 1466–1481. (<a
href="https://doi.org/10.1080/01621459.2020.1863222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate estimates of malaria vaccine efficacy require a reliable definition of a malaria case. However, the symptoms of clinical malaria are unspecific, overlapping with other childhood illnesses. Additionally, children in endemic areas tolerate varying levels of parasitemia without symptoms. Together, this makes finding a gold-standard case definition challenging. We present a method to identify and estimate malaria vaccine efficacy that does not require an observable gold-standard case definition. Instead, we leverage genetic traits that are protective against malaria but not against other illnesses, for example, the sickle cell trait, to identify vaccine efficacy in a randomized trial. Inspired by Mendelian randomization, we introduce Mendelian factorial design, a method that augments a randomized trial with genetic variation to produce a natural factorial experiment, which identifies vaccine efficacy under realistic assumptions. A robust, covariance adjusted estimation procedure is developed for estimating vaccine efficacy on the risk ratio and incidence rate ratio scales. Simulations suggest that our estimator has good performance whereas standard methods are systematically biased. We demonstrate that a combined estimator using both our proposed estimator and the standard approach yields significant improvements when the Mendelian factor is only weakly protective. Our method can be applied in vaccine and prevention trials of other childhood diseases that have no gold-standard case definition and known genetic risk factors. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Raiden B. Hasegawa and Dylan S. Small},
  doi          = {10.1080/01621459.2020.1863222},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1466-1481},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimating malaria vaccine efficacy in the absence of a gold standard case definition: Mendelian factorial design},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Minimax efficient random experimental design strategies with
application to model-robust design for prediction. <em>JASA</em>,
<em>117</em>(539), 1452–1465. (<a
href="https://doi.org/10.1080/01621459.2020.1863221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In game theory and statistical decision theory, a random (i.e., mixed) decision strategy often outperforms a deterministic strategy in minimax expected loss. As experimental design can be viewed as a game pitting the Statistician against Nature, the use of a random strategy to choose a design will often be beneficial. However, the topic of minimax-efficient random strategies for design selection is mostly unexplored, with consideration limited to Fisherian randomization of the allocation of a predetermined set of treatments to experimental units. Here, for the first time, novel and more flexible random design strategies are shown to have better properties than their deterministic counterparts in linear model estimation and prediction, including stronger bounds on both the expectation and survivor function of the loss distribution. Design strategies are considered for three important statistical problems: (i) parameter estimation in linear potential outcomes models, (ii) point prediction from a correct linear model, and (iii) global prediction from a linear model taking into account an L 2 -class of possible model discrepancy functions. The new random design strategies proposed for (iii) give a finite bound on the expected loss, a dramatic improvement compared to existing deterministic exact designs for which the expected loss is unbounded. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Timothy W. Waite and David C. Woods},
  doi          = {10.1080/01621459.2020.1863221},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1452-1465},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Minimax efficient random experimental design strategies with application to model-robust design for prediction},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multicategory angle-based learning for estimating optimal
dynamic treatment regimes with censored data. <em>JASA</em>,
<em>117</em>(539), 1438–1451. (<a
href="https://doi.org/10.1080/01621459.2020.1862671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An optimal dynamic treatment regime (DTR) consists of a sequence of decision rules in maximizing long-term benefits, which is applicable for chronic diseases such as HIV infection or cancer. In this article, we develop a novel angle-based approach to search the optimal DTR under a multicategory treatment framework for survival data. The proposed method targets to maximize the conditional survival function of patients following a DTR. In contrast to most existing approaches which are designed to maximize the expected survival time under a binary treatment framework, the proposed method solves the multicategory treatment problem given multiple stages for censored data. Specifically, the proposed method obtains the optimal DTR via integrating estimations of decision rules at multiple stages into a single multicategory classification algorithm without imposing additional constraints, which is also more computationally efficient and robust. In theory, we establish Fisher consistency and provide the risk bound for the proposed estimator under regularity conditions. Our numerical studies show that the proposed method outperforms competing methods in terms of maximizing the conditional survival probability. We apply the proposed method to two real datasets: Framingham heart study data and acquired immunodeficiency syndrome clinical data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Fei Xue and Yanqing Zhang and Wenzhuo Zhou and Haoda Fu and Annie Qu},
  doi          = {10.1080/01621459.2020.1862671},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1438-1451},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Multicategory angle-based learning for estimating optimal dynamic treatment regimes with censored data},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal design of experiments for implicit models.
<em>JASA</em>, <em>117</em>(539), 1424–1437. (<a
href="https://doi.org/10.1080/01621459.2020.1862670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explicit models representing the response variables as functions of the control variables are standard in virtually all scientific fields. For these models, there is a vast literature on the optimal design of experiments (ODoE) to provide good estimates of the parameters with the use of minimal resources. Contrarily, the ODoE for implicit models is more complex and has not been systematically addressed. Nevertheless, there are practical examples where the models relating the response variables, the parameters and the factors are implicit or hardly convertible into an explicit form. We propose a general formulation for developing the theory of the ODoE for implicit algebraic models to specifically find continuous local designs. The treatment relies on converting the ODoE problem into an optimization problem of the nonlinear programming (NLP) class which includes the construction of the parameter sensitivities and the Cholesky decomposition of the Fisher information matrix. The NLP problem generated has multiple local optima, and we use global solvers, combined with an equivalence theorem from the theory of ODoE, to ensure the global optimality of our continuous optimal designs. We consider D- and A-optimality criteria and apply the approach to five examples of practical interest in chemistry and thermodynamics. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Belmiro P. M. Duarte and Anthony C. Atkinson and José F. O. Granjo and Nuno M. C. Oliveira},
  doi          = {10.1080/01621459.2020.1862670},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1424-1437},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal design of experiments for implicit models},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric inference for nonmonotone
missing-not-at-random data: The no self-censoring model. <em>JASA</em>,
<em>117</em>(539), 1415–1423. (<a
href="https://doi.org/10.1080/01621459.2020.1862669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the identification and estimation of statistical functionals of multivariate data missing nonmonotonically and not-at-random, taking a semiparametric approach. Specifically, we assume that the missingness mechanism satisfies what has been previously called “no self-censoring” or “itemwise conditionally independent nonresponse,” which roughly corresponds to the assumption that no partially observed variable directly determines its own missingness status. We show that this assumption, combined with an odds ratio parameterization of the joint density, enables identification of functionals of interest, and we establish the semiparametric efficiency bound for the nonparametric model satisfying this assumption. We propose a practical augmented inverse probability weighted estimator, and in the setting with a (possibly high-dimensional) always-observed subset of covariates, our proposed estimator enjoys a certain double-robustness property. We explore the performance of our estimator with simulation experiments and on a previously studied dataset of HIV-positive mothers in Botswana. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Daniel Malinsky and Ilya Shpitser and Eric J. Tchetgen Tchetgen},
  doi          = {10.1080/01621459.2020.1862669},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1415-1423},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Semiparametric inference for nonmonotone missing-not-at-random data: The no self-censoring model},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling high-dimensional time series: A factor model with
dynamically dependent factors and diverging eigenvalues. <em>JASA</em>,
<em>117</em>(539), 1398–1414. (<a
href="https://doi.org/10.1080/01621459.2020.1862668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new approach to modeling high-dimensional time series by treating a p -dimensional time series as a nonsingular linear transformation of certain common factors and idiosyncratic components. Unlike the approximate factor models, we assume that the factors capture all the nontrivial dynamics of the data, but the cross-sectional dependence may be explained by both the factors and the idiosyncratic components. Under the proposed model, (a) the factor process is dynamically dependent and the idiosyncratic component is a white noise process, and (b) the largest eigenvalues of the covariance matrix of the idiosyncratic components may diverge to infinity as the dimension p increases. We propose a white noise testing procedure for high-dimensional time series to determine the number of white noise components and, hence, the number of common factors, and introduce a projected principal component analysis (PCA) to eliminate the diverging effect of the idiosyncratic noises. Asymptotic properties of the proposed method are established for both fixed p and diverging p as the sample size n increases to infinity. We use both simulated data and real examples to assess the performance of the proposed method. We also compare our method with two commonly used methods in the literature concerning the forecastability of the extracted factors and find that the proposed approach not only provides interpretable results, but also performs well in out-of-sample forecasting. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zhaoxing Gao and Ruey S. Tsay},
  doi          = {10.1080/01621459.2020.1862668},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1398-1414},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Modeling high-dimensional time series: A factor model with dynamically dependent factors and diverging eigenvalues},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiscale quantile segmentation. <em>JASA</em>,
<em>117</em>(539), 1384–1397. (<a
href="https://doi.org/10.1080/01621459.2020.1859380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new methodology for analyzing serial data by quantile regression assuming that the underlying quantile function consists of constant segments. The procedure does not rely on any distributional assumption besides serial independence. It is based on a multiscale statistic, which allows to control the (finite sample) probability for selecting the correct number of segments S at a given error level, which serves as a tuning parameter. For a proper choice of this parameter, this probability tends exponentially fast to one, as sample size increases. We further show that the location and size of segments are estimated at minimax optimal rate (compared to a Gaussian setting) up to a log-factor. Thereby, our approach leads to (asymptotically) uniform confidence bands for the entire quantile regression function in a fully nonparametric setup. The procedure is efficiently implemented using dynamic programming techniques with double heap structures, and software is provided. Simulations and data examples from genetic sequencing and ion channel recordings confirm the robustness of the proposed procedure, which at the same time reliably detects changes in quantiles from arbitrary distributions with precise statistical guarantees. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Laura Jula Vanegas and Merle Behr and Axel Munk},
  doi          = {10.1080/01621459.2020.1859380},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1384-1397},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Multiscale quantile segmentation},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LAWS: A locally adaptive weighting and screening approach to
spatial multiple testing. <em>JASA</em>, <em>117</em>(539), 1370–1383.
(<a href="https://doi.org/10.1080/01621459.2020.1859379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploiting spatial patterns in large-scale multiple testing promises to improve both power and interpretability of false discovery rate (FDR) analyses. This article develops a new class of locally adaptive weighting and screening (LAWS) rules that directly incorporates useful local patterns into inference. The idea involves constructing robust and structure-adaptive weights according to the estimated local sparsity levels. LAWS provides a unified framework for a broad range of spatial problems and is fully data-driven. It is shown that LAWS controls the FDR asymptotically under mild conditions on dependence. The finite sample performance is investigated using simulated data, which demonstrates that LAWS controls the FDR and outperforms existing methods in power. The efficiency gain is substantial in many settings. We further illustrate the merits of LAWS through applications to the analysis of two-dimensional and three-dimensional images. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {T. Tony Cai and Wenguang Sun and Yin Xia},
  doi          = {10.1080/01621459.2020.1859379},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1370-1383},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {LAWS: A locally adaptive weighting and screening approach to spatial multiple testing},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical transformed scale mixtures for flexible
modeling of spatial extremes on datasets with many locations.
<em>JASA</em>, <em>117</em>(539), 1357–1369. (<a
href="https://doi.org/10.1080/01621459.2020.1858838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– Flexible spatial models that allow transitions between tail dependence classes have recently appeared in the literature. However, inference for these models is computationally prohibitive, even in moderate dimensions, due to the necessity of repeatedly evaluating the multivariate Gaussian distribution function. In this work, we attempt to achieve truly high-dimensional inference for extremes of spatial processes, while retaining the desirable flexibility in the tail dependence structure, by modifying an established class of models based on scale mixtures Gaussian processes. We show that the desired extremal dependence properties from the original models are preserved under the modification, and demonstrate that the corresponding Bayesian hierarchical model does not involve the expensive computation of the multivariate Gaussian distribution function. We fit our model to exceedances of a high threshold, and perform coverage analyses and cross-model checks to validate its ability to capture different types of tail characteristics. We use a standard adaptive Metropolis algorithm for model fitting, and further accelerate the computation via parallelization and Rcpp. Lastly, we apply the model to a dataset of a fire threat index on the Great Plains region of the United States, which is vulnerable to massively destructive wildfires. We find that the joint tail of the fire threat index exhibits a decaying dependence structure that cannot be captured by limiting extreme value models. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Likun Zhang and Benjamin A. Shaby and Jennifer L. Wadsworth},
  doi          = {10.1080/01621459.2020.1858838},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1357-1369},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Hierarchical transformed scale mixtures for flexible modeling of spatial extremes on datasets with many locations},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-dimensional vector autoregressive time series modeling
via tensor decomposition. <em>JASA</em>, <em>117</em>(539), 1338–1356.
(<a href="https://doi.org/10.1080/01621459.2020.1855183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classical vector autoregressive model is a fundamental tool for multivariate time series analysis. However, it involves too many parameters when the number of time series and lag order are even moderately large. This article proposes to rearrange the transition matrices of the model into a tensor form such that the parameter space can be restricted along three directions simultaneously via tensor decomposition. In contrast, the reduced-rank regression method can restrict the parameter space in only one direction. Besides achieving substantial dimension reduction, the proposed model is interpretable from the factor modeling perspective. Moreover, to handle high-dimensional time series, this article considers imposing sparsity on factor matrices to improve the model interpretability and estimation efficiency, which leads to a sparsity-inducing estimator. For the low-dimensional case, we derive asymptotic properties of the proposed least squares estimator and introduce an alternating least squares algorithm. For the high-dimensional case, we establish nonasymptotic properties of the sparsity-inducing estimator and propose an ADMM algorithm for regularized estimation. Simulation experiments and a real data example demonstrate the advantages of the proposed approach over various existing methods. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Di Wang and Yao Zheng and Heng Lian and Guodong Li},
  doi          = {10.1080/01621459.2020.1855183},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1338-1356},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {High-dimensional vector autoregressive time series modeling via tensor decomposition},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mehler’s formula, branching process, and compositional
kernels of deep neural networks. <em>JASA</em>, <em>117</em>(539),
1324–1337. (<a
href="https://doi.org/10.1080/01621459.2020.1853547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– We use a connection between compositional kernels and branching processes via Mehler’s formula to study deep neural networks. This new probabilistic insight provides us a novel perspective on the mathematical role of activation functions in compositional neural networks. We study the unscaled and rescaled limits of the compositional kernels and explore the different phases of the limiting behavior, as the compositional depth increases. We investigate the memorization capacity of the compositional kernels and neural networks by characterizing the interplay among compositional depth, sample size, dimensionality, and nonlinearity of the activation. Explicit formulas on the eigenvalues of the compositional kernel are provided, which quantify the complexity of the corresponding reproducing kernel Hilbert space. On the methodological front, we propose a new random features algorithm, which compresses the compositional layers by devising a new activation function. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Tengyuan Liang and Hai Tran-Bach},
  doi          = {10.1080/01621459.2020.1853547},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1324-1337},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Mehler’s formula, branching process, and compositional kernels of deep neural networks},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Functional sequential treatment allocation. <em>JASA</em>,
<em>117</em>(539), 1311–1323. (<a
href="https://doi.org/10.1080/01621459.2020.1851236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a setting in which a policy maker assigns subjects to treatments, observing each outcome before the next subject arrives. Initially, it is unknown which treatment is best, but the sequential nature of the problem permits learning about the effectiveness of the treatments. While the multi-armed-bandit literature has shed much light on the situation when the policy maker compares the effectiveness of the treatments through their mean, much less is known about other targets. This is restrictive, because a cautious decision maker may prefer to target a robust location measure such as a quantile or a trimmed mean. Furthermore, socio-economic decision making often requires targeting purpose specific characteristics of the outcome distribution, such as its inherent degree of inequality, welfare or poverty. In the present article, we introduce and study sequential learning algorithms when the distributional characteristic of interest is a general functional of the outcome distribution. Minimax expected regret optimality results are obtained within the subclass of explore-then-commit policies, and for the unrestricted class of all policies. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Anders Bredahl Kock and David Preinerstorfer and Bezirgen Veliyev},
  doi          = {10.1080/01621459.2020.1851236},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1311-1323},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Functional sequential treatment allocation},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting the number of future events. <em>JASA</em>,
<em>117</em>(539), 1296–1310. (<a
href="https://doi.org/10.1080/01621459.2020.1850461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article describes prediction methods for the number of future events from a population of units associated with an on-going time-to-event process. Examples include the prediction of warranty returns and the prediction of the number of future product failures that could cause serious threats to property or life. Important decisions such as whether a product recall should be mandated are often based on such predictions. Data, generally right-censored (and sometimes left truncated and right-censored), are used to estimate the parameters of a time-to-event distribution. This distribution can then be used to predict the number of events over future periods of time. Such predictions are sometimes called within-sample predictions and differ from other prediction problems considered in most of the prediction literature. This article shows that the plug-in (also known as estimative or naive) prediction method is not asymptotically correct (i.e., for large amounts of data, the coverage probability always fails to converge to the nominal confidence level). However, a commonly used prediction calibration method is shown to be asymptotically correct for within-sample predictions, and two alternative predictive-distribution-based methods that perform better than the calibration method are presented and justified. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Qinglong Tian and Fanqi Meng and Daniel J. Nordman and William Q. Meeker},
  doi          = {10.1080/01621459.2020.1850461},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1296-1310},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Predicting the number of future events},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gaining outlier resistance with progressive quantiles: Fast
algorithms and theoretical studies. <em>JASA</em>, <em>117</em>(539),
1282–1295. (<a
href="https://doi.org/10.1080/01621459.2020.1850460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outliers widely occur in big-data applications and may severely affect statistical estimation and inference. In this article, a framework of outlier-resistant estimation is introduced to robustify an arbitrarily given loss function. It has a close connection to the method of trimming and includes explicit outlyingness parameters for all samples, which in turn facilitates computation, theory, and parameter tuning. To tackle the issues of nonconvexity and nonsmoothness, we develop scalable algorithms with implementation ease and guaranteed fast convergence. In particular, a new technique is proposed to alleviate the requirement on the starting point such that on regular datasets, the number of data resamplings can be substantially reduced. Based on combined statistical and computational treatments, we are able to perform nonasymptotic analysis beyond M -estimation. The obtained resistant estimators, though not necessarily globally or even locally optimal, enjoy minimax rate optimality in both low dimensions and high dimensions. Experiments in regression, classification, and neural networks show excellent performance of the proposed methodology at the occurrence of gross outliers. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yiyuan She and Zhifeng Wang and Jiahui Shen},
  doi          = {10.1080/01621459.2020.1850460},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1282-1295},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Gaining outlier resistance with progressive quantiles: Fast algorithms and theoretical studies},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variational bayes for high-dimensional linear regression
with sparse priors. <em>JASA</em>, <em>117</em>(539), 1270–1281. (<a
href="https://doi.org/10.1080/01621459.2020.1847121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a mean-field spike and slab variational Bayes (VB) approximation to Bayesian model selection priors in sparse high-dimensional linear regression. Under compatibility conditions on the design matrix, oracle inequalities are derived for the mean-field VB approximation, implying that it converges to the sparse truth at the optimal rate and gives optimal prediction of the response vector. The empirical performance of our algorithm is studied, showing that it works comparably well as other state-of-the-art Bayesian variable selection methods. We also numerically demonstrate that the widely used coordinate-ascent variational inference algorithm can be highly sensitive to the parameter updating order, leading to potentially poor performance. To mitigate this, we propose a novel prioritized updating scheme that uses a data-driven updating order and performs better in simulations. The variational algorithm is implemented in the R package sparsevb. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Kolyan Ray and Botond Szabó},
  doi          = {10.1080/01621459.2020.1847121},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1270-1281},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Variational bayes for high-dimensional linear regression with sparse priors},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Smaller p-values via indirect information. <em>JASA</em>,
<em>117</em>(539), 1254–1269. (<a
href="https://doi.org/10.1080/01621459.2020.1844720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops p -values for evaluating means of normal populations that make use of indirect or prior information. A p -value of this type is based on a biased frequentist hypothesis test that has optimal average power with respect to a probability distribution that encodes indirect information about the mean parameter, resulting in a smaller p -value if the indirect information is accurate. In a variety of multiparameter settings, we show how to adaptively estimate the indirect information for each mean parameter while still maintaining uniformity of the p -values under their null hypotheses. This is done using a linking model through which indirect information about the mean of one population may be obtained from the data of other populations. Importantly, the linking model does not need to be correct to maintain the uniformity of the p -values under their null hypotheses. This methodology is illustrated in several data analysis scenarios, including small area inference, spatially arranged populations, interactions in linear regression, and generalized linear models. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Peter Hoff},
  doi          = {10.1080/01621459.2020.1844720},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1254-1269},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Smaller p-values via indirect information},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coupled generation. <em>JASA</em>, <em>117</em>(539),
1243–1253. (<a
href="https://doi.org/10.1080/01621459.2020.1844719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance generation creates representative examples to interpret a learning model, as in regression and classification. For example, representative sentences of a topic of interest describe the topic specifically for sentence categorization. In such a situation, a large number of unlabeled observations may be available in addition to labeled data, for example, many unclassified text corpora (unlabeled instances) are available with only a few classified sentences (labeled instances). In this article, we introduce a novel generative method, called a coupled generator, producing instances given a specific learning outcome, based on indirect and direct generators. The indirect generator uses the inverse principle to yield the corresponding inverse probability, enabling to generate instances by leveraging an unlabeled data. The direct generator learns the distribution of an instance given its learning outcome. Then, the coupled generator seeks the best one from the indirect and direct generators, which is designed to enjoy the benefits of both and deliver higher generation accuracy. For sentence generation given a topic, we develop an embedding-based regression/classification in conjuncture with an unconditional recurrent neural network for the indirect generator, whereas a conditional recurrent neural network is natural for the corresponding direct generator. Moreover, we derive finite-sample generation error bounds for the indirect and direct generators to reveal the generative aspects of both methods thus explaining the benefits of the coupled generator. Finally, we apply the proposed methods to a real benchmark of abstract classification and demonstrate that the coupled generator composes reasonably good sentences from a dictionary to describe a specific topic of interest. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ben Dai and Xiaotong Shen and Wing Wong},
  doi          = {10.1080/01621459.2020.1844719},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1243-1253},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Coupled generation},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regression analysis of asynchronous longitudinal functional
and scalar data. <em>JASA</em>, <em>117</em>(539), 1228–1242. (<a
href="https://doi.org/10.1080/01621459.2020.1844211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many modern large-scale longitudinal neuroimaging studies, such as the Alzheimer’s Disease Neuroimaging Initiative (ADNI) study, have collected/are collecting asynchronous scalar and functional variables that are measured at distinct time points. The analyses of temporally asynchronous functional and scalar variables pose major technical challenges to many existing statistical approaches. We propose a class of generalized functional partial-linear varying-coefficient models to appropriately deal with these challenges through introducing both scalar and functional coefficients of interest and using kernel weighting methods. We design penalized kernel-weighted estimating equations to estimate scalar and functional coefficients, in which we represent functional coefficients by using a rich truncated tensor product penalized B-spline basis. We establish the theoretical properties of scalar and functional coefficient estimators including consistency, convergence rate, prediction accuracy, and limiting distributions. We also propose a bootstrap method to test the nullity of both parametric and functional coefficients, while establishing the bootstrap consistency. Simulation studies and the analysis of the ADNI study are used to assess the finite sample performance of our proposed approach. Our real data analysis reveals significant relationship between fractional anisotropy density curves and cognitive function with education, baseline disease status and APOE4 gene as major contributing factors. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ting Li and Tengfei Li and Zhongyi Zhu and Hongtu Zhu},
  doi          = {10.1080/01621459.2020.1844211},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1228-1242},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Regression analysis of asynchronous longitudinal functional and scalar data},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Random partition models for microclustering tasks.
<em>JASA</em>, <em>117</em>(539), 1215–1227. (<a
href="https://doi.org/10.1080/01621459.2020.1841647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional Bayesian random partition models assume that the size of each cluster grows linearly with the number of data points. While this is appealing for some applications, this assumption is not appropriate for other tasks such as entity resolution (ER), modeling of sparse networks, and DNA sequencing tasks. Such applications require models that yield clusters whose sizes grow sublinearly with the total number of data points—the microclustering property . Motivated by these issues, we propose a general class of random partition models that satisfy the microclustering property with well-characterized theoretical properties. Our proposed models overcome major limitations in the existing literature on microclustering models, namely a lack of interpretability, identifiability, and full characterization of model asymptotic properties. Crucially, we drop the classical assumption of having an exchangeable sequence of data points, and instead assume an exchangeable sequence of clusters. In addition, our framework provides flexibility in terms of the prior distribution of cluster sizes, computational tractability, and applicability to a large number of microclustering tasks. We establish theoretical properties of the resulting class of priors, where we characterize the asymptotic behavior of the number of clusters and of the proportion of clusters of a given size. Our framework allows a simple and efficient Markov chain Monte Carlo algorithm to perform statistical inference. We illustrate our proposed methodology on the microclustering task of ER, where we provide a simulation study and real experiments on survey panel data.},
  archive      = {J_JASA},
  author       = {Brenda Betancourt and Giacomo Zanella and Rebecca C. Steorts},
  doi          = {10.1080/01621459.2020.1841647},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1215-1227},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Random partition models for microclustering tasks},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Markov neighborhood regression for high-dimensional
inference. <em>JASA</em>, <em>117</em>(539), 1200–1214. (<a
href="https://doi.org/10.1080/01621459.2020.1841646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an innovative method for constructing confidence intervals and assessing p -values in statistical inference for high-dimensional linear models. The proposed method has successfully broken the high-dimensional inference problem into a series of low-dimensional inference problems: For each regression coefficient β i , the confidence interval and p -value are computed by regressing on a subset of variables selected according to the conditional independence relations between the corresponding variable X i and other variables. Since the subset of variables forms a Markov neighborhood of X i in the Markov network formed by all the variables X 1 , X 2 , … , X p , the proposed method is coined as Markov neighborhood regression (MNR). The proposed method is tested on high-dimensional linear, logistic, and Cox regression. The numerical results indicate that the proposed method significantly outperforms the existing ones. Based on the MNR, a method of learning causal structures for high-dimensional linear models is proposed and applied to identification of drug sensitive genes and cancer driver genes. The idea of using conditional independence relations for dimension reduction is general and potentially can be extended to other high-dimensional or big data problems as well. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Faming Liang and Jingnan Xue and Bochao Jia},
  doi          = {10.1080/01621459.2020.1841646},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1200-1214},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Markov neighborhood regression for high-dimensional inference},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Rejoinder: Confidence intervals for nonparametric empirical
bayes analysis. <em>JASA</em>, <em>117</em>(539), 1192–1199. (<a
href="https://doi.org/10.1080/01621459.2022.2093729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Nikolaos Ignatiadis and Stefan Wager},
  doi          = {10.1080/01621459.2022.2093729},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1192-1199},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rejoinder: Confidence intervals for nonparametric empirical bayes analysis},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discussion of “confidence intervals for nonparametric
empirical bayes analysis.” <em>JASA</em>, <em>117</em>(539), 1186–1191.
(<a href="https://doi.org/10.1080/01621459.2022.2093727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Dongyue Xie and Matthew Stephens},
  doi          = {10.1080/01621459.2022.2093727},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1186-1191},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of “Confidence intervals for nonparametric empirical bayes analysis”},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discussion of “confidence intervals for nonparametric
empirical bayes analysis.” <em>JASA</em>, <em>117</em>(539), 1183–1185.
(<a href="https://doi.org/10.1080/01621459.2022.2096039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Marianna Pensky},
  doi          = {10.1080/01621459.2022.2096039},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1183-1185},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of “Confidence intervals for nonparametric empirical bayes analysis”},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comment on: “Confidence intervals for nonparametric
empirical bayes analysis” by ignatiadis and wager. <em>JASA</em>,
<em>117</em>(539), 1181–1182. (<a
href="https://doi.org/10.1080/01621459.2022.2102501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Guido Imbens},
  doi          = {10.1080/01621459.2022.2102501},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1181-1182},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comment on: “Confidence intervals for nonparametric empirical bayes analysis” by ignatiadis and wager},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discussion of “confidence intervals for nonparametric
empirical bayes analysis” by nikolaos ignatiadis and stefan wager.
<em>JASA</em>, <em>117</em>(539), 1179–1180. (<a
href="https://doi.org/10.1080/01621459.2022.2093725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Bradley Efron},
  doi          = {10.1080/01621459.2022.2093725},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1179-1180},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of “Confidence intervals for nonparametric empirical bayes analysis” by nikolaos ignatiadis and stefan wager},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Coverage properties of empirical bayes intervals: A
discussion of “confidence intervals for nonparametric empirical bayes
analysis” by ignatiadis and wager. <em>JASA</em>, <em>117</em>(539),
1175–1178. (<a
href="https://doi.org/10.1080/01621459.2022.2093728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Peter Hoff},
  doi          = {10.1080/01621459.2022.2093728},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1175-1178},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Coverage properties of empirical bayes intervals: A discussion of “Confidence intervals for nonparametric empirical bayes analysis” by ignatiadis and wager},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discussion of “confidence intervals for nonparametric
empirical bayes analysis” by ignatiadis and wager. <em>JASA</em>,
<em>117</em>(539), 1171–1174. (<a
href="https://doi.org/10.1080/01621459.2022.2093726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Subhashis Ghosal},
  doi          = {10.1080/01621459.2022.2093726},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1171-1174},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of “Confidence intervals for nonparametric empirical bayes analysis” by ignatiadis and wager},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonparametric empirical bayes prediction. <em>JASA</em>,
<em>117</em>(539), 1167–1170. (<a
href="https://doi.org/10.1080/01621459.2022.2096040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Noel Cressie},
  doi          = {10.1080/01621459.2022.2096040},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1167-1170},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric empirical bayes prediction},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Confidence intervals for nonparametric empirical bayes
analysis. <em>JASA</em>, <em>117</em>(539), 1149–1166. (<a
href="https://doi.org/10.1080/01621459.2021.2008403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an empirical Bayes analysis, we use data from repeated sampling to imitate inferences made by an oracle Bayesian with extensive knowledge of the data-generating distribution. Existing results provide a comprehensive characterization of when and why empirical Bayes point estimates accurately recover oracle Bayes behavior. In this paper, we develop flexible and practical confidence intervals that provide asymptotic frequentist coverage of empirical Bayes estimands, such as the posterior mean or the local false sign rate. The coverage statements hold even when the estimands are only partially identified or when empirical Bayes point estimates converge very slowly. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Nikolaos Ignatiadis and Stefan Wager},
  doi          = {10.1080/01621459.2021.2008403},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1149-1166},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Confidence intervals for nonparametric empirical bayes analysis},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multimodal multilevel neuroimaging model for investigating
brain connectome development. <em>JASA</em>, <em>117</em>(539),
1134–1148. (<a
href="https://doi.org/10.1080/01621459.2022.2055559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements of multimodal neuroimaging such as functional MRI (fMRI) and diffusion MRI (dMRI) offers unprecedented opportunities to understand brain development. Most existing neurodevelopmental studies focus on using a single imaging modality to study microstructure or neural activations in localized brain regions. The developmental changes of brain network architecture in childhood and adolescence are not well understood. Our study made use of dMRI and resting-state fMRI imaging data sets from Philadelphia Neurodevelopmental Cohort (PNC) study to characterize developmental changes in both structural as well as functional brain connectomes. A multimodal multilevel model (MMM) is developed and implemented in PNC study to investigate brain maturation in both white matter structural connection and intrinsic functional connection. MMM addresses several major challenges in multimodal connectivity analysis. First, by using a first-level data generative model for observed measures and a second-level latent network modeling, MMM effectively infers underlying connection states from noisy imaging-based connectivity measurements. Second, MMM models the interplay between the structural and functional connections to capture the relationship between different brain connectomes. Third, MMM incorporates covariate effects in the network modeling to investigate network heterogeneity across subpopoulations. Finally, by using a module-wise parameterization based on brain network topology, MMM is scalable to whole-brain connectomics. MMM analysis of the PNC study generates new insights in neurodevelopment during adolescence including revealing the majority of the white fiber connectivity growth are related to the cognitive networks where the most significant increase is found between the default mode and the executive control network with a 15\% increase in the probability of structural connections. We also uncover functional connectome development mainly derived from global functional integration rather than direct anatomical connections. To the best of our knowledge, these findings have not been reported in the literature using multimodal connectomics. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Yingtian Hu and Mahmoud Zeydabadinezhad and Longchuan Li and Ying Guo},
  doi          = {10.1080/01621459.2022.2055559},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1134-1148},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A multimodal multilevel neuroimaging model for investigating brain connectome development},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian inferences on neural activity in EEG-based
brain-computer interface. <em>JASA</em>, <em>117</em>(539), 1122–1133.
(<a href="https://doi.org/10.1080/01621459.2022.2041422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A brain-computer interface (BCI) is a system that translates brain activity into commands to operate technology. A common design for an electroencephalogram (EEG) BCI relies on the classification of the P300 event-related potential (ERP), which is a response elicited by the rare occurrence of target stimuli among common nontarget stimuli. Few existing ERP classifiers directly explore the underlying mechanism of the neural activity. To this end, we perform a novel Bayesian analysis of the probability distribution of multi-channel real EEG signals under the P300 ERP-BCI design. We aim to identify relevant spatial temporal differences of the neural activity, which provides statistical evidence of P300 ERP responses and helps design individually efficient and accurate BCIs. As one key finding of our single participant analysis, there is a 90\% posterior probability that the target ERPs of the channels around visual cortex reach their negative peaks around 200 milliseconds poststimulus. Our analysis identifies five important channels (PO7, PO8, Oz, P4, Cz) for the BCI speller leading to a 100\% prediction accuracy. From the analyses of nine other participants, we consistently select the identified five channels, and the selection frequencies are robust to small variations of bandpass filters and kernel hyper parameters. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Tianwen Ma and Yang Li and Jane E. Huggins and Ji Zhu and Jian Kang},
  doi          = {10.1080/01621459.2022.2041422},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1122-1133},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian inferences on neural activity in EEG-based brain-computer interface},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-dimensional mediation analysis for selecting DNA
methylation loci mediating childhood trauma and cortisol stress
reactivity. <em>JASA</em>, <em>117</em>(539), 1110–1121. (<a
href="https://doi.org/10.1080/01621459.2022.2053136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Childhood trauma tends to influence cortisol stress reactivity through the mediating effects of DNA methylation. Houtepen et al. conducted a study to investigate the role of DNA methylation in cortisol stress reactivity and its relationship with childhood trauma. The study collected a dataset consisting of 385,882 DNA methylation loci, cortisol stress reactivity, one-dimensional score on a childhood trauma questionnaire and several covariates for 85 healthy individuals. Of great scientific interest is to identify the active mediating loci out of the 385,882 ones. Houtepen et al. conducted 385,882 linear mediation analyses, in each of which one locus was considered, and identified three active mediating loci. More recently, van Kesteren and Oberski proposed a coordinate-wise mediation filter (CMF) and applied it to the same dataset. They identified five active mediating loci. Unfortunately, the three loci identified by Houtepen et al. are completely different from the five loci identified by van Kesteren and Oberski, probably because both Houtepen et al. and van Kesteren and Oberski did not consider all loci jointly in their analyses. The high dimensional DNA methylation loci indeed necessitate new techniques for identifying active mediating loci and testing the direct and indirect effects of the early life traumatic stress on later cortisol alteration. Motivated by the contradictory results in the aforementioned two scientific works, we develop a new estimating and testing procedure, and apply it to the same dataset as that analyzed by the two works. We identify three new loci: cg19230917, cg06422529 and cg03199124, and their effect sizes and p -values are 321.196 ( p -value = 0.035965), 418.173 ( p -value = 0.000234) and 471.865 ( p -value = 0.001691), respectively. These three loci possess both reasonably neurobiological interpretations and statistically significant effects via our proposed tests. Based on our new procedure, we further confirm that the childhood trauma does not have significant direct effects on cortisol change—it only indirectly affects cortisol through DNA methylation, and the indirect effect is negative. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xu Guo and Runze Li and Jingyuan Liu and Mudong Zeng},
  doi          = {10.1080/01621459.2022.2053136},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1110-1121},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {High-dimensional mediation analysis for selecting DNA methylation loci mediating childhood trauma and cortisol stress reactivity},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Copula regression for compound distributions with endogenous
covariates with applications in insurance deductible pricing.
<em>JASA</em>, <em>117</em>(539), 1094–1109. (<a
href="https://doi.org/10.1080/01621459.2022.2040519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concerns deductible pricing in nonlife insurance contracts. The primary interest of insurers is the effect of the contract deductible on a policyholder’s aggregate loss that is determined by a compound distribution where the sum of individual claim amount is stopped by the number of claims. Policyholders choose the deductible level based on their hidden risks, which makes deductible endogenous in the regressions for both claim frequency and claim severity. To address the endogeneity in the regression for the compound aggregate loss, we introduce a novel approach using pair copula constructions to jointly model the policyholder’s deductible, number of claims, and individual claim amounts, in the context of compound distributions. The proposed method provides insurers an empirical tool to uncover the underlying risk distribution of the potential customers. In the application we consider an insurance portfolio from the property insurance program that provides property coverage for building and contents of local government entities of the Wisconsin. Using the historical data on policyholder and insurance claims, we first provide empirical evidence of the endogeneity of the deductible. Interestingly, we find that the policyholder’s deductible is negatively associated with the claim frequency but positively associated with the claim severity. For the portfolio of policyholders, the endogenous deductible model provides superior prediction for 65\% and 71\% of policyholders for claim frequency and severity, respectively. The endogeneity of deductible shows significant managerial implications on insurance operations. In particular, the risk score suggested by the proposed method allows the insurer to identify additional profitable underwriting strategies which are quantified by the Gini indices of 0.22 and 0.13 when switching from the exogenous deductible premium and the insurer’s contract premium, respectively. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Peng Shi and Gee Y. Lee},
  doi          = {10.1080/01621459.2022.2040519},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1094-1109},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Copula regression for compound distributions with endogenous covariates with applications in insurance deductible pricing},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A mechanistic model of annual sulfate concentrations in the
united states. <em>JASA</em>, <em>117</em>(539), 1082–1093. (<a
href="https://doi.org/10.1080/01621459.2022.2027774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding how individual pollution sources contribute to ambient sulfate pollution is critical for assessing past and future air quality regulations. Since attribution to specific sources is typically not encoded in spatial air pollution data, we develop a mechanistic model which we use to estimate, with uncertainty, the contribution of ambient sulfate concentrations attributable specifically to sulfur dioxide (SO 2 ) emissions from individual coal-fired power plants in the central United States. We propose a multivariate Ornstein–Uhlenbeck (OU) process approximation to the dynamics of the underlying space-time chemical transport process, and its distributional properties are leveraged to specify novel probability models for spatial data that are viewed as either a snapshot or time-averaged observation of the OU process. Using US EPA SO 2 emissions data from 193 power plants and state-of-the-art estimates of ground-level annual mean sulfate concentrations, we estimate that in 2011—a time of active power plant regulatory action—existing flue-gas desulfurization (FGD) technologies at 66 power plants reduced population-weighted exposure to ambient sulfate by 1.97 μ g/m 3 (95\% CI: 1.80–2.15). Furthermore, we anticipate future regulatory benefits by estimating that installing FGD technologies at the five largest SO 2 -emitting facilities would reduce human exposure to ambient sulfate by an additional 0.45 μ g/m 3 (95\% CI: 0.33–0.54). Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Nathan B. Wikle and Ephraim M. Hanks and Lucas R. F. Henneman and Corwin M. Zigler},
  doi          = {10.1080/01621459.2022.2027774},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1082-1093},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A mechanistic model of annual sulfate concentrations in the united states},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic stochastic blockmodel regression for network data:
Application to international militarized conflicts. <em>JASA</em>,
<em>117</em>(539), 1068–1081. (<a
href="https://doi.org/10.1080/01621459.2021.2024436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The decision to engage in military conflict is shaped by many factors, including state- and dyad-level characteristics as well as the state’s membership in geopolitical coalitions. Supporters of the democratic peace theory, for example, hypothesize that the community of democratic states is less likely to wage war with each other. Such theories explain the ways in which nodal and dyadic characteristics affect the evolution of conflict patterns over time via their effects on group memberships. To test these arguments, we develop a dynamic model of network data by combining a hidden Markov model with a mixed-membership stochastic blockmodel that identifies latent groups underlying the network structure. Unlike existing models, we incorporate covariates that predict dynamic node memberships in latent groups as well as the direct formation of edges between dyads. While prior substantive research often assumes the decision to engage in international militarized conflict is independent across states and static over time, we demonstrate that conflict is driven by states’ evolving membership in geopolitical blocs. Our analysis of militarized disputes from 1816 to 2010 identifies two distinct blocs of democratic states, only one of which exhibits unusually low rates of conflict. Changes in monadic covariates like democracy shift states between coalitions, making some states more pacific but others more belligerent. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Santiago Olivella and Tyler Pratt and Kosuke Imai},
  doi          = {10.1080/01621459.2021.2024436},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1068-1081},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Dynamic stochastic blockmodel regression for network data: Application to international militarized conflicts},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Rejoinder. <em>JASA</em>, <em>117</em>(539), 1066–1067. (<a
href="https://doi.org/10.1080/01621459.2022.2101797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Yang Zhou and Lirong Xue and Zhengyu Shi and Libo Wu and Jianqing Fan},
  doi          = {10.1080/01621459.2022.2101797},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1066-1067},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rejoinder},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discussion of “measuring housing vitality from multi-source
big data and machine learning.” <em>JASA</em>, <em>117</em>(539),
1063–1065. (<a
href="https://doi.org/10.1080/01621459.2022.2098135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Sudipto Banerjee},
  doi          = {10.1080/01621459.2022.2098135},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1063-1065},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of “Measuring housing vitality from multi-source big data and machine learning”},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comments on “measuring housing vitality from multi-source
big data and machine learning.” <em>JASA</em>, <em>117</em>(539),
1060–1062. (<a
href="https://doi.org/10.1080/01621459.2022.2097086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Wei Tu and Bei Jiang and Linglong Kong},
  doi          = {10.1080/01621459.2022.2097086},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1060-1062},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comments on “Measuring housing vitality from multi-source big data and machine learning”},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Measuring housing vitality from multi-source big data and
machine learning. <em>JASA</em>, <em>117</em>(539), 1045–1059. (<a
href="https://doi.org/10.1080/01621459.2022.2096038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring timely high-resolution socioeconomic outcomes is critical for policymaking and evaluation, but hard to reliably obtain. With the help of machine learning and cheaply available data such as social media and nightlight, it is now possible to predict such indices in fine granularity. This article demonstrates an adaptive way to measure the time trend and spatial distribution of housing vitality (number of occupied houses) with the help of multiple easily accessible datasets: energy, nightlight, and land-use data. We first identified the high-frequency housing occupancy status from energy consumption data and then matched it with the monthly nightlight data. We then introduced the Factor-Augmented Regularized Model for prediction (FarmPredict) to deal with the dependence and collinearity issue among predictors by effectively lifting the prediction space, which is suitable to most machine learning algorithms. The heterogeneity issue in big data analysis is mitigated through the land-use data. FarmPredict allows us to extend the regional results to the city level, with a 76\% out-of-sample explanation of the spatial and timeliness variation in the house usage. Since energy is indispensable for life, our method is highly transferable with the only requirement of publicly accessible data. Our article provides an alternative approach with statistical machine learning to predict socioeconomic outcomes without the reliance on existing census and survey data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yang Zhou and Lirong Xue and Zhengyu Shi and Libo Wu and Jianqing Fan},
  doi          = {10.1080/01621459.2022.2096038},
  journal      = {Journal of the American Statistical Association},
  number       = {539},
  pages        = {1045-1059},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Measuring housing vitality from multi-source big data and machine learning},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correction. <em>JASA</em>, <em>117</em>(538), 1043. (<a
href="https://doi.org/10.1080/01621459.2022.2060607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  doi          = {10.1080/01621459.2022.2060607},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {1043},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Correction},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian thinking in biostatistics. <em>JASA</em>,
<em>117</em>(538), 1041–1042. (<a
href="https://doi.org/10.1080/01621459.2022.2069442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Yang Ni},
  doi          = {10.1080/01621459.2022.2069442},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {1041-1042},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian thinking in biostatistics.},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heteroscedasticity-adjusted ranking and thresholding for
large-scale multiple testing. <em>JASA</em>, <em>117</em>(538),
1028–1040. (<a
href="https://doi.org/10.1080/01621459.2020.1840992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standardization has been a widely adopted practice in multiple testing, for it takes into account the variability in sampling and makes the test statistics comparable across different study units. However, despite conventional wisdom to the contrary, we show that there can be a significant loss in information from basing hypothesis tests on standardized statistics rather than the full data. We develop a new class of heteroscedasticity-adjusted ranking and thresholding (HART) rules that aim to improve existing methods by simultaneously exploiting commonalities and adjusting heterogeneities among the study units. The main idea of HART is to bypass standardization by directly incorporating both the summary statistic and its variance into the testing procedure. A key message is that the variance structure of the alternative distribution, which is subsumed under standardized statistics, is highly informative and can be exploited to achieve higher power. The proposed HART procedure is shown to be asymptotically valid and optimal for false discovery rate (FDR) control. Our simulation results demonstrate that HART achieves substantial power gain over existing methods at the same FDR level. We illustrate the implementation through a microarray analysis of myeloma.},
  archive      = {J_JASA},
  author       = {Luella Fu and Bowen Gang and Gareth M. James and Wenguang Sun},
  doi          = {10.1080/01621459.2020.1840992},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {1028-1040},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Heteroscedasticity-adjusted ranking and thresholding for large-scale multiple testing},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonlinear spectral analysis: A local gaussian approach.
<em>JASA</em>, <em>117</em>(538), 1010–1027. (<a
href="https://doi.org/10.1080/01621459.2020.1840991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spectral distribution f ( ω ) f ( ω ) f(ω) of a stationary time series { Y t } t ∈ ℤ { Y t } t ∈ Z {Yt}t∈Z can be used to investigate whether or not periodic structures are present in { Y t } t ∈ ℤ { Y t } t ∈ Z {Yt}t∈Z , but f ( ω ) f ( ω ) f(ω) has some limitations due to its dependence on the autocovariances γ ( h ) γ ( h ) γ(h) . For example, f ( ω ) f ( ω ) f(ω) can not distinguish white iid noise from GARCH-type models (whose terms are dependent, but uncorrelated), which implies that f ( ω ) f ( ω ) f(ω) can be an inadequate tool when { Y t } t ∈ ℤ { Y t } t ∈ Z {Yt}t∈Z contains asymmetries and nonlinear dependencies. Asymmetries between the upper and lower tails of a time series can be investigated by means of the local Gaussian autocorrelations , and these local measures of dependence can be used to construct the local Gaussian spectral density presented in this paper. A key feature of the new local spectral density is that it coincides with f ( ω ) for Gaussian time series, which implies that it can be used to detect non-Gaussian traits in the time series under investigation. In particular, if f ( ω ) is flat, then peaks and troughs of the new local spectral density can indicate nonlinear traits, which potentially might discover local periodic phenomena that remain undetected in an ordinary spectral analysis.},
  archive      = {J_JASA},
  author       = {Lars Arne Jordanger and Dag Tjøstheim},
  doi          = {10.1080/01621459.2020.1840991},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {1010-1027},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonlinear spectral analysis: A local gaussian approach},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymptotic theory of eigenvectors for random matrices with
diverging spikes. <em>JASA</em>, <em>117</em>(538), 996–1009. (<a
href="https://doi.org/10.1080/01621459.2020.1840990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Characterizing the asymptotic distributions of eigenvectors for large random matrices poses important challenges yet can provide useful insights into a range of statistical applications. To this end, in this article we introduce a general framework of asymptotic theory of eigenvectors for large spiked random matrices with diverging spikes and heterogeneous variances, and establish the asymptotic properties of the spiked eigenvectors and eigenvalues for the scenario of the generalized Wigner matrix noise. Under some mild regularity conditions, we provide the asymptotic expansions for the spiked eigenvalues and show that they are asymptotically normal after some normalization. For the spiked eigenvectors, we establish asymptotic expansions for the general linear combination and further show that it is asymptotically normal after some normalization, where the weight vector can be arbitrary. We also provide a more general asymptotic theory for the spiked eigenvectors using the bilinear form. Simulation studies verify the validity of our new theoretical results. Our family of models encompasses many popularly used ones such as the stochastic block models with or without overlapping communities for network analysis and the topic models for text analysis, and our general theory can be exploited for statistical inference in these large-scale applications. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jianqing Fan and Yingying Fan and Xiao Han and Jinchi Lv},
  doi          = {10.1080/01621459.2020.1840990},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {996-1009},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Asymptotic theory of eigenvectors for random matrices with diverging spikes},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust post-matching inference. <em>JASA</em>,
<em>117</em>(538), 983–995. (<a
href="https://doi.org/10.1080/01621459.2020.1840383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nearest-neighbor matching is a popular nonparametric tool to create balance between treatment and control groups in observational studies. As a preprocessing step before regression, matching reduces the dependence on parametric modeling assumptions. In current empirical practice, however, the matching step is often ignored in the calculation of standard errors and confidence intervals. In this article, we show that ignoring the matching step results in asymptotically valid standard errors if matching is done without replacement and the regression model is correctly specified relative to the population regression function of the outcome variable on the treatment variable and all the covariates used for matching. However, standard errors that ignore the matching step are not valid if matching is conducted with replacement or, more crucially, if the second step regression model is misspecified in the sense indicated above. Moreover, correct specification of the regression model is not required for consistent estimation of treatment effects with matched data. We show that two easily implementable alternatives produce approximations to the distribution of the post-matching estimator that are robust to misspecification. A simulation study and an empirical example demonstrate the empirical relevance of our results. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Alberto Abadie and Jann Spiess},
  doi          = {10.1080/01621459.2020.1840383},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {983-995},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust post-matching inference},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Highly scalable bayesian geostatistical modeling via meshed
gaussian processes on partitioned domains. <em>JASA</em>,
<em>117</em>(538), 969–982. (<a
href="https://doi.org/10.1080/01621459.2020.1833889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a class of scalable Bayesian hierarchical models for the analysis of massive geostatistical datasets. The underlying idea combines ideas on high-dimensional geostatistics by partitioning the spatial domain and modeling the regions in the partition using a sparsity-inducing directed acyclic graph (DAG). We extend the model over the DAG to a well-defined spatial process, which we call the meshed Gaussian process (MGP). A major contribution is the development of an MGPs on tessellated domains, accompanied by a Gibbs sampler for the efficient recovery of spatial random effects. In particular, the cubic MGP (Q-MGP) can harness high-performance computing resources by executing all large-scale operations in parallel within the Gibbs sampler, improving mixing and computing time compared to sequential updating schemes. Unlike some existing models for large spatial data, a Q-MGP facilitates massive caching of expensive matrix operations, making it particularly apt in dealing with spatiotemporal remote-sensing data. We compare Q-MGPs with large synthetic and real world data against state-of-the-art methods. We also illustrate using Normalized Difference Vegetation Index data from the Serengeti park region to recover latent multivariate spatiotemporal random effects at millions of locations. The source code is available at github.com/mkln/meshgp. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Michele Peruzzi and Sudipto Banerjee and Andrew O. Finley},
  doi          = {10.1080/01621459.2020.1833889},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {969-982},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Highly scalable bayesian geostatistical modeling via meshed gaussian processes on partitioned domains},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical community detection by recursive partitioning.
<em>JASA</em>, <em>117</em>(538), 951–968. (<a
href="https://doi.org/10.1080/01621459.2020.1833888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of community detection in networks is usually formulated as finding a single partition of the network into some “correct” number of communities. We argue that it is more interpretable and in some regimes more accurate to construct a hierarchical tree of communities instead. This can be done with a simple top-down recursive partitioning algorithm, starting with a single community and separating the nodes into two communities by spectral clustering repeatedly, until a stopping rule suggests there are no further communities. This class of algorithms is model-free, computationally efficient, and requires no tuning other than selecting a stopping rule. We show that there are regimes where this approach outperforms K -way spectral clustering, and propose a natural framework for analyzing the algorithm’s theoretical performance, the binary tree stochastic block model. Under this model, we prove that the algorithm correctly recovers the entire community tree under relatively mild assumptions. We apply the algorithm to a gene network based on gene co-occurrence in 1580 research papers on anemia, and identify six clusters of genes in a meaningful hierarchy. We also illustrate the algorithm on a dataset of statistics papers. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Tianxi Li and Lihua Lei and Sharmodeep Bhattacharyya and Koen Van den Berge and Purnamrita Sarkar and Peter J. Bickel and Elizaveta Levina},
  doi          = {10.1080/01621459.2020.1833888},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {951-968},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Hierarchical community detection by recursive partitioning},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Causal bounds for outcome-dependent sampling in
observational studies. <em>JASA</em>, <em>117</em>(538), 939–950. (<a
href="https://doi.org/10.1080/01621459.2020.1832502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outcome-dependent sampling designs are common in many different scientific fields including epidemiology, ecology, and economics. As with all observational studies, such designs often suffer from unmeasured confounding, which generally precludes the nonparametric identification of causal effects. Nonparametric bounds can provide a way to narrow the range of possible values for a nonidentifiable causal effect without making additional untestable assumptions. The nonparametric bounds literature has almost exclusively focused on settings with random sampling, and the bounds have often been derived with a particular linear programming method. We derive novel bounds for the causal risk difference, often referred to as the average treatment effect, in six settings with outcome-dependent sampling and unmeasured confounding for a binary outcome and exposure. Our derivations of the bounds illustrate two approaches that may be applicable in other settings where the bounding problem cannot be directly stated as a system of linear constraints. We illustrate our derived bounds in a real data example involving the effect of vitamin D concentration on mortality. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Erin E. Gabriel and Michael C. Sachs and Arvid Sjölander},
  doi          = {10.1080/01621459.2020.1832502},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {939-950},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Causal bounds for outcome-dependent sampling in observational studies},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Center-outward r-estimation for semiparametric VARMA models.
<em>JASA</em>, <em>117</em>(538), 925–938. (<a
href="https://doi.org/10.1080/01621459.2020.1832501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new class of R-estimators for semiparametric VARMA models in which the innovation density plays the role of the nuisance parameter. Our estimators are based on the novel concepts of multivariate center-outward ranks and signs. We show that these concepts, combined with Le Cam’s asymptotic theory of statistical experiments, yield a class of semiparametric estimation procedures, which are efficient (at a given reference density), root- n consistent, and asymptotically normal under a broad class of (possibly non-elliptical) actual innovation densities. No kernel density estimation is required to implement our procedures. A Monte Carlo comparative study of our R-estimators and other routinely applied competitors demonstrates the benefits of the novel methodology, in large and small sample. Proofs, computational aspects, and further numerical results are available in the supplementary materials . Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {M. Hallin and D. La Vecchia and H. Liu},
  doi          = {10.1080/01621459.2020.1832501},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {925-938},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Center-outward R-estimation for semiparametric VARMA models},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning latent factors from diversified projections and its
applications to over-estimated and weak factors. <em>JASA</em>,
<em>117</em>(538), 909–924. (<a
href="https://doi.org/10.1080/01621459.2020.1831927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimations and applications of factor models often rely on the crucial condition that the number of latent factors is consistently estimated, which in turn also requires that factors be relatively strong, data are stationary and weakly serially dependent, and the sample size be fairly large, although in practical applications, one or several of these conditions may fail. In these cases, it is difficult to analyze the eigenvectors of the data matrix. To address this issue, we propose simple estimators of the latent factors using cross-sectional projections of the panel data, by weighted averages with predetermined weights. These weights are chosen to diversify away the idiosyncratic components, resulting in “diversified factors.” Because the projections are conducted cross-sectionally, they are robust to serial conditions, easy to analyze and work even for finite length of time series. We formally prove that this procedure is robust to over-estimating the number of factors, and illustrate it in several applications, including post-selection inference, big data forecasts, large covariance estimation, and factor specification tests. We also recommend several choices for the diversified weights. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jianqing Fan and Yuan Liao},
  doi          = {10.1080/01621459.2020.1831927},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {909-924},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Learning latent factors from diversified projections and its applications to over-estimated and weak factors},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A minimax optimal ridge-type set test for global hypothesis
with applications in whole genome sequencing association studies.
<em>JASA</em>, <em>117</em>(538), 897–908. (<a
href="https://doi.org/10.1080/01621459.2020.1831926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing a global hypothesis for a set of variables is a fundamental problem in statistics with a wide range of applications. A few well-known classical tests include the Hotelling’s T 2 test, the F -test, and the empirical Bayes based score test. These classical tests, however, are not robust to the signal strength and could have a substantial loss of power when signals are weak or moderate, a situation we commonly encounter in contemporary applications. In this article, we propose a minimax optimal ridge-type set test (MORST), a simple and generic method for testing a global hypothesis. The power of MORST is robust and considerably higher than that of the classical tests when the strength of signals is weak or moderate. In the meantime, MORST only requires a slight increase in computation compared to these existing tests, making it applicable to the analysis of massive genome-wide data. We also provide the generalizations of MORST that are parallel to the traditional Wald test and Rao’s score test in asymptotic settings. Extensive simulations demonstrated the robust power of MORST and that the Type I error of MORST was well controlled. We applied MORST to the analysis of the whole-genome sequencing data from the Atherosclerosis Risk in Communities study, where MORST detected 20\%–250\% more signal regions than the classical tests. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yaowu Liu and Zilin Li and Xihong Lin},
  doi          = {10.1080/01621459.2020.1831926},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {897-908},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A minimax optimal ridge-type set test for global hypothesis with applications in whole genome sequencing association studies},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heteroscedasticity-robust inference in linear regression
models with many covariates. <em>JASA</em>, <em>117</em>(538), 887–896.
(<a href="https://doi.org/10.1080/01621459.2020.1831924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider inference in linear regression models that is robust to heteroscedasticity and the presence of many control variables. When the number of control variables increases at the same rate as the sample size the usual heteroscedasticity-robust estimators of the covariance matrix are inconsistent. Hence, tests based on these estimators are size distorted even in large samples. An alternative covariance-matrix estimator for such a setting is presented that complements recent work by Cattaneo, Jansson, and Newey. We provide high-level conditions for our approach to deliver (asymptotically) size-correct inference as well as more primitive conditions for three special cases. Simulation results and an empirical illustration to inference on the union premium are also provided. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Koen Jochmans},
  doi          = {10.1080/01621459.2020.1831924},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {887-896},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Heteroscedasticity-robust inference in linear regression models with many covariates},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Balancing unobserved covariates with covariate-adaptive
randomized experiments. <em>JASA</em>, <em>117</em>(538), 875–886. (<a
href="https://doi.org/10.1080/01621459.2020.1825450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Balancing important covariates is often critical in clinical trials and causal inference. Stratified permuted block (STR-PB) and covariate-adaptive randomization (CAR) procedures are widely used to balance observed covariates in practice. The balance properties of these procedures with respect to the observed covariates have been well studied. However, it has been questioned whether these methods will also yield a good balance for the unobserved covariates. In this article, we develop a general framework for the analysis of the unobserved covariates imbalance. These results are applicable to develop and compare the balance properties of complete randomization (CR), STR-PB, and CAR procedures with respect to the unobserved covariates. To quantify the improvement obtained by using STR-PB and CAR procedures rather than CR, we introduce the percentage reduction in variance of the unobserved covariates imbalance and compare these quantities. Our results demonstrate the benefits of using CAR or STR-PB (when the number of strata is small relative to the sample size) in terms of balancing unobserved covariates. These results also pave the way for future research into the effect of unobserved covariates in covariate-adaptive randomized experiments in clinical trials, as well as many other applications. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yang Liu and Feifang Hu},
  doi          = {10.1080/01621459.2020.1825450},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {875-886},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Balancing unobserved covariates with covariate-adaptive randomized experiments},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian regression using a prior on the model fit: The
r2-d2 shrinkage prior. <em>JASA</em>, <em>117</em>(538), 862–874. (<a
href="https://doi.org/10.1080/01621459.2020.1825449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior distributions for high-dimensional linear regression require specifying a joint distribution for the unobserved regression coefficients, which is inherently difficult. We instead propose a new class of shrinkage priors for linear regression via specifying a prior first on the model fit, in particular, the coefficient of determination, and then distributing through to the coefficients in a novel way. The proposed method compares favorably to previous approaches in terms of both concentration around the origin and tail behavior, which leads to improved performance both in posterior contraction and in empirical performance. The limiting behavior of the proposed prior is 1 / x , both around the origin and in the tails. This behavior is optimal in the sense that it simultaneously lies on the boundary of being an improper prior both in the tails and around the origin. None of the existing shrinkage priors obtain this behavior in both regions simultaneously. We also demonstrate that our proposed prior leads to the same near-minimax posterior contraction rate as the spike-and-slab prior. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yan Dora Zhang and Brian P. Naughton and Howard D. Bondell and Brian J. Reich},
  doi          = {10.1080/01621459.2020.1825449},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {862-874},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian regression using a prior on the model fit: The r2-d2 shrinkage prior},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating number of factors by adjusted eigenvalues
thresholding. <em>JASA</em>, <em>117</em>(538), 852–861. (<a
href="https://doi.org/10.1080/01621459.2020.1825448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the number of common factors is an important and practical topic in high-dimensional factor models. The existing literature is mainly based on the eigenvalues of the covariance matrix. Owing to the incomparability of the eigenvalues of the covariance matrix caused by the heterogeneous scales of the observed variables, it is not easy to find an accurate relationship between these eigenvalues and the number of common factors. To overcome this limitation, we appeal to the correlation matrix and demonstrate, surprisingly, that the number of eigenvalues greater than 1 of the population correlation matrix is the same as the number of common factors under certain mild conditions. To use such a relationship, we study random matrix theory based on the sample correlation matrix to correct biases in estimating the top eigenvalues and to take into account of estimation errors in eigenvalue estimation. Thus, we propose a tuning-free scale-invariant adjusted correlation thresholding (ACT) method for determining the number of common factors in high-dimensional factor models, taking into account the sampling variabilities and biases of top sample eigenvalues. We also establish the optimality of the proposed ACT method in terms of minimal signal strength and the optimal threshold. Simulation studies lend further support to our proposed method and show that our estimator outperforms competing methods in most test cases. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jianqing Fan and Jianhua Guo and Shurong Zheng},
  doi          = {10.1080/01621459.2020.1825448},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {852-861},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimating number of factors by adjusted eigenvalues thresholding},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Warp bridge sampling: The next generation. <em>JASA</em>,
<em>117</em>(538), 835–851. (<a
href="https://doi.org/10.1080/01621459.2020.1825447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bridge sampling is an effective Monte Carlo (MC) method for estimating the ratio of normalizing constants of two probability densities, a routine computational problem in statistics, physics, chemistry, and other fields. The MC error of the bridge sampling estimator is determined by the amount of overlap between the two densities. In the case of unimodal densities, Warp-I, II, and III transformations are effective for increasing the initial overlap, but they are less so for multimodal densities. This article introduces Warp-U transformations that aim to transform multimodal densities into unimodal ones (hence “U”) without altering their normalizing constants. The construction of a Warp-U transformation starts with a normal (or other convenient) mixture distribution ϕ mix that has reasonable overlap with the target density p , whose normalizing constant is unknown. The stochastic transformation that maps ϕ mix back to its generating distribution N ( 0 , 1 ) is then applied to p yielding its Warp-U version, which we denote p ˜ . Typically, p ˜ is unimodal and has substantially increased overlap with ϕ . Furthermore, we prove that the overlap between p ˜ and N ( 0 , 1 ) is guaranteed to be no less than the overlap between p and ϕ mix , in terms of any f -divergence. We propose a computationally efficient method to find an appropriate ϕ mix , and a simple but effective approach to remove the bias which results from estimating the normalizing constant and fitting ϕ mix with the same data. We illustrate our findings using 10 and 50 dimensional highly irregular multimodal densities, and demonstrate how Warp-U sampling can be used to improve the final estimation step of the Generalized Wang–Landau algorithm, a powerful sampling and estimation approach. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Lazhi Wang and David E. Jones and Xiao-Li Meng},
  doi          = {10.1080/01621459.2020.1825447},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {835-851},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Warp bridge sampling: The next generation},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simultaneous detection of signal regions using quadratic
scan statistics with applications to whole genome association studies.
<em>JASA</em>, <em>117</em>(538), 823–834. (<a
href="https://doi.org/10.1080/01621459.2020.1822849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider in this article detection of signal regions associated with disease outcomes in whole genome association studies. Gene- or region-based methods have become increasingly popular in whole genome association analysis as a complementary approach to traditional individual variant analysis. However, these methods test for the association between an outcome and the genetic variants in a prespecified region, for example, a gene. In view of massive intergenic regions in whole genome sequencing (WGS) studies, we propose a computationally efficient quadratic scan (Q-SCAN) statistic based method to detect the existence and the locations of signal regions by scanning the genome continuously. The proposed method accounts for the correlation (linkage disequilibrium) among genetic variants, and allows for signal regions to have both causal and neutral variants, and the effects of signal variants to be in different directions. We study the asymptotic properties of the proposed Q-SCAN statistics. We derive an empirical threshold that controls for the family-wise error rate, and show that under regularity conditions the proposed method consistently selects the true signal regions. We perform simulation studies to evaluate the finite sample performance of the proposed method. Our simulation results show that the proposed procedure outperforms the existing methods, especially when signal regions have causal variants whose effects are in different directions, or are contaminated with neutral variants. We illustrate Q-SCAN by analyzing the WGS data from the Atherosclerosis Risk in Communities study. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zilin Li and Yaowu Liu and Xihong Lin},
  doi          = {10.1080/01621459.2020.1822849},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {823-834},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Simultaneous detection of signal regions using quadratic scan statistics with applications to whole genome association studies},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-rank covariance function estimation for multidimensional
functional data. <em>JASA</em>, <em>117</em>(538), 809–822. (<a
href="https://doi.org/10.1080/01621459.2020.1820344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multidimensional function data arise from many fields nowadays. The covariance function plays an important role in the analysis of such increasingly common data. In this article, we propose a novel nonparametric covariance function estimation approach under the framework of reproducing kernel Hilbert spaces (RKHS) that can handle both sparse and dense functional data. We extend multilinear rank structures for (finite-dimensional) tensors to functions, which allow for flexible modeling of both covariance operators and marginal structures. The proposed framework can guarantee that the resulting estimator is automatically semipositive definite, and can incorporate various spectral regularizations. The trace-norm regularization in particular can promote low ranks for both covariance operator and marginal structures. Despite the lack of a closed form, under mild assumptions, the proposed estimator can achieve unified theoretical results that hold for any relative magnitudes between the sample size and the number of observations per sample field, and the rate of convergence reveals the phase-transition phenomenon from sparse to dense functional data. Based on a new representer theorem, an ADMM algorithm is developed for the trace-norm regularization. The appealing numerical performance of the proposed estimator is demonstrated by a simulation study and the analysis of a dataset from the Argo project. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jiayi Wang and Raymond K. W. Wong and Xiaoke Zhang},
  doi          = {10.1080/01621459.2020.1820344},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {809-822},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Low-rank covariance function estimation for multidimensional functional data},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Large-scale datastreams surveillance via
pattern-oriented-sampling. <em>JASA</em>, <em>117</em>(538), 794–808.
(<a href="https://doi.org/10.1080/01621459.2020.1819295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring large-scale datastreams with limited resources has become increasingly important for real-time detection of abnormal activities in many applications. Despite the availability of large datasets, the challenges associated with designing an efficient change-detection when clustering or spatial pattern exists are not yet well addressed. In this article, a design-adaptive testing procedure is developed when only a limited number of streaming observations can be accessed at each time. We derive an optimal sampling strategy, the pattern-oriented-sampling, with which the proposed test possesses asymptotically and locally best power under alternatives. Then, a sequential change-detection procedure is proposed by integrating this test with generalized likelihood ratio approach. Benefiting from dynamically estimating the optimal sampling design, the proposed procedure is able to improve the sensitivity in detecting clustered changes compared with existing procedures. Its advantages are demonstrated in numerical simulations and a real data example. Ignoring the neighboring information of spatially structured data will tend to diminish the detection effectiveness of traditional detection procedures. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Haojie Ren and Changliang Zou and Nan Chen and Runze Li},
  doi          = {10.1080/01621459.2020.1819295},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {794-808},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Large-scale datastreams surveillance via pattern-oriented-sampling},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Copula gaussian graphical models for functional data.
<em>JASA</em>, <em>117</em>(538), 781–793. (<a
href="https://doi.org/10.1080/01621459.2020.1817750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a statistical graphical model for multivariate functional data, which are common in medical applications such as EEG and fMRI. Recently published functional graphical models rely on the multivariate Gaussian process assumption, but we relax it by introducing the functional copula Gaussian graphical model (FCGGM). This model removes the marginal Gaussian assumption but retains the simplicity of the Gaussian dependence structure, which is particularly attractive for large data. We develop four estimators for the FCGGM and establish the consistency and the convergence rates of one of them. We compare our FCGGM with the existing functional Gaussian graphical model by simulations, and apply our method to an EEG dataset to construct brain networks. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Eftychia Solea and Bing Li},
  doi          = {10.1080/01621459.2020.1817750},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {781-793},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Copula gaussian graphical models for functional data},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Monte carlo approximation of bayes factors via mixing with
surrogate distributions. <em>JASA</em>, <em>117</em>(538), 765–780. (<a
href="https://doi.org/10.1080/01621459.2020.1811100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By mixing the target posterior distribution with a surrogate distribution, of which the normalizing constant is tractable, we propose a method for estimating the marginal likelihood using the Wang–Landau algorithm. We show that a faster convergence of the proposed method can be achieved via the momentum acceleration. Two implementation strategies are detailed: (i) facilitating global jumps between the posterior and surrogate distributions via the multiple-try Metropolis (MTM); (ii) constructing the surrogate via the variational approximation. When a surrogate is difficult to come by, we describe a new jumping mechanism for general reversible jump Markov chain Monte Carlo algorithms, which combines the MTM and a directional sampling algorithm. We illustrate the proposed methods on several statistical models, including the log-Gaussian Cox process, the Bayesian Lasso, the logistic regression, and the g-prior Bayesian variable selection. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Chenguang Dai and Jun S. Liu},
  doi          = {10.1080/01621459.2020.1811100},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {765-780},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Monte carlo approximation of bayes factors via mixing with surrogate distributions},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A semiparametric approach to model effect modification.
<em>JASA</em>, <em>117</em>(538), 752–764. (<a
href="https://doi.org/10.1080/01621459.2020.1811099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One fundamental statistical question for research areas such as precision medicine and health disparity is about discovering effect modification of treatment or exposure by observed covariates. We propose a semiparametric framework for identifying such effect modification. Instead of using the traditional outcome models, we directly posit semiparametric models on contrasts, or expected differences of the outcome under different treatment choices or exposures. Through semiparametric estimation theory, all valid estimating equations, including the efficient scores, are derived. Besides doubly robust loss functions, our approach also enables dimension reduction in presence of many covariates. The asymptotic and non-asymptotic properties of the proposed methods are explored via a unified statistical and algorithmic analysis. Comparison with existing methods in both simulation and real data analysis demonstrates the superiority of our estimators especially for an efficiency improved version. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Muxuan Liang and Menggang Yu},
  doi          = {10.1080/01621459.2020.1811099},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {752-764},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A semiparametric approach to model effect modification},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonparametric maximum likelihood methods for binary response
models with random coefficients. <em>JASA</em>, <em>117</em>(538),
732–751. (<a
href="https://doi.org/10.1080/01621459.2020.1802284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The venerable method of maximum likelihood has found numerous recent applications in nonparametric estimation of regression and shape constrained densities. For mixture models the nonparametric maximum likelihood estimator (NPMLE) of Kiefer and Wolfowitz plays a central role in recent developments of empirical Bayes methods. The NPMLE has also been proposed by Cosslett as an estimation method for single index linear models for binary response with random coefficients. However, computational difficulties have hindered its application. Combining recent developments in computational geometry and convex optimization, we develop a new approach to computation for such models that dramatically increases their computational tractability. Consistency of the method is established for an expanded profile likelihood formulation. The methods are evaluated in simulation experiments, compared to the deconvolution methods of Gautier and Kitamura and illustrated in an application to modal choice for journey-to-work data in the Washington DC area. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jiaying Gu and Roger Koenker},
  doi          = {10.1080/01621459.2020.1802284},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {732-751},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric maximum likelihood methods for binary response models with random coefficients},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-validation for correlated data. <em>JASA</em>,
<em>117</em>(538), 718–731. (<a
href="https://doi.org/10.1080/01621459.2020.1801451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract–K -fold cross-validation (CV) with squared error loss is widely used for evaluating predictive models, especially when strong distributional assumptions cannot be taken. However, CV with squared error loss is not free from distributional assumptions, in particular in cases involving non-iid data. This article analyzes CV for correlated data. We present a criterion for suitability of standard CV in presence of correlations. When this criterion does not hold, we introduce a bias corrected CV estimator which we term CV c , CV c , CVc, that yields an unbiased estimate of prediction error in many settings where standard CV is invalid. We also demonstrate our results numerically, and find that introducing our correction substantially improves both, model evaluation and model selection in simulations and real data studies. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Assaf Rabinowicz and Saharon Rosset},
  doi          = {10.1080/01621459.2020.1801451},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {718-731},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Cross-validation for correlated data},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Angle-based hierarchical classification using exact label
embedding. <em>JASA</em>, <em>117</em>(538), 704–717. (<a
href="https://doi.org/10.1080/01621459.2020.1801450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical classification problems are commonly seen in practice. However, most existing methods do not fully use the hierarchical information among class labels. In this article, a novel label embedding approach is proposed, which keeps the hierarchy of labels exactly, and reduces the complexity of the hypothesis space significantly. Based on the newly proposed label embedding approach, a new angle-based classifier is developed for hierarchical classification. Moreover, to handle massive data, a new (weighted) linear loss is designed, which has a closed form solution and is computationally efficient. Theoretical properties of the new method are established and intensive numerical comparisons with other methods are conducted. Both simulations and applications in document categorization demonstrate the advantages of the proposed method. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yiwei Fan and Xiaoling Lu and Yufeng Liu and Junlong Zhao},
  doi          = {10.1080/01621459.2020.1801450},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {704-717},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Angle-based hierarchical classification using exact label embedding},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation of low rank high-dimensional multivariate linear
models for multi-response data. <em>JASA</em>, <em>117</em>(538),
693–703. (<a
href="https://doi.org/10.1080/01621459.2020.1799813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study low rank high-dimensional multivariate linear models (LRMLM) for high-dimensional multi-response data. We propose an intuitively appealing estimation approach and develop an algorithm for implementation purposes. Asymptotic properties are established to justify the estimation procedure theoretically. Intensive simulation studies are also conducted to demonstrate performance when the sample size is finite, and a comparison is made with some popular methods from the literature. The results show the proposed estimator outperforms all of the alternative methods under various circumstances. Finally, using our suggested estimation procedure we apply the LRMLM to analyze an environmental dataset and predict concentrations of PM2.5 at the locations concerned. The results illustrate how the proposed method provides more accurate predictions than the alternative approaches.},
  archive      = {J_JASA},
  author       = {Changliang Zou and Yuan Ke and Wenyang Zhang},
  doi          = {10.1080/01621459.2020.1799813},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {693-703},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimation of low rank high-dimensional multivariate linear models for multi-response data},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inferring phenotypic trait evolution on large trees with
many incomplete measurements. <em>JASA</em>, <em>117</em>(538), 678–692.
(<a href="https://doi.org/10.1080/01621459.2020.1799812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparative biologists are often interested in inferring covariation between multiple biological traits sampled across numerous related taxa. To properly study these relationships, we must control for the shared evolutionary history of the taxa to avoid spurious inference. An additional challenge arises as obtaining a full suite of measurements becomes increasingly difficult with increasing taxa. This generally necessitates data imputation or integration, and existing control techniques typically scale poorly as the number of taxa increases. We propose an inference technique that integrates out missing measurements analytically and scales linearly with the number of taxa by using a post-order traversal algorithm under a multivariate Brownian diffusion (MBD) model to characterize trait evolution. We further exploit this technique to extend the MBD model to account for sampling error or nonheritable residual variance. We test these methods to examine mammalian life history traits, prokaryotic genomic and phenotypic traits, and HIV infection traits. We find computational efficiency increases that top two orders-of-magnitude over current best practices. While we focus on the utility of this algorithm in phylogenetic comparative methods, our approach generalizes to solve long-standing challenges in computing the likelihood for matrix-normal and multivariate normal distributions with missing data at scale. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Gabriel Hassler and Max R. Tolkoff and William L. Allen and Lam Si Tung Ho and Philippe Lemey and Marc A. Suchard},
  doi          = {10.1080/01621459.2020.1799812},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {678-692},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inferring phenotypic trait evolution on large trees with many incomplete measurements},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bottom-up approach to testing hypotheses that have a
branching tree dependence structure, with error rate control.
<em>JASA</em>, <em>117</em>(538), 664–677. (<a
href="https://doi.org/10.1080/01621459.2020.1799811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern statistical analyses often involve testing large numbers of hypotheses. In many situations, these hypotheses may have an underlying tree structure that both helps determine the order that tests should be conducted but also imposes a dependency between tests that must be accounted for. Our motivating example comes from testing the association between a trait of interest and groups of microbes that have been organized into operational taxonomic units (OTUs) or amplicon sequence variants (ASVs). Given p -values from association tests for each individual OTU or ASV, we would like to know if we can declare a certain species, genus, or higher taxonomic group to be associated with the trait. For this problem, a bottom-up testing algorithm that starts at the lowest level of the tree (OTUs or ASVs) and proceeds upward through successively higher taxonomic groupings (species, genus, family, etc.) is required. We develop such a bottom-up testing algorithm that controls a novel error rate that we call the false selection rate. By simulation, we also show that our approach is better at finding driver taxa , the highest level taxa below which there are dense association signals. We illustrate our approach using data from a study of the microbiome among patients with ulcerative colitis and healthy controls. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yunxiao Li and Yi-Juan Hu and Glen A. Satten},
  doi          = {10.1080/01621459.2020.1799811},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {664-677},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A bottom-up approach to testing hypotheses that have a branching tree dependence structure, with error rate control},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric fractional imputation using gaussian mixture
models for handling multivariate missing data. <em>JASA</em>,
<em>117</em>(538), 654–663. (<a
href="https://doi.org/10.1080/01621459.2020.1796358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Item nonresponse is frequently encountered in practice. Ignoring missing data can lose efficiency and lead to misleading inference. Fractional imputation is a frequentist approach of imputation for handling missing data. However, the parametric fractional imputation may be subject to bias under model misspecification. In this article, we propose a novel semiparametric fractional imputation (SFI) method using Gaussian mixture models. The proposed method is computationally efficient and leads to robust estimation. The proposed method is further extended to incorporate the categorical auxiliary information. The asymptotic model consistency and n -consistency of the SFI estimator are also established. Some simulation studies are presented to check the finite sample performance of the proposed method. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Hejian Sang and Jae Kwang Kim and Danhyang Lee},
  doi          = {10.1080/01621459.2020.1796358},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {654-663},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Semiparametric fractional imputation using gaussian mixture models for handling multivariate missing data},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The hellinger correlation. <em>JASA</em>, <em>117</em>(538),
639–653. (<a
href="https://doi.org/10.1080/01621459.2020.1791132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the defining properties of any valid measure of the dependence between two continuous random variables are revisited and complemented with two original ones, shown to imply other usual postulates. While other popular choices are proved to violate some of these requirements, a class of dependence measures satisfying all of them is identified. One particular measure, that we call the Hellinger correlation, appears as a natural choice within that class due to both its theoretical and intuitive appeal. A simple and efficient nonparametric estimator for that quantity is proposed, with its implementation publicly available in the R package HellCor. Synthetic and real-data examples illustrate the descriptive ability of the measure, which can also be used as test statistic for exact independence testing. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Gery Geenens and Pierre Lafaye de Micheaux},
  doi          = {10.1080/01621459.2020.1791132},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {639-653},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {The hellinger correlation},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distribution-free multisample tests based on optimal
matchings with applications to single cell genomics. <em>JASA</em>,
<em>117</em>(538), 627–638. (<a
href="https://doi.org/10.1080/01621459.2020.1791131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a nonparametric graphical test based on optimal matching, for assessing the equality of multiple unknown multivariate probability distributions. Our procedure pools the data from the different classes to create a graph based on the minimum non-bipartite matching, and then utilizes the number of edges connecting data points from different classes to examine the closeness between the distributions. The proposed test is exactly distribution-free (the null distribution does not depend on the distribution of the data) and can be efficiently applied to multivariate as well as non-Euclidean data, whenever the inter-point distances are well-defined. We show that the test is universally consistent, and prove a distributional limit theorem for the test statistic under general alternatives. Through simulation studies, we demonstrate its superior performance against other common and well-known multisample tests. The method is applied to single cell transcriptomics data obtained from the peripheral blood, cancer tissue, and tumor-adjacent normal tissue of human subjects with hepatocellular carcinoma and non-small-cell lung cancer. Our method unveils patterns in how biochemical metabolic pathways are altered across immune cells in a cancer setting, depending on the tissue location. All of the methods described herein are implemented in the R package multicross. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Somabha Mukherjee and Divyansh Agarwal and Nancy R. Zhang and Bhaswar B. Bhattacharya},
  doi          = {10.1080/01621459.2020.1791131},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {627-638},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Distribution-free multisample tests based on optimal matchings with applications to single cell genomics},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimizing the JSM program. <em>JASA</em>,
<em>117</em>(538), 617–626. (<a
href="https://doi.org/10.1080/01621459.2021.1978466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sometimes the Joint Statistical Meetings (JSM) is frustrating to attend, because multiple sessions on the same topic are scheduled at the same time. This article uses seeded latent Dirichlet allocation and a scheduling optimization algorithm to very significantly reduce overlapping content in the original schedule for the 2020 JSM program. Specifically, a measure based on total variation distance that ranges from 0 (random scheduling) to 1 (no overlapping content) finds that the original schedule had a score of 0.058, whereas our proposed schedule achieved a score of 0.371. This is a huge improvement that would (i) increase participant satisfaction as measured by the post-JSM satisfaction survey, and (ii) save the American Statistical Association significant money by obviating the need for the traditional in-person meeting of the 47 program chairs and other organizers. The methodology developed in this work immediately applies to future JSMs and is easily modified to improve scheduling for any other scientific conference that has parallel sessions.},
  archive      = {J_JASA},
  author       = {Luca Frigau and Qiuyi Wu and David Banks},
  doi          = {10.1080/01621459.2021.1978466},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {617-626},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimizing the JSM program},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling pregnancy outcomes through sequentially nested
regression models. <em>JASA</em>, <em>117</em>(538), 602–616. (<a
href="https://doi.org/10.1080/01621459.2021.2006666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The polycystic ovary syndrome (PCOS) is a most common cause of infertility among women of reproductive age. Unfortunately, the etiology of PCOS is poorly understood. Large-scale clinical trials for pregnancy in polycystic ovary syndrome (PPCOS) were conducted to evaluate the effectiveness of treatments. Ovulation, pregnancy, and live birth are three sequentially nested binary outcomes, typically analyzed separately. However, the separate models may lose power in detecting the treatment effects and influential variables for live birth, due to decreased sample sizes and unbalanced event counts. It has been a long-held hypothesis among the clinicians that some of the important variables for early pregnancy outcomes may continue their influence on live birth. To consider this possibility, we develop an l 0 -norm based regularization method in favor of variables that have been identified from an earlier stage. Our approach explicitly bridges the connections across nested outcomes through computationally easy algorithms and enjoys theoretical guarantee of estimation and variable selection. By analyzing the PPCOS data, we successfully uncover the hidden influence of risk factors on live birth, which confirm clinical experience. Moreover, we provide novel infertility treatment recommendations (e.g., letrozole vs. clomiphene citrate) for women with PCOS to improve their chances of live birth. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xuan Bi and Long Feng and Cai Li and Heping Zhang},
  doi          = {10.1080/01621459.2021.2006666},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {602-616},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Modeling pregnancy outcomes through sequentially nested regression models},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward causal inference for spatio-temporal data: Conflict
and forest loss in colombia. <em>JASA</em>, <em>117</em>(538), 591–601.
(<a href="https://doi.org/10.1080/01621459.2021.2013241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How does armed conflict influence tropical forest loss? For Colombia, both enhancing and reducing effect estimates have been reported. However, a lack of causal methodology has prevented establishing clear causal links between these two variables. In this work, we propose a class of causal models for spatio-temporal stochastic processes which allows us to formally define and quantify the causal effect of a vector of covariates X on a real-valued response Y . We introduce a procedure for estimating causal effects and a nonparametric hypothesis test for these effects being zero. Our application is based on geospatial information on conflict events and remote-sensing-based data on forest loss between 2000 and 2018 in Colombia. Across the entire country, we estimate the effect to be slightly negative (conflict reduces forest loss) but insignificant ( P = 0.578), while at the provincial level, we find both positive effects (e.g., La Guajira, P = 0.047) and negative effects (e.g., Magdalena, P = 0.004). The proposed methods do not make strong distributional assumptions, and allow for arbitrarily many latent confounders, given that these confounders do not vary across time. Our theoretical findings are supported by simulations, and code is available online.},
  archive      = {J_JASA},
  author       = {Rune Christiansen and Matthias Baumann and Tobias Kuemmerle and Miguel D. Mahecha and Jonas Peters},
  doi          = {10.1080/01621459.2021.2013241},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {591-601},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Toward causal inference for spatio-temporal data: Conflict and forest loss in colombia},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Do we exploit all information for counterfactual analysis?
Benefits of factor models and idiosyncratic correction. <em>JASA</em>,
<em>117</em>(538), 574–590. (<a
href="https://doi.org/10.1080/01621459.2021.2004895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal pricing, that is determining the price level that maximizes profit or revenue of a given product, is a vital task for the retail industry. To select such a quantity, one needs first to estimate the price elasticity from the product demand. Regression methods usually fail to recover such elasticities due to confounding effects and price endogeneity. Therefore, randomized experiments are typically required. However, elasticities can be highly heterogeneous depending on the location of stores, for example. As the randomization frequently occurs at the municipal level, standard difference-in-differences methods may also fail. Possible solutions are based on methodologies to measure the effects of treatments on a single (or just a few) treated unit(s) based on counterfactuals constructed from artificial controls. For example, for each city in the treatment group, a counterfactual may be constructed from the untreated locations. In this article, we apply a novel high-dimensional statistical method to measure the effects of price changes on daily sales from a major retailer in Brazil. The proposed methodology combines principal components (factors) and sparse regressions, resulting in a method called Factor-Adjusted Regularized Method for Treatment evaluation (FarmTreat). The data consist of daily sales and prices of five different products over more than 400 municipalities. The products considered belong to the sweet and candies category and experiments have been conducted over the years of 2016 and 2017. Our results confirm the hypothesis of a high degree of heterogeneity yielding very different pricing strategies over distinct municipalities. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jianqing Fan and Ricardo Masini and Marcelo C. Medeiros},
  doi          = {10.1080/01621459.2021.2004895},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {574-590},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Do we exploit all information for counterfactual analysis? benefits of factor models and idiosyncratic correction},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A cross-validated ensemble approach to robust hypothesis
testing of continuous nonlinear interactions: Application to
nutrition-environment studies. <em>JASA</em>, <em>117</em>(538),
561–573. (<a
href="https://doi.org/10.1080/01621459.2021.1962889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gene-environment and nutrition-environment studies often involve testing of high-dimensional interactions between two sets of variables, each having potentially complex nonlinear main effects on an outcome. Construction of a valid and powerful hypothesis test for such an interaction is challenging, due to the difficulty in constructing an efficient and unbiased estimator for the complex, nonlinear main effects. In this work, we address this problem by proposing a cross-validated ensemble of kernels (CVEK) that learns the space of appropriate functions for the main effects using a cross-validated ensemble approach. With a carefully chosen library of base kernels, CVEK flexibly estimates the form of the main-effect functions from the data, and encourages test power by guarding against over-fitting under the alternative. The method is motivated by a study on the interaction between metal exposures in utero and maternal nutrition on children’s neurodevelopment in rural Bangladesh. The proposed tests identified evidence of an interaction between minerals and vitamins intake and arsenic and manganese exposures. Results suggest that the detrimental effects of these metals are most pronounced at low intake levels of the nutrients, suggesting nutritional interventions in pregnant women could mitigate the adverse impacts of in utero metal exposures on the children’s neurodevelopment. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Jeremiah Zhe Liu and Wenying Deng and Jane Lee and Pi-i Debby Lin and Linda Valeri and David C. Christiani and David C. Bellinger and Robert O. Wright and Maitreyi M. Mazumdar and Brent A. Coull},
  doi          = {10.1080/01621459.2021.1962889},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {561-573},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A cross-validated ensemble approach to robust hypothesis testing of continuous nonlinear interactions: Application to nutrition-environment studies},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian spatial binary regression for label fusion in
structural neuroimaging. <em>JASA</em>, <em>117</em>(538), 547–560. (<a
href="https://doi.org/10.1080/01621459.2021.2014854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease is a neurodegenerative condition that accelerates cognitive decline relative to normal aging. It is of critical scientific importance to gain a better understanding of early disease mechanisms in the brain to facilitate effective, targeted therapies. The volume of the hippocampus is often used in diagnosis and monitoring of the disease. Measuring this volume via neuroimaging is difficult since each hippocampus must either be manually identified or automatically delineated, a task referred to as segmentation. Automatic hippocampal segmentation often involves mapping a previously manually segmented image to a new brain image and propagating the labels to obtain an estimate of where each hippocampus is located in the new image. A more recent approach to this problem is to propagate labels from multiple manually segmented atlases and combine the results using a process known as label fusion. To date, most label fusion algorithms employ voting procedures with voting weights assigned directly or estimated via optimization. We propose using a fully Bayesian spatial regression model for label fusion that facilitates direct incorporation of covariate information while making accessible the entire posterior distribution. Our results suggest that incorporating tissue classification (e.g., gray matter) into the label fusion procedure can greatly improve segmentation when relatively homogeneous, healthy brains are used as atlases for diseased brains. The fully Bayesian approach also produces meaningful uncertainty measures about hippocampal volumes, information which can be leveraged to detect significant, scientifically meaningful differences between healthy and diseased populations, improving the potential for early detection and tracking of the disease. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {D. Andrew Brown and Christopher S. McMahan and Russell T. Shinohara and Kristin A. Linn and for the Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1080/01621459.2021.2014854},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {547-560},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian spatial binary regression for label fusion in structural neuroimaging},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian edge regression in undirected graphical models to
characterize interpatient heterogeneity in cancer. <em>JASA</em>,
<em>117</em>(538), 533–546. (<a
href="https://doi.org/10.1080/01621459.2021.2000866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well established that interpatient heterogeneity in cancer may significantly affect genomic data analyses and in particular, network topologies. Most existing graphical model methods estimate a single population-level graph for genomic or proteomic network. In many investigations, these networks depend on patient-specific indicators that characterize the heterogeneity of individual networks across subjects with respect to subject-level covariates. Examples include assessments of how the network varies with patient-specific prognostic scores or comparisons of tumor and normal graphs while accounting for tumor purity as a continuous predictor. In this article, we propose a novel edge regression model for undirected graphs, which estimates conditional dependencies as a function of subject-level covariates. We evaluate our model performance through simulation studies focused on comparing tumor and normal graphs while adjusting for tumor purity. In application to a dataset of proteomic measurements on plasma samples from patients with hepatocellular carcinoma (HCC), we ascertain how blood protein networks vary with disease severity, as measured by HepatoScore, a novel biomarker signature measuring disease severity. Our case study shows that the network connectivity increases with HepatoScore and a set of hub proteins as well as important protein connections are identified under different HepatoScore, which may provide important biological insights to the development of precision therapies for HCC.},
  archive      = {J_JASA},
  author       = {Zeya Wang and Veerabhadran Baladandayuthapani and Ahmed O. Kaseb and Hesham M. Amin and Manal M. Hassan and Wenyi Wang and Jeffrey S. Morris},
  doi          = {10.1080/01621459.2021.2000866},
  journal      = {Journal of the American Statistical Association},
  number       = {538},
  pages        = {533-546},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian edge regression in undirected graphical models to characterize interpatient heterogeneity in cancer},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correction to: Semiparametric inference for non-monotone
missing-not-at-random data: The no self-censoring model. <em>JASA</em>,
<em>117</em>(537), 530. (<a
href="https://doi.org/10.1080/01621459.2021.2016421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  doi          = {10.1080/01621459.2021.2016421},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {530},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Correction to: semiparametric inference for non-monotone missing-not-at-random data: the no self-censoring model},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). From fixed-x to random-x regression: Bias-variance
decompositions, covariance penalties, and prediction error estimation:
correction. <em>JASA</em>, <em>117</em>(537), 529. (<a
href="https://doi.org/10.1080/01621459.2021.2016420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  doi          = {10.1080/01621459.2021.2016420},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {529},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {From fixed-X to random-X regression: bias-variance decompositions, covariance penalties, and prediction error estimation: correction},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An introduction to acceptance sampling and SPC with r.
<em>JASA</em>, <em>117</em>(537), 528. (<a
href="https://doi.org/10.1080/01621459.2022.2035160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Youngjun Choe},
  doi          = {10.1080/01621459.2022.2035160},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {528},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {An introduction to acceptance sampling and SPC with r},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic treatment regimes: Statistical methods for precision
medicine. <em>JASA</em>, <em>117</em>(537), 527. (<a
href="https://doi.org/10.1080/01621459.2022.2035159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Ying-Qi Zhao},
  doi          = {10.1080/01621459.2022.2035159},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {527},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Dynamic treatment regimes: Statistical methods for precision medicine},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Likelihood-based inference for partially observed epidemics
on dynamic networks. <em>JASA</em>, <em>117</em>(537), 510–526. (<a
href="https://doi.org/10.1080/01621459.2020.1790376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a generative model and an inference scheme for epidemic processes on dynamic, adaptive contact networks. Network evolution is formulated as a link-Markovian process, which is then coupled to an individual-level stochastic susceptible-infectious-recovered model, to describe the interplay between the dynamics of the disease spread and the contact network underlying the epidemic. A Markov chain Monte Carlo framework is developed for likelihood-based inference from partial epidemic observations, with a novel data augmentation algorithm specifically designed to deal with missing individual recovery times under the dynamic network setting. Through a series of simulation experiments, we demonstrate the validity and flexibility of the model as well as the efficacy and efficiency of the data augmentation inference scheme. The model is also applied to a recent real-world dataset on influenza-like-illness transmission with high-resolution social contact tracking records. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Fan Bu and Allison E. Aiello and Jason Xu and Alexander Volfovsky},
  doi          = {10.1080/01621459.2020.1790376},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {510-526},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Likelihood-based inference for partially observed epidemics on dynamic networks},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AdaBoost semiparametric model averaging prediction for
multiple categories. <em>JASA</em>, <em>117</em>(537), 495–509. (<a
href="https://doi.org/10.1080/01621459.2020.1790375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model average techniques are very useful for model-based prediction. However, most earlier works in this field focused on parametric models and continuous responses. In this article, we study varying coefficient multinomial logistic models and propose a semiparametric model averaging prediction (SMAP) approach for multi-category outcomes. The proposed procedure does not need any artificial specification of the index variable in the adopted varying coefficient sub-model structure to forecast the response. In particular, this new SMAP method is more flexible and robust against model misspecification. To improve the practical predictive performance, we combine SMAP with the AdaBoost algorithm to obtain more accurate estimations of class probabilities and model averaging weights. We compare our proposed methods with all existing model averaging approaches and a wide range of popular classification methods via extensive simulations. An automobile classification study is included to illustrate the merits of our methodology. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jialiang Li and Jing Lv and Alan T. K. Wan and Jun Liao},
  doi          = {10.1080/01621459.2020.1790375},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {495-509},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {AdaBoost semiparametric model averaging prediction for multiple categories},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Restricted spatial regression methods: Implications for
inference. <em>JASA</em>, <em>117</em>(537), 482–494. (<a
href="https://doi.org/10.1080/01621459.2020.1788949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The issue of spatial confounding between the spatial random effect and the fixed effects in regression analyses has been identified as a concern in the statistical literature. Multiple authors have offered perspectives and potential solutions. In this article, for the areal spatial data setting, we show that many of the methods designed to alleviate spatial confounding can be viewed as special cases of a general class of models. We refer to this class as restricted spatial regression (RSR) models, extending terminology currently in use. We offer a mathematically based exploration of the impact that RSR methods have on inference for regression coefficients for the linear model. We then explore whether these results hold in the generalized linear model setting for count data using simulations. We show that the use of these methods have counterintuitive consequences which defy the general expectations in the literature. In particular, our results and the accompanying simulations suggest that RSR methods will typically perform worse than nonspatial methods. These results have important implications for dimension reduction strategies in spatial regression modeling. Specifically, we demonstrate that the problems with RSR models cannot be fixed with a selection of “better” spatial basis vectors or dimension reduction techniques. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Kori Khan and Catherine A. Calder},
  doi          = {10.1080/01621459.2020.1788949},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {482-494},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Restricted spatial regression methods: Implications for inference},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric estimation of the distribution of
episodically consumed foods measured with error. <em>JASA</em>,
<em>117</em>(537), 469–481. (<a
href="https://doi.org/10.1080/01621459.2020.1787840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dietary data collected from 24-hour dietary recalls are observed with significant measurement errors. In the nonparametric curve estimation literature, much of the effort has been devoted to designing methods that are consistent under contamination by noise, and which have been traditionally applied for analyzing those data. However, some foods such as alcohol or fruits are consumed only episodically, and may not be consumed during the day when the 24-hour recall is administered. These so-called excess zeros make existing nonparametric estimators break down, and new techniques need to be developed for such data. We develop two new consistent semiparametric estimators of the distribution of such episodically consumed food data, making parametric assumptions only on some less important parts of the model. We establish its theoretical properties and illustrate the good performance of our fully data-driven method in simulated and real data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Félix Camirand Lemyre and Raymond J. Carroll and Aurore Delaigle},
  doi          = {10.1080/01621459.2020.1787840},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {469-481},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Semiparametric estimation of the distribution of episodically consumed foods measured with error},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Irrational exuberance: Correcting bias in probability
estimates. <em>JASA</em>, <em>117</em>(537), 455–468. (<a
href="https://doi.org/10.1080/01621459.2020.1787175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the common setting where one observes probability estimates for a large number of events, such as default risks for numerous bonds. Unfortunately, even with unbiased estimates, selecting events corresponding to the most extreme probabilities can result in systematically underestimating the true level of uncertainty. We develop an empirical Bayes approach “excess certainty adjusted probabilities” (ECAP), using a variant of Tweedie’s formula, which updates probability estimates to correct for selection bias. ECAP is a flexible nonparametric method, which directly estimates the score function associated with the probability estimates, so it does not need to make any restrictive assumptions about the prior on the true probabilities. ECAP also works well in settings where the probability estimates are biased. We demonstrate through theoretical results, simulations, and an analysis of two real world datasets, that ECAP can provide significant improvements over the original probability estimates. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Gareth M. James and Peter Radchenko and Bradley Rava},
  doi          = {10.1080/01621459.2020.1787175},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {455-468},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Irrational exuberance: Correcting bias in probability estimates},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating a change point in a sequence of very
high-dimensional covariance matrices. <em>JASA</em>, <em>117</em>(537),
444–454. (<a
href="https://doi.org/10.1080/01621459.2020.1785477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the problem of estimating a change point in the covariance matrix in a sequence of high-dimensional vectors, where the dimension is substantially larger than the sample size. A two-stage approach is proposed to efficiently estimate the location of the change point. The first step consists of a reduction of the dimension to identify elements of the covariance matrices corresponding to significant changes. In a second step, we use the components after dimension reduction to determine the position of the change point. Theoretical properties are developed for both steps, and numerical studies are conducted to support the new methodology. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Holger Dette and Guangming Pan and Qing Yang},
  doi          = {10.1080/01621459.2020.1785477},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {444-454},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimating a change point in a sequence of very high-dimensional covariance matrices},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-free feature screening and FDR control with knockoff
features. <em>JASA</em>, <em>117</em>(537), 428–443. (<a
href="https://doi.org/10.1080/01621459.2020.1783274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a model-free and data-adaptive feature screening method for ultrahigh-dimensional data. The proposed method is based on the projection correlation which measures the dependence between two random vectors. This projection correlation based method does not require specifying a regression model, and applies to data in the presence of heavy tails and multivariate responses. It enjoys both sure screening and rank consistency properties under weak assumptions. A two-step approach, with the help of knockoff features, is advocated to specify the threshold for feature screening such that the false discovery rate (FDR) is controlled under a prespecified level. The proposed two-step approach enjoys both sure screening and FDR control simultaneously if the prespecified FDR level is greater or equal to 1/ s , where s is the number of active features. The superior empirical performance of the proposed method is illustrated by simulation examples and real data applications. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Wanjun Liu and Yuan Ke and Jingyuan Liu and Runze Li},
  doi          = {10.1080/01621459.2020.1783274},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {428-443},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Model-free feature screening and FDR control with knockoff features},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Covariate adaptive false discovery rate control with
applications to omics-wide multiple testing. <em>JASA</em>,
<em>117</em>(537), 411–427. (<a
href="https://doi.org/10.1080/01621459.2020.1783273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional multiple testing procedures often assume hypotheses for different features are exchangeable. However, in many scientific applications, additional covariate information regarding the patterns of signals and nulls are available. In this article, we introduce an FDR control procedure in large-scale inference problem that can incorporate covariate information. We develop a fast algorithm to implement the proposed procedure and prove its asymptotic validity even when the underlying likelihood ratio model is misspecified and the p -values are weakly dependent (e.g., strong mixing). Extensive simulations are conducted to study the finite sample performance of the proposed method and we demonstrate that the new approach improves over the state-of-the-art approaches by being flexible, robust, powerful, and computationally efficient. We finally apply the method to several omics datasets arising from genomics studies with the aim to identify omics features associated with some clinical and biological phenotypes. We show that the method is overall the most powerful among competing methods, especially when the signal is sparse. The proposed covariate adaptive multiple testing procedure is implemented in the R package CAMT. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xianyang Zhang and Jun Chen},
  doi          = {10.1080/01621459.2020.1783273},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {411-427},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Covariate adaptive false discovery rate control with applications to omics-wide multiple testing},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distribution-free consistent independence tests via
center-outward ranks and signs. <em>JASA</em>, <em>117</em>(537),
395–410. (<a
href="https://doi.org/10.1080/01621459.2020.1782223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the problem of testing independence of two random vectors of general dimensions. For this, we give for the first time a distribution-free consistent test. Our approach combines distance covariance with the center-outward ranks and signs developed by Marc Hallin and collaborators. In technical terms, the proposed test is consistent and distribution-free in the family of multivariate distributions with nonvanishing (Lebesgue) probability densities. Exploiting the (degenerate) U -statistic structure of the distance covariance and the combinatorial nature of Hallin’s center-outward ranks and signs, we are able to derive the limiting null distribution of our test statistic. The resulting asymptotic approximation is accurate already for moderate sample sizes and makes the test implementable without requiring permutation. The limiting distribution is derived via a more general result that gives a new type of combinatorial noncentral limit theorem for double- and multiple-indexed permutation statistics. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Hongjian Shi and Mathias Drton and Fang Han},
  doi          = {10.1080/01621459.2020.1782223},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {395-410},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Distribution-free consistent independence tests via center-outward ranks and signs},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mini-batch metropolis–hastings with reversible SGLD
proposal. <em>JASA</em>, <em>117</em>(537), 386–394. (<a
href="https://doi.org/10.1080/01621459.2020.1782222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional Markov chain Monte Carlo (MCMC) algorithms are computationally intensive and do not scale well to large data. In particular, the Metropolis–Hastings (MH) algorithm requires passing over the entire dataset to evaluate the likelihood ratio in each iteration. We propose a general framework for performing MH-MCMC using mini-batches of the whole dataset and show that this gives rise to approximately a tempered stationary distribution. We prove that the algorithm preserves the modes of the original target distribution and derive an error bound on the approximation with mild assumptions on the likelihood. To further extend the utility of the algorithm to high-dimensional settings, we construct a proposal with forward and reverse moves using stochastic gradient and show that the construction leads to reasonable acceptance probabilities. We demonstrate the performance of our algorithm in both low dimensional models and high dimensional neural network applications. Particularly in the latter case, compared to popular optimization methods, our method is more robust to the choice of learning rate and improves testing accuracy. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Tung-Yu Wu and Y. X. Rachel Wang and Wing H. Wong},
  doi          = {10.1080/01621459.2020.1782222},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {386-394},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Mini-batch Metropolis–Hastings with reversible SGLD proposal},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On design orthogonality, maximin distance, and projection
uniformity for computer experiments. <em>JASA</em>, <em>117</em>(537),
375–385. (<a
href="https://doi.org/10.1080/01621459.2020.1782221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Space-filling designs are widely used in both computer and physical experiments. Column-orthogonality, maximin distance, and projection uniformity are three basic and popular space-filling criteria proposed from different perspectives, but their relationships have been rarely investigated. We show that the average squared correlation metric is a function of the pairwise L 2 -distances between the rows only. We further explore the connection between uniform projection designs and maximin L 1 -distance designs. Based on these connections, we develop new lower and upper bounds for column-orthogonality and projection uniformity from the perspective of distance between design points. These results not only provide new theoretical justifications for each criterion but also help in finding better space-filling designs under multiple criteria. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yaping Wang and Fasheng Sun and Hongquan Xu},
  doi          = {10.1080/01621459.2020.1782221},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {375-385},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {On design orthogonality, maximin distance, and projection uniformity for computer experiments},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation of subgraph densities in noisy networks.
<em>JASA</em>, <em>117</em>(537), 361–374. (<a
href="https://doi.org/10.1080/01621459.2020.1778482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While it is common practice in applied network analysis to report various standard network summary statistics, these numbers are rarely accompanied by uncertainty quantification. Yet any error inherent in the measurements underlying the construction of the network, or in the network construction procedure itself, necessarily must propagate to any summary statistics reported. Here we study the problem of estimating the density of an arbitrary subgraph, given a noisy version of some underlying network as data. Under a simple model of network error, we show that consistent estimation of such densities is impossible when the rates of error are unknown and only a single network is observed. Accordingly, we develop method-of-moment estimators of network subgraph densities and error rates for the case where a minimal number of network replicates are available. These estimators are shown to be asymptotically normal as the number of vertices increases to infinity. We also provide confidence intervals for quantifying the uncertainty in these estimates based on the asymptotic normality. To construct the confidence intervals, a new and nonstandard bootstrap method is proposed to compute asymptotic variances, which is infeasible otherwise. We illustrate the proposed methods in the context of gene coexpression networks. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jinyuan Chang and Eric D. Kolaczyk and Qiwei Yao},
  doi          = {10.1080/01621459.2020.1778482},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {361-374},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimation of subgraph densities in noisy networks},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mean and covariance estimation for functional snippets.
<em>JASA</em>, <em>117</em>(537), 348–360. (<a
href="https://doi.org/10.1080/01621459.2020.1777138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider estimation of mean and covariance functions of functional snippets, which are short segments of functions possibly observed irregularly on an individual specific subinterval that is much shorter than the entire study interval. Estimation of the covariance function for functional snippets is challenging since information for the far off-diagonal regions of the covariance structure is completely missing. We address this difficulty by decomposing the covariance function into a variance function component and a correlation function component. The variance function can be effectively estimated nonparametrically, while the correlation part is modeled parametrically, possibly with an increasing number of parameters, to handle the missing information in the far off-diagonal regions. Both theoretical analysis and numerical simulations suggest that this hybrid strategy is effective. In addition, we propose a new estimator for the variance of measurement errors and analyze its asymptotic properties. This estimator is required for the estimation of the variance function from noisy measurements. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zhenhua Lin and Jane-Ling Wang},
  doi          = {10.1080/01621459.2020.1777138},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {348-360},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Mean and covariance estimation for functional snippets},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An empirical bayes method for chi-squared data.
<em>JASA</em>, <em>117</em>(537), 334–347. (<a
href="https://doi.org/10.1080/01621459.2020.1777137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a thought-provoking paper, Efron investigated the merit and limitation of an empirical Bayes method to correct selection bias based on Tweedie’s formula first reported in the study by Robbins. The exceptional virtue of Tweedie’s formula for the normal distribution lies in its representation of selection bias as a simple function of the derivative of log marginal likelihood. Since the marginal likelihood and its derivative can be estimated from the data directly without invoking prior information, bias correction can be carried out conveniently. We propose a Bayesian hierarchical model for chi-squared data such that the resulting Tweedie’s formula has the same virtue as that of the normal distribution. Because the family of noncentral chi-squared distributions, the common alternative distributions for chi-squared tests, does not constitute an exponential family, our results cannot be obtained by extending existing results. Furthermore, the corresponding Tweedie’s formula manifests new phenomena quite different from those of the normal distribution and suggests new ways of analyzing chi-squared data.},
  archive      = {J_JASA},
  author       = {Lilun Du and Inchi Hu},
  doi          = {10.1080/01621459.2020.1777137},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {334-347},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {An empirical bayes method for chi-squared data},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regularized optimal transport of covariates and outcomes in
data recoding. <em>JASA</em>, <em>117</em>(537), 320–333. (<a
href="https://doi.org/10.1080/01621459.2020.1775615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When databases are constructed from heterogeneous sources, it is not unusual that different encodings are used for the same outcome. In such case, it is necessary to recode the outcome variable before merging two databases. The method proposed for the recoding is an application of optimal transportation where we search for a bijective mapping between the distributions of such variable in two databases. In this article, we build upon the work by Garés et al., where they transport the distributions of categorical outcomes assuming that they are distributed equally in the two databases. Here, we extend the scope of the model to treat all the situations where the covariates explain the outcomes similarly in the two databases. In particular, we do not require that the outcomes be distributed equally. For this, we propose a model where joint distributions of outcomes and covariates are transported. We also propose to enrich the model by relaxing the constraints on marginal distributions and adding an L 1 regularization term. The performances of the models are evaluated in a simulation study, and they are applied to a real dataset. The code used in the computational assessment and in the simulation of test cases is publicly available on Github repository: https://github.com/otrecoding/OTRecod.jl .},
  archive      = {J_JASA},
  author       = {Valérie Garès and Jérémy Omer},
  doi          = {10.1080/01621459.2020.1775615},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {320-333},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Regularized optimal transport of covariates and outcomes in data recoding},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Embedding learning. <em>JASA</em>, <em>117</em>(537),
307–319. (<a
href="https://doi.org/10.1080/01621459.2020.1775614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical embedding has become one standard technique for processing and analyzing unstructured data that cannot be expressed in a predefined fashion. It stores the main characteristics of data by mapping it onto a numerical vector. An embedding is often unsupervised and constructed by transfer learning from large-scale unannotated data. Given an embedding, a downstream learning method, referred to as a two-stage method, is applicable to unstructured data. In this article, we introduce a novel framework of embedding learning to deliver a higher learning accuracy than the two-stage method while identifying an optimal learning-adaptive embedding. In particular, we propose a concept of U -minimal sufficient learning-adaptive embeddings, based on which we seek an optimal one to maximize the learning accuracy subject to an embedding constraint. Moreover, when specializing the general framework to classification, we derive a graph embedding classifier based on a hyperlink tensor representing multiple hypergraphs, directed or undirected, characterizing multi-way relations of unstructured data. Numerically, we design algorithms based on blockwise coordinate descent and projected gradient descent to implement linear and feed-forward neural network classifiers, respectively. Theoretically, we establish a learning theory to quantify the generalization error of the proposed method. Moreover, we show, in linear regression, that the one-hot encoder is more preferable among two-stage methods, yet its dimension restriction hinders its predictive performance. For a graph embedding classifier, the generalization error matches up to the standard fast rate or the parametric rate for linear or nonlinear classification. Finally, we demonstrate the utility of the classifiers on two benchmarks in grammatical classification and sentiment analysis. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ben Dai and Xiaotong Shen and Junhui Wang},
  doi          = {10.1080/01621459.2020.1775614},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {307-319},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Embedding learning},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detection of local differences in spatial characteristics
between two spatiotemporal random fields. <em>JASA</em>,
<em>117</em>(537), 291–306. (<a
href="https://doi.org/10.1080/01621459.2020.1775613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparing the spatial characteristics of spatiotemporal random fields is often at demand. However, the comparison can be challenging due to the high-dimensional feature and dependency in the data. We develop a new multiple testing approach to detect local differences in the spatial characteristics of two spatiotemporal random fields by taking the spatial information into account. Our method adopts a two-component mixture model for location wise p -values and then derives a new false discovery rate (FDR) control, called mirror procedure, to determine the optimal rejection region. This procedure is robust to model misspecification and allows for weak dependency among hypotheses. To integrate the spatial heterogeneity, we model the mixture probability as well as study the benefit if any of allowing the alternative distribution to be spatially varying. An EM-algorithm is developed to estimate the mixture model and implement the FDR procedure. We study the FDR control and the power of our new approach both theoretically and numerically, and apply the approach to compare the mean and teleconnection pattern between two synthetic climate fields. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Sooin Yun and Xianyang Zhang and Bo Li},
  doi          = {10.1080/01621459.2020.1775613},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {291-306},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Detection of local differences in spatial characteristics between two spatiotemporal random fields},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A mode-jumping algorithm for bayesian factor analysis.
<em>JASA</em>, <em>117</em>(537), 277–290. (<a
href="https://doi.org/10.1080/01621459.2020.1773833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploratory factor analysis is a dimension-reduction technique commonly used in psychology, finance, genomics, neuroscience, and economics. Advances in computational power have opened the door for fully Bayesian treatments of factor analysis. One open problem is enforcing rotational identifability of the latent factor loadings, as the loadings are not identified from the likelihood without further restrictions. Nonidentifability of the loadings can cause posterior multimodality, which can produce misleading posterior summaries. The positive-diagonal, lower-triangular (PLT) constraint is the most commonly used restriction to guarantee identifiability, in which the upper m × m submatrix of the loadings is constrained to be a lower-triangular matrix with positive-diagonal elements. The PLT constraint can fail to guarantee identifiability if the constrained submatrix is singular. Furthermore, though the PLT constraint addresses identifiability-related multimodality, it introduces additional mixing issues. We introduce a new Bayesian sampling algorithm that efficiently explores the multimodal posterior surface and addresses issues with PLT-constrained approaches. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Albert Xingyi Man and Steven Andrew Culpepper},
  doi          = {10.1080/01621459.2020.1773833},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {277-290},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A mode-jumping algorithm for bayesian factor analysis},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal distributed subsampling for maximum quasi-likelihood
estimators with massive data. <em>JASA</em>, <em>117</em>(537), 265–276.
(<a href="https://doi.org/10.1080/01621459.2020.1773832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonuniform subsampling methods are effective to reduce computational burden and maintain estimation efficiency for massive data. Existing methods mostly focus on subsampling with replacement due to its high computational efficiency. If the data volume is so large that nonuniform subsampling probabilities cannot be calculated all at once, then subsampling with replacement is infeasible to implement. This article solves this problem using Poisson subsampling. We first derive optimal Poisson subsampling probabilities in the context of quasi-likelihood estimation under the A- and L-optimality criteria. For a practically implementable algorithm with approximated optimal subsampling probabilities, we establish the consistency and asymptotic normality of the resultant estimators. To deal with the situation that the full data are stored in different blocks or at multiple locations, we develop a distributed subsampling framework, in which statistics are computed simultaneously on smaller partitions of the full data. Asymptotic properties of the resultant aggregated estimator are investigated. We illustrate and evaluate the proposed strategies through numerical experiments on simulated and real datasets. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jun Yu and HaiYing Wang and Mingyao Ai and Huiming Zhang},
  doi          = {10.1080/01621459.2020.1773832},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {265-276},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal distributed subsampling for maximum quasi-likelihood estimators with massive data},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint structural break detection and parameter estimation in
high-dimensional nonstationary VAR models. <em>JASA</em>,
<em>117</em>(537), 251–264. (<a
href="https://doi.org/10.1080/01621459.2020.1770097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assuming stationarity is unrealistic in many time series applications. A more realistic alternative is to assume piecewise stationarity, where the model can change at potentially many change points. We propose a three-stage procedure for simultaneous estimation of change points and parameters of high-dimensional piecewise vector autoregressive (VAR) models. In the first step, we reformulate the change point detection problem as a high-dimensional variable selection one, and solve it using a penalized least square estimator with a total variation penalty. We show that the penalized estimation method over-estimates the number of change points, and propose a selection criterion to identify the change points. In the last step of our procedure, we estimate the VAR parameters in each of the segments. We prove that the proposed procedure consistently detects the number and location of change points, and provides consistent estimates of VAR parameters. The performance of the method is illustrated through several simulated and real data examples. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Abolfazl Safikhani and Ali Shojaie},
  doi          = {10.1080/01621459.2020.1770097},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {251-264},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Joint structural break detection and parameter estimation in high-dimensional nonstationary VAR models},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Smoothing spline semiparametric density models.
<em>JASA</em>, <em>117</em>(537), 237–250. (<a
href="https://doi.org/10.1080/01621459.2020.1769636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density estimation plays a fundamental role in many areas of statistics and machine learning. Parametric, nonparametric, and semiparametric density estimation methods have been proposed in the literature. Semiparametric density models are flexible in incorporating domain knowledge and uncertainty regarding the shape of the density function. Existing literature on semiparametric density models is scattered and lacks a systematic framework. In this article, we consider a unified framework based on reproducing kernel Hilbert space for modeling, estimation, computation, and theory. We propose general semiparametric density models for both a single sample and multiple samples which include many existing semiparametric density models as special cases. We develop penalized likelihood based estimation methods and computational methods under different situations. We establish joint consistency and derive convergence rates of the proposed estimators for both finite dimensional Euclidean parameters and an infinite-dimensional functional parameter. We validate our estimation methods empirically through simulations and an application. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jiahui Yu and Jian Shi and Anna Liu and Yuedong Wang},
  doi          = {10.1080/01621459.2020.1769636},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {237-250},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Smoothing spline semiparametric density models},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating and accounting for unobserved covariates in
high-dimensional correlated data. <em>JASA</em>, <em>117</em>(537),
225–236. (<a
href="https://doi.org/10.1080/01621459.2020.1769635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many high-dimensional and high-throughput biological datasets have complex sample correlation structures, which include longitudinal and multiple tissue data, as well as data with multiple treatment conditions or related individuals. These data, as well as nearly all high-throughput “omic” data, are influenced by technical and biological factors unknown to the researcher, which, if unaccounted for, can severely obfuscate estimation of and inference on the effects of interest. We therefore developed CBCV and CorrConf: provably accurate and computationally efficient methods to choose the number of and estimate latent confounding factors present in high-dimensional data with correlated or nonexchangeable residuals. We demonstrate each method’s superior performance compared to other state of the art methods by analyzing simulated multi-tissue gene expression data and identifying sex-associated DNA methylation sites in a real, longitudinal twin study. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Chris McKennan and Dan Nicolae},
  doi          = {10.1080/01621459.2020.1769635},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {225-236},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimating and accounting for unobserved covariates in high-dimensional correlated data},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). “Generalized measures of correlation for asymmetry,
nonlinearity, and beyond”: Some antecedents on causality. <em>JASA</em>,
<em>117</em>(537), 214–224. (<a
href="https://doi.org/10.1080/01621459.2020.1768101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This note comments on the generalized measure of correlation (GMC) that was suggested by Zheng, Shi, and Zhang. The GMC concept was partly anticipated in some publications over 100 years earlier by Yule in the Proceedings of the Royal Society, and by Kendall. Other antecedents discussed include work on dependency by Renyi and Doksum and Samarov, together with the Yule–Simpson paradox. The GMC metric partly extends the concept of Granger causality, so that we consider causality, graphical analysis and alternative measures of dependency provided by copulas.},
  archive      = {J_JASA},
  author       = {David E. Allen and Michael McAleer},
  doi          = {10.1080/01621459.2020.1768101},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {214-224},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {“Generalized measures of correlation for asymmetry, nonlinearity, and beyond”: Some antecedents on causality},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multiple-testing procedure for high-dimensional mediation
hypotheses. <em>JASA</em>, <em>117</em>(537), 198–213. (<a
href="https://doi.org/10.1080/01621459.2020.1765785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis is of rising interest in epidemiologic studies and clinical trials. Among existing methods, the joint significance test yields an overly conservative Type I error rate and low power, particularly for high-dimensional mediation hypotheses. In this article, we develop a multiple-testing procedure that accurately controls the family-wise error rate (FWER) and the false discovery rate (FDR) when testing high-dimensional mediation hypotheses. The core of our procedure is based on estimating the proportions of component null hypotheses and the underlying mixture null distribution of p -values. Theoretical developments and simulation experiments prove that the proposed procedure effectively controls FWER and FDR. Two mediation analyses on DNA methylation and cancer research are presented: assessing the mediation role of DNA methylation in genetic regulation of gene expression in primary prostate cancer samples; exploring the possibility of DNA methylation mediating the effect of exercise on prostate cancer progression. Results of data examples include well-behaved quantile-quantile plots and improved power to detect novel mediation relationships. An R package HDMT implementing the proposed procedure is freely accessible in CRAN. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {James Y. Dai and Janet L. Stanford and Michael LeBlanc},
  doi          = {10.1080/01621459.2020.1765785},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {198-213},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A multiple-testing procedure for high-dimensional mediation hypotheses},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spike-and-slab group lassos for grouped regression and
sparse generalized additive models. <em>JASA</em>, <em>117</em>(537),
184–197. (<a
href="https://doi.org/10.1080/01621459.2020.1765784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– We introduce the spike-and-slab group lasso (SSGL) for Bayesian estimation and variable selection in linear regression with grouped variables. We further extend the SSGL to sparse generalized additive models (GAMs), thereby introducing the first nonparametric variant of the spike-and-slab lasso methodology. Our model simultaneously performs group selection and estimation, while our fully Bayes treatment of the mixture proportion allows for model complexity control and automatic self-adaptivity to different levels of sparsity. We develop theory to uniquely characterize the global posterior mode under the SSGL and introduce a highly efficient block coordinate ascent algorithm for maximum a posteriori estimation. We further employ de-biasing methods to provide uncertainty quantification of our estimates. Thus, implementation of our model avoids the computational intensiveness of Markov chain Monte Carlo in high dimensions. We derive posterior concentration rates for both grouped linear regression and sparse GAMs when the number of covariates grows at nearly exponential rate with sample size. Finally, we illustrate our methodology through extensive simulations and data analysis. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ray Bai and Gemma E. Moran and Joseph L. Antonelli and Yong Chen and Mary R. Boland},
  doi          = {10.1080/01621459.2020.1765784},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {184-197},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Spike-and-slab group lassos for grouped regression and sparse generalized additive models},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Separable effects for causal inference in the presence of
competing events. <em>JASA</em>, <em>117</em>(537), 175–183. (<a
href="https://doi.org/10.1080/01621459.2020.1765783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In time-to-event settings, the presence of competing events complicates the definition of causal effects. Here we propose the new separable effects to study the causal effect of a treatment on an event of interest. The separable direct effect is the treatment effect on the event of interest not mediated by its effect on the competing event. The separable indirect effect is the treatment effect on the event of interest only through its effect on the competing event. Similar to Robins and Richardson’s extended graphical approach for mediation analysis, the separable effects can only be identified under the assumption that the treatment can be decomposed into two distinct components that exert their effects through distinct causal pathways. Unlike existing definitions of causal effects in the presence of competing events, our estimands do not require cross-world contrasts or hypothetical interventions to prevent death. As an illustration, we apply our approach to a randomized clinical trial on estrogen therapy in individuals with prostate cancer. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Mats J. Stensrud and Jessica G. Young and Vanessa Didelez and James M. Robins and Miguel A. Hernán},
  doi          = {10.1080/01621459.2020.1765783},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {175-183},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Separable effects for causal inference in the presence of competing events},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Testing for balance in social networks. <em>JASA</em>,
<em>117</em>(537), 156–174. (<a
href="https://doi.org/10.1080/01621459.2020.1764850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Friendship and antipathy exist in concert with one another in real social networks. Despite the role they play in social interactions, antagonistic ties are poorly understood and infrequently measured. One important theory of negative ties that has received relatively little empirical evaluation is balance theory, the codification of the adage “the enemy of my enemy is my friend” and similar sayings. Unbalanced triangles are those with an odd number of negative ties, and the theory posits that such triangles are rare. To test for balance, previous works have used a permutation test on the edge signs. The flaw in this method, however, is that it assumes that negative and positive edges are interchangeable. In reality, they could not be more different. Here, we propose a novel test of balance that accounts for this discrepancy and show that our test is more accurate at detecting balance. Along the way, we prove asymptotic normality of the test statistic under our null model, which is of independent interest. Our case study is a novel dataset of signed networks we collected from 32 isolated, rural villages in Honduras. Contrary to previous results, we find that there is only marginal evidence for balance in social tie formation in this setting.},
  archive      = {J_JASA},
  author       = {Derek Feng and Randolf Altmeyer and Derek Stafford and Nicholas A. Christakis and Harrison H. Zhou},
  doi          = {10.1080/01621459.2020.1764850},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {156-174},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Testing for balance in social networks},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spectral inference under complex temporal dynamics.
<em>JASA</em>, <em>117</em>(537), 133–155. (<a
href="https://doi.org/10.1080/01621459.2020.1764365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a unified theory and methodology for the inference of evolutionary Fourier power spectra for a general class of locally stationary and possibly nonlinear processes. In particular, simultaneous confidence regions (SCR) with asymptotically correct coverage rates are constructed for the evolutionary spectral densities on a nearly optimally dense grid of the joint time-frequency domain. A simulation based bootstrap method is proposed to implement the SCR. The SCR enables researchers and practitioners to visually evaluate the magnitude and pattern of the evolutionary power spectra with asymptotically accurate statistical guarantee. The SCR also serves as a unified tool for a wide range of statistical inference problems in time-frequency analysis ranging from tests for white noise, stationarity and time-frequency separability to the validation for non-stationary linear models.},
  archive      = {J_JASA},
  author       = {Jun Yang and Zhou Zhou},
  doi          = {10.1080/01621459.2020.1764365},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {133-155},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Spectral inference under complex temporal dynamics},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Rejoinder. <em>JASA</em>, <em>117</em>(537), 128–132. (<a
href="https://doi.org/10.1080/01621459.2022.2035099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Rong Chen and Dan Yang and Cun-Hui Zhang},
  doi          = {10.1080/01621459.2022.2035099},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {128-132},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rejoinder},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comments on “factor models for high-dimensional tensor time
series.” <em>JASA</em>, <em>117</em>(537), 124–127. (<a
href="https://doi.org/10.1080/01621459.2022.2028630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Jialin Ouyang and Ming Yuan},
  doi          = {10.1080/01621459.2022.2028630},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {124-127},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comments on “Factor models for high-dimensional tensor time series”},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comment on “factor models for high-dimensional tensor time
series.” <em>JASA</em>, <em>117</em>(537), 118–123. (<a
href="https://doi.org/10.1080/01621459.2021.2024214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Daniel Peña},
  doi          = {10.1080/01621459.2021.2024214},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {118-123},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comment on “Factor models for high-dimensional tensor time series”},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comment on “factor models for high-dimensional tensor time
series” by rong chen, dan yang, and cun-hui zhang. <em>JASA</em>,
<em>117</em>(537), 117. (<a
href="https://doi.org/10.1080/01621459.2021.2018328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Oliver B. Linton and Haihan Tang},
  doi          = {10.1080/01621459.2021.2018328},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {117},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comment on “Factor models for high-dimensional tensor time series” by rong chen, dan yang, and cun-hui zhang},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Factor models for high-dimensional tensor time series.
<em>JASA</em>, <em>117</em>(537), 94–116. (<a
href="https://doi.org/10.1080/01621459.2021.1912757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large tensor (multi-dimensional array) data routinely appear nowadays in a wide range of applications, due to modern data collection capabilities. Often such observations are taken over time, forming tensor time series. In this article we present a factor model approach to the analysis of high-dimensional dynamic tensor time series and multi-category dynamic transport networks. This article presents two estimation procedures along with their theoretical properties and simulation results. We present two applications to illustrate the model and its interpretations.},
  archive      = {J_JASA},
  author       = {Rong Chen and Dan Yang and Cun-Hui Zhang},
  doi          = {10.1080/01621459.2021.1912757},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {94-116},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Factor models for high-dimensional tensor time series},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Earthquake risk embedded in property prices: Evidence from
five japanese cities. <em>JASA</em>, <em>117</em>(537), 82–93. (<a
href="https://doi.org/10.1080/01621459.2021.1928512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the impact of short-run (90 days) and long-run (30 years) earthquake risk on real estate transaction prices in five Japanese cities (Tokyo, Osaka, Nagoya, Fukuoka, and Sapporo), using quarterly data over the period 2006–2015. We exploit a rich panel dataset (331,343 observations) with property characteristics, ward attractiveness information, macroeconomic variables, and long-run seismic hazard data, supplemented with short-run earthquake probabilities generated from a seismic excitation model using historical earthquake occurrences. We design a hedonic property price model that allows for subjective probability weighting, employ a multivariate error components structure, and develop associated maximum likelihood estimation and variance computation procedures. Our approach enables us to identify the total compensation for earthquake risk embedded in property prices, to decompose this into pieces stemming from short-run and long-run risk, and to distinguish between objective and subjectively weighted (“distorted”) earthquake probabilities. We find that objective long-run earthquake probabilities have a statistically significant negative impact on property prices, whereas short-run earthquake probabilities become statistically significant only when we allow them to be distorted. The total compensation for earthquake risk amounts to an average –2.0\% of log property prices, slightly more than the annual income of a middle-income Japanese household. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Masako Ikefuji and Roger J. A. Laeven and Jan R. Magnus and Yuan Yue},
  doi          = {10.1080/01621459.2021.1928512},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {82-93},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Earthquake risk embedded in property prices: Evidence from five japanese cities},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Large-scale hypothesis testing for causal mediation effects
with applications in genome-wide epigenetic studies. <em>JASA</em>,
<em>117</em>(537), 67–81. (<a
href="https://doi.org/10.1080/01621459.2021.1914634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In genome-wide epigenetic studies, it is of great scientific interest to assess whether the effect of an exposure on a clinical outcome is mediated through DNA methylations. However, statistical inference for causal mediation effects is challenged by the fact that one needs to test a large number of composite null hypotheses across the whole epigenome. Two popular tests, the Wald-type Sobel’s test and the joint significant test using the traditional null distribution are underpowered and thus can miss important scientific discoveries. In this article, we show that the null distribution of Sobel’s test is not the standard normal distribution and the null distribution of the joint significant test is not uniform under the composite null of no mediation effect, especially in finite samples and under the singular point null case that the exposure has no effect on the mediator and the mediator has no effect on the outcome. Our results explain why these two tests are underpowered, and more importantly motivate us to develop a more powerful divide-aggregate composite-null test (DACT) for the composite null hypothesis of no mediation effect by leveraging epigenome-wide data. We adopted Efron’s empirical null framework for assessing statistical significance of the DACT test. We showed analytically that the proposed DACT method had improved power, and could well control Type I error rate. Our extensive simulation studies showed that, in finite samples, the DACT method properly controlled the Type I error rate and outperformed Sobel’s test and the joint significance test for detecting mediation effects. We applied the DACT method to the U.S. Department of Veterans Affairs Normative Aging Study, an ongoing prospective cohort study which included men who were aged 21 to 80 years at entry. We identified multiple DNA methylation CpG sites that might mediate the effect of smoking on lung function with effect sizes ranging from –0.18 to –0.79 and false discovery rate controlled at the level 0.05, including the CpG sites in the genes AHRR and F2RL3. Our sensitivity analysis found small residual correlations (less than 0.01) of the error terms between the outcome and mediator regressions, suggesting that our results are robust to unmeasured confounding factors. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Zhonghua Liu and Jincheng Shen and Richard Barfield and Joel Schwartz and Andrea A. Baccarelli and Xihong Lin},
  doi          = {10.1080/01621459.2021.1914634},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {67-81},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Large-scale hypothesis testing for causal mediation effects with applications in genome-wide epigenetic studies},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prioritizing autism risk genes using personalized graphical
models estimated from single-cell RNA-seq data. <em>JASA</em>,
<em>117</em>(537), 38–51. (<a
href="https://doi.org/10.1080/01621459.2021.1933495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hundreds of autism risk genes have been reported recently, mainly based on genetic studies where these risk genes have more de novo mutations in autism subjects than healthy controls. However, as a complex disease, autism is likely associated with more risk genes and many of them may not be identifiable through de novo mutations. We hypothesize that more autism risk genes can be identified through their connections with known autism risk genes in personalized gene–gene interaction graphs. We estimate such personalized graphs using single-cell RNA sequencing (scRNA-seq) while appropriately modeling the cell dependence and possible zero-inflation in the scRNA-seq data. The sample size, which is the number of cells per individual, ranges from 891 to 1241 in our case study using scRNA-seq data in autism subjects and controls. We consider 1500 genes in our analysis. Since the number of genes is larger or comparable to the sample size, we perform penalized estimation. We score each gene’s relevance by applying a simple graph kernel smoothing method to each personalized graph. The molecular functions of the top-scored genes are related to autism diseases. For example, a candidate gene RYR2 that encodes protein ryanodine receptor 2 is involved in neurotransmission, a process that is impaired in ASD patients. While our method provides a systemic and unbiased approach to prioritize autism risk genes, the relevance of these genes needs to be further validated in functional studies. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Jianyu Liu and Haodong Wang and Wei Sun and Yufeng Liu},
  doi          = {10.1080/01621459.2021.1933495},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {38-51},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Prioritizing autism risk genes using personalized graphical models estimated from single-cell RNA-seq data},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling the marked presence-only data: A case study of
estimating the female sex worker size in malawi. <em>JASA</em>,
<em>117</em>(537), 27–37. (<a
href="https://doi.org/10.1080/01621459.2021.1944873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Certain subpopulations like female sex workers (FSW), men who have sex with men (MSM), and people who inject drugs (PWID) often have higher prevalence of HIV/AIDS and are difficult to map directly due to stigma, discrimination, and criminalization. Fine-scale mapping of those populations contributes to the progress toward reducing the inequalities and ending the AIDS epidemic. In 2016 and 2017, the PLACE surveys were conducted at 3290 venues in 20 out of the total 28 districts in Malawi to estimate the FSW sizes. These venues represent a presence-only dataset where, instead of knowing both where people live and do not live (presence–absence data), only information about visited locations is available. In this study, we develop a Bayesian model for presence-only data and utilize the PLACE data to estimate the FSW size and uncertainty interval at a 1.5 × 1.5 -km resolution for all of Malawi. The estimates can also be aggregated to any desirable level (city/district/region) for implementing targeted HIV prevention and treatment programs in FSW communities, which have been successful in lowering the incidence of HIV and other sexually transmitted infections. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Ian Laga and Xiaoyue Niu and Le Bao},
  doi          = {10.1080/01621459.2021.1944873},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {27-37},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Modeling the marked presence-only data: A case study of estimating the female sex worker size in malawi},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Elucidating age and sex-dependent association between
frontal EEG asymmetry and depression: An application of multiple
imputation in functional regression. <em>JASA</em>, <em>117</em>(537),
12–26. (<a href="https://doi.org/10.1080/01621459.2021.1942011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frontal power asymmetry (FA), a measure of brain function derived from electroencephalography, is a potential biomarker for major depressive disorder (MDD). Though FA is functional in nature, it is typically reduced to a scalar value prior to analysis, possibly obscuring its relationship with MDD and leading to a number of studies that have provided contradictory results. To overcome this issue, we sought to fit a functional regression model to characterize the association between FA and MDD status, adjusting for age, sex, cognitive ability, and handedness using data from a large clinical study that included both MDD and healthy control (HC) subjects. Since nearly 40\% of the observations are missing data on either FA or cognitive ability, we propose an extension of multiple imputation (MI) by chained equations that allows for the imputation of both scalar and functional data. We also propose an extension of Rubin’s Rules for conducting valid inference in this setting. The proposed methods are evaluated in a simulation and applied to our FA data. For our FA data, a pooled analysis from the imputed datasets yielded similar results to those of the complete case analysis. We found that, among young females, HCs tended to have higher FA over the θ , α , and β frequency bands, but that the difference between HC and MDD subjects diminishes and ultimately reverses with age. For males, HCs tended to have higher FA in the β frequency band, regardless of age. Young male HCs had higher FA in the θ and α bands, but this difference diminishes with increasing age in the α band and ultimately reverses with increasing age in the θ band. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Adam Ciarleglio and Eva Petkova and Ofer Harel},
  doi          = {10.1080/01621459.2021.1942011},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {12-26},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Elucidating age and sex-dependent association between frontal EEG asymmetry and depression: An application of multiple imputation in functional regression},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On genetic correlation estimation with summary statistics
from genome-wide association studies. <em>JASA</em>, <em>117</em>(537),
1–11. (<a href="https://doi.org/10.1080/01621459.2021.1906684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-trait polygenic risk score (PRS) method has gained popularity for assessing genetic correlation of complex traits using summary statistics from biobank-scale genome-wide association studies (GWAS). However, empirical evidence has shown a common bias phenomenon that highly significant cross-trait PRS can only account for a very small amount of genetic variance ( R 2 can be &amp;lt;1\% ) in independent testing GWAS. The aim of this paper is to investigate and address the bias phenomenon of cross-trait PRS in numerous GWAS applications. We show that the estimated genetic correlation can be asymptotically biased toward zero. A consistent cross-trait PRS estimator is then proposed to correct such asymptotic bias. In addition, we investigate whether or not SNP screening by GWAS p -values can lead to improved estimation and show the effect of overlapping samples among GWAS. We analyze GWAS summary statistics of reaction time and brain structural magnetic resonance imaging-based features measured in the Pediatric Imaging, Neurocognition, and Genetics study. We find that the raw cross-trait PRS estimators heavily underestimate the genetic similarity between cognitive function and human brain structures (mean R 2 = 1.32\% ), whereas the bias-corrected estimators uncover the moderate degree of genetic overlap between these closely related heritable traits (mean R 2 = 22.42\% ). Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Bingxin Zhao and Hongtu Zhu},
  doi          = {10.1080/01621459.2021.1906684},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {1-11},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {On genetic correlation estimation with summary statistics from genome-wide association studies},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
