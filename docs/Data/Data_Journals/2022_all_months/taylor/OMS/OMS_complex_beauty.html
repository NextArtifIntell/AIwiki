<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>OMS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="oms---89">OMS - 89</h2>
<ul>
<li><details>
<summary>
(2022). Sequential approximate optimization with adaptive parallel
infill strategy assisted by inaccurate pareto front. <em>OMS</em>,
<em>37</em>(6), 2352–2376. (<a
href="https://doi.org/10.1080/10556788.2022.2091560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential Approximate Optimization (SAO) has been widely used in engineering optimization design problems to improve efficiency. The infilling strategy is one of the critical techniques of the SAO, which is of paramount importance to the surrogate model accuracy and optimization efficiency. In this paper, an adaptive parallel infill strategy for surrogate-based single-objective optimization is proposed within a multi-objective optimization framework to balance exploration and exploitation during the optimization process. Within this method, an inaccurate Pareto Front is adopted to assist the infilling of the sampling points. The proposed SAO method with its adaptive parallel sampling strategy is tested on several numerical test cases and an engineering test case with the optimization results compared to state-of-the-art optimization algorithms. The results show that the proposed SAO with the adaptive parallel sampling strategy possesses excellent performance and better stability.},
  archive      = {J_OMS},
  author       = {Wenjie Wang and Pengyu Wang and Jiawei Yang and Fei Xiao and Weihua Zhang and Zeping Wu},
  doi          = {10.1080/10556788.2022.2091560},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {2352-2376},
  shortjournal = {Optim. Methods Softw.},
  title        = {Sequential approximate optimization with adaptive parallel infill strategy assisted by inaccurate pareto front},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exact gradient methods with memory. <em>OMS</em>,
<em>37</em>(6), 2324–2351. (<a
href="https://doi.org/10.1080/10556788.2022.2091559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Inexact Gradient Method with Memory (IGMM) is able to considerably outperform the Gradient Method by employing a piece-wise linear lower model on the smooth part of the objective. However, the auxiliary problem can only be solved within a fixed tolerance at every iteration. The need to contain the inexactness narrows the range of problems to which IGMM can be applied and degrades the worst-case convergence rate. In this work, we show how a simple modification of IGMM removes the tolerance parameter from the analysis. The resulting Exact Gradient Method with Memory (EGMM) is as broadly applicable as the Bregman Distance Gradient Method/NoLips and has the same worst-case rate of O ( 1 / k ) , the best for its class. Under necessarily stricter assumptions, we can accelerate EGMM without error accumulation yielding an Accelerated Gradient Method with Memory (AGMM) possessing a worst-case rate of O ( 1 / k 2 ) . In our preliminary computational experiments EGMM displays excellent performance, sometimes surpassing accelerated methods. When the model discards old information, AGMM also consistently exceeds the Fast Gradient Method.},
  archive      = {J_OMS},
  author       = {Mihai I. Florea},
  doi          = {10.1080/10556788.2022.2091559},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {2324-2351},
  shortjournal = {Optim. Methods Softw.},
  title        = {Exact gradient methods with memory},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Operator splitting for adaptive radiation therapy with
nonlinear health dynamics. <em>OMS</em>, <em>37</em>(6), 2300–2323. (<a
href="https://doi.org/10.1080/10556788.2022.2078824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an optimization-based approach to radiation treatment planning over time. Our approach formulates treatment planning as an optimal control problem with nonlinear patient health dynamics derived from the standard linear-quadratic cell survival model. As the formulation is nonconvex, we propose a method for obtaining an approximate solution by solving a sequence of convex optimization problems. This method is fast, efficient, and robust to model error, adapting readily to changes in the patient&#39;s health between treatment sessions. Moreover, we show that it can be combined with the operator splitting method ADMM to produce an algorithm that is highly scalable and can handle large clinical cases. We introduce an open-source Python implementation of our algorithm, AdaRad, and demonstrate its performance on several examples.},
  archive      = {J_OMS},
  author       = {Anqi Fu and Lei Xing and Stephen Boyd},
  doi          = {10.1080/10556788.2022.2078824},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {2300-2323},
  shortjournal = {Optim. Methods Softw.},
  title        = {Operator splitting for adaptive radiation therapy with nonlinear health dynamics},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A quasi-newton method in shape optimization for a
transmission problem. <em>OMS</em>, <em>37</em>(6), 2273–2299. (<a
href="https://doi.org/10.1080/10556788.2022.2078823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider optimal design problems in stationary diffusion for mixtures of two isotropic phases. The goal is to find an optimal distribution of the phases such that the energy functional is maximized. By following the identity perturbation method, we calculate the first- and second-order shape derivatives in the distributional representation under weak regularity assumptions. Ascent methods based on the distributed first- and second-order shape derivatives are implemented and tested in classes of problems for which the classical solutions exist and can be explicitly calculated from the optimality conditions. A proposed quasi-Newton method offers a better ascent vector compared to gradient methods, reaching the optimal design in half as many steps. The method applies well also for multiple state problems.},
  archive      = {J_OMS},
  author       = {Petar Kunštek and Marko Vrdoljak},
  doi          = {10.1080/10556788.2022.2078823},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {2273-2299},
  shortjournal = {Optim. Methods Softw.},
  title        = {A quasi-newton method in shape optimization for a transmission problem},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Isotonicity of the proximity operator and stochastic
optimization problems in hilbert quasi-lattices endowed with lorentz
cones. <em>OMS</em>, <em>37</em>(6), 2251–2272. (<a
href="https://doi.org/10.1080/10556788.2022.2064467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we discuss the isotonicity of the proximity operator in Hilbert quasi-lattices endowed with different Lorentz cones. The extended Lorentz cone is first defined by the Minkowski functionals of some subsets. We then establish some sufficient conditions for the isotonicity of the proximity operator concerning one order and two mutually dual orders induced by Lorentz cones, respectively. Similarly, the cases of the extended Lorentz cones and other ordered inequality properties of the proximity operator are analysed. By adopting these characterizations, some solvability and iterative algorithm theorems for the stochastic optimization problem are established by different order approaches. For solvability, the gradient of the mappings does not need to be continuous, and the solutions are optimal with respect to the orders. In the stochastic proximal algorithms, the mappings satisfy inequality conditions just for comparable elements, but the convergence direction and convergence rate are more optimal.},
  archive      = {J_OMS},
  author       = {Dezhou Kong and Li Sun and Haibin Chen and Yun Wang},
  doi          = {10.1080/10556788.2022.2064467},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {2251-2272},
  shortjournal = {Optim. Methods Softw.},
  title        = {Isotonicity of the proximity operator and stochastic optimization problems in hilbert quasi-lattices endowed with lorentz cones},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Addendum to the paper “nonsmooth DC-constrained
optimization: Constraint qualification and minimizing methodologies.”
<em>OMS</em>, <em>37</em>(6), 2241–2250. (<a
href="https://doi.org/10.1080/10556788.2022.2063861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this addendum to our paper published in [Optim. Meth. Softw. 34(1) (2019), pp. 890–920], we provide more details concerning Proposition 2.1 figuring therein. The original proposition implicitly assumed that the constraint mapping was Clarke-regular. Here, not only do we fix some inaccuracies and confusing statements in Section 2 of that paper, but we also provide an extension of our previous results well beyond that setting.},
  archive      = {J_OMS},
  author       = {W. van Ackooij and W. de Oliveira},
  doi          = {10.1080/10556788.2022.2063861},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {2241-2250},
  shortjournal = {Optim. Methods Softw.},
  title        = {Addendum to the paper ‘Nonsmooth DC-constrained optimization: Constraint qualification and minimizing methodologies’},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quadratic rates of asymptotic regularity for the
tikhonov–mann iteration. <em>OMS</em>, <em>37</em>(6), 2225–2240. (<a
href="https://doi.org/10.1080/10556788.2022.2060974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we compute quadratic rates of asymptotic regularity for the Tikhonov–Mann iteration in W -hyperbolic spaces. This iteration is an extension to a nonlinear setting of the modified Mann iteration defined recently by Boţ, Csetnek and Meier in Hilbert spaces. Furthermore, we show that the Douglas–Rachford and forward-backward algorithms with Tikhonov regularization terms are special cases, in Hilbert spaces, of our Tikhonov–Mann iteration.},
  archive      = {J_OMS},
  author       = {Horaţiu Cheval and Laurenţiu Leuştean},
  doi          = {10.1080/10556788.2022.2060974},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {2225-2240},
  shortjournal = {Optim. Methods Softw.},
  title        = {Quadratic rates of asymptotic regularity for the Tikhonov–Mann iteration},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convergence to a second-order critical point by a
primal-dual interior point trust-region method for nonlinear
semidefinite programming. <em>OMS</em>, <em>37</em>(6), 2190–2224. (<a
href="https://doi.org/10.1080/10556788.2022.2060973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a primal-dual interior point trust-region method for solving nonlinear semidefinite programming problems, in which the iterates converge to a point that satisfies the first-order and second-order optimality conditions. The method consists of the outer iteration (SDPIP-revised) that finds a Karush-Kuhn-Tucker (KKT) point which satisfies the second-order optimality condition, and the inner iteration (SDPTR-revised) that calculates an approximate barrier KKT point. Algorithm SDPTR-revised uses a commutative class of Newton-like directions within the framework of the trust-region method in the primal-dual space. In addition, we also use a direction of negative curvature when it exists. The proposed algorithm employs a new method that generates negative-curvature directions in the existence of l 1 -type penalty term for equality constraints. It is proved that there exists a limit point of the generated sequence which satisfies the second-order optimality condition along with the barrier KKT conditions.},
  archive      = {J_OMS},
  author       = {Hiroshi Yamashita},
  doi          = {10.1080/10556788.2022.2060973},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {2190-2224},
  shortjournal = {Optim. Methods Softw.},
  title        = {Convergence to a second-order critical point by a primal-dual interior point trust-region method for nonlinear semidefinite programming},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A penalty decomposition approach for multi-objective
cardinality-constrained optimization problems. <em>OMS</em>,
<em>37</em>(6), 2157–2189. (<a
href="https://doi.org/10.1080/10556788.2022.2060972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this manuscript, we consider multi-objective optimization problems with a cardinality constraint on the vector of decision variables and additional linear constraints. For this class of problems, we analyse necessary and sufficient conditions of Pareto optimality. We afterwards propose a Penalty Decomposition type algorithm, exploiting multi-objective descent methods, to tackle the aforementioned family of problems. We conduct a rigorous convergence analysis for the proposed method, where we prove that the produced sequence of points has limit points, each one being feasible and satisfying first-order optimality conditions. Numerical computational experiments, carried out on instances of relevant real-world problems such as sparse mean/variance portfolio selection and sparse regularized logistic regression, in their multi-objective formulation, show that the proposed procedure is effective at finding solutions forming good Pareto sets approximations.},
  archive      = {J_OMS},
  author       = {Matteo Lapucci},
  doi          = {10.1080/10556788.2022.2060972},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {2157-2189},
  shortjournal = {Optim. Methods Softw.},
  title        = {A penalty decomposition approach for multi-objective cardinality-constrained optimization problems},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using first-order information in direct multisearch for
multiobjective optimization. <em>OMS</em>, <em>37</em>(6), 2135–2156.
(<a href="https://doi.org/10.1080/10556788.2022.2060971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Derivatives are an important tool for single-objective optimization. In fact, it is commonly accepted that derivative-based methods present a better performance than derivative-free optimization approaches. In this work, we will show that the same does not always apply to multiobjective derivative-based optimization, when the goal is to compute an approximation to the complete Pareto front of a given problem. The competitiveness of direct multisearch (DMS), a robust and efficient derivative-free optimization algorithm, will be stated for derivative-based multiobjective optimization (MOO) problems, by comparison with MOSQP, a state-of-art derivative-based MOO solver. We will then assess the potential enrichment of adding first-order information to the DMS framework. Derivatives will be used to prune the positive spanning sets considered at the poll step of the algorithm. The role of ascent directions, that conform to the geometry of the nearby feasible region, will then be highlighted.},
  archive      = {J_OMS},
  author       = {R. Andreani and A. L. Custódio and M. Raydan},
  doi          = {10.1080/10556788.2022.2060971},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {2135-2156},
  shortjournal = {Optim. Methods Softw.},
  title        = {Using first-order information in direct multisearch for multiobjective optimization},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A wide neighbourhood predictor–corrector
infeasible-interior-point algorithm for symmetric cone programming.
<em>OMS</em>, <em>37</em>(6), 2117–2134. (<a
href="https://doi.org/10.1080/10556788.2022.2060970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new predictor–corrector infeasible-interior-point algorithm for symmetric cone programming. Each iterate always follows the usual wide neighbourhood 𝒩 − ∞ N ∞ − N∞− , it does not necessarily stay within it but must stay within the wider neighbourhood 𝒩 ( τ , β ) N ( τ , β ) N(τ,β) . We prove that, besides the predictor step, each corrector step also reduces the duality gap by a rate of 1 − 1 O ( r √ ) 1 − 1 O ( r ) 1−1O(r) , where r is the rank of the associated Euclidean Jordan algebra. Moreover, we improve the theoretical complexity bound of an infeasible-interior-point method. Some numerical results are provided as well.},
  archive      = {J_OMS},
  author       = {M. Sayadi Shahraki and H. Mansouri and A. Delavarkhalafi},
  doi          = {10.1080/10556788.2022.2060970},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {2117-2134},
  shortjournal = {Optim. Methods Softw.},
  title        = {A wide neighbourhood predictor–corrector infeasible-interior-point algorithm for symmetric cone programming},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Length-constrained cycle partition with an application to
UAV routing*. <em>OMS</em>, <em>37</em>(6), 2080–2116. (<a
href="https://doi.org/10.1080/10556788.2022.2053972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article discusses the Length-Constrained Cycle Partition Problem (LCCP), which constitutes a new generalization of the Travelling Salesperson Problem (TSP). Apart from nonnegative edge weights, the undirected graph in LCCP features a nonnegative critical length parameter for each vertex. A cycle partition, i.e. a vertex-disjoint cycle cover, is a feasible solution for LCCP if the length of each cycle is not greater than the critical length of each vertex contained in it. The goal is to find a feasible partition having a minimum number of cycles. Besides analyzing theoretical properties and developing preprocessing techniques, we propose an elaborate heuristic algorithm that produces solutions of good quality even for large-size instances. Moreover, we present two exact mixed-integer programming formulations (MIPs) for LCCP, which are inspired by well-known modeling approaches for TSP. Further, we introduce the concept of conflict hypergraphs, whose cliques yield valid constraints for the MIP models. We conclude with a discussion on computational experiments that we conducted using (A)TSPLIB-based problem instances. As a motivating example application, we describe a routing problem where a fleet of uncrewed aerial vehicles (UAVs) must patrol a given set of areas.},
  archive      = {J_OMS},
  author       = {Kai Hoppmann-Baum and Oleg Burdakov and Gioni Mexi and Carl Johan Casselgren and Thorsten Koch},
  doi          = {10.1080/10556788.2022.2053972},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {2080-2116},
  shortjournal = {Optim. Methods Softw.},
  title        = {Length-constrained cycle partition with an application to UAV routing*},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Primal-dual algorithms for multi-agent structured
optimization over message-passing architectures with bounded
communication delays. <em>OMS</em>, <em>37</em>(6), 2052–2079. (<a
href="https://doi.org/10.1080/10556788.2021.2023524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider algorithms for solving structured convex optimization problems over a network of agents with communication delays. It is assumed that each agent performs its local updates using possibly outdated information from its neighbours under the assumption that the delay with respect to each neighbour is bounded but otherwise arbitrary. The private objective of each agent is represented by the sum of two possibly nonsmooth functions one of which is composed with a linear mapping. The global optimization problem is the aggregate of local cost functions and a common Lipschitz-differentiable function. When the coupling between agents is represented only through the common function, the V\~u-Condat primal-dual algorithm is studied. In the case when the linear maps introduce additional coupling between agents a new algorithm is developed. Moreover, a randomized variant of this algorithm is presented that allows the agents to wake up at random and independently from one another. The convergence of the proposed algorithms is established under strong convexity assumptions.},
  archive      = {J_OMS},
  author       = {Puya Latafat and Panagiotis Patrinos},
  doi          = {10.1080/10556788.2021.2023524},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {2052-2079},
  shortjournal = {Optim. Methods Softw.},
  title        = {Primal-dual algorithms for multi-agent structured optimization over message-passing architectures with bounded communication delays},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variants of the a-HPE and large-step a-HPE algorithms for
strongly convex problems with applications to accelerated high-order
tensor methods. <em>OMS</em>, <em>37</em>(6), 2021–2051. (<a
href="https://doi.org/10.1080/10556788.2021.2022148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For solving strongly convex optimization problems, we propose and study the global convergence of variants of the accelerated hybrid proximal extragradient (A-HPE) and large-step A-HPE algorithms of R.D.C. Monteiro and B.F. Svaiter [An accelerated hybrid proximal extragradient method for convex optimization and its implications to second-order methods, SIAM J. Optim. 23 (2013), pp. 1092–1125.]. We prove linear and the superlinear O ( k − k ( p − 1 p + 1 ) ) global rates for the proposed variants of the A-HPE and large-step A-HPE methods, respectively. The parameter p ≥ 2 appears in the (high-order) large-step condition of the new large-step A-HPE algorithm. We apply our results to high-order tensor methods, obtaining a new inexact (relative-error) tensor method for (smooth) strongly convex optimization with iteration-complexity O ( k − k ( p − 1 p + 1 ) ) . In particular, for p = 2, we obtain an inexact proximal-Newton algorithm with fast global O ( k − k / 3 ) convergence rate.},
  archive      = {J_OMS},
  author       = {M. Marques Alves},
  doi          = {10.1080/10556788.2021.2022148},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {2021-2051},
  shortjournal = {Optim. Methods Softw.},
  title        = {Variants of the A-HPE and large-step A-HPE algorithms for strongly convex problems with applications to accelerated high-order tensor methods},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Jordan symmetry reduction for conic optimization over the
doubly nonnegative cone: Theory and software. <em>OMS</em>,
<em>37</em>(6), 2001–2020. (<a
href="https://doi.org/10.1080/10556788.2021.2022146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common computational approach for polynomial optimization problems (POPs) is to use (hierarchies of) semidefinite programming (SDP) relaxations. When the variables in the POP are required to be nonnegative – as is the case for combinatorial optimization problems, for example – these SDP problems typically involve nonnegative matrices, i.e. they are conic optimization problems over the doubly nonnegative cone. The Jordan reduction, a symmetry reduction method for conic optimization, was recently introduced for symmetric cones by Parrilo and Permenter [Mathematical Programming 181(1), 2020]. We extend this method to the doubly nonnegative cone, and investigate its application to known relaxations of the quadratic assignment and maximum stable set problems. We also introduce new Julia software where the symmetry reduction is implemented.},
  archive      = {J_OMS},
  author       = {Daniel Brosch and Etienne de Klerk},
  doi          = {10.1080/10556788.2021.2022146},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {2001-2020},
  shortjournal = {Optim. Methods Softw.},
  title        = {Jordan symmetry reduction for conic optimization over the doubly nonnegative cone: Theory and software},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A symmetric grouped and ordered multi-secant quasi-newton
update formula. <em>OMS</em>, <em>37</em>(6), 1979–2000. (<a
href="https://doi.org/10.1080/10556788.2022.2053970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For Quasi-Newton methods, one of the most important challenges is to find an estimate of the Jacobian matrix as close as possible to the real matrix. While in root-finding problems multi-secant methods are regularly used, in optimization, it is the symmetric methods (in particular BFGS) that are popular. Combining multi-secant and symmetric methods in one single update formula would combine their benefits. However, it can be proved that the symmetry and multi-secant property are generally not compatible. In this paper, we try to work around this impossibility and approach the combination of both properties into a single update formula. The novelty of our method is to group secant equations based on their relative importance and to order those groups. This leads to a generic formulation of a symmetric Quasi-Newton method that is as close as possible to satisfying multiple secant equations. Our new update formula is modular and can be used in different applications where multiple secant equations, coming from different sources, are available. The formulation encompasses also different existing Quasi-Newton symmetric update formulas that try to approach the multi-secant property.},
  archive      = {J_OMS},
  author       = {Nicolas Boutet and Joris Degroote and Rob Haelterman},
  doi          = {10.1080/10556788.2022.2053970},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {1979-2000},
  shortjournal = {Optim. Methods Softw.},
  title        = {A symmetric grouped and ordered multi-secant quasi-newton update formula},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal order multigrid preconditioners for the distributed
control of parabolic equations with coarsening in space and time.
<em>OMS</em>, <em>37</em>(5), 1930–1964. (<a
href="https://doi.org/10.1080/10556788.2021.2022145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We devise multigrid preconditioners for linear-quadratic space-time distributed parabolic optimal control problems. While our method is rooted in earlier work on elliptic control, the temporal dimension presents new challenges in terms of algorithm design and quality. Our primary focus is on the cG( s )dG( r ) discretizations which are based on functions that are continuous in space and discontinuous in time, but our technique is applicable to various other space-time finite element discretizations. We construct and analyse two kinds of multigrid preconditioners: the first is based on full coarsening in space and time, while the second is based on semi-coarsening in space only. Our analysis, in conjunction with numerical experiments, shows that both preconditioners are of optimal order with respect to the discretization in case of cG(1)dG( r ) for r = 0, 1 and exhibit a suboptimal behaviour in time for Crank–Nicolson. We also show that, under certain conditions, the preconditioner using full space-time coarsening is more efficient than the one involving semi-coarsening in space, a phenomenon that has not been observed previously. Our numerical results confirm the theoretical findings.},
  archive      = {J_OMS},
  author       = {Andrei Drăgănescu and Mona Hajghassem},
  doi          = {10.1080/10556788.2021.2022145},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1930-1964},
  shortjournal = {Optim. Methods Softw.},
  title        = {Optimal order multigrid preconditioners for the distributed control of parabolic equations with coarsening in space and time},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved high-dimensional regression models with matrix
approximations applied to the comparative case studies with support
vector machines. <em>OMS</em>, <em>37</em>(5), 1912–1929. (<a
href="https://doi.org/10.1080/10556788.2021.2022144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, high-dimensional data appear in many practical applications such as biosciences. In the regression analysis literature, the well-known ordinary least-squares estimation may be misleading when the full ranking of the design matrix is  missed. As a popular issue, outliers may corrupt normal distribution of the residuals. Thus, since not being sensitive to the outlying data points, robust estimators are frequently applied in confrontation with the issue. Ill-conditioning in high-dimensional data is another common problem in modern regression analysis under which applying the least-squares estimator is hardly possible. So, it is necessary to deal with estimation methods to tackle these problems. As known, a successful approach for high-dimension cases is the penalized scheme with the aim of obtaining a subset of effective explanatory variables that predict the response as the best, while setting the other parameters to zero. Here, we develop several penalized mixed-integer nonlinear programming models to be used in high-dimension regression analysis. The given matrix approximations have simple structures, decreasing computational cost of the models. Moreover, the models are effectively solvable by metaheuristic algorithms. Numerical tests are made to shed light on performance of the proposed methods on simulated and real world high-dimensional data sets.},
  archive      = {J_OMS},
  author       = {Mahdi Roozbeh and Saman Babaie-Kafaki and Zohre Aminifard},
  doi          = {10.1080/10556788.2021.2022144},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1912-1929},
  shortjournal = {Optim. Methods Softw.},
  title        = {Improved high-dimensional regression models with matrix approximations applied to the comparative case studies with support vector machines},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven distributionally robust risk parity portfolio
optimization. <em>OMS</em>, <em>37</em>(5), 1876–1911. (<a
href="https://doi.org/10.1080/10556788.2021.2022143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a distributionally robust formulation of the traditional risk parity portfolio optimization problem. Distributional robustness is introduced by targeting the discrete probabilities attached to each observation used during parameter estimation. Instead of assuming that all observations are equally likely, we consider an ambiguity set that provides us with the flexibility to find the most adversarial probability distribution based on the investor&#39;s desired degree of robustness. This allows us to derive robust estimates to parametrize the distribution of asset returns without having to impose any particular structure on the data. The resulting distributionally robust optimization problem is a constrained convex–concave minimax problem. Our approach is financially meaningful and attempts to attain full risk diversification with respect to the worst-case instance of the portfolio risk measure. We propose a novel algorithmic approach to solve this minimax problem, which blends projected gradient ascent with sequential convex programming. This algorithm is highly flexible and allows the user to choose among alternative statistical distance measures to define the ambiguity set. Moreover, the algorithm is highly tractable and scalable. Our numerical experiments suggest that a distributionally robust risk parity portfolio can yield a higher risk-adjusted rate of return when compared against the nominal portfolio.},
  archive      = {J_OMS},
  author       = {Giorgio Costa and Roy H. Kwon},
  doi          = {10.1080/10556788.2021.2022143},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1876-1911},
  shortjournal = {Optim. Methods Softw.},
  title        = {Data-driven distributionally robust risk parity portfolio optimization},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stochastic block projection algorithms with extrapolation
for convex feasibility problems. <em>OMS</em>, <em>37</em>(5),
1845–1875. (<a
href="https://doi.org/10.1080/10556788.2021.1998492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stochastic alternating projection (SP) algorithm is a simple but powerful approach for solving convex feasibility problems. At each step, the method projects the current iterate onto a random individual set from the intersection. Hence, it has simple iteration, but, usually, convergences slowly. In this paper, we develop accelerated variants of basic SP method. We achieve acceleration using two ingredients: blocks of sets and adaptive extrapolation. We propose SP-based algorithms based on extrapolated iterations of convex combinations of projections onto block of sets. Our approach is based on several new ideas and tools, including stochastic selection rules for the blocks, stochastic conditioning of feasibility problem, and novel strategies for designing adaptive extrapolated stepsizes. We prove that, under linear regularity of the sets, our stochastic block projection algorithms converge linearly in expectation, with a rate depending on the condition number of the (block) feasibility problem and on the size of the blocks. Otherwise, we prove that our methods converge sublinearly. Our convergence analysis reveals that such algorithms are most effective when a good sampling of the sets into well-conditioned blocks is given. The convergence rates also explain when algorithms combining block projections with adaptive extrapolation work better than their nonextrapolated variants.},
  archive      = {J_OMS},
  author       = {I. Necoara},
  doi          = {10.1080/10556788.2021.1998492},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1845-1875},
  shortjournal = {Optim. Methods Softw.},
  title        = {Stochastic block projection algorithms with extrapolation for convex feasibility problems},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A dual active-set proximal newton algorithm for sparse
approximation of correlation matrices. <em>OMS</em>, <em>37</em>(5),
1820–1844. (<a
href="https://doi.org/10.1080/10556788.2021.1998491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel dual active-set algorithm that is based on proximal gradient and semi-smooth Newton iterations for the sparse approximation of correlation matrices in the Frobenius norm. A new dual formulation with upper and lower bounds is derived. To solve the dual, the proximal gradient method is developed to guarantee global convergence. Also, it provides information to estimate active/inactive constraints. Then, the semi-smooth Newton method is applied to accelerate the convergence of the proximal gradient method, which is the key ingredient of our algorithm. It is shown that the proposed algorithm for the dual is globally convergent under certain conditions. Some preliminary numerical results are given to illustrate the effectiveness of our algorithm on synthetic and real data sets.},
  archive      = {J_OMS},
  author       = {Xiao Liu and Chungen Shen and Li Wang},
  doi          = {10.1080/10556788.2021.1998491},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1820-1844},
  shortjournal = {Optim. Methods Softw.},
  title        = {A dual active-set proximal newton algorithm for sparse approximation of correlation matrices},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Off-line exploration of rectangular cellular environments
with a rectangular obstacle. <em>OMS</em>, <em>37</em>(5), 1805–1819.
(<a href="https://doi.org/10.1080/10556788.2021.1977811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider exploring a known rectangular cellular environment that has a rectangular obstacle using a mobile robot. The robot has to visit each cell and return to its starting cell. The goal is to find the shortest tour that visits all the cells. We give a linear-time algorithm that finds the exploration tour of optimal length. While the previous algorithms for environments with obstacles are approximation, the algorithm is presented in this paper is optimal. This algorithm also works for L -shaped and C -shaped environments. The main idea of the algorithm is, first, to find the longest simple exploring cycle, then extend it to include the unvisited cells.},
  archive      = {J_OMS},
  author       = {Fatemeh Keshavarz-Kohjerdi},
  doi          = {10.1080/10556788.2021.1977811},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1805-1819},
  shortjournal = {Optim. Methods Softw.},
  title        = {Off-line exploration of rectangular cellular environments with a rectangular obstacle},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semismooth newton-type method for bilevel optimization:
Global convergence and extensive numerical experiments. <em>OMS</em>,
<em>37</em>(5), 1770–1804. (<a
href="https://doi.org/10.1080/10556788.2021.1977810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the standard optimistic bilevel optimization problem, in particular upper- and lower-level constraints can be coupled. By means of the lower-level value function, the problem is transformed into a single-level optimization problem with a penalization of the value function constraint. For treating the latter problem, we develop a framework that does not rely on the direct computation of the lower-level value function or its derivatives. For each penalty parameter, the framework leads to a semismooth system of equations. This allows us to extend the semismooth Newton method to bilevel optimization. Besides global convergence properties of the method, we focus on achieving local superlinear convergence to a solution of the semismooth system. To this end, we formulate an appropriate CD-regularity assumption and derive sufficient conditions so that it is fulfilled. Moreover, we develop conditions to guarantee that a solution of the semismooth system is a local solution of the bilevel optimization problem. Extensive numerical experiments on 124 examples of nonlinear bilevel optimization problems from the literature show that this approach exhibits a remarkable performance, where only a few penalty parameters need to be considered.},
  archive      = {J_OMS},
  author       = {Andreas Fischer and Alain B. Zemkoho and Shenglong Zhou},
  doi          = {10.1080/10556788.2021.1977810},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1770-1804},
  shortjournal = {Optim. Methods Softw.},
  title        = {Semismooth newton-type method for bilevel optimization: Global convergence and extensive numerical experiments},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Global optimization for sparse solution of least squares
problems. <em>OMS</em>, <em>37</em>(5), 1740–1769. (<a
href="https://doi.org/10.1080/10556788.2021.1977809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding solutions to least-squares problems with low cardinality has found many applications, including portfolio optimization, subset selection in statistics, and inverse problems in signal processing. Although most works consider local approaches that scale with high-dimensional problems, some others address its global optimization via mixed integer programming (MIP) reformulations. We propose dedicated branch-and-bound methods for the exact resolution of moderate-size, yet difficult, sparse optimization problems, through three possible formulations: cardinality-constrained and cardinality-penalized least-squares, and cardinality minimization under quadratic constraints. A specific tree exploration strategy is built. Continuous relaxation problems involved at each node are reformulated as ℓ 1 -norm-based optimization problems, for which a dedicated algorithm is designed. The obtained certified solutions are shown to better estimate sparsity patterns than standard methods on simulated variable selection problems involving highly correlated variables. Problem instances selecting up to 24 components among 100 variables, and up to 15 components among 1000 variables, can be solved in less than 1000 s. Unguaranteed solutions obtained by limiting the computing time to 1s are also shown to provide competitive estimates. Our algorithms strongly outperform the CPLEX MIP solver as the dimension increases, especially for quadratically-constrained problems. The source codes are made freely available online.},
  archive      = {J_OMS},
  author       = {Ramzi Ben Mhenni and Sébastien Bourguignon and Jordan Ninin},
  doi          = {10.1080/10556788.2021.1977809},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1740-1769},
  shortjournal = {Optim. Methods Softw.},
  title        = {Global optimization for sparse solution of least squares problems},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient hybrid conjugate gradient method with
sufficient descent property for unconstrained optimization.
<em>OMS</em>, <em>37</em>(5), 1725–1739. (<a
href="https://doi.org/10.1080/10556788.2021.1977808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to take advantage of the strong theoretical properties of the FR method and computational efficiency of the P R P + P R P + PRP+ method, we present a new hybrid conjugate gradient method based on the convex combination of these methods. In our method, the search directions satisfy the sufficient descent condition independent of any line search. Under some standard assumptions, we established global convergence property of our proposed method for general functions. Numerical comparisons on some test problems from the CUTEst library illustrate the efficiency and robustness of our proposed method in practice.},
  archive      = {J_OMS},
  author       = {M. Lotfi and S. Mohammad Hosseini},
  doi          = {10.1080/10556788.2021.1977808},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1725-1739},
  shortjournal = {Optim. Methods Softw.},
  title        = {An efficient hybrid conjugate gradient method with sufficient descent property for unconstrained optimization},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exact penalties for decomposable convex optimization
problems. <em>OMS</em>, <em>37</em>(5), 1705–1724. (<a
href="https://doi.org/10.1080/10556788.2021.1977807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a general decomposable convex optimization problem. By using right-hand side allocation technique, it can be transformed into a collection of small dimensional optimization problems. The master problem is a convex non-smooth optimization problem. We propose to apply the exact non-smooth penalty method, which gives a solution of the initial problem under some fixed penalty parameter and provides the consistency of lower level problems. The master problem can be solved with a suitable non-smooth optimization method. The simplest of them is the custom subgradient projection method using the divergent series step-size rule without line-search, whose convergence may be, however, rather low. We suggest to enhance its step-size selection by using a two-speed rule. Preliminary results of computational experiments confirm efficiency of this technique.},
  archive      = {J_OMS},
  author       = {I. V. Konnov},
  doi          = {10.1080/10556788.2021.1977807},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1705-1724},
  shortjournal = {Optim. Methods Softw.},
  title        = {Exact penalties for decomposable convex optimization problems},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quasi-newton methods for machine learning: Forget the past,
just sample. <em>OMS</em>, <em>37</em>(5), 1668–1704. (<a
href="https://doi.org/10.1080/10556788.2021.1977806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present two sampled quasi-Newton methods (sampled LBFGS and sampled LSR1) for solving empirical risk minimization problems that arise in machine learning. Contrary to the classical variants of these methods that sequentially build Hessian or inverse Hessian approximations as the optimization progresses, our proposed methods sample points randomly around the current iterate at every iteration to produce these approximations. As a result, the approximations constructed make use of more reliable (recent and local) information and do not depend on past iterate information that could be significantly stale. Our proposed algorithms are efficient in terms of accessed data points (epochs) and have enough concurrency to take advantage of parallel/distributed computing environments. We provide convergence guarantees for our proposed methods. Numerical tests on a toy classification problem as well as on popular benchmarking binary classification and neural network training tasks reveal that the methods outperform their classical variants.},
  archive      = {J_OMS},
  author       = {A. S. Berahas and M. Jahani and P. Richtárik and M. Takáč},
  doi          = {10.1080/10556788.2021.1977806},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1668-1704},
  shortjournal = {Optim. Methods Softw.},
  title        = {Quasi-newton methods for machine learning: Forget the past, just sample},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). General framework for binary classification on top samples.
<em>OMS</em>, <em>37</em>(5), 1636–1667. (<a
href="https://doi.org/10.1080/10556788.2021.1965601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many binary classification problems minimize misclassification above (or below) a threshold. We show that instances of ranking problems, accuracy at the top, or hypothesis testing may be written in this form. We propose a general framework to handle these classes of problems and show which formulations (both known and newly proposed) fall into this framework. We provide a theoretical analysis of this framework and mention selected possible pitfalls the formulations may encounter. We show the convergence of the stochastic gradient descent for selected formulations even though the gradient estimate is inherently biased. We suggest several numerical improvements, including the implicit derivative and stochastic gradient descent. We provide an extensive numerical study.},
  archive      = {J_OMS},
  author       = {L. Adam and V. Mácha and V. Šmídl and T. Pevný},
  doi          = {10.1080/10556788.2021.1965601},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1636-1667},
  shortjournal = {Optim. Methods Softw.},
  title        = {General framework for binary classification on top samples},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stochastic approximation versus sample average approximation
for wasserstein barycenters. <em>OMS</em>, <em>37</em>(5), 1603–1635.
(<a href="https://doi.org/10.1080/10556788.2021.1965600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the machine learning and optimization community, there are two main approaches for the convex risk minimization problem, namely the Stochastic Approximation (SA) and the Sample Average Approximation (SAA). In terms of the oracle complexity (required number of stochastic gradient evaluations), both approaches are considered equivalent on average (up to a logarithmic factor). The total complexity depends on a specific problem, however, starting from the work [A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro, Robust stochastic approximation approach to stochastic programming , SIAM. J. Opt. 19 (2009), pp. 1574–1609] it was generally accepted that the SA is better than the SAA. We show that for the Wasserstein barycenter problem, this superiority can be inverted. We provide a detailed comparison by stating the complexity bounds for the SA and SAA implementations calculating barycenters defined with respect to optimal transport distances and entropy-regularized optimal transport distances. As a byproduct, we also construct confidence intervals for the barycenter defined with respect to entropy-regularized optimal transport distances in the ℓ 2 -norm. The preliminary results are derived for a general convex optimization problem given by the expectation to have other applications besides the Wasserstein barycenter problem.},
  archive      = {J_OMS},
  author       = {Darina Dvinskikh},
  doi          = {10.1080/10556788.2021.1965600},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1603-1635},
  shortjournal = {Optim. Methods Softw.},
  title        = {Stochastic approximation versus sample average approximation for wasserstein barycenters},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On well-structured convex–concave saddle point problems and
variational inequalities with monotone operators. <em>OMS</em>,
<em>37</em>(5), 1567–1602. (<a
href="https://doi.org/10.1080/10556788.2021.1928121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For those acquainted with CVX (aka disciplined convex programming) of Grant and Boyd ( Matlab software for disciplined convex programming, version 2.2 , CVX Research, Inc., 2020. http://cvxr.com/cvx/doc/ ), the motivation of this work is the desire to extend the scope of CVX beyond convex minimization – to convex–concave saddle point problems and variational inequalities with monotone operators. To attain this goal, given a family K of cones (e.g. Lorentz, semidefinite, geometric, etc.), we introduce the notions of K -conic representation of a convex–concave saddle point problem and of variational inequality with monotone operator. We demonstrate that given such a representation of the problem of interest, the latter can be reduced straightforwardly to a conic problem on a cone from K and thus can be solved by (any) solver capable to handle conic problems on cones from K (e.g. Mosek or SDPT3 in the case of semidefinite cones). We also show that K -representations of convex–concave functions and monotone vector fields admit a fully algorithmic calculus which helps to recognize the cases when a saddle point problem or variational inequality can be converted into a conic problem on a cone from K and to carry out such conversion.},
  archive      = {J_OMS},
  author       = {Anatoli Juditsky and Arkadi Nemirovski},
  doi          = {10.1080/10556788.2021.1928121},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1567-1602},
  shortjournal = {Optim. Methods Softw.},
  title        = {On well-structured convex–concave saddle point problems and variational inequalities with monotone operators},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reflected three-operator splitting method for monotone
inclusion problem. <em>OMS</em>, <em>37</em>(4), 1527–1565. (<a
href="https://doi.org/10.1080/10556788.2021.1924715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider reflected three-operator splitting methods for monotone inclusion problems in real Hilbert spaces. To do this, we first obtain weak convergence analysis and nonasymptotic O ( 1 / n ) O ( 1 / n ) O(1/n) convergence rate of the reflected Krasnosel&#39;skiĭ-Mann iteration for finding a fixed point of nonexpansive mapping in real Hilbert spaces under some seemingly easy to implement conditions on the iterative parameters. We then apply our results to three-operator splitting for the monotone inclusion problem and consequently obtain the corresponding convergence analysis. Furthermore, we derive reflected primal-dual algorithms for highly structured monotone inclusion problems. Some numerical implementations are drawn from splitting methods to support the theoretical analysis.},
  archive      = {J_OMS},
  author       = {Olaniyi S. Iyiola and Cyril D. Enyi and Yekini Shehu},
  doi          = {10.1080/10556788.2021.1924715},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {1527-1565},
  shortjournal = {Optim. Methods Softw.},
  title        = {Reflected three-operator splitting method for monotone inclusion problem},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An inertial subgradient extragradient algorithm with
adaptive stepsizes for variational inequality problems. <em>OMS</em>,
<em>37</em>(4), 1507–1526. (<a
href="https://doi.org/10.1080/10556788.2021.1910946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce an efficient subgradient extragradient (SE) based method for solving variational inequality problems with monotone operator in Hilbert space. In many existing SE methods, two values of operator are needed over each iteration and the Lipschitz constant of the operator or linesearch is required for estimating step sizes, which are usually not practical and expensive. To overcome these drawbacks, we present an inertial SE based algorithm with adaptive step sizes, estimated by using an approximation of the local Lipschitz constant without running a linesearch. Each iteration of the method only requires a projection on the feasible set and a value of the operator. The numerical experiments illustrate the efficiency of the proposed algorithm.},
  archive      = {J_OMS},
  author       = {Xiaokai Chang and Sanyang Liu and Zhao Deng and Suoping Li},
  doi          = {10.1080/10556788.2021.1910946},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {1507-1526},
  shortjournal = {Optim. Methods Softw.},
  title        = {An inertial subgradient extragradient algorithm with adaptive stepsizes for variational inequality problems},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A globally convergent regularized interior point method for
constrained optimization. <em>OMS</em>, <em>37</em>(4), 1471–1506. (<a
href="https://doi.org/10.1080/10556788.2021.1908283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a globally convergent regularized interior point method that involves a specifically designed regularization strategy for constrained optimization. The main concept of the proposed algorithm is that when a proper regularization parameter is selected, the direction obtained from the regularized barrier equation is a descent direction for either the objective function or constraint violation. Accordingly, by embedding a flexible strategy of choosing a regularization parameter in a trust-funnel-like interior point scheme, we propose the new algorithm. Global convergence under the mild assumptions of relaxed constant rank constraint qualification (RCRCQ) and local consistency of the linearized active and equality constraints is shown. Preliminary numerical experiments are conducted, and the results are encouraging.},
  archive      = {J_OMS},
  author       = {Songqiang Qiu},
  doi          = {10.1080/10556788.2021.1908283},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {1471-1506},
  shortjournal = {Optim. Methods Softw.},
  title        = {A globally convergent regularized interior point method for constrained optimization},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using nemirovski’s mirror-prox method as basic procedure in
chubanov’s method for solving homogeneous feasibility problems.
<em>OMS</em>, <em>37</em>(4), 1447–1470. (<a
href="https://doi.org/10.1080/10556788.2021.2023523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new variant of Chubanov&#39;s method for solving linear homogeneous systems with positive variables. In the Basic Procedure we use a recently introduced cut in combination with Nemirovski&#39;s Mirror-Prox method. We show that the cut requires at most O ( n 3 ) time, just as Chubanov&#39;s cut. In an earlier paper it was shown that the new cuts are at least as sharp as those of Chubanov. Our Modified Main Algorithm is in essence the same as Chubanov&#39;s Main Algorithm, except that it uses the new Basic Procedure as a subroutine. The new method has O ( n 3.5 log 2 ⁡ ( 1 / δ A ) ) time complexity, where δ A is a suitably defined condition number. As we show, a simplified version of the new Basic Procedure competes well with the Smooth Perceptron Scheme of Peña and Soheili and, when combined with Rescaling, also with two commercial codes for linear optimization.},
  archive      = {J_OMS},
  author       = {Zhang Wei and Kees Roos},
  doi          = {10.1080/10556788.2021.2023523},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {1447-1470},
  shortjournal = {Optim. Methods Softw.},
  title        = {Using nemirovski&#39;s mirror-prox method as basic procedure in chubanov&#39;s method for solving homogeneous feasibility problems},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A quasi-newton proximal bundle method using gradient
sampling technique for minimizing nonsmooth convex functions.
<em>OMS</em>, <em>37</em>(4), 1415–1446. (<a
href="https://doi.org/10.1080/10556788.2021.2023522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to merge the well-established ideas of bundle and Gradient Sampling (GS) methods to develop an algorithm for locating a minimizer of a nonsmooth convex function. In the proposed method, with the help of the GS technique, we sample a number of differentiable auxiliary points around the current iterate. Then, by applying the standard techniques used in bundle methods, we construct a polyhedral (piecewise linear) model of the objective function. Moreover, by performing quasi-Newton updates on the set of auxiliary points, this polyhedral model is augmented with a regularization term that enjoys second-order information. If required, this initial model is improved by the techniques frequently used in GS and bundle methods. We analyse the global convergence of the proposed method. As opposed to the original GS method and some of its variants, our convergence analysis is independent of the size of the sample. In our numerical experiments, various aspects of the proposed method are examined using a variety of test problems. In particular, in contrast with many variants of bundle methods, we will see that the user can supply gradients approximately. Moreover, we compare the proposed method with some efficient variants of GS and bundle methods.},
  archive      = {J_OMS},
  author       = {Morteza Maleknia and Mostafa Shamsi},
  doi          = {10.1080/10556788.2021.2023522},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {1415-1446},
  shortjournal = {Optim. Methods Softw.},
  title        = {A quasi-newton proximal bundle method using gradient sampling technique for minimizing nonsmooth convex functions},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Solution approaches for the vehicle routing problem with
occasional drivers and time windows. <em>OMS</em>, <em>37</em>(4),
1384–1414. (<a
href="https://doi.org/10.1080/10556788.2021.2022142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficient management of last-mile delivery is one of the main challenges faced by on-line retailers and logistic companies. The main aim is to offer personalized delivery services, that meet speed, flexibility, and control requirements and try to reduce environmental impacts as well. Crowd-sourced shipping is an emerging strategy that can be used to optimize the last-mile delivery process. The main idea is to deliver packages to customers with the aid of non-professional couriers, called occasional drivers. In this paper, we address the vehicle routing problem with occasional drivers, time window constraints and multiple deliveries. To handle this problem, we design some greedy randomized adaptive search procedures (GRASP). In order to assess the behaviour of the proposed algorithms, computational experiments are carried out on benchmark instances and new generated test sets. A comparison with previous published approaches, tailored for the problem at hand, is also provided. The numerical results are very encouraging and highlight the superiority, in terms of both efficiency and effectiveness, of the proposed GRASP algorithms.},
  archive      = {J_OMS},
  author       = {Luigi Di Puglia Pugliese and Daniele Ferone and Paola Festa and Francesca Guerriero and Giusy Macrina},
  doi          = {10.1080/10556788.2021.2022142},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {1384-1414},
  shortjournal = {Optim. Methods Softw.},
  title        = {Solution approaches for the vehicle routing problem with occasional drivers and time windows},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient hybrid conjugate gradient method for
unconstrained optimization. <em>OMS</em>, <em>37</em>(4), 1370–1383. (<a
href="https://doi.org/10.1080/10556788.2021.1998490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a hybrid conjugate gradient method for unconstrained optimization, obtained by a convex combination of the LS and KMD conjugate gradient parameters. A favourite property of the proposed method is that the search direction satisfies the Dai–Liao conjugacy condition and the quasi-Newton direction. In addition, this property does not depend on the line search. Under a modified strong Wolfe line search, we establish the global convergence of the method. Numerical comparison using a set of 109 unconstrained optimization test problems from the CUTEst library show that the proposed method outperforms the Liu–Storey and Hager–Zhang conjugate gradient methods.},
  archive      = {J_OMS},
  author       = {Abdulkarim Hassan Ibrahim and Poom Kumam and Ahmad Kamandi and Auwal Bala Abubakar},
  doi          = {10.1080/10556788.2021.1998490},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {1370-1383},
  shortjournal = {Optim. Methods Softw.},
  title        = {An efficient hybrid conjugate gradient method for unconstrained optimization},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Linear systems arising in interior methods for convex
optimization: A symmetric formulation with bounded condition number.
<em>OMS</em>, <em>37</em>(4), 1344–1369. (<a
href="https://doi.org/10.1080/10556788.2021.1965599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide eigenvalues bounds for a new formulation of the step equations in interior methods for convex quadratic optimization. The matrix of our formulation, named K 2.5 K 2.5 K2.5 , has bounded condition number, converges to a well-defined limit under strict complementarity, and has the same size as the traditional, ill-conditioned, saddle-point formulation. We evaluate the performance in the context of a Matlab object-oriented implementation of PDCO, an interior-point solver for minimizing a smooth convex function subject to linear constraints. The main benefit of our implementation, named PDCOO, is to separate the logic of the interior-point method from the formulation of the system used to compute a step at each iteration and the method used to solve the system. Thus, PDCOO allows easy addition of a new system formulation and/or solution method for experimentation. Our numerical experiments indicate that the K 2.5 formulation has the same storage requirements as the traditional ill-conditioned saddle-point formulation, and its condition is often more favourable than the unsymmetric block 3 × 3 formulation.},
  archive      = {J_OMS},
  author       = {Alexandre Ghannad and Dominique Orban and Michael A. Saunders},
  doi          = {10.1080/10556788.2021.1965599},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {1344-1369},
  shortjournal = {Optim. Methods Softw.},
  title        = {Linear systems arising in interior methods for convex optimization: A symmetric formulation with bounded condition number},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A competitive inexact nonmonotone filter SQP method:
Convergence analysis and numerical results. <em>OMS</em>,
<em>37</em>(4), 1310–1343. (<a
href="https://doi.org/10.1080/10556788.2021.1913155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an inexact nonmonotone successive quadratic programming (SQP) algorithm for solving nonlinear programming problems with equality constraints and bounded variables. Regarding the value of the current feasibility violation and the minimum value of its linear approximation over a trust region, several scenarios are envisaged. In one scenario, a possible infeasible stationary point is detected. In other scenarios, the search direction is computed using an inexact (truncated) solution of a feasible strictly convex quadratic program (QP). The search direction is shown to be a descent direction for the objective function or the feasibility violation in the feasible or infeasible iterations, respectively. A new penalty parameter updating formula is proposed to turn the search direction into a descent direction for an ℓ 1 -penalty function. In certain iterations, an accelerator direction is developed to obtain a superlinear local convergence rate of the algorithm. Using a nonmonotone filter strategy, the global convergence of the algorithm and a superlinear local rate of convergence are guaranteed. The main advantage of the algorithm is that the global convergence of the algorithm is established using inexact solutions of the QPs. Furthermore, the use of inexact solutions instead of exact solutions of the subproblems enhances the robustness and efficiency of the algorithm. The algorithm is implemented using MATLAB and the program is tested on a wide range of test problems from the CUTEst library. Comparison of the obtained numerical results with those obtained by testing some similar SQP algorithms affirms the efficiency and robustness of the proposed algorithm.},
  archive      = {J_OMS},
  author       = {Hani Ahmadzadeh and Nezam Mahdavi-Amiri},
  doi          = {10.1080/10556788.2021.1913155},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {1310-1343},
  shortjournal = {Optim. Methods Softw.},
  title        = {A competitive inexact nonmonotone filter SQP method: Convergence analysis and numerical results},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust piecewise linear l1-regression via nonsmooth DC
optimization. <em>OMS</em>, <em>37</em>(4), 1289–1309. (<a
href="https://doi.org/10.1080/10556788.2020.1855171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Piecewise linear L 1 L 1 L1 -regression problem is formulated as an unconstrained difference of convex (DC) optimization problem and an algorithm for solving this problem is developed. Auxiliary problems are introduced to design an adaptive approach to generate a suitable piecewise linear regression model and starting points for solving the underlying DC optimization problems. The performance of the proposed algorithm as both approximation and prediction tool is evaluated using synthetic and real-world data sets containing outliers. It is also compared with mainstream machine learning regression algorithms using various performance measures. Results demonstrate that the new algorithm is robust to outliers and in general, provides better predictions than the other alternative regression algorithms for most data sets used in the numerical experiments.},
  archive      = {J_OMS},
  author       = {Adil M. Bagirov and Sona Taheri and Napsu Karmitsa and Nargiz Sultanova and Soodabeh Asadi},
  doi          = {10.1080/10556788.2020.1855171},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {1289-1309},
  shortjournal = {Optim. Methods Softw.},
  title        = {Robust piecewise linear l1-regression via nonsmooth DC optimization},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the barzilai–borwein gradient methods with structured
secant equation for nonlinear least squares problems. <em>OMS</em>,
<em>37</em>(4), 1269–1288. (<a
href="https://doi.org/10.1080/10556788.2020.1855170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose structured spectral gradient algorithms for solving nonlinear least squares problems based on a modified structured secant equation. The idea was to integrate more details of the Hessian of the objective function into the standardized spectral parameters with the goal of improving numerical efficiency. We safeguard the structured spectral parameters to avoid negative curvature search direction. The sequence of the search direction generated by the respective proposed algorithm satisfies the sufficient descent condition. Using a nonmonotone line search strategy, we establish the global convergence of the proposed algorithms under some standard assumptions. Numerical experiments on some benchmark test problems show that the proposed algorithms are efficient and outperform some existing competitors.},
  archive      = {J_OMS},
  author       = {Aliyu Muhammed Awwal and Poom Kumam and Lin Wang and Mahmoud Muhammad Yahaya and Hassan Mohammad},
  doi          = {10.1080/10556788.2020.1855170},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {1269-1288},
  shortjournal = {Optim. Methods Softw.},
  title        = {On the Barzilai–Borwein gradient methods with structured secant equation for nonlinear least squares problems},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Diminishing stepsize methods for nonconvex composite
problems via ghost penalties: From the general to the convex regular
constrained case. <em>OMS</em>, <em>37</em>(4), 1242–1268. (<a
href="https://doi.org/10.1080/10556788.2020.1854253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we first extend the diminishing stepsize method for nonconvex constrained problems presented in F. Facchinei, V. Kungurtsev, L. Lampariello and G. Scutari [ Ghost penalties in nonconvex constrained optimization: Diminishing stepsizes and iteration complexity , To appear on Math. Oper. Res. 2020. Available at https://arxiv.org/abs/1709.03384 .] to deal with equality constraints and a nonsmooth objective function of composite type. We then consider the particular case in which the constraints are convex and satisfy a standard constraint qualification and show that in this setting the algorithm can be considerably simplified, reducing the computational burden of each iteration.},
  archive      = {J_OMS},
  author       = {Francisco Facchinei and Vyacheskav Kungurtsev and Lorenzo Lampariello and Gesualdo Scutari},
  doi          = {10.1080/10556788.2020.1854253},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {1242-1268},
  shortjournal = {Optim. Methods Softw.},
  title        = {Diminishing stepsize methods for nonconvex composite problems via ghost penalties: From the general to the convex regular constrained case},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A class of smooth exact penalty function methods for
optimization problems with orthogonality constraints. <em>OMS</em>,
<em>37</em>(4), 1205–1241. (<a
href="https://doi.org/10.1080/10556788.2020.1852236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Updating the augmented Lagrangian multiplier by closed-form expression yields efficient first-order infeasible approach for optimization problems with orthogonality constraints. Hence, parallelization becomes tractable in solving this type of problems. Inspired by this closed-form updating scheme, we propose a novel penalty function with compact convex constraints (PenC). We show that PenC  can act as an exact penalty model which shares the same global minimizers as the original problem with orthogonality constraints. Based on PenC, we first propose a first-order algorithm called PenCF  and establish its global convergence and local linear convergence rate under some mild assumptions. For the case that the computation and storage of Hessian is achievable, and we pursue high precision solution and fast local convergence rate, a second-order approach called PenCS  is proposed for solving PenC. To avoid expensive calculation or solving a hard subproblem in computing the Newton step, we propose a new strategy to do it approximately which still leads to quadratic convergence locally. Moreover, the main iterations of both PenCF  and PenCS  are orthonormalization-free and hence parallelizable. Numerical experiments illustrate that PenCF  is comparable with the existing first-order methods. Furthermore, PenCS  shows its stability and high efficiency in obtaining high precision solution comparing with the existing second-order methods.},
  archive      = {J_OMS},
  author       = {Nachuan Xiao and Xin Liu and Ya-xiang Yuan},
  doi          = {10.1080/10556788.2020.1852236},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {1205-1241},
  shortjournal = {Optim. Methods Softw.},
  title        = {A class of smooth exact penalty function methods for optimization problems with orthogonality constraints},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preface. <em>OMS</em>, <em>37</em>(4), 1203–1204. (<a
href="https://doi.org/10.1080/10556788.2022.2151748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_OMS},
  author       = {Mehiddin Al-Baali and Anton Purnama and Ekkehard W. Sachs},
  doi          = {10.1080/10556788.2022.2151748},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {1203-1204},
  shortjournal = {Optim. Methods Softw.},
  title        = {Preface},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A primer on the application of neural networks to covering
array generation. <em>OMS</em>, <em>37</em>(3), 1165–1202. (<a
href="https://doi.org/10.1080/10556788.2021.1907384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past, combinatorial structures have been used only to tune parameters of neural networks. In this work, we employ neural networks in the form of Boltzmann machines and Hopfield networks for the construction of a specific class of combinatorial designs, namely covering arrays (CAs). In past works, these neural networks were successfully used to solve set cover instances. For the construction of CAs, we consider the corresponding set cover instances and use neural networks to solve such instances. We adapt existing algorithms for solving general set cover instances, which are based on Boltzmann machines and Hopfield networks and apply them for CA construction. Furthermore, for the algorithm based on Boltzmann machines, we consider newly designed versions, where we deploy structural changes of the underlying Boltzmann machine, adding a feedback loop. Additionally, one variant of this algorithm employs learning techniques based on neural networks to adjust the various connections encountered in the graph representing the considered set cover instances. Culminating in a comprehensive experimental evaluation, our work presents the first study of applications of neural networks in the field of covering array generation and related discrete structures and may act as a guideline for future investigations.},
  archive      = {J_OMS},
  author       = {Ludwig Kampel and Michael Wagner and Ilias S. Kotsireas and Dimitris E. Simos},
  doi          = {10.1080/10556788.2021.1907384},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {1165-1202},
  shortjournal = {Optim. Methods Softw.},
  title        = {A primer on the application of neural networks to covering array generation},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A modified damped gauss–newton method for non-monotone
weighted linear complementarity problems. <em>OMS</em>, <em>37</em>(3),
1145–1164. (<a
href="https://doi.org/10.1080/10556788.2021.1903007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the weighted linear complementarity problem (denoted by wLCP). Many numerical algorithms have been proposed for solving the monotone wLCP. In this paper, we propose a damped Gauss–Newton method to solve the non-monotone wLCP which is designed based on a derivative-free non-monotone line search. We show that the proposed method is well defined and it is globally convergent without any problem assumptions. Moreover, we analyze the local quadratic convergence of the proposed method under the non-singularity condition and the local error bound condition, respectively. Our method not only has encouraging local convergence properties but also can be used to solve non-monotone wLCPs. Preliminary numerical results are reported.},
  archive      = {J_OMS},
  author       = {Jingyong Tang and Jinchuan Zhou},
  doi          = {10.1080/10556788.2021.1903007},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {1145-1164},
  shortjournal = {Optim. Methods Softw.},
  title        = {A modified damped Gauss–Newton method for non-monotone weighted linear complementarity problems},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shortest path reoptimization vs resolution from scratch: A
computational comparison. <em>OMS</em>, <em>37</em>(3), 1122–1144. (<a
href="https://doi.org/10.1080/10556788.2021.1895153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Shortest Path Problem (SPP) is among the most studied problems in Operations Research, for its theoretical aspects but also because it appears as sub-problem in many combinatorial optimization problems, e.g. Vehicle Routing and Maximum Flow-Minimum Cost problems. Given a sequence of SPPs, suppose that two subsequent instances solely differ by a slight change in the graph structure: that is the set of nodes, the set of arcs or both have changed; then, the goal of the reoptimization consists in solving the k t h SPP of the sequence by reusing valuable information gathered in the solution of the ( k − 1 ) t h one. We focused on the most general scenario, i.e. multiple changes for any subset of arcs, for which, only the description of a dual-primal approach has been proposed so far [S. Pallottino and M.G. Scutell‘a, A new algorithm for reoptimizing shortest paths when the arc costs change, Oper. Res. Lett. 31 (2003), pp. 149-160.]. We implemented this framework exploiting efficient data structures, i.e. the Multi Level Bucket. In addition, we compare the performance of our proposal with the well-known Dijkstra&#39;s algorithm, applied for solving each modified problem from scratch. In this way, we draw the line – in terms of cost, topology, and size – among the instances where the reoptimization approach is efficient from those that should be solved from scratch.},
  archive      = {J_OMS},
  author       = {Paola Festa and Serena Fugaro and Francesca Guerriero},
  doi          = {10.1080/10556788.2021.1895153},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {1122-1144},
  shortjournal = {Optim. Methods Softw.},
  title        = {Shortest path reoptimization vs resolution from scratch: A computational comparison},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly-convex–concave min–max optimization: Provable
algorithms and applications in machine learning. <em>OMS</em>,
<em>37</em>(3), 1087–1121. (<a
href="https://doi.org/10.1080/10556788.2021.1895152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Min–max problems have broad applications in machine learning, including learning with non-decomposable loss and learning with robustness to data distribution. Convex–concave min–max problem is an active topic of research with efficient algorithms and sound theoretical foundations developed. However, it remains a challenge to design provably efficient algorithms for non-convex min–max problems with or without smoothness. In this paper, we study a family of non-convex min–max problems, whose objective function is weakly convex in the variables of minimization and is concave in the variables of maximization. We propose a proximally guided stochastic subgradient method and a proximally guided stochastic variance-reduced method for the non-smooth and smooth instances, respectively, in this family of problems. We analyse the time complexities of the proposed methods for finding a nearly stationary point of the outer minimization problem corresponding to the min–max problem.},
  archive      = {J_OMS},
  author       = {Hassan Rafique and Mingrui Liu and Qihang Lin and Tianbao Yang},
  doi          = {10.1080/10556788.2021.1895152},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {1087-1121},
  shortjournal = {Optim. Methods Softw.},
  title        = {Weakly-convex–concave min–max optimization: Provable algorithms and applications in machine learning},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A c++ application programming interface for co-evolutionary
biased random-key genetic algorithms for solution and scenario
generation. <em>OMS</em>, <em>37</em>(3), 1065–1086. (<a
href="https://doi.org/10.1080/10556788.2021.1884250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a C++ application programming interface for a co-evolutionary algorithm for solution and scenario generation in stochastic problems. Based on a two-space biased random-key genetic algorithm, it involves two types of populations that are mutually impacted by the fitness calculations. In the solution population, high-quality solutions evolve, representing first-stage decisions evaluated by their performance in the face of the scenario population. The scenario population ultimately generates a diverse set of scenarios regarding their impact on the solutions. This application allows the straightforward implementation of this algorithm, where the user needs only to define the problem-dependent decoding procedure and may adjust the risk profile of the decision-maker. This paper presents the co-evolutionary algorithm and structures the interface. We also present some experiments that validate the impact of relevant features of the application.},
  archive      = {J_OMS},
  author       = {Beatriz Brito Oliveira and Maria Antónia Carravilla and José Fernando Oliveira and Maurício G. C. Resende},
  doi          = {10.1080/10556788.2021.1884250},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {1065-1086},
  shortjournal = {Optim. Methods Softw.},
  title        = {A c++ application programming interface for co-evolutionary biased random-key genetic algorithms for solution and scenario generation},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An algorithm for nonsymmetric conic optimization inspired by
MOSEK. <em>OMS</em>, <em>37</em>(3), 1027–1064. (<a
href="https://doi.org/10.1080/10556788.2021.1882457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyse the scaling matrix, search direction, and neighbourhood used in MOSEK&#39;s algorithm for nonsymmetric conic optimization [J. Dahl and E.D. Andersen, A primal-dual interior-point algorithm for nonsymmetric exponential-cone optimization , preprint (2019)]. It is proven that these can be used to compute a near-optimal solution to the homogeneous self-dual model in polynomial time. This provides a theoretical foundation for MOSEK&#39;s nonsymmetric conic algorithm. The main steps in the analysis are sandwiching MOSEK&#39;s scaling matrix between the primal and dual barrier&#39;s Hessians, and using this information to carefully check all the neighbourhood conditions after a small, improving step is taken.},
  archive      = {J_OMS},
  author       = {Riley Badenbroek and Joachim Dahl},
  doi          = {10.1080/10556788.2021.1882457},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {1027-1064},
  shortjournal = {Optim. Methods Softw.},
  title        = {An algorithm for nonsymmetric conic optimization inspired by MOSEK},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A benson-type algorithm for bounded convex vector
optimization problems with vertex selection. <em>OMS</em>,
<em>37</em>(3), 1006–1026. (<a
href="https://doi.org/10.1080/10556788.2021.1880579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an algorithm for approximately solving bounded convex vector optimization problems. The algorithm provides both an outer and an inner polyhedral approximation of the upper image. It is a modification of the primal algorithm presented by Löhne, Rudloff, and Ulus in 2014. There, vertices of an already known outer approximation are successively cutoff to improve the approximation error. We propose a new and efficient selection rule for deciding which vertex to cutoff. Numerical examples are provided which illustrate that this method may solve fewer scalar problems overall and therefore may be faster while achieving the same approximation quality.},
  archive      = {J_OMS},
  author       = {Daniel Dörfler and Andreas Löhne and Christopher Schneider and Benjamin Weißing},
  doi          = {10.1080/10556788.2021.1880579},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {1006-1026},
  shortjournal = {Optim. Methods Softw.},
  title        = {A benson-type algorithm for bounded convex vector optimization problems with vertex selection},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal algorithms for some inverse uncapacitated facility
location problems on networks. <em>OMS</em>, <em>37</em>(3), 982–1005.
(<a href="https://doi.org/10.1080/10556788.2021.1880578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with two variants of the inverse uncapacitated facility location problem with service cost modifications on tree networks under the rectilinear norm. In the first variant, the aim is to modify the underlying service costs in the cheapest possible way with respect to modification bounds such that a set of predetermined facility locations becomes optimal under the new parameters. Two types of this problem are investigated. The second variant asks to modify the service costs within an associated budget until the predetermined facility locations become as close as possible to the customers. Two types of this problem are also studied. We develop exact optimal algorithms for solving the problems under investigation. By an example, the efficiency of the proposed algorithms is illustrated.},
  archive      = {J_OMS},
  author       = {Mehran Hasanzadeh and Behrooz Alizadeh and Fahimeh Baroughi},
  doi          = {10.1080/10556788.2021.1880578},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {982-1005},
  shortjournal = {Optim. Methods Softw.},
  title        = {Optimal algorithms for some inverse uncapacitated facility location problems on networks},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparktope: Linear programs from algorithms. <em>OMS</em>,
<em>37</em>(3), 954–981. (<a
href="https://doi.org/10.1080/10556788.2020.1864370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a recent paper, Avis, Bremner, Tiwary and Watanabe gave a method for constructing linear programs (LPs) based on algorithms written in a simple programming language called Sparks . If an algorithm produces the solution x to a problem in polynomial time and space then the LP constructed is also of polynomial size and its optimum solution contains x as well as a complete execution trace of the algorithm. Their method led us to the construction of a compiler called sparktope which we describe in this paper. This compiler allows one to generate polynomial sized LPs for problems in P that have exponential extension complexity, such as matching problems in non-bipartite graphs. In this paper, we describe sparktope, the language Sparks , and the assembler instructions and LP constraints it produces. This is followed by two concrete examples, the makespan problem and the problem of testing if a matching in a graph is maximum, both of which are known to have exponential extension complexity. Computational results are given. In discussing these examples, we make use of visualization techniques included in sparktope that may be of independent interest. The extremely large linear programs produced by the compiler appear to be quite challenging to solve using currently available software. Since the optimum LP solutions can be computed independently they may be useful as benchmarks. Further enhancements of the compiler and its application are also discussed.},
  archive      = {J_OMS},
  author       = {David Avis and David Bremner},
  doi          = {10.1080/10556788.2020.1864370},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {954-981},
  shortjournal = {Optim. Methods Softw.},
  title        = {Sparktope: Linear programs from algorithms},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gradient methods with memory. <em>OMS</em>, <em>37</em>(3),
936–953. (<a
href="https://doi.org/10.1080/10556788.2020.1858831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider gradient methods for minimizing smooth convex functions, which employ the information obtained at the previous iterations in order to accelerate the convergence towards the optimal solution. This information is used in the form of a piece-wise linear model of the objective function, which provides us with much better prediction abilities as compared with the standard linear model. To the best of our knowledge, this approach was never really applied in Convex Minimization to differentiable functions in view of the high complexity of the corresponding auxiliary problems. However, we show that all necessary computations can be done very efficiently. Consequently, we get new optimization methods, which are better than the usual Gradient Methods both in the number of oracle calls and in the computational time. Our theoretical conclusions are confirmed by preliminary computational experiments.},
  archive      = {J_OMS},
  author       = {Yurii Nesterov and Mihai I. Florea},
  doi          = {10.1080/10556788.2020.1858831},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {936-953},
  shortjournal = {Optim. Methods Softw.},
  title        = {Gradient methods with memory},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient numerical methods to solve sparse linear equations
with application to PageRank. <em>OMS</em>, <em>37</em>(3), 907–935. (<a
href="https://doi.org/10.1080/10556788.2020.1858297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last two decades, the PageRank problem has received increased interest from the academic community as an efficient tool to estimate web-page importance in information retrieval. Despite numerous developments, the design of efficient optimization algorithms for the PageRank problem is still a challenge. This paper proposes three new algorithms with a linear time complexity for solving the problem over a bounded-degree graph. The idea behind them is to set up the PageRank as a convex minimization problem over a unit simplex, and then solve it using iterative methods with small iteration complexity. Our theoretical results are supported by an extensive empirical justification using real-world and simulated data.},
  archive      = {J_OMS},
  author       = {Anton Anikin and Alexander Gasnikov and Alexander Gornov and Dmitry Kamzolov and Yury Maximov and Yurii Nesterov},
  doi          = {10.1080/10556788.2020.1858297},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {907-935},
  shortjournal = {Optim. Methods Softw.},
  title        = {Efficient numerical methods to solve sparse linear equations with application to PageRank},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inexact basic tensor methods for some classes of convex
optimization problems. <em>OMS</em>, <em>37</em>(3), 878–906. (<a
href="https://doi.org/10.1080/10556788.2020.1854252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we analyse the Basic Tensor Methods, which use approximate solutions of the auxiliary problems. The quality of this solution is described by the residual in the function value, which must be proportional to ϵ p + 1 p ϵ p + 1 p ϵp+1p , where p ≥ 1 p ≥ 1 p≥ 1 is the order of the method and ϵ is the desired accuracy in the main optimization problem. We analyse in details the auxiliary schemes for the third- and second-order tensor methods. The auxiliary problems for the third-order scheme can be solved very efficiently by a linearly convergent gradient-type method with a preconditioner. The most expensive operation in this process is a preliminary factorization of the Hessian of the objective function. For solving the auxiliary problem for the second order scheme, we suggest two variants of the Fast Gradient Methods with restart, which converge as O ( 1 k 6 ) , where k is the iteration counter. Finally, we present the results of the preliminary computational experiments.},
  archive      = {J_OMS},
  author       = {Yurii Nesterov},
  doi          = {10.1080/10556788.2020.1854252},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {878-906},
  shortjournal = {Optim. Methods Softw.},
  title        = {Inexact basic tensor methods for some classes of convex optimization problems},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fully stochastic second-order trust region method.
<em>OMS</em>, <em>37</em>(3), 844–877. (<a
href="https://doi.org/10.1080/10556788.2020.1852403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A stochastic second-order trust region method is proposed, which can be viewed as an extension of the trust-region-ish (TRish) algorithm proposed by Curtis et al. [ A stochastic trust region algorithm based on careful step normalization. INFORMS J. Optim. 1(3) 200–220, 2019]. In each iteration, a search direction is computed by (approximately) solving a subproblem defined by stochastic gradient and Hessian estimates. The algorithm has convergence guarantees in the fully stochastic regime, i.e. when each stochastic gradient is merely an unbiased estimate of the gradient with bounded variance and the stochastic Hessian estimates are bounded. This framework covers a variety of implementations, such as when the stochastic Hessians are defined by sampled second-order derivatives or diagonal matrices, such as in RMSprop, Adagrad, Adam and other popular algorithms. The proposed algorithm has a worst-case complexity guarantee in the nearly deterministic regime, i.e. when the stochastic gradients and Hessians are close in expectation to the true gradients and Hessians. The results of numerical experiments for training CNNs for image classification and an RNN for time series forecasting are presented. These results show that the algorithm can outperform a stochastic gradient and first-order TRish algorithm.},
  archive      = {J_OMS},
  author       = {Frank E. Curtis and Rui Shi},
  doi          = {10.1080/10556788.2020.1852403},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {844-877},
  shortjournal = {Optim. Methods Softw.},
  title        = {A fully stochastic second-order trust region method},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Global convergence of a new sufficient descent spectral
three-term conjugate gradient class for large-scale optimization.
<em>OMS</em>, <em>37</em>(3), 830–843. (<a
href="https://doi.org/10.1080/10556788.2020.1843167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve a large-scale unconstrained optimization problem, in this paper we propose a class of spectral three-term conjugate gradient methods. We indicate that the proposed class, in fact, generates sufficient descent directions and also fulfill Dai–Liao conjugacy condition. We prove the global convergence of the presented class for either uniformly convex or general smooth functions under some suitable conditions, in detail. Finally, in a set of numerical experiments which contains eight conjugate gradient methods and 260 standard problems, we illustrate the efficiency and effectiveness of our class.},
  archive      = {J_OMS},
  author       = {M. R. Eslahchi and S. Bojari},
  doi          = {10.1080/10556788.2020.1843167},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {830-843},
  shortjournal = {Optim. Methods Softw.},
  title        = {Global convergence of a new sufficient descent spectral three-term conjugate gradient class for large-scale optimization},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new interior-point approach for large separable convex
quadratic two-stage stochastic problems. <em>OMS</em>, <em>37</em>(3),
801–829. (<a
href="https://doi.org/10.1080/10556788.2020.1841190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-stage stochastic models give rise to very large optimization problems. Several approaches have been devised for efficiently solving them, including interior-point methods (IPMs). However, using IPMs, the linking columns associated with first-stage decisions cause excessive fill-in for the solution of the normal equations. This downside is usually alleviated if variable splitting is applied to first-stage variables. This work presents a specialized IPM that applies variable splitting and exploits the structure of the deterministic equivalent of the stochastic problem. The specialized IPM combines Cholesky factorizations and preconditioned conjugate gradients for solving the normal equations. This specialized IPM outperforms other approaches when the number of first-stage variables is large enough. This paper provides computational results for two stochastic problems: (1) a supply chain system and (2) capacity expansion in an electric system. Both linear and convex quadratic formulations were used, obtaining instances of up to 38 million variables and 6 million constraints. The computational results show that our procedure is more efficient than alternative state-of-the-art IPM implementations (e.g. CPLEX) and other specialized solvers for stochastic optimization.},
  archive      = {J_OMS},
  author       = {Jordi Castro and Paula de la Lama-Zubirán},
  doi          = {10.1080/10556788.2020.1841190},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {801-829},
  shortjournal = {Optim. Methods Softw.},
  title        = {A new interior-point approach for large separable convex quadratic two-stage stochastic problems},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Solving quadratic multi-leader-follower games by smoothing
the follower’s best response. <em>OMS</em>, <em>37</em>(2), 772–799. (<a
href="https://doi.org/10.1080/10556788.2020.1828412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyse the existence of Nash equilibria for a class of quadratic multi-leader-follower games using the nonsmooth best response function. To overcome the challenge of nonsmoothness, we pursue a smoothing approach resulting in a reformulation as a smooth Nash equilibrium problem. The existence and uniqueness of solutions are proven for all smoothing parameters. Accumulation points of Nash equilibria exist for a decreasing sequence of these smoothing parameters and we show that these candidates fulfil the conditions of S-stationarity and are Nash equilibria to the multi-leader-follower game. Finally, we propose an update on the leader variables for efficient computation and numerically compare nonsmooth Newton and subgradient methods.},
  archive      = {J_OMS},
  author       = {Michael Herty and Sonja Steffensen and Anna Thünen},
  doi          = {10.1080/10556788.2020.1828412},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {772-799},
  shortjournal = {Optim. Methods Softw.},
  title        = {Solving quadratic multi-leader-follower games by smoothing the follower&#39;s best response},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting aggregate sparsity in second-order cone
relaxations for quadratic constrained quadratic programming problems.
<em>OMS</em>, <em>37</em>(2), 753–771. (<a
href="https://doi.org/10.1080/10556788.2020.1827256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among many approaches to increase the computational efficiency of semidefinite programming (SDP) relaxation for nonconvex quadratic constrained quadratic programming problems (QCQPs), exploiting the aggregate sparsity of the data matrices in the SDP by Fukuda et al. [ Exploiting sparsity in semidefinite programming via matrix completion I: General framework , SIAM J. Optim. 11(3) (2001), pp. 647–674] and second-order cone programming (SOCP) relaxation have been popular. In this paper, we exploit the aggregate sparsity of SOCP relaxation of nonconvex QCQPs. Specifically, we prove that exploiting the aggregate sparsity reduces the number of second-order cones in the SOCP relaxation, and that we can simplify the matrix completion procedure by Fukuda et al. in both primal and dual of the SOCP relaxation problem without losing the max-determinant property. For numerical experiments, nonconvex QCQPs from the lattice graph and pooling problem are tested as their SOCP relaxations provide the same optimal value as the SDP relaxations. We demonstrate that exploiting the aggregate sparsity improves the computational efficiency of the SOCP relaxation for the same objective value as the SDP relaxation, thus much larger problems can be handled by the proposed SOCP relaxation than the SDP relaxation.},
  archive      = {J_OMS},
  author       = {Heejune Sheen and Makoto Yamashita},
  doi          = {10.1080/10556788.2020.1827256},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {753-771},
  shortjournal = {Optim. Methods Softw.},
  title        = {Exploiting aggregate sparsity in second-order cone relaxations for quadratic constrained quadratic programming problems},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preference robust models in multivariate utility-based
shortfall risk minimization. <em>OMS</em>, <em>37</em>(2), 712–752. (<a
href="https://doi.org/10.1080/10556788.2020.1827255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utility-based shortfall risk measure (SR) has received increasing attentions over the past few years. Recently Delage et al. [ Shortfall Risk Models When Information of Loss Function Is Incomplete , GERAD HEC, Montréal, 2018] consider a situation where a decision maker&#39;s true loss function in the definition of SR is unknown but it is possible to elicit a set of plausible utility functions with partial information and consequently propose a robust formulation of SR based on the worst-case utility function. In this paper, we extend this new stream of research to a multi-attribute prospect space since multi-attribute decision-making problems are ubiquitous in practical applications. Specifically, we introduce a preference robust multivariate utility-based shortfall risk measure (PRMSR) and demonstrate that it is law invariant and convex. We then apply the PRMSR to an optimal decision-making problem where the objective is to minimize the PRMSR of a vector-valued cost function and propose some numerical scheme for solving the resulting optimization problem in the case when the underlying exogenous uncertainty is finitely distributed. Finally, we discuss statistical robustness of the PRMSR based optimization model by examining qualitative stability of the estimator of the optimal value obtained with potentially contaminated data. A case study is carried out to examine the performance of the proposed robust model and numerical scheme.},
  archive      = {J_OMS},
  author       = {Yuan Zhang and Huifu Xu and Wei Wang},
  doi          = {10.1080/10556788.2020.1827255},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {712-752},
  shortjournal = {Optim. Methods Softw.},
  title        = {Preference robust models in multivariate utility-based shortfall risk minimization},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simultaneous iterative solutions for the trust-region and
minimum eigenvalue subproblem. <em>OMS</em>, <em>37</em>(2), 692–711.
(<a href="https://doi.org/10.1080/10556788.2020.1827254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the inability to foresee all possible scenarios, it is justified to desire an efficient trust-region subproblem solver capable of delivering any desired level of accuracy on demand; that is, the accuracy obtainable for a given trust-region subproblem should not be partially dependent on the problem itself. Current state-of-the-art iterative eigensolvers all fall into the class of restarted Lanczos methods; whereas, current iterative trust-region solvers at best reduce to unrestarted Lanczos methods; which in this context are well known to be numerically unstable with impractical memory requirements. In this paper, we present the first iterative trust region subproblem solver that at its core contains a robust and practical eigensolver. Our solver leverages the recently announced work of Stathopoulos and Orginos which has not been noticed by the optimization community and can be utilized because, unlike other restarted Lanczos methods, its restarts do not necessarily modify the current Lanczos sequence generated by Conjugate Gradient methods (CG). This innovated strategy can be utilized in the context of TR solvers as well. Moreover, our TR subproblem solver adds negligible computational overhead compared to existing iterative TR approaches.},
  archive      = {J_OMS},
  author       = {I. G. Akrotirianakis and M. Gratton and J. D. Griffin and S. Yektamaram and W. Zhou},
  doi          = {10.1080/10556788.2020.1827254},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {692-711},
  shortjournal = {Optim. Methods Softw.},
  title        = {Simultaneous iterative solutions for the trust-region and minimum eigenvalue subproblem},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Γ-robust linear complementarity problems. <em>OMS</em>,
<em>37</em>(2), 658–691. (<a
href="https://doi.org/10.1080/10556788.2020.1825708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complementarity problems are often used to compute equilibria made up of specifically coordinated solutions of different optimization problems. Specific examples are game-theoretic settings like the bimatrix game or energy market models like for electricity or natural gas. While optimization under uncertainties is rather well-developed, the field of equilibrium models represented by complementarity problems under uncertainty – especially using the concepts of robust optimization – is still in its infancy. In this paper, we extend the theory of strictly robust linear complementarity problems (LCPs) to Γ-robust settings, where existence of worst-case-hedged equilibria cannot be guaranteed. Thus, we study the minimization of the worst-case gap function of Γ-robust counterparts of LCPs. For box and ℓ 1 -norm uncertainty sets we derive tractable convex counterparts for monotone LCPs and study their feasibility as well as the existence and uniqueness of solutions. To this end, we consider uncertainties in the vector and in the matrix defining the LCP. We additionally study so-called ρ -robust solutions, i.e. solutions of relaxed uncertain LCPs. Finally, we illustrate the Γ-robust concept applied to LCPs in the light of the above mentioned classical examples of bimatrix games and market equilibrium modelling.},
  archive      = {J_OMS},
  author       = {Vanessa Krebs and Martin Schmidt},
  doi          = {10.1080/10556788.2020.1825708},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {658-691},
  shortjournal = {Optim. Methods Softw.},
  title        = {Γ-robust linear complementarity problems},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A DC approach for minimax fractional optimization programs
with ratios of convex functions. <em>OMS</em>, <em>37</em>(2), 639–657.
(<a href="https://doi.org/10.1080/10556788.2020.1818234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with minimax fractional programs whose objective functions are the maximum of finite ratios of convex functions, with arbitrary convex constraints set. For such problems, Dinkelbach-type algorithms fail to work since the parametric subproblems may be nonconvex, whereas the latter need a global optimal solution of these subproblems. We give necessary optimality conditions for such problems, by means of convex analysis tools. We then propose a method, based on solving approximately a sequence of parametric convex problems, which acts as dc (difference of convex functions) algorithm, if the parameter is positive and as Dinkelbach algorithm if not. We show that every cluster point of the sequence of optimal solutions of these subproblems satisfies necessary optimality conditions of KKT criticality type, that are also of Clarke stationarity type. Finally we end with some numerical tests to illustrate the behaviour of the algorithm.},
  archive      = {J_OMS},
  author       = {A. Ghazi and A. Roubi},
  doi          = {10.1080/10556788.2020.1818234},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {639-657},
  shortjournal = {Optim. Methods Softw.},
  title        = {A DC approach for minimax fractional optimization programs with ratios of convex functions},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tensor methods for finding approximate stationary points of
convex functions. <em>OMS</em>, <em>37</em>(2), 605–638. (<a
href="https://doi.org/10.1080/10556788.2020.1818082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the problem of finding ε -approximate stationary points of convex functions that are p -times differentiable with ν -Hölder continuous p th derivatives. We present tensor methods with and without acceleration. Specifically, we show that the non-accelerated schemes take at most 𝒪 ( ϵ − 1 / ( p + ν − 1 ) ) O ϵ − 1 / ( p + ν − 1 ) Oϵ−1/(p+ν−1) iterations to reduce the norm of the gradient of the objective below given ϵ ∈ ( 0 , 1 ) . For accelerated tensor schemes, we establish improved complexity bounds of O ϵ − ( p + ν ) / [ ( p + ν − 1 ) ( p + ν + 1 ) ] and O | log ⁡ ( ϵ ) | ϵ − 1 / ( p + ν ) , when the Hölder parameter ν ∈ [ 0 , 1 ] is known. For the case in which ν is unknown, we obtain a bound of O ϵ − ( p + 1 ) / [ ( p + ν − 1 ) ( p + 2 ) ] for a universal accelerated scheme. Finally, we also obtain a lower complexity bound of O ϵ − 2 / [ 3 ( p + ν ) − 2 ] for finding ε -approximate stationary points using p -order tensor methods.},
  archive      = {J_OMS},
  author       = {G. N. Grapiglia and Yurii Nesterov},
  doi          = {10.1080/10556788.2020.1818082},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {605-638},
  shortjournal = {Optim. Methods Softw.},
  title        = {Tensor methods for finding approximate stationary points of convex functions},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Newton projection method as applied to assembly simulation.
<em>OMS</em>, <em>37</em>(2), 577–604. (<a
href="https://doi.org/10.1080/10556788.2020.1818079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider Newton projection method for solving the quadratic programming problem that emerges in simulation of joining process for assembly with compliant parts. This particular class of problems has specific features such as an ill-conditioned Hessian and a sparse matrix of constraints as well as a requirement for the large-scale computations. We use the projected Newton method with a quadratic rate of convergence and suggest some improvements to reduce the solving time: a method for solving the system of linear equations, so-called constraint recalculation method, and compare different approaches for step-size selection. We use the duality principle to formulate alternative forms of the minimization problem that, as a rule, can be solved faster. We describe how to solve the considered nonlinear minimization problem with the nonsmooth objective function by modifying Newton projection method and employing subgradients. In addition, we prove the convergence of the suggested algorithm. Finally, we compare Newton projection method with the other quadratic programming techniques on a number of assembly simulation problems.},
  archive      = {J_OMS},
  author       = {S. Baklanov and M. Stefanova and S. Lupuleac},
  doi          = {10.1080/10556788.2020.1818079},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {577-604},
  shortjournal = {Optim. Methods Softw.},
  title        = {Newton projection method as applied to assembly simulation},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A partitioned scheme for adjoint shape sensitivity analysis
of fluid–structure interactions involving non-matching meshes.
<em>OMS</em>, <em>37</em>(2), 546–576. (<a
href="https://doi.org/10.1080/10556788.2020.1806275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a partitioned solution procedure to compute shape gradients in fluid–structure interaction (FSI) using black-box adjoint solvers. Special attention is paid to project the gradients onto the undeformed configuration due to the mixed Lagrangian–Eulerian formulation of large-deformation FSI in this work. The adjoint FSI problem is partitioned as an assembly of well-known adjoint fluid and structural problems. The sub-adjoint problems are coupled with each other by augmenting the target functions with auxiliary functions, independent of the concrete choice of the underlying adjoint formulations. The auxiliary functions are linear force-based or displacement-based functionals which are readily available in well-established single-disciplinary adjoint solvers. Adjoint structural displacements, adjoint fluid displacements, and domain-based adjoint sensitivities of the fluid are the coupling fields to be exchanged between the adjoint solvers. A reduced formulation is also derived for the case of boundary-based adjoint shape sensitivity analysis for fluids. Numerical studies show that the complete formulation computes accurate shape gradients whereas inaccuracies appear in the reduced gradients. Mapping techniques including nearest element interpolation and the mortar method are studied in computational adjoint FSI. It is numerically shown that the mortar method does not introduce spurious oscillations in primal and sensitivity fields along non-matching interfaces.},
  archive      = {J_OMS},
  author       = {Reza Najian Asl and Ihar Antonau and Aditya Ghantasala and Wulf G. Dettmer and Roland Wüchner and Kai-Uwe Bletzinger},
  doi          = {10.1080/10556788.2020.1806275},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {546-576},
  shortjournal = {Optim. Methods Softw.},
  title        = {A partitioned scheme for adjoint shape sensitivity analysis of fluid–structure interactions involving non-matching meshes},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A study of one-parameter regularization methods for
mathematical programs with vanishing constraints. <em>OMS</em>,
<em>37</em>(2), 503–545. (<a
href="https://doi.org/10.1080/10556788.2020.1797025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mathematical programs with vanishing constraints (MPVCs) are a class of nonlinear optimization problems with applications to various engineering problems such as truss topology design and robot motion planning. MPVCs are difficult problems from both a theoretical and numerical perspective: the combinatorial nature of the vanishing constraints often prevents standard constraint qualifications and optimality conditions from being attained; moreover, the feasible set is inherently nonconvex, and often has no interior around points of interest. In this paper, we therefore study and compare four regularization methods for the numerical solution of MPVCs. Each method depends on a single regularization parameter, which is used to embed the original MPVC into a sequence of standard nonlinear programs. Convergence results for these methods based on both exact and approximate stationary of the subproblems are established under weak assumptions. The improved regularity of the subproblems is studied by providing sufficient conditions for the existence of KKT multipliers. Numerical experiments, based on applications in truss topology design and an optimal control problem from aerothermodynamics, complement the theoretical analysis and comparison of the regularization methods. The computational results highlight the benefit of using regularization over applying a standard solver directly, and they allow us to identify two promising regularization schemes.},
  archive      = {J_OMS},
  author       = {Tim Hoheisel and Blanca Pablos and Aram Pooladian and Alexandra Schwartz and Luke Steverango},
  doi          = {10.1080/10556788.2020.1797025},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {503-545},
  shortjournal = {Optim. Methods Softw.},
  title        = {A study of one-parameter regularization methods for mathematical programs with vanishing constraints},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized derivatives of computer programs. <em>OMS</em>,
<em>37</em>(2), 480–502. (<a
href="https://doi.org/10.1080/10556788.2020.1797024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A method for evaluating lexicographical directional (LD)-derivatives of functional programs is presented, extending previous methods to programs containing conditional branches and loops. A language for imperative programs is given, and conditions under which LD-derivatives can be calculated automatically for conditional branches and loops are described, along with a full description of the source transformation procedures necessary.},
  archive      = {J_OMS},
  author       = {Matthew R. Billingsley and Paul I. Barton},
  doi          = {10.1080/10556788.2020.1797024},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {480-502},
  shortjournal = {Optim. Methods Softw.},
  title        = {Generalized derivatives of computer programs},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Projections onto the canonical simplex with additional
linear inequalities. <em>OMS</em>, <em>37</em>(2), 451–479. (<a
href="https://doi.org/10.1080/10556788.2020.1797023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the distributionally robust optimization and show that computing the distributional worst-case is equivalent to computing the projection onto the canonical simplex with additional linear inequality. We consider several distance functions to measure the distance of distributions. We write the projections as optimization problems and show that they are equivalent to finding a zero of real-valued functions. We prove that these functions possess nice properties such as monotonicity or convexity. We design optimization methods with guaranteed convergence and derive their theoretical complexity. We demonstrate that our methods have (almost) linear observed complexity.},
  archive      = {J_OMS},
  author       = {L. Adam and V. Mácha},
  doi          = {10.1080/10556788.2020.1797023},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {451-479},
  shortjournal = {Optim. Methods Softw.},
  title        = {Projections onto the canonical simplex with additional linear inequalities},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EAGO.jl: Easy advanced global optimization in julia.
<em>OMS</em>, <em>37</em>(2), 425–450. (<a
href="https://doi.org/10.1080/10556788.2020.1786566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An extensible open-source deterministic global optimizer (EAGO) programmed entirely in the Julia language is presented. EAGO was developed to serve the need for supporting higher-complexity user-defined functions (e.g. functions defined implicitly via algorithms) within optimization models. EAGO embeds a first-of-its-kind implementation of McCormick arithmetic in an Evaluator structure allowing for the construction of convex/concave relaxations using a combination of source code transformation, multiple dispatch, and context-specific approaches. Utilities are included to parse user-defined functions into a directed acyclic graph representation and perform symbolic transformations enabling dramatically improved solution speed. EAGO is compatible with a wide variety of local optimizers, the most exhaustive library of transcendental functions, and allows for easy accessibility through the JuMP modelling language. Together with Julia&#39;s minimalist syntax and competitive speed, these powerful features make EAGO a versatile research platform enabling easy construction of novel meta-solvers, incorporation and utilization of new relaxations, and extension to advanced problem formulations encountered in engineering and operations research (e.g. multilevel problems, user-defined functions). The applicability and flexibility of this novel software is demonstrated on a diverse set of examples. Lastly, EAGO is demonstrated to perform comparably to state-of-the-art commercial optimizers on a benchmarking test set.},
  archive      = {J_OMS},
  author       = {M. E. Wilhelm and M. D. Stuber},
  doi          = {10.1080/10556788.2020.1786566},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {425-450},
  shortjournal = {Optim. Methods Softw.},
  title        = {EAGO.jl: Easy advanced global optimization in julia},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the complexity of solving feasibility problems with
regularized models. <em>OMS</em>, <em>37</em>(2), 405–424. (<a
href="https://doi.org/10.1080/10556788.2020.1786564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity of solving feasibility problems is considered in this work. It is assumed that the constraints that define the problem can be divided into expensive and cheap constraints. At each iteration, the introduced method minimizes a regularized p th-order model of the sum of squares of the expensive constraints subject to the cheap constraints. Under a Hölder continuity property with constant β ∈ ( 0 , 1 ] β ∈ ( 0 , 1 ] β∈(0,1] on the p th derivatives of the expensive constraints, it is shown that finding a feasible point with precision ε &gt; 0 or an infeasible point that is stationary with tolerance γ &gt; 0 of minimizing the sum of squares of the expensive constraints subject to the cheap constraints has iteration complexity O ( | log ⁡ ( ε ) | γ ζ ( p , β ) ω p 1 + ( 1 / 2 ) ζ ( p , β ) ) and evaluation complexity (of the expensive constraints) O ( | log ⁡ ( ε ) | [ γ ζ ( p , β ) ω p 1 + ( 1 / 2 ) ζ ( p , β ) + ( 1 − β ) / ( p + β − 1 ) | log ⁡ ( γ ε ) | ] ) , where ζ ( p , β ) = − ( p + β ) / ( p + β − 1 ) and ω p = ε if p = 1, while ω p = Φ ( x 0 ) if p &gt;1. Moreover, if the derivatives satisfy a Lipschitz condition and a uniform regularity assumption holds, both complexities reduce to O ( | log ⁡ ( ε ) | ) , independently of p .},
  archive      = {J_OMS},
  author       = {E. G. Birgin and L. F. Bueno and J. M. Martínez},
  doi          = {10.1080/10556788.2020.1786564},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {405-424},
  shortjournal = {Optim. Methods Softw.},
  title        = {On the complexity of solving feasibility problems with regularized models},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Further results on sum-of-squares tensors. <em>OMS</em>,
<em>37</em>(1), 387–403. (<a
href="https://doi.org/10.1080/10556788.2020.1768389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sum-of-squares (SOS) tensors plays an important role in tensor positive definiteness and polynomial optimization. So it is important to figure out what kind of tensors are SOS tensors. In this paper, we first show that several types of even order symmetric tensors are SOS tensors. The inclusive relation between several types of existing SOS tensors are developed under suitable conditions. By exploring the structure of LLK tensor, which is a special type of SOS tensor, we propose a numerical algorithm to check whether a given even order symmetric tensor is LLK tensor or not. Numerical experiments are also reported to show the efficiency of the proposed method.},
  archive      = {J_OMS},
  author       = {Haibin Chen and Liqun Qi and Yiju Wang and Guanglu Zhou},
  doi          = {10.1080/10556788.2020.1768389},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {387-403},
  shortjournal = {Optim. Methods Softw.},
  title        = {Further results on sum-of-squares tensors},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On a multilevel levenberg–marquardt method for the training
of artificial neural networks and its application to the solution of
partial differential equations. <em>OMS</em>, <em>37</em>(1), 361–386.
(<a href="https://doi.org/10.1080/10556788.2020.1775828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new multilevel Levenberg–Marquardt optimizer for the training of artificial neural networks with quadratic loss function. This setting allows us to get further insight into the potential of multilevel optimization methods. Indeed, when the least squares problem arises from the training of artificial neural networks, the variables subject to optimization are not related by any geometrical constraints and the standard interpolation and restriction operators cannot be employed any longer. A heuristic, inspired by algebraic multigrid methods, is then proposed to construct the multilevel transfer operators. We test the new optimizer on an important application: the approximate solution of partial differential equations by means of artificial neural networks. The learning problem is formulated as a least squares problem, choosing the nonlinear residual of the equation as a loss function, whereas the multilevel method is employed as a training method. Numerical experiments show encouraging results related to the efficiency of the new multilevel optimization method compared to the corresponding one-level procedure in this context.},
  archive      = {J_OMS},
  author       = {H. Calandra and S. Gratton and E. Riccietti and X. Vasseur},
  doi          = {10.1080/10556788.2020.1775828},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {361-386},
  shortjournal = {Optim. Methods Softw.},
  title        = {On a multilevel Levenberg–Marquardt method for the training of artificial neural networks and its application to the solution of partial differential equations},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Solving a continuous multifacility location problem by DC
algorithms. <em>OMS</em>, <em>37</em>(1), 338–360. (<a
href="https://doi.org/10.1080/10556788.2020.1771335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents a new approach to solve multifacility location problems, which is based on mixed integer programming and algorithms for minimizing differences of convex (DC) functions. The main challenges for solving the multifacility location problems under consideration come from their intrinsic discrete, nonconvex, and nondifferentiable nature. We provide a reformulation of these problems as those of continuous optimization and then develop a new DC type algorithm for their solutions involving Nesterov&#39;s smoothing. The proposed algorithm is computationally implemented via MATLAB numerical tests on both artificial and real data sets.},
  archive      = {J_OMS},
  author       = {Anuj Bajaj and Boris S. Mordukhovich and Nguyen Mau Nam and Tuyen Tran},
  doi          = {10.1080/10556788.2020.1771335},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {338-360},
  shortjournal = {Optim. Methods Softw.},
  title        = {Solving a continuous multifacility location problem by DC algorithms},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On SOR-like iteration methods for solving weakly nonlinear
systems. <em>OMS</em>, <em>37</em>(1), 320–337. (<a
href="https://doi.org/10.1080/10556788.2020.1755861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a class of SOR-like iteration methods for solving the systems of the weakly nonlinear equation, which is by reformulating equivalently the weakly nonlinear equation as a two-by-two block nonlinear equation. Two types of the global convergence theorems are given under suitable choices of the involved splitting matrix and parameter. Numerical results for the three-dimensional nonlinear convection-diffusion equation and the linear complementarity problem show that the proposed iteration methods are feasible and efficient for solving the weakly nonlinear equations.},
  archive      = {J_OMS},
  author       = {Yifen Ke and Changfeng Ma},
  doi          = {10.1080/10556788.2020.1755861},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {320-337},
  shortjournal = {Optim. Methods Softw.},
  title        = {On SOR-like iteration methods for solving weakly nonlinear systems},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exponential augmented lagrangian methods for equilibrium
problems. <em>OMS</em>, <em>37</em>(1), 295–319. (<a
href="https://doi.org/10.1080/10556788.2020.1751154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce exponential augmented Lagrangian methods for solving equilibrium problems in finite-dimensional spaces, extending the so-called quadratic augmented Lagrangian methods. Unlike the quadratic augmented Lagrangian methods that are at most first-order differentiable, our exponential augmented Lagrangian methods can be differentiable at any order. Therefore, second-order methods, such as Newton&#39;s methods, can be used to solve the subproblems generated by the augmented Lagrangian methods. We also present numerical results obtained based on implementing our proposed algorithms in Matlab.},
  archive      = {J_OMS},
  author       = {E. M. R. Torrealba and L. C. Matioli and M. Nasri and R. A. Castillo},
  doi          = {10.1080/10556788.2020.1751154},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {295-319},
  shortjournal = {Optim. Methods Softw.},
  title        = {Exponential augmented lagrangian methods for equilibrium problems},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asynchronous variance-reduced block schemes for composite
non-convex stochastic optimization: Block-specific steplengths and
adapted batch-sizes. <em>OMS</em>, <em>37</em>(1), 264–294. (<a
href="https://doi.org/10.1080/10556788.2020.1746963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work considers the minimization of a sum of an expectation-valued coordinate-wise smooth nonconvex function and a nonsmooth block-separable convex regularizer. We propose an asynchronous variance-reduced algorithm, where in each iteration, a single block is randomly chosen to update its estimates by a proximal variable sample-size stochastic gradient scheme, while the remaining blocks are kept invariant. Notably, each block employs a steplength relying on its block-specific Lipschitz constant while batch-sizes are updated as a function of the number of times that block is selected. We show that every limit point is a stationary point and establish the ergodic non-asymptotic rate O ( 1 / K ) . Iteration and oracle complexity to obtain an ε -stationary point are shown to be O ( 1 / ϵ ) and O ( 1 / ϵ 2 ) , respectively. Furthermore, under a proximal Polyak–Łojasiewicz condition with batch sizes increasing at a geometric rate, we prove that the suboptimality diminishes at a geometric rate, the optimal deterministic rate while iteration and oracle complexity to obtain an ε -optimal solution are O ( ln ⁡ ( 1 / ϵ ) ) and O ( 1 / ϵ ) 1 + c with c ≥ 0 . In the single block setting, we obtain the optimal oracle complexity O ( 1 / ϵ ) . Finally, preliminary numerics suggest that the schemes compare well with competitors reliant on global Lipschitz constants.},
  archive      = {J_OMS},
  author       = {Jinlong Lei and Uday V. Shanbhag},
  doi          = {10.1080/10556788.2020.1746963},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {264-294},
  shortjournal = {Optim. Methods Softw.},
  title        = {Asynchronous variance-reduced block schemes for composite non-convex stochastic optimization: Block-specific steplengths and adapted batch-sizes},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Direct search nonsmooth constrained optimization via rounded
ℓ1 penalty functions. <em>OMS</em>, <em>37</em>(1), 241–263. (<a
href="https://doi.org/10.1080/10556788.2020.1746961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A class of direct search methods for locally minimizing a Lipschitz continuous black-box function f subject to locally Lipschitz constraints is presented. A sequence of smoothed ℓ 1 ℓ 1 ℓ1 penalty functions is used. Each smoothed penalty function is approximately minimized in turn. The smoothing is reduced after each minimization, exposing the ℓ 1 ℓ 1 ℓ1 exact penalty function in the limit. Convergence to a constrained Clarke stationary point is shown under appropriate regularity conditions. Convergence to one or more KKT points is shown under similar conditions when f and all active constraints are strictly differentiable at each limit point. An implementation of one method in this class is numerically tested and shown to be effective in practice. The implementation uses a discrete quasi-Newton step when possible. Otherwise a global direction search is used to locate a descent direction. Theoretical convergence properties are independent of the quasi-Newton step.},
  archive      = {J_OMS},
  author       = {C. J. Price},
  doi          = {10.1080/10556788.2020.1746961},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {241-263},
  shortjournal = {Optim. Methods Softw.},
  title        = {Direct search nonsmooth constrained optimization via rounded ℓ1 penalty functions},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Projection-free accelerated method for convex optimization.
<em>OMS</em>, <em>37</em>(1), 214–240. (<a
href="https://doi.org/10.1080/10556788.2020.1734806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a projection-free accelerated method for solving convex optimization problems with unbounded feasible set. The method is an accelerated gradient scheme such that each projection subproblem is approximately solved by means of a conditional gradient scheme. Under reasonable assumptions, it is shown that an ϵ -approximate solution (concept related to the optimal value of the problem) is obtained in at most 𝒪 ( 1 / ε √ ) O ( 1 / ε ) O(1/ε) gradient evaluations and 𝒪 ( 1 / ε ) O ( 1 / ε ) O(1/ε) linear oracle calls. We also discuss a notion of approximate solution based on the first-order optimality condition of the problem and present iteration-complexity results for the proposed method to obtain an approximate solution in this sense. Finally, numerical experiments illustrating the practical behaviour of the proposed scheme are discussed.},
  archive      = {J_OMS},
  author       = {Max L. N. Gonçalves and Jefferson G. Melo and Renato D. C. Monteiro},
  doi          = {10.1080/10556788.2020.1734806},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {214-240},
  shortjournal = {Optim. Methods Softw.},
  title        = {Projection-free accelerated method for convex optimization},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Full nesterov-todd step feasible interior-point algorithm
for symmetric cone horizontal linear complementarity problem based on a
positive-asymptotic barrier function. <em>OMS</em>, <em>37</em>(1),
192–213. (<a
href="https://doi.org/10.1080/10556788.2020.1734803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a feasible full step interior-point algorithm to solve the P ∗ ( κ ) P ∗ ( κ ) P∗(κ) horizontal linear complementarity problem defined on a Cartesian product of symmetric cones, which is not based on a usual barrier function. The full steps are scaled utilizing the Nesterov-Todd (NT) scaling point. Our approach generates the search directions leading to the full-NT steps by algebraically transforming the centring equation of the system which defines the central trajectory using the induced barrier of a so-called positive-asymptotic kernel function. We establish the global convergence as well as a local quadratic rate of convergence of our proposed method. Finally, we demonstrate that our algorithm bears a complexity bound matching the best available one for the algorithms of its kind.},
  archive      = {J_OMS},
  author       = {S. Asadi and N. Mahdavi-Amiri and Zs. Darvay and P. R. Rigó},
  doi          = {10.1080/10556788.2020.1734803},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {192-213},
  shortjournal = {Optim. Methods Softw.},
  title        = {Full nesterov-todd step feasible interior-point algorithm for symmetric cone horizontal linear complementarity problem based on a positive-asymptotic barrier function},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On high-order model regularization for multiobjective
optimization. <em>OMS</em>, <em>37</em>(1), 175–191. (<a
href="https://doi.org/10.1080/10556788.2020.1719408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A p -order regularization method for finding weak stationary points of multiobjective optimization problems with constraints is introduced. Under Hölder conditions on the derivatives of the objective functions, complexity results are obtained that generalize properties recently proved for scalar optimization.},
  archive      = {J_OMS},
  author       = {L. Calderón and M. A. Diniz-Ehrhardt and J. M. Martínez},
  doi          = {10.1080/10556788.2020.1719408},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {175-191},
  shortjournal = {Optim. Methods Softw.},
  title        = {On high-order model regularization for multiobjective optimization},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two heuristic methods based on decomposition to the
integrated multi-agent supply chain scheduling and distribution problem.
<em>OMS</em>, <em>37</em>(1), 150–174. (<a
href="https://doi.org/10.1080/10556788.2020.1714615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supply chain integration has become one of the most attractive topics for researchers in recent years. One of the advantages of this integration is in improving overall profit in comparison to separate decisions. In this study, an integrated scheduling and distribution problem is investigated. One of the contributions of this paper is to study this problem from a multi-agent viewpoint. In this case, each agent has a set of jobs with its own objective and compete with each other to acquire supply chain resources. Here, a two-agent problem is discussed where the objectives of the agents are the minimization of the total tardiness and the total cost of distribution. A mathematical formulation and two heuristics based on decomposition approaches are presented. In the first approach, a modified Benders decomposition is presented. Also, some valid inequalities are introduced to increase the convergence speed of this algorithm. In the second approach, a decomposition and cutting approach is developed. The results represent the good performance of both algorithms in comparison to other exact methods.},
  archive      = {J_OMS},
  author       = {Ali Gharaei and Fariborz Jolai},
  doi          = {10.1080/10556788.2020.1714615},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {150-174},
  shortjournal = {Optim. Methods Softw.},
  title        = {Two heuristic methods based on decomposition to the integrated multi-agent supply chain scheduling and distribution problem},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finding zeros of hölder metrically subregular mappings via
globally convergent levenberg–marquardt methods. <em>OMS</em>,
<em>37</em>(1), 113–149. (<a
href="https://doi.org/10.1080/10556788.2020.1712602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce LMLS and LMQR, two globally convergent Levenberg–Marquardt methods for finding zeros of Hölder metrically subregular mappings that may have non-isolated zeros. The first method unifies the Levenberg–Marquardt direction and an Armijo-type line search, while the second incorporates this direction with a non-monotone quadratic regularization technique. For both methods, we prove the global convergence to a first-order stationary point of the associated merit function. Furthermore, the worst-case global complexity of these methods are provided, indicating that an approximate stationary point can be computed in at most O ( ε − 2 ) function and gradient evaluations, for an accuracy parameter ε &gt; 0 . We also study the conditions for the proposed methods to converge to a zero of the associated mappings. Computing a moiety conserved steady state for biochemical reaction networks can be cast as the problem of finding a zero of a Hölder metrically subregular mapping. We report encouraging numerical results for finding a zero of such mappings derived from real-world biological data, which supports our theoretical foundations.},
  archive      = {J_OMS},
  author       = {Masoud Ahookhosh and Ronan M. T. Fleming and Phan T. Vuong},
  doi          = {10.1080/10556788.2020.1712602},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {113-149},
  shortjournal = {Optim. Methods Softw.},
  title        = {Finding zeros of hölder metrically subregular mappings via globally convergent Levenberg–Marquardt methods},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An inexact first-order method for constrained nonlinear
optimization. <em>OMS</em>, <em>37</em>(1), 79–112. (<a
href="https://doi.org/10.1080/10556788.2020.1712601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary focus of this paper is on designing an inexact first-order algorithm for solving constrained nonlinear optimization problems. By controlling the inexactness of the subproblem solution, we can significantly reduce the computational cost needed for each iteration. A penalty parameter updating strategy during the process of solving the subproblem enables the algorithm to automatically detect infeasibility. Global convergence for both feasible and infeasible cases is proved. Complexity analysis for the KKT residual is also derived under mild assumptions. Numerical experiments exhibit the ability of the proposed algorithm to rapidly find inexact optimal solution through cheap computational cost.},
  archive      = {J_OMS},
  author       = {Hao Wang and Fan Zhang and Jiashan Wang and Yuyang Rong},
  doi          = {10.1080/10556788.2020.1712601},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {79-112},
  shortjournal = {Optim. Methods Softw.},
  title        = {An inexact first-order method for constrained nonlinear optimization},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Newton-type multilevel optimization method. <em>OMS</em>,
<em>37</em>(1), 45–78. (<a
href="https://doi.org/10.1080/10556788.2019.1700256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by multigrid methods for linear systems of equations, multilevel optimization methods have been proposed to solve structured optimization problems. Multilevel methods make more assumptions regarding the structure of the optimization model, and as a result, they outperform single-level methods, especially for large-scale models. The impressive performance of multilevel optimization methods is an empirical observation, and no theoretical explanation has so far been proposed. In order to address this issue, we study the convergence properties of a multilevel method that is motivated by second-order methods. We take the first step toward establishing how the structure of an optimization problem is related to the convergence rate of multilevel algorithms.},
  archive      = {J_OMS},
  author       = {Chin Pang Ho and Michal Kočvara and Panos Parpas},
  doi          = {10.1080/10556788.2019.1700256},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {45-78},
  shortjournal = {Optim. Methods Softw.},
  title        = {Newton-type multilevel optimization method},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast scenario reduction by conditional scenarios in
two-stage stochastic MILP problems. <em>OMS</em>, <em>37</em>(1), 23–44.
(<a href="https://doi.org/10.1080/10556788.2019.1697696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common approach to model stochastic programming problems is based on scenarios. An option to manage the difficulty of these problems corresponds to reduce the original set of scenarios. In this paper we study a new fast scenario reduction method based on Conditional Scenarios (CS). We analyse the degree of similarity between the original large set of scenarios and the small set of conditional scenarios in terms of the first two moments. In our numerical experiment, based on the stochastic capacitated facility location problem, we compare two fast scenario reduction methods: the CS method and the Monte Carlo (MC) method. The empirical conclusion is twofold: On the one hand, the achieved expected costs obtained by the two approaches are similar, although the MC method obtains a better approximation to the original set of of scenarios in terms of the moment matching criterion. On the other hand, the CS approach outperforms the MC approach with the same number of scenarios in terms of solution time.},
  archive      = {J_OMS},
  author       = {C. Beltran-Royo},
  doi          = {10.1080/10556788.2019.1697696},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {23-44},
  shortjournal = {Optim. Methods Softw.},
  title        = {Fast scenario reduction by conditional scenarios in two-stage stochastic MILP problems},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integer linear programming formulations for double roman
domination problem. <em>OMS</em>, <em>37</em>(1), 1–22. (<a
href="https://doi.org/10.1080/10556788.2019.1679142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a graph G = ( V , E ) G = ( V , E ) G=(V,E) , a double Roman dominating function (DRDF) is a function f : V → { 0 , 1 , 2 , 3 } f : V → { 0 , 1 , 2 , 3 } f:V→{0,1,2,3} having the property that if f ( v ) = 0 f ( v ) = 0 f(v)=0 , then vertex v must have at least two neighbours assigned 2 under f or at least one neighbour u with f ( u ) = 3 f ( u ) = 3 f(u)=3 , and if f ( v ) = 1 f ( v ) = 1 f(v)=1 , then vertex v must have at least one neighbour u with f ( u ) ≥ 2 f ( u ) ≥ 2 f(u)≥2 . In this paper, we consider the double Roman domination problem, which is an optimization problem of finding the DRDF f such that ∑ v ∈ V f ( v ) ∑ v ∈ V f ( v ) ∑v∈Vf(v) is minimum. We propose five integer linear programming (ILP) formulations and one mixed integer linear programming formulation with polynomial number of constraints for this problem. Some additional valid inequalities and bounds are also proposed for some of these formulations. Further, we prove that the first four models indeed solve the double Roman domination problem, and the last two models are equivalent to the others regardless of the variable relaxation or usage of a smaller number of constraints and variables. Additionally, we use one ILP formulation to give an H ( 2 ( Δ + 1 ) ) -approximation algorithm. All proposed formulations and approximation algorithm are evaluated on randomly generated graphs to compare the performance.},
  archive      = {J_OMS},
  author       = {Qingqiong Cai and Neng Fan and Yongtang Shi and Shunyu Yao},
  doi          = {10.1080/10556788.2019.1679142},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Optim. Methods Softw.},
  title        = {Integer linear programming formulations for double roman domination problem},
  volume       = {37},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
