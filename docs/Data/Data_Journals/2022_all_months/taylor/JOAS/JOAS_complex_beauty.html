<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JOAS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="joas---215">JOAS - 215</h2>
<ul>
<li><details>
<summary>
(2022). Regression models using the LINEX loss to predict lower
bounds for the number of points for approximating planar contour shapes.
<em>JOAS</em>, <em>49</em>(16), 4294–4313. (<a
href="https://doi.org/10.1080/02664763.2021.1986685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers in statistical shape analysis often analyze outlines of objects. Even though these contours are infinite-dimensional in theory, they must be discretized in practice. When discretizing, it is important to reduce the number of sampling points considerably to reduce computational costs, but to not use too few points so as to result in too much approximation error. Unfortunately, determining the minimum number of points needed to achieve sufficiently approximate the contours is computationally expensive. In this paper, we fit regression models to predict these lower bounds using characteristics of the contours that are computationally cheap as predictor variables. However, least squares regression is inadequate for this task because it treats overestimation and underestimation equally, but underestimation of lower bounds is far more serious. Instead, to fit the models, we use the LINEX loss function, which allows us to penalize underestimation at an exponential rate while penalizing overestimation only linearly. We present a novel approach to select the shape parameter of the loss function and tools for analyzing how well the model fits the data. Through validation methods, we show that the LINEX models work well for reducing the underestimation for the lower bounds.},
  archive      = {J_JOAS},
  author       = {J. M. Thilini Jayasinghe and Leif Ellingson and Chalani Prematilake},
  doi          = {10.1080/02664763.2021.1986685},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {4294-4313},
  shortjournal = {J. Appl. Stat.},
  title        = {Regression models using the LINEX loss to predict lower bounds for the number of points for approximating planar contour shapes},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A resample-replace lasso procedure for combining
high-dimensional markers with limit of detection. <em>JOAS</em>,
<em>49</em>(16), 4278–4293. (<a
href="https://doi.org/10.1080/02664763.2021.1977785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In disease screening, a biomarker combination developed by combining multiple markers tends to have a higher sensitivity than an individual marker. Parametric methods for marker combination rely on the inverse of covariance matrices, which is often a non-trivial problem for high-dimensional data generated by modern high-throughput technologies. Additionally, another common problem in disease diagnosis is the existence of limit of detection (LOD) for an instrument – that is, when a biomarker&#39;s value falls below the limit, it cannot be observed and is assigned an NA value. To handle these two challenges in combining high-dimensional biomarkers with the presence of LOD, we propose a resample-replace lasso procedure. We first impute the values below LOD and then use the graphical lasso method to estimate the means and precision matrices for the high-dimensional biomarkers. The simulation results show that our method outperforms alternative methods such as either substitute NA values with LOD values or remove observations that have NA values. A real case analysis on a protein profiling study of glioblastoma patients on their survival status indicates that the biomarker combination obtained through the proposed method is more accurate in distinguishing between two groups.},
  archive      = {J_JOAS},
  author       = {Jinjuan Wang and Yunpeng Zhao and Larry L. Tang and Claudius Mueller and Qizhai Li},
  doi          = {10.1080/02664763.2021.1977785},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {4278-4293},
  shortjournal = {J. Appl. Stat.},
  title        = {A resample-replace lasso procedure for combining high-dimensional markers with limit of detection},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dealing with separation or near-to-separation in the model
for multinomial response with application to childhood health seeking
behavior data from a complex survey. <em>JOAS</em>, <em>49</em>(16),
4254–4277. (<a
href="https://doi.org/10.1080/02664763.2021.1977260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Separation or monotone-likelihood can be observed in fitting process of a multinomial logistic model using maximum likelihood estimation (MLE) when sample size is small and/or one of the outcome categories is rare and/or there is one or more influential covariates, resulting in infinite or biased estimate of at least one regression coefficient of the model. This study investigated empirically to identify the optimal data condition to define both ‘separation’ and ‘near-to-separation’ (partial separation) and explored their consequences in MLE and provided a solution by applying a penalized likelihood approach, which has been proposed in the literature, by adding a Jeffreys prior-based penalty term to the original likelihood function to remove the first-order bias in the MLEs of the multinomial logit model via equivalent Poisson regression. Furthermore, the penalized estimating equation (PMLE) is extended to a weighted estimating equation allowing for survey-weight for analyzing data from a complex survey. The simulation study suggests that the PMLE outperforms the MLE by providing smaller amount of bias and mean squared of error and better coverage. The methods are applied to analyze data on choice of health facility for treatment of childhood diseases.},
  archive      = {J_JOAS},
  author       = {Nowrin Nusrat and M. S. Rahman},
  doi          = {10.1080/02664763.2021.1977260},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {4254-4277},
  shortjournal = {J. Appl. Stat.},
  title        = {Dealing with separation or near-to-separation in the model for multinomial response with application to childhood health seeking behavior data from a complex survey},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On a log-symmetric quantile tobit model applied to female
labor supply data. <em>JOAS</em>, <em>49</em>(16), 4225–4253. (<a
href="https://doi.org/10.1080/02664763.2021.1976120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of female labor supply has been a topic of relevance in the economic literature. Generally, the data are left-censored and the classic tobit model has been extensively used in the modeling strategy. This model, however, assumes normality for the error distribution and is not recommended for data with positive skewness, heavy-tails and heteroscedasticity, as is the case of female labor supply data. Moreover, it is well-known that the quantile regression approach accounts for the influences of different quantiles in the estimated coefficients. We take all these features into account and propose a parametric quantile tobit regression model based on quantile log-symmetric distributions. The proposed method allows one to model data with positive skewness (which is not suitable for the classic tobit model), to study the influence of the quantiles of interest, and to account for heteroscedasticity. The model parameters are estimated by maximum likelihood and a Monte Carlo experiment is performed to evaluate alternative estimators. The new method is applied to two distinct female labor supply data sets. The results indicate that the log-symmetric quantile tobit model fits better the data than the classic tobit model.},
  archive      = {J_JOAS},
  author       = {Danúbia R. Cunha and Jose Angelo Divino and Helton Saulo},
  doi          = {10.1080/02664763.2021.1976120},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {4225-4253},
  shortjournal = {J. Appl. Stat.},
  title        = {On a log-symmetric quantile tobit model applied to female labor supply data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The modified slash lindley–weibull distribution with
applications to nutrition data. <em>JOAS</em>, <em>49</em>(16),
4206–4224. (<a
href="https://doi.org/10.1080/02664763.2021.1975661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents an extension of the slash Lindley–Weibull distribution, of which it can be considered a modification. The new family is obtained by using the quotient of two independent random variables: a two-parameter Lindley–Weibull distribution divided by a power of the exponential distribution with parameter equal to 2. We present the pdf and cdf of the new distribution, analyzing their risk functions. Some statistical properties are studied and the moments and coefficients of asymmetry and kurtosis are shown. The parameter estimation problem is carried out by the maximum likelihood method. The method is assessed by a Monte Carlo simulation study. We use nutrition data, which are characterized by high kurtosis, to illustrate the usefulness of the proposed model.},
  archive      = {J_JOAS},
  author       = {Jimmy Reyes and Jaime Arrué and Osvaldo Venegas and Héctor W. Gómez},
  doi          = {10.1080/02664763.2021.1975661},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {4206-4224},
  shortjournal = {J. Appl. Stat.},
  title        = {The modified slash Lindley–Weibull distribution with applications to nutrition data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new class of efficient and debiased two-step shrinkage
estimators: Method and application. <em>JOAS</em>, <em>49</em>(16),
4181–4205. (<a
href="https://doi.org/10.1080/02664763.2021.1973389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new class of efficient and debiased two-step shrinkage estimators for a linear regression model in the presence of multicollinearity. We derive the proposed estimators’ mean square error and define the necessary and sufficient conditions for superiority over the existing estimators. In addition, we develop an algorithm for selecting the shrinkage parameters for the proposed estimators. The comparison of the new estimators versus the traditional ordinary least squares, ridge regression, Liu, and the two-parameter estimators is done by a matrix mean square error criterion. The Monte Carlo simulation results show the superiority of the proposed estimators under certain conditions. In the presence of high but imperfect multicollinearity, the two-step shrinkage estimators’ performance is relatively better. Finally, two real-world chemical data are analyzed to demonstrate the advantages and the empirical relevance of our newly proposed estimators. It is shown that the standard errors and the estimated mean square error decrease substantially for the proposed estimator. Hence, the precision of the estimated parameters is increased, which of course is one of the main objectives of the practitioners.},
  archive      = {J_JOAS},
  author       = {Muhammad Qasim and Kristofer Månsson and Pär Sjölander and B. M. Golam Kibria},
  doi          = {10.1080/02664763.2021.1973389},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {4181-4205},
  shortjournal = {J. Appl. Stat.},
  title        = {A new class of efficient and debiased two-step shrinkage estimators: Method and application},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Competing risks proportional-hazards cure model and
generalized extreme value regression: An application to bank failures
and acquisitions in the united states. <em>JOAS</em>, <em>49</em>(16),
4162–4180. (<a
href="https://doi.org/10.1080/02664763.2021.1973386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several commercial banks in the United States disappeared during the last decades due to failure or acquisition by another entity. From a survival analysis perspective, however, the high censoring rate suggests that some institutions are likely to be immune to failure and/or acquisition. In this study, we use a competing risks proportional-hazards cure model in order to measure the impact of bank-specific and macroeconomic variables on the probabilities of being susceptible to these events (i.e. incidence) and on the survival time of susceptible banks (i.e. latency). Moreover, we propose to model the incidence distribution using Generalized Extreme Value regression and compare the results with the ones obtained by the usual logistic regression model. The proposed methodology is evaluated by means of a simulation study and then applied to a dataset of more than 4000 United States commercial banks spanning the period 1993–2018.},
  archive      = {J_JOAS},
  author       = {A. Beretta and C. Heuchenne and M. Restaino},
  doi          = {10.1080/02664763.2021.1973386},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {4162-4180},
  shortjournal = {J. Appl. Stat.},
  title        = {Competing risks proportional-hazards cure model and generalized extreme value regression: An application to bank failures and acquisitions in the united states},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new regression model for rates and proportions data with
applications. <em>JOAS</em>, <em>49</em>(16), 4137–4161. (<a
href="https://doi.org/10.1080/02664763.2021.1973385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new continuous distribution in the interval ( 0 , 1 ) ( 0 , 1 ) (0,1) based on the generalized odd log-logistic-G family, whose density function can be symmetrical, asymmetric, unimodal and bimodal. The new model is implemented using the gamlss packages in R . We propose an extended regression based on this distribution which includes as sub-models some important regressions. We employ a frequentist and Bayesian analysis to estimate the parameters and adopt the non-parametric and parametric bootstrap methods to obtain better efficiency of the estimators. Some simulations are conducted to verify the empirical distribution of the maximum likelihood estimators. We compare the empirical distribution of the quantile residuals with the standard normal distribution. The extended regression can give more realistic fits than other regressions in the analysis of proportional data.},
  archive      = {J_JOAS},
  author       = {F. Prataviera and G. M. Cordeiro and E. M. M. Ortega and E. M. Hashimoto and V. G. Cancho},
  doi          = {10.1080/02664763.2021.1973385},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {4137-4161},
  shortjournal = {J. Appl. Stat.},
  title        = {A new regression model for rates and proportions data with applications},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online monitoring of high-dimensional binary data streams
with application to extreme weather surveillance. <em>JOAS</em>,
<em>49</em>(16), 4122–4136. (<a
href="https://doi.org/10.1080/02664763.2021.1971633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of modern sensor technology, high-dimensional data streams appear frequently nowadays, bringing urgent needs for effective statistical process control (SPC) tools. In such a context, the online monitoring problem of high-dimensional and correlated binary data streams is becoming very important. Conventional SPC methods for monitoring multivariate binary processes may fail when facing high-dimensional applications due to high computational complexity and the lack of efficiency. In this paper, motivated by an application in extreme weather surveillance, we propose a novel pairwise approach that considers the most informative pairwise correlation between any two data streams. The information is then integrated into an exponential weighted moving average (EWMA) charting scheme to monitor abnormal mean changes in high-dimensional binary data streams. Extensive simulation study together with a real-data analysis demonstrates the efficiency and applicability of the proposed control chart.},
  archive      = {J_JOAS},
  author       = {Zhiwen Fang and Wendong Li and Xin Liu and Xiaolong Pu and Dongdong Xiang},
  doi          = {10.1080/02664763.2021.1971633},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {4122-4136},
  shortjournal = {J. Appl. Stat.},
  title        = {Online monitoring of high-dimensional binary data streams with application to extreme weather surveillance},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parametric inference of the process capability index for
exponentiated exponential distribution. <em>JOAS</em>, <em>49</em>(16),
4097–4121. (<a
href="https://doi.org/10.1080/02664763.2021.1971632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process capability indices (PCIs) are most effective devices/techniques used in industries for determining the quality of products and performance of manufacturing processes. In this article, we consider the PCI C pc which is based on the proportion of conformance and is applicable to normally as well as non-normally and continuous as well as discrete distributed processes. In order to estimate the PCI C pc when the process follows exponentiated exponential distribution, we have used five classical methods of estimation. The performances of these classical estimators are compared with respect to their biases and mean squared errors (MSEs) of the index C pc through simulation study. Also, the confidence intervals for the index C pc are constructed using five bootstrap confidence interval (BCIs) methods. Monte Carlo simulation study has been carried out to compare the performances of these five BCIs in terms of their average width and coverage probabilities. Besides, net sensitivity (NS) analysis for the given PCI C pc is considered. We use two data sets related to electronic and food industries and two failure time data sets to illustrate the performance of the proposed methods of estimation and BCIs. Additionally, we have developed PCI C pc using aforementioned methods for generalized Rayleigh distribution.},
  archive      = {J_JOAS},
  author       = {Mahendra Saha and Sanku Dey and Saralees Nadarajah},
  doi          = {10.1080/02664763.2021.1971632},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {4097-4121},
  shortjournal = {J. Appl. Stat.},
  title        = {Parametric inference of the process capability index for exponentiated exponential distribution},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An approach for knowledge acquisition from a survey data by
conducting bayesian network modeling, adopting the robust coplot method.
<em>JOAS</em>, <em>49</em>(16), 4069–4096. (<a
href="https://doi.org/10.1080/02664763.2021.1971631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a methodological approach for extracting useful knowledge from survey data by performing Bayesian network (BN) modeling and adopting the robust coplot analysis results as prior knowledge about association patterns hidden in the data. By addressing the issue of BN construction when the expert knowledge is limited/not available, this proposed approach facilitates the modeling of large data sets describing numerously observed and latent variables. By answering the question of which node(s)/link(s) should be retained or discarded from a BN, we aim to determine a compact model of variables while considering the desired properties of data. The proposed method steps are explained on real data extracted from Turkey Demographic and Health Survey. First, a BN structure is created, which is based solely on the judgment of the analyst. Then the coplot results are employed to update the BN structure and the model parameters are updated using the updated structure and data. Loss scores of the BNs are used to ensure the success of the updated BN that inherits knowledge from coplot.},
  archive      = {J_JOAS},
  author       = {Derya Ersel and Yasemin Kayhan Atılgan},
  doi          = {10.1080/02664763.2021.1971631},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {4069-4096},
  shortjournal = {J. Appl. Stat.},
  title        = {An approach for knowledge acquisition from a survey data by conducting bayesian network modeling, adopting the robust coplot method},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An expectation maximization algorithm for high-dimensional
model selection for the ising model with misclassified states*.
<em>JOAS</em>, <em>49</em>(16), 4049–4068. (<a
href="https://doi.org/10.1080/02664763.2021.1970121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the misclassified Ising Model: a framework for analyzing dependent binary data where the binary state is susceptible to error. We extend previous theoretical results of a model selection method based on applying the LASSO to logistic regression at each node and show that the method will still correctly identify edges in the underlying graphical model under suitable misclassification settings. With knowledge of the misclassification process, an expectation maximization algorithm is developed that accounts for misclassification during model selection. We illustrate the increase of performance of the proposed expectation maximization algorithm with simulated data, and using data from a functional magnetic resonance imaging analysis.},
  archive      = {J_JOAS},
  author       = {David G. Sinclair and Giles Hooker},
  doi          = {10.1080/02664763.2021.1970121},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {4049-4068},
  shortjournal = {J. Appl. Stat.},
  title        = {An expectation maximization algorithm for high-dimensional model selection for the ising model with misclassified states*},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical models of near-accident event and pedestrian
behavior at non-signalized intersections. <em>JOAS</em>,
<em>49</em>(15), 4028–4048. (<a
href="https://doi.org/10.1080/02664763.2021.1962263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an innovative framework of modeling the statistical properties of the near-accident event and pedestrian behavior at non-signalized intersections based on Poisson process and logistic regression. The first contribution of this study is that the predictive intensity model of the near-accident event is established by regarding the near-accident event as a Poisson process on space of the vehicle velocity, distance to the intersection and lateral distance to the pedestrian at the time when pedestrian appears. Besides, logistic regression is used to build the model which can predict the probability of pedestrian behavior. The two proposed models are validated in a generative simulation. The simulated pedestrian behavior data is generated by the proposed models and compared with the real data. The real data set is from the drive recorder data base of Smart Mobility Research Center (SMRC) at Tokyo University of Agriculture and Technology. Accident and near-accident data has been collected in the city streets with an image-captured drive recorder mounted on a taxi since 2006. The findings in this study are expected to be useful for constructions of traffic simulators or safety control design which considers the pedestrian-vehicle interaction.},
  archive      = {J_JOAS},
  author       = {Xun Shen and Pongsathorn Raksincharoensak},
  doi          = {10.1080/02664763.2021.1962263},
  journal      = {Journal of Applied Statistics},
  number       = {15},
  pages        = {4028-4048},
  shortjournal = {J. Appl. Stat.},
  title        = {Statistical models of near-accident event and pedestrian behavior at non-signalized intersections},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation and selection in linear mixed models with missing
data under compound symmetric structure. <em>JOAS</em>, <em>49</em>(15),
4003–4027. (<a
href="https://doi.org/10.1080/02664763.2021.1969342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is quite appealing to extend existing theories in classical linear models to correlated responses where linear mixed-effects models are utilized and the dependency in the data is modeled by random effects. In the mixed modeling framework, missing values occur naturally due to dropouts or non-responses, which is frequently encountered when dealing with real data. Motivated by such problems, we aim to investigate the estimation and model selection performance in linear mixed models when missing data are present. Inspired by the property of the indicator function for missingness and its relation to missing rates, we propose an approach that records missingness in an indicator-based matrix and derive the likelihood-based estimators for all parameters involved in the linear mixed-effects models. Based on the proposed method for estimation, we explore the relationship between estimation and selection behavior over missing rates. Simulations and a real data application are conducted for illustrating the effectiveness of the proposed method in selecting the most appropriate model and in estimating parameters.},
  archive      = {J_JOAS},
  author       = {Yi-Ching Lee and Junfeng Shang},
  doi          = {10.1080/02664763.2021.1969342},
  journal      = {Journal of Applied Statistics},
  number       = {15},
  pages        = {4003-4027},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimation and selection in linear mixed models with missing data under compound symmetric structure},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust and efficient estimation of GARCH models based on
hellinger distance. <em>JOAS</em>, <em>49</em>(15), 3976–4002. (<a
href="https://doi.org/10.1080/02664763.2021.1970120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that financial data frequently contain outlying observations. Almost all methods and techniques used to estimate GARCH models are likelihood-based and thus generally non-robust against outliers. Minimum distance method, as an important tool for statistical inferences and a competitive alternative for achieving robustness, has surprisingly not been well explored for GARCH models. In this paper, we proposed a minimum Hellinger distance estimator (MHDE) and a minimum profile Hellinger distance estimator (MPHDE), depending on whether the innovation distribution is specified or not, for estimating the parameters in GARCH models. The construction and investigation of the two estimators are quite involved due to the non-i.i.d. nature of data. We proved that the MHDE is a consistent estimator and derived its bias in explicit expression. For both of the proposed estimators, we demonstrated their finite-sample performance through simulation studies and compared with the well-established methods including MLE, Gaussian Quasi-MLE, Non-Gaussian Quasi-MLE and Least Absolute Deviation estimator. Our numerical results showed that MHDE and MPHDE have much better performance than MLE-based methods when data are contaminated while simultaneously they are very competitive when data is clean, which testified to the robustness and efficiency of the two proposed MHD-type estimations.},
  archive      = {J_JOAS},
  author       = {Qiang Zhao and Liang Chen and Jingjing Wu},
  doi          = {10.1080/02664763.2021.1970120},
  journal      = {Journal of Applied Statistics},
  number       = {15},
  pages        = {3976-4002},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust and efficient estimation of GARCH models based on hellinger distance},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inference on moderation effect with third-variable effect
analysis – application to explore the trend of racial disparity in
oncotype dx test for breast cancer treatment. <em>JOAS</em>,
<em>49</em>(15), 3958–3975. (<a
href="https://doi.org/10.1080/02664763.2021.1968358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Third variable effect refers to the effect from a third variable that explains an observed relationship between an exposure and an outcome. Depending on whether there is causal relationship, typically, a third variable takes the format of a mediator or a confounder. A moderation effect is a special case of the third-variable effect, where the moderator and other variables have an interactive effect on the outcome. In this paper, we extend the R package ‘mma’ for moderation analysis so that third-variable effects can be reported at different levels of the moderator. The proposed moderation analysis use tree-structured models to automatically detect moderation effects and can handle both categorical and numerical moderators. We propose algorithms and graphical methods for making inference on moderation effects and illustrate the method under different scenarios of moderation effects. Finally, we apply the proposed method to explore the trend of racial disparities in the use of Oncotype DX recurrence tests among breast cancer patients. We found that the unexplained racial differences in using the tests have decreased from 2010 to 2015.},
  archive      = {J_JOAS},
  author       = {Qingzhao Yu and Lu Zhang and Xiaocheng Wu and Bin Li},
  doi          = {10.1080/02664763.2021.1968358},
  journal      = {Journal of Applied Statistics},
  number       = {15},
  pages        = {3958-3975},
  shortjournal = {J. Appl. Stat.},
  title        = {Inference on moderation effect with third-variable effect analysis – application to explore the trend of racial disparity in oncotype dx test for breast cancer treatment},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A one-sided exponentially weighted moving average control
chart for time between events. <em>JOAS</em>, <em>49</em>(15),
3928–3957. (<a
href="https://doi.org/10.1080/02664763.2021.1967894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exponentially weighted moving average (EWMA) control charts for time-between-events (TBE) are commonly suggested to monitor high-quality processes for the early detection of process deteriorations. In this study, an enhanced one-sided EWMA TBE scheme is developed for rapid detection of increases or decreases in the process mean. The use of the truncation method helps to improve the sensitivity of the proposed scheme in the process mean detection. Moreover, by taking the effects of parameter estimation into account, the proposed scheme with estimated parameters is also investigated. Both the average run length (ARL) and standard deviation of run length (SDRL) performances of the proposed scheme with known and estimated parameters are studied using the Markov chain method, respectively. Furthermore, an optimal design procedure is developed for the recommended one-sided EWMA TBE chart based on ARL. Numerical results show that the proposed optimal one-sided EWMA TBE chart is more sensitive than the existing optimal one-sided exponential EWMA chart in monitoring both upward and downward mean shifts. Meanwhile, it also performs better than the existing comparative scheme in resisting the effect of parameter estimation. Finally, two illustrative examples are considered to show the implementation of the proposed scheme for simulated and real datasets.},
  archive      = {J_JOAS},
  author       = {FuPeng Xie and Philippe Castagliola and YuLong Qiao and XueLong Hu and JinSheng Sun},
  doi          = {10.1080/02664763.2021.1967894},
  journal      = {Journal of Applied Statistics},
  number       = {15},
  pages        = {3928-3957},
  shortjournal = {J. Appl. Stat.},
  title        = {A one-sided exponentially weighted moving average control chart for time between events},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Confidence sets for dynamic poverty indexes. <em>JOAS</em>,
<em>49</em>(15), 3908–3927. (<a
href="https://doi.org/10.1080/02664763.2021.1967893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we consider different poverty indexes in a dynamic framework where individuals change their rate of income randomly in time. The primary objective of this paper is to assess the accuracy of the approximation of the indexes that can be obtained by applying the strong law of large numbers to an economic system composed of an infinite number of agents. The main result is a multivariate central limit theorem for dynamic poverty measures, which is obtained applying the theory of U-statistics. We also show how to get the confidence sets for the considered dynamic indexes, which show the appropriateness of the model. An application to the Italian income data from 1998 to 2012 confirms the effectiveness of the considered approach and the possibility to determine the evolution of poverty and inequality in real economies.},
  archive      = {J_JOAS},
  author       = {Guglielmo D&#39;Amico and Riccardo De Blasis},
  doi          = {10.1080/02664763.2021.1967893},
  journal      = {Journal of Applied Statistics},
  number       = {15},
  pages        = {3908-3927},
  shortjournal = {J. Appl. Stat.},
  title        = {Confidence sets for dynamic poverty indexes},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dimension-wise sparse low-rank approximation of a matrix
with application to variable selection in high-dimensional integrative
analyzes of association. <em>JOAS</em>, <em>49</em>(15), 3889–3907. (<a
href="https://doi.org/10.1080/02664763.2021.1967892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many research proposals involve collecting multiple sources of information from a set of common samples, with the goal of performing an integrative analysis describing the associations between sources. We propose a method that characterizes the dominant modes of co-variation between the variables in two datasets while simultaneously performing variable selection. Our method relies on a sparse, low rank approximation of a matrix containing pairwise measures of association between the two sets of variables. We show that the proposed method shares a close connection with another group of methods for integrative data analysis – sparse canonical correlation analysis (CCA). Under some assumptions, the proposed method and sparse CCA aim to select the same subsets of variables. We show through simulation that the proposed method can achieve better variable selection accuracies than two state-of-the-art sparse CCA algorithms. Empirically, we demonstrate through the analysis of DNA methylation and gene expression data that the proposed method selects variables that have as high or higher canonical correlation than the variables selected by sparse CCA methods, which is a rather surprising finding given that objective function of the proposed method does not actually maximize the canonical correlation.},
  archive      = {J_JOAS},
  author       = {J. C. Poythress and Cheolwoo Park and Jeongyoun Ahn},
  doi          = {10.1080/02664763.2021.1967892},
  journal      = {Journal of Applied Statistics},
  number       = {15},
  pages        = {3889-3907},
  shortjournal = {J. Appl. Stat.},
  title        = {Dimension-wise sparse low-rank approximation of a matrix with application to variable selection in high-dimensional integrative analyzes of association},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Markov (set) chains application to predict mortality rates
using extended milevsky–promislov generalized mortality models.
<em>JOAS</em>, <em>49</em>(15), 3868–3888. (<a
href="https://doi.org/10.1080/02664763.2021.1967891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mortality rates ( μ x , t μ x , t μx,t ) measure the frequency of deaths in a fixed: population and time interval. The ability to model and forecast μ x , t μ x , t μx,t allows determining, among others, fundamental characteristics of life expectancy tables, e.g. used to determine the amount of premium in life insurance, adequate to the risk of death. The article proposes a new method of modelling and forecasting μ x , t μ x , t μx,t , using the class of stochastic Milevsky–Promislov switch models with excitations. The excitations are modelled by second, fourth and sixth order polynomials of outputs from the non-Gaussian Linear Scalar Filter (nGLSF) model and taking into account the Markov (Set) chain. The Markov (Set) chain state space is defined based on even orders of the nGLSF polynomial. The model order determines the theoretical values of the death rates. The obtained results usually provide a more precise forecast of the mortality rates than the commonly used Lee–Carter model.},
  archive      = {J_JOAS},
  author       = {Piotr Sliwka},
  doi          = {10.1080/02664763.2021.1967891},
  journal      = {Journal of Applied Statistics},
  number       = {15},
  pages        = {3868-3888},
  shortjournal = {J. Appl. Stat.},
  title        = {Markov (Set) chains application to predict mortality rates using extended Milevsky–Promislov generalized mortality models},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A generalized BLUE approach for combining location and scale
information in a meta-analysis. <em>JOAS</em>, <em>49</em>(15),
3846–3867. (<a
href="https://doi.org/10.1080/02664763.2021.1967890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In systematic reviews and meta-analyses, one is interested in combining information from a variety of sources in order to obtain unbiased and efficient pooled estimates of the mean treatment effect compared to a control group along with the corresponding standard errors and confidence intervals, particularly when the source data is unavailable. However, in many studies the mean and standard deviation are not reported in lieu of other descriptive measures such as the median and quartiles. In this note we provide a theoretically optimal best linear unbiased estimator (BLUE) strategy for combining different types of summary information in order to pool results and estimate the overall treatment effect and the corresponding confidence intervals. Our approach is less biased and much more flexible than past attempts at solving this problem and can accommodate combining a variety of summary information across studies. We show that confidence intervals based on our methods have the appropriate coverage probabilities. Our proposed methods are theoretically justified and verified by simulation studies. The BLUE method is illustrated via a real data application.},
  archive      = {J_JOAS},
  author       = {Xin Yang and Alan D. Hutson and Dongliang Wang},
  doi          = {10.1080/02664763.2021.1967890},
  journal      = {Journal of Applied Statistics},
  number       = {15},
  pages        = {3846-3867},
  shortjournal = {J. Appl. Stat.},
  title        = {A generalized BLUE approach for combining location and scale information in a meta-analysis},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ensemble and calibration multiply robust estimation for
quantile treatment effect. <em>JOAS</em>, <em>49</em>(15), 3823–3845.
(<a href="https://doi.org/10.1080/02664763.2021.1966397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile treatment effects can be important causal estimands in the evaluation of biomedical treatments or interventions for health outcomes such as birthweight and medical cost. However, the existing estimators require either a propensity score model or a conditional density vector model is correctly specified, which is difficult to verify in practice. In this paper, we allow multiple models for propensity score and conditional density vector, then construct a class of calibration estimators based on multiple imputation and inverse probability weighting approaches via empirical likelihood. The resulting estimators multiply robust in the sense that they are consistent if any one of these models is correctly specified. Moreover, we propose another class of ensemble estimators to reduce computational burden while ensuring multiple robustness. Simulations are performed to evaluate the finite sample performance of the proposed estimators. Two applications to the birthweight of infants born in the United States and AIDS Clinical Trials Group Protocol 175 data are also presented.},
  archive      = {J_JOAS},
  author       = {Xiaohong He and Lei Wang},
  doi          = {10.1080/02664763.2021.1966397},
  journal      = {Journal of Applied Statistics},
  number       = {15},
  pages        = {3823-3845},
  shortjournal = {J. Appl. Stat.},
  title        = {Ensemble and calibration multiply robust estimation for quantile treatment effect},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical inference for a relaxation index of stochastic
dominance under density ratio model. <em>JOAS</em>, <em>49</em>(15),
3804–3822. (<a
href="https://doi.org/10.1080/02664763.2021.1965966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic dominance is usually used to rank random variables by comparing their distributions, so it is widely applied in economics and finance. In actual applications, complete stochastic dominance is too demanding to meet, so relaxation indexes of stochastic dominance have attracted more attention. The π index, the biggest gap between two distributions, can be a measure of the degree of deviation from complete dominance. The traditional estimation method is to use the empirical distribution functions to estimate it. Considering the populations under comparison are generally of the same nature, we can link the populations through density ratio model under certain condition. Based on this model, we propose a new estimator and establish its statistical inference theory. Simulation results show that the proposed estimator substantially improves estimation efficiency and power of the tests and coverage probabilities satisfactorily match the confidence levels of the tests, which show the superiority of the proposed estimator. Finally we apply our method to a real example of the Chinese household incomes.},
  archive      = {J_JOAS},
  author       = {Weiwei Zhuang and Yadong Li and Guoxin Qiu},
  doi          = {10.1080/02664763.2021.1965966},
  journal      = {Journal of Applied Statistics},
  number       = {15},
  pages        = {3804-3822},
  shortjournal = {J. Appl. Stat.},
  title        = {Statistical inference for a relaxation index of stochastic dominance under density ratio model},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new approach to modeling positive random variables with
repeated measures. <em>JOAS</em>, <em>49</em>(15), 3784–3803. (<a
href="https://doi.org/10.1080/02664763.2021.1963422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many situations, it is common to have more than one observation per experimental unit, thus generating the experiments with repeated measures. In the modeling of such experiments, it is necessary to consider and model the intra-unit dependency structure. In the literature, there are several proposals to model positive continuous data with repeated measures. In this paper, we propose one more with the generalization of the beta prime regression model. We consider the possibility of dependence between observations of the same unit. Residuals and diagnostic tools also are discussed. To evaluate the finite-sample performance of the estimators, using different correlation matrices and distributions, we conducted a Monte Carlo simulation study. The methodology proposed is illustrated with an analysis of a real data set. Finally, we create an R package for easy access to publicly available the methodology described in this paper.},
  archive      = {J_JOAS},
  author       = {João Victor B. de Freitas and Juvêncio S. Nobre and Marcelo Bourguignon and Manoel Santos-Neto},
  doi          = {10.1080/02664763.2021.1963422},
  journal      = {Journal of Applied Statistics},
  number       = {15},
  pages        = {3784-3803},
  shortjournal = {J. Appl. Stat.},
  title        = {A new approach to modeling positive random variables with repeated measures},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An endemic–epidemic beta model for time series of infectious
disease proportions. <em>JOAS</em>, <em>49</em>(15), 3769–3783. (<a
href="https://doi.org/10.1080/02664763.2021.1962264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series of proportions of infected patients or positive specimens are frequently encountered in disease control and prevention. Since proportions are bounded and often asymmetrically distributed, conventional Gaussian time series models only apply to suitably transformed proportions. Here we borrow both from beta regression and from the well-established HHH model for infectious disease counts to propose an endemic–epidemic beta model for proportion time series. It accommodates the asymmetric shape and heteroskedasticity of proportion distributions and is consistent for complementary proportions. Coefficients can be interpreted in terms of odds ratios. A multivariate formulation with spatial power-law weights enables the joint estimation of model parameters from multiple regions. In our application to a flu activity index in the USA, we find that the endemic–epidemic beta model provides a better fit than a seasonal ARIMA model for the logit-transformed proportions. Furthermore, a multivariate approach can improve regional forecasts and reduce model complexity in comparison to univariate beta models stratified by region.},
  archive      = {J_JOAS},
  author       = {Junyi Lu and Sebastian Meyer},
  doi          = {10.1080/02664763.2021.1962264},
  journal      = {Journal of Applied Statistics},
  number       = {15},
  pages        = {3769-3783},
  shortjournal = {J. Appl. Stat.},
  title        = {An endemic–epidemic beta model for time series of infectious disease proportions},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Causal mediation analysis between resistance exercise and
reduced risk of cardiovascular disease based on the aerobics center
longitudinal study. <em>JOAS</em>, <em>49</em>(14), 3750–3767. (<a
href="https://doi.org/10.1080/02664763.2021.1962260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health benefits of resistance exercise (RE), particularly in lowering cardiovascular disease (CVD) risks, are less understood in comparison to aerobic exercise (AE). Motivated by big data from the Aerobics Center Longitudinal Study (ACLS), we study the direct and indirect effects of RE on CVD risks. The primary outcome in our study, total CVD events (CVD morbidity and mortality combined), is modeled as a survival outcome. To investigate the pathway from RE to CVD outcome through potential mediators, we first conduct causal mediation analysis based on marginal structural models (MSMs). To fully account the information from repeated measurements of the mediators, we also adopt a joint model of the CVD survival outcome and multiple longitudinal trajectories of the mediators. Results show statistically significant direct effects of RE and AE on lowering the risk of total CVD events under each pathway. The causal effect of RE and AE on CVD risk is also studied across different age and gender groups. Furthermore, we produce a ranking for the relative importance of the potential risk factors for CVD, with total cholesterol ranking the highest.},
  archive      = {J_JOAS},
  author       = {Jiasheng Huang and Yehua Li and Angelique G. Brellenthin and Duck-chul Lee and Xuemei Sui and Steven N. Blair},
  doi          = {10.1080/02664763.2021.1962260},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {3750-3767},
  shortjournal = {J. Appl. Stat.},
  title        = {Causal mediation analysis between resistance exercise and reduced risk of cardiovascular disease based on the aerobics center longitudinal study},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time series modelling methods to forecast the volume of
self-assessment tax returns in the UK. <em>JOAS</em>, <em>49</em>(14),
3732–3749. (<a
href="https://doi.org/10.1080/02664763.2021.1953448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Her Majesty&#39;s Revenue and Customs (HMRC) has the ambitious target of making tax digital for all its customers and collecting tax in a more efficient, effective and accurate manner for both the government and UK taxpayers. Self-assessment tax returns, the biggest key business event for HMRC, is also one of the most popular digital services with over 90\% of the approximately 12 million taxpayers in self assessment filing their return online each year. The majority of returns are filed in January immediately prior to the self-assessment deadline (31st January), putting significant pressure not only on the self-assessment digital service but also on all other HMRC digital services. Hence, understanding and predicting demand for the system is vital to provide a robust and responsive service. We therefore developed mathematical models with Bayesian inference techniques to forecast volumes of Self-assessment (SA) returns submitted online during January, providing accurate hourly predictions of traffic on the digital system in the run up to the SA deadline. Because none of the models being considered is believed to be the true model, we use an ensemble modelling technique that combines forecasts from different models to develop a less risky demand forecast.},
  archive      = {J_JOAS},
  author       = {Garo Panikian and Gabby Colmenares Reverol and Jayne Rhodes and Emma McLarnon and Sarah Keast and Kokouvi Gamado},
  doi          = {10.1080/02664763.2021.1953448},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {3732-3749},
  shortjournal = {J. Appl. Stat.},
  title        = {Time series modelling methods to forecast the volume of self-assessment tax returns in the UK},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Likelihood ratio test for genetic association study with
case–control data under probit model. <em>JOAS</em>, <em>49</em>(14),
3717–3731. (<a
href="https://doi.org/10.1080/02664763.2021.1962261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probit and Logit models are the most popular for binary disease statusing in genetic association studies. They are equally used and nearly exchangeable in the analysis of prospectively collected data. However, no strong inferences were made based on Probit models for the retrospectively collected case–control data, especially in the presence of random effects. This paper systematically investigates the performance of Probit mixed-effects models for case–control data. We find that the retrospective likelihood has a closed-form, which motivates the development of likelihood ratio tests for genetic association. Specifically, we developed four likelihood ratio tests based on whether the disease prevalence is completely unavailable, partly available, or completely available. We show that their limiting distribution without a genetic effect is an equal mixture of two chi-square distributions with degrees of freedom 1 and 2, respectively. Our simulations indicate that they can have a remarkable power gain against the popular Logit-model-based score tests, and the disease prevalence information can enhance the power of the likelihood ratio tests. After analyzing a Kenya malaria data, we found out that the proposed test produces a significant result on the association of the gene ABO with malaria, whereas the commonly used competitors fail.},
  archive      = {J_JOAS},
  author       = {Zhen Sheng and Yukun Liu and Pengfei Li and Jing Qin},
  doi          = {10.1080/02664763.2021.1962261},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {3717-3731},
  shortjournal = {J. Appl. Stat.},
  title        = {Likelihood ratio test for genetic association study with case–control data under probit model},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shrinkage estimation of fixed and random effects in linear
quantile mixed models. <em>JOAS</em>, <em>49</em>(14), 3693–3716. (<a
href="https://doi.org/10.1080/02664763.2021.1962262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a Bayesian analysis of linear mixed models for quantile regression using a modified Cholesky decomposition for the covariance matrix of random effects and an asymmetric Laplace distribution for the error distribution. We consider several novel Bayesian shrinkage approaches for both fixed and random effects in a linear mixed quantile model using extended L 1 L 1 L1 penalties. To improve mixing of the Markov chains, a simple and efficient partially collapsed Gibbs sampling algorithm is developed for posterior inference. We also extend the framework to a Bayesian mixed expectile model and develop a Metropolis–Hastings acceptance–rejection (MHAR) algorithm using proposal densities based on iteratively weighted least squares estimation. The proposed approach is then illustrated via both simulated and real data examples. Results indicate that the proposed approach performs very well in comparison to the other approaches.},
  archive      = {J_JOAS},
  author       = {Yonggang Ji and Haifang Shi},
  doi          = {10.1080/02664763.2021.1962262},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {3693-3716},
  shortjournal = {J. Appl. Stat.},
  title        = {Shrinkage estimation of fixed and random effects in linear quantile mixed models},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robust and efficient variable selection method for linear
regression. <em>JOAS</em>, <em>49</em>(14), 3677–3692. (<a
href="https://doi.org/10.1080/02664763.2021.1962259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable selection is fundamental to high dimensional statistical modeling, and many approaches have been proposed. However, existing variable selection methods do not perform well in presence of outliers in response variable or/and covariates. In order to ensure a high probability of correct selection and efficient parameter estimation, we investigate a robust variable selection method based on a modified Huber&#39;s function with an exponential squared loss tail. We also prove that the proposed method has oracle properties. Furthermore, we carry out simulation studies to evaluate the performance of the proposed method for both p n . Our simulation results indicate that the proposed method is efficient and robust against outliers and heavy-tailed distributions. Finally, a real dataset from an air pollution mortality study is used to illustrate the proposed method.},
  archive      = {J_JOAS},
  author       = {Zhuoran Yang and Liya Fu and You-Gan Wang and Zhixiong Dong and Yunlu Jiang},
  doi          = {10.1080/02664763.2021.1962259},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {3677-3692},
  shortjournal = {J. Appl. Stat.},
  title        = {A robust and efficient variable selection method for linear regression},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Doubly multivariate linear models with block exchangeable
distributed errors and site-dependent covariates. <em>JOAS</em>,
<em>49</em>(14), 3659–3676. (<a
href="https://doi.org/10.1080/02664763.2021.1959529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of testing the intercept and slope parameters of doubly multivariate linear models with site-dependent covariates using Rao&#39;s score test (RST) is studied. The RST statistic is developed for a block exchangeable covariance structure on the error vector under the assumption of multivariate normality. We compare our developed RST statistic with the likelihood ratio test (LRT) statistic. Monte Carlo simulations indicate that the RST statistic is much more accurate than its counterpart LRT statistic and it takes significantly less computation time than the LRT statistic. The proposed method is illustrated with an example of multiple response variables measured on multiple trees in a single plot in an agricultural study.},
  archive      = {J_JOAS},
  author       = {Timothy Opheim and Anuradha Roy},
  doi          = {10.1080/02664763.2021.1959529},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {3659-3676},
  shortjournal = {J. Appl. Stat.},
  title        = {Doubly multivariate linear models with block exchangeable distributed errors and site-dependent covariates},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multicriteria decision frontiers for prescription anomaly
detection over time. <em>JOAS</em>, <em>49</em>(14), 3638–3658. (<a
href="https://doi.org/10.1080/02664763.2021.1959528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health care prescription fraud and abuse result in major financial losses and adverse health effects. The growing budget deficits of health insurance programs and recent opioid drug abuse crisis in the United States have accelerated the use of analytical methods. Unsupervised methods such as clustering and anomaly detection could help the health care auditors to evaluate the billing patterns when embedded into rule-based frameworks. These decision models can aid policymakers in detecting potential suspicious activities. This manuscript proposes an unsupervised temporal learning-based decision frontier model using the real world Medicare Part D prescription data collected over 5 years. First, temporal probabilistic hidden groups of drugs are retrieved using a structural topic model with covariates. Next, we construct combined concentration curves and Gini measures considering the weighted impact of temporal observations for prescription patterns, in addition to the Gini values for the cost. The novel decision frontier utilizes this output and enables health care practitioners to assess the trade-offs among different criteria and to identify audit leads.},
  archive      = {J_JOAS},
  author       = {Babak Zafari and Tahir Ekin and Fabrizio Ruggeri},
  doi          = {10.1080/02664763.2021.1959528},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {3638-3658},
  shortjournal = {J. Appl. Stat.},
  title        = {Multicriteria decision frontiers for prescription anomaly detection over time},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A family of bimodal distributions generated by distributions
with positive support. <em>JOAS</em>, <em>49</em>(14), 3614–3637. (<a
href="https://doi.org/10.1080/02664763.2021.1959527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bimodal data sets are very common in different areas of knowledge. The crude birth rates data, fish length data, egg diameter data, the eruption and interruption times of the Old Faithful geyser, are examples of this type of data. In this paper, a new class of symmetric density functions for modeling bimodal data as described above are presented. From density functions with support on [ 0 , + ∞ ) [ 0 , + ∞ ) [0,+∞) , the symmetry is getting by reflecting the density function in the negative semi-axis with their respective normalization. In this way, if the primitive density function is unimodal, then the resulting density will be bimodal. We introduce asymmetry parameters and study their behavior, in particular the values of their modes and some other statistical values of interest. The cases for densities generated by Gamma, Weibull, Log-normal, and Birnbaum-Saunders densities, among others are studied. Statistical inference is performed from a classical perspective. A small simulation study to evaluate the benefits and limitations of the new proposal. In addition, an application to a data set related to the fetal weight in grams obtained through ultrasound in a sample of 500 units is also presented; the results show the great usefulness of the model in practical situations.},
  archive      = {J_JOAS},
  author       = {Guillermo Martínez-Flórez and Eliseo Martínez and Roger Tovar-Falón and Héctor W. Gómez},
  doi          = {10.1080/02664763.2021.1959527},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {3614-3637},
  shortjournal = {J. Appl. Stat.},
  title        = {A family of bimodal distributions generated by distributions with positive support},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bias-corrected estimators for proportion of true null
hypotheses: Application of adaptive FDR-controlling in segmented failure
data. <em>JOAS</em>, <em>49</em>(14), 3591–3613. (<a
href="https://doi.org/10.1080/02664763.2021.1957790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two recently introduced model-based bias-corrected estimators for proportion of true null hypotheses ( π 0 π 0 π0 ) under multiple hypotheses testing scenario have been restructured for random observations under a suitable failure model, available for each of the common hypotheses. Based on stochastic ordering, a new motivation behind formulation of some related estimators for π 0 π 0 π0 is given. The reduction of bias for the model-based estimators are theoretically justified and algorithms for computing the estimators are also presented. The estimators are also used to formulate a popular adaptive multiple testing procedure. Extensive numerical study supports superiority of the bias-corrected estimators. The necessity of the proper distributional assumption for the failure data in the context of the model-based bias-corrected method has been highlighted. A case-study is done with a real-life dataset in connection with reliability and warranty studies to demonstrate the applicability of the procedure, under a non-Gaussian setup. The results obtained are in line with the intuition and experience of the subject expert. An intriguing discussion has been attempted to conclude the article that also indicates the future scope of study.},
  archive      = {J_JOAS},
  author       = {Aniket Biswas and Gaurangadeb Chattopadhyay and Aditya Chatterjee},
  doi          = {10.1080/02664763.2021.1957790},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {3591-3613},
  shortjournal = {J. Appl. Stat.},
  title        = {Bias-corrected estimators for proportion of true null hypotheses: Application of adaptive FDR-controlling in segmented failure data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new GEE method to account for heteroscedasticity using
asymmetric least-square regressions. <em>JOAS</em>, <em>49</em>(14),
3564–3590. (<a
href="https://doi.org/10.1080/02664763.2021.1957789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized estimating equations ( G E E ) ( G E E ) (GEE) are widely used to analyze longitudinal data; however, they are not appropriate for heteroscedastic data, because they only estimate regressor effects on the mean response – and therefore do not account for data heterogeneity. Here, we combine the G E E G E E GEE with the asymmetric least squares (expectile) regression to derive a new class of estimators, which we call generalized expectile estimating equations ( G E E E ) . The G E E E model estimates regressor effects on the expectiles of the response distribution, which provides a detailed view of regressor effects on the entire response distribution. In addition to capturing data heteroscedasticity, the GEEE extends the various working correlation structures to account for within-subject dependence. We derive the asymptotic properties of the G E E E estimators and propose a robust estimator of its covariance matrix for inference (see our R package, github.com/AmBarry/expectgee ). Our simulations show that the GEEE estimator is non-biased and efficient, and our real data analysis shows it captures heteroscedasticity.},
  archive      = {J_JOAS},
  author       = {Amadou Barry and Karim Oualkacha and Arthur Charpentier},
  doi          = {10.1080/02664763.2021.1957789},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {3564-3590},
  shortjournal = {J. Appl. Stat.},
  title        = {A new GEE method to account for heteroscedasticity using asymmetric least-square regressions},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wavelet analysis of variance box plot. <em>JOAS</em>,
<em>49</em>(14), 3536–3563. (<a
href="https://doi.org/10.1080/02664763.2021.1951685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional box plots satisfy two needs; visualization of functional data, and the calculation of important box plot statistics. Data visualization illuminates key characteristics of functional sets missed by statistical tests and summary statistics. The calculation of box plot statistics for functional sets permits a novel comparison more suited to functional data. The functional box plot uses a depth method to visualize and rank smooth functional curves in terms of a mean, box, whiskers, and outliers. The functional box plot improves upon other classic functional data analysis tools such as functional principal components and discriminant analysis for outlier detection. This research adds wavelet analysis as a generating mechanism along with depth for functional box plots to visualize functional data and calculate relevant statistics. The wavelet analysis of variance box plot tool gives competitive error rates in Gaussian test cases with magnitude outliers, and outperforms the functional box plot, for Gaussian test cases with shape outliers. Further, we show wavelet analysis is well suited at approximating irregular and noisy functional data and show the enhanced capability of WANOVA box plots to classify shape outliers which follow a different pattern than other functional data for both simulated and real data instances.},
  archive      = {J_JOAS},
  author       = {Jeffrey Williams and Raymond R. Hill and Joseph J. Pignatiello Jr. and Eric Chicken},
  doi          = {10.1080/02664763.2021.1951685},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {3536-3563},
  shortjournal = {J. Appl. Stat.},
  title        = {Wavelet analysis of variance box plot},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric methods for incomplete longitudinal count
data with an application to health and retirement study. <em>JOAS</em>,
<em>49</em>(14), 3513–3535. (<a
href="https://doi.org/10.1080/02664763.2021.1951684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose and explore a novel semiparametric approach to analyzing longitudinal count data. We address the issue of missingness in longitudinal data and propose a weighted generalized estimations equations approach to fitting marginal mean response models for count responses with dropouts. Also, we investigate a spline regression approach to approximating the curvilinear relationship between the mean response and covariates. The asymptotic properties of the proposed estimators are studied in some detail. The empirical properties of the estimators are investigated using Monte Carlo simulations. An application is also provided using actual survey data obtained from the Health and Retirement Study (HRS).},
  archive      = {J_JOAS},
  author       = {Seema Zubair and Sanjoy K. Sinha},
  doi          = {10.1080/02664763.2021.1951684},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {3513-3535},
  shortjournal = {J. Appl. Stat.},
  title        = {Semiparametric methods for incomplete longitudinal count data with an application to health and retirement study},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Investigation of the correlation of successive earthquakes
preceding main shocks in the greek territory. <em>JOAS</em>,
<em>49</em>(13), 3495–3512. (<a
href="https://doi.org/10.1080/02664763.2021.1939661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Canonical Correlation Analysis (CCA) estimates the correlation between two vector variables by maximizing the correlation of linear combinations of their respective components. Here, the CCA is used to find correlation patterns in the last five successive, per pairs, earthquakes ( M ≥ 4.0 M ≥ 4.0 M≥4.0 ) preceding 271 main shocks ( M ≥ 5.5 M ≥ 5.5 M≥5.5 ) that occurred in the Greek territory during 1964–2018. The vector variables have two components, the earthquake magnitude and interevent time. The statistical significance of CCA is determined by the standard parametric test along with two proposed randomization tests, one using random shuffling of each paired dataset and one using randomly selected pairs of successive earthquakes. Simulations were designed on synthetic data from vector variables having the statistical characteristics of the real observations. The results on uncorrelated variables showed the correct size for the two randomization tests but larger type I error for the parametric significance test for small sample size. For correlated variables, the test power was equally high for both test types. The application of CCA and the significance tests to the Greek seismicity evidence the significant correlation among the last five successive preshocks, proving to be a promising tool in an a posteriori short-term earthquake forecasting.},
  archive      = {J_JOAS},
  author       = {D. Chorozoglou and D. Kugiumtzis and E. Papadimitriou},
  doi          = {10.1080/02664763.2021.1939661},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {3495-3512},
  shortjournal = {J. Appl. Stat.},
  title        = {Investigation of the correlation of successive earthquakes preceding main shocks in the greek territory},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An overview of heavy-tail extensions of multivariate
gaussian distribution and their relations. <em>JOAS</em>,
<em>49</em>(13), 3477–3494. (<a
href="https://doi.org/10.1080/02664763.2022.2044018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many extensions of the multivariate normal distribution to heavy-tailed distributions are proposed in the literature, which includes scale Gaussian mixture distribution, elliptical distribution, generalized elliptical distribution and transelliptical distribution. The inferences for each family of distributions are well studied. However, extensions are overlapped or similar to each other, and it is hard to differentiate one extension from the other. For this reason, in practice, researchers simply pick one of many extensions and apply it to the analysis. In this paper, to enlighten practitioners who should conduct statistical procedures not based on their preferences but based on how data look like, we comparatively review various extensions and their estimators. Also, we fully investigate the inclusion and exclusion relations of different extensions by Venn diagrams and examples. Moreover, in the numerical study, we illustrate visual differences of the extensions by bivariate plots and analyze different scatter matrix estimators based on the microarray data.},
  archive      = {J_JOAS},
  author       = {Seongoh Park and Johan Lim},
  doi          = {10.1080/02664763.2022.2044018},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {3477-3494},
  shortjournal = {J. Appl. Stat.},
  title        = {An overview of heavy-tail extensions of multivariate gaussian distribution and their relations},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inverse lindley power series distributions: A new
compounding family and regression model with censored data.
<em>JOAS</em>, <em>49</em>(13), 3451–3476. (<a
href="https://doi.org/10.1080/02664763.2021.1951683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new class of distributions by compounding the inverse Lindley distribution and power series distributions which is called compound inverse Lindley power series (CILPS) distributions. An important feature of this distribution is that the lifetime of the component associated with a particular risk is not observable, rather only the minimum lifetime value among all risks is observable. Further, these distributions exhibit an unimodal failure rate. Various properties of the distribution are derived. Besides, two special models of the new family are investigated. The model parameters of the two sub-models of the new family are obtained by the methods of maximum likelihood, least square, weighted least square and maximum product of spacing and compared them using the Monte Carlo simulation study. Besides, the log compound inverse Lindley regression model for censored data is proposed. Three real data sets are analyzed to illustrate the flexibility and importance of the proposed models.},
  archive      = {J_JOAS},
  author       = {Mohammed K. Shakhatreh and Sanku Dey and Devendra Kumar},
  doi          = {10.1080/02664763.2021.1951683},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {3451-3476},
  shortjournal = {J. Appl. Stat.},
  title        = {Inverse lindley power series distributions: A new compounding family and regression model with censored data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian variable selection in quantile regression with
random effects: An application to municipal human development index.
<em>JOAS</em>, <em>49</em>(13), 3436–3450. (<a
href="https://doi.org/10.1080/02664763.2021.1950654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to the Atlas of Human Development in Brazil, the income dimension of Municipal Human Development Index (MHDI-I) is an indicator that shows the population&#39;s ability in a municipality to ensure a minimum standard of living to provide their basic needs, such as water, food and shelter. In public policy, one of the research objectives is to identify social and economic variables that are associated with this index. Due to the income inequality, evaluate these associations in quantiles, instead of the mean, could be more interest. Thus, in this paper, we develop a Bayesian variable selection in quantile regression models with hierarchical random effects. In particular, we assume a likelihood function based on the Generalized Asymmetric Laplace distribution, and a spike-and-slab prior is used to perform variable selection. The Generalized Asymmetric Laplace distribution is a more general alternative than the Asymmetric Laplace one, which is a common approach used in quantile regression under the Bayesian paradigm. The performance of the proposed method is evaluated via a comprehensive simulation study, and it is applied to the MHDI-I from municipalities located in the state of Rio de Janeiro.},
  archive      = {J_JOAS},
  author       = {Marcus G. L. Nascimento and Kelly C. M. Gonçalves},
  doi          = {10.1080/02664763.2021.1950654},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {3436-3450},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian variable selection in quantile regression with random effects: An application to municipal human development index},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Confidence intervals for assessing equivalence of two
treatments with combined unilateral and bilateral data. <em>JOAS</em>,
<em>49</em>(13), 3414–3435. (<a
href="https://doi.org/10.1080/02664763.2021.1949440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Responses from the paired organs are generally highly correlated in bilateral studies, statistical procedures ignoring the correlation could lead to incorrect results. Note the intraclass correlation in the study of combined unilateral and bilateral outcomes; 11 confidence intervals (CIs) including 7 asymptotic CIs and 4 Bootstrap-resampling CIs for assessing the equivalence of 2 treatments are derived under Rosner&#39;s correlated binary data model. Performance is evaluated with respect to the empirical coverage probability (ECP), the empirical coverage width (ECW) and the ratio of the mesial non-coverage probability to the non-coverage probability (RMNCP) via simulation studies. Simulation results show that (i) all CIs except for the Wald CI and the bias-corrected Bootstrap percentile CI generally produce satisfactory ECPs and hence are recommended; (ii) all CIs except for the bias-corrected Bootstrap percentile CI provide preferred RMNCPs and are more symmetrical; (iii) as the measurement of the dependence increases, the ECWs of all CIs except for the score CI and the profile likelihood CI show increasing patterns that look like linear, while there is no obvious pattern on the ECPs of all CIs except for the profile likelihood CI. A data set from an otolaryngologic study is used to illustrate the proposed methods.},
  archive      = {J_JOAS},
  author       = {Shi-Fang Qiu and Ji-Ran Tao},
  doi          = {10.1080/02664763.2021.1949440},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {3414-3435},
  shortjournal = {J. Appl. Stat.},
  title        = {Confidence intervals for assessing equivalence of two treatments with combined unilateral and bilateral data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating correlations between vaccine clinical trial
outcomes. <em>JOAS</em>, <em>49</em>(13), 3392–3413. (<a
href="https://doi.org/10.1080/02664763.2021.1949439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We demonstrate how a linear factor model with latent variables can be used to estimate correlations between the outcomes of clinical trials. These correlations are needed for many policy questions of drug/vaccine development (such as calculating the optimal size of financial incentives) and the literature so far has relied on expert opinions. We apply our methodology to the case of vaccines and show that the estimated correlations are highly significant. We also illustrate how the estimated correlations can be used to find the probability of obtaining a successful vaccine out of a certain number of candidates and to determine optimal investment in vaccine development.},
  archive      = {J_JOAS},
  author       = {Alexey Rey and Olga Rozanova and Sergey Zhuk},
  doi          = {10.1080/02664763.2021.1949439},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {3392-3413},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimating correlations between vaccine clinical trial outcomes},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Compound poisson frailty model with a gamma process prior
for the baseline hazard: Accounting for a cured fraction. <em>JOAS</em>,
<em>49</em>(13), 3377–3391. (<a
href="https://doi.org/10.1080/02664763.2021.1947997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cox model and traditional frailty models assume that all individuals will eventually experience the event of interest. This assumption is often overlooked, and situations will arise where it is not realistic. We introduce Compound Poisson frailty model for survival analysis to deal with populations in which some of the individuals will not experience the event of interest. This model assumes that the target population is a mixture of individuals with zero frailty and those with positive frailty. In this paper, we consider a compound Poisson frailty model for right-censored event times from a Bayesian perspective and compute the Bayesian estimator using the Markov Chain Monte Carlo method, where a Gamma process prior is adopted for the baseline hazard function. Furthermore, we evaluate the approach using simulation studies and demonstrate the methodology by analyzing the data from achalasia patient cohort.},
  archive      = {J_JOAS},
  author       = {Maryam Rahmati and Parisa Rezanejad Asl and Javad Mikaeli and Hojjat Zeraati and Aliakbar Rasekhi},
  doi          = {10.1080/02664763.2021.1947997},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {3377-3391},
  shortjournal = {J. Appl. Stat.},
  title        = {Compound poisson frailty model with a gamma process prior for the baseline hazard: Accounting for a cured fraction},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A joint modeling approach for longitudinal outcomes and
non-ignorable dropout under population heterogeneity in mental health
studies. <em>JOAS</em>, <em>49</em>(13), 3361–3376. (<a
href="https://doi.org/10.1080/02664763.2021.1945000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper proposes a joint mixture model to model non-ignorable drop-out in longitudinal cohort studies of mental health outcomes. The model combines a (non)-linear growth curve model for the time-dependent outcomes and a discrete-time survival model for the drop-out with random effects shared by the two sub-models. The mixture part of the model takes into account population heterogeneity by accounting for latent subgroups of the shared effects that may lead to different patterns for the growth and the drop-out tendency. A simulation study shows that the joint mixture model provides greater precision in estimating the average slope and covariance matrix of random effects. We illustrate its benefits with data from a longitudinal cohort study that characterizes depression symptoms over time yet is hindered by non-trivial participant drop-out.},
  archive      = {J_JOAS},
  author       = {Jung Yeon Park and Melanie M. Wall and Irini Moustaki and Arnold H. Grossman},
  doi          = {10.1080/02664763.2021.1945000},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {3361-3376},
  shortjournal = {J. Appl. Stat.},
  title        = {A joint modeling approach for longitudinal outcomes and non-ignorable dropout under population heterogeneity in mental health studies},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Maximum precision estimation for a step-stress model using
two-stage methodologies. <em>JOAS</em>, <em>49</em>(13), 3344–3360. (<a
href="https://doi.org/10.1080/02664763.2021.1944997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a two-stage sequential estimation procedure to estimate the parameters of a cumulative exposure model under an accelerated testing scenario. In particular, we focus on a step-stress model where the stress level changes after a pre-specified number of failures occur, which is also random. This is termed as a ‘random stress change time’ in the literature. We further aim to estimate these parameters using maximum precision and hence use a certain variance optimality criteria. Our proposed two-stage estimation procedures follow interesting efficiency properties and their applicability is seen through extensive simulation analyses and a pseudo-real data example from reliability studies.},
  archive      = {J_JOAS},
  author       = {Sudeep R. Bapat and Yan Zhuang},
  doi          = {10.1080/02664763.2021.1944997},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {3344-3360},
  shortjournal = {J. Appl. Stat.},
  title        = {Maximum precision estimation for a step-stress model using two-stage methodologies},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regression modelling of interval censored data based on the
adaptive ridge procedure. <em>JOAS</em>, <em>49</em>(13), 3319–3343. (<a
href="https://doi.org/10.1080/02664763.2021.1944996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new method for the analysis of time to ankylosis complication on a dataset of replanted teeth is proposed. In this context of left-censored, interval-censored and right-censored data, a Cox model with piecewise constant baseline hazard is introduced. Estimation is carried out with the expectation maximisation (EM) algorithm by treating the true event times as unobserved variables. This estimation procedure is shown to produce a block diagonal Hessian matrix of the baseline parameters. Taking advantage of this interesting feature in the EM algorithm, a L 0 penalised likelihood method is implemented in order to automatically determine the number and locations of the cuts of the baseline hazard. This procedure allows to detect specific areas of time where patients are at greater risks for ankylosis. The method can be directly extended to the inclusion of exact observations and to a cure fraction. Theoretical results are obtained which allow to derive statistical inference of the model parameters from asymptotic likelihood theory. Through simulation studies, the penalisation technique is shown to provide a good fit of the baseline hazard and precise estimations of the resulting regression parameters.},
  archive      = {J_JOAS},
  author       = {Olivier Bouaziz and Eva Lauridsen and Grégory Nuel},
  doi          = {10.1080/02664763.2021.1944996},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {3319-3343},
  shortjournal = {J. Appl. Stat.},
  title        = {Regression modelling of interval censored data based on the adaptive ridge procedure},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient design of geographically-defined clusters with
spatial autocorrelation. <em>JOAS</em>, <em>49</em>(13), 3300–3318. (<a
href="https://doi.org/10.1080/02664763.2021.1941807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clusters form the basis of a number of research study designs including survey and experimental studies. Cluster-based designs can be less costly but also less efficient than individual-based designs due to correlation between individuals within the same cluster. Their design typically relies on ad hoc choices of correlation parameters, and is insensitive to variations in cluster design. This article examines how to efficiently design clusters where they are geographically defined by demarcating areas incorporating individuals and households or other units. Using geostatistical models for spatial autocorrelation, we generate approximations to within cluster average covariance in order to estimate the effective sample size given particular cluster design parameters. We show how the number of enumerated locations, cluster area, proportion sampled, and sampling method affect the efficiency of the design and consider the optimization problem of choosing the most efficient design subject to budgetary constraints. We also consider how the parameters from these approximations can be interpreted simply in terms of ‘real-world’ quantities and used in design analysis.},
  archive      = {J_JOAS},
  author       = {Samuel I. Watson},
  doi          = {10.1080/02664763.2021.1941807},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {3300-3318},
  shortjournal = {J. Appl. Stat.},
  title        = {Efficient design of geographically-defined clusters with spatial autocorrelation},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The fay–herriot model for multiply imputed data with an
application to regional wealth estimation in germany. <em>JOAS</em>,
<em>49</em>(13), 3278–3299. (<a
href="https://doi.org/10.1080/02664763.2021.1941805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing inequality of private income and wealth requires the redistribution of financial resources. Thus, several financial support schemes allocate budget across countries or regions. This work shows how to estimate private wealth at low regional levels by means of a modified Fay–Herriot approach that deals with (a) unit and item non-response, especially with used multiple imputation, (b) the skewness of the wealth distribution, and (c) inconsistencies of the regional estimates with the national direct estimate. One compelling example for financial redistribution is the promoted catching-up process of East Germany after the German reunification. This work shows that 25 years after the reunification differences are more diverse than just between the East and the West by estimating private wealth at two regional levels in Germany. The analysis is based on the Household Finance and Consumption Survey (HFCS) that the European Central Bank launched for all euro area countries in 2010. Although the application in this paper focuses particularly on Germany, the approach proposed is applicable to the other countries participating in the HFCS as well as to other surveys that make use of multiple imputation.},
  archive      = {J_JOAS},
  author       = {Ann-Kristin Kreutzmann and Philipp Marek and Marina Runge and Nicola Salvati and Timo Schmid},
  doi          = {10.1080/02664763.2021.1941805},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {3278-3299},
  shortjournal = {J. Appl. Stat.},
  title        = {The Fay–Herriot model for multiply imputed data with an application to regional wealth estimation in germany},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving logistic regression on the imbalanced data by a
novel penalized log-likelihood function. <em>JOAS</em>, <em>49</em>(13),
3257–3277. (<a
href="https://doi.org/10.1080/02664763.2021.1939662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logistic regression is estimated by maximizing the log-likelihood objective function formulated under the assumption of maximizing the overall accuracy. That does not apply to the imbalanced data. The resulting models tend to be biased towards the majority class (i.e. non-event), which can bring great loss in practice. One strategy for mitigating such bias is to penalize the misclassification costs of observations differently in the log-likelihood function. Existing solutions require either hard hyperparameter estimating or high computational complexity. We propose a novel penalized log-likelihood function by including penalty weights as decision variables for observations in the minority class (i.e. event) and learning them from data along with model coefficients. In the experiments, the proposed logistic regression model is compared with the existing ones on the statistics of area under receiver operating characteristics (ROC) curve from 10 public datasets and 16 simulated datasets, as well as the training time. A detailed analysis is conducted on an imbalanced credit dataset to examine the estimated probability distributions, additional performance measurements (i.e. type I error and type II error) and model coefficients. The results demonstrate that both the discrimination ability and computation efficiency of logistic regression models are improved using the proposed log-likelihood function as the learning objective.},
  archive      = {J_JOAS},
  author       = {Lili Zhang and Trent Geisler and Herman Ray and Ying Xie},
  doi          = {10.1080/02664763.2021.1939662},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {3257-3277},
  shortjournal = {J. Appl. Stat.},
  title        = {Improving logistic regression on the imbalanced data by a novel penalized log-likelihood function},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Validation of risk-based quality control techniques: A case
study from the automotive industry. <em>JOAS</em>, <em>49</em>(12),
3236–3255. (<a
href="https://doi.org/10.1080/02664763.2021.1936466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality control is an outstanding area of production management. The effectiveness of applied quality control methods strongly depends on the performance of the measurement system. Many researchers aimed to analyze the effect of measurement errors on conformity or process control and proposed solutions to treat measurement uncertainty. Although both risk-based conformity control and process control solutions have been designed, verification and validation of these methods have not been provided through laboratory experiments. This paper proposes a case study from the automotive industry regarding the application of risk-based conformity control and risk-based control charts. Acceptance intervals and control limits are optimized to minimize the loss associated with incorrect decisions. The optimization is conducted assuming two scenarios: first, the process and measurement errors are simulated, and second, all data points are measured in the laboratory. This study verifies the applicability of risk-based approaches to real industrial problems and compares the results obtained by simulations and experiments, providing information about the achievable cost reduction opportunities granted by simulations.},
  archive      = {J_JOAS},
  author       = {A. I. Katona},
  doi          = {10.1080/02664763.2021.1936466},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {3236-3255},
  shortjournal = {J. Appl. Stat.},
  title        = {Validation of risk-based quality control techniques: A case study from the automotive industry},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian semiparametric analysis on the relationship between
BMI and income for rural and urban workers in china. <em>JOAS</em>,
<em>49</em>(12), 3215–3235. (<a
href="https://doi.org/10.1080/02664763.2021.1935803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines the nonlinear relationship between BMI and earnings for workers in China using Bayesian semiparametric methods. Markov chain Monte Carlo (MCMC) methods are used to obtain the posterior distribution. We stratify the whole sample into four subsamples based on gender and type of residence area. Using longitudinal data from the China Health and Nutrition Survey (CHNS) from 1989 to 2011, we find nonlinear relationship for each group of workers, especially for rural females. For females in both rural and urban areas, being overweight and obese is associated with lower earnings. However, for males in both areas, earnings are not penalized for extra weight.},
  archive      = {J_JOAS},
  author       = {Lijuan Feng and Murat Munkin},
  doi          = {10.1080/02664763.2021.1935803},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {3215-3235},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian semiparametric analysis on the relationship between BMI and income for rural and urban workers in china},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling nematode population dynamics using a multivariate
poisson model with spike and slab variable selection. <em>JOAS</em>,
<em>49</em>(12), 3195–3214. (<a
href="https://doi.org/10.1080/02664763.2021.1935800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based learning of organism dynamics is challenging, particularly when modeling count correlated data. In this paper, we adapt the multivariate Poisson distribution to model nematode dynamics. This distribution relaxes the mean-equal-variance property of the univariate Poisson distribution and allows recovery of the correlation among nematode genera. An observational dataset with 68 soil samples, 11 nematode genera, and 12 soil parameters is analyzed. The Spike and Slab Variable Selection procedure is adapted to obtain parsimonious models for the nematode occurrence. Nematode genus to genus interaction is assessed through the correlation matrix of the model. A simulation study validated the model&#39;s implementation. As a result, the model determined the most important covariates for each nematode and classified pairs of nematodes as: sympathetic, antagonistic or neutral, based on their estimated correlations. The model is useful for researchers and practitioners interested in studying population dynamics. In particular, the current results are important inputs when planning strategies for improving or managing soil health regarding nematodes.},
  archive      = {J_JOAS},
  author       = {Gill Giese and Dayna P. Saldaña Zepeda and Jacquelin Beacham and Ciro Velasco Cruz},
  doi          = {10.1080/02664763.2021.1935800},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {3195-3214},
  shortjournal = {J. Appl. Stat.},
  title        = {Modeling nematode population dynamics using a multivariate poisson model with spike and slab variable selection},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bayesian approach for the zero-inflated cure model: An
application in a brazilian invasive cervical cancer database.
<em>JOAS</em>, <em>49</em>(12), 3178–3194. (<a
href="https://doi.org/10.1080/02664763.2021.1933923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to discuss the Bayesian estimation approach for the zero-inflated cure class of models, which extends the standard cure model by accommodating zero-inflated data in the survival analysis context. A comprehensive simulation study is carried out to assess the performance of the estimation procedure. A new estimation methodology is illustrated using a real dataset related to women diagnosed with invasive cervical cancer in Brazil.},
  archive      = {J_JOAS},
  author       = {Hayala Cristina Cavenague de Souza and Francisco Louzada and Pedro Luiz Ramos and Mauro Ribeiro de Oliveira Júnior and Gleici da Silva Castro Perdoná},
  doi          = {10.1080/02664763.2021.1933923},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {3178-3194},
  shortjournal = {J. Appl. Stat.},
  title        = {A bayesian approach for the zero-inflated cure model: An application in a brazilian invasive cervical cancer database},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A transition copula model for analyzing multivariate
longitudinal data with missing responses. <em>JOAS</em>,
<em>49</em>(12), 3164–3177. (<a
href="https://doi.org/10.1080/02664763.2021.1931055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multivariate longitudinal studies, several outcomes are repeatedly measured for each subject over time. The data structure of these studies creates two types of associations which should take into account by the model: association of outcomes at a given time point and association among repeated measurements over time for a specific outcome. In our approach, because of some advantageous arisen from features like flexibility of marginal distributions, a copula-based approach is used for joint modeling of multivariate outcomes at each time points, also a transition model is used for considering the association of longitudinal measurements over time. For the problem of incomplete data, missingness mechanism is assumed to be ignorable. Some simulation results are reported in different scenarios using the Gaussian, t and several commonly used copulas of the family of Archimedean copulas. Akaike information criterion (AIC) is used to select the best copula function. The proposed approach is also used for analyzing a real obesity data set.},
  archive      = {J_JOAS},
  author       = {A. Ahmadi and T. Baghfalaki and M. Ganjali and A. Kabir and A. Pazouki},
  doi          = {10.1080/02664763.2021.1931055},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {3164-3177},
  shortjournal = {J. Appl. Stat.},
  title        = {A transition copula model for analyzing multivariate longitudinal data with missing responses},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How reliable are the multiple comparison methods for odds
ratio? <em>JOAS</em>, <em>49</em>(12), 3141–3163. (<a
href="https://doi.org/10.1080/02664763.2022.2104229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The homogeneity tests of odds ratios are used in clinical trials and epidemiological investigations as a preliminary step of meta-analysis. In recent studies, the severity or mortality of COVID-19 in relation to demographic characteristics, comorbidities, and other conditions has been popularly discussed by interpreting odds ratios and using meta-analysis. According to the homogeneity test results, a common odds ratio summarizes all of the odds ratios in a series of studies. If the aim is not to find a common odds ratio, but to find which of the sub-characteristics/groups is different from the others or is under risk, then the implementation of a multiple comparison procedure is required. In this article, the focus is placed on the accuracy and reliability of the homogeneity of odds ratio tests for multiple comparisons when the odds ratios are heterogeneous at the omnibus level. Three recently proposed multiple comparison tests and four homogeneity of odds ratios tests with six adjustment methods to control the type-I error rate are considered. The reliability and accuracy of the methods are discussed in relation to COVID-19 severity data associated with diabetes on a country-by-country basis, and a simulation study to assess the powers and type-I error rates of the tests is conducted.},
  archive      = {J_JOAS},
  author       = {Ayfer Ezgi Yilmaz},
  doi          = {10.1080/02664763.2022.2104229},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {3141-3163},
  shortjournal = {J. Appl. Stat.},
  title        = {How reliable are the multiple comparison methods for odds ratio?},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical inference of adaptive type II progressive hybrid
censored data with dependent competing risks under bivariate exponential
distribution. <em>JOAS</em>, <em>49</em>(12), 3120–3140. (<a
href="https://doi.org/10.1080/02664763.2021.1937961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Marshall–Olkin bivariate exponential distribution is used to statistically infer the adaptive type II progressive hybrid censored data under dependent competition risk model. For complex censored data with only partial failure reasons observed, maximum likelihood estimation and approximate confidence interval based on Fisher information are established. At the same time, Bayesian estimation is performed under the highly flexible Gamma–Dirichlet prior distribution and the highest posterior density interval using Gibbs sampling and Metropolis–Hastings algorithm is obtained. Then the performance of two methods is compared through several indexes. In addition, the Monte Carlo method is used for data simulation of multiple sets of variables to give experimental suggestions. Finally, a practical example is given to illustrate the operability and applicability of the proposed algorithm to efficiently carry out reliability test.},
  archive      = {J_JOAS},
  author       = {Yuge Du and Wenhao Gui},
  doi          = {10.1080/02664763.2021.1937961},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {3120-3140},
  shortjournal = {J. Appl. Stat.},
  title        = {Statistical inference of adaptive type II progressive hybrid censored data with dependent competing risks under bivariate exponential distribution},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A homogeneously weighted moving average control chart for
conway–maxwell poisson distribution. <em>JOAS</em>, <em>49</em>(12),
3090–3119. (<a
href="https://doi.org/10.1080/02664763.2021.1937582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The homogeneously weighted moving average (HWMA) control chart is a new memory-type chart that allocates a specific weight to the current sample and the remaining weight is distributed equally to the previous samples. In this paper, the HWMA control chart is proposed for monitoring count data. This chart is based on the Conway–Maxwell (COM) distribution, which can be used to model under-spread and over-spread count data. The performance of the proposed chart is evaluated in terms of the average run-length (ARL), standard deviation of the run-length (SDRL), median run-length (MRL) as well as the expected ARL, SDRL and MRL values for both location and dispersion shifts in the process. The sensitivity of the new control chart is compared with those of some existing well-known COM-Poisson memory-type control charts in terms of the out-of-control ARL. The results of this study reveal that the proposed control chart performs competitively well with the existing charts in detecting shifts in the location and dispersion parameters in many situations. Numerical examples are given to demonstrate the application of the proposed chart.},
  archive      = {J_JOAS},
  author       = {Olatunde Adebayo Adeoti and Jean-Claude Malela-Majika and Sandile Charles Shongwe and Muhammad Aslam},
  doi          = {10.1080/02664763.2021.1937582},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {3090-3119},
  shortjournal = {J. Appl. Stat.},
  title        = {A homogeneously weighted moving average control chart for Conway–Maxwell poisson distribution},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multivariate piecewise joint models with random
change-points for skewed-longitudinal and survival data. <em>JOAS</em>,
<em>49</em>(12), 3063–3089. (<a
href="https://doi.org/10.1080/02664763.2021.1935797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methodological development and application of joint models for longitudinal and time-to-event data have mostly coupled a single longitudinal outcome-based linear mixed-effects model with normal distribution and Cox proportional hazards model. In practice, however, (i) profile of subject&#39;s longitudinal response may follow a `broken-stick nonlinear&#39; (piecewise) trajectory. Such multiple phases are an important indicator to help quantify treatment effect, disease diagnosis and clinical decision-making. (ii) Normality in longitudinal models is a routine assumption, but it may be unrealistically obscuring important features of subject variations. (iii) Data collected are often featured by multivariate longitudinal outcomes which are significantly correlated, ignoring their correlation may lead to biased estimation. (iv) It is of importance to investigate how multivariate longitudinal outcomes are associated with event time of interest. In the article, driven by a motivating example, we propose Bayesian multivariate piecewise joint models with a skewed distribution and random change-points for longitudinal measures with an attempt to cope with correlated multivariate longitudinal data, adjust departures from normality, mediate accuracy from longitudinal trajectories with random change-point and tailor linkage in specifying a time-to-event process. A real example is analyzed to demonstrate methodology and simulation studies are conducted to evaluate performance of the proposed models and method.},
  archive      = {J_JOAS},
  author       = {Yangxin Huang and Nian-Sheng Tang and Jiaqing Chen},
  doi          = {10.1080/02664763.2021.1935797},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {3063-3089},
  shortjournal = {J. Appl. Stat.},
  title        = {Multivariate piecewise joint models with random change-points for skewed-longitudinal and survival data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bimodal weibull distribution: Properties and inference.
<em>JOAS</em>, <em>49</em>(12), 3044–3062. (<a
href="https://doi.org/10.1080/02664763.2021.1931822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modelling is challenging topic and using parametric models is important stage to reach flexible function for modelling. Weibull distribution has shape and scale parameters which play the main role for modelling. Bimodality parameter is added and so bimodal Weibull distribution can capture real data set with bimodality which can be actually combination of two populations. The properties of the proposed distribution and estimation method are examined extensively to show its usability in modelling accurately and safely for practitioners. After examination as first stage in modelling issue, it is appropriate to use bimodal Weibull for modelling bimodality in real data sets if it exists. Two estimation methods including objective functions are used to estimate the parameters of shape, scale and bimodality parameters of function. The second stage in modelling is overcome by using heuristic algorithms for optimization of function according to parameters due to the fact that converging to global point of objective function is performed by heuristic algorithms from stochastic optimization. Real data sets are provided to show the modelling competence of objective functions from bimodal forms of Weibull and Gamma distributions having well defined shape, scale and bimodality parameters and potentially less parameters when compared with the existing distributions.},
  archive      = {J_JOAS},
  author       = {Roberto Vila and Mehmet Niyazi Çankaya},
  doi          = {10.1080/02664763.2021.1931822},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {3044-3062},
  shortjournal = {J. Appl. Stat.},
  title        = {A bimodal weibull distribution: Properties and inference},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric inference for the scale-mixture of normal
partial linear regression model with censored data. <em>JOAS</em>,
<em>49</em>(12), 3022–3043. (<a
href="https://doi.org/10.1080/02664763.2021.1931821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the censored data exploration, the classical linear regression model which assumes normally distributed random errors is perhaps one of the commonly used frameworks. However, practical studies have often criticized the classical linear regression model because of its sensitivity to departure from the normality and partial nonlinearity. This paper proposes to solve these potential issues simultaneously in the context of the partial linear regression model by assuming that the random errors follow a scale-mixture of normal (SMN) family of distributions. The postulated method allows us to model data with great flexibility, accommodating heavy tails and outliers. By implementing the B-spline approximation and using the convenient hierarchical representation of the SMN distributions, a computationally analytical EM-type algorithm is developed for obtaining maximum likelihood (ML) parameter estimates. Various simulation studies are conducted to investigate the finite sample properties, as well as the robustness of the model in dealing with the heavy tails distributed datasets. Real-world data examples are finally analyzed for illustrating the usefulness of the proposed methodology.},
  archive      = {J_JOAS},
  author       = {Mehrdad Naderi and Elham Mirfarah and Matthew Bernhardt and Ding-Geng Chen},
  doi          = {10.1080/02664763.2021.1931821},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {3022-3043},
  shortjournal = {J. Appl. Stat.},
  title        = {Semiparametric inference for the scale-mixture of normal partial linear regression model with censored data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic dirichlet process mixture model for identifying
voting coalitions in the united nations general assembly human rights
roll call votes. <em>JOAS</em>, <em>49</em>(12), 3002–3021. (<a
href="https://doi.org/10.1080/02664763.2021.1931820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scholars have been interested in the politicization of humans rights within the United Nations for some time. However, previous research typically looks at simple associations between voting coalitions and observable variables, such as geographic location or membership in international organizations. Our study is the first attempt at estimating the latent coalition structure based on the voting data. We propose a Bayesian Dynamic Dirichlet Process Mixture (DDPM) model to identify voting coalitions based on roll call vote data across multiple time periods. We also propose post-processing methods for analyzing the outputs of the DDPM model. We apply these methods to the United Nations General Assembly (UNGA) human rights roll call vote data from 1992 to 2017. We identify human rights voting coalitions in the UNGA after the Cold War, and polarizing resolutions that divide countries into different coalitions.},
  archive      = {J_JOAS},
  author       = {Qiushi Yu},
  doi          = {10.1080/02664763.2021.1931820},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {3002-3021},
  shortjournal = {J. Appl. Stat.},
  title        = {Dynamic dirichlet process mixture model for identifying voting coalitions in the united nations general assembly human rights roll call votes},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian and non-bayesian inference under adaptive type-II
progressive censored sample with exponentiated power lindley
distribution. <em>JOAS</em>, <em>49</em>(12), 2981–3001. (<a
href="https://doi.org/10.1080/02664763.2021.1931819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the statistical inference of the unknown parameters of three-parameter exponentiated power Lindley distribution under adaptive progressive type-II censored samples. The maximum likelihood estimator (MLE) cannot be expressed explicitly, hence approximate MLEs are conducted using the Newton–Raphson method. Bayesian estimation is studied and the Markov Chain Monte Carlo method is used for computing the Bayes estimation. For Bayesian estimation, we consider two loss functions, namely: squared error and linear exponential (LINEX) loss functions, furthermore, we perform asymptotic confidence intervals and the credible intervals for the unknown parameters. A comparison between Bayes estimation and the MLE is observed using simulation analysis and we perform an optimally criterion for some suggested censoring schemes by minimizing bias and mean square error for the point estimation of the parameters. Finally, a real data example is used for the illustration of the goodness of fit for this model.},
  archive      = {J_JOAS},
  author       = {Hanan Haj Ahmad and Mukhtar M. Salah and M. S. Eliwa and Ziyad Ali Alhussain and Ehab M. Almetwally and Essam A. Ahmed},
  doi          = {10.1080/02664763.2021.1931819},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {2981-3001},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian and non-bayesian inference under adaptive type-II progressive censored sample with exponentiated power lindley distribution},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation of the parameters of symmetric stable ARMA and
ARMA–GARCH models. <em>JOAS</em>, <em>49</em>(11), 2964–2980. (<a
href="https://doi.org/10.1080/02664763.2021.1928019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we first propose the modified Hannan–Rissanen Method for estimating the parameters of autoregressive moving average (ARMA) process with symmetric stable noise and symmetric stable generalized autoregressive conditional heteroskedastic (GARCH) noise. Next, we propose the modified empirical characteristic function method for the estimation of GARCH parameters with symmetric stable noise. Further, we show the efficiency, accuracy and simplicity of our methods with Monte-Carlo simulation. Finally, we apply our proposed methods to model the financial data.},
  archive      = {J_JOAS},
  author       = {Aastha M. Sathe and N. S. Upadhye},
  doi          = {10.1080/02664763.2021.1928019},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2964-2980},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimation of the parameters of symmetric stable ARMA and ARMA–GARCH models},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Solving unobserved heterogeneity with latent class inflated
poisson regression model. <em>JOAS</em>, <em>49</em>(11), 2953–2963. (<a
href="https://doi.org/10.1080/02664763.2021.1929875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inflated data and over-dispersion are two common problems when modeling count data with traditional Poisson regression models. In this study, we propose a latent class inflated Poisson (LCIP) regression model to solve the unobserved heterogeneity that leads to inflations and over-dispersion. The performance of the model estimation is evaluated through simulation studies. We illustrate the usefulness of introducing a latent class variable by analyzing the Behavioral Risk Factor Surveillance System (BRFSS) data, which contain several excessive values and characterized by over-dispersion. As a result, the new model we proposed displays a better fit than the standard Poisson regression and zero-inflated Poisson regression models for the inflated counts.},
  archive      = {J_JOAS},
  author       = {Ting Hsiang Lin and Min-Hsiao Tsai},
  doi          = {10.1080/02664763.2021.1929875},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2953-2963},
  shortjournal = {J. Appl. Stat.},
  title        = {Solving unobserved heterogeneity with latent class inflated poisson regression model},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A discrete analogue of odd weibull-g family of
distributions: Properties, classical and bayesian estimation with
applications to count data. <em>JOAS</em>, <em>49</em>(11), 2928–2952.
(<a href="https://doi.org/10.1080/02664763.2021.1928018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the statistical literature, several discrete distributions have been developed so far. However, in this progressive technological era, the data generated from different fields is getting complicated day by day, making it difficult to analyze this real data through the various discrete distributions available in the existing literature. In this context, we have proposed a new flexible family of discrete models named discrete odd Weibull-G (DOW-G) family. Its several impressive distributional characteristics are derived. A key feature of the proposed family is its failure rate function that can take a variety of shapes for distinct values of the unknown parameters, like decreasing, increasing, constant, J-, and bathtub-shaped. Furthermore, the presented family not only adequately captures the skewed and symmetric data sets, but it can also provide a better fit to equi-, over-, under-dispersed data. After producing the general class, two particular distributions of the DOW-G family are extensively studied. The parameters estimation of the proposed family, are explored by the method of maximum likelihood and Bayesian approach. A compact Monte Carlo simulation study is performed to assess the behavior of the estimation methods. Finally, we have explained the usefulness of the proposed family by using two different real data sets.},
  archive      = {J_JOAS},
  author       = {M. El-Morshedy and M. S. Eliwa and Abhishek Tyagi},
  doi          = {10.1080/02664763.2021.1928018},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2928-2952},
  shortjournal = {J. Appl. Stat.},
  title        = {A discrete analogue of odd weibull-G family of distributions: Properties, classical and bayesian estimation with applications to count data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A risk set adjustment for proportional hazards modeling of
combined cohort data. <em>JOAS</em>, <em>49</em>(11), 2913–2927. (<a
href="https://doi.org/10.1080/02664763.2021.1928015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sporting careers observed over a preset time interval can be partitioned into two distinct subsamples. These samples consist of individuals whose careers had already commenced at the start of the time interval (prevalent subsample) and individuals whose careers began during the time interval (incident subsample) as well the respective individual-level covariate data such as salary, height, weight, performance statistics, draft position, etc. Under the assumption of a proportional hazards model, we propose a partial likelihood estimator to model the effect of covariates on survival via an adjusted risk set sampling procedure for when the incident cohort data is used in conjunction with the prevalent cohort data. We use simulated failure time data to validate the combined cohort proportional hazards methodology and illustrate our model using an NBA data set for career durations measured between 1990 and 2008.},
  archive      = {J_JOAS},
  author       = {J. H. McVittie and V. Addona},
  doi          = {10.1080/02664763.2021.1928015},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2913-2927},
  shortjournal = {J. Appl. Stat.},
  title        = {A risk set adjustment for proportional hazards modeling of combined cohort data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inference on nadarajah–haghighi distribution with constant
stress partially accelerated life tests under progressive type-II
censoring. <em>JOAS</em>, <em>49</em>(11), 2891–2912. (<a
href="https://doi.org/10.1080/02664763.2021.1928014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents methods of estimation of the parameters and acceleration factor for Nadarajah–Haghighi distribution based on constant-stress partially accelerated life tests. Based on progressive Type-II censoring, Maximum likelihood and Bayes estimates of the model parameters and acceleration factor are established, respectively. In addition, approximate confidence interval are constructed via asymptotic variance and covariance matrix, and Bayesian credible intervals are obtained based on importance sampling procedure. For comparison purpose, alternative bootstrap confidence intervals for unknown parameters and acceleration factor are also presented. Finally, extensive simulation studies are conducted for investigating the performance of the our results, and two data sets are analyzed to show the applicabilities of the proposed methods.},
  archive      = {J_JOAS},
  author       = {Sanku Dey and Liang Wang and Mazen Nassar},
  doi          = {10.1080/02664763.2021.1928014},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2891-2912},
  shortjournal = {J. Appl. Stat.},
  title        = {Inference on Nadarajah–Haghighi distribution with constant stress partially accelerated life tests under progressive type-II censoring},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effect of non-normality on the monitoring of simple linear
profiles in two-stage processes: A remedial measure for
gamma-distributed responses. <em>JOAS</em>, <em>49</em>(11), 2870–2890.
(<a href="https://doi.org/10.1080/02664763.2021.1928013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The relationship between the response variable and one or more independent variables refers to the quality characteristic in some statistical quality control applications, which is called profile. Most research dealt with the monitoring of profiles in single-stage processes considering a basic assumption of normality. However, some processes are made up of several sub-processes; thus, the effect of cascade property in multistage processes should be considered. Moreover, sometimes in practice, the assumption of normally distributed data does not hold. This paper first examines the effect of non-normal data to monitor simple linear profiles in two-stage processes in Phase II. We study non-normal distributions such as the skewed gamma distribution and the heavy-tailed symmetric t -distribution to measure the non-normality effect using the average run length criterion. Next, generalized linear models have been used and a monitoring approach based on generalized likelihood ratio (GLR) has been developed for gamma-distributed responses as a remedial measure to reduce the detrimental effects of non-normality. The results of simulation studies reveal that the performance of the GLR procedure is satisfactory for the multistage non-normal linear profiles. Finally, the simulated and real case studies with gamma-distributed data have been provided to show the application of the competing monitoring approaches.},
  archive      = {J_JOAS},
  author       = {Paria Soleimani and Shervin Asadzadeh},
  doi          = {10.1080/02664763.2021.1928013},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2870-2890},
  shortjournal = {J. Appl. Stat.},
  title        = {Effect of non-normality on the monitoring of simple linear profiles in two-stage processes: A remedial measure for gamma-distributed responses},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric zero-inflated bernoulli regression with
applications. <em>JOAS</em>, <em>49</em>(11), 2845–2869. (<a
href="https://doi.org/10.1080/02664763.2021.1925228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the observed proportion of zeros in a data set consisting of binary outcome data is larger than expected under a regular logistic regression model, it is frequently suggested to use a zero-inflated Bernoulli (ZIB) regression model. A spline-based ZIB regression model is proposed to describe the potentially nonlinear effect of a continuous covariate. A spline is used to approximate the unknown smooth function. Under the smoothness condition, the spline estimator of the unknown smooth function is uniformly consistent, and the regression parameter estimators are asymptotically normally distributed. We propose an easily implemented and consistent estimation method for the variances of the regression parameter estimators. Extensive simulations are conducted to investigate the finite-sample performance of the proposed method. A real-life data set is used to illustrate the practical use of the proposed methodology. The real-life data analysis indicates that the prediction performance of the proposed semiparametric ZIB regression model is better compared to the parametric ZIB regression model.},
  archive      = {J_JOAS},
  author       = {Chin-Shang Li and Minggen Lu},
  doi          = {10.1080/02664763.2021.1925228},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2845-2869},
  shortjournal = {J. Appl. Stat.},
  title        = {Semiparametric zero-inflated bernoulli regression with applications},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved maximum likelihood estimation of the shape-scale
family based on the generalized progressive hybrid censoring scheme.
<em>JOAS</em>, <em>49</em>(11), 2825–2844. (<a
href="https://doi.org/10.1080/02664763.2021.1924638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In parametric estimates, the maximum likelihood estimation method is the most popular method widely used in the social sciences and psychology, although it is biased in situations where sample sizes are small or the data are heavily censored. Therefore, the main objective of this research is to improve this estimation method using the Runge–Kutta technique. The improved method was applied to derive the estimators of the shape scale family parameters and compare them with Bayesian estimators based on the informative and kernel priors, via Monte Carlo simulation. The simulation results showed that the improved maximum likelihood estimation method is highly efficient and outperforms the Bayesian method for different sample sizes. Finally, from a future perspective, the proposed model could be important for analyzing real data sets including data on COVID-19 deaths in Egypt, for potential comparative studies with other countries.},
  archive      = {J_JOAS},
  author       = {M. Maswadah},
  doi          = {10.1080/02664763.2021.1924638},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2825-2844},
  shortjournal = {J. Appl. Stat.},
  title        = {Improved maximum likelihood estimation of the shape-scale family based on the generalized progressive hybrid censoring scheme},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The new neyman type a generalized odd log-logistic-g-family
with cure fraction. <em>JOAS</em>, <em>49</em>(11), 2805–2824. (<a
href="https://doi.org/10.1080/02664763.2021.1922994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The work proposes a new family of survival models called the Odd log-logistic generalized Neyman type A long-term. We consider different activation schemes in which the number of factors M has the Neyman type A distribution and the time of occurrence of an event follows the odd log-logistic generalized family. The parameters are estimated by the classical and Bayesian methods. We investigate the mean estimates, biases, and root mean square errors in different activation schemes using Monte Carlo simulations. The residual analysis via the frequentist approach is used to verify the model assumptions. We illustrate the applicability of the proposed model for patients with gastric adenocarcinoma. The choice of the adenocarcinoma data is because the disease is responsible for most cases of stomach tumors. The estimated cured proportion of patients under chemoradiotherapy is higher compared to patients undergoing only surgery. The estimated hazard function for the chemoradiotherapy level tends to decrease when the time increases. More information about the data is addressed in the application section.},
  archive      = {J_JOAS},
  author       = {Valdemiro P. Vigas and Edwin M. M. Ortega and Gauss M. Cordeiro and Adriano K. Suzuki and Giovana O. Silva},
  doi          = {10.1080/02664763.2021.1922994},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2805-2824},
  shortjournal = {J. Appl. Stat.},
  title        = {The new neyman type a generalized odd log-logistic-G-family with cure fraction},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A simple two-step procedure using the fellegi–sunter model
for frequency-based record linkage. <em>JOAS</em>, <em>49</em>(11),
2789–2804. (<a
href="https://doi.org/10.1080/02664763.2021.1922615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widely used Fellegi–Sunter model for probabilistic record linkage does not leverage information contained in field values and consequently leads to identical classification of match status regardless of whether records agree on rare or common values. Since agreement on rare values is less likely to occur by chance than agreement on common values, records agreeing on rare values are more likely to be matches. Existing frequency-based methods typically rely on knowledge of error probabilities associated with field values and frequencies of agreed field values among matches, often derived using prior studies or training data. When such information is unavailable, applications of these methods are challenging. In this paper, we propose a simple two-step procedure for frequency-based matching using the Fellegi–Sunter framework to overcome these challenges. Matching weights are adjusted based on frequency distributions of the agreed field values among matches and non-matches, estimated by the Fellegi–Sunter model without relying on prior studies or training data. Through a real-world application and simulation, our method is found to produce comparable or better performance than the unadjusted method. Furthermore, frequency-based matching provides greater improvement in matching accuracy when using poorly discriminating fields with diminished benefit as the discriminating power of matching fields increases.},
  archive      = {J_JOAS},
  author       = {Huiping Xu and Xiaochun Li and Shaun Grannis},
  doi          = {10.1080/02664763.2021.1922615},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2789-2804},
  shortjournal = {J. Appl. Stat.},
  title        = {A simple two-step procedure using the Fellegi–Sunter model for frequency-based record linkage},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved kth power expectile regression with nonignorable
dropouts. <em>JOAS</em>, <em>49</em>(11), 2767–2788. (<a
href="https://doi.org/10.1080/02664763.2021.1919606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The k th ( 1 1&amp;lt;k≤ 2 ) power expectile regression (ER) can balance robustness and effectiveness between the ordinary quantile regression and ER simultaneously. Motivated by a longitudinal ACTG 193A data with nonignorable dropouts, we propose a two-stage estimation procedure and statistical inference methods based on the k th power ER and empirical likelihood to accommodate both the within-subject correlations and nonignorable dropouts. Firstly, we construct the bias-corrected generalized estimating equations by combining the k th power ER and inverse probability weighting approaches. Subsequently, the generalized method of moments is utilized to estimate the parameters in the nonignorable dropout propensity based on sufficient instrumental estimating equations. Secondly, in order to incorporate the within-subject correlations under an informative working correlation structure, we borrow the idea of quadratic inference function to obtain the improved empirical likelihood procedures. The asymptotic properties of the corresponding estimators and their confidence regions are derived. The finite-sample performance of the proposed estimators is studied through simulation and an application to the ACTG 193A data is also presented.},
  archive      = {J_JOAS},
  author       = {Dongyu Li and Lei Wang},
  doi          = {10.1080/02664763.2021.1919606},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2767-2788},
  shortjournal = {J. Appl. Stat.},
  title        = {Improved kth power expectile regression with nonignorable dropouts},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multivariate zero-inflated binomial model for the analysis
of correlated proportional data. <em>JOAS</em>, <em>49</em>(11),
2740–2766. (<a
href="https://doi.org/10.1080/02664763.2021.1918649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new multivariate zero-inflated binomial (MZIB) distribution is proposed to analyse the correlated proportional data with excessive zeros. The distributional properties of purposed model are studied. The Fisher scoring algorithm and EM algorithm are given for the computation of estimates of parameters in the proposed MZIB model with/without covariates. The score tests and the likelihood ratio tests are derived for assessing both the zero-inflation and the equality of multiple binomial probabilities in correlated proportional data. A limited simulation study is performed to evaluate the performance of derived EM algorithms for the estimation of parameters in the model with/without covariates and to compare the nominal levels and powers of both score tests and likelihood ratio tests. The whitefly data is used to illustrate the proposed methodologies.},
  archive      = {J_JOAS},
  author       = {Dianliang Deng and Yiguang Sun and Guo-Liang Tian},
  doi          = {10.1080/02664763.2021.1918649},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2740-2766},
  shortjournal = {J. Appl. Stat.},
  title        = {A multivariate zero-inflated binomial model for the analysis of correlated proportional data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized poisson integer-valued autoregressive processes
with structural changes. <em>JOAS</em>, <em>49</em>(11), 2717–2739. (<a
href="https://doi.org/10.1080/02664763.2021.1915255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a new first-order generalized Poisson integer-valued autoregressive process, for modeling integer-valued time series exhibiting a piecewise structure and overdispersion. Basic probabilistic and statistical properties of this model are discussed. Conditional least squares and conditional maximum likelihood estimators are derived. The asymptotic properties of the estimators are established. Moreover, two special cases of the process are discussed. Finally, some numerical results of the estimates and a real data example are presented.},
  archive      = {J_JOAS},
  author       = {Chenhui Zhang and Dehui Wang and Kai Yang and Han Li and Xiaohong Wang},
  doi          = {10.1080/02664763.2021.1915255},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2717-2739},
  shortjournal = {J. Appl. Stat.},
  title        = {Generalized poisson integer-valued autoregressive processes with structural changes},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Assessing the discriminatory power of loss given default
models. <em>JOAS</em>, <em>49</em>(10), 2700–2716. (<a
href="https://doi.org/10.1080/02664763.2021.1910936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For banks using the Advanced Internal Ratings-Based Approach in accordance with Basel III requirements, the amount of required regulatory capital relies on the banks&#39; estimates of the probability of default, the loss given default and the conversion factor for their credit risk portfolio. Therefore, for both model development and validation, assessing the models&#39; predictive and discriminatory abilities is of key importance in order to ensure an adequate quantification of risk. This paper compares different measures of discriminatory power suitable for multi-class target variables such as in loss given default (LGD) models, which are currently used among banks and supervisory authorities. This analysis highlights the disadvantages of using measures that solely rely on pairwise comparisons when applied in a multi-class setting. Thus, for multi-class classification problems, we suggest using a generalisation of the well-known area under the receiver operating characteristic (ROC) curve known as the volume under the ROC surface (VUS). Furthermore, we present the R -package VUROCS , which allows for a time-efficient computation of the VUS as well as associated (co)variance estimates and illustrate its usage based on real-world loss data and validation principles.},
  archive      = {J_JOAS},
  author       = {Hannes Kazianka and Anna Morgenbesser and Thomas Nowak},
  doi          = {10.1080/02664763.2021.1910936},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2700-2716},
  shortjournal = {J. Appl. Stat.},
  title        = {Assessing the discriminatory power of loss given default models},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning and design of experiments with an
application to product innovation in the chemical industry.
<em>JOAS</em>, <em>49</em>(10), 2674–2699. (<a
href="https://doi.org/10.1080/02664763.2021.1907840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial statistics plays a major role in the areas of both quality management and innovation. However, existing methodologies must be integrated with the latest tools from the field of Artificial Intelligence. To this end, a background on the joint application of Design of Experiments (DOE) and Machine Learning (ML) methodologies in industrial settings is presented here, along with a case study from the chemical industry. A DOE study is used to collect data, and two ML models are applied to predict responses which performance show an advantage over the traditional modeling approach. Emphasis is placed on causal investigation and quantification of prediction uncertainty, as these are crucial for an assessment of the goodness and robustness of the models developed. Within the scope of the case study, the models learned can be implemented in a semi-automatic system that can assist practitioners who are inexperienced in data analysis in the process of new product development.},
  archive      = {J_JOAS},
  author       = {Rosa Arboretti and Riccardo Ceccato and Luca Pegoraro and Luigi Salmaso and Chris Housmekerides and Luca Spadoni and Elisabetta Pierangelo and Sara Quaggia and Catherine Tveit and Sebastiano Vianello},
  doi          = {10.1080/02664763.2021.1907840},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2674-2699},
  shortjournal = {J. Appl. Stat.},
  title        = {Machine learning and design of experiments with an application to product innovation in the chemical industry},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Binary dynamic logit for correlated ordinal: Estimation,
application and simulation. <em>JOAS</em>, <em>49</em>(10), 2657–2673.
(<a href="https://doi.org/10.1080/02664763.2021.1906849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We evaluate the estimation performance of the Binary Dynamic Logit model for correlated ordinal variables (BDLCO model), and compare it to GEE and Ordinal Logistic Regression performance in terms of bias and Mean Absolute Percentage Error (MAPE) via Monte Carlo simulation. Our results indicate that when the proportional-odds assumption does not hold, the proposed BDLCO method is superior to existing models in estimating correlated ordinal data. Moreover, this method is flexible in terms of modeling dependence and allows unequal slopes for each category, and can be used to estimate an apple bloom data set where the proportional-odds assumption is violated. We also provide a function in R to implement BDLCO.},
  archive      = {J_JOAS},
  author       = {Yingzi Li and Huinan Liu and Nairanjana Dasgupta},
  doi          = {10.1080/02664763.2021.1906849},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2657-2673},
  shortjournal = {J. Appl. Stat.},
  title        = {Binary dynamic logit for correlated ordinal: Estimation, application and simulation},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new multivariate t distribution with variant tail weights
and its application in robust regression analysis. <em>JOAS</em>,
<em>49</em>(10), 2629–2656. (<a
href="https://doi.org/10.1080/02664763.2021.1913106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new kind of multivariate t distribution by allowing different degrees of freedom for each univariate component. Compared with the classical multivariate t distribution, it is more flexible in the model specification that can be used to deal with the variant amounts of tail weights on marginals in multivariate data modeling. In particular, it could include components following the multivariate normal distribution, and it contains the product of independent t -distributions as a special case. Subsequently, it is extended to the regression model as the joint distribution of the error terms. Important distributional properties are explored and useful statistical methods are developed. The flexibility of the specified structure in better capturing the characteristic of data is exemplified by both simulation studies and real data analyses.},
  archive      = {J_JOAS},
  author       = {Chi Zhang and Guo-Liang Tian and Kam Chuen Yuen and Pengyi Liu and Man-Lai Tang},
  doi          = {10.1080/02664763.2021.1913106},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2629-2656},
  shortjournal = {J. Appl. Stat.},
  title        = {A new multivariate t distribution with variant tail weights and its application in robust regression analysis},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian beta nonlinear models with constrained parameters
to describe ruminal degradation kinetics. <em>JOAS</em>,
<em>49</em>(10), 2612–2628. (<a
href="https://doi.org/10.1080/02664763.2021.1913105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The models used to describe the kinetics of ruminal degradation are usually nonlinear models where the dependent variable is the proportion of degraded food. The method of least squares is the standard approach used to estimate the unknown parameters but this method can lead to unacceptable predictions. To solve this issue, a beta nonlinear model and the Bayesian perspective is proposed in this article. The application of standard methodologies to obtain prior distributions, such as the Jeffreys prior or the reference priors , involves serious difficulties here because this model is a nonlinear non-normal regression model, and the constrained parameters appear in the log-likelihood function through the Gamma function. This paper proposes an objective method to obtain the prior distribution, which can be applied to other models with similar complexity, can be easily implemented in OpenBUGS, and solves the problem of unacceptable predictions. The model is generalized to a larger class of models. The methodology was applied to real data with three models that were compared using the Deviance Information Criterion and the root mean square prediction error. A simulation study was performed to evaluate the coverage of the credible intervals.},
  archive      = {J_JOAS},
  author       = {Diego Salmerón},
  doi          = {10.1080/02664763.2021.1913105},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2612-2628},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian beta nonlinear models with constrained parameters to describe ruminal degradation kinetics},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The CUSUM statistics of change-point models based on
dependent sequences. <em>JOAS</em>, <em>49</em>(10), 2593–2611. (<a
href="https://doi.org/10.1080/02664763.2021.1913104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the mean change-point models based on associated sequences. Under some weak conditions, we obtain a limit distribution of CUSUM statistic which can be used to judge the mean change-mount δ n δ n δn is satisfied or dissatisfied n 1 / 2 δ n = o ( 1 ) n 1 / 2 δ n = o ( 1 ) n1/2δn=o(1) . We also study the consistency of sample covariances and change-point location statistics. Based on Normality and Lognormality data, some simulations such as empirical sizes, empirical powers and convergence are presented to test our results. As an important application, we use CUSUM statistics to do the mean change-point analysis for a financial series.},
  archive      = {J_JOAS},
  author       = {Saisai Ding and Hongyan Fang and Xiang Dong and Wenzhi Yang},
  doi          = {10.1080/02664763.2021.1913104},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2593-2611},
  shortjournal = {J. Appl. Stat.},
  title        = {The CUSUM statistics of change-point models based on dependent sequences},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Confounding adjustment methods for multi-level treatment
comparisons under lack of positivity and unknown model specification.
<em>JOAS</em>, <em>49</em>(10), 2570–2592. (<a
href="https://doi.org/10.1080/02664763.2021.1911966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalances in covariates between treatment groups are frequent in observational studies and can lead to biased comparisons. Various adjustment methods can be employed to correct these biases in the context of multi-level treatments (&gt; 2). Analytical challenges, such as positivity violations and incorrect model specification due to unknown functional relationships between covariates and treatment or outcome, may affect their ability to yield unbiased results. Such challenges were expected in a comparison of fire-suppression interventions for preventing fire growth. We identified the overlap weights, augmented overlap weights, bias-corrected matching and targeted maximum likelihood as methods with the best potential to address those challenges. A simple variance estimator for the overlap weight estimators that can naturally be combined with machine learning is proposed. In a simulation study, we investigated the performance of these methods as well as those of simpler alternatives. Adjustment methods that included an outcome modeling component performed better than those that focused on the treatment mechanism in our simulations. Additionally, machine learning implementation was observed to efficiently compensate for the unknown model specification for the former methods, but not the latter. Based on these results, we compared the effectiveness of fire-suppression interventions using the augmented overlap weight estimator.},
  archive      = {J_JOAS},
  author       = {S. Arona Diop and Thierry Duchesne and Steven G. Cumming and Awa Diop and Denis Talbot},
  doi          = {10.1080/02664763.2021.1911966},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2570-2592},
  shortjournal = {J. Appl. Stat.},
  title        = {Confounding adjustment methods for multi-level treatment comparisons under lack of positivity and unknown model specification},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detection of outliers in high-dimensional data using
nu-support vector regression. <em>JOAS</em>, <em>49</em>(10), 2550–2569.
(<a href="https://doi.org/10.1080/02664763.2021.1911965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support Vector Regression (SVR) is gaining in popularity in the detection of outliers and classification problems in high-dimensional data (HDD) as this technique does not require the data to be of full rank. In real application, most of the data are of high dimensional. Classification of high-dimensional data is needed in applied sciences, in particular, as it is important to discriminate cancerous cells from non-cancerous cells. It is also imperative that outliers are identified before constructing a model on the relationship between the dependent and independent variables to avoid misleading interpretations about the fitting of a model. The standard SVR and the μ - ε -SVR are able to detect outliers; however, they are computationally expensive. The fixed parameters support vector regression (FP- ε -SVR) was put forward to remedy this issue. However, the FP- ε -SVR using ε -SVR is not very successful in identifying outliers. In this article, we propose an alternative method to detect outliers i.e. by employing nu -SVR. The merit of our proposed method is confirmed by three real examples and the Monte Carlo simulation. The results show that our proposed nu -SVR method is very successful in identifying outliers under a variety of situations, and with less computational running time.},
  archive      = {J_JOAS},
  author       = {Abdullah Mohammed Rashid and Habshah Midi and Waleed Dhhan and Jayanthi Arasan},
  doi          = {10.1080/02664763.2021.1911965},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2550-2569},
  shortjournal = {J. Appl. Stat.},
  title        = {Detection of outliers in high-dimensional data using nu-support vector regression},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conservative confidence intervals for the intraclass
correlation coefficient for clustered binary data. <em>JOAS</em>,
<em>49</em>(10), 2535–2549. (<a
href="https://doi.org/10.1080/02664763.2021.1910939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asymptotic approaches are traditionally used to calculate confidence intervals for intraclass correlation coefficient in a clustered binary study. When sample size is small to medium, or correlation or response rate is near the boundary, asymptotic intervals often do not have satisfactory performance with regard to coverage. We propose using the importance sampling method to construct the profile confidence limits for the intraclass correlation coefficient. Importance sampling is a simulation based approach to reduce the variance of the estimated parameter. Four existing asymptotic limits are used as statistical quantities for sample space ordering in the importance sampling method. Simulation studies are performed to evaluate the performance of the proposed accurate intervals with regard to coverage and interval width. Simulation results indicate that the accurate intervals based on the asymptotic limits by Fleiss and Cuzick generally have shorter width than others in many cases, while the accurate intervals based on Zou and Donner asymptotic limits outperform others when correlation and response rate are close to their boundaries.},
  archive      = {J_JOAS},
  author       = {Guogen Shan},
  doi          = {10.1080/02664763.2021.1910939},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2535-2549},
  shortjournal = {J. Appl. Stat.},
  title        = {Conservative confidence intervals for the intraclass correlation coefficient for clustered binary data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time-varying coefficient model estimation through radial
basis functions. <em>JOAS</em>, <em>49</em>(10), 2510–2534. (<a
href="https://doi.org/10.1080/02664763.2021.1910938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we estimate the dynamic parameters of a time-varying coefficient model through radial kernel functions in the context of a longitudinal study. Our proposal is based on a linear combination of weighted kernel functions involving a bandwidth, centered around a given set of time points. In addition, we study different alternatives of estimation and inference including a Frequentist approach using weighted least squares along with bootstrap methods, and a Bayesian approach through both Markov chain Monte Carlo and variational methods. We compare the estimation strategies mention above with each other, and our radial kernel functions proposal with an expansion based on regression spline, by means of an extensive simulation study considering multiples scenarios in terms of sample size, number of repeated measurements, and subject-specific correlation. Our experiments show that the capabilities of our proposal based on radial kernel functions are indeed comparable with or even better than those obtained from regression splines. We illustrate our methodology by analyzing data from two AIDS clinical studies.},
  archive      = {J_JOAS},
  author       = {Juan Sosa and Lina Buitrago},
  doi          = {10.1080/02664763.2021.1910938},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2510-2534},
  shortjournal = {J. Appl. Stat.},
  title        = {Time-varying coefficient model estimation through radial basis functions},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling bivariate geyser eruption system with
covariate-adjusted recurrent event process. <em>JOAS</em>,
<em>49</em>(10), 2488–2509. (<a
href="https://doi.org/10.1080/02664763.2021.1910937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geyser eruption is one of the most popular signature attractions at the Yellowstone National Park. The interdependence of geyser eruptions and impacts of covariates are of interest to researchers in geyser studies. In this paper, we propose a parametric covariate-adjusted recurrent event model for estimating the eruption gap time. We describe a general bivariate recurrent event process, where a bivariate lognormal distribution and a Gumbel copula with different marginal distributions are used to model an interdependent dual-type event system. The maximum likelihood approach is used to estimate model parameters. The proposed method is applied to analyzing the Yellowstone geyser eruption data for a bivariate geyser system and offers a deeper understanding of the event occurrence mechanism of individual events as well as the system as a whole. A comprehensive simulation study is conducted to evaluate the performance of the proposed method.},
  archive      = {J_JOAS},
  author       = {Zhongnan Jin and Lu Lu and Khaled Bedair and Yili Hong},
  doi          = {10.1080/02664763.2021.1910937},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2488-2509},
  shortjournal = {J. Appl. Stat.},
  title        = {Modeling bivariate geyser eruption system with covariate-adjusted recurrent event process},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A one-parameter discrete distribution for over-dispersed
data: Statistical and reliability properties with applications.
<em>JOAS</em>, <em>49</em>(10), 2467–2487. (<a
href="https://doi.org/10.1080/02664763.2021.1905787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the literature of distribution theory, a vast proportion is acquired by discrete distributions and their applications in real-world phenomena. However, in a rapidly changing technological era, the data generated is becoming increasingly complex day by day, making it difficult for us to capture various aspects of this real data through existing discrete models. In view of this, we propose a new flexible discrete distribution with one parameter. Some statistical and reliability are derived. These properties can be expressed as closed-forms. One of the important virtues of this newly evolved model is that it can model not only over-dispersed, positively skewed and leptokurtic data sets, but it can also be utilized for modeling increasing, decreasing and unimodal failure rate. Various estimation approaches are utilized to estimate the model parameter. A simulation study is carried out to examine the performance of the estimators for different sample size. The flexibility of the new model for analyzing different types of data is explained by utilizing four real data sets in different fields. Finally, the proposed model can serve as an alternative model to other distributions in the existing literature for modeling positive real data in several areas.},
  archive      = {J_JOAS},
  author       = {M. S. Eliwa and M. El-Morshedy},
  doi          = {10.1080/02664763.2021.1905787},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2467-2487},
  shortjournal = {J. Appl. Stat.},
  title        = {A one-parameter discrete distribution for over-dispersed data: Statistical and reliability properties with applications},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stopping for efficacy in single-arm phase II clinical
trials. <em>JOAS</em>, <em>49</em>(10), 2447–2466. (<a
href="https://doi.org/10.1080/02664763.2021.1904846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase II clinical trials investigate whether a new drug or treatment has sufficient evidence of effectiveness against the disease under study. Two-stage designs are popular for phase II since they can stop in the first stage if the drug is ineffective. Investigators often face difficulties in determining the target response rates, and adaptive designs can help to set the target response rate tested in the second stage based on the number of responses observed in the first stage. Popular adaptive designs consider two alternate response rates, and they generally minimise the expected sample size at the maximum uninterested response rate. Moreover, these designs consider only futility as the reason for early stopping and have high expected sample sizes if the provided drug is effective. Motivated by this problem, we propose an adaptive design that enables us to terminate the single-arm trial at the first stage for efficacy and conclude which alternate response rate to choose. Comparing the proposed design with a popular adaptive design from literature reveals that the expected sample size decreases notably if any of the two target response rates are correct. In contrast, the expected sample size remains almost the same under the null hypothesis.},
  archive      = {J_JOAS},
  author       = {Rezoanoor Rahman and M. Iftakhar Alam},
  doi          = {10.1080/02664763.2021.1904846},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2447-2466},
  shortjournal = {J. Appl. Stat.},
  title        = {Stopping for efficacy in single-arm phase II clinical trials},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An application of a non-homogeneous poisson model to study
PM2.5 exceedances in mexico city and bogota. <em>JOAS</em>,
<em>49</em>(9), 2430–2445. (<a
href="https://doi.org/10.1080/02664763.2021.1897972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is very important to study the occurrence of high levels of particulate matter due to the potential harm to people&#39;s health and to the environment. In the present work we use a non-homogeneous Poisson model to analyse the rate of exceedances of particulate matter with diameter smaller that 2.5 microns (PM 2.5 2.5 2.5 ). Models with and without change-points are considered and they are applied to data from Bogota, Colombia, and Mexico City, Mexico. Results show that whereas in Bogota larger particles pose a more serious problem, in Mexico City, even though nowadays levels are more controlled, in the recent past PM 2.5 were the ones causing serious problems.},
  archive      = {J_JOAS},
  author       = {Biviana M. Súarez-Sierra and Eliane R. Rodrigues and Guadalupe Tzintzun},
  doi          = {10.1080/02664763.2021.1897972},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {2430-2445},
  shortjournal = {J. Appl. Stat.},
  title        = {An application of a non-homogeneous poisson model to study PM2.5 exceedances in mexico city and bogota},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The log-normal zero-inflated cure regression model for labor
time in an african obstetric population. <em>JOAS</em>, <em>49</em>(9),
2416–2429. (<a
href="https://doi.org/10.1080/02664763.2021.1896684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In obstetrics and gynecology, knowledge about how women&#39;s features are associated with childbirth is important. This leads to establishing guidelines and can help managers to describe the dynamics of pregnant women&#39;s hospital stays. Then, time is a variable of great importance and can be described by survival models. An issue that should be considered in the modeling is the inclusion of women for whom the duration of labor cannot be observed due to fetal death, generating a proportion of times equal to zero. Additionally, another proportion of women&#39;s time may be censored due to some intervention. The aim of this paper was to present the Log-Normal zero-inflated cure regression model and to evaluate likelihood-based parameter estimation by a simulation study. In general, the inference procedures showed a better performance for larger samples and low proportions of zero inflation and cure. To exemplify how this model can be an important tool for investigating the course of the childbirth process, we considered the Better Outcomes in Labor Difficulty project dataset and showed that parity and educational level are associated with the main outcomes. We acknowledge the World Health Organization for granting us permission to use the dataset.},
  archive      = {J_JOAS},
  author       = {Hayala Cristina Cavenague de Souza and Francisco Louzada and Mauro Ribeiro de Oliveira and Bukola Fawole and Adesina Akintan and Lawal Oyeneyin and Wilfred Sanni and Gleici da Silva Castro Perdoná},
  doi          = {10.1080/02664763.2021.1896684},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {2416-2429},
  shortjournal = {J. Appl. Stat.},
  title        = {The log-normal zero-inflated cure regression model for labor time in an african obstetric population},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The investigation of temperature data in turkey’s black sea
region using functional data analysis. <em>JOAS</em>, <em>49</em>(9),
2403–2415. (<a
href="https://doi.org/10.1080/02664763.2021.1896683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the field of study expands, or as the number of observations in a sample increases, data observed at discrete points is generally assumed to be sampled from an underlying real function. As the number of observation points increases, those observations are likely to be sampled from a real-valued function. In this case, the derived data will be examples of a functional structure. We analyzed the daily average temperature data at 65 discrete points in 18 cities in Turkey&#39;s Black Sea Region. Fourier basis functions were used as a basis function approach because the temperature data had a periodic structure. The data were then transformed into a continuous function using the basis function and roughness penalty approach. Functional data were obtained using the roughness penalty approach. The generalized cross-validation method was used to determine the smoothing parameter of the variable (temperature variable). Finally, smoothed functional principal components analysis was applied to the functional data. In this way, changes in temperature functions, which seem hard to tackle, were evaluated on the same graph using the mean function generated for the principal component function and using the functions and the mean function obtained using the principal component function.},
  archive      = {J_JOAS},
  author       = {Çağlar Sözen and Yüksel Öner},
  doi          = {10.1080/02664763.2021.1896683},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {2403-2415},
  shortjournal = {J. Appl. Stat.},
  title        = {The investigation of temperature data in turkey’s black sea region using functional data analysis},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Handling missing data in a composite outcome with partially
observed components: Simulation study based on clustered paediatric
routine data. <em>JOAS</em>, <em>49</em>(9), 2389–2402. (<a
href="https://doi.org/10.1080/02664763.2021.1895087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composite scores are useful in providing insights and trends about complex and multidimensional quality of care processes. However, missing data in subcomponents may hinder the overall reliability of a composite measure. In this study, strategies for handling missing data in Paediatric Admission Quality of Care (PAQC) score, an ordinal composite outcome, were explored through a simulation study. Specifically, the implications of the conventional method employed in addressing missing PAQC score subcomponents, consisting of scoring missing PAQC score components with a zero, and a multiple imputation (MI)-based strategy, were assessed. The latent normal joint modelling MI approach was used for the latter. Across simulation scenarios, MI of missing PAQC score elements at item level produced minimally biased estimates compared to the conventional method. Moreover, regression coefficients were more prone to bias compared to standards errors. Magnitude of bias was dependent on the proportion of missingness and the missing data generating mechanism. Therefore, incomplete composite outcome subcomponents should be handled carefully to alleviate potential for biased estimates and misleading inferences. Further research on other strategies of imputing at the component and composite outcome level and imputing compatibly with the substantive model in this setting, is needed.},
  archive      = {J_JOAS},
  author       = {Susan Gachau and Edmund Njeru Njagi and Nelson Owuor and Paul Mwaniki and Matteo Quartagno and Rachel Sarguta and Mike English and Philip Ayieko},
  doi          = {10.1080/02664763.2021.1895087},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {2389-2402},
  shortjournal = {J. Appl. Stat.},
  title        = {Handling missing data in a composite outcome with partially observed components: Simulation study based on clustered paediatric routine data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Review of bayesian selection methods for categorical
predictors using JAGS. <em>JOAS</em>, <em>49</em>(9), 2370–2388. (<a
href="https://doi.org/10.1080/02664763.2021.1902955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The formulation of variable selection has been widely developed in the Bayesian literature by linking a random binary indicator to each variable. This Bayesian inference has the advantage of stochastically exploring the set of possible sub-models, whatever their dimension. Bayesian selection approaches, appropriate for categorical predictors, are generally beyond the scope of the standard Bayesian selection of regressors in the linear model since all levels of a categorical variable should be jointly handled in the selection procedure. For categorical covariates, new strategies have been developed to detect the effect of grouped covariates rather than the single effect of a quantitative regressor. In this paper, we review three Bayesian selection methods for categorical predictors: Bayesian Group Lasso with Spike and Slab priors, Bayesian Sparse Group Selection and Bayesian Effect Fusion using model-based clustering. The motivation behind this paper is to provide detailed information about the implementation of the three Bayesian selection methods mentioned above, appropriate for categorical predictors, using the JAGS software. Selection performance and sensitivity analysis of the hyperparameters tuning for prior specifications are assessed under various simulated scenarios. JAGS helps user implement these three Bayesian selection methods for more complex model structures such as hierarchical ones with latent layers.},
  archive      = {J_JOAS},
  author       = {Rana Jreich and Christine Hatte and Eric Parent},
  doi          = {10.1080/02664763.2021.1902955},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {2370-2388},
  shortjournal = {J. Appl. Stat.},
  title        = {Review of bayesian selection methods for categorical predictors using JAGS},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adjusting statistical benchmark risk analysis to account for
non-spatial autocorrelation, with application to natural hazard risk
assessment. <em>JOAS</em>, <em>49</em>(9), 2349–2369. (<a
href="https://doi.org/10.1080/02664763.2021.1904385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop and study a quantitative, interdisciplinary strategy for conducting statistical risk analyses within the ‘benchmark risk’ paradigm of contemporary risk assessment when potential autocorrelation exists among sample units. We use the methodology to explore information on vulnerability to natural hazards across 3108 counties in the conterminous 48 US states, applying a place-based resilience index to an existing knowledgebase of hazardous incidents and related human casualties. An extension of a centered autologistic regression model is applied to relate local, county-level vulnerability to hazardous outcomes. Adjustments for autocorrelation embedded in the resiliency information are applied via a novel, non-spatial neighborhood structure. Statistical risk-benchmarking techniques are then incorporated into the modeling framework, wherein levels of high and low vulnerability to hazards are identified.},
  archive      = {J_JOAS},
  author       = {Jingyu Liu and Walter W. Piegorsch and A. Grant Schissler and Rachel R. McCaster and Susan L. Cutter},
  doi          = {10.1080/02664763.2021.1904385},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {2349-2369},
  shortjournal = {J. Appl. Stat.},
  title        = {Adjusting statistical benchmark risk analysis to account for non-spatial autocorrelation, with application to natural hazard risk assessment},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Univariate fast initial response statistical process control
with taut strings. <em>JOAS</em>, <em>49</em>(9), 2326–2348. (<a
href="https://doi.org/10.1080/02664763.2021.1900798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel real-time univariate monitoring scheme for detecting a sustained departure of a process mean from some given standard assuming a constant variance. Our proposed stopping rule is based on the total variation of a nonparametric taut string estimator of the process mean and is designed to provide a desired average run length for an in-control situation. Compared to the more prominent CUSUM fast initial response (FIR) methodology and allowing for a restart following a false alarm, the proposed two-sided taut string (TS) scheme produces a significant reduction in average run length for a wide range of changes in the mean that occur at or immediately after process monitoring begins. A decision rule for when to choose our proposed TS chart compared to the CUSUM FIR chart that takes into account both false alarm rate and average run length to detect a shift in the mean is proposed and implemented. Supplementary materials are available online.},
  archive      = {J_JOAS},
  author       = {Michael Pokojovy and J. Marcus Jobe},
  doi          = {10.1080/02664763.2021.1900798},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {2326-2348},
  shortjournal = {J. Appl. Stat.},
  title        = {Univariate fast initial response statistical process control with taut strings},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integrated statistical and decision models for multi-stage
health care audit sampling. <em>JOAS</em>, <em>49</em>(9), 2307–2325.
(<a href="https://doi.org/10.1080/02664763.2021.1900797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health care audits are crucial in managing the government insurance programs that are estimated to have losses amounting to billions of dollars every year. Statistical methods such as sampling have long been used to handle their size and complexity. Sampling from health care claims data can benefit from multi-stage approaches, especially when the evaluation of the tradeoffs between precision and cost is important. The use of decision models could facilitate health care auditors and policy makers make the best use of these sampling outputs. This paper proposes an integrated multi-stage sampling and decision-making framework that enables auditors address the tradeoffs between audit costs and expected overpayment recovery. We illustrate the framework and discuss insights utilizing a variety of overpayment scenarios for payment populations including U.S. Medicare Part B claims payment data.},
  archive      = {J_JOAS},
  author       = {Tahir Ekin and R. M. Musal},
  doi          = {10.1080/02664763.2021.1900797},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {2307-2325},
  shortjournal = {J. Appl. Stat.},
  title        = {Integrated statistical and decision models for multi-stage health care audit sampling},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inverse power maxwell distribution: Statistical properties,
estimation and application. <em>JOAS</em>, <em>49</em>(9), 2287–2306.
(<a href="https://doi.org/10.1080/02664763.2021.1899143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduced a new probability distribution named as inverse power Maxwell distribution. The proposal distribution can be seen as an extension of the Maxwell distribution with more flexibility in modeling upside-down lifetime data. Some statistical properties of this distribution are derived. In estimation viewpoint, five methods are used for estimating the unknown parameters of the distribution and these methods are performed through the simulation study. Finally, two real data sets were analyzed to illustrate the applicability of the proposed distribution, proving that it fits each real data set much better than some other existing distributions.},
  archive      = {J_JOAS},
  author       = {H. S. Al-Kzzaz and M. M. E. Abd El-Monsef},
  doi          = {10.1080/02664763.2021.1899143},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {2287-2306},
  shortjournal = {J. Appl. Stat.},
  title        = {Inverse power maxwell distribution: Statistical properties, estimation and application},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Empirical likelihood estimation for linear regression models
with AR(p) error terms with numerical examples. <em>JOAS</em>,
<em>49</em>(9), 2271–2286. (<a
href="https://doi.org/10.1080/02664763.2021.1899142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear regression models are useful statistical tools to analyze data sets in different fields. There are several methods to estimate the parameters of a linear regression model. These methods usually perform under normally distributed and uncorrelated errors. If error terms are correlated the Conditional Maximum Likelihood (CML) estimation method under normality assumption is often used to estimate the parameters of interest. The CML estimation method is required a distributional assumption on error terms. However, in practice, such distributional assumptions on error terms may not be plausible. In this paper, we propose to estimate the parameters of a linear regression model with autoregressive error term using Empirical Likelihood (EL) method, which is a distribution free estimation method. A small simulation study is provided to evaluate the performance of the proposed estimation method over the CML method. The results of the simulation study show that the proposed estimators based on EL method are remarkably better than the estimators obtained from CML method in terms of mean squared errors (MSE) and bias in almost all the simulation configurations. These findings are also confirmed by the results of the numerical and real data examples.},
  archive      = {J_JOAS},
  author       = {Şenay Özdemir and Yeşim Güney and Yetkin Tuaç and Olcay Arslan},
  doi          = {10.1080/02664763.2021.1899142},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {2271-2286},
  shortjournal = {J. Appl. Stat.},
  title        = {Empirical likelihood estimation for linear regression models with AR(p) error terms with numerical examples},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical estimation and comparison of group-specific
bivariate correlation coefficients in family-type clustered studies.
<em>JOAS</em>, <em>49</em>(9), 2246–2270. (<a
href="https://doi.org/10.1080/02664763.2021.1899141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bivariate correlation coefficients (BCCs) are often calculated to gauge the relationship between two variables in medical research. In a family-type clustered design where multiple participants from same units/families are enrolled, BCCs can be defined and estimated at various hierarchical levels (subject level, family level and marginal BCC). Heterogeneity usually exists between subject groups and, as a result, subject level BCCs may differ between subject groups. In the framework of bivariate linear mixed effects modeling, we define and estimate BCCs at various hierarchical levels in a family-type clustered design, accommodating subject group heterogeneity. Simplified and modified asymptotic confidence intervals are constructed to the BCC differences and Wald type tests are conducted. A real-world family-type clustered study of Alzheimer disease (AD) is analyzed to estimate and compare BCCs among well-established AD biomarkers between mutation carriers and non-carriers in autosomal dominant AD asymptomatic individuals. Extensive simulation studies are conducted across a wide range of scenarios to evaluate the performance of the proposed estimators and the type-I error rate and power of the proposed statistical tests. Abbreviations: BCC: bivariate correlation coefficient; BLM: bivariate linear mixed effects model; CI: confidence interval; AD: Alzheimer’s disease; DIAN: The Dominantly Inherited Alzheimer Network; SA: simple asymptotic; MA: modified asymptotic},
  archive      = {J_JOAS},
  author       = {Jingqin Luo and Feng Gao and Jingxia Liu and Guoqiao Wang and Ling Chen and Anne M. Fagan and Gregory S. Day and Jonathan Vöglein and Jasmeer P. Chhatwal and Chengjie Xiong and the Dominantly Inherited Alzheimer Network (DIAN) Steering Committee},
  doi          = {10.1080/02664763.2021.1899141},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {2246-2270},
  shortjournal = {J. Appl. Stat.},
  title        = {Statistical estimation and comparison of group-specific bivariate correlation coefficients in family-type clustered studies},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint modelling of longitudinal response and time-to-event
data using conditional distributions: A bayesian perspective.
<em>JOAS</em>, <em>49</em>(9), 2228–2245. (<a
href="https://doi.org/10.1080/02664763.2021.1897971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last 20 or more years a lot of clinical applications and methodological development in the area of joint models of longitudinal and time-to-event outcomes have come up. In these studies, patients are followed until an event, such as death, occurs. In most of the work, using subject-specific random-effects as frailty, the dependency of these two processes has been established. In this article, we propose a new joint model that consists of a linear mixed-effects model for longitudinal data and an accelerated failure time model for the time-to-event data. These two sub-models are linked via a latent random process. This model will capture the dependency of the time-to-event on the longitudinal measurements more directly. Using standard priors, a Bayesian method has been developed for estimation. All computations are implemented using OpenBUGS. Our proposed method is evaluated by a simulation study, which compares the conditional model with a joint model with local independence by way of calibration. Data on Duchenne muscular dystrophy (DMD) syndrome and a set of data in AIDS patients have been analysed.},
  archive      = {J_JOAS},
  author       = {Srimanti Dutta and Geert Molenberghs and Arindom Chakraborty},
  doi          = {10.1080/02664763.2021.1897971},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {2228-2245},
  shortjournal = {J. Appl. Stat.},
  title        = {Joint modelling of longitudinal response and time-to-event data using conditional distributions: A bayesian perspective},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Isotonic regression for metallic microstructure data:
Estimation and testing under order restrictions. <em>JOAS</em>,
<em>49</em>(9), 2208–2227. (<a
href="https://doi.org/10.1080/02664763.2021.1896685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Investigating the main determinants of the mechanical performance of metals is not a simple task. Already known physically inspired qualitative relations between 2D microstructure characteristics and 3D mechanical properties can act as the starting point of the investigation. Isotonic regression allows to take into account ordering relations and leads to more efficient and accurate results when the underlying assumptions actually hold. The main goal in this paper is to test order relations in a model inspired by a materials science application. The statistical estimation procedure is described considering three different scenarios according to the knowledge of the variances: known variance ratio, completely unknown variances, and variances under order restrictions. New likelihood ratio tests are developed in the last two cases. Both parametric and non-parametric bootstrap approaches are developed for finding the distribution of the test statistics under the null hypothesis. Finally an application on the relation between geometrically necessary dislocations and number of observed microstructure precipitations is shown.},
  archive      = {J_JOAS},
  author       = {Martina Vittorietti and Javier Hidalgo and Jilt Sietsma and Wei Li and Geurt Jongbloed},
  doi          = {10.1080/02664763.2021.1896685},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {2208-2227},
  shortjournal = {J. Appl. Stat.},
  title        = {Isotonic regression for metallic microstructure data: Estimation and testing under order restrictions},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spike-and-slab type variable selection in the cox
proportional hazards model for high-dimensional features. <em>JOAS</em>,
<em>49</em>(9), 2189–2207. (<a
href="https://doi.org/10.1080/02664763.2021.1893285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop a variable selection framework with the spike-and-slab prior distribution via the hazard function of the Cox model. Specifically, we consider the transformation of the score and information functions for the partial likelihood function evaluated at the given data from the parameter space into the space generated by the logarithm of the hazard ratio. Thereby, we reduce the nonlinear complexity of the estimation equation for the Cox model and allow the utilization of a wider variety of stable variable selection methods. Then, we use a stochastic variable search Gibbs sampling approach via the spike-and-slab prior distribution to obtain the sparsity structure of the covariates associated with the survival outcome. Additionally, we conduct numerical simulations to evaluate the finite-sample performance of our proposed method. Finally, we apply this novel framework on lung adenocarcinoma data to find important genes associated with decreased survival in subjects with the disease.},
  archive      = {J_JOAS},
  author       = {Ryan Wu and Mihye Ahn and Hojin Yang},
  doi          = {10.1080/02664763.2021.1893285},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {2189-2207},
  shortjournal = {J. Appl. Stat.},
  title        = {Spike-and-slab type variable selection in the cox proportional hazards model for high-dimensional features},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the modified burr IV model for hydrological events:
Development, properties, characterizations and applications.
<em>JOAS</em>, <em>49</em>(9), 2167–2188. (<a
href="https://doi.org/10.1080/02664763.2021.1893284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new distribution for modeling extreme events about frequency analysis called modified Burr IV (MBIV) distribution. We derive the MBIV distribution on the basis of the generalized Pearson differential equation. The proposed model turns out to be flexible: its density function can be symmetrical, right-skewed, left-skewed, J and bimodal shaped. Its hazard rate has shapes such as bathtub and modified bathtub, increasing, decreasing, and increasing-decreasing-increasing. To show the importance of the MBIV distribution, we establish various mathematical properties such as random number generator, sub-models, moments related properties, inequality measures, reliability measures, uncertainty measures and characterizations. We utilize the maximum likelihood estimation technique to estimate the model parameters. We assess the behavior of the maximum likelihood estimators (MLEs) of the MBIV parameters via a simulation study. Five data sets related to frequency analysis are considered to elucidate the significance of the MBIV distribution. We show that the MBIV model is the best model to analyze data for hydrological events, motivating its high level of adaptability in the applied setting.},
  archive      = {J_JOAS},
  author       = {Fiaz Ahmad Bhatti and Munir Ahmad},
  doi          = {10.1080/02664763.2021.1893284},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {2167-2188},
  shortjournal = {J. Appl. Stat.},
  title        = {On the modified burr IV model for hydrological events: Development, properties, characterizations and applications},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating the efficiency of brazilian electricity
distribution utilities. <em>JOAS</em>, <em>49</em>(8), 2157–2166. (<a
href="https://doi.org/10.1080/02664763.2021.1890000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a differing methodology from the Brazilian Electricity Regulatory Agency on the efficiency estimation for the Brazilian electricity distribution sector. Our proposal combines robust state-space models and stochastic frontier analysis to measure the operational cost efficiency in a panel data set from 60 Brazilian electricity distribution utilities. The modeling joins the main literature in energy economics with advanced econometric and statistic techniques in order to estimate the efficiencies. Moreover, the suggested model is able to deal with changes in the inefficiencies across time whilst the Bayesian paradigm – through Markov chain Monte Carlo techniques – facilitates the inference on all unknowns. The method enables a significant degree of flexibility in the resultant efficiencies and a complete photography about the distribution sector.},
  archive      = {J_JOAS},
  author       = {Marcus Gerardus L. Nascimento and Ralph S. Silva and Mario Jorge Mendonça and Amaro Olimpio Pereira Jr.},
  doi          = {10.1080/02664763.2021.1890000},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {2157-2166},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimating the efficiency of brazilian electricity distribution utilities},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Capturing spatiotemporal dynamics of alaskan groundfish
catch using signed-rank estimation for varying coefficient models.
<em>JOAS</em>, <em>49</em>(8), 2137–2156. (<a
href="https://doi.org/10.1080/02664763.2021.1889996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Varying coefficient models (VCMs) are commonly used for their high degree of flexibility in modeling complex systems. Many applications in fisheries utilize VCMs to capture spatial variation in populations of marine fishes. All of these applications use the penalized least squares method for estimation. However, this approach is known to be sensitive to non-normal distributions and outliers, a common feature of ecological data. Robust estimation methods are more appropriate for handling noisy and non-normal data. We present the application of a signed-rank-based procedure for obtaining robust estimates in VCMs on a fisheries dataset from the North Pacific Ocean. We demonstrates that the signed-rank-based estimation method provides better fit and improved prediction in comparison to the classical likelihood VCM fits in both simulations and the real data application, particularly when the distributions are non-normal and may be misspecified. Rank-based estimation of VCMs is therefore valuable for modeling ecological data and obtaining useful inferences where non-normality and outliers are common.},
  archive      = {J_JOAS},
  author       = {H. E. Correia and A. Abebe},
  doi          = {10.1080/02664763.2021.1889996},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {2137-2156},
  shortjournal = {J. Appl. Stat.},
  title        = {Capturing spatiotemporal dynamics of alaskan groundfish catch using signed-rank estimation for varying coefficient models},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modified ridge-type for the poisson regression model:
Simulation and application. <em>JOAS</em>, <em>49</em>(8), 2124–2136.
(<a href="https://doi.org/10.1080/02664763.2021.1889998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Poisson regression model (PRM) is employed in modelling the relationship between a count variable (y) and one or more explanatory variables. The parameters of PRM are popularly estimated using the Poisson maximum likelihood estimator (PMLE). There is a tendency that the explanatory variables grow together, which results in the problem of multicollinearity. The variance of the PMLE becomes inflated in the presence of multicollinearity. The Poisson ridge regression (PRRE) and Liu estimator (PLE) have been suggested as an alternative to the PMLE. However, in this study, we propose a new estimator to estimate the regression coefficients for the PRM when multicollinearity is a challenge. We perform a simulation study under different specifications to assess the performance of the new estimator and the existing ones. The performance was evaluated using the scalar mean square error criterion and the mean squared error prediction error. The aircraft damage data was adopted for the application study and the estimators’ performance judged by the SMSE and the mean squared prediction error. The theoretical comparison shows that the proposed estimator outperforms other estimators. This is further supported by the simulation study and the application result.},
  archive      = {J_JOAS},
  author       = {Adewale F. Lukman and Benedicta Aladeitan and Kayode Ayinde and Mohamed R. Abonazel},
  doi          = {10.1080/02664763.2021.1889998},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {2124-2136},
  shortjournal = {J. Appl. Stat.},
  title        = {Modified ridge-type for the poisson regression model: Simulation and application},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust inference for skewed data in health sciences.
<em>JOAS</em>, <em>49</em>(8), 2093–2123. (<a
href="https://doi.org/10.1080/02664763.2021.1891527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health data are often not symmetric to be adequately modeled through the usual normal distributions; most of them exhibit skewed patterns. They can indeed be modeled better through the larger family of skew-normal distributions covering both skewed and symmetric cases. Since outliers are not uncommon in complex real-life experimental datasets, a robust methodology automatically taking care of the noises in the data would be of great practical value to produce stable and more precise research insights leading to better policy formulation. In this paper, we develop a class of robust estimators and testing procedures for the family of skew-normal distributions using the minimum density power divergence approach with application to health data. In particular, a robust procedure for testing of symmetry is discussed in the presence of outliers. Two efficient computational algorithms are discussed. Besides deriving the asymptotic and robustness theory for the proposed methods, their advantages and utilities are illustrated through simulations and a couple of real-life applications for health data of athletes from Australian Institute of Sports and AIDS clinical trial data.},
  archive      = {J_JOAS},
  author       = {Amarnath Nandy and Ayanendranath Basu and Abhik Ghosh},
  doi          = {10.1080/02664763.2021.1891527},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {2093-2123},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust inference for skewed data in health sciences},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inference for partially observed competing risks model for
kumaraswamy distribution under generalized progressive hybrid censoring.
<em>JOAS</em>, <em>49</em>(8), 2064–2092. (<a
href="https://doi.org/10.1080/02664763.2021.1889999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, inference for a competing risks model is studied when latent failure times follow Kumaraswamy distribution and causes of failure are partially observed. Under generalized progressive hybrid censoring, existence and uniqueness of maximum likelihood estimators of model parameters are established. The confidence intervals are obtained by using asymptotic distribution theory. We further compute Bayes estimators along with credible intervals. In addition, inference is also discussed when there is order restricted shape parameters. The performance of all estimates is investigated using Monte-Carlo simulations. Finally, analysis of a real data set is presented for illustration purposes.},
  archive      = {J_JOAS},
  author       = {Amulya Kumar Mahto and Chandrakant Lodhi and Yogesh Mani Tripathi and Liang Wang},
  doi          = {10.1080/02664763.2021.1889999},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {2064-2092},
  shortjournal = {J. Appl. Stat.},
  title        = {Inference for partially observed competing risks model for kumaraswamy distribution under generalized progressive hybrid censoring},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bootstrap aggregated classification for sparse functional
data. <em>JOAS</em>, <em>49</em>(8), 2052–2063. (<a
href="https://doi.org/10.1080/02664763.2021.1889997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse functional data are commonly observed in real-data analyzes. For such data, we propose a new classification method based on functional principal component analysis (FPCA) and bootstrap aggregating. Bootstrap aggregating is believed to improve the single classifier. In this paper, we apply this belief to an FPCA based classification, and compare the classification performance with that of the single classifiers. The simulation results show that the proposed method performs better than the conventional single classifiers. We then conduct two real-data analyzes.},
  archive      = {J_JOAS},
  author       = {Hyunsung Kim and Yaeji Lim},
  doi          = {10.1080/02664763.2021.1889997},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {2052-2063},
  shortjournal = {J. Appl. Stat.},
  title        = {Bootstrap aggregated classification for sparse functional data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new heteroscedastic regression to analyze mass loss of
wood in civil construction in brazil. <em>JOAS</em>, <em>49</em>(8),
2035–2051. (<a
href="https://doi.org/10.1080/02664763.2021.1890001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A heteroscedastic regression based on the odd log-logistic Marshall–Olkin normal (OLLMON) distribution is defined by extending previous models. Some structural properties of this distribution are presented. The estimation of the parameters is addressed by maximum likelihood. For different parameter settings, sample sizes and some scenarios, various simulations investigate the performance of the heteroscedastic OLLMON regression. We use residual analysis to detect influential observations and to check the model assumptions. The new regression explains the mass loss of different wood species in civil construction in Brazil.},
  archive      = {J_JOAS},
  author       = {J. C. S. Vasconcelos and E. M. M. Ortega and J. S. Vasconcelos and G. M. Cordeiro and A. L. Vivan and M. A. M. Biaggioni},
  doi          = {10.1080/02664763.2021.1890001},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {2035-2051},
  shortjournal = {J. Appl. Stat.},
  title        = {A new heteroscedastic regression to analyze mass loss of wood in civil construction in brazil},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new linearized ridge poisson estimator in the presence of
multicollinearity. <em>JOAS</em>, <em>49</em>(8), 2016–2034. (<a
href="https://doi.org/10.1080/02664763.2021.1887103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Poisson regression is a very commonly used technique for modeling the count data in applied sciences, in which the model parameters are usually estimated by the maximum likelihood method. However, the presence of multicollinearity inflates the variance of maximum likelihood (ML) estimator and the estimated parameters give unstable results. In this article, a new linearized ridge Poisson estimator is introduced to deal with the problem of multicollinearity. Based on the asymptotic properties of ML estimator, the bias, covariance and mean squared error of the proposed estimator are obtained and the optimal choice of shrinkage parameter is derived. The performance of the existing estimators and proposed estimator is evaluated through Monte Carlo simulations and two real data applications. The results clearly reveal that the proposed estimator outperforms the existing estimators in the mean squared error sense.},
  archive      = {J_JOAS},
  author       = {Nileshkumar H. Jadhav},
  doi          = {10.1080/02664763.2021.1887103},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {2016-2034},
  shortjournal = {J. Appl. Stat.},
  title        = {A new linearized ridge poisson estimator in the presence of multicollinearity},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Negligible interaction test for continuous predictors.
<em>JOAS</em>, <em>49</em>(8), 2001–2015. (<a
href="https://doi.org/10.1080/02664763.2021.1887102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Behavioral science researchers are often interested in whether there is negligible interaction among continuous predictors of an outcome variable. For example, a researcher might be interested in demonstrating that the effect of perfectionism on depression is very consistent across age. In this case, the researcher is interested in assessing whether the interaction between the predictors is too small to be meaningful. Unfortunately, most researchers address the above research question using a traditional association-based null hypothesis test (e.g. regression) where their goal is to fail to reject the null hypothesis of no interaction. Common problems with traditional tests are their sensitivity to sample size and their opposite (and hence inappropriate) hypothesis setup for finding a negligible interaction effect. In this study, we investigated a method for testing for negligible interaction between continuous predictors using unstandardized and standardized regression-based models and equivalence testing. A Monte Carlo study provides evidence for the effectiveness of the equivalence-based test relative to traditional approaches.},
  archive      = {J_JOAS},
  author       = {Yasaman Jabbari and Robert Cribbie},
  doi          = {10.1080/02664763.2021.1887102},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {2001-2015},
  shortjournal = {J. Appl. Stat.},
  title        = {Negligible interaction test for continuous predictors},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatio-temporal parse network-based trajectory modeling on
the dynamics of criminal justice system. <em>JOAS</em>, <em>49</em>(8),
1979–2000. (<a
href="https://doi.org/10.1080/02664763.2021.1887101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We extend the existing group-based trajectory modeling by proposing the network-based trajectory modeling based on judicious design and analysis of a spatio-temporal parse network (STPN) as a representation of neighborhood structure that evolves in time. The STPN offers a principled qualitative specification for an explicit paradigm framework to deal with complex real-world problems. The framework is completed by developing a quantitative specification of latent field representation to merge seamlessly on or alongside the established STPN via hierarchical modeling. The models adopt spatial random effects to characterize the heterogeneity and autocorrelation over the locations where nonlinear trajectories were observed. The trajectories are then investigated in the presence of the operational constraints of the dependence structure induced by the spatial and temporal dimensions. With the framework, complex developmental trajectory problems can be discerned, communicated, diagnosed and modeled in a relatively simple way that interpretation is accessible to nontechnical audiences and quickly comprehensible to technically sophisticated audiences. The proposed modeling is applied to address the challenges of the trajectory modeling of nonlinear dynamics arising from a motivating criminal justice empirical process.},
  archive      = {J_JOAS},
  author       = {Han Yu and Shanhe Jiang and Hong Huang},
  doi          = {10.1080/02664763.2021.1887101},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1979-2000},
  shortjournal = {J. Appl. Stat.},
  title        = {Spatio-temporal parse network-based trajectory modeling on the dynamics of criminal justice system},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient accounting for estimation uncertainty in coherent
forecasting of count processes. <em>JOAS</em>, <em>49</em>(8),
1957–1978. (<a
href="https://doi.org/10.1080/02664763.2021.1887104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coherent forecasting techniques for count processes generate forecasts that consist of count values themselves. In practice, forecasting always relies on a fitted model and so the obtained forecast values are affected by estimation uncertainty. Thus, they may differ from the true forecast values as they would have been obtained from the true data generating process. We propose a computationally efficient resampling scheme that allows to express the uncertainty in common types of coherent forecasts for count processes. The performance of the resampling scheme, which results in ensembles of forecast values, is investigated in a simulation study. A real-data example is used to demonstrate the application of the proposed approach in practice. It is shown that the obtained ensembles of forecast values can be presented in a visual way that allows for an intuitive interpretation.},
  archive      = {J_JOAS},
  author       = {Christian H. Weiß and Annika Homburg and Layth C. Alwan and Gabriel Frahm and Rainer Göb},
  doi          = {10.1080/02664763.2021.1887104},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1957-1978},
  shortjournal = {J. Appl. Stat.},
  title        = {Efficient accounting for estimation uncertainty in coherent forecasting of count processes},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An one-parameter compounding discrete distribution.
<em>JOAS</em>, <em>49</em>(8), 1935–1956. (<a
href="https://doi.org/10.1080/02664763.2021.1884846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a new one-parameter discrete distribution obtained by compounding the Poisson and xgamma distributions is proposed. Some statistical properties of the new distribution are obtained including moments and probability and moment generating functions. Two methods are used for the estimation of the unknown parameter: the maximum likelihood method and the method of moments. Additionally, the count regression model and integer-valued autoregressive process of the proposed distribution are introduced. Some possible applications of the introduced models are considered and discussed.},
  archive      = {J_JOAS},
  author       = {Emrah Altun and Gauss M. Cordeiro and Miroslav M. Ristić},
  doi          = {10.1080/02664763.2021.1884846},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1935-1956},
  shortjournal = {J. Appl. Stat.},
  title        = {An one-parameter compounding discrete distribution},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation of stress–strength reliability for marshall–olkin
distributions based on progressively type-II censored samples.
<em>JOAS</em>, <em>49</em>(8), 1913–1934. (<a
href="https://doi.org/10.1080/02664763.2021.1884207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are mainly interested in estimating the stress–strength parameter, say R , when the parent distribution follows the well-known Marshall–Olkin model and the accessible data has the form of the progressively Type-II censored samples. In this case, the stress–strength parameter is free of the base distribution employed in the Marshall–Olkin model. Thus, we use the exponential distribution for simplicity. The maximum likelihood methods as well as some Bayesian approaches are used for the estimation purpose. The corresponding estimators of the latter approach are obtained by using Lindley&#39;s approximation and Gibbs sampling methods since the Bayesian estimator of R cannot be obtained as an explicit form. Moreover, some confidence intervals of various types are derived for R and then compared via a Monte Carlo simulation. Finally, the survival times of head and neck cancer patients are analyzed by two therapies for illustrating purposes.},
  archive      = {J_JOAS},
  author       = {Sara Ghanbari and Abdolhamid Rezaei Roknabadi and Mahdi Salehi},
  doi          = {10.1080/02664763.2021.1884207},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1913-1934},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimation of stress–strength reliability for Marshall–Olkin distributions based on progressively type-II censored samples},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic factor structure of team performances in liga MX.
<em>JOAS</em>, <em>49</em>(7), 1900–1912. (<a
href="https://doi.org/10.1080/02664763.2021.1881946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Team performance of the Mexican Football League (Liga MX), measured as the percentage of the total points obtained during each short tournament, is analyzed using Dynamic Factor Models (DFMs). The estimation of the common components is carried out with Principal Components and the stochastic nature of the DFM is studied through Panel Analysis of Non-stationarity in Idiosyncratic and Common Components. The results reveal that there are two common factors, one being possibly non-stationary. These factors show an interesting dynamic behavior in the league and allow to split the teams into two groups, namely, top competitors and emerging or relegated teams. Some discussion is given in this direction.},
  archive      = {J_JOAS},
  author       = {Francisco Corona and Nelson Muriel and Graciela González-Farías},
  doi          = {10.1080/02664763.2021.1881946},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1900-1912},
  shortjournal = {J. Appl. Stat.},
  title        = {Dynamic factor structure of team performances in liga MX},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How many people participated in candlelight protests?
Counting the size of a dynamic crowd. <em>JOAS</em>, <em>49</em>(7),
1890–1899. (<a
href="https://doi.org/10.1080/02664763.2021.1871591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent controversy about the size of crowds at candlelight protests in Korea raises an interesting question regarding the methods used to estimate crowd size. Protest organizers tend to count all participants in the event from its start to finish, while the police usually report the crowd size at its peak. While several counting methods are available to estimate the size of a crowd at a given time, counting the total number of the participants at a protest is not straightforward. In this paper, we propose a new estimator to count the total number of participants that we call the size of a dynamic crowd. We assume that the arrival and departure times of the crowd are randomly observed and that the number of the attendees in the crowd at a specific time is estimable. We estimate the number of total attendees during the entire gathering based on the capture-recapture model. We also propose a bootstrap procedure to construct a confidence interval for the crowd size. We demonstrate the performance of the proposed method with simulation studies and the data from Korea&#39;s March for Science, a global event across the world on Earth Day, April 22, 2017.},
  archive      = {J_JOAS},
  author       = {Seonghun Cho and Johan Lim and Woncheol Jang},
  doi          = {10.1080/02664763.2021.1871591},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1890-1899},
  shortjournal = {J. Appl. Stat.},
  title        = {How many people participated in candlelight protests? counting the size of a dynamic crowd},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shape-constrained gaussian process regression for surface
reconstruction and multimodal, non-rigid image registration.
<em>JOAS</em>, <em>49</em>(7), 1865–1889. (<a
href="https://doi.org/10.1080/02664763.2021.1897970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new statistical framework for landmark ?&gt;curve-based image registration and surface reconstruction. The proposed method first elastically aligns geometric features (continuous, parameterized curves) to compute local deformations, and then uses a Gaussian random field model to estimate the full deformation vector field as a spatial stochastic process on the entire surface or image domain. The statistical estimation is performed using two different methods: maximum likelihood and Bayesian inference via Markov Chain Monte Carlo sampling. The resulting deformations accurately match corresponding curve regions while also being sufficiently smooth over the entire domain. We present several qualitative and quantitative evaluations of the proposed method on both synthetic and real data. We apply our approach to two different tasks on real data: (1) multimodal medical image registration, and (2) anatomical and pottery surface reconstruction.},
  archive      = {J_JOAS},
  author       = {Thomas Deregnaucourt and Chafik Samir and Sebastian Kurtek and Anne-Francoise Yao},
  doi          = {10.1080/02664763.2021.1897970},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1865-1889},
  shortjournal = {J. Appl. Stat.},
  title        = {Shape-constrained gaussian process regression for surface reconstruction and multimodal, non-rigid image registration},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On correlation rank screening for ultra-high dimensional
competing risks data. <em>JOAS</em>, <em>49</em>(7), 1848–1864. (<a
href="https://doi.org/10.1080/02664763.2021.1884209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, numerous feature screening schemes have been developed for ultra-high dimensional standard survival data with only one failure event. Nevertheless, existing literature pays little attention to related investigations for competing risks data, in which subjects suffer from multiple mutually exclusive failures. In this article, we develop a new marginal feature screening for ultra-high dimensional time-to-event data to allow for competing risks. The proposed procedure is model-free, and robust against heavy-tailed distributions and potential outliers for time to the type of failure of interest. Apart from this, it is invariant to any monotone transformation of event time of interest. Under rather mild assumptions, it is shown that the newly suggested approach possesses the ranking consistency and sure independence screening properties. Some numerical studies are conducted to evaluate the finite-sample performance of our method and make a comparison with its competitor, while an application to a real data set is provided to serve as an illustration.},
  archive      = {J_JOAS},
  author       = {Xiaolin Chen and Chenguang Li and Tao Zhang and Zhenlong Gao},
  doi          = {10.1080/02664763.2021.1884209},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1848-1864},
  shortjournal = {J. Appl. Stat.},
  title        = {On correlation rank screening for ultra-high dimensional competing risks data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modelling and monitoring of INAR(1) process with
geometrically inflated poisson innovations. <em>JOAS</em>,
<em>49</em>(7), 1821–1847. (<a
href="https://doi.org/10.1080/02664763.2021.1884206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To analyse count time series data inflated at the r + 1 values { 0 , 1 , … , r } { 0 , 1 , … , r } {0,1,…,r} , we propose a new first-order integer-valued autoregressive process with r -geometrically inflated Poisson innovations. Some statistical properties together with conditional maximum likelihood estimate are provided. For the purpose of statistical monitoring, we focus on the cumulative sum chart, exponentially weighted moving average chart and combined jumps chart towards the proposed process. Numerical simulations indicate that the conditional maximum likelihood estimator is unbiased. Moreover, the cumulative sum chart is the best choice to monitor our model in practice. Some applications about telephone complaints data are provided to illustrate the proposed methods.},
  archive      = {J_JOAS},
  author       = {Cong Li and Haixiang Zhang and Dehui Wang},
  doi          = {10.1080/02664763.2021.1884206},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1821-1847},
  shortjournal = {J. Appl. Stat.},
  title        = {Modelling and monitoring of INAR(1) process with geometrically inflated poisson innovations},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Competing risks model for clustered data based on the
subdistribution hazards with spatial random effects. <em>JOAS</em>,
<em>49</em>(7), 1802–1820. (<a
href="https://doi.org/10.1080/02664763.2021.1884208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In some applications, the clustered survival data are arranged spatially such as clinical centers or geographical regions. Incorporating spatial variation in these data not only can improve the accuracy and efficiency of the parameter estimation, but it also investigates the spatial patterns of survivorship for identifying high-risk areas. Competing risks in survival data concern a situation where there is more than one cause of failure, but only the occurrence of the first one is observable. In this paper, we considered Bayesian subdistribution hazard regression models with spatial random effects for the clustered HIV/AIDS data. An intrinsic conditional autoregressive (ICAR) distribution was employed to model the areal spatial random effects. Comparison among competing models was performed by the deviance information criterion. We illustrated the gains of our model through application to the HIV/AIDS data and the simulation studies.},
  archive      = {J_JOAS},
  author       = {Somayeh Momenyan and Farzane Ahmadi and Jalal Poorolajal},
  doi          = {10.1080/02664763.2021.1884208},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1802-1820},
  shortjournal = {J. Appl. Stat.},
  title        = {Competing risks model for clustered data based on the subdistribution hazards with spatial random effects},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cause-specific hazard regression estimation for modified
weibull distribution under a class of non-informative priors.
<em>JOAS</em>, <em>49</em>(7), 1784–1801. (<a
href="https://doi.org/10.1080/02664763.2021.1882407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In time to event analysis, the situation of competing risks arises when the individual (or subject) may experience p mutually exclusive causes of death (failure), where cause-specific hazard function is of great importance in this framework. For instance, in malignancy-related death, colorectal cancer is one of the leading causes of the death in the world and death due to other causes considered as competing causes. We include prognostic variables in the model through parametric Cox proportional hazards model. Mostly, in literature exponential, Weibull, etc. distributions have been used for parametric modelling of cause-specific hazard function but they are incapable to accommodate non-monotone failure rate. Therefore, in this article, we consider a modified Weibull distribution which is capable to model survival data with non-monotonic behaviour of hazard rate. For estimating the cumulative cause-specific hazard function, we utilized maximum likelihood and Bayesian methods. A class of non-informative types of prior (uniform, Jeffrey’s and half- t ) is introduced for Bayes estimation under squared error (symmetric) as well as LINEX (asymmetric) loss functions. A simulation study is performed for a comprehensive comparison of Bayes and maximum likelihood estimators of cumulative cause-specific hazard function. Real data on colorectal cancer is used to demonstrate the proposed model.},
  archive      = {J_JOAS},
  author       = {H. Rehman and N. Chandra and Fatemeh Sadat Hosseini-Baharanchi and Ahmad Reza Baghestani and Mohamad Amin Pourhoseingholi},
  doi          = {10.1080/02664763.2021.1882407},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1784-1801},
  shortjournal = {J. Appl. Stat.},
  title        = {Cause-specific hazard regression estimation for modified weibull distribution under a class of non-informative priors},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Current status data with two competing risks and missing
failure types: A parametric approach. <em>JOAS</em>, <em>49</em>(7),
1769–1783. (<a
href="https://doi.org/10.1080/02664763.2021.1881453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing cause of failure is a common problem in competing risks data. Here we consider a general missing pattern in which one observes a set of possible causes containing the true cause. In this work, we focus on the parametric analysis of current status data with two competing risks and the above-mentioned missing pattern. We make some simpler assumptions on the conditional probability of observing a set of possible causes of failure given the true cause and carry out maximum-likelihood estimation of the model parameters. Asymptotic properties of the maximum-likelihood estimators are also discussed. Simulation studies are performed to study the finite sample properties of the estimators and also to investigate the role of the monitoring time distribution. Finally, the method is illustrated through the analysis of a real data set.},
  archive      = {J_JOAS},
  author       = {Tamalika Koley and Anup Dewanji},
  doi          = {10.1080/02664763.2021.1881453},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1769-1783},
  shortjournal = {J. Appl. Stat.},
  title        = {Current status data with two competing risks and missing failure types: A parametric approach},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of mixed correlated overdispersed binomial and
ordinal longitudinal responses: LogLindley-binomial and ordinal random
effects model. <em>JOAS</em>, <em>49</em>(7), 1742–1768. (<a
href="https://doi.org/10.1080/02664763.2021.1881455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new model called LogLindley-Binomial and ordinal joint model with random effects for analyzing mixed overdispersed binomial and ordinal longitudinal responses. A new distribution called the LogLindley-Binomial is presented, which is appropriate for the analysis of overdispersed binomial variables. A full likelihood-based approach is used to obtain maximum likelihood estimates. A comparison between LogLindley-Binomial and Beta-Binomial distributions are given by a simulation study. Also, to illustrate the utility of the proposed model, some simulation studies are conducted. In simulation studies, the performances of the LogLindley-Binomial distribution and the proposed model are well in some situations. Also, the new model&#39;s performance for analyzing a real dataset, extracted from the British Household Panel Survey, is studied. The proposed model performs well in comparison with another model for analyzing real data. Finally, the proposed distribution and the new model are found to be applicable for analyzing overdispersed binomial and mixed data.},
  archive      = {J_JOAS},
  author       = {Seyede Sedighe Azimi and Ehsan Bahrami Samani and Mojtaba Ganjali},
  doi          = {10.1080/02664763.2021.1881455},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1742-1768},
  shortjournal = {J. Appl. Stat.},
  title        = {Analysis of mixed correlated overdispersed binomial and ordinal longitudinal responses: LogLindley-binomial and ordinal random effects model},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multivariate nonparametric methods in two-way balanced
designs: Performances and limitations in small samples. <em>JOAS</em>,
<em>49</em>(7), 1714–1741. (<a
href="https://doi.org/10.1080/02664763.2021.1915256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Investigations of multivariate population are pretty common in applied researches, and the two-way crossed factorial design is a common design used at the exploratory phase in industrial applications. When assumptions such as multivariate normality and covariance homogeneity are violated, the conventional wisdom is to resort to nonparametric tests for hypotheses testing. In this paper we compare the performances, and in particular the power, of some nonparametric and semi-parametric methods that have been developed in recent years. Specifically, we examined resampling methods and robust versions of classical multivariate analysis of variance (MANOVA) tests. In a simulation study, we generate data sets with different configurations of factor&#39;s effect, number of replicates, number of response variables under null hypothesis, and number of response variables under alternative hypothesis. The objective is to elicit practical advice and guides to practitioners regarding the sensitivity of the tests in the various configurations, the tradeoff between power and type I error, the strategic impact of increasing number of response variables, and the favourable performance of one test when the alternative is sparse. A real case study from an industrial engineering experiment in thermoformed packaging production is used to compare and illustrate the application of the various methods.},
  archive      = {J_JOAS},
  author       = {Fabrizio Ronchi and Solomon W. Harrar and Luigi Salmaso},
  doi          = {10.1080/02664763.2021.1915256},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1714-1741},
  shortjournal = {J. Appl. Stat.},
  title        = {Multivariate nonparametric methods in two-way balanced designs: Performances and limitations in small samples},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian selector of adaptive bandwidth for multivariate
gamma kernel estimator on [0,∞ )d. <em>JOAS</em>, <em>49</em>(7),
1692–1713. (<a
href="https://doi.org/10.1080/02664763.2021.1881456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian bandwidth selections in multivariate associated kernel estimation of probability density functions are known to improve classical methods such as cross-validation techniques in terms of execution time and smoothing quality. The paper focuses on a basic multivariate gamma kernel which is appropriated to estimate densities with support [ 0 , ∞ ) d [ 0 , ∞ ) d [0,∞)d . For this purpose, we consider a Bayesian adaptive estimation of the bandwidths vector under the usual quadratic loss function. The exact expression of the posterior distribution and the vector of bandwidths are obtained. Simulations studies highlight the excellent performance of the proposed approach, comparing to the global cross-validation bandwidth selection, and under integrated squared errors. Two bivariate and trivariate applications to the Old Faithful geyser data and new ones on drinking water pumps in the Sahel, respectively, are made.},
  archive      = {J_JOAS},
  author       = {Sobom M. Somé and Célestin C. Kokonendji},
  doi          = {10.1080/02664763.2021.1881456},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1692-1713},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian selector of adaptive bandwidth for multivariate gamma kernel estimator on [0,∞ )d},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Testing differentially methylated regions through functional
principal component analysis. <em>JOAS</em>, <em>49</em>(7), 1677–1691.
(<a href="https://doi.org/10.1080/02664763.2021.1877636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DNA methylation is an epigenetic modification that plays an important role in many biological processes and diseases. Several statistical methods have been proposed to test for DNA methylation differences between conditions at individual cytosine sites, followed by a post hoc aggregation procedure to explore regional differences. While there are benefits to analyzing CpGs individually, there are both biological and statistical reasons to test entire genomic regions for differential methylation. Variability in methylation levels measured by Next-Generation Sequencing (NGS) is often observed across CpG sites in a genomic region. Evaluating meaningful changes in regional level methylation profiles between conditions over noisy site-level measurements is often difficult to implement with parametric models. To overcome these limitations, this study develops a nonparametric approach to detect predefined differentially methylated regions (DMR) based on functional principal component analysis (FPCA). The performance of this approach is compared with two alternative methods (GIFT and M3D), using real and simulated data.},
  archive      = {J_JOAS},
  author       = {Mohamed Milad and Gayla R. Olbricht},
  doi          = {10.1080/02664763.2021.1877636},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1677-1691},
  shortjournal = {J. Appl. Stat.},
  title        = {Testing differentially methylated regions through functional principal component analysis},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robust estimation method for the linear regression model
parameters with correlated error terms and outliers. <em>JOAS</em>,
<em>49</em>(7), 1663–1676. (<a
href="https://doi.org/10.1080/02664763.2021.1881454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Independence of error terms in a linear regression model, often not established. So a linear regression model with correlated error terms appears in many applications. According to the earlier studies, this kind of error terms, basically can affect the robustness of the linear regression model analysis. It is also shown that the robustness of the parameters estimators of a linear regression model can stay using the M-estimator. But considering that, it acquires this feature as the result of establishment of its efficiency. Whereas, it has been shown that the minimum Matusita distance estimators, has both features robustness and efficiency at the same time. On the other hand, because the Cochrane and Orcutt adjusted least squares estimators are not affected by the dependence of the error terms, so they are efficient estimators. Here we are using of a non-parametric kernel density estimation method, to give a new method of obtaining the minimum Matusita distance estimators for the linear regression model with correlated error terms in the presence of outliers. Also, simulation and real data study both are done for the introduced estimation method. In each case, the proposed method represents lower biases and mean squared errors than the other two methods.},
  archive      = {J_JOAS},
  author       = {Sajjad Piradl and Ali Shadrokh and Masoud Yarmohammadi},
  doi          = {10.1080/02664763.2021.1881454},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1663-1676},
  shortjournal = {J. Appl. Stat.},
  title        = {A robust estimation method for the linear regression model parameters with correlated error terms and outliers},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rapid detection of hot-spots via tensor decomposition with
applications to crime rate data. <em>JOAS</em>, <em>49</em>(7),
1636–1662. (<a
href="https://doi.org/10.1080/02664763.2021.1874892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world applications of monitoring multivariate spatio-temporal data that are non-stationary over time, one is often interested in detecting hot-spots with spatial sparsity and temporal consistency, instead of detecting system-wise changes as in traditional statistical process control (SPC) literature. In this paper, we propose an efficient method to detect hot-spots through tensor decomposition, and our method has three steps. First, we fit the observed data into a Smooth Sparse Decomposition Tensor (SSD-Tensor) model that serves as a dimension reduction and de-noising technique: it is an additive model decomposing the original data into: smooth but non-stationary global mean, sparse local anomalies, and random noises. Next, we estimate model parameters by the penalized framework that includes Least Absolute Shrinkage and Selection Operator (LASSO) and fused LASSO penalty. An efficient recursive optimization algorithm is developed based on Fast Iterative Shrinkage Thresholding Algorithm (FISTA). Finally, we apply a Cumulative Sum (CUSUM) Control Chart to monitor model residuals after removing global means, which helps to detect when and where hot-spots occur. To demonstrate the usefulness of our proposed SSD-Tensor method, we compare it with several other methods including scan statistics, LASSO-based, PCA-based, T2-based control chart in extensive numerical simulation studies and a real crime rate dataset.},
  archive      = {J_JOAS},
  author       = {Yujie Zhao and Hao Yan and Sarah Holte and Yajun Mei},
  doi          = {10.1080/02664763.2021.1874892},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1636-1662},
  shortjournal = {J. Appl. Stat.},
  title        = {Rapid detection of hot-spots via tensor decomposition with applications to crime rate data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new flexible generalized family for constructing many
families of distributions. <em>JOAS</em>, <em>49</em>(7), 1615–1635. (<a
href="https://doi.org/10.1080/02664763.2021.1874891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new flexible generalized family (NFGF) for constructing many families of distributions. The importance of the NFGF is that any baseline distribution can be chosen and it does not involve any additional parameters. Some useful statistical properties of the NFGF are determined such as a linear representation for the family density, analytical shapes of the density and hazard rate, random variable generation, moments and generating function. Further, the structural properties of a special model named the new flexible Kumaraswamy (NFKw) distribution, are investigated, and the model parameters are estimated by maximum-likelihood method. A simulation study is carried out to assess the performance of the estimates. The usefulness of the NFKw model is proved empirically by means of three real-life data sets. In fact, the two-parameter NFKw model performs better than three-parameter transmuted-Kumaraswamy, three-parameter exponentiated-Kumaraswamy and the well-known two-parameter Kumaraswamy models.},
  archive      = {J_JOAS},
  author       = {M. H. Tahir and M. Adnan Hussain and Gauss M. Cordeiro},
  doi          = {10.1080/02664763.2021.1874891},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1615-1635},
  shortjournal = {J. Appl. Stat.},
  title        = {A new flexible generalized family for constructing many families of distributions},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Correction notice. <em>JOAS</em>, <em>49</em>(6),
1612–1614. (<a
href="https://doi.org/10.1080/02664763.2022.2051984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOAS},
  doi          = {10.1080/02664763.2022.2051984},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1612-1614},
  shortjournal = {J. Appl. Stat.},
  title        = {Correction notice},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Correction notice. <em>JOAS</em>, <em>49</em>(6), 1611. (<a
href="https://doi.org/10.1080/02664763.2022.2049996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOAS},
  doi          = {10.1080/02664763.2022.2049996},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1611},
  shortjournal = {J. Appl. Stat.},
  title        = {Correction notice},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Risk analysis in the brazilian stock market: Copula-APARCH
modeling for value-at-risk. <em>JOAS</em>, <em>49</em>(6), 1598–1610.
(<a href="https://doi.org/10.1080/02664763.2020.1865883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risk management of stock portfolios is a fundamental problem for the financial analysis since it indicates the potential losses of an investment at any given time. The objective of this study is to use bivariate static conditional copulas to quantify the dependence structure and to estimate the risk measure Value-at-Risk (VaR). There were selected stocks that have been performing outstandingly on the Brazilian Stock Exchange to compose pairs trading portfolios (B3, Gerdau, Magazine Luiza, and Petrobras). Due to the flexibility that this methodology offers in the construction of multivariate distributions and risk aggregation in finance, we used the copula-APARCH approach with the Normal, T -student, and Joe-Clayton copula functions. In most scenarios, the results showed a pattern of dependence at the extremes. Moreover, the copula form seems not to be relevant for VaR estimation, since in most portfolios the appropriate copulas lead to significant VaR estimates. It has found that the best models fitted provided conservative risk measures, estimates at 5\% and 1\%, in a scenario more aggressive.},
  archive      = {J_JOAS},
  author       = {Marcela de Marillac Carvalho and Thelma Sáfadi},
  doi          = {10.1080/02664763.2020.1865883},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1598-1610},
  shortjournal = {J. Appl. Stat.},
  title        = {Risk analysis in the brazilian stock market: Copula-APARCH modeling for value-at-risk},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The process of transferring negative impulses in capital
markets – a wavelet analysis. <em>JOAS</em>, <em>49</em>(6), 1574–1597.
(<a href="https://doi.org/10.1080/02664763.2020.1864811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The empirical research that is presented herein deals with the process of transferring negative impulses in capital markets during the subprime crisis (contagion, comovements, crisis transmission and shocks). A significant and positive contribution of the research conducted is the demonstration of how the wavelet analysis can be used in examining the various responses of the financial markets. The first stage of the research involved an analysis of the response of seven European markets (CAC40, DAX, FTSE100, IBEX, ATHEX, BUX and WIG20 indexes) to the proceedings in the US market, exemplified by the Dow Jones Industrial Average Index. The second stage involved examining the relationships of strong European markets (CAC40, DAX, FTSE100), and then the impact that the strongest German market DAX had on four other and weaker European markets – two from Western Europe (IBEX, ATHEX) and two from Central-Eastern Europe (BUX and WIG20). This article presents a methodological approach to transfer impulses on capital markets.},
  archive      = {J_JOAS},
  author       = {Milda Maria Burzala},
  doi          = {10.1080/02664763.2020.1864811},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1574-1597},
  shortjournal = {J. Appl. Stat.},
  title        = {The process of transferring negative impulses in capital markets – a wavelet analysis},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Penalized likelihood approach for the four-parameter kappa
distribution. <em>JOAS</em>, <em>49</em>(6), 1559–1573. (<a
href="https://doi.org/10.1080/02664763.2021.1871592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The four-parameter kappa distribution (K4D) is a generalized form of some commonly used distributions such as generalized logistic, generalized Pareto, generalized Gumbel, and generalized extreme value (GEV) distributions. Owing to its flexibility, the K4D is widely applied in modeling in several fields such as hydrology and climatic change. For the estimation of the four parameters, the maximum likelihood approach and the method of L-moments are usually employed. The L-moment estimator (LME) method works well for some parameter spaces, with up to a moderate sample size, but it is sometimes not feasible in terms of computing the appropriate estimates. Meanwhile, using the maximum likelihood estimator (MLE) with small sample sizes shows substantially poor performance in terms of a large variance of the estimator. We therefore propose a maximum penalized likelihood estimation (MPLE) of K4D by adjusting the existing penalty functions that restrict the parameter space. Eighteen combinations of penalties for two shape parameters are considered and compared. The MPLE retains modeling flexibility and large sample optimality while also improving on small sample properties. The properties of the proposed estimator are verified through a Monte Carlo simulation, and an application case is demonstrated taking Thailand’s annual maximum temperature data.},
  archive      = {J_JOAS},
  author       = {Nipada Papukdee and Jeong-Soo Park and Piyapatr Busababodhin},
  doi          = {10.1080/02664763.2021.1871592},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1559-1573},
  shortjournal = {J. Appl. Stat.},
  title        = {Penalized likelihood approach for the four-parameter kappa distribution},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The optimized CUSUM and EWMA multi-charts for jointly
detecting a range of mean and variance change. <em>JOAS</em>,
<em>49</em>(6), 1540–1558. (<a
href="https://doi.org/10.1080/02664763.2020.1870670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the problem of jointly monitoring the mean and variance of a process by multi-chart schemes. Multi-chart is a combination of several single charts which detects changes in a process quickly. Asymptotic analyses and simulation studies show that the optimized CUSUM multi-chart has optimal performance than optimized EWMA multi-chart in jointly detecting mean and variance shifts in an i . i . d . i . i . d . i.i.d. normal observation. A real example that monitors the changes in IBM&#39;s stock returns (mean) and risks (variance) is used to demonstrate the performance of the above two multi-charts. The proposed method has been compared to a benchmark and it performed better.},
  archive      = {J_JOAS},
  author       = {Gideon Mensah Engmann and Dong Han},
  doi          = {10.1080/02664763.2020.1870670},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1540-1558},
  shortjournal = {J. Appl. Stat.},
  title        = {The optimized CUSUM and EWMA multi-charts for jointly detecting a range of mean and variance change},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Streaming constrained binary logistic regression with online
standardized data. <em>JOAS</em>, <em>49</em>(6), 1519–1539. (<a
href="https://doi.org/10.1080/02664763.2020.1870672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online learning is a method for analyzing very large datasets (‘big data’) as well as data streams. In this article, we consider the case of constrained binary logistic regression and show the interest of using processes with an online standardization of the data, in particular to avoid numerical explosions or to allow the use of shrinkage methods. We prove the almost sure convergence of such a process and propose using a piecewise constant step-size such that the latter does not decrease too quickly and does not reduce the speed of convergence. We compare twenty-four stochastic approximation processes with raw or online standardized data on five real or simulated data sets. Results show that, unlike processes with raw data, processes with online standardized data can prevent numerical explosions and yield the best results.},
  archive      = {J_JOAS},
  author       = {Benoît Lalloué and Jean-Marie Monnez and Eliane Albuisson},
  doi          = {10.1080/02664763.2020.1870672},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1519-1539},
  shortjournal = {J. Appl. Stat.},
  title        = {Streaming constrained binary logistic regression with online standardized data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A nonlinear measurement error model and its application to
describing the dependency of health outcomes on dietary intake.
<em>JOAS</em>, <em>49</em>(6), 1485–1518. (<a
href="https://doi.org/10.1080/02664763.2020.1870671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many nutritional studies focus on the relationship between individuals&#39; diets and resulting health outcomes. When examining these relationships, researchers are generally interested in individuals&#39; long-term, average intake of nutrients; however, typically only 1–2 days of data are collected. If analyses are performed without accounting for the error in estimating usual intake, estimates will be biased. In this work, we focus on situations where the association between intake and health outcomes is nonlinear. Since we can only obtain noisy measurements of intake, we propose implementing a nonlinear measurement error model which accounts for the nuisance day-to-day variance when estimating long-term average intake. Estimation of the model is performed using maximum likelihood. Properties of the estimators are explored for a model where we assume that the unobservable usual intake is normally distributed. We then propose an extended model where we no longer assume that the distribution for the unobservable predictor is normal, but is instead a finite mixture of discrete distributions. We finish with an application using data from the 2015–2016 National Health and Nutrition Examination Survey (NHANES) where we examine the association between potassium intake and systolic blood pressure.},
  archive      = {J_JOAS},
  author       = {B. Curley},
  doi          = {10.1080/02664763.2020.1870671},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1485-1518},
  shortjournal = {J. Appl. Stat.},
  title        = {A nonlinear measurement error model and its application to describing the dependency of health outcomes on dietary intake},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive kernel scaling support vector machine with
application to a prostate cancer image study. <em>JOAS</em>,
<em>49</em>(6), 1465–1484. (<a
href="https://doi.org/10.1080/02664763.2020.1870669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The support vector machine (SVM) is a popularly used classifier in applications such as pattern recognition, texture mining and image retrieval owing to its flexibility and interpretability. However, its performance deteriorates when the response classes are imbalanced. To enhance the performance of the support vector machine classifier in the imbalanced cases we investigate a new two stage method by adaptively scaling the kernel function. Based on the information obtained from the standard SVM in the first stage, we conformally rescale the kernel function in a data adaptive fashion in the second stage so that the separation between two classes can be effectively enlarged with incorporation of observation imbalance. The proposed method takes into account the location of the support vectors in the feature space, therefore is especially appealing when the response classes are imbalanced. The resulting algorithm can efficiently improve the classification accuracy, which is confirmed by intensive numerical studies as well as a real prostate cancer imaging data application.},
  archive      = {J_JOAS},
  author       = {Xin Liu and Wenqing He},
  doi          = {10.1080/02664763.2020.1870669},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1465-1484},
  shortjournal = {J. Appl. Stat.},
  title        = {Adaptive kernel scaling support vector machine with application to a prostate cancer image study},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Directional monitoring and diagnosis for covariance
matrices. <em>JOAS</em>, <em>49</em>(6), 1449–1464. (<a
href="https://doi.org/10.1080/02664763.2020.1867830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical surveillance for covariance matrices has attracted increasing attention recently. Many approaches have been developed for monitoring general shifts that are arbitrary deviations, as well as sparse shifts occurring in only a few elements. This paper considers directional shifts that occur in only one independent parameter, which is common if the process is relatively stable. A directional covariance matrix control chart is proposed, which fully exploits directional shift information and borrows the strong power of likelihood ratio test. Therefore, this chart provides a powerful tool for monitoring covariance matrices. In addition, the proposed chart does not require specifying the regularisation parameter, and it enjoys a concise quadratic form, thereby easy to implement. Furthermore, this chart naturally leads to a diagnostic prescription for identifying the shifting element in the covariance matrix. Simulation results have demonstrated the efficiency of the suggested control chart and its accompanying diagnostic scheme.},
  archive      = {J_JOAS},
  author       = {Hongying Jing and Jian Li and Kaizong Bai},
  doi          = {10.1080/02664763.2020.1867830},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1449-1464},
  shortjournal = {J. Appl. Stat.},
  title        = {Directional monitoring and diagnosis for covariance matrices},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian hierarchical models for linear networks.
<em>JOAS</em>, <em>49</em>(6), 1421–1448. (<a
href="https://doi.org/10.1080/02664763.2020.1864814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this study is to highlight dangerous motorways via estimating the intensity of accidents and study its pattern across the UK motorway network. Two methods have been developed to achieve this aim. First, the motorway-specific intensity is estimated by using a homogeneous Poisson process. The heterogeneity across motorways is incorporated using two-level hierarchical models. The data structure is multilevel since each motorway consists of junctions that are joined by grouped segments. In the second method, the segment-specific intensity is estimated. The homogeneous Poisson process is used to model accident data within grouped segments but heterogeneity across grouped segments is incorporated using three-level hierarchical models. A Bayesian method via Markov Chain Monte Carlo is used to estimate the unknown parameters in the models and the sensitivity to the choice of priors is assessed. The performance of the proposed models is evaluated by a simulation study and an application to traffic accidents in 2016 on the UK motorway network. The deviance information criterion (DIC) and the widely applicable information criterion (WAIC) are employed to choose between models.},
  archive      = {J_JOAS},
  author       = {Zainab Al-kaabawi and Yinghui Wei and Rana Moyeed},
  doi          = {10.1080/02664763.2020.1864814},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1421-1448},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian hierarchical models for linear networks},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analyzing partially paired data: When can the unpaired
portion(s) be safely ignored? <em>JOAS</em>, <em>49</em>(6), 1402–1420.
(<a href="https://doi.org/10.1080/02664763.2020.1864813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partially paired data, either with incompleteness in one or both arms, are common in practice. For testing equality of means of two arms, practitioners often use only the portion of data with complete pairs and perform paired tests. Although such tests (referred as ‘naive paired tests’) are legitimate, their powers might be low as only partial data are utilized. The recently proposed ‘ P -value pooling methods’, based on combining P -values from two tests, use all data, have reasonable type-I error control and good power property. While it is generally believed that ‘ P -value pooling methods’ are superior to ‘naive paired tests’ in terms of power as the former use more data than the latter, no detailed power comparison has been done. This paper aims to compare powers of ‘naive paired tests’ and ‘ P -value pooling methods’ analytically and our findings are counterintuitive, i.e. the ‘ P -value pooling methods’ do not always outperform the naive paired tests in terms of power. Based on these results, we present guidance on how to select the best test for testing equality of means with partially paired data.},
  archive      = {J_JOAS},
  author       = {Qianya Qi and Li Yan and Lili Tian},
  doi          = {10.1080/02664763.2020.1864813},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1402-1420},
  shortjournal = {J. Appl. Stat.},
  title        = {Analyzing partially paired data: When can the unpaired portion(s) be safely ignored?},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic bayesian adjustment of anticipatory covariates in
retrospective data: Application to the effect of education on divorce
risk. <em>JOAS</em>, <em>49</em>(6), 1382–1401. (<a
href="https://doi.org/10.1080/02664763.2020.1864812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address a problem in inference from retrospective studies where the value of a variable is measured at the date of the survey but is used as covariate to events that have occurred long before the survey. This causes problem because the value of the current-date (anticipatory) covariate does not follow the temporal order of events. We propose a dynamic Bayesian approach for modelling jointly the anticipatory covariate and the event of interest, and allowing the effects of the anticipatory covariate to vary over time. The issues are illustrated with data on the effects of education attained by the survey-time on divorce risks among Swedish men. The overall results show that failure to adjust for the anticipatory nature of education leads to elevated relative risks of divorce across educational levels. The results are partially in accordance with previous findings based on analyses of the same data set. More importantly, our findings provide new insights in that the bias due to anticipatory covariates varies over marriage duration.},
  archive      = {J_JOAS},
  author       = {Parfait Munezero and Gebrenegus Ghilagaber},
  doi          = {10.1080/02664763.2020.1864812},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1382-1401},
  shortjournal = {J. Appl. Stat.},
  title        = {Dynamic bayesian adjustment of anticipatory covariates in retrospective data: Application to the effect of education on divorce risk},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sequential asymmetric third order rotatable designs
(SATORDs). <em>JOAS</em>, <em>49</em>(6), 1364–1381. (<a
href="https://doi.org/10.1080/02664763.2020.1864817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rotatable designs that are available for process/ product optimization trials are mostly symmetric in nature. In many practical situations, response surface designs (RSDs) with mixed factor (unequal) levels are more suitable as these designs explore more regions in the design space but it is hard to get rotatable designs with a given level of asymmetry. When experimenting with unequal factor levels via asymmetric second order rotatable design (ASORDs), the lack of fit of the model may become significant which ultimately leads to the estimation of parameters based on a higher (or third) order model. Experimenting with a new third order rotatable design (TORD) in such a situation would be expensive as the responses observed from the first stage runs would be kept underutilized. In this paper, we propose a method of constructing asymmetric TORD by sequentially augmenting some additional points to the ASORDs without discarding the runs in the first stage. The proposed designs will be more economical to obtain the optimum response as the design in the first stage can be used to fit the second order model and with some additional runs, third order model can be fitted without discarding the initial design.},
  archive      = {J_JOAS},
  author       = {M. Hemavathi and Eldho Varghese and Shashi Shekhar and Seema Jaggi},
  doi          = {10.1080/02664763.2020.1864817},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1364-1381},
  shortjournal = {J. Appl. Stat.},
  title        = {Sequential asymmetric third order rotatable designs (SATORDs)},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High precision implementation of steck’s recursion method
for use in goodness-of-fit tests. <em>JOAS</em>, <em>49</em>(6),
1348–1363. (<a
href="https://doi.org/10.1080/02664763.2020.1861224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical continuous goodness-of-fit (GOF) testing is employed for examining whether the data come from an assumed parametric model. In many cases, GOF tests assume a uniform null distribution and examine extreme values of the order statistics of the samples. Many of these statistics can be expressed by a function of the order statistics and the p -values amount to a joint probability statement based on the uniform order statistics. In this paper, we utilize Steck&#39;s recursion method and propose two high precision computing algorithms to compute the p -values for these GOF statistics. The numerical difficulties in implementing Steck&#39;s method are discussed and compared with solutions provided in high precision libraries.},
  archive      = {J_JOAS},
  author       = {Jiefei Wang and Jeffrey C. Miecznikowski},
  doi          = {10.1080/02664763.2020.1861224},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1348-1363},
  shortjournal = {J. Appl. Stat.},
  title        = {High precision implementation of steck&#39;s recursion method for use in goodness-of-fit tests},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymmetric autoregressive models: Statistical aspects and a
financial application under COVID-19 pandemic. <em>JOAS</em>,
<em>49</em>(5), 1323–1347. (<a
href="https://doi.org/10.1080/02664763.2021.1913103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present study, we provide a motivating example with a financial application under COVID-19 pandemic to investigate autoregressive (AR) modeling and its diagnostics based on asymmetric distributions. The objectives of this work are: (i) to formulate asymmetric AR models and their estimation and diagnostics; (ii) to assess the performance of the parameters estimators and of the local influence technique for these models; and (iii) to provide a tool to show how data following an asymmetric distribution under an AR structure should be analyzed. We take the advantages of the stochastic representation of the skew-normal distribution to estimate the parameters of the corresponding AR model efficiently with the expectation-maximization algorithm. Diagnostic analytics are conducted by using the local influence technique with four perturbation schemes. By employing Monte Carlo simulations, we evaluate the statistical behavior of the corresponding estimators and of the local influence technique. An illustration with financial data updated until 2020, analyzed using the methodology introduced in the present work, is presented as an example of effective applications, from where it is possible to explain atypical cases from the COVID-19 pandemic.},
  archive      = {J_JOAS},
  author       = {Yonghui Liu and Chaoxuan Mao and Víctor Leiva and Shuangzhe Liu and Waldemiro A. Silva Neto},
  doi          = {10.1080/02664763.2021.1913103},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1323-1347},
  shortjournal = {J. Appl. Stat.},
  title        = {Asymmetric autoregressive models: Statistical aspects and a financial application under COVID-19 pandemic},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bayesian approach on the two-piece scale mixtures of
normal homoscedastic nonlinear regression models. <em>JOAS</em>,
<em>49</em>(5), 1305–1322. (<a
href="https://doi.org/10.1080/02664763.2020.1854203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this application note paper, we propose and examine the performance of a Bayesian approach for a homoscedastic nonlinear regression ( NLR ) model assuming errors with two-piece scale mixtures of normal ( TP - SMN ) distributions. The TP - SMN is a large family of distributions, covering both symmetrical/ asymmetrical distributions as well as light/heavy tailed distributions, and provides an alternative to another well-known family of distributions, called scale mixtures of skew-normal distributions. The proposed family and Bayesian approach provides considerable flexibility and advantages for NLR modelling in different practical settings. We examine the performance of the approach using simulated and real data.},
  archive      = {J_JOAS},
  author       = {Zahra Barkhordar and Mohsen Maleki and Zahra Khodadadi and Darren Wraith and Farajollah Negahdari},
  doi          = {10.1080/02664763.2020.1854203},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1305-1322},
  shortjournal = {J. Appl. Stat.},
  title        = {A bayesian approach on the two-piece scale mixtures of normal homoscedastic nonlinear regression models},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A review of tests for exponentiality with monte carlo
comparisons. <em>JOAS</em>, <em>49</em>(5), 1277–1304. (<a
href="https://doi.org/10.1080/02664763.2020.1854202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, 91 different tests for exponentiality are reviewed. Some of the tests are universally consistent while others are against some special classes of life distributions. Power performances of 40 of these different tests for exponentiality of datasets are compared through extensive Monte Carlo simulations. The comparisons are conducted for different sample sizes of 10, 25, 50 and 100 for different groups of distributions according to the shape of their hazard functions at 5 percent level of significance. Also, the techniques are applied to two real-world datasets and a measure of power is employed for the comparison of the tests. The results show that some tests which are very good under one group of alternative distributions are not so under another group. Also, some tests maintained relatively high power over all the groups of alternative distributions studied while some others maintained poor power performances over all the groups of alternative distributions. Again, the result obtained from real-world datasets agree completely with those of the simulation studies.},
  archive      = {J_JOAS},
  author       = {Everestus O. Ossai and Mbanefo S. Madukaife and Abimibola V. Oladugba},
  doi          = {10.1080/02664763.2020.1854202},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1277-1304},
  shortjournal = {J. Appl. Stat.},
  title        = {A review of tests for exponentiality with monte carlo comparisons},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bivariate birnbaum-saunders accelerated lifetime model:
Estimation and diagnostic analysis. <em>JOAS</em>, <em>49</em>(5),
1252–1276. (<a
href="https://doi.org/10.1080/02664763.2020.1859466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we discuss the bivariate Birnbaum-Saunders accelerated lifetime model, in which we have modeled the dependence structure of bivariate survival data through the use of frailty models. Specifically, we propose the bivariate model Birnbaum-Saunders with the following frailty distributions: gamma, positive stable and logarithmic series. We present a study of inference and diagnostic analysis for the proposed model, more concisely, are proposed a diagnostic analysis based in local influence and residual analysis to assess the fit model, as well as, to detect influential observations. In this regard, we derived the normal curvatures of local influence under different perturbation schemes and we performed some simulation studies for assessing the potential of residuals to detect misspecification in the systematic component, the presence in the stochastic component of the model and to detect outliers. Finally, we apply the methodology studied to real data set from recurrence in times of infections of 38 kidney patients using a portable dialysis machine, we analyzed these data considering independence within the pairs and using the bivariate Birnbaum-Saunders accelerated lifetime model, so that we could make a comparison and verify the importance of modeling dependence within the times of infection associated with the same patient.},
  archive      = {J_JOAS},
  author       = {Maria Ioneris Oliveira and Michelli Barros and Joelson Campos and Francisco José A. Cysneiros},
  doi          = {10.1080/02664763.2020.1859466},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1252-1276},
  shortjournal = {J. Appl. Stat.},
  title        = {Bivariate birnbaum-saunders accelerated lifetime model: Estimation and diagnostic analysis},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analyzing the impacts of socio-economic factors on french
departmental elections with CoDa methods. <em>JOAS</em>, <em>49</em>(5),
1235–1251. (<a
href="https://doi.org/10.1080/02664763.2020.1858274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vote shares by party on a given subdivision of a territory form a vector called composition (mathematically, a vector belonging to a simplex). It is interesting to model these shares and study the impact of the characteristics of the territorial units on the outcome of the elections. In the political economy literature, few regression models are adapted to the case of more than two political parties. In the statistical literature, there are regression models adapted to share vectors including Compositional Data (CoDa) models, but also Dirichlet models, and others. Our goal is to discuss and illustrate the use CoDa regression models for political economy models for more than two parties. The models are fitted on French electoral data of the 2015 departmental elections.},
  archive      = {J_JOAS},
  author       = {T. H. A. Nguyen and T. Laurent and C. Thomas-Agnan and A. Ruiz-Gazen},
  doi          = {10.1080/02664763.2020.1858274},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1235-1251},
  shortjournal = {J. Appl. Stat.},
  title        = {Analyzing the impacts of socio-economic factors on french departmental elections with CoDa methods},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MAP segmentation in bayesian hidden markov models: A case
study. <em>JOAS</em>, <em>49</em>(5), 1203–1234. (<a
href="https://doi.org/10.1080/02664763.2020.1858273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of estimating the maximum posterior probability (MAP) state sequence for a finite state and finite emission alphabet hidden Markov model (HMM) in the Bayesian setup, where both emission and transition matrices have Dirichlet priors. We study a training set consisting of thousands of protein alignment pairs. The training data is used to set the prior hyperparameters for Bayesian MAP segmentation. Since the Viterbi algorithm is not applicable any more, there is no simple procedure to find the MAP path, and several iterative algorithms are considered and compared. The main goal of the paper is to test the Bayesian setup against the frequentist one, where the parameters of HMM are estimated using the training data.},
  archive      = {J_JOAS},
  author       = {Alexey Koloydenko and Kristi Kuljus and Jüri Lember},
  doi          = {10.1080/02664763.2020.1858273},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1203-1234},
  shortjournal = {J. Appl. Stat.},
  title        = {MAP segmentation in bayesian hidden markov models: A case study},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust bootstrap prediction intervals for univariate and
multivariate autoregressive time series models. <em>JOAS</em>,
<em>49</em>(5), 1179–1202. (<a
href="https://doi.org/10.1080/02664763.2020.1856351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bootstrap procedure has emerged as a general framework to construct prediction intervals for future observations in autoregressive time series models. Such models with outlying data points are standard in real data applications, especially in the field of econometrics. These outlying data points tend to produce high forecast errors, which reduce the forecasting performances of the existing bootstrap prediction intervals calculated based on non-robust estimators. In the univariate and multivariate autoregressive time series, we propose a robust bootstrap algorithm for constructing prediction intervals and forecast regions. The proposed procedure is based on the weighted likelihood estimates and weighted residuals. Its finite sample properties are examined via a series of Monte Carlo studies and two empirical data examples.},
  archive      = {J_JOAS},
  author       = {Ufuk Beyaztas and Han Lin Shang},
  doi          = {10.1080/02664763.2020.1856351},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1179-1202},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust bootstrap prediction intervals for univariate and multivariate autoregressive time series models},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ultrahigh-dimensional sufficient dimension reduction for
censored data with measurement error in covariates. <em>JOAS</em>,
<em>49</em>(5), 1154–1178. (<a
href="https://doi.org/10.1080/02664763.2020.1856352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the ultrahigh-dimensional sufficient dimension reduction (SDR) for censored data and measurement error in covariates. We first propose the feature screening procedure based on censored data and the covariates subject to measurement error. With the suitable correction of mismeasurement, the error-contaminated variables detected by the proposed feature screening procedure are the same as the truly important variables. Based on the selected active variables, we develop the SDR method to estimate the central subspace and the structural dimension with both censored data and measurement error incorporated. The theoretical results of the proposed method are established. Simulation studies are reported to assess the performance of the proposed method. The proposed method is implemented to NKI breast cancer data.},
  archive      = {J_JOAS},
  author       = {Li-Pang Chen},
  doi          = {10.1080/02664763.2020.1856352},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1154-1178},
  shortjournal = {J. Appl. Stat.},
  title        = {Ultrahigh-dimensional sufficient dimension reduction for censored data with measurement error in covariates},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variational bayesian inference for association over
phylogenetic trees for microorganisms. <em>JOAS</em>, <em>49</em>(5),
1140–1153. (<a
href="https://doi.org/10.1080/02664763.2020.1854200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advance of next generation sequencing technologies, researchers now routinely obtain a collection of microbial sequences with complex phylogenetic relationships. It is often of interest to analyze the association between certain environmental factors and characteristics of the microbial collection. Though methods have been developed to test for association between the microbial composition with environmental factors as well as between coevolving traits, a flexible model that can provide a comprehensive picture of the relationship between microbial community characteristics and environmental variables will be tremendously beneficial. We developed a Bayesian approach for association analysis while incorporating the phylogenetic structure to account for the dependence between observations. To overcome the computational difficulty related to the phylogenetic tree, a variational algorithm was developed to evaluate the posterior distribution. As the posterior distribution can be readily obtained for parameters of interest and any derived variables, the association relationship can be examined comprehensively. With two application examples, we demonstrated that the Bayesian approach can uncover nuanced details of the microbial assemblage with regard to the environmental factor. The proposed Bayesian approach and variational algorithm can be extended for other problems involving dependence over tree-like structures.},
  archive      = {J_JOAS},
  author       = {Xiaojuan Hao and Kent M. Eskridge and Dong Wang},
  doi          = {10.1080/02664763.2020.1854200},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1140-1153},
  shortjournal = {J. Appl. Stat.},
  title        = {Variational bayesian inference for association over phylogenetic trees for microorganisms},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Testing and dating structural changes in copula-based
dependence measures. <em>JOAS</em>, <em>49</em>(5), 1121–1139. (<a
href="https://doi.org/10.1080/02664763.2020.1850655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with testing and dating structural breaks in the dependence structure of multivariate time series. We consider a cumulative sum (CUSUM) type test for constant copula-based dependence measures, such as Spearman&#39;s rank correlation and quantile dependencies. The asymptotic null distribution is not known in closed form and critical values are estimated by an i.i.d. bootstrap procedure. We analyze size and power properties in a simulation study under different dependence measure settings, such as skewed and fat-tailed distributions. To date breakpoints and to decide whether two estimated break locations belong to the same break event, we propose a pivot confidence interval procedure. Finally, we apply the test to the historical data of 10 large financial firms during the last financial crisis from 2002 to mid-2013.},
  archive      = {J_JOAS},
  author       = {Florian Stark and Sven Otto},
  doi          = {10.1080/02664763.2020.1850655},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1121-1139},
  shortjournal = {J. Appl. Stat.},
  title        = {Testing and dating structural changes in copula-based dependence measures},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Structured sparse support vector machine with ordered
features. <em>JOAS</em>, <em>49</em>(5), 1105–1120. (<a
href="https://doi.org/10.1080/02664763.2020.1849053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the application of high-dimensional data classification, several attempts have been made to achieve variable selection by replacing the ℓ 2 ℓ 2 ℓ2 -penalty with other penalties for the support vector machine (SVM). However, these high-dimensional SVM methods usually do not take into account the special structure among covariates (features). In this article, we consider a classification problem, where the covariates are ordered in some meaningful way, and the number of covariates p can be much larger than the sample size n . We propose a structured sparse SVM to tackle this type of problems, which combines the non-convex penalty and cubic spline estimation procedure (i.e. penalizing second-order derivatives of the coefficients) to the SVM. From a theoretical point of view, the proposed method satisfies the local oracle property. Simulations show that the method works effectively both in feature selection and classification accuracy. A real application is conducted to illustrate the benefits of the method.},
  archive      = {J_JOAS},
  author       = {Kuangnan Fang and Peng Wang and Xiaochen Zhang and Qingzhao Zhang},
  doi          = {10.1080/02664763.2020.1849053},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1105-1120},
  shortjournal = {J. Appl. Stat.},
  title        = {Structured sparse support vector machine with ordered features},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal designing of two-level skip-lot sampling
reinspection plan. <em>JOAS</em>, <em>49</em>(5), 1086–1104. (<a
href="https://doi.org/10.1080/02664763.2020.1849059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skip-lot sampling plan is often applied in industries for reducing the cost and effort of the inspection of the product having excellent quality history. Consequence of skip-lot sampling plans is to reduce the cost of inspection so which are more attractive in economical aspect. In this paper, we develop a sampling plan by incorporating the idea of resampling in two-level skip lot sampling plan and the new plan is designated as SkSP-2L.1-R. This paper presents the Markov chain formulation of the proposed plan along with the derivation of performance measures of the plan. We also provide the designing methodology to determine the optimal parameters of the SkSP-2L.1-R plan so as to minimize the average sample number by using two points on the operating characteristic curve approach. By contemplating various combinations of producer and consumer quality levels along with respective risks, a table is constructed to determine the optimal parameters. An industrial application of the proposed SkSP-2L.1-R plan is discussed. The SkSP-2L.1-R with single sampling plan as a reference plan is compared with the conventional single sampling plan, SkSP-2 plan and SkSP-2-R plan and proved that the proposed SkSP-2L.1-R plan outperforms these plans.},
  archive      = {J_JOAS},
  author       = {N. Murugeswari and P. Jeyadurga and S. Balamurali},
  doi          = {10.1080/02664763.2020.1849059},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1086-1104},
  shortjournal = {J. Appl. Stat.},
  title        = {Optimal designing of two-level skip-lot sampling reinspection plan},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantification of model risk that is caused by model
misspecification. <em>JOAS</em>, <em>49</em>(5), 1065–1085. (<a
href="https://doi.org/10.1080/02664763.2020.1849055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we suggest a technique to quantify model risk, particularly model misspecification for binary response regression problems found in financial risk management, such as in credit risk modelling. We choose the probability of default model as one instance of many other credit risk models that may be misspecified in a financial institution. By way of illustrating the model misspecification for probability of default, we carry out quantification of two specific statistical predictive response techniques, namely the binary logistic regression and complementary log–log. The maximum likelihood estimation technique is employed for parameter estimation. The statistical inference, precisely the goodness of fit and model performance measurements, are assessed. Using the simulation dataset and Taiwan credit card default dataset, our finding reveals that with the same sample size and very small simulation iterations, the two techniques produce similar goodness-of-fit results but completely different performance measures. However, when the iterations increase, the binary logistic regression technique for balanced dataset reveals prominent goodness of fit and performance measures as opposed to the complementary log–log technique for both simulated and real datasets.},
  archive      = {J_JOAS},
  author       = {M.B. Seitshiro and H.P. Mashele},
  doi          = {10.1080/02664763.2020.1849055},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1065-1085},
  shortjournal = {J. Appl. Stat.},
  title        = {Quantification of model risk that is caused by model misspecification},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stress–strength reliability estimation involving paired
observation with ties using bivariate exponentiated half-logistic model.
<em>JOAS</em>, <em>49</em>(5), 1049–1064. (<a
href="https://doi.org/10.1080/02664763.2020.1849054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the problem of maximum likelihood and Bayesian estimation of stress–strength reliability involving paired observation with ties using bivariate exponentiated half-logistic distribution. This problem is of importance because in some real applications the strength of the component is highly dependent on the stress experienced by it. A bivariate extension of exponentiated half-logistic is discussed and an expression for the stress–strength reliability is obtained. This model is also useful to analyse data having the unusual feature of having a number of pairs with tied scores, even when the scores are continuous. The maximum likelihood estimate and interval estimate of the stress–strength reliability has been developed. The Bayes estimates of the stress–strength reliability under squared error loss function are obtained using importance sampling technique. Simulation studies are conducted to evaluate the performance of maximum likelihood and Bayes estimates. Two real-life data sets are analysed to numerically illustrate the usefulness of the developed method.},
  archive      = {J_JOAS},
  author       = {Thomas Xavier and Joby K. Jose},
  doi          = {10.1080/02664763.2020.1849054},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1049-1064},
  shortjournal = {J. Appl. Stat.},
  title        = {Stress–strength reliability estimation involving paired observation with ties using bivariate exponentiated half-logistic model},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonparametric panel stationarity testing with an application
to crude oil production. <em>JOAS</em>, <em>49</em>(4), 1033–1048. (<a
href="https://doi.org/10.1080/02664763.2020.1846691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A nonparametric panel stationarity test is proposed which offers the advantage of not requiring prior specification of the trend function for each of the series in the panel. A bootstrap implementation of the test is outlined and its finite sample performance is analyzed via Monte Carlo simulations. An application is also included where the proposed test is used to analyze the stochastic properties of monthly crude oil production for a panel of 20 -both OPEC and non-OPEC- countries from 1973 to 2015. Our analysis detects strong evidence of non-stationarity, both globally and group-wise. Results have implications for the effectiveness of government intervention and stabilization policies.},
  archive      = {J_JOAS},
  author       = {María José Presno and Manuel Landajo and Paula Fernandez-Gonzalez},
  doi          = {10.1080/02664763.2020.1846691},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {1033-1048},
  shortjournal = {J. Appl. Stat.},
  title        = {Nonparametric panel stationarity testing with an application to crude oil production},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Migration and students’ performance: Detecting geographical
differences following a curves clustering approach. <em>JOAS</em>,
<em>49</em>(4), 1018–1032. (<a
href="https://doi.org/10.1080/02664763.2020.1845624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Students&#39; migration mobility is the new form of migration: students migrate to improve their skills and become more valued for the job market. The data regard the migration of Italian Bachelors who enrolled at Master Degree level, moving typically from poor to rich areas. This paper investigates the migration and other possible determinants on the Master Degree students&#39; performance. The Clustering of Effects approach for Quantile Regression Coefficients Modelling has been used to cluster the effects of some variables on the students&#39; performance for three Italian macro-areas. Results show evidence of similarity between Southern and Centre students, with respect to the Northern ones.},
  archive      = {J_JOAS},
  author       = {Giovanni Boscaino and Gianluca Sottile and Giada Adelfio},
  doi          = {10.1080/02664763.2020.1845624},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {1018-1032},
  shortjournal = {J. Appl. Stat.},
  title        = {Migration and students&#39; performance: Detecting geographical differences following a curves clustering approach},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A mixture model with poisson and zero-truncated poisson
components to analyze road traffic accidents in turkey. <em>JOAS</em>,
<em>49</em>(4), 1003–1017. (<a
href="https://doi.org/10.1080/02664763.2020.1843610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of traffic accident data is crucial to address numerous concerns, such as understanding contributing factors in an accident&#39;s chain-of-events, identifying hotspots, and informing policy decisions about road safety management. The majority of statistical models employed for analyzing traffic accident data are logically count regression models (commonly Poisson regression) since a count – like the number of accidents – is used as the response. However, features of the observed data frequently do not make the Poisson distribution a tenable assumption. For example, observed data rarely demonstrate an equal mean and variance and often times possess excess zeros. Sometimes, data may have heterogeneous structure consisting of a mixture of populations, rather than a single population. In such data analyses, mixtures-of-Poisson-regression models can be used. In this study, the number of injuries resulting from casualties of traffic accidents registered by the General Directorate of Security (Turkey, 2005–2014) are modeled using a novel mixture distribution with two components: a Poisson and zero-truncated-Poisson distribution. Such a model differs from existing mixture models in literature where the components are either all Poisson distributions or all zero-truncated Poisson distributions. The proposed model is compared with the Poisson regression model via simulation and in the analysis of the traffic data.},
  archive      = {J_JOAS},
  author       = {Hande Konşuk Ünlü and Derek S. Young and Ayten Yiğiter and L. Hilal Özcebe},
  doi          = {10.1080/02664763.2020.1843610},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {1003-1017},
  shortjournal = {J. Appl. Stat.},
  title        = {A mixture model with poisson and zero-truncated poisson components to analyze road traffic accidents in turkey},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Revisit to functional data analysis of sleeping energy
expenditure. <em>JOAS</em>, <em>49</em>(4), 988–1002. (<a
href="https://doi.org/10.1080/02664763.2020.1838457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the classification problem of functional data including the sleeping energy expenditure (SEE) data, focusing on functional classification. Many existing classification rules are not effective in distinguishing the two classes of SEE data, because the trajectories of each observation have very different patterns for each class. It is often observed that some aspect of data such as the variability of paths is helpful in classification of functional data. To reflect this issue, we introduce a variable measuring the length of path in functional data and then propose a logistic model with fused lasso that considers the behavior of fluctuation of path as well as local correlations within each path. Our proposed model shows a significant improvement over some models used in the existing literature on the classification accuracy rate of functional data such as SEE data. We carry out simulation studies to show the finite sample performance and the gain that it makes in comparison with fused lasso without considering path length. With two more real datasets studied in some existing literature, we demonstrate that the new model achieves better or similar accuracy rate than the best accuracy rates reported in those studies.},
  archive      = {J_JOAS},
  author       = {Seungchul Baek and Yewon Kim and Junyong Park and Jong Soo Lee},
  doi          = {10.1080/02664763.2020.1838457},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {988-1002},
  shortjournal = {J. Appl. Stat.},
  title        = {Revisit to functional data analysis of sleeping energy expenditure},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal partitioning for the proportional hazards model.
<em>JOAS</em>, <em>49</em>(4), 968–987. (<a
href="https://doi.org/10.1080/02664763.2020.1846690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses methods for clustering a continuous covariate in a survival analysis model. The advantages of using a categorical covariate defined from discretizing a continuous covariate (via clustering) is (i) enhanced interpretability of the covariate&#39;s impact on survival and (ii) relaxing model assumptions that are usually required for survival models, such as the proportional hazards model. Simulations and an example are provided to illustrate the methods.},
  archive      = {J_JOAS},
  author       = {Usha Govindarajulu and Thaddeus Tarpey},
  doi          = {10.1080/02664763.2020.1846690},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {968-987},
  shortjournal = {J. Appl. Stat.},
  title        = {Optimal partitioning for the proportional hazards model},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian analyses of an exponential-poisson and related zero
augmented type models. <em>JOAS</em>, <em>49</em>(4), 949–967. (<a
href="https://doi.org/10.1080/02664763.2020.1846692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider several alternatives to the continuous exponential-Poisson distribution in order to accommodate the occurrence of zeros. Three of these are modifications of the exponential-Poisson model. One of these remains a fully continuous model. The other models we consider are all semi-continuous models, each with a discrete point mass at zero and a continuous density on the positive values. All of the models are applied to two environmental data sets concerning precipitation, and their Bayesian analyses using MCMC are discussed. This discussion covers convergence of the MCMC simulations and model selection procedures and considerations.},
  archive      = {J_JOAS},
  author       = {David P. M. Scollnik},
  doi          = {10.1080/02664763.2020.1846692},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {949-967},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian analyses of an exponential-poisson and related zero augmented type models},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian inference: Weibull poisson model for censored data
using the expectation–maximization algorithm and its application to
bladder cancer data. <em>JOAS</em>, <em>49</em>(4), 926–948. (<a
href="https://doi.org/10.1080/02664763.2020.1845626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the parameter estimation of experimental items/units from Weibull Poisson Model under progressive type-II censoring with binomial removals (PT-II CBRs). The expectation–maximization algorithm has been used for maximum likelihood estimators (MLEs). The MLEs and Bayes estimators have been obtained under symmetric and asymmetric loss functions. Performance of competitive estimators have been studied through their simulated risks. One sample Bayes prediction and expected experiment time have also been studied. Furthermore, through real bladder cancer data set, suitability of considered model and proposed methodology have been illustrated.},
  archive      = {J_JOAS},
  author       = {Anurag Pathak and Manoj Kumar and Sanjay Kumar Singh and Umesh Singh},
  doi          = {10.1080/02664763.2020.1845626},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {926-948},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian inference: Weibull poisson model for censored data using the expectation–maximization algorithm and its application to bladder cancer data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust estimation of models for longitudinal data with
dropouts and outliers. <em>JOAS</em>, <em>49</em>(4), 902–925. (<a
href="https://doi.org/10.1080/02664763.2020.1845623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data and outliers usually arise in longitudinal studies. Ignoring the effects of missing data and outliers will make the classical generalized estimating equation approach invalid. The longitudinal cohort study of rheumatoid arthritis patients was designed to investigate whether the Health Assessment Questionnaire score was associated with baseline covariates and changed with time. There exist dropouts and outliers in the data. In order to analyze the data, we develop a robust estimating equation approach. To deal with the responses missing at random, we extend a doubly robust method. To achieve robustness against outliers, we utilize an outlier robust method, which corrects the bias induced by outliers through centralizing the covariate matrix in the estimating equation. The doubly robust method for dropouts is easy to combine with the outlier robust method. The proposed method has the property of robustness in the sense that the proposed estimator is not only doubly robust against model misspecification for dropouts when there is no outlier in the data, but also robust against outliers. Consistency and asymptotic normality of the proposed estimator are established under regularity conditions. A comprehensive simulation study and real data analysis demonstrate that the proposed estimator does have the property of robustness.},
  archive      = {J_JOAS},
  author       = {Yuexia Zhang and Guoyou Qin and Zhongyi Zhu and Bo Fu},
  doi          = {10.1080/02664763.2020.1845623},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {902-925},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust estimation of models for longitudinal data with dropouts and outliers},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A three-part regression calibration to handle excess zeroes,
skewness and heteroscedasticity in adjusting for measurement error in
dietary intake data. <em>JOAS</em>, <em>49</em>(4), 884–901. (<a
href="https://doi.org/10.1080/02664763.2020.1845622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exposure measurement error (ME) biases exposure-outcome associations. Calibration dietary intake data used in the regression calibration (RC) response to adjust for ME are usually right-skewed, heteroscedastic and with excess zeroes. We proposed three-part RC models to handle these distributional complexities simultaneously, while correcting for ME in fish intake. We applied data from the National Health and Nutrition Examination Survey (NHANES), where long-term intake was measured with food frequency questionnaire (FFQ) in the main study and short-term intake with 24-hour recall (24HR) in the calibration study. In the three-part RC models, never consumers were modelled using two approaches: a zero distribution (Three-part RC-het-det), and logistic distribution (Three-part RC-het-prob); heteroscedasticity using an exponential distribution and right-skewness using generalized gamma distribution. The proposed models were compared with two-part RC model that ignores never consumers, and with methods that estimate intakes using FFQ and 24HR. The models were evaluated in a simulation study. With NHANES data, mean increase in the mercury level (in μ g / L ) was 1.20 using FFQ-method, 0.4 using 24HR-method, 1.87 using two-part RC and 2.02 using three-part RC-het-prob method. The three-part RC estimated the association with the least bias in the simulation study.},
  archive      = {J_JOAS},
  author       = {George O. Agogo and Alexander K. Muoka},
  doi          = {10.1080/02664763.2020.1845622},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {884-901},
  shortjournal = {J. Appl. Stat.},
  title        = {A three-part regression calibration to handle excess zeroes, skewness and heteroscedasticity in adjusting for measurement error in dietary intake data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical modeling of computer malware propagation
dynamics in cyberspace. <em>JOAS</em>, <em>49</em>(4), 858–883. (<a
href="https://doi.org/10.1080/02664763.2020.1845621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling cyber threats, such as the computer malicious software (malware) propagation dynamics in cyberspace, is an important research problem because models can deepen our understanding of dynamical cyber threats. In this paper, we study the statistical modeling of the macro-level evolution of dynamical cyber attacks. Specifically, we propose a Bayesian structural time series approach for modeling the computer malware propagation dynamics in cyberspace. Our model not only possesses the parsimony property (i.e. using few model parameters) but also can provide the predictive distribution of the dynamics by accommodating uncertainty. Our simulation study shows that the proposed model can fit and predict the computer malware propagation dynamics accurately, without requiring to know the information about the underlying attack-defense interaction mechanism and the underlying network topology. We use the model to study the propagation of two particular kinds of computer malware, namely the Conficker and Code Red worms, and show that our model has very satisfactory fitting and prediction accuracies.},
  archive      = {J_JOAS},
  author       = {Zijian Fang and Peng Zhao and Maochao Xu and Shouhuai Xu and Taizhong Hu and Xing Fang},
  doi          = {10.1080/02664763.2020.1845621},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {858-883},
  shortjournal = {J. Appl. Stat.},
  title        = {Statistical modeling of computer malware propagation dynamics in cyberspace},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling the effects of multiple exposures with unknown
group memberships: A bayesian latent variable approach. <em>JOAS</em>,
<em>49</em>(4), 831–857. (<a
href="https://doi.org/10.1080/02664763.2020.1843611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Bayesian latent variable model to allow estimation of the covariate-adjusted relationships between an outcome and a small number of latent exposure variables, using data from multiple observed exposures. Each latent variable is assumed to be represented by multiple exposures, where membership of the observed exposures to latent groups is unknown. Our model assumes that one measured exposure variable can be considered as a sentinel marker for each latent variable, while membership of the other measured exposures is estimated using MCMC sampling based on a classical measurement error model framework. We illustrate our model using data on multiple cytokines and birth weight from the Seychelles Child Development Study, and evaluate the performance of our model in a simulation study. Classification of cytokines into Th1 and Th2 cytokine classes in the Seychelles study revealed some differences from standard Th1/Th2 classifications. In simulations, our model correctly classified measured exposures into latent groups, and estimated model parameters with little bias and with coverage that was similar to the oracle model.},
  archive      = {J_JOAS},
  author       = {Alexis Zavez and Emeir M. McSorley and Alison J. Yeates and Sally W. Thurston},
  doi          = {10.1080/02664763.2020.1843611},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {831-857},
  shortjournal = {J. Appl. Stat.},
  title        = {Modeling the effects of multiple exposures with unknown group memberships: A bayesian latent variable approach},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the rank-deficient canonical correlation technique solved
by analytic spectral decomposition. <em>JOAS</em>, <em>49</em>(4),
819–830. (<a
href="https://doi.org/10.1080/02664763.2020.1843608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regularization is a well-known and used statistical approach covering individual points or limit approximations. In this study, the canonical correlation analysis (CCA) process of the paths is discussed with partial least squares (PLS) as the other boundary covering transformation to a symmetric eigenvalue (or singular value) problem dependent on a parameter. Two regularizations of the original criterion in the parameterization domain are compared, i.e. using projection and by identity matrix. We discuss the existence and uniqueness of the analytic path for eigenvalues and corresponding elements of eigenvectors. Specifically, canonical analysis is applied to an ill-conditioned case of singular within-sets input matrices encompassing tourism accommodation data.},
  archive      = {J_JOAS},
  author       = {Lukáš Malec},
  doi          = {10.1080/02664763.2020.1843608},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {819-830},
  shortjournal = {J. Appl. Stat.},
  title        = {On the rank-deficient canonical correlation technique solved by analytic spectral decomposition},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient estimators with categorical ranked set samples:
Estimation procedures for osteoporosis. <em>JOAS</em>, <em>49</em>(4),
803–818. (<a
href="https://doi.org/10.1080/02664763.2020.1841742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ranked set sampling (RSS) design as a cost-effective sampling is a powerful tool in situations where measuring the variable of interest is costly and time-consuming; however, ranking information about sampling units can be obtained easily through inexpensive and easy to measure characteristics at little or no cost. In this paper, we study RSS data for analysis of an ordinal population. First, we compare the problem of non-representative extreme samples under RSS and commonly-used simple random sampling. Using RSS data with tie information, we propose non-parametric and maximum likelihood estimators for population parameters. Through extensive numerical studies, we investigate the effect of various factors including ranking ability, tie generating mechanisms, the number of categories and population setting on the performance of the estimators. Finally, we apply the proposed methods to the bone disorder data to estimate the proportions of patients with osteopenia and osteoporosis status.},
  archive      = {J_JOAS},
  author       = {Armin Hatefi and Amirhossein Alvandi},
  doi          = {10.1080/02664763.2020.1841742},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {803-818},
  shortjournal = {J. Appl. Stat.},
  title        = {Efficient estimators with categorical ranked set samples: Estimation procedures for osteoporosis},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Testing exponentiality based on the extropy of record
values. <em>JOAS</em>, <em>49</em>(4), 782–802. (<a
href="https://doi.org/10.1080/02664763.2020.1840535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we first present a characterization of exponential distribution based on the extropy of record values and next introduce a goodness-of-fit test for exponentiality. Monte Carlo simulation is used to compute the critical values of our proposed test for different sample sizes and significance levels. To show the advantage of the proposed test, we adopt 58 competitor tests and compute the adjusted power against different alternatives with distinct types of hazard function. The power results show that our proposed test has superior adjusted power if the alternatives have increasing failure rates or bathtub decreasing-increasing failure rates, especially when the sample size is small. Finally, three real examples are used to illustrate the applicability and robustness of our proposed test by monitoring the p -values of the tests.},
  archive      = {J_JOAS},
  author       = {Peihan Xiong and Weiwei Zhuang and Guoxin Qiu},
  doi          = {10.1080/02664763.2020.1840535},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {782-802},
  shortjournal = {J. Appl. Stat.},
  title        = {Testing exponentiality based on the extropy of record values},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A statistical methodology to select covariates in
high-dimensional data under dependence. Application to the
classification of genetic profiles in oncology. <em>JOAS</em>,
<em>49</em>(3), 764–781. (<a
href="https://doi.org/10.1080/02664763.2020.1837083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new methodology for selecting and ranking covariates associated with a variable of interest in a context of high-dimensional data under dependence but few observations. The methodology successively intertwines the clustering of covariates, decorrelation of covariates using Factor Latent Analysis, selection using aggregation of adapted methods and finally ranking. A simulation study shows the interest of the decorrelation inside the different clusters of covariates. We first apply our method to transcriptomic data of 37 patients with advanced non-small-cell lung cancer who have received chemotherapy, to select the transcriptomic covariates that explain the survival outcome of the treatment. Secondly, we apply our method to 79 breast tumor samples to define patient profiles for a new metastatic biomarker and associated gene network in order to personalize the treatments.},
  archive      = {J_JOAS},
  author       = {B. Bastien and T. Boukhobza and H. Dumond and A. Gégout-Petit and A. Muller-Gueudin and C. Thiébaut},
  doi          = {10.1080/02664763.2020.1837083},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {764-781},
  shortjournal = {J. Appl. Stat.},
  title        = {A statistical methodology to select covariates in high-dimensional data under dependence. application to the classification of genetic profiles in oncology},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The GLM framework of the lee–carter model: A multi-country
study. <em>JOAS</em>, <em>49</em>(3), 752–763. (<a
href="https://doi.org/10.1080/02664763.2020.1833183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Lee–Carter model is a well-known model in modeling mortality. We aim to compare three probability models (Poisson, negative binomial and binomial) based on the Generalized Linear Model (GLM) framework of the Lee–Carter model. These models are applied to mortality data for 10 selected countries (Japan, United States, United Kingdom, Australia, Sweden, Spain, Belgium, Canada, Netherlands and Bulgaria) and the fit of these models is assessed using the deviance statistics and standardized residuals against fitted value plot. Among these three models, the negative binomial Lee–Carter model gave the best fit based on the deviance statistics and estimates of the log of deaths.},
  archive      = {J_JOAS},
  author       = {Shafiqah Azman and Dharini Pathmanathan},
  doi          = {10.1080/02664763.2020.1833183},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {752-763},
  shortjournal = {J. Appl. Stat.},
  title        = {The GLM framework of the Lee–Carter model: A multi-country study},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regional source apportionment of PM2.5 in seoul using
bayesian multivariate receptor model. <em>JOAS</em>, <em>49</em>(3),
738–751. (<a
href="https://doi.org/10.1080/02664763.2020.1822305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seoul, the capital city of Korea with over 10 million residents, has been experiencing serious air pollution problems. Previous studies on source apportionment of PM2.5 in Seoul are based on measurements of chemical compositions of PM2.5 from a single monitoring site. In this paper, we analyse PM2.5 concentration data collected from multiple sites in 24 districts of Seoul and estimate regional source profiles using Bayesian multivariate receptor model. The regional source profiles provide information for the identification of major PM2.5 sources as well as the regions relatively more seriously affected by each source than other regions. These regional characteristics relevant to PM2.5 can help establish effective, customised, region-specific PM2.5 control strategies for each region rather than general strategies that apply to every region of Seoul.},
  archive      = {J_JOAS},
  author       = {Man-Suk Oh and Chee Kyung Park},
  doi          = {10.1080/02664763.2020.1822305},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {738-751},
  shortjournal = {J. Appl. Stat.},
  title        = {Regional source apportionment of PM2.5 in seoul using bayesian multivariate receptor model},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new selection criterion for statistical home range
estimation. <em>JOAS</em>, <em>49</em>(3), 722–737. (<a
href="https://doi.org/10.1080/02664763.2020.1822302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The home range of an animal describes the geographic area where this individual spends most of the time while doing its usual activities. From a statistical viewpoint, the problem of home range estimation can be considered as a set estimation one. In the ecological literature, there are a variety of home range estimators. We address the open question of choosing the ‘best’ home range from a collection of them constructed on the same sample. We introduce the penalized overestimation ratio, a numerical index to rank the estimated home ranges. The key idea is to balance the excess area covered by the estimator (with respect to the sample) and a shape descriptor measuring the over-adjustment of the home range to the data. To our knowledge, apart from computing the home range area, our ranking procedure is the first one both applicable to real data and to any type of home range estimator. Further, optimization of the selection index provides a way to select the tuning parameters of nonparametric home ranges. For illustration purposes, we apply our selection proposal to a dataset of a Mongolian wolf and we carry out a simulation study.},
  archive      = {J_JOAS},
  author       = {A. Baíllo and J. E. Chacón},
  doi          = {10.1080/02664763.2020.1822302},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {722-737},
  shortjournal = {J. Appl. Stat.},
  title        = {A new selection criterion for statistical home range estimation},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On a generalization of the test of endogeneity in a two
stage least squares estimation. <em>JOAS</em>, <em>49</em>(3), 709–721.
(<a href="https://doi.org/10.1080/02664763.2020.1837084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In situations that the predictors are correlated with the error term, we propose a bridge estimator in the two-stage least squares estimation. We apply this estimator to overcome the multicollinearity and sparsity of the explanatory variables, when the endogeneity problem is present.The proposed estimator was applied to modify the Durbin-Wu-Hausman (DWH) test of endogeneity in the presence of multicollinearity. To compare our modified test with the existing DWH for detection of an endogenous problem in multi-collinear data, some numerical assessments are carried out. The numerical results showed that the proposed estimators and the suggested test perform better for the multi-collinear data. Finally, a genetical data set is applied for illustration the our results by estimating the coefficients parameters in the presence of endogeneity and multicollinearity.},
  archive      = {J_JOAS},
  author       = {Ayyub Sheikhi and Fatemeh Bahador and Mohammad Arashi},
  doi          = {10.1080/02664763.2020.1837084},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {709-721},
  shortjournal = {J. Appl. Stat.},
  title        = {On a generalization of the test of endogeneity in a two stage least squares estimation},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A proportional-hazards model for survival analysis and
long-term survivors modeling: Application to amyotrophic lateral
sclerosis data. <em>JOAS</em>, <em>49</em>(3), 694–708. (<a
href="https://doi.org/10.1080/02664763.2020.1830954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of survival data are affected by explanatory variables. We develop a new regression model for survival data analysis. As an alternative to standard mixture models, another model is proposed to describe the eventual presence of a surviving fraction. The proposed models are based on the Marshall–Olkin extended generalized Gompertz distribution. A maximum-likelihood inference is presented in the presence of covariates and a censorship phenomenon. Explanatory variables are incorporated into the model through proportional-hazards to evaluate the effect of risk factors on overall survival under different assumptions. Parametric, semi-parametric, and non-parametric methods are applied to survival analysis of patients treated for amyotrophic lateral sclerosis. Interesting results about riluzole use and other treatment effects on patients&#39; survival have been obtained.},
  archive      = {J_JOAS},
  author       = {Tasnime Hamdeni and Soufiane Gasmi},
  doi          = {10.1080/02664763.2020.1830954},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {694-708},
  shortjournal = {J. Appl. Stat.},
  title        = {A proportional-hazards model for survival analysis and long-term survivors modeling: Application to amyotrophic lateral sclerosis data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian inference in based-kernel regression: Comparison of
count data of condition factor of fish in pond systems. <em>JOAS</em>,
<em>49</em>(3), 676–693. (<a
href="https://doi.org/10.1080/02664763.2020.1830953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discrete kernel-based regression approach generally provides pointwise estimates of count data that do not account for uncertainty about both parameters and resulting estimates. This work aims to provide probabilistic kernel estimates of count regression function by using Bayesian approach and then allows for a readily quantification of uncertainty. Bayesian approach enables to incorporate prior knowledge of parameters used in discrete kernel-based regression. An application was proposed on count data of condition factor of fish ( K ) provided from an experimental project that analyzed various pond management strategies. The probabilistic distribution of estimates were contrasted by discrete kernels, as a support to theoretical results on the performance of kernels. More practically, Bayesian credibility intervals of K -estimates were evaluated to compare pond management strategies. Thus, similarities were found between performances of semi-intensive and coupled fishponds, with formulated feed, in comparison with extensive fishponds, without formulated feed. In particular, the fish development was less predictable in extensive fishpond, dependent on natural resources, than in the two other fishponds, supplied in formulated feed.},
  archive      = {J_JOAS},
  author       = {T. Senga Kiessé and Etienne Rivot and Christophe Jaeger and Joël Aubin},
  doi          = {10.1080/02664763.2020.1830953},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {676-693},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian inference in based-kernel regression: Comparison of count data of condition factor of fish in pond systems},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Survival analysis for the inverse gaussian distribution with
the gibbs sampler. <em>JOAS</em>, <em>49</em>(3), 656–675. (<a
href="https://doi.org/10.1080/02664763.2020.1828314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a comprehensive survival analysis for the inverse Gaussian distribution employing Bayesian and Fiducial approaches. It focuses on making inferences on the inverse Gaussian (IG) parameters μ and λ and the average remaining time of censored units. A flexible Gibbs sampling approach applicable in the presence of censoring is discussed and illustrations with Type II, progressive Type II, and random rightly censored observations are included. The analyses are performed using both simulated IG data and empirical data examples. Further, the bootstrap comparisons are made between the Bayesian and Fiducial estimates. It is concluded that the shape parameter ( ϕ = λ / μ ) of the inverse Gaussian distribution has the most impact on the two analyses, Bayesian vs. Fiducial, and so does the size of censoring in data to a lesser extent. Overall, both these approaches are effective in estimating IG parameters and the average remaining lifetime. The suggested Gibbs sampler allowed a great deal of flexibility in implementation for all types of censoring considered.},
  archive      = {J_JOAS},
  author       = {Kalanka. P. Jayalath and Raj S. Chhikara},
  doi          = {10.1080/02664763.2020.1828314},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {656-675},
  shortjournal = {J. Appl. Stat.},
  title        = {Survival analysis for the inverse gaussian distribution with the gibbs sampler},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bayesian shared parameter model for joint modeling of
longitudinal continuous and binary outcomes. <em>JOAS</em>,
<em>49</em>(3), 638–655. (<a
href="https://doi.org/10.1080/02664763.2020.1822303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint modeling of associated mixed biomarkers in longitudinal studies leads to a better clinical decision by improving the efficiency of parameter estimates. In many clinical studies, the observed time for two biomarkers may not be equivalent and one of the longitudinal responses may have recorded in a longer time than the other one. In addition, the response variables may have different missing patterns. In this paper, we propose a new joint model of associated continuous and binary responses by accounting different missing patterns for two longitudinal outcomes. A conditional model for joint modeling of the two responses is used and two shared random effects models are considered for intermittent missingness of two responses. A Bayesian approach using Markov Chain Monte Carlo (MCMC) is adopted for parameter estimation and model implementation. The validation and performance of the proposed model are investigated using some simulation studies. The proposed model is also applied for analyzing a real data set of bariatric surgery.},
  archive      = {J_JOAS},
  author       = {T. Baghfalaki and M. Ganjali and A. Kabir and A. Pazouki},
  doi          = {10.1080/02664763.2020.1822303},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {638-655},
  shortjournal = {J. Appl. Stat.},
  title        = {A bayesian shared parameter model for joint modeling of longitudinal continuous and binary outcomes},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A non-parametric hawkes model of the spread of ebola in west
africa. <em>JOAS</em>, <em>49</em>(3), 621–637. (<a
href="https://doi.org/10.1080/02664763.2020.1825646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently developed methods for the non-parametric estimation of Hawkes point process models facilitate their application for describing and forecasting the spread of epidemic diseases. We use data from the 2014 Ebola outbreak in West Africa to evaluate how well a simple Hawkes point process model can forecast the spread of Ebola virus in Guinea, Sierra Leone, and Liberia. For comparison, SEIR models that fit previously to the same data are evaluated using identical metrics. To test the predictive power of each of the models, we simulate the ability to make near real-time predictions during an actual outbreak by using the first 75\% of the data for estimation and the subsequent 25\% of the data for evaluation. Forecasts generated from Hawkes models more accurately describe the spread of Ebola in each of the three countries investigated and result in a 38\% reduction in RMSE for weekly case estimation across all countries when compared to SEIR models (total RMSE of 59.8 cases/week using SEIR compared to 37.1 for Hawkes). We demonstrate that the improved fit from Hawkes modeling cannot be attributed to overfitting and evaluate the advantages and disadvantages of Hawkes models in general for forecasting the spread of epidemic diseases.},
  archive      = {J_JOAS},
  author       = {Junhyung Park and Adam W. Chaffee and Ryan J. Harrigan and Frederic Paik Schoenberg},
  doi          = {10.1080/02664763.2020.1825646},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {621-637},
  shortjournal = {J. Appl. Stat.},
  title        = {A non-parametric hawkes model of the spread of ebola in west africa},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Goodness-of-fit test for exponentiality based on spacings
for general progressive type-II censored data. <em>JOAS</em>,
<em>49</em>(3), 599–620. (<a
href="https://doi.org/10.1080/02664763.2020.1821613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There have been numerous tests proposed to determine whether or not the exponential model is suitable for a given data set. In this article, we propose a new test statistic based on spacings to test whether the general progressive Type-II censored samples are from exponential distribution. The null distribution of the test statistic is discussed and it could be approximated by the standard normal distribution. Meanwhile, we propose an approximate method for calculating the expectation and variance of samples under null hypothesis and corresponding power function is also given. Then, a simulation study is conducted. We calculate the approximation of the power based on normality and compare the results with those obtained by Monte Carlo simulation under different alternatives with distinct types of hazard function. Results of simulation study disclose that the power properties of this statistic by using Monte Carlo simulation are better for the alternatives with monotone increasing hazard function, and otherwise, normal approximation simulation results are relatively better. Finally, two illustrative examples are presented.},
  archive      = {J_JOAS},
  author       = {Xinyan Qin and Jiao Yu and Wenhao Gui},
  doi          = {10.1080/02664763.2020.1821613},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {599-620},
  shortjournal = {J. Appl. Stat.},
  title        = {Goodness-of-fit test for exponentiality based on spacings for general progressive type-II censored data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regularized robust estimation in binary regression models.
<em>JOAS</em>, <em>49</em>(3), 574–598. (<a
href="https://doi.org/10.1080/02664763.2020.1822304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate robust parameter estimation and variable selection for binary regression models with grouped data . We investigate estimation procedures based on the minimum-distance approach. In particular, we employ minimum Hellinger and minimum symmetric chi-squared distances criteria and propose regularized minimum-distance estimators. These estimators appear to possess a certain degree of automatic robustness against model misspecification and/or for potential outliers. We show that the proposed non-penalized and penalized minimum-distance estimators are efficient under the model and simultaneously have excellent robustness properties. We study their asymptotic properties such as consistency, asymptotic normality and oracle properties. Using Monte Carlo studies, we examine the small-sample and robustness properties of the proposed estimators and compare them with traditional likelihood estimators. We also study two real-data applications to illustrate our methods. The numerical studies indicate the satisfactory finite-sample performance of our procedures.},
  archive      = {J_JOAS},
  author       = {Qingguo Tang and Rohana J. Karunamuni and Boxiao Liu},
  doi          = {10.1080/02664763.2020.1822304},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {574-598},
  shortjournal = {J. Appl. Stat.},
  title        = {Regularized robust estimation in binary regression models},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EWMA control charts for monitoring correlated counts with
finite range. <em>JOAS</em>, <em>49</em>(3), 553–573. (<a
href="https://doi.org/10.1080/02664763.2020.1820959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we develop and study upper and lower one-sided EWMA control charts for monitoring correlated counts with finite range. Often in practice, data of that kind can be adequately described by a first-order binomial or beta-binomial autoregressive model. Especially, when there is evidence that data demonstrate extra-binomial variation, the latter model is preferable than the former. The proposed charts can be used for detecting upward or downward shifts in process mean level. Practical guidelines concerning the statistical design of the proposed charts are given, while the effect of the extra-binomial variation is investigated as well. Comparisons with existing control charting procedures are also provided. Finally, an illustrative real-data example is also given.},
  archive      = {J_JOAS},
  author       = {Maria Anastasopoulou and Athanasios C. Rakitzis},
  doi          = {10.1080/02664763.2020.1820959},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {553-573},
  shortjournal = {J. Appl. Stat.},
  title        = {EWMA control charts for monitoring correlated counts with finite range},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A semi-analytical solution to the maximum-likelihood fit of
poisson data to a linear model using the cash statistic. <em>JOAS</em>,
<em>49</em>(3), 522–552. (<a
href="https://doi.org/10.1080/02664763.2020.1820960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Cash statistic, also known as the C C C statistic, is commonly used for the analysis of low-count Poisson data, including data with null counts for certain values of the independent variable. The use of this statistic is especially attractive for low-count data that cannot be combined, or re-binned, without loss of resolution. This paper presents a new maximum-likelihood solution for the best-fit parameters of a linear model using the Poisson-based Cash statistic. The solution presented in this paper provides a new and simple method to measure the best-fit parameters of a linear model for any Poisson-based data, including data with null counts. In particular, the method enforces the requirement that the best-fit linear model be non-negative throughout the support of the independent variable. The method is summarized in a simple algorithm to fit Poisson counting data of any size and counting rate with a linear model, by-passing entirely the use of the traditional χ 2 statistic.},
  archive      = {J_JOAS},
  author       = {Massimiliano Bonamente and David Spence},
  doi          = {10.1080/02664763.2020.1820960},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {522-552},
  shortjournal = {J. Appl. Stat.},
  title        = {A semi-analytical solution to the maximum-likelihood fit of poisson data to a linear model using the cash statistic},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time-varying coefficient cumulative gap time models for
intensive longitudinal ecological momentary assessment data with
missingness. <em>JOAS</em>, <em>49</em>(2), 498–521. (<a
href="https://doi.org/10.1080/02664763.2020.1815676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ecological momentary assessment (EMA) studies investigate intensive repeated observations of the current behavior and experiences of subjects in real time. In particular, such studies aim to minimize recall bias and maximize ecological validity, thereby strengthening the investigation and inference of microprocesses that influence behavior in real-world contexts by gathering intensive information on the temporal patterning of behavior of study subjects. Throughout this paper, we focus on the data analysis of an EMA study that examined behavior of intermittent smokers (ITS). Specifically, we sought to explore the pattern of clustered smoking behavior of ITS, or smoking ‘bouts’, as well as the covariates that predict such smoking behavior. To do this, in this paper we introduce a framework for characterizing the temporal behavior of ITS via the functions of event gap time to distinguish the smoking bouts. We used the time-varying coefficient models for the cumulative log gap time and to characterize the temporal patterns of smoking behavior, while simultaneously adjusting for behavioral covariates, and incorporated the inverse probability weighting into the models to accommodate missing data. Simulation studies showed that irrespective of whether missing by design or missing at random, the model was able to reliably determine prespecified time-varying functional forms of a given covariate coefficient, provided the the within-subject level was small.},
  archive      = {J_JOAS},
  author       = {Xiaoxue Li and Stewart J. Anderson and Saul Shiffman and Bo Zhang},
  doi          = {10.1080/02664763.2020.1815676},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {498-521},
  shortjournal = {J. Appl. Stat.},
  title        = {Time-varying coefficient cumulative gap time models for intensive longitudinal ecological momentary assessment data with missingness},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A surrogate model for estimating extreme tower loads on wind
turbines based on random forest proximities. <em>JOAS</em>,
<em>49</em>(2), 485–497. (<a
href="https://doi.org/10.1080/02664763.2020.1815675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present paper, we present a surrogate model, which can be used to estimate extreme tower loads on a wind turbine from a number of signals and a suitable simulation tool. Due to the requirements of the International Electrotechnical Commission (IEC) Standard 61400-1, assessing extreme tower loads on wind turbines constitutes a key component of the design phase. The proposed model imputes tower loads by matching observed signals with simulated quantities using proximities induced by random forests. In this way, the algorithm&#39;s adaptability to high-dimensional and sparse settings is exploited without using regression-based surrogate loads (which may display misleading probabilistic characteristics). Finally, the model is applied to estimate tower loads on an operating wind turbine from data on its operational statistics.},
  archive      = {J_JOAS},
  author       = {Mikkel Slot Nielsen and Victor Rohde},
  doi          = {10.1080/02664763.2020.1815675},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {485-497},
  shortjournal = {J. Appl. Stat.},
  title        = {A surrogate model for estimating extreme tower loads on wind turbines based on random forest proximities},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantifying treatment differences in confirmatory trials
under non-proportional hazards. <em>JOAS</em>, <em>49</em>(2), 466–484.
(<a href="https://doi.org/10.1080/02664763.2020.1815673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proportional hazards are a common assumption when designing confirmatory clinical trials in oncology. With the emergence of immunotherapy and novel targeted therapies, departure from the proportional hazard assumption is not rare in nowadays clinical research. Under non-proportional hazards, the hazard ratio does not have a straightforward clinical interpretation, and the log-rank test is no longer the most powerful statistical test even though it is still valid. Nevertheless, the log-rank test and the hazard ratio are still the primary analysis tools, and traditional approaches such as sample size increase are still proposed to account for the impact of non-proportional hazards. The weighed log-rank test and the test based on the restricted mean survival time (RMST) are receiving a lot of attention as a potential alternative to the log-rank test. We conduct a simulation study comparing the performance and operating characteristics of the log-rank test, the weighted log-rank test and the test based on the RMST, including a treatment effect estimation, under different non-proportional hazards patterns. Results show that, under non-proportional hazards, the hazard ratio and weighted hazard ratio have no straightforward clinical interpretation whereas the RMST ratio can be interpreted regardless of the proportional hazards assumption. In terms of power, the RMST achieves a similar performance when compared to the log-rank test.},
  archive      = {J_JOAS},
  author       = {José L. Jiménez},
  doi          = {10.1080/02664763.2020.1815673},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {466-484},
  shortjournal = {J. Appl. Stat.},
  title        = {Quantifying treatment differences in confirmatory trials under non-proportional hazards},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling population and subject-specific growth in a latent
trait measured by multiple instruments over time using a hierarchical
bayesian framework. <em>JOAS</em>, <em>49</em>(2), 449–465. (<a
href="https://doi.org/10.1080/02664763.2020.1817346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Psychometric growth curve modeling techniques are used to describe a person’s latent ability and how that ability changes over time based on a specific measurement instrument. However, the same instrument cannot always be used over a period of time to measure that latent ability. This is often the case when measuring traits longitudinally in children. Reasons may be that over time some measurement tools that were difficult for young children become too easy as they age resulting in floor effects or ceiling effects or both. We propose a Bayesian hierarchical model for such a scenario. Within the Bayesian model we combine information from multiple instruments used at different age ranges and having different scoring schemes to examine growth in latent ability over time. The model includes between-subject variance and within-subject variance and does not require linking item specific difficulty between the measurement tools. The model’s utility is demonstrated on a study of language ability in children from ages one to ten who are hard of hearing where measurement tool specific growth and subject-specific growth are shown in addition to a group level latent growth curve comparing the hard of hearing children to children with normal hearing.},
  archive      = {J_JOAS},
  author       = {Caitlin Ward and Jacob Oleson and J. Bruce Tomblin and Elizabeth Walker},
  doi          = {10.1080/02664763.2020.1817346},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {449-465},
  shortjournal = {J. Appl. Stat.},
  title        = {Modeling population and subject-specific growth in a latent trait measured by multiple instruments over time using a hierarchical bayesian framework},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two multivariate online change detection models.
<em>JOAS</em>, <em>49</em>(2), 427–448. (<a
href="https://doi.org/10.1080/02664763.2020.1815674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online change point detection methods monitor changes in the distribution of a data stream. This article discusses two non-parametric online change detection methods based on the energy statistics and Mahalanobis depth. To apply the energy statistic, we use sliding-window algorithm with efficient training and updating procedures. For Mahalanobis depth, we propose an algorithm to train the threshold with desired protective ability against false alarms and discuss factors that have an influence on the threshold. Numerical studies evaluate and compare the performance of the proposed models with three existing methods to detect changes in the mean and variability of a data stream. The methods are applied to detecting changes in the flowing volume of the Mississippi River.},
  archive      = {J_JOAS},
  author       = {Lingzhe Guo and Reza Modarres},
  doi          = {10.1080/02664763.2020.1815674},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {427-448},
  shortjournal = {J. Appl. Stat.},
  title        = {Two multivariate online change detection models},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature selection based on distance correlation: A filter
algorithm. <em>JOAS</em>, <em>49</em>(2), 411–426. (<a
href="https://doi.org/10.1080/02664763.2020.1815672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS) is one of the most powerful techniques to cope with the curse of dimensionality. In the study, a new filter approach to feature selection based on distance correlation is presented (DCFS, for short), which keeps the model-free advantage without any pre-specified parameters. Our method consists of two steps: hard step (forward selection) and soft step (backward selection). In the hard step, two types of associations, between univariate feature and the classes and between group feature and the classes, are involved to pick out the most relevant features with respect to the target classes. Due to the strict screening condition in the first step, some of the useful features are likely removed. Therefore, in the soft step, a feature-relationship gain (like feature score) based on the distance correlation is introduced, which is concerned with five kinds of associations. We sort the feature gain values and implement the backward selection procedure until the errors stop declining. The simulation results show that our method becomes more competitive on several datasets compared with some of the representative feature selection methods based on several classification models.},
  archive      = {J_JOAS},
  author       = {Hongwei Tan and Guodong Wang and Wendong Wang and Zili Zhang},
  doi          = {10.1080/02664763.2020.1815672},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {411-426},
  shortjournal = {J. Appl. Stat.},
  title        = {Feature selection based on distance correlation: A filter algorithm},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predictive analysis for joint progressive censoring plans: A
bayesian approach. <em>JOAS</em>, <em>49</em>(2), 394–410. (<a
href="https://doi.org/10.1080/02664763.2020.1815671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparative lifetime experiments are of particular importance in production processes when one wishes to determine the relative merits of several competing products with regard to their reliability. This paper confines itself to the data obtained by running a joint progressive Type-II censoring plan on samples in a combined manner. The problem of Bayesian predicting failure times of surviving units is discussed in details when parent populations are exponential. Two real data sets are analyzed in order to illustrate all the inferential procedures developed here. When destructive experiments under a censoring scheme finished, the researchers are usually interested to estimate remaining lifetimes of surviving units for sequel experiments. Findings of this paper are useful for these purposes specially when samples are non-homogeneous such as those taken from industrial storages.},
  archive      = {J_JOAS},
  author       = {Mohammad Vali Ahmadi and Mahdi Doostparast},
  doi          = {10.1080/02664763.2020.1815671},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {394-410},
  shortjournal = {J. Appl. Stat.},
  title        = {Predictive analysis for joint progressive censoring plans: A bayesian approach},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exponentiated teissier distribution with increasing,
decreasing and bathtub hazard functions. <em>JOAS</em>, <em>49</em>(2),
371–393. (<a
href="https://doi.org/10.1080/02664763.2020.1813694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a two-parameter exponentiated Teissier distribution. It is the main advantage of the distribution to have increasing, decreasing and bathtub shapes for its hazard rate function. The expressions of the ordinary moments, identifiability, quantiles, moments of order statistics, mean residual life function and entropy measure are derived. The skewness and kurtosis of the distribution are explored using the quantiles. In order to study two independent random variables, stress–strength reliability and stochastic orderings are discussed. Estimators based on likelihood, least squares, weighted least squares and product spacings are constructed for estimating the unknown parameters of the distribution. An algorithm is presented for random sample generation from the distribution. Simulation experiments are conducted to compare the performances of the considered estimators of the parameters and percentiles. Three sets of real data are fitted by using the proposed distribution over the competing distributions.},
  archive      = {J_JOAS},
  author       = {Vikas Kumar Sharma and Sudhanshu V. Singh and Komal Shekhawat},
  doi          = {10.1080/02664763.2020.1813694},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {371-393},
  shortjournal = {J. Appl. Stat.},
  title        = {Exponentiated teissier distribution with increasing, decreasing and bathtub hazard functions},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian estimation of extropy and goodness of fit tests.
<em>JOAS</em>, <em>49</em>(2), 357–370. (<a
href="https://doi.org/10.1080/02664763.2020.1812545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extropy, a complementary dual of entropy, is considered in this paper. A Bayesian approach based on the Dirichlet process is proposed for the estimation of extropy. A goodness of fit test is also developed. Many theoretical properties of the procedure are derived. Several examples are discussed to illustrate the approach.},
  archive      = {J_JOAS},
  author       = {Luai Al-Labadi and Sean Berry},
  doi          = {10.1080/02664763.2020.1812545},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {357-370},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian estimation of extropy and goodness of fit tests},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian generalizations of the integer-valued
autoregressive model. <em>JOAS</em>, <em>49</em>(2), 336–356. (<a
href="https://doi.org/10.1080/02664763.2020.1812544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop two Bayesian generalizations of the Poisson integer-valued autoregressive model. The AdINAR(1) model accounts for overdispersed data by means of an innovation process whose marginal distributions are finite mixtures, while the DP-INAR(1) model is a hierarchical extension involving a Dirichlet process, which is capable of modeling a latent pattern of heterogeneity in the distribution of the innovations rates. The probabilistic forecasting capabilities of both models are put to test in the analysis of crime data in Pittsburgh, with favorable results.},
  archive      = {J_JOAS},
  author       = {Paulo C. Marques F. and Helton Graziadei and Hedibert F. Lopes},
  doi          = {10.1080/02664763.2020.1812544},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {336-356},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian generalizations of the integer-valued autoregressive model},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kalman filtering with censored measurements. <em>JOAS</em>,
<em>49</em>(2), 317–335. (<a
href="https://doi.org/10.1080/02664763.2020.1810645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns Kalman filtering when the measurements of the process are censored. The censored measurements are addressed by the Tobit model of Type I and are one-dimensional with two censoring limits, while the (hidden) state vectors are multidimensional. For this model, Bayesian estimates for the state vectors are provided through a recursive algorithm of Kalman filtering type. Experiments are presented to illustrate the effectiveness and applicability of the algorithm. The experiments show that the proposed method outperforms other filtering methodologies in minimizing the computational cost as well as the overall Root Mean Square Error (RMSE) for synthetic and real data sets.},
  archive      = {J_JOAS},
  author       = {Kostas Loumponias and George Tsaklidis},
  doi          = {10.1080/02664763.2020.1810645},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {317-335},
  shortjournal = {J. Appl. Stat.},
  title        = {Kalman filtering with censored measurements},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting the guttman effect with the help of ordinal
correspondence analysis in synchrotron x-ray diffraction data analysis.
<em>JOAS</em>, <em>49</em>(2), 291–316. (<a
href="https://doi.org/10.1080/02664763.2020.1810644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method for detecting a Guttman effect in a complete disjunctive table U U U with Q questions. Since such an investigation is a nonsense when the Q variables are independent, we reuse a previous unpublished work about the chi-squared independence test for Burt&#39;s tables. Then, we introduce a two-steps method consisting in plugging the first singular vector from a preliminary Correspondence Analysis (CA) of U as a score x into a subsequent singly-ordered Ordinal Correspondence Analysis (OCA) of U . OCA mainly consists in completing x by a sequence of orthogonal polynomials superseding the classical factors of CA. As a consequence, in presence of a pure Guttman effect, we should in principle have that the second singular vector coincide with the polynomial of degree 2, etc. The hybrid decomposition of the Pearson chi-squared statistics (resulting from OCA) used in association with permutation tests makes possible to reveal such relationships, i.e. the presence of a Guttman effect in the structure of U , and to determine its degree - with an accuracy depending on the signal to noise ratio. The proposed method is successively tested on artificial data (more or less noisy), a well-known benchmark, and synchrotron X-ray diffraction data of soil samples.},
  archive      = {J_JOAS},
  author       = {C. Manté and S. Cornu and D. Borschneck and C. Mocuta and R. van den Bogaert},
  doi          = {10.1080/02664763.2020.1810644},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {291-316},
  shortjournal = {J. Appl. Stat.},
  title        = {Detecting the guttman effect with the help of ordinal correspondence analysis in synchrotron X-ray diffraction data analysis},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust analogs to the coefficient of variation.
<em>JOAS</em>, <em>49</em>(2), 268–290. (<a
href="https://doi.org/10.1080/02664763.2020.1808599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coefficient of variation (CV) is commonly used to measure relative dispersion. However, since it is based on the sample mean and standard deviation, outliers can adversely affect it. Additionally, for skewed distributions the mean and standard deviation may be difficult to interpret and, consequently, that may also be the case for the CV CV CV . Here we investigate the extent to which quantile-based measures of relative dispersion can provide appropriate summary information as an alternative to the CV. In particular, we investigate two measures, the first being the interquartile range (in lieu of the standard deviation), divided by the median (in lieu of the mean), and the second being the median absolute deviation, divided by the median, as robust estimators of relative dispersion. In addition to comparing the influence functions of the competing estimators and their asymptotic biases and variances, we compare interval estimators using simulation studies to assess coverage.},
  archive      = {J_JOAS},
  author       = {Chandima N. P. G. Arachchige and Luke A. Prendergast and Robert G. Staudte},
  doi          = {10.1080/02664763.2020.1808599},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {268-290},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust analogs to the coefficient of variation},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The semiparametric regression model for bimodal data with
different penalized smoothers applied to climatology, ethanol and air
quality data. <em>JOAS</em>, <em>49</em>(1), 248–267. (<a
href="https://doi.org/10.1080/02664763.2020.1803812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semiparametric regressions can be used to model data when covariables and the response variable have a nonlinear relationship. In this work, we propose three flexible regression models for bimodal data called the additive, additive partial and semiparametric regressions, basing on the odd log-logistic generalized inverse Gaussian distribution under three types of penalized smoothers, where the main idea is not to confront the three forms of smoothings but to show the versatility of the distribution with three types of penalized smoothers. We present several Monte Carlo simulations carried out for different configurations of the parameters and some sample sizes to verify the precision of the penalized maximum-likelihood estimators. The usefulness of the proposed regressions is proved empirically through three applications to climatology, ethanol and air quality data.},
  archive      = {J_JOAS},
  author       = {J. C. S. Vasconcelos and G. M. Cordeiro and E. M. M. Ortega},
  doi          = {10.1080/02664763.2020.1803812},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {248-267},
  shortjournal = {J. Appl. Stat.},
  title        = {The semiparametric regression model for bimodal data with different penalized smoothers applied to climatology, ethanol and air quality data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable module detection for attributed networks with
applications to breast cancer. <em>JOAS</em>, <em>49</em>(1), 230–247.
(<a href="https://doi.org/10.1080/02664763.2020.1803811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of network module detection is to identify groups of nodes within a network structure that are tightly connected. Nodes in a network often have attributes (aka metadata) associated with them. It is often desirable to identify groups of nodes that are tightly connected in the network structure, but also have strong similarity in their attributes. Utilizing attribute information in module detection is a major challenge because it requires bridging the structural network with attribute data. A Weighted Fast Greedy (WFG) algorithm for attribute-based module detection is proposed. WFG utilizes logistic regression to bridge the structural and attribute spaces. The logistic function naturally emphasizes associations between attributes and network structure accordingly, and can be easily interpreted. A breast cancer application is presented that connects a protein–protein interaction network gene expression data and a survival outcome. This application demonstrates the importance of embedding attribute information into the community detection framework on a breast cancer dataset. Five modules were significant for survival and they contained known pathways and markers for cancer, including cell cycle, p53 pathway, BRCA1 , BRCA2 , and AURKB , among others. Whereas, neither the gene expression data nor the network structure alone gave rise to these cancer biomarkers and signatures.},
  archive      = {J_JOAS},
  author       = {Han Yu and Rachael Hageman Blair},
  doi          = {10.1080/02664763.2020.1803811},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {230-247},
  shortjournal = {J. Appl. Stat.},
  title        = {Scalable module detection for attributed networks with applications to breast cancer},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A model-based approach to spotify data analysis: A beta
GLMM. <em>JOAS</em>, <em>49</em>(1), 214–229. (<a
href="https://doi.org/10.1080/02664763.2020.1803810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital music distribution is increasingly powered by automated mechanisms that continuously capture, sort and analyze large amounts of Web-based data. This paper deals with the management of songs audio features from a statistical point of view. In particular, it explores the data catching mechanisms enabled by Spotify Web API and suggests statistical tools for the analysis of these data. Special attention is devoted to songs popularity and a Beta model, including random effects, is proposed in order to give the first answer to questions like: which are the determinants of popularity? The identification of a model able to describe this relationship, the determination within the set of characteristics of those considered most important in making a song popular is a very interesting topic for those who aim to predict the success of new products.},
  archive      = {J_JOAS},
  author       = {Mariangela Sciandra and Irene Carola Spera},
  doi          = {10.1080/02664763.2020.1803810},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {214-229},
  shortjournal = {J. Appl. Stat.},
  title        = {A model-based approach to spotify data analysis: A beta GLMM},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint regression modeling of location and scale parameters
of the skew t distribution with application in soil chemistry data.
<em>JOAS</em>, <em>49</em>(1), 195–213. (<a
href="https://doi.org/10.1080/02664763.2020.1801608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In regression model applications, the errors may frequently present a symmetric shape. In such cases, the normal and Student t distributions are commonly used. In this paper, we shall be concerned only to model heavy-tailed, skewed errors and absence of variance homogeneity with two regression structures based on the skew t distribution. We consider a classic analysis for the parameters of the proposed model. We perform a diagnostic analysis based on global influence and quantile residuals. For different parameter settings and sample sizes, various simulation results are obtained and compared to evaluate the performance of the skew t regression. Further, we illustrate the usefulness of the new regression by means of a real data set (amount of potassium in different soil areas) from a study carried out at the Department of Soil Science of the Luiz de Queiroz School of Agriculture, University of São Paulo.},
  archive      = {J_JOAS},
  author       = {F. Prataviera and A. M. Batista and P. L. Libardi and G. M. Cordeiro and E. M. M. Ortega},
  doi          = {10.1080/02664763.2020.1801608},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {195-213},
  shortjournal = {J. Appl. Stat.},
  title        = {Joint regression modeling of location and scale parameters of the skew t distribution with application in soil chemistry data},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation of parameters of inverse weibull distribution and
application to multi-component stress-strength model. <em>JOAS</em>,
<em>49</em>(1), 169–194. (<a
href="https://doi.org/10.1080/02664763.2020.1803815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of estimation of the parameters of two-parameter inverse Weibull distributions has been considered. We establish existence and uniqueness of the maximum likelihood estimators of the scale and shape parameters. We derive Bayes estimators of the parameters under the entropy loss function. Hierarchical Bayes estimator, equivariant estimator and a class of minimax estimators are derived when shape parameter is known. Ordered Bayes estimators using information about second population are also derived. We investigate the reliability of multi-component stress-strength model using classical and Bayesian approaches. Risk comparison of the classical and Bayes estimators is done using Monte Carlo simulations. Applications of the proposed estimators are shown using real data sets.},
  archive      = {J_JOAS},
  author       = {Nabakumar Jana and Samadrita Bera},
  doi          = {10.1080/02664763.2020.1803815},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {169-194},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimation of parameters of inverse weibull distribution and application to multi-component stress-strength model},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Small area estimation of expenditure means and ratios under
a unit-level bivariate linear mixed model. <em>JOAS</em>,
<em>49</em>(1), 143–168. (<a
href="https://doi.org/10.1080/02664763.2020.1803809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under a unit-level bivariate linear mixed model, this paper introduces small area predictors of expenditure means and ratios, and derives approximations and estimators of the corresponding mean squared errors. For the considered model, the REML estimation method is implemented. Several simulation experiments, designed to analyze the behavior of the introduced fitting algorithm, predictors and mean squared error estimators, are carried out. An application to real data from the Spanish household budget survey illustrates the behavior of the proposed statistical methodology. The target is the estimation of means of food and non-food household annual expenditures and of ratios of food household expenditures by Spanish provinces.},
  archive      = {J_JOAS},
  author       = {María Dolores Esteban and María José Lombardía and Esther López-Vizcaíno and Domingo Morales and Agustín Pérez},
  doi          = {10.1080/02664763.2020.1803809},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {143-168},
  shortjournal = {J. Appl. Stat.},
  title        = {Small area estimation of expenditure means and ratios under a unit-level bivariate linear mixed model},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reliability analysis of multicomponent stress–strength
reliability from a bathtub-shaped distribution. <em>JOAS</em>,
<em>49</em>(1), 122–142. (<a
href="https://doi.org/10.1080/02664763.2020.1803808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, inference for a multicomponent stress–strength model is studied. When latent strength and stress random variables follow a bathtub-shaped distribution and the failure times are Type-II censored, the maximum likelihood estimate of the multicomponent stress–strength reliability (MSR) is established when there are common strength and stress parameters. Approximate confidence interval is also constructed by using the asymptotic distribution theory and delta method. Furthermore, another alternative generalized point and confidence interval estimators for the MSR are constructed based on pivotal quantities. Moreover, the likelihood and the pivotal quantities-based estimates for the MSR are also provided under unequal strength and stress parameter case. To compare the equivalence of the stress and strength parameters, the likelihood ratio test for hypothesis of interest is also provided. Finally, simulation studies and a real data example are given for illustration.},
  archive      = {J_JOAS},
  author       = {Liang Wang and Ke Wu and Yogesh Mani Tripathi and Chandrakant Lodhi},
  doi          = {10.1080/02664763.2020.1803808},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {122-142},
  shortjournal = {J. Appl. Stat.},
  title        = {Reliability analysis of multicomponent stress–strength reliability from a bathtub-shaped distribution},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MulticlusterKDE: A new algorithm for clustering based on
multivariate kernel density estimation. <em>JOAS</em>, <em>49</em>(1),
98–121. (<a
href="https://doi.org/10.1080/02664763.2020.1799958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose the MulticlusterKDE algorithm applied to classify elements of a database into categories based on their similarity. MulticlusterKDE is centered on the multiple optimization of the kernel density estimator function with multivariate Gaussian kernel. One of the main features of the proposed algorithm is that the number of clusters is an optional input parameter. Furthermore, it is very simple, easy to implement, well defined and stops at a finite number of steps and it always converges regardless of the data set. We illustrate our findings by implementing the algorithm in R software. The results indicate that the MulticlusterKDE algorithm is competitive when compared to K-means, K-medoids, CLARA, DBSCAN and PdfCluster algorithms. Features such as simplicity and efficiency make the proposed algorithm an attractive and promising research field that can be used as basis for its improvement and also for the development of new density-based clustering algorithms.},
  archive      = {J_JOAS},
  author       = {D. Scaldelai and L. C. Matioli and S. R. Santos and M. Kleina},
  doi          = {10.1080/02664763.2020.1799958},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {98-121},
  shortjournal = {J. Appl. Stat.},
  title        = {MulticlusterKDE: A new algorithm for clustering based on multivariate kernel density estimation},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exact unconditional inference for analyzing contingency
tables in finite populations. <em>JOAS</em>, <em>49</em>(1), 86–97. (<a
href="https://doi.org/10.1080/02664763.2020.1798363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With recent developments in computer power the application of exact inferential methods has become more feasible which has resulted in increasing popularity of these approaches. However, there is a lack of such methodology for populations with more complex structure, such as finite populations. When a small sample is drawn from a finite population, the number of individuals with a specific characteristic of interest follows hypergeometric distribution. In order to test for the comparison of two proportions in finite populations we develop an exact unconditional test. We utilize the information gained from the sample to restrict our search for the maximum p -value. Our proposed test has power equal to its competitors while maintains the pre-specified nominal significance level.},
  archive      = {J_JOAS},
  author       = {Shiva S. Dibaj and Alan D. Hutson and Graham W. Warren and Gregory E. Wilding},
  doi          = {10.1080/02664763.2020.1798363},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {86-97},
  shortjournal = {J. Appl. Stat.},
  title        = {Exact unconditional inference for analyzing contingency tables in finite populations},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conservative confidence intervals on multiple correlation
coefficient for high-dimensional elliptical data using random projection
methodology. <em>JOAS</em>, <em>49</em>(1), 64–85. (<a
href="https://doi.org/10.1080/02664763.2020.1796937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {So called multiple correlation coefficient (MCC) is a measure of linear relationship between a given variable and set of covariates. In the multiple correlation and regression analysis, it is common practice to construct a confidence interval for the population MCC. In high-dimensional data settings, by which the data dimension p is much larger than the sample size n , due to the singularity of the sample covariance matrix, the classical confidence intervals for the MCC are no longer useable. For high-dimensional elliptical data, some (conservative) confidence intervals for the population MCC are presented using the random projection methodology. To evaluate and compare the performance of the proposed confidence intervals, some simulations are conducted in terms of the coverage probability and average interval length. Experimental validation of the proposed intervals is carried out on two real gene expression datasets.},
  archive      = {J_JOAS},
  author       = {Dariush Najarzadeh},
  doi          = {10.1080/02664763.2020.1796937},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {64-85},
  shortjournal = {J. Appl. Stat.},
  title        = {Conservative confidence intervals on multiple correlation coefficient for high-dimensional elliptical data using random projection methodology},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Combination of multiple functional markers to improve
diagnostic accuracy. <em>JOAS</em>, <em>49</em>(1), 44–63. (<a
href="https://doi.org/10.1080/02664763.2020.1796945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combination of multiple biomarkers to improve diagnostic accuracy is meaningful for practitioners and clinicians, and are attractive to lots of researchers. Nowadays, with development of modern techniques, functional markers such as curves or images, play an important role in diagnosis. There exists rich literature developing combination methods for continuous scalar markers. Unfortunately, only sporadic works have studied how functional markers affect diagnosis in the literature. Moreover, no publication can be found to do combination of multiple functional markers to improve the diagnostic accuracy. It is impossible to apply scalar combination methods to the multiple functional markers directly because of infinite dimensionality of functional markers. In this article, we propose a one-dimension scalar feature motivated by square loss distance, as an alternative of the original functional curve in the sense that, it can retain information to the most extent. The square loss distance is defined as the function of projection scores generated from functional principal component decomposition. Then existing variety of scalar combination methods can be applied to scalar features of functional markers after dimension reduction to improve the diagnostic accuracy. Area under the receiver operating characteristic curve and Youden index are used to assess performances of various methods in numerical studies. We also analyzed the high- or low- hospital admissions due to respiratory diseases between 2010 and 2017 in Hong Kong by combining weather conditions and media information, which are regarded as functional markers. Finally, we provide an R function for convenient application.},
  archive      = {J_JOAS},
  author       = {Haiqiang Ma and Jin Yang and Sheng Xu and Chao Liu and Qinyi Zhang},
  doi          = {10.1080/02664763.2020.1796945},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {44-63},
  shortjournal = {J. Appl. Stat.},
  title        = {Combination of multiple functional markers to improve diagnostic accuracy},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extensions of empirical likelihood and chi-squared-based
tests for ordered alternatives. <em>JOAS</em>, <em>49</em>(1), 24–43.
(<a href="https://doi.org/10.1080/02664763.2020.1796944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several methods for comparing k populations have been proposed in the literature. These methods assess the same null hypothesis of equal distributions but differ in the alternative hypothesis they consider. We focus on two important alternative hypotheses: monotone and umbrella ordering. Two new families of test statistics are proposed, including two known tests, as well as two new powerful tests under monotone ordering. Furthermore, these families are adapted for testing umbrella ordering. We compare some members of the families with respect to power and Type I errors under different simulation scenarios. Finally, the methods are illustrated in several applications to real data.},
  archive      = {J_JOAS},
  author       = {M. Carmen Pardo and Ying Lu and Alba M. Franco-Pereira},
  doi          = {10.1080/02664763.2020.1796944},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {24-43},
  shortjournal = {J. Appl. Stat.},
  title        = {Extensions of empirical likelihood and chi-squared-based tests for ordered alternatives},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Zero-inflated models for adjusting varying exposures: A
cautionary note on the pitfalls of using offset. <em>JOAS</em>,
<em>49</em>(1), 1–23. (<a
href="https://doi.org/10.1080/02664763.2020.1796943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-inflated count data are frequently encountered in public health and epidemiology research. Two-parts model is often used to model the excessive zeros, which are a mixture of two components: a point mass at zero and a count distribution, such as a Poisson distribution. When the rate of events per unit exposure is of interest, offset is commonly used to account for the varying extent of exposure, which is essentially a predictor whose regression coefficient is fixed at one. Such an assumption of exposure effect is, however, quite restrictive for many practical problems. Further, for zero-inflated models, offset is often only included in the count component of the model. However, the probability of excessive zero component could also be affected by the amount of ‘exposure’. We, therefore, proposed incorporating the varying exposure as a covariate rather than an offset term in both the probability of excessive zeros and conditional counts components of the zero-inflated model. A real example is used to illustrate the usage of the proposed methods, and simulation studies are conducted to assess the performance of the proposed methods for a broad variety of situations.},
  archive      = {J_JOAS},
  author       = {Cindy Feng},
  doi          = {10.1080/02664763.2020.1796943},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {1-23},
  shortjournal = {J. Appl. Stat.},
  title        = {Zero-inflated models for adjusting varying exposures: A cautionary note on the pitfalls of using offset},
  volume       = {49},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
