<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tai---80">TAI - 80</h2>
<ul>
<li><details>
<summary>
(2022). Semisupervised deep learning for image classification with
distribution mismatch: A survey. <em>TAI</em>, <em>3</em>(6), 1015–1029.
(<a href="https://doi.org/10.1109/TAI.2022.3196326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methodologies have been employed in several different fields, with an outstanding success in image recognition applications, such as material quality control, medical imaging, autonomous driving, etc. Deep learning models rely on the abundance of labeled observations to train a prospective model. These models are composed of millions of parameters to estimate, increasing the need of more training observations. Frequently, it is expensive to gather labeled observations of data, making the usage of deep learning models not ideal, as the model might overfit data. In a semisupervised setting, unlabeled data are used to improve the levels of accuracy and generalization of a model with small labeled datasets. Nevertheless, in many situations different unlabeled data sources might be available. This raises the risk of a significant distribution mismatch between the labeled and unlabeled datasets. Such phenomena can cause a considerable performance hit to typical semisupervised deep learning (SSDL) frameworks, which often assume that both labeled and unlabeled datasets are drawn from similar distributions. Therefore, in this article we study the latest approaches for SSDL for image recognition. Emphasis is made in SSDL models designed to deal with a distribution mismatch between the labeled and unlabeled datasets. We address open challenges with the aim to encourage the community to tackle them, and overcome the high data demand of traditional deep learning pipelines under real-world usage settings.},
  archive      = {J_TAI},
  author       = {Saul Calderon-Ramirez and Shengxiang Yang and David Elizondo},
  doi          = {10.1109/TAI.2022.3196326},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1015-1029},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Semisupervised deep learning for image classification with distribution mismatch: A survey},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on siamese network: Methodologies, applications,
and opportunities. <em>TAI</em>, <em>3</em>(6), 994–1014. (<a
href="https://doi.org/10.1109/TAI.2022.3207112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Siamese network has obtained growing attention in real-life applications. In this survey, we present an comprehensive review on Siamese network from the aspects of methodologies, applications, and interesting topics for further exploration. We first introduce framework designs of Siamese network, followed by methodologies about learning with unlabeled data. Then, we review application scenarios in terms of classification and regression, together with relative methodologies. We also discuss the promising area of few-shot learning, followed by interesting topics about opportunities and challenges.},
  archive      = {J_TAI},
  author       = {Yikai Li and C. L. Philip Chen and Tong Zhang},
  doi          = {10.1109/TAI.2022.3207112},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {994-1014},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A survey on siamese network: Methodologies, applications, and opportunities},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On supervised class-imbalanced learning: An updated
perspective and some key challenges. <em>TAI</em>, <em>3</em>(6),
973–993. (<a href="https://doi.org/10.1109/TAI.2022.3160658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of class imbalance has always been considered as a significant challenge to traditional machine learning and the emerging deep learning research communities. A classification problem can be considered as class imbalanced if the training set does not contain an equal number of labeled examples from all the classes. A classifier trained on such an imbalanced training set is likely to favor those classes containing a larger number of training examples than the others. Unfortunately, the classes that contain a small number of labelled instances usually correspond to rare and significant events. Thus, poor classification accuracy on these classes may lead to severe consequences. In this article, we aim to provide a comprehensive summary of the rich pool of research works attempting to combat the adversarial effects of class imbalance efficiently. Specifically, following a formal definition of the problem of class imbalance, we explore the plethora of traditional machine learning approaches aiming to mitigate its adversarial effects. We further discuss the state-of-the-art deep-learning-based approaches for improving a classifier’s resilience against class imbalance and highlight the need for techniques tailored for such a paradigm. Moreover, we look at the emerging applications where class imbalance can be a major concern. Finally, we outline a few open problems along with the various challenges emerging with the advent of modern applications, deep learning paradigm, and new sources of data.},
  archive      = {J_TAI},
  author       = {Swagatam Das and Sankha Subhra Mullick and Ivan Zelinka},
  doi          = {10.1109/TAI.2022.3160658},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {973-993},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {On supervised class-imbalanced learning: An updated perspective and some key challenges},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decentralized deep learning for multi-access edge computing:
A survey on communication efficiency and trustworthiness. <em>TAI</em>,
<em>3</em>(6), 963–972. (<a
href="https://doi.org/10.1109/TAI.2021.3133819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wider coverage and a better solution to a latency reduction in 5G necessitate its combination with multi-access edge computing technology. Decentralized deep learning (DDL), such as federated learning and swarm learning, as a promising solution to privacy-preserving data processing for millions of smart edge devices leverages distributed computing of multilayer neural networks within the networking of local clients, without disclosing the original local training data. Notably, in industries such as finance and healthcare, where sensitive data of transactions and personal medical records are cautiously maintained, DDL can facilitate the collaboration among these institutes to improve the performance of trained models while protecting the data privacy of participating clients. In this survey article, we demonstrate the technical fundamentals of DDL that benefit many walks of society through decentralized learning. Furthermore, we offer a comprehensive overview of the current state of the art in the field by outlining the challenges of DDL and the most relevant solutions from novel perspectives of communication efficiency and trustworthiness.},
  archive      = {J_TAI},
  author       = {Yuwei Sun and Hideya Ochiai and Hiroshi Esaki},
  doi          = {10.1109/TAI.2021.3133819},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {963-972},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Decentralized deep learning for multi-access edge computing: A survey on communication efficiency and trustworthiness},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integrating constraints into dimensionality reduction for
visualization: A survey. <em>TAI</em>, <em>3</em>(6), 944–962. (<a
href="https://doi.org/10.1109/TAI.2022.3204734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This survey reviews and organizes existing methods for integrating constraints into dimensionality reduction (DR). In the world of high-dimensional data, DR methods help to reduce dimensionality while preserving important structures to facilitate subsequent tasks, such as data visualization. While DR methods only reveal hidden structures from the original data, additional information, such as class labels, external features, or even feedback or prior knowledge from users can help to enrich low-dimensional representations. We consider all these types of additional information as constraints. Integrating constraints into classification and clustering methods is well studied, yet, a systematic review on constraint integration in DR methods for visualization is still lacking. We contribute to the literature of constraints in DR visualizations with a novel categorization focusing on constraint types. This survey also introduces new perspectives on the subject, and suggests new trends and future research directions for combining constraints and DR methods.},
  archive      = {J_TAI},
  author       = {Viet Minh Vu and Adrien Bibal and Benoît Frénay},
  doi          = {10.1109/TAI.2022.3204734},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {944-962},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Integrating constraints into dimensionality reduction for visualization: A survey},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluation methods and measures for causal learning
algorithms. <em>TAI</em>, <em>3</em>(6), 924–943. (<a
href="https://doi.org/10.1109/TAI.2022.3150264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The convenient access to copious multifaceted data has encouraged machine learning researchers to reconsider correlation-based learning and embrace the opportunity of causality-based learning, i.e., causal machine learning (causal learning). Recent years have, therefore, witnessed great effort in developing causal learning algorithms aiming to help artificial intelligence (AI) achieve human-level intelligence. Due to the lack of ground-truth data, one of the biggest challenges in current causal learning research is algorithm evaluations. This largely impedes the cross-pollination of AI and causal inference and hinders the two fields to benefit from the advances of the other. To bridge from conventional causal inference (i.e., based on statistical methods) to causal learning with Big Data (i.e., the intersection of causal inference and machine learning), in this survey, we review commonly used datasets, evaluation methods, and measures for causal learning using an evaluation pipeline similar to conventional machine learning. We focus on the two fundamental causal inference tasks and causality-aware machine learning tasks. Limitations of current evaluation procedures are also discussed. We, then, examine popular causal inference tools/packages and conclude with primary challenges and opportunities for benchmarking causal learning algorithms in the era of Big Data. The survey seeks to bring to the forefront the urgency of developing publicly available benchmarks and consensus-building standards for causal learning evaluation with observational data. In doing so, we hope to broaden the discussions and facilitate collaboration to advance the innovation and application of causal learning.},
  archive      = {J_TAI},
  author       = {Lu Cheng and Ruocheng Guo and Raha Moraffah and Paras Sheth and K. Selçuk Candan and Huan Liu},
  doi          = {10.1109/TAI.2022.3150264},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {924-943},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Evaluation methods and measures for causal learning algorithms},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intellectual property protection for deep learning models:
Taxonomy, methods, attacks, and evaluations. <em>TAI</em>,
<em>3</em>(6), 908–923. (<a
href="https://doi.org/10.1109/TAI.2021.3133824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The training and creation of deep learning model is usually costly, thus the trained model can be regarded as an intellectual property (IP) of the model creator. However, malicious users who obtain high-performance models may illegally copy, redistribute, or abuse the models without permission. To deal with such security threats, a few deep neural networks (DNN) IP protection methods have been proposed in recent years. This article attempts to provide a review of the existing DNN IP protection works and also an outlook. First, we propose the first taxonomy for DNN IP protection methods in terms of six attributes—scenario, mechanism, capacity, type, function, and target models. Then, we present a survey on existing DNN IP protection works in terms of the above six attributes, especially focusing on the challenges these methods face, whether these methods can provide proactive protection, and their resistances to different levels of attacks. After that, we analyze the potential attacks on DNN IP protection methods from the aspects of model modifications, evasion attacks, and active attacks. Besides, a systematic evaluation method for DNN IP protection methods with respect to basic functional metrics, attack-resistance metrics, and customized metrics for different application scenarios is given. Finally, challenges and future research opportunities on DNN IP protection are presented.},
  archive      = {J_TAI},
  author       = {Mingfu Xue and Yushu Zhang and Jian Wang and Weiqiang Liu},
  doi          = {10.1109/TAI.2021.3133824},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {908-923},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Intellectual property protection for deep learning models: Taxonomy, methods, attacks, and evaluations},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The new generation brain-inspired sparse learning: A
comprehensive survey. <em>TAI</em>, <em>3</em>(6), 887–907. (<a
href="https://doi.org/10.1109/TAI.2022.3170001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the enormous demand for computing resources resulting from massive data and complex network models has become the limitation of deep learning. In the large-scale problems with massive samples and ultrahigh feature dimensions, sparsity has gradually drawn much attention from academia and the industrial field. In this article, the new generation of brain-inspired sparse learning is reviewed comprehensively. First, sparse cognition learning is introduced from the visual biology mechanism to modeling for the natural image. Second, the sparse representation algorithms are summarized to sort out the research progress of sparse learning. Third, the relevant research on sparse feature selection learning is reviewed. Then, the sparse deep networks and applications are summed up. Last but not least, ten public issues and challenges of sparse learning are discussed. By investigating the development process of sparse learning, this article summarizes the advantages, disadvantages, limitations, and future research directions of the algorithm, which can help readers conduct further study.},
  archive      = {J_TAI},
  author       = {Licheng Jiao and Yuting Yang and Fang Liu and Shuyuan Yang and Biao Hou},
  doi          = {10.1109/TAI.2022.3170001},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {887-907},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {The new generation brain-inspired sparse learning: A comprehensive survey},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An overview of emotion in artificial intelligence.
<em>TAI</em>, <em>3</em>(6), 867–886. (<a
href="https://doi.org/10.1109/TAI.2022.3159614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of artificial intelligence (AI) has gained immense traction over the past decade, producing increasingly successful applications as research strives to understand and exploit neural processing specifics. Nonetheless emotion, despite its demonstrated significance to reinforcement, social integration, and general development, remains a largely stigmatized and consequently disregarded topic by most engineers and computer scientists. In this article, we endorse emotion’s value for the advancement of artificial cognitive processing, as well as explore real-world use cases of emotion-augmented AI. A schematization is provided on the psychological-neurophysiologic basics of emotion in order to bridge the interdisciplinary gap preventing emulation and integration in AI methodology, as well as exploitation by current systems. In addition, we overview three major subdomains of AI greatly benefiting from emotion, and produce a systematic survey of meaningful yet recent contributions to each area. To conclude, we address crucial challenges and promising research paths for the future of emotion in AI with the hope that more researchers will develop an interest for the topic and find it easier to develop their own contributions.},
  archive      = {J_TAI},
  author       = {Gustavo Assunção and Bruno Patrão and Miguel Castelo-Branco and Paulo Menezes},
  doi          = {10.1109/TAI.2022.3159614},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {867-886},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An overview of emotion in artificial intelligence},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recent advances in trustworthy explainable artificial
intelligence: Status, challenges, and perspectives. <em>TAI</em>,
<em>3</em>(6), 852–866. (<a
href="https://doi.org/10.1109/TAI.2021.3133846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) and machine learning (ML) have come a long way from the earlier days of conceptual theories, to being an integral part of today’s technological society. Rapid growth of AI/ML and their penetration within a plethora of civilian and military applications, while successful, has also opened new challenges and obstacles. With almost no human involvement required for some of the new decision-making AI/ML systems, there is now a pressing need to gain better insights into how these decisions are made. This has given rise to a new field of AI research, explainable AI (XAI). In this article, we present a survey of XAI characteristics and properties. We provide an indepth review of XAI themes, and describe the different methods for designing and developing XAI systems, both during and post model-development. We include a detailed taxonomy of XAI goals, methods, and evaluation, and sketch the major milestones in XAI research. An overview of XAI for security and cybersecurity of XAI systems is also provided. Open challenges are delineated, and measures for evaluating XAI system robustness are described.},
  archive      = {J_TAI},
  author       = {Atul Rawal and James McCoy and Danda B. Rawat and Brian M. Sadler and Robert St. Amant},
  doi          = {10.1109/TAI.2021.3133846},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {852-866},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Recent advances in trustworthy explainable artificial intelligence: Status, challenges, and perspectives},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). WindGMMN: Scenario forecasting for wind power using
generative moment matching networks. <em>TAI</em>, <em>3</em>(5),
843–850. (<a href="https://doi.org/10.1109/TAI.2021.3128368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing penetration of wind power generation, the fluctuating and intermittent behavior of wind power poses huge challenges to the operation and planning of distribution networks. A popular way to mitigate these challenges is to provide a group of possible wind power forecasting scenarios instead of depending on deterministic point forecasting values, so that system operators can consider the uncertainties. This letter proposes a novel WindGMMN method for wind power scenario forecasting, in which necessary modifications are made on the generative moment matching network (GMMN), and an optimization strategy is designed to find a series of wind power scenarios with similar shapes, probability distributions, and temporal correlations as potential scenarios. Simulations and analyses were performed on a public dataset with 2190 wind power generation curves and their corresponding meteorological features. The results show that the proposed WindGMMN outperforms popular baselines (e.g., variational auto-encoders and generative adversarial networks) for scenario forecasting of wind power, without any restrictions on the time horizon (e.g., times ranging from 10 min to 24 h).},
  archive      = {J_TAI},
  author       = {Wenlong Liao and Zhe Yang and Xinxin Chen and Yaqi Li},
  doi          = {10.1109/TAI.2021.3128368},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {843-850},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {WindGMMN: Scenario forecasting for wind power using generative moment matching networks},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel temporal attentive-pooling based convolutional
recurrent architecture for acoustic signal enhancement. <em>TAI</em>,
<em>3</em>(5), 833–842. (<a
href="https://doi.org/10.1109/TAI.2022.3169995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Removing background noise from acoustic observations to obtain clean signals is an important research topic regarding numerous real acoustic applications. Owing to their strong model capacity in function mapping, deep neural network-based algorithms have been successfully applied in target signal enhancement in acoustic applications. As most target signals carry semantic information encoded in a hierarchal structure in short- and long-term contexts, noise may distort such structures nonuniformly. In most deep neural network-based algorithms, such local and global effects are not explicitly considered in a modeling architecture for signal enhancement. In this article, we propose a temporal attentive pooling (TAP) mechanism combined with a conventional convolutional recurrent neural network (CRNN) model, called TAP-CRNN, which explicitly considers both global and local information for acoustic signal enhancement (ASE). In the TAP-CRNN model, we first use a convolution layer to extract local information from acoustic signals and a recurrent neural network (RNN) architecture to characterize temporal contextual information. Second, we exploit a novel attention mechanism to contextually process salient regions of noisy signals. We evaluate the proposed ASE system using an infant cry dataset. The experimental results confirm the effectiveness of the proposed TAP-CRNN, compared with related deep neural network models, and demonstrate that the proposed TAP-CRNN can more effectively reduce noise components from infant cry signals with unseen background noises at different signal-to-noise levels. We further tested the TAP-CRNN ASE system on a downstream infant cry detection (ICD) system, which determines whether a sound segment is involved in an infant cry event. Experimental results show that TAP-CRNN ASE can effectively reduce the noise components, thereby improving the performance of ICD under noisy conditions.},
  archive      = {J_TAI},
  author       = {Tassadaq Hussain and Wei-Chien Wang and Mandar Gogate and Kia Dashtipour and Yu Tsao and Xugang Lu and Adeel Ahsan and Amir Hussain},
  doi          = {10.1109/TAI.2022.3169995},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {833-842},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A novel temporal attentive-pooling based convolutional recurrent architecture for acoustic signal enhancement},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A supervised-learning assisted computation method for power
system planning. <em>TAI</em>, <em>3</em>(5), 819–832. (<a
href="https://doi.org/10.1109/TAI.2021.3133821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve a low-carbon economy, the comprehensive planning of the early retirement of coal-fired power plants (CFPPs), the construction of renewable energy plants, and the investment in energy storage systems (ESSs) are critical. However, the nonlinear characteristic of the alternating current power flow (ACPF) brings difficulties in solving the long-term planning problem efficiently. Hence, in this article, a learning and optimization integrated framework is presented to help the power sector to reach the emission reduction goal economically while guaranteeing system security and reliability. First, the electricity network transition from the fossil fuel-dominated system to the low-carbon-oriented system is planned. Then, a data-driven method is utilized to regress the power flows, and a method is proposed to integrate the data-driven model into an optimization problem to alleviate the heavy computational burden. Because the penetration of intermittent renewable energy is high in a low-carbon energy system, the security and reliability of the planning decisions are verified in the second level by the N-1 security check. Simulation results reveal that the data-driven method can calculate power flows more accurately than direct current power flow (DCPF) and linearized ACPF models. Also, the supervised-learning method can help reduce computing time. The proposed planning model is verified on the IEEE 30-bus system. Through case studies, it can be concluded that our proposed method can reach the low-carbon goals with the lowest cost, and reliability can be ensured at the same time. Because of the coordinated retirement of CFPPs and the proper planning of ESS, the planned electricity system can cope with uncertainties better.},
  archive      = {J_TAI},
  author       = {Yuechuan Tao and Jing Qiu and Shuying Lai},
  doi          = {10.1109/TAI.2021.3133821},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {819-832},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A supervised-learning assisted computation method for power system planning},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Early stator fault detection and condition identification in
induction motor using novel deep network. <em>TAI</em>, <em>3</em>(5),
809–818. (<a href="https://doi.org/10.1109/TAI.2021.3135799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault diagnosis of induction motor is an important task for both the researchers and industry. The interturn fault is one of the frequented faults and accounts for more than 37% of the failures. Thus, the diagnosis of interturn fault at an early stage has become vital to avoid the catastrophic failures and production losses. In this work, the advantages of deep learning-based methods are explored to detect the interturn fault at an incipient stage. Hybrid architectures has been proposed in this work for incipient inter turn fault diagnosis [i.e., 1D convolution neural network-long short-term memory (1DCNN-LSTM) and 1DCNN-gated recurrent unit (GRU) based methods]. The performance of the hybrid methods has been compared with standalone techniques (1DCNN, LSTM, and GRU), and the results show that the hybrid models (1DCNN-LSTM and 1DCNN-GRU) outperform the individual architectures (in terms of accuracy, sensitivity, and specificity), for fault diagnosis and isolation. The computational time has also been found to be comparable, which is an added advantage for fast diagnosis tasks. The approach combines the feature extraction and classification tasks, to perform early diagnosis, in presence of ambiguous conditions. Here, the incipient diagnosis of fault has been augmented with condition identification i.e., healthy (balanced voltages), healthy (unbalanced voltages), faulty (balanced voltages), and faulty (unbalanced voltages), to effect both early detection and isolation. The performance metrics show the robustness of the proposed method for not only detecting fault, but also identifying such confusing conditions for critical applications, in which induction motors are employed.},
  archive      = {J_TAI},
  author       = {Fatimatelbatoul Husari and Jeevanand Seshadrinath},
  doi          = {10.1109/TAI.2021.3135799},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {809-818},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Early stator fault detection and condition identification in induction motor using novel deep network},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An incremental learning model for mobile robot: From
short-term memory to long-term memory. <em>TAI</em>, <em>3</em>(5),
798–808. (<a href="https://doi.org/10.1109/TAI.2021.3139264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the environmental cognition, how to realize the efficient incremental learning of a mobile robot is a great challenge. The existing methods suffer from the low efficiency. This article proposes a novel methodology to address it. A memory model inspired by the human brains is constructed to realize the transmission of short-term memory (STM) to long-term memory (LTM) in the offline states, thus realize the efficient incremental learning of the mobile robot. Concretely, during the online process, when the sensory information is input to the developmental network (DN), similarity between the new input information and the knowledge memorized in the DN is calculated first. If the similarity is larger than the threshold, this input is transmitted to motor layer and determine the optimal decision. Otherwise, the sensory input will be temporarily stored in a neuron in the STM by an evaluation function. During the offline states, a self-triggering mechanism is designed to trigger the DN to work again. Then, the lateral excitation of the internal neurons is designed to fire more neurons to memorize the knowledge transferred from the STM. Then, the synaptic weights of the new fire neurons are updated, and the STM becomes the LTM, thus realizing the incremental learning. In the following task, if the robot encounters a similar scenario, it can make a quick decision, based on the knowledge learned during the offline states. This continuous learning pattern reduces the training samples used in the network, hence enhancing its learning efficiency. Most importantly, this methodology makes the robot continuously improve its intelligence through the incremental learning, even in off-line states. Extensive simulation and experiment results of the mobile robot navigation demonstrate its potential.},
  archive      = {J_TAI},
  author       = {Dongshu Wang and Kai Yang and Lei Liu and Heshan Wang},
  doi          = {10.1109/TAI.2021.3139264},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {798-808},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An incremental learning model for mobile robot: From short-term memory to long-term memory},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Federated multitask learning for HyperFace. <em>TAI</em>,
<em>3</em>(5), 788–797. (<a
href="https://doi.org/10.1109/TAI.2021.3133816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitask learning (MTL) is a promising field in machine learning owing to its capability to improve the generalization performance of all tasks by sharing knowledge among the related tasks. MTL has attracted a large amount of attention in the multitask-related community. In recent years, with the rapid development of distributed machine learning, MTL in a distributed environment becomes a hot research topic. Although MTL can reap great fruit in a distributed environment, the sensitive information, such as photos and voice recordings of the owner, in distributed data may be leaked during training. Consequently, in the distributed settings, one prominent challenge of MTL is to prevent the disclosure of sensitive private data. In this article, we propose a novel approach, federated multitask learning (FMTL), which applies federated learning to MTL in a distributed environment in order to protect the MTL model from being leaked and to better optimize the utility of the MTL model. In FMTL, all the participants independently train their local models on their own dataset in parallel and only transmit their model updates to the central server for aggregation at every epoch. As a result, their learning accuracy is enhanced beyond what is achieved only on their own training data. In this manner, it can achieve the best tradeoff between the utility and privacy: participants not only preserve the privacy of their own data, but also benefit from the models of other participants. The experimental results on annotated facial landmarks in the wild (AFLW) and annotated faces in the wild (AFW) benchmark datasets verify the effectiveness of our framework.},
  archive      = {J_TAI},
  author       = {Chen Zhang and Hang Bai and Yonggang Zhang and Xinyi Niu and Bin Yu and Yuan Gao and Yu Xie},
  doi          = {10.1109/TAI.2021.3133816},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {788-797},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Federated multitask learning for HyperFace},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning nonstationary time-series with dynamic pattern
extractions. <em>TAI</em>, <em>3</em>(5), 778–787. (<a
href="https://doi.org/10.1109/TAI.2021.3130529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The era of information explosion had prompted the accumulation of a tremendous amount of time-series data, including stationary and nonstationary time-series data. State-of-the-art algorithms have achieved a decent performance in dealing with stationary temporal data. However, traditional algorithms that tackle stationary time-series do not apply to nonstationary series like Forex trading. This article investigates applicable models that can improve the accuracy of forecasting future trends of nonstationary time-series sequences. In particular, we focus on identifying potential models and investigate the effects of recognizing patterns from historical data. We propose a combination of the seq2seq model based on recurrent neural network, along with an attention mechanism and an enriched set features extracted via dynamic time warping and zigzag peak valley indicators. Customized loss functions and evaluating metrics have been designed to focus more on the predicting sequence’s peaks and valley points. Our results show that our model can predict 4-h future trends with high accuracy in the Forex dataset, which is crucial in realistic scenarios to assist foreign exchange trading decision making. We further provide evaluations of the effects of various loss functions, evaluation metrics, model variants, and components on model performance.},
  archive      = {J_TAI},
  author       = {Xipei Wang and Haoyu Zhang and Yuanbo Zhang and Meng Wang and Jiarui Song and Tin Lai and Matloob Khushi},
  doi          = {10.1109/TAI.2021.3130529},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {778-787},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learning nonstationary time-series with dynamic pattern extractions},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-domain modality fusion for dense video captioning.
<em>TAI</em>, <em>3</em>(5), 763–777. (<a
href="https://doi.org/10.1109/TAI.2021.3134190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense video captioning requires localization and description of multiple events in long videos. Prior works detect events in videos solely relying on the visual content and completely ignore the semantics (captions) related to the events. This is undesirable because human-provided captions often also describe events that are visually nonpresent or subtle to detect. In this research, we propose to capitalize on this natural kinship between events and their human-provided descriptions. We propose a semantic contextualization network to encode the visual content of videos by representing it in a semantic space. The representation is further refined to incorporate temporal information and transformed into event descriptors using a hierarchical application of short Fourier transform. Our proposal network exploits the fusion of semantic and visual content enabling it to generate semantically meaningful event proposals. For each proposed event, we attentively fuse its hidden state and descriptors to compute discriminative representation for the subsequent captioning network. Thorough experiments on the standard large-scale ActivityNet Captions dataset and additionally on the YouCook-II dataset show that our method achieves competitive or better performance on multiple popular metrics for the problem.},
  archive      = {J_TAI},
  author       = {Nayyer Aafaq and Ajmal Mian and Wei Liu and Naveed Akhtar and Mubarak Shah},
  doi          = {10.1109/TAI.2021.3134190},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {763-777},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Cross-domain modality fusion for dense video captioning},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incomplete multiview clustering with cross-view feature
transformation. <em>TAI</em>, <em>3</em>(5), 749–762. (<a
href="https://doi.org/10.1109/TAI.2021.3139573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multiview clustering (IMC) has attracted considerable attention as it can flexibly fuse the multiview information when part of the view samples are unobserved. Considering that the main challenge of IMC is the unobserved samples causing the information loss, in this article, we propose a novel IMC model to complete the unobserved samples, named cross-view feature transformation-based incomplete multiview clustering (CFTIMC). Unlike the previous completion methods that simply complete the unobserved samples with the average value or involve a third subspace, CFTIMC directly models the alignment relation between views by the cross-view feature transformation for completion, where only the overlapping views and its overlapping parts are selected for avoiding the affection of the nonoverlapping parts. Simultaneously, this completion process is integrated into multiview graph learning, where they can boost each other. To solve the proposed model, we develop an alternate optimization scheme-based iterative algorithm, together with its computational cost analysis and convergence analysis. Finally, the clustering experiments on several benchmark datasets verify that our method can outperform the related state-of-the-art methods.},
  archive      = {J_TAI},
  author       = {Naiyao Liang and Zuyuan Yang and Lingjiang Li and Zhenni Li and Shengli Xie},
  doi          = {10.1109/TAI.2021.3139573},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {749-762},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Incomplete multiview clustering with cross-view feature transformation},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Random sharing parameters in the global region of
convolutional neural network. <em>TAI</em>, <em>3</em>(5), 738–748. (<a
href="https://doi.org/10.1109/TAI.2021.3136494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building an efficient model with a compact structure and less parameters while preserving its competitive performance is meaningful in the field of neural networks. Traditionally, a unique group of parameters is identified for each convolution layer. Inspired by the universal approximation theorem, in this study, we explore a flexible way to configure the parameters of a convolutional neural network model. First, we set a parameter pool that stores a certain number of parameters, through which we can also control the number of parameters of a neural model. Second, we randomly select a group of continuous position parameters from the pool for each convolution layer. Finally, we perform extensive experiments for the standard architectures of the ResNet and DenseNet on several benchmark datasets. In the experiments, on CIFAR-10, most of the models could perform almost as well as the original ones within a 0.7% decline. On the difficult tasks CIFAR-100 and ImageNet, most of the models perform a little less within an approximately 1.5% decline. In this study, we extend weight sharing from the interior of one feature map to any layers, through which we can control the number of parameters of a neural model.},
  archive      = {J_TAI},
  author       = {Dawei Dai and Zhiguo Zhuang and Jianchao Wei and Shuyin Xia and Yutang Li and Hongfei Zhu},
  doi          = {10.1109/TAI.2021.3136494},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {738-748},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Random sharing parameters in the global region of convolutional neural network},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SoloGAN: Multi-domain multimodal unpaired image-to-image
translation via a single generative adversarial network. <em>TAI</em>,
<em>3</em>(5), 722–737. (<a
href="https://doi.org/10.1109/TAI.2022.3187384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant advances in image-to-image (I2I) translation with generative adversarial networks (GANs), it remains challenging to effectively translate an image to a set of diverse images in multiple target domains using a pair of generator and discriminator. Existing multimodal I2I translation methods adopt multiple domain-specific content encoders for different domains, where each domain-specific content encoder is trained with images from the same domain only. Nevertheless, we argue that the content (domain-invariance) features should be learned from images among all of the domains. Consequently, each domain-specific content encoder of existing schemes fails to extract the domain-invariant features efficiently. To address this issue, we present a flexible and general SoloGAN model for efficient multimodal I2I translation among multiple domains with unpaired data. In contrast to existing methods, the SoloGAN algorithm uses a single projection discriminator with an additional auxiliary classifier and shares the encoder and generator for all domains. As such, the SoloGAN model can be trained effectively with images from all domains so that the domain-invariance content representation can be efficiently extracted. Qualitative and quantitative results over a wide range of datasets against several counterparts and variants of the SoloGAN model demonstrate the merits of the method, especially for challenging I2I translation tasks, i.e., tasks that involve extreme shape variations or need to keep the complex backgrounds unchanged after translations. Furthermore, we demonstrate the contribution of each component using ablation studies.},
  archive      = {J_TAI},
  author       = {Shihua Huang and Cheng He and Ran Cheng},
  doi          = {10.1109/TAI.2022.3187384},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {722-737},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {SoloGAN: Multi-domain multimodal unpaired image-to-image translation via a single generative adversarial network},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intergroup cascade broad learning system with optimized
parameters for chaotic time series prediction. <em>TAI</em>,
<em>3</em>(5), 709–721. (<a
href="https://doi.org/10.1109/TAI.2022.3143079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of chaotic time series can well describe the physical properties and influencing factors of a sequence of numerical data points in successive order and has played an increasingly crucial role in various scientific and engineering communities. These time series data require the prediction model to have strong dynamic feature representation ability, which often leads to excessive calculation burden. As an emerging and effective learning method, a broad learning system (BLS) can be trained quickly by incremental learning, and the system can be reconstructed without requiring a lengthy retraining process, which has been shown to facilitate superior time-saving performance. However, the historical information of time series data is often ignored, resulting in the lack of dynamic feature representation ability of the original BLS algorithm. In this article, a novel intergroup cascade BLS (ICBLS) method and its variant with optimized parameters, termed MOICBLS, are proposed, which can combine the previous knowledge with current information to determine the output results by constructing a novel intergroup cascade structure for the enhancement nodes. The new algorithms try to retain the time-saving advantage of BLS and greatly enhance the extraction ability of dynamic features of chaotic time series data. In addition, it cannot be ignored that the network structure parameters have an unquestionable impact on the performance of a BLS. In response, two conflicting goals of prediction accuracy and diversity are formulated as multiobjective optimization functions in the training phase to generate the optimized network structure parameters needed by multiple candidate models. The experimental results on two benchmark chaotic time series dataset, the Lorenz system and Rossler system, and a geomagnetic disturbance storm time chaotic time series dataset demonstrate that our proposed algorithms are effective for prediction of chaotic time series, and the prediction accuracy is better than some existing approaches.},
  archive      = {J_TAI},
  author       = {Jun Yi and Jiahua Huang and Wei Zhou and Guorong Chen and Meng Zhao},
  doi          = {10.1109/TAI.2022.3143079},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {709-721},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Intergroup cascade broad learning system with optimized parameters for chaotic time series prediction},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context- and sentiment-aware networks for emotion
recognition in conversation. <em>TAI</em>, <em>3</em>(5), 699–708. (<a
href="https://doi.org/10.1109/TAI.2022.3149234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition in conversation (ERC) has promising potential in many fields, such as recommendation systems, man–machine interaction, and medical care. In contrast to other emotion identification tasks, conversation is essentially a process of dynamic interaction in which people often convey emotional messages relying on context and common-sense knowledge. In this article, we propose a context- and sentiment-aware framework, termed Sentic GAT, to solve this challenge. In Sentic GAT, common-sense knowledge is dynamically represented by the context- and sentiment-aware graph attention mechanism based on sentimental consistency, and context information is captured by the dialogue transformer (DT) with hierarchical multihead attention (HMAT), where HMAT is used to obtain the dependency of historical utterances on themselves and other utterances for better context representation. Additionally, we explore a contrastive loss to discriminate context-free and context-sensitive utterances in emotion identification to enhance context representation in straightforward conversations that directly express ideas. The experimental results show that context and sentimental information can promote the representation of common-sense knowledge, and the intra- and inter-dependency of contextual utterances effectively improve the performance of Sentic GAT. Moreover, our Sentic GAT using emotional intensity outperforms the most advanced model on the tested datasets.},
  archive      = {J_TAI},
  author       = {Geng Tu and Jintao Wen and Cheng Liu and Dazhi Jiang and Erik Cambria},
  doi          = {10.1109/TAI.2022.3149234},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {699-708},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Context- and sentiment-aware networks for emotion recognition in conversation},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Smoothed generalized dirichlet: A novel count-data model for
detecting emotional states. <em>TAI</em>, <em>3</em>(5), 685–698. (<a
href="https://doi.org/10.1109/TAI.2021.3120043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose novel approaches to deal with the problem of burstiness, the challenge of count-data sparseness, and the curse of dimensionality. We introduce a smoothed generalized Dirichlet distribution that is a smoothed variant of the generalized Dirichlet distribution and a generalization of the smoothed Dirichlet. We provide different learning methods based on mixture models and agglomerative clustering-based geometrical information: Kullback–Leibler divergence, Fisher metric, and Bhattacharyya distance. Moreover, we show that the new smoothed generalized Dirichlet could be considered as a prior to the multinomial, which generates a new distribution for count data that we call the smoothed generalized Dirichlet multinomial. In particular, we present an approximation based on Taylor series expansion for better performance and optimized running time in the case of high-dimensional count data. The proposed models are evaluated through two emotion detection applications: disaster-tweet-related emotions and pain intensity estimation. Experiments show the efficiency and the robustness of our approaches when dealing with texts, videos, and images.},
  archive      = {J_TAI},
  author       = {Fatma Najar and Nizar Bouguila},
  doi          = {10.1109/TAI.2021.3120043},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {685-698},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Smoothed generalized dirichlet: A novel count-data model for detecting emotional states},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A review of text style transfer using deep learning.
<em>TAI</em>, <em>3</em>(5), 669–684. (<a
href="https://doi.org/10.1109/TAI.2021.3115992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Style is an integral component of a sentence indicated by the choice of words a person makes. Different people have different ways of expressing themselves; however, they adjust their speaking and writing style to a social context, an audience, an interlocutor, or the formality of an occasion. Text style transfer is defined as a task of adapting and/or changing the stylistic manner in which a sentence is written, while preserving the meaning of the original sentence. A systematic review of text style transfer methodologies using deep learning is presented in this article. We point out the technological advances in deep neural networks that have been the driving force behind current successes in the fields of natural language understanding and generation. The review is structured around two key stages in the text style transfer process, namely, representation learning and sentence generation in a new style. The discussion highlights the commonalities and differences between proposed solutions as well as challenges and opportunities that are expected to direct and foster further research in the field.},
  archive      = {J_TAI},
  author       = {Martina Toshevska and Sonja Gievska},
  doi          = {10.1109/TAI.2021.3115992},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {669-684},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A review of text style transfer using deep learning},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward text data augmentation for sentiment analysis.
<em>TAI</em>, <em>3</em>(5), 657–668. (<a
href="https://doi.org/10.1109/TAI.2021.3114390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A significant part of natural language processing (NLP) techniques for sentiment analysis is based on supervised methods, which are affected by the quality of data. Therefore, sentiment analysis needs to be prepared for data quality issues, such as imbalance and lack of labeled data. Data augmentation methods, widely adopted in image classification tasks, include data-space solutions to tackle the problem of limited data and enhance the size and quality of training datasets to provide better models. In this work, we study the advantages and drawbacks of text augmentation methods such as easy data augmentation, back-translation, BART, and pretrained data augmentor) with recent classification algorithms (long short-term memory, convolutional neural network, bidirectional encoder representations of transformers, support vector machine, gated recurrent units, random forests, and enhanced language representation with informative entities, that have attracted sentiment-analysis researchers and industry applications. We explored seven sentiment-analysis datasets to provide scenarios of imbalanced datasets and limited data to discuss the influence of a given classifier in overcoming these problems, and provide insights into promising combinations of transformation, paraphrasing, and generation methods of sentence augmentation. The results revealed improvements from the augmented dataset, mainly for reduced datasets. Furthermore, when balanced by augmenting the minority class, the datasets were found to have improved quality, leading to more robust classifiers. The contributions to this article include the taxonomy of NLP augmentation methods and their efficiency over several classifiers from recent research trends in sentiment analysis and related fields.},
  archive      = {J_TAI},
  author       = {Hugo Queiroz Abonizio and Emerson Cabrera Paraiso and Sylvio Barbon},
  doi          = {10.1109/TAI.2021.3114390},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {657-668},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Toward text data augmentation for sentiment analysis},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bibliometric review of soft computing for recommender
systems and sentiment analysis. <em>TAI</em>, <em>3</em>(5), 642–656.
(<a href="https://doi.org/10.1109/TAI.2021.3116551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soft computing, which focuses on approximate models and provides solutions to complicated real-life issues, has gained increasing momentum in application-specific domains, such as sentiment analysis and recommender systems, to emulate cognitive processes behind decision-making. In this work, bibliometrics and structural topic modeling (STM) were adopted to analyze the text contents of research articles concerning soft computing for sentiment analysis and recommender systems. Results indicated that this research field had experienced a dramatic increase in both quantity and quality as measured by scientific output and their received citations. Using STM, we identified 17 research topics frequently discussed within the analyzed articles. The analysis of annual topic prevalence indicated a shift in research foci from recommender applications to sentiment analysis and a growing interest in soft computing. This study served as a guideline for those seeking to contribute to research on soft computing for sentiment analysis and recommender systems. We also made methodological contributions by combining the leading-edge text mining algorithms to make the time-honored bibliometrics adaptive to the analysis of large quantities of unstructured texts beyond structured publication data statistics.},
  archive      = {J_TAI},
  author       = {Xieling Chen and Haoran Xie and Jingjing Wang and Zongxi Li and Gary Cheng and Man Leung Wong and Fu Lee Wang},
  doi          = {10.1109/TAI.2021.3116551},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {642-656},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A bibliometric review of soft computing for recommender systems and sentiment analysis},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guest editorial: Sentiment analysis as a multidisciplinary
research area. <em>TAI</em>, <em>3</em>(5), 638–641. (<a
href="https://doi.org/10.1109/TAI.2022.3205985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special section focus on bringing multidisciplinary knowledge into sentiment analysis. The last two decades have witnessed an enormous amount of research works on sentiment analysis and significant progress has been made. In terms of depth, finer-grained semantic schemas are defined, such as aspect, target, and category. In terms of broadness, multimodal sentiment analysis touches audio and video channels. Conversational sentiment analysis considers contextual and time-dependent relations and domain-specific sentiment analysis settings tackles real-life challenges, including but not limited to user profiling, financial prediction, abusive language detection, and mental health.},
  archive      = {J_TAI},
  author       = {Erik Cambria and Frank Xing and Mike Thelwall and Roy Welsch},
  doi          = {10.1109/TAI.2022.3205985},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {638-641},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Guest editorial: Sentiment analysis as a multidisciplinary research area},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Perturbed composite attention model for macular optical
coherence tomography image classification. <em>TAI</em>, <em>3</em>(4),
625–635. (<a href="https://doi.org/10.1109/TAI.2021.3135797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a deep architecture stemming from a perturbed composite attention mechanism with the following two novel attention modules: Multilevel perturbed spatial attention (MPSA) and multidimension attention (MDA) for macular optical coherence tomography (OCT) image (scan) classification. MPSA is designed by adding positive perturbations to the attention layers, thereby amplifying both the salient regions of input images and discriminative features obtained from intermediate layers of the network. On the other hand, the MDA encodes the normalized interdependency of spatial information among various channels of the extracted feature maps. The perturbed composite attention mechanism enables the new architecture to automatically extract relevant diagnostic features at different levels of feature representation resulting in the superior classification of macular diseases such as age-related macular degeneration (AMD), diabetic macular edema (DME), and choroidal neovascularization (CNV). The proposed end-to-end trainable architecture does not require preprocessing steps, such as region of interest extraction, denoising, and retinal flattening, making the network more robust and fully automatic. Experimental results on three macular OCT datasets and ablation studies show that our proposed network outperforms the current state-of-the-art methodologies.},
  archive      = {J_TAI},
  author       = {Sapna S. Mishra and Bappaditya Mandal and Niladri B. Puhan},
  doi          = {10.1109/TAI.2021.3135797},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {625-635},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Perturbed composite attention model for macular optical coherence tomography image classification},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CSNAS: Contrastive self-supervised learning neural
architecture search via sequential model-based optimization.
<em>TAI</em>, <em>3</em>(4), 609–624. (<a
href="https://doi.org/10.1109/TAI.2021.3121663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel contrastive self-supervised neural architecture search (NAS) algorithm, which completely alleviates the expensive costs of data labeling inherited from supervised learning. Our algorithm capitalizes on the effectiveness of self-supervised learning for image representations, which is an increasingly crucial topic of computer vision. First, using only a small amount of unlabeled train data under contrastive self-supervised learning allows us to search on a more extensive search space, discovering better neural architectures without surging the computational resources. Second, we entirely relieve the cost for labeled data (by contrastive loss) in the search stage without compromising architectures’ final performance in the evaluation phase. Finally, we tackle the inherent discrete search space of the NAS problem by sequential model-based optimization via the tree-parzen estimator, enabling us to significantly reduce the computational expense response surface. An extensive number of experiments empirically show that our search algorithm can achieve state-of-the-art results with better efficiency in data labeling cost, searching time, and accuracy in final validation.},
  archive      = {J_TAI},
  author       = {Nam Nguyen and J. Morris Chang},
  doi          = {10.1109/TAI.2021.3121663},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {609-624},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {CSNAS: Contrastive self-supervised learning neural architecture search via sequential model-based optimization},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On a sparse shortcut topology of artificial neural networks.
<em>TAI</em>, <em>3</em>(4), 595–608. (<a
href="https://doi.org/10.1109/TAI.2021.3128132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In established network architectures, shortcut connections are often used to take the outputs of earlier layers as additional inputs to later layers. Despite the extraordinary effectiveness of shortcuts, there remain open questions on the mechanism and characteristics. For example, why are shortcuts powerful? Why do shortcuts generalize well? In this article, we investigate the expressivity and generalizability of a novel sparse shortcut topology. First, we demonstrate that this topology can empower a one-neuron-wide deep network to approximate any univariate continuous function. Then, we present a novel width-bounded universal approximator in contrast to depth-bounded universal approximators and extend the approximation result to a family of equally competent networks. Furthermore, with generalization bound theory, we show that the proposed shortcut topology enjoys excellent generalizability. Finally, we corroborate our theoretical analyses by comparing the proposed topology with popular architectures, including ResNet and DenseNet, on well-known benchmarks and perform a saliency map analysis to interpret the proposed topology. Our work helps understand the role of shortcuts and suggests further opportunities to innovate neural architectures.},
  archive      = {J_TAI},
  author       = {Feng-Lei Fan and Dayang Wang and Hengtao Guo and Qikui Zhu and Pingkun Yan and Ge Wang and Hengyong Yu},
  doi          = {10.1109/TAI.2021.3128132},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {595-608},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {On a sparse shortcut topology of artificial neural networks},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiadvisor reinforcement learning for multiagent
multiobjective smart home energy control. <em>TAI</em>, <em>3</em>(4),
581–594. (<a href="https://doi.org/10.1109/TAI.2021.3125918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective automated smart home energy control is essential for smart grid approaches to demand response (DR). This is a multiobjective adaptive control problem because it balances an appliance’s primary objective with demand response objectives. One challenge comes from the heterogeneous nature of objectives, requiring tradeoffs between comfort, cost, and other objectives. Another challenge comes from the heterogeneous dynamics, which result from different environments and the different appliances used. Another challenge is nonstationary nature of dynamics and rewards due to seasonal changes and time-varying user preferences. Finally, we consider computational challenges, required by the real-time aspect of the control problem, particularly notable due to “the curse of dimensionality.” We propose a multiagent multiadvisor reinforcement learning framework to address these challenges. We design a smart-home simulation to demonstrate the performance (in terms of weighted reward) of our approach relative to competitive single-objective reinforcement learning algorithms. Furthermore, we theoretically and empirically demonstrate the linear computational scalability of the algorithm. Finally, we identify the need for key performance measures of the proposed system by considering the effect of selected preferences on agents. Overall, the proposed algorithm is reasonably competitive with conventional approaches while simultaneously enabling behavior changes with change in preferences without requiring more data.},
  archive      = {J_TAI},
  author       = {Andrew Tittaferrante and Abdulsalam Yassine},
  doi          = {10.1109/TAI.2021.3125918},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {581-594},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multiadvisor reinforcement learning for multiagent multiobjective smart home energy control},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Potential impacts of smart homes on human behavior: A
reinforcement learning approach. <em>TAI</em>, <em>3</em>(4), 567–580.
(<a href="https://doi.org/10.1109/TAI.2021.3127483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart homes are becoming increasingly popular as a result of advances in machine learning and cloud computing. Devices, such as smart thermostats and speakers, are now capable of learning from user feedback and adaptively adjust their settings to human preferences. Nonetheless, these devices might in turn impact human behavior. To investigate the potential impacts of smart homes on human behavior, we simulate a series of hierarchical-reinforcement learning-based human models capable of performing various activities—namely, setting temperature and humidity for thermal comfort inside a Q-Learning-based smart home model. We then investigate the possibility of the human models’ behaviors being altered as a result of the smart home and the human model adapting to one another. For our human model, the activities are based on hierarchical-reinforcement learning. This allows the human to learn how long it must continue a given activity and decide when to leave it. We then integrate our human model in the environment along with the smart home model and perform rigorous experiments considering various scenarios involving a model of a single human and models of two different humans with the smart home. Our experiments show that with the smart home, the human model can exhibit unexpected behaviors such as frequent changing of activities and an increase in the time required to modify the thermal preferences. With two human models, we interestingly observe that certain combinations of models result in normal behaviors, while other combinations exhibit the same unexpected behaviors as those observed from the single human experiment.},
  archive      = {J_TAI},
  author       = {Shashi Suman and Ali Etemad and Francois Rivest},
  doi          = {10.1109/TAI.2021.3127483},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {567-580},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Potential impacts of smart homes on human behavior: A reinforcement learning approach},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Traded control of human–machine systems for sequential
decision-making based on reinforcement learning. <em>TAI</em>,
<em>3</em>(4), 553–566. (<a
href="https://doi.org/10.1109/TAI.2021.3127857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential decision-making (SDM) is a common type of decision-making problem with sequential and multistage characteristics. Among them, the learning and updating of policy are the main challenges in solving SDM problems. Unlike previous machine autonomy driven by artificial intelligence alone, we improve the control performance of SDM tasks by combining human intelligence and machine intelligence. Specifically, this article presents a paradigm of a human–machine traded control systems based on reinforcement learning methods to optimize the solution process of sequential decision problems. By designing the idea of autonomous boundary and credibility assessment, we enable humans and machines at the decision-making level of the systems to collaborate more effectively. And the arbitration in the human–machine traded control systems introduces the Bayesian neural network and the dropout mechanism to consider the uncertainty and security constraints. Finally, experiments involving machine traded control, human traded control were implemented. The preliminary experimental results of this article show that our traded control method improves decision-making performance and verifies the effectiveness for SDM problems.},
  archive      = {J_TAI},
  author       = {Qianqian Zhang and Yu Kang and Yun-Bo Zhao and Pengfei Li and Shiyi You},
  doi          = {10.1109/TAI.2021.3127857},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {553-566},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Traded control of Human–Machine systems for sequential decision-making based on reinforcement learning},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Histogram layers for texture analysis. <em>TAI</em>,
<em>3</em>(4), 541–552. (<a
href="https://doi.org/10.1109/TAI.2021.3135804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An essential aspect of texture analysis is the extraction of features that describe the distribution of values in local, spatial regions. We present a localized histogram layer for artificial neural networks. Instead of computing global histograms as done previously, the proposed histogram layer directly computes the local, spatial distribution of features for texture analysis, and parameters for the layer are estimated during backpropagation. We compare our method to state-of-the-art texture encoding methods such as: The deep encoding pooling network, deep texture encoding network, Fisher vector convolutional neural network, and multilevel texture encoding and representation. We used three material/texture datasets: 1) The describable texture dataset; 2) an extension of the ground terrain in outdoor scenes dataset; and 3) a subset of the materials in context dataset. Results indicate that the inclusion of the proposed histogram layer improves performance.},
  archive      = {J_TAI},
  author       = {Joshua Peeples and Weihuang Xu and Alina Zare},
  doi          = {10.1109/TAI.2021.3135804},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {541-552},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Histogram layers for texture analysis},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ArcText: A unified text approach to describing convolutional
neural network architectures. <em>TAI</em>, <em>3</em>(4), 526–540. (<a
href="https://doi.org/10.1109/TAI.2021.3128502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The superiority of convolutional neural networks (CNNs) largely relies on their architectures that are usually manually crafted with extensive human expertise. Unfortunately, such kind of domain knowledge is not necessarily owned by every interested user. Data mining on existing CNNs can discover useful patterns and fundamental comments from their architectures, providing researchers with strong prior knowledge to design effective CNN architectures when they have no expertise in CNNs. There are various state-of-the-art data mining algorithms at hand, while there is only rare work on mining CNN architectures. One of the main reasons is the gap between CNN architectures and data mining algorithms. Specifically, the current CNN architecture descriptions cannot be exactly vectorized to feed to a data mining algorithm. In this article, we propose a unified approach, named ArcText, to describing CNN architectures based on text. Particularly, four different units and an ordering method have been elaborately designed in ArcText, to uniquely describe the same CNN architecture with sufficient information. Also, the resulted description can be exactly converted back to the corresponding CNN architecture. ArcText bridges the gap between CNN architectures and data mining researchers, and has the potential to be utilized to wider scenarios.},
  archive      = {J_TAI},
  author       = {Yanan Sun and Gary G. Yen and Bing Xue and Mengjie Zhang and Jiancheng Lv},
  doi          = {10.1109/TAI.2021.3128502},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {526-540},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {ArcText: A unified text approach to describing convolutional neural network architectures},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ignorance is bliss: Exploring defenses against
invariance-based attacks on neural machine translation systems.
<em>TAI</em>, <em>3</em>(4), 518–525. (<a
href="https://doi.org/10.1109/TAI.2021.3123931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses an invariance-based attack on the transformer, a state-of-the-art neural machine translation (NMT) system. Such attacks make multiple changes to the source sentence with the goal of keeping the predicted translation unchanged. Since the gold translation is not available for the adversarial sentences, tackling invariance-based attacks is a challenging task. We propose two contrasting defense strategies for the same, learn to deal and learn to ignore . In learn to deal , NMT system is trained not to predict the same translation for a clean text and its noisy counterpart, whereas in learn to ignore , NMT system is trained to output a dummy sentence in the target language whenever it encounters a noisy text. The experiments on two language pairs, English–German (en–de) and English–French (en–fr), show that learn to deal strategy reduces the attack success rate from 84.0% to 62.2% for en–de and from 84.6% to 73.8% for en–fr, whereas learn to ignore strategy reduces the attack success rate from 84.0% to 27.2% for en–de and from 84.6% to 37.0% for en–fr.},
  archive      = {J_TAI},
  author       = {Akshay Chaturvedi and Abhisek Chakrabarty and Masao Utiyama and Eiichiro Sumita and Utpal Garain},
  doi          = {10.1109/TAI.2021.3123931},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {518-525},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Ignorance is bliss: Exploring defenses against invariance-based attacks on neural machine translation systems},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient temporal piecewise-linear numeric planning with
lazy consistency checking. <em>TAI</em>, <em>3</em>(4), 506–517. (<a
href="https://doi.org/10.1109/TAI.2022.3146797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal planning often involves numeric effects that are directly proportional to their action’s duration. These include continuous effects, where a numeric variable is subjected to a rate of change while the action is being executed, and discrete duration-dependent effects, where the variable is updated instantaneously but the magnitude of such change is computed from the action’s duration. When these effects are linear, state-of-the-art temporal planners often make use of linear programming to ensure that these numeric updates are consistent with the chosen start times and durations of the plan’s actions. This is typically done for each evaluated state as a part of the search process. This exhaustive approach is not scalable to solve real-world problems that require long plans, because the size of the linear program becomes larger and slower to solve. In this article, we propose techniques that minimize this overhead by computing these checks more selectively and formulating linear programs that have a smaller footprint. The effectiveness of these techniques is demonstrated on domains that use a mix of discrete and continuous effects, which is typical of real-world planning problems. The resultant planner also outperforms most state-of-the-art temporal–numeric and hybrid planners, in terms of both coverage and scalability.},
  archive      = {J_TAI},
  author       = {Josef Bajada and Maria Fox and Derek Long},
  doi          = {10.1109/TAI.2022.3146797},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {506-517},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Efficient temporal piecewise-linear numeric planning with lazy consistency checking},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Balanced graph cut with exponential inter-cluster
compactness. <em>TAI</em>, <em>3</em>(4), 498–505. (<a
href="https://doi.org/10.1109/TAI.2021.3123126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, balanced graph-based clustering has been a hot issue in clustering domain, but the balanced theoretical guarantees of previous models are either qualitative or based on a probabilistic random graph, which may fail to various real data. To make up this vital flaw, this letter explores a novel balanced graph-based clustering model, named exponential-cut (Exp-Cut), via redesigning the intercluster compactness based on the exponential transformation &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\exp \lbrace \mu x\rbrace$&lt;/tex-math&gt;&lt;/inline-formula&gt; . It is worth noting that exponential transformation not only provides a bounded balanced tendency for Exp-Cut, but also helps Exp-Cut to achieve balanced results on an arbitrary graph via adjusting its curvature &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\mu$&lt;/tex-math&gt;&lt;/inline-formula&gt; . To solve the optimization problem involved in Exp-Cut model, an efficient heuristic solver is proposed and the computational complexity is &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\mathcal {O}(n^2)$&lt;/tex-math&gt;&lt;/inline-formula&gt; per iteration. Experimental results demonstrate that our proposals outperform competitors on all benchmarks with respect to clustering performance, balanced property, and efficiency.},
  archive      = {J_TAI},
  author       = {Danyang Wu and Feiping Nie and Jitao Lu and Rong Wang and Xuelong Li},
  doi          = {10.1109/TAI.2021.3123126},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {498-505},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Balanced graph cut with exponential inter-cluster compactness},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep multiscale multi-instance networks with regional
scoring for mammogram classification. <em>TAI</em>, <em>3</em>(3),
485–496. (<a href="https://doi.org/10.1109/TAI.2021.3136146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mammography is one of the most commonly used methods for breast cancer screening. Most mammogram classification or segmentation models are trained with additional manual segmentation annotation data, which is difficult to obtain. Meanwhile, these models rarely take into account the fact that lesion features may vary at different scales. Moreover, most models have difficulty in quantifying the effect of single lesion regions on prediction results. To solve the above problems, we propose deep multiscale multi-instance networks with regional scoring. This model classifies mammograms automatically by image labels without using any other annotation information during the training procedure. The proposed multiscale structures include large scale, medium scale, and small scale, and they can extract features of mammograms from global to local. Because there may be more than one breast lesion region in a mammogram, the proposed multi-instance structure divides the image into multiple regions and selects few regions of interest to represent the whole image together. The regional scoring mechanism quantifies the contribution of each region to the prediction results, and thus, makes the results interpretable. In addition, the proposed method can roughly localize the lesion area by the score of each region. The experimental results on a publicly available dataset and a private dataset have shown that the proposed method achieved the highest AUC on both datasets compared with the state-of-the-art models.},
  archive      = {J_TAI},
  author       = {Wenjie Liu and Xin Shu and Lei Zhang and Dong Li and Qing Lv},
  doi          = {10.1109/TAI.2021.3136146},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {485-496},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep multiscale multi-instance networks with regional scoring for mammogram classification},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attacks on data-driven process monitoring systems: Subspace
transfer networks. <em>TAI</em>, <em>3</em>(3), 470–484. (<a
href="https://doi.org/10.1109/TAI.2022.3145335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of information technology, intelligent upgrading of the manufacturing industry has broken the closed environment of traditional industrial control systems (ICS); thus, the information security of ICS has been seriously threatened. As part of ICS, the process monitoring system (PMS) is heavily subject to external risks. Data-driven PMSs have been widely used as initial lines of defense to ensure ICS safety. Once the PMS is under attack, the consequences on the whole ICS will be unimaginable. Unfortunately, the safety issues of the PMS have received inadequate attention. This article reveals PMS’s vulnerabilities through effective attacks. A novel method called subspace transfer network (STN) is proposed to conduct adversarial and poisoning attacks on the PMS simultaneously. Then the attack task flow is defined and explained to make online adversarial attacks and data poisoning on PMS. Meanwhile, aiming at two poisoning goals, targeted and untargeted attacks of STN are designed, respectively. Finally, the PMS’s fragility is verified in two industrial benchmarks.},
  archive      = {J_TAI},
  author       = {Xiaoyu Jiang and Zhiqiang Ge},
  doi          = {10.1109/TAI.2022.3145335},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {470-484},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Attacks on data-driven process monitoring systems: Subspace transfer networks},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bidirectional gated recurrent unit-based lower upper bound
estimation method for wind power interval prediction. <em>TAI</em>,
<em>3</em>(3), 461–469. (<a
href="https://doi.org/10.1109/TAI.2021.3123928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality interval prediction is helpful to accurately capture the uncertainty of wind power generation and provide support to grid dispatchers and operators. As an effective and reliable prediction interval (PI) construction framework, lower upper bound estimation (LUBE) method is used in forecasting tasks. This article proposes a new data-driven PI construction method for wind power prediction based on LUBE theory and a bidirectional gated recurrent unit (GRU) neural network, which integrates a loss function based on likelihood for model training. In this framework, the bidirectional GRU acts as the core predictor, and learns the long-term dependence of wind power time series in chronological and reverse chronological order. The learned features are merged by using a fully connected layer to generate the upper and the lower bounds of the target PI. The proposed method is evaluated on a real wind farm dataset. Numerical results show the superiority of the proposed method by comparing with the traditional LUBE method, long short-term memory-based LUBE method, mean-variance estimation method, and an emerging efficient gradient descent method.},
  archive      = {J_TAI},
  author       = {Fang Liu and Qing Tao and Dechang Yang and Denis Sidorov},
  doi          = {10.1109/TAI.2021.3123928},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {461-469},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Bidirectional gated recurrent unit-based lower upper bound estimation method for wind power interval prediction},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robot path planning via neural-network-driven prediction.
<em>TAI</em>, <em>3</em>(3), 451–460. (<a
href="https://doi.org/10.1109/TAI.2021.3119890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel heuristic method named neural-network-driven prediction (NEED) for robot path planning problems. Different from classical heuristic methods, NEED is not designed for some specific environments, which can be applied to various environments. NEED has an encoder–decoder structure, which consists of three modules: convolutional neural network backbone, spatial pooling module, and decoder module. By learning from a number of successful path planning cases, NEED can learn to analyze the environment structure and predict the promising search region for the new path planning problem. This predicted region serves as a heuristic to guide the search direction of path planning algorithms. A series of simulation experiments are conducted to demonstrate that NEED significantly improves the algorithm performance on solving new path planning problems. Meanwhile, NEED can also be applied to highly dynamic environments since it can output the prediction results at a speed of over 100 Hz on a laptop.},
  archive      = {J_TAI},
  author       = {Jiankun Wang and Jianbang Liu and Weinan Chen and Wenzheng Chi and Max Q.-H. Meng},
  doi          = {10.1109/TAI.2021.3119890},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {451-460},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Robot path planning via neural-network-driven prediction},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evolutionary neural architecture search for automatic
esophageal lesion identification and segmentation. <em>TAI</em>,
<em>3</em>(3), 436–450. (<a
href="https://doi.org/10.1109/TAI.2021.3134600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic esophageal lesion identification (ESEI) is of great importance to clinically aid the endoscopists with the early detection of esophageal cancer. However, accurate identification of esophageal lesion is challenging due to the varying shape, size, illumination condition, and complex background with artifacts in endoscopic images. Although deep neural network based approaches have considerably boosted the performance by automatically learning features from esophageal images, the configuration of the network architecture is highly dependent on domain expertise and is a daunting task to be manually tuned. In this article, we propose an evolutionary algorithm based approach to search for the optimal multitask network architecture for ESEI. Different from existing studies, we first design a multitask network search space, which considers the lesion identification as two steps including esophageal image classification and esophageal lesion segmentation. In particular, the input image resolution is covered in the search space, and the classification utilizes both downsampled and upsampled features. Besides, to avoid scratch training of sampled network architectures in the evolutionary algorithm, the one-shot supernet strategy is developed for searching the optimal network architecture. Results from the performed experiments on a collected sizeable clinical esophageal image dataset show that the proposed method improves on the state of the art on all measured metrics.},
  archive      = {J_TAI},
  author       = {Yao Zhou and Xianglei Yuan and Xiaozhi Zhang and Wei Liu and Yu Wu and Gary G. Yen and Bing Hu and Zhang Yi},
  doi          = {10.1109/TAI.2021.3134600},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {436-450},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Evolutionary neural architecture search for automatic esophageal lesion identification and segmentation},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). System neural network: Evolution and change based structure
learning. <em>TAI</em>, <em>3</em>(3), 426–435. (<a
href="https://doi.org/10.1109/TAI.2022.3143778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {System evolution analytics with artificial neural networks is a challenging and path-breaking direction, which could ease intelligent processes for systems that evolve over time. In this article, we contribute an approach to do Evolution and Change Learning (ECL), which uses an evolution representor and forms a System Neural Network (SysNN). We proposed an algorithm System Structure Learning , which is divided in two steps. First step uses the evolution representor as an Evolving Design Structure Matrix (EDSM) for intelligent design learning. Second step uses a Deep Evolution Learner that learns from evolution and changes patterns of an EDSM to generate Deep SysNN. The result demonstrates application of the proposed approach to analyze four real-world system domains: software, natural-language, retail market, and movie genre. We achieved significant learning over highly imbalanced datasets. The learning from previous states formed SysNN as a feed-forward neural network, and then memorized information as an output matrix to recommend entity-connections.},
  archive      = {J_TAI},
  author       = {Animesh Chaturvedi and Aruna Tiwari and Shubhangi Chaturvedi and Pietro Liò},
  doi          = {10.1109/TAI.2022.3143778},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {426-435},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {System neural network: Evolution and change based structure learning},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed semisupervised partial label learning over
networks. <em>TAI</em>, <em>3</em>(3), 414–425. (<a
href="https://doi.org/10.1109/TAI.2022.3148059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial label learning (PLL) deals with the classification from sufficient training data associated with a candidate set of labels but not the only correct one. In this article, we focus on PLL with some ambiguously labeled and many unlabeled data collected from multiple nodes distributed over a network. To solve this problem, a distributed semisupervised PLL (dS &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$^2$&lt;/tex-math&gt;&lt;/inline-formula&gt; PLL) framework is formulated, in which the weighted logistic loss with respect to partially labeled (PL) and unlabeled data is utilized. In the proposed dS &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$^2$&lt;/tex-math&gt;&lt;/inline-formula&gt; PLL algorithm, the parameters of the classifier are adapted in a collaborative manner, meanwhile the weights of each training sample and the ground-truth confidence of candidate labels are adaptively learned to disambiguate the correct label from the candidate label set. The performance of the proposed algorithm is analyzed theoretically and verified by simulations on both synthetic and real datasets. Results show that the proposed algorithm achieves good classification performance and robustness to the ambiguity in the labels.},
  archive      = {J_TAI},
  author       = {Ying Liu and Zhen Xu and Chen Zhang},
  doi          = {10.1109/TAI.2022.3148059},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {414-425},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Distributed semisupervised partial label learning over networks},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spherical linguistic petri nets for knowledge representation
and reasoning under large group environment. <em>TAI</em>,
<em>3</em>(3), 402–413. (<a
href="https://doi.org/10.1109/TAI.2022.3140282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy Petri nets (FPNs) are a promising modeling tool for knowledge representation and reasoning of rule-based expert systems. However, there exist limitations in representing ambiguous knowledge and performing approximate inference in traditional FPNs. Additionally, knowledge parameters are usually provided by some experts in existing FPN methods. In response to these issues, a new version of FPNs, named spherical linguistic Petri nets (SLPNs), is introduced in this article for knowledge representation and reasoning in the large group context. To this end, spherical linguistic sets are applied to capture imprecise knowledge and represent the uncertainty of experts’ judgements. Furthermore, a large group knowledge acquisition approach is developed to determine knowledge parameters. A bidirectional inference algorithm is developed for implementing the reasoning process and identifying the root causes of an appointed event. Finally, the efficacy and superiority of our developed SLPNs are illustrated by a realistic example regarding stampede risk level assessment in a high-speed railway station.},
  archive      = {J_TAI},
  author       = {Xun Mou and Ling-Xiang Mao and Hu-Chen Liu and MengChu Zhou},
  doi          = {10.1109/TAI.2022.3140282},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {402-413},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Spherical linguistic petri nets for knowledge representation and reasoning under large group environment},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed GAN: Toward a faster
reinforcement-learning-based architecture search. <em>TAI</em>,
<em>3</em>(3), 391–401. (<a
href="https://doi.org/10.1109/TAI.2021.3133509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the existing reinforcement learning (RL)-based neural architecture search (NAS) methods for a generative adversarial network (GAN), both the generator and the discriminator architecture are usually treated as the search objects. In this article, we take a different perspective to propose an approach by treating the generator as the search objective and the discriminator as the judge to evaluate the performance of the generator architecture. Consequently, we can convert this NAS problem to a GAN-style problem, similar to using a controller to generate sequential data via reinforcement learning in a sequence GAN, except that the controller in our methods generates serialized data information of architecture. Furthermore, we adopt an RL-based distributed search method to update the controller parameters &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\theta$&lt;/tex-math&gt;&lt;/inline-formula&gt; . Generally, the reward value is calculated after the whole architecture searched, but as another novelty in this article, we employ the reward shaping method to judge the intermediate reward and assign it to every cell in the architecture to encourage the diversity and the integrity of all cells. The main contribution of this article is to provide a novel performance estimation mechanism, which could speed up the efficiency of architecture search, and improve the searching results with specific supplementary strategies. Crucially, this estimation mechanism can be applied to most RL-based NAS methods for the GAN. The experiments demonstrate that our methods achieve satisfactory results against our design objectives.},
  archive      = {J_TAI},
  author       = {Jiachen Shi and Yi Fan and Guoqiang Zhou and Jun Shen},
  doi          = {10.1109/TAI.2021.3133509},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {391-401},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Distributed GAN: Toward a faster reinforcement-learning-based architecture search},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quick learning mechanism with cross-domain adaptation for
intelligent fault diagnosis. <em>TAI</em>, <em>3</em>(3), 381–390. (<a
href="https://doi.org/10.1109/TAI.2021.3123935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fault diagnostic model trained for a laboratory case machine fails to perform well on the industrial machines running under variable operating conditions. For every new operating condition of such machines, a new diagnostic model has to be trained which is a time-consuming and uneconomical process. Therefore, we propose a quick learning mechanism that can transform the existing diagnostic model into a new model suitable for industrial machines operating in different conditions. The proposed method uses the Net2Net transformation followed by a fine-tuning to cancel/minimize the maximum mean discrepancy between the new data and the previous one. The fine-tuning of the model requires a very less amount of labeled target samples and very few iterations of training. Therefore, the proposed method is capable of learning the new target data pattern quickly. The effectiveness of the proposed fault diagnosis method has been demonstrated on the Case Western Reserve University dataset, Intelligent Maintenance Systems bearing dataset, and Paderborn university dataset under the wide variations of the operating conditions. It has been validated that the diagnostic model trained on artificially damaged fault datasets can be used to quickly train another model for a real damage dataset.},
  archive      = {J_TAI},
  author       = {Arun K. Sharma and Nishchal K. Verma},
  doi          = {10.1109/TAI.2021.3123935},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {381-390},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Quick learning mechanism with cross-domain adaptation for intelligent fault diagnosis},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised domain-adaptation-based tensor feature learning
with structure preservation. <em>TAI</em>, <em>3</em>(3), 370–380. (<a
href="https://doi.org/10.1109/TAI.2022.3163114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation (DA) is widely used in computer vision and pattern recognition applications. It is an effective process where a model is trained on objects from the source domain to predict the categories of the objects in the target domain. The aim of feature extraction in domain adaptation is to learn the best representation of the data in a certain domain and use it in other domains. However, the main challenge here is the difference between the data distributions of the source and target domains. Also, in computer vision, the data are represented as tensor objects such as 3-D images and video sequences. Most of the existing methods in DA apply vectorization to the data, which leads to information loss due to failure to preserve the natural tensor structure in a low-dimensional space. Thus, in this article, we propose unsupervised DA-based tensor feature learning (UDA-TFL) as a novel adapted feature extraction method that aims to avoid vectorization during transfer knowledge simultaneously; retain the structure of the tensor objects; reduce the data discrepancy between source and target domains; and represent the original tensor object in a lower dimensional space that is resistant to noise. Therefore, multilinear projections are determined to learn the tensor subspace without vectorizing the original tensor objects via an alternating optimization strategy. We integrate maximum mean discrepancy in the objective function to reduce the difference between source and target distributions. Extensive experiments are conducted on 39 cross-domain datasets from different fields, including images and videos. The promising results indicate that UDA-TFL significantly outperforms the state-of-the-art.},
  archive      = {J_TAI},
  author       = {Ali Braytee and Mohamad Naji and Paul J. Kennedy},
  doi          = {10.1109/TAI.2022.3163114},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {370-380},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Unsupervised domain-adaptation-based tensor feature learning with structure preservation},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Assessment of the clusterability of data using a multimodal
convolutional neural network. <em>TAI</em>, <em>3</em>(3), 355–369. (<a
href="https://doi.org/10.1109/TAI.2021.3117537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering assigns data points into groups called clusters, which define the characteristics of similar data points. Our work defines a model to identify and assess the presence of a clusterable structure initially in a two-dimensional density grid of a dataset, which is respectively expanded into a multidimensional density grid according the dimensionality of the dataset. Clusterability is defined as the tendency of a dataset having a structure for successful clustering. Our approach consists of a multimodal convolutional neural network to assess the clusterability of a dataset. Multimodality is the utilization of multiple sources of information. The output of our approach, the created model, also identifies the type of the clusterable structure (none, centroid, and density). Our approach does not require an initial clustering of the data to define its clusterability. In the assessment of the clusterability of high-dimensional data, we utilize random rotations accompanied with an ensemble approach. The multiple experiments of various clustering problems illustrate that our proposed approach is capable of assessing the clusterability of data and of identifying the type of the clusterable structure.},
  archive      = {J_TAI},
  author       = {Niko Reunanen and Tomi Räty and Timo Lintonen and Juho J. Jokinen},
  doi          = {10.1109/TAI.2021.3117537},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {355-369},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Assessment of the clusterability of data using a multimodal convolutional neural network},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FairDrop: Biased edge dropout for enhancing fairness in
graph representation learning. <em>TAI</em>, <em>3</em>(3), 344–354. (<a
href="https://doi.org/10.1109/TAI.2021.3133818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph representation learning has become a ubiquitous component in many scenarios, ranging from social network analysis to energy forecasting in smart grids. In several applications, ensuring the fairness of the node (or graph) representations with respect to some protected attributes is crucial for their correct deployment. Yet, fairness in graph deep learning remains underexplored, with few solutions available. In particular, the tendency of similar nodes to cluster on several real-world graphs (i.e., homophily) can dramatically worsen the fairness of these procedures. In this article, we propose a novel biased edge dropout algorithm (FairDrop) to counter-act homophily and improve fairness in graph representation learning. FairDrop can be plugged in easily on many existing algorithms, is efficient, adaptable, and can be combined with other fairness-inducing solutions. After describing the general algorithm, we demonstrate its application on two benchmark tasks, specifically, as a random walk model for producing node embeddings, and to a graph convolutional network for link prediction. We prove that the proposed algorithm can successfully improve the fairness of all models with only a small or negligible drop in accuracy, and compares favourably with existing state-of-the-art solutions. In an ablation study, we demonstrate that our algorithm can flexibly interpolate between biasing towards fairness and an unbiased edge dropout. Furthermore, to better evaluate the gains, we propose a new dyadic group definition to measure the bias of a link prediction task when paired with group-based fairness metrics. In particular, we extend the metric used to measure the bias in node embeddings by taking account of the graph structure.},
  archive      = {J_TAI},
  author       = {Indro Spinelli and Simone Scardapane and Amir Hussain and Aurelio Uncini},
  doi          = {10.1109/TAI.2021.3133818},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {344-354},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {FairDrop: Biased edge dropout for enhancing fairness in graph representation learning},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on masked facial detection methods and datasets for
fighting against COVID-19. <em>TAI</em>, <em>3</em>(3), 323–343. (<a
href="https://doi.org/10.1109/TAI.2021.3139058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus disease 2019 (COVID-19) continues to pose a great challenge to the world since its outbreak. To fight against the disease, a series of artificial intelligence (AI) techniques are developed and applied to real-world scenarios such as safety monitoring, disease diagnosis, infection risk assessment, and lesion segmentation of COVID-19 CT scans. The coronavirus epidemics have forced people wear masks to counteract the transmission of virus, which also brings difficulties to monitor large groups of people wearing masks. In this article, we primarily focus on the AI techniques of masked facial detection and related datasets. We survey the recent advances, beginning with the descriptions of masked facial detection datasets. A total of 13 available datasets are described and discussed in detail. Then, the methods are roughly categorized into two classes: conventional methods and neural network-based methods. The conventional methods are usually trained by boosting algorithms with hand-crafted features, which accounts for a small proportion. Neural network-based methods are further classified as three parts according to the number of processing stages. Representative algorithms are described in detail, coupled with some typical techniques that are described briefly. Finally, we summarize the recent benchmarking results, give the discussions on the limitations of datasets and methods, and expand future research directions. To our knowledge, this is the first survey about masked facial detection methods and datasets. Hopefully our survey could provide some help to fight against epidemics.},
  archive      = {J_TAI},
  author       = {Bingshu Wang and Jiangbin Zheng and C. L. Philip Chen},
  doi          = {10.1109/TAI.2021.3139058},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {323-343},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A survey on masked facial detection methods and datasets for fighting against COVID-19},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A support vector neural network for p300 EEG signal
classification. <em>TAI</em>, <em>3</em>(2), 309–321. (<a
href="https://doi.org/10.1109/TAI.2021.3105493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain–computer interface (BCI) P300 speller can help severely disabled patients communicate and control with external machines or robots, so that the classification methods of P300 electroencephalogram (EEG) signal play an important role in the development of BCI system and technology. In this article, a novel support vector neural network (SVNN) is proposed and developed to obtain more accurate and effective EEG classification results. It is the first time to combine linear variational inequality based primal-dual neural network with convex quadratic programming problem based on support vector machine to solve the classification problem. It has been proved that the SVNN globally converges to the optimal solution of convex optimization problem and iterates the parameters in the form of matrix, which means that the method has global convergence and parallelism. The proposed SVNN method is used to solve the classification problem of P300 EEG signals. Experimental results on dataset IIb from BCI competition II and dataset II from BCI competition III show that the accuracy of the proposed SVNN method is 100% and 98%, respectively. Compared with most of the state-of-the-art algorithms, SVNN has the highest recognition accuracy and information transfer rate.},
  archive      = {J_TAI},
  author       = {Zhijun Zhang and Guangqiang Chen and Siyuan Chen},
  doi          = {10.1109/TAI.2021.3105493},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {309-321},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A support vector neural network for p300 EEG signal classification},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Character-level street view text spotting based on deep
multisegmentation network for smarter autonomous driving. <em>TAI</em>,
<em>3</em>(2), 297–308. (<a
href="https://doi.org/10.1109/TAI.2021.3116216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban scenes are full of street entities with sign boards. Therefore, in autonomous driving, street view text spotting techniques will play a significant role in the precise understanding of surrounding scenes during driving, because texts contained in the images usually provide important clues for accurate image understanding, while it is often ambiguous for existing computer vision algorithms to understand scene images without texts. In this work, we propose a M ulti- S egmentation network for character-level scene T ext D etection (MSTD). The MSTD introduces a densely connected atrous spatial pyramid pooling module to enlarge the receptive field of the feature extraction layer, so as to localize long as well as large-sized text instances. Moreover, it devises a double segmentation subnetwork to utilize two independent but inherently complementary losses to co-optimize the network and increase the reliability of the confidence scores in predicting the text/nontext areas. With the character instances detected by the MSTD, one can easily perform scene text spotting with classic object recognition networks such as ResNet and DenseNet. We carried out extensive experiments on nine scene text datasets to demonstrate the outstanding performance of the MSTD on character-level and line-level text instance localization and scene text recognition, where the MSTD significantly outperforms the state-of-the-art scene text detection methods and the sequence-to-sequence-learning-based scene text recognizers.},
  archive      = {J_TAI},
  author       = {Chongsheng Zhang and Yuefeng Tao and Kai Du and Weiping Ding and Bin Wang and Ji Liu and Wei Wang},
  doi          = {10.1109/TAI.2021.3116216},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {297-308},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Character-level street view text spotting based on deep multisegmentation network for smarter autonomous driving},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fault diagnosis of machines using deep convolutional
beta-variational autoencoder. <em>TAI</em>, <em>3</em>(2), 287–296. (<a
href="https://doi.org/10.1109/TAI.2021.3110835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industries are using fault diagnosis methods to prevent any downtime, which eventually led them to make profits and take necessary steps beforehand to avoid any mishaps. In recent years, deep learning methods have shown extraordinary performance in massive data applications with advancement in computing power. In this article, a novel intelligent fault diagnosis scheme based on deep convolutional variable-beta variational autoencoder (VAE) is proposed to extract discriminative features. A new min–max algorithm for data points reduction and a random sampling technique to get 2-D data has been proposed. The proposed fault diagnosis combines all intermediate steps (from preprocessing to classification) in a single framework, and an end-to-end training has been performed. The proposed training method with variable beta uses VAE as a feature extractor and classifier rather than just being a probabilistic generative model, which further improved the performance of the overall model. The proposed scheme reduces the needs of domain/expertise knowledge on time-series data. The proposed method has also been validated in the presence of noise. The proposed approach is validated through two case studies by utilizing rotating machinery datasets: First, on the case western reserve university vibration dataset (VD), and second, on the air compressor acoustic dataset (AD). Highest accuracies obtained are 99.93% and 99.91% on case western reserve university VD and air compressor AD, respectively, using the proposed scheme. Finally, a comparative study has been presented.},
  archive      = {J_TAI},
  author       = {Gaurav Dewangan and Seetaram Maurya},
  doi          = {10.1109/TAI.2021.3110835},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {287-296},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Fault diagnosis of machines using deep convolutional beta-variational autoencoder},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation and applications of quantiles in deep binary
classification. <em>TAI</em>, <em>3</em>(2), 275–286. (<a
href="https://doi.org/10.1109/TAI.2021.3115078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional quantiles obtained via regression are used as a robust alternative to classical conditional means in econometrics and statistics, as they can capture the uncertainty in a prediction, and model tail behaviors, while making very few distributional assumptions. In this work, we extend the notion of conditional quantiles to the binary classification setting—allowing us to quantify the uncertainty in the predictions, increase resilience to label noise, and provide new insights into the functions learnt by the models. We accomplish this by defining a new loss called binary quantile regression loss. We compute the Lipschitz constant of the proposed loss and show that its curvature is bounded under some regularity conditions. These properties are later used to characterize the error rates of the learning algorithms and to accelerate the training regime with using Lipschitz adaptive learning rates. We leverage the estimated quantiles to obtain individualized confidence scores that provide an accurate measure of a prediction being misclassified. We aggregate these scores to provide two additional metrics, namely, confidence score and retention rate, which can be used to withhold decisions and increase model accuracy. We also study the robustness of the proposed nonparametric binary quantile classification framework, and finally, we demonstrate that quantiles aid in explainability as they can be used to obtain several univariate summary statistics that can be directly applied to existing explanation tools.},
  archive      = {J_TAI},
  author       = {Anuj Tambwekar and Anirudh Maiya and Soma Dhavala and Snehanshu Saha},
  doi          = {10.1109/TAI.2021.3115078},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {275-286},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Estimation and applications of quantiles in deep binary classification},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stand-alone composite attention network for concrete
structural defect classification. <em>TAI</em>, <em>3</em>(2), 265–274.
(<a href="https://doi.org/10.1109/TAI.2021.3114385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automation in structural health monitoring involves a critical step of automatic classification of concrete defect images/videos. Although interdisciplinary research community in AI has responded with some progress, immense challenges are still involved because of the predominant overlapping nature of the defect classes, exacerbated by the large variations in their visual appearance. However, current methodologies mostly consider single-class nonoverlapping defects and emphasize equally over the entire image plane; thus unable to focus on specific defect regions for robust feature selection. Thereby, the classification performance gets degraded and the methodologies became less suitable for real-world scenarios. In this work, we propose a novel stand-alone composite attention network that automatically exerts higher emphasis on the defective regions and less emphasis on the healthy regions to recognize multitarget multiclass and single-class concrete structural defects. This architecture stems from the novel GFGA mechanism as the building block to capture minute local features from visually-similar defect classes. We then propose the MSAM that encompasses multiscale discriminative information to capture variations in image properties. The MSAM incorporates BMAM to obtain crucial channel-spatial descriptors making the overall architecture an end-to-end-trainable network. Extensive experimentation and analysis on three large concrete defect datasets show the superiority of our proposed network as compared to the current state-of-the-art methodologies.},
  archive      = {J_TAI},
  author       = {Gaurab Bhattacharya and Niladri B. Puhan and Bappaditya Mandal},
  doi          = {10.1109/TAI.2021.3114385},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {265-274},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Stand-alone composite attention network for concrete structural defect classification},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and implementation of a vision- and
grating-sensor-based intelligent unmanned settlement system.
<em>TAI</em>, <em>3</em>(2), 254–264. (<a
href="https://doi.org/10.1109/TAI.2021.3116227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a new vision- and grating-sensor-based intelligent unmanned settlement (IUS) system is proposed for convenience stores to automatically recognize the shopping behavior of customers, record their identities, and generate invoices. First, we design a new IUS architecture, which includes a shelf module and exit module. To achieve automatic settlement for each customer, a shopping event detection method is proposed. In this method, a vision-based human pose estimation algorithm is used to detect a human form standing in front of a shelf. The hand actions of each customer are detected by a grating sensor, and an image recognition method based on a convolutional neural network (CNN) is applied to recognize the items in the hands of customers. To reduce the image annotation workload, we propose a semisupervised training method for the recognition network. Based on hand action detection and item recognition, a shopping event recognition method is designed for the system, and a facial image of the customer corresponding to each shopping behavior is captured. Finally, each detected shopping event is added to the invoice of the corresponding customer via a facial recognition method. To verify the effectiveness of the proposed IUS system, we have built a handheld item image dataset and a shopping event dataset for an unmanned convenience store. The experimental results show that the proposed system can accurately recognize shopping behaviors and generate invoices.},
  archive      = {J_TAI},
  author       = {Hong-Bo Zhang and Yi-Zhong Zhou and Li-Jia Dong and Qing Lei and Ji-Xiang Du},
  doi          = {10.1109/TAI.2021.3116227},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {254-264},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Design and implementation of a vision- and grating-sensor-based intelligent unmanned settlement system},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust scale-aware stereo matching network. <em>TAI</em>,
<em>3</em>(2), 244–253. (<a
href="https://doi.org/10.1109/TAI.2021.3115401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep convolutional neural networks (CNNs) have emerged as powerful tools for the correspondence problem in stereo matching task. However, the existence of multiscale objects and inevitable ill-conditioned regions, such as textureless regions, in real-world scene images continue to challenge current CNN architectures. In this article, we present a robust scale-aware stereo matching network, which aims to predict multiscale disparity maps and fuse them to achieve a more accurate disparity map. To this end, powerful feature representations are extracted from stereo images and are concatenated into a 4-D feature volume. The feature volume is then fed into a series of connected encoder–decoder cost aggregation structures for the construction of multiscale cost volumes. Following this, we regress multiscale disparity maps from the multiscale cost volumes and feed them into a fusion module to predict final disparity map. However, uncertainty estimations at each scale and complex disparity relationships among neighboring pixels pose a challenge on the disparity fusion. To overcome this challenge, we design a robust learning-based scale-aware disparity map fusion model, which seeks to map multiscale disparity maps onto the ground truth disparity map by leveraging their complementary strengths. Experimental results show that the proposed network is more robust and outperforms recent methods on standard stereo evaluation benchmarks.},
  archive      = {J_TAI},
  author       = {James Okae and Bohan Li and Juan Du and Yueming Hu},
  doi          = {10.1109/TAI.2021.3115401},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {244-253},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Robust scale-aware stereo matching network},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fine-grained food classification methods on the UEC FOOD-100
database. <em>TAI</em>, <em>3</em>(2), 238–243. (<a
href="https://doi.org/10.1109/TAI.2021.3108126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of an automatic food recognition system has severalinteresting applications ranging from waste food management, to advertisement, to calorie estimation, and daily diet monitoring. Despite the importance of this subject, the number of related studies is still limited. Moreover, the comparison in the literature was currently done over the best-shot performance without considering the most common method of averaging over several trials. This article surveys the most common deep learning methods used for food classification, it presents the publicly available databases of food, it releases benchmark results for the food classification experiment averaged over five-trials, and it beats the current best-shot performance experiment reaching the state-of-the-art accuracy of 90.02% on the UEC Food-100 database. The best results have been achieved by the ensemble method averaging the predictions of ResNeXt and DenseNet models. All the experiments are run on the UEC Food-100 database because it is one of the most used databases, and it is challenging due to the presence of multifood images, which need to be cropped before processing. This article aims to contribute to automatic food recognition by presenting the most common algorithms used for food classification, introducing the main databases of food items currently available, and reaching the state-of-the-art performance in the best-shot classification experiment of the UEC Food-100 database. That is, this article improves the current best-shot performance by 0.44 percentage points, and fixes it to 90.02%. Furthermore, with the best of our knowledge, this is the first article to introduce to the research community comparison of performances of the classification experiment on the UEC Food-100 database averaged over five-trails. As expected, performance averaged is slightly lower thanthe best-shot one.},
  archive      = {J_TAI},
  author       = {Berker Arslan and Sefer Memiş and Elena Battini Sönmez and Okan Zafer Batur},
  doi          = {10.1109/TAI.2021.3108126},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {238-243},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Fine-grained food classification methods on the UEC FOOD-100 database},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Event-triggered optimal control for the continuous stirred
tank reactor system. <em>TAI</em>, <em>3</em>(2), 228–237. (<a
href="https://doi.org/10.1109/TAI.2021.3107367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous stirred tank reactor (CSTR)is essential equipment found in chemical processing industries. Control of the CSTR process has long been a challenging issue due to the high complexity and strong nonlinearity of the chemical process. In this article, a novel event-triggered (ET) adaptive dynamic programming (ADP) optimal control algorithm is developed for the CSTR system. To reduce the computational load and the communication data between the controller and the actuator, a novel event-triggering condition based on the Taylor series expansion is designed for the ET controller. With the novel event-triggering condition, the control policy does not update unless the event-triggering condition is not satisfied. Compared with the time-triggering mechanism, the update frequency of the controller can be obviously reduced. Then, an identifer–actor–critic structure is developed to implement the ET ADP controller. In particular, to overcome the challenge of establishing an exact dynamic for the CSTR system, an identifier neural network (NN) is employed to reconstruct the unknown system dynamic based on offline data. Furthermore, the actor–critic structure is developed to obtain the ET control law and the value function. In actor and critic NNs, weights are turned just at the triggered instant and remained constant during the interevent times. Finally, the developed ET ADP controller is applied to the CSTR system. Experimental results show that the developed ET approach can cut down the update frequency to 66%, which is very significant for the real CSTR system.},
  archive      = {J_TAI},
  author       = {Wei Zhou and Jun Yi and Lizhong Yao and Guorong Chen},
  doi          = {10.1109/TAI.2021.3107367},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {228-237},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Event-triggered optimal control for the continuous stirred tank reactor system},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive finite-time neural constrained control for
nonlinear active suspension systems based on the command filter.
<em>TAI</em>, <em>3</em>(2), 218–227. (<a
href="https://doi.org/10.1109/TAI.2021.3107226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the adaptive finite-time command filter control problem for nonlinear quarter active suspension systems with actuator failure. In the previous designs of active suspension systems, a second-order system for the vehicle body movement is employed to achieve the controller design and a zero-dynamic analysis is needed, thus the designed controller only contains a part of system variables and there is a difficulty in selecting a proper Lyapunov function in the zero-dynamic analysis. In order to overcome the aforementioned problems, a novel active suspension system is shown based on the Butterworth low-pass filter. The neural networks (NNs) are used to identify the unknown functions of active suspension systems. Meanwhile, the command filter is proposed to handle the “explosion of complexity” problem caused by the adaptive backstepping technique. Then, the adaptive practical finite-time control scheme combined with the command filter and neural identify technique is constructed in a unified framework, which can handle the “singularity” problem. Through the practical finite-time stability analysis, it is easily to obtain that all signals of closed-loop systems are bounded in a finite time. Finally, simulation results for the nonlinear quarter active suspension systems are provided to verify the effectiveness of the NN control scheme.},
  archive      = {J_TAI},
  author       = {Lei Liu and Changqi Zhu and Yan-Jun Liu and Shaocheng Tong},
  doi          = {10.1109/TAI.2021.3107226},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {218-227},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive finite-time neural constrained control for nonlinear active suspension systems based on the command filter},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). KAM-net: Keypoint-aware and keypoint-matching network for
vehicle detection from 2-d point cloud. <em>TAI</em>, <em>3</em>(2),
207–217. (<a href="https://doi.org/10.1109/TAI.2021.3112945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-dimesional (2-D) LiDAR is an efficient alternative sensor for vehicle detection, which is one of the most critical tasks in autonomous driving. Compared to the fully developed 3-D LiDAR vehicle detection, 2-D LiDAR vehicle detection has much room to improve. Most existing state-of-the-art works represent 2-D point clouds as pseudo-images and then perform detection with traditional object detectors on 2-D images. However, they ignore the sparse representation and geometric information of vehicles in the 2-D cloud points. To address these issues, in this article, we present a novel keypoint-aware and keypoint-matching network termed as KAM-Net, which focuses on better detecting the vehicles by explicitly capturing and extracting the sparse information of L-shape in 2-D LiDAR point clouds. The whole framework consists of two stages—namely, keypoint-aware stage and keypoint-matching stage. The keypoint-aware stage utilizes the heatmap and edge extraction module to simultaneously predict the position of L-shaped keypoints and inflection offset of L-shaped endpoints. The keypoint-matching stage is followed to group the keypoints and produce the oriented bounding boxes with axis by utilizing the endpoint-matching and L-shaped-matching methods. Further, we conduct extensive experiments on a recently released public dataset to evaluate the effectiveness of our approach. The results show that our KAM-Net achieves a new state-of-the-art performance. The source code is available at https://github.com/ispc-lab/KAM-Net .},
  archive      = {J_TAI},
  author       = {Tianpei Zou and Guang Chen and Zhijun Li and Wei He and Sanqing Qu and Shangding Gu and Alois Knoll},
  doi          = {10.1109/TAI.2021.3112945},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {207-217},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {KAM-net: Keypoint-aware and keypoint-matching network for vehicle detection from 2-D point cloud},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ANIMC: A soft approach for autoweighted noisy and incomplete
multiview clustering. <em>TAI</em>, <em>3</em>(2), 192–206. (<a
href="https://doi.org/10.1109/TAI.2021.3116546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview clustering has wide real-world applications because it can process data from multiple sources. However, these data often contain missing instances and noises, which are ignored by most multiview clustering methods. Missing instances may make these methods difficult to use directly, and noises will lead to unreliable clustering results. In this article, we propose a novel autoweighted noisy and incomplete multiview clustering (ANIMC) approach via a soft autoweighted strategy and a doubly soft regular regression model. First, by designing adaptive semiregularized nonnegative matrix factorization, the soft autoweighted strategy assigns a proper weight to each view and adds a soft boundary to balance the influence of noises and incompleteness. Second, by proposing &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\theta$&lt;/tex-math&gt;&lt;/inline-formula&gt; -norm, the doubly soft regularized regression model adjusts the sparsity of our model by choosing different &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\theta$&lt;/tex-math&gt;&lt;/inline-formula&gt; . Compared with previous methods, ANIMC has three unique advantages: 1) it is a soft algorithm to adjust our approach in different scenarios, thereby improving its generalization ability; 2) it automatically learns a proper weight for each view, thereby reducing the influence of noises; 3) it performs doubly soft regularized regression that aligns the same instances in different views, thereby decreasing the impact of missing instances. Extensive experimental results show its superior advantages over other state-of-the-art works.},
  archive      = {J_TAI},
  author       = {Xiang Fang and Yuchong Hu and Pan Zhou and Dapeng Wu},
  doi          = {10.1109/TAI.2021.3116546},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {192-206},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {ANIMC: A soft approach for autoweighted noisy and incomplete multiview clustering},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decomposition-based classifier chains for multi-dimensional
classification. <em>TAI</em>, <em>3</em>(2), 176–191. (<a
href="https://doi.org/10.1109/TAI.2021.3110935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-dimensional classification, the semantics of objects are characterized by multiple class variables from different dimensions. To model the dependencies among class variables, one natural strategy is to build a number of multiclass classifiers in a chaining structure, one per dimension, where the subsequent classifiers on the chain augment the feature space with all labeling information used by the preceding classifiers. However, it is shown that this strategy cannot compete with existing state-of-the-art approaches via comparative studies. One possible reason is that inaccurate predictions of preceding classifiers would degenerate the performance of subsequent ones. Besides, it is more difficult to learn a multiclass classifier than a binary one with the same accuracy, and better performance can be expected if the multi-dimensional classification problem can be solved by building multiple binary classifiers in a chaining structure. Based on these conjectures, this article proposes an approach, which builds a chain of binary classifiers to solve the multi-dimensional classification problem with the help of one-versus-one decomposition. To address the issue that different one-versus-one decomposed problems involve different training examples, the feature space is augmented with the binary predictions of preceding classifiers on the chain to train the subsequent ones. To alleviate the effect of the specified chaining order, the ensemble version of the proposed approach is further investigated. Comparative studies over 20 benchmark datasets clearly show the superiority of the proposed approach against the state-of-the-art multi-dimensional classification baselines.},
  archive      = {J_TAI},
  author       = {Bin-Bin Jia and Min-Ling Zhang},
  doi          = {10.1109/TAI.2021.3110935},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {176-191},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Decomposition-based classifier chains for multi-dimensional classification},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development of an optimal entropy classifier and prudent
learning model. <em>TAI</em>, <em>3</em>(2), 164–175. (<a
href="https://doi.org/10.1109/TAI.2021.3117491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article gives the representation of both probabilistic uncertainty and possibilistic certainty including the Bayesian learning in the framework of information set theory which is an offshoot of the Hanman–Anirban entropy function. Being information theoretic and parametric, this function deals with both probability and possibility. If a set of information source (attribute) values is fitted with this entropy function it gives rise to information values and the sum of these values is certainty. An adaptive form of this function yields the Hanman transform (HT) that gives the higher order certainty. An optimal entropy classifier is developed by learning the weight (support) vectors of all classes by minimizing this entropy of all the error vectors between the training feature vectors and the weight vector. To this end, we have proposed prudent learning model that favors competition with both the worst performer and the best performer based on the HT. The conversion of Renyi entropy function into the possibilistic domain helps us generate Renyi sigmoid and Renyi energy features. These new features and classifier are implemented on two datasets: Finger-knuckle-print for the authentication of persons and defect classification in the fabrics. The experimental results vindicate the effectiveness of the proposed features, classifier, and the learning model.},
  archive      = {J_TAI},
  author       = {Jyotsana Grover and Madasu Hanmandlu},
  doi          = {10.1109/TAI.2021.3117491},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {164-175},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Development of an optimal entropy classifier and prudent learning model},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Delayed reward bernoulli bandits: Optimal policy and
predictive meta-algorithm PARDI. <em>TAI</em>, <em>3</em>(2), 152–163.
(<a href="https://doi.org/10.1109/TAI.2021.3117743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bernoulli multi-armed bandits are a reinforcement learning model used to optimize the sequences of decisions with binary outcomes. Well-known bandit algorithms, including the optimal policy, assume that before a decision is made the outcomes of previous decisions are known. This assumption is often not satisfied in real-life scenarios. As demonstrated in this article, if decision outcomes are affected by delays, the performance of existing algorithms can be severely affected. We present the first practically applicable method to compute statistically optimal decisions in the presence of outcome delays. Our method has a predictive component abstracted out into a meta-algorithm, predictive algorithm reducing delay impact (PARDI), which significantly reduces the impact of delays on commonly used algorithms. We demonstrate empirically that PARDI-enhanced Whittle index is nearly optimal for a wide range of Bernoulli bandit parameters and delays. In a wide spectrum of experiments, it performed better than any other suboptimal algorithm, e.g., UCB1-tuned and Thompson sampling. PARDI-enhanced Whittle index can be used when computational requirements of the optimal policy are too high.},
  archive      = {J_TAI},
  author       = {Sebastian Pilarski and Slawomir Pilarski and Dániel Varró},
  doi          = {10.1109/TAI.2021.3117743},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {152-163},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Delayed reward bernoulli bandits: Optimal policy and predictive meta-algorithm PARDI},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep q-learning: Theoretical insights from an asymptotic
analysis. <em>TAI</em>, <em>3</em>(2), 139–151. (<a
href="https://doi.org/10.1109/TAI.2021.3111142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Q-learning is an important reinforcement learning algorithm, which involves training a deep neural network, called deep Q-network, to approximate the well-known Q-function. Although wildly successful under laboratory conditions, serious gaps between theory and practice as well as a lack of formal guarantees prevent its use in the real world. Adopting a dynamical systems perspective, we provide a theoretical analysis of a popular version of deep Q-learning under realistic and verifiable assumptions. More specifically, we prove an important result on the convergence of the algorithm, characterizing the asymptotic behavior of the learning process. Our result sheds light on hitherto unexplained properties of the algorithm and helps understand empirical observations, such as performance inconsistencies even after training. Unlike previous theories, our analysis accommodates state Markov processes with multiple stationary distributions. In spite of the focus on deep Q-learning, we believe that our theory may be applied to understand other deep learning algorithms.},
  archive      = {J_TAI},
  author       = {Arunselvan Ramaswamy and Eyke Hüllermeier},
  doi          = {10.1109/TAI.2021.3111142},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {139-151},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep Q-learning: Theoretical insights from an asymptotic analysis},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated COVID-19 grading with convolutional neural
networks in computed tomography scans: A systematic comparison.
<em>TAI</em>, <em>3</em>(2), 129–138. (<a
href="https://doi.org/10.1109/TAI.2021.3115093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amidst the ongoing pandemic, the assessment of computed tomography (CT) images for COVID-19 presence can exceed the workload capacity of radiologists. Several studies addressed this issue by automating COVID-19 classification and grading from CT images with convolutional neural networks (CNNs). Many of these studies reported initial results of algorithms that were assembled from commonly used components. However, the choice of the components of these algorithms was often pragmatic rather than systematic and systems were not compared to each other across papers in a fair manner. We systematically investigated the effectiveness of using 3-D CNNs instead of 2-D CNNs for seven commonly used architectures, including DenseNet, Inception, and ResNet variants. For the architecture that performed best, we furthermore investigated the effect of initializing the network with pretrained weights, providing automatically computed lesion maps as additional network input, and predicting a continuous instead of a categorical output. A 3-D DenseNet-201 with these components achieved an area under the receiver operating characteristic curve of 0.930 on our test set of 105 CT scans and an AUC of 0.919 on a publicly available set of 742 CT scans, a substantial improvement in comparison with a previously published 2-D CNN. This article provides insights into the performance benefits of various components for COVID-19 classification and grading systems. We have created a challenge on grand-challenge.org to allow for a fair comparison between the results of this and future research.},
  archive      = {J_TAI},
  author       = {Coen de Vente and Luuk H. Boulogne and Kiran Vaidhya Venkadesh and Cheryl Sital and Nikolas Lessmann and Colin Jacobs and Clara I. Sánchez and Bram van Ginneken},
  doi          = {10.1109/TAI.2021.3115093},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {129-138},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Automated COVID-19 grading with convolutional neural networks in computed tomography scans: A systematic comparison},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Negative selection algorithm research and applications in
the last decade: A review. <em>TAI</em>, <em>3</em>(2), 110–128. (<a
href="https://doi.org/10.1109/TAI.2021.3114661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The negative selection algorithm (NSA) is one of the important methods in the field of immunological computation (or artificial immune systems). Over the years, some progress was made that turns this algorithm (NSA) into an efficient approach to solve problems in different domain. This review takes into account these signs of progress during the last decade and categorizes those based on different characteristics and performances. Our study shows that NSA’s evolution can be labeled in four ways highlighting the most notable NSA variations and their limitations in different application domains. We also present alternative approaches to NSA for comparison and analysis. It is evident that NSA performs better for nonlinear representation than most of the other methods, and it can outperform neural-based models in computation time. We summarize NSA’s development and highlight challenges in NSA research in comparison with other similar models.},
  archive      = {J_TAI},
  author       = {Kishor Datta Gupta and Dipankar Dasgupta},
  doi          = {10.1109/TAI.2021.3114661},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {110-128},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Negative selection algorithm research and applications in the last decade: A review},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Challenges and countermeasures for adversarial attacks on
deep reinforcement learning. <em>TAI</em>, <em>3</em>(2), 90–109. (<a
href="https://doi.org/10.1109/TAI.2021.3111139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) has numerous applications in the real world, thanks to its ability to achieve high performance in a range of environments with little manual oversight. Despite its great advantages, DRL is susceptible to adversarial attacks, which precludes its use in real-life critical systems and applications (e.g., smart grids, traffic controls, and autonomous vehicles) unless its vulnerabilities are addressed and mitigated. To address this problem, we provide a comprehensive survey that discusses emerging attacks on DRL-based systems and the potential countermeasures to defend against these attacks. We first review the fundamental background on DRL and present emerging adversarial attacks on machine learning techniques. We then investigate the vulnerabilities that an adversary can exploit to attack DRL along with state-of-the-art countermeasures to prevent such attacks. Finally, we highlight open issues and research challenges for developing solutions to deal with attacks on DRL-based intelligent systems.},
  archive      = {J_TAI},
  author       = {Inaam Ilahi and Muhammad Usama and Junaid Qadir and Muhammad Umar Janjua and Ala Al-Fuqaha and Dinh Thai Hoang and Dusit Niyato},
  doi          = {10.1109/TAI.2021.3111139},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {90-109},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Challenges and countermeasures for adversarial attacks on deep reinforcement learning},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MLRP-KG: Mine landslide risk prediction based on knowledge
graph. <em>TAI</em>, <em>3</em>(1), 78–87. (<a
href="https://doi.org/10.1109/TAI.2021.3114652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mine landslide risk prediction is a fundamental task for the safety management of the digital mining system, which is dependent on the analysis of the open pit mine exploitation slope stability. Such stability analysis involves a lot of local natural and human factors. Training a learning model using these factors, with the aim of discovering their relationship with the slope stability, is an intuitive way for the mine landslide risk prediction. The key issue is how to explore the complex nonlinear relations among these factors by using a small amount of high-dimensional historical slope data. Traditional factor-led methods for the issue only focus on the impacts of landslide factors, but ignore the correlations between historical slope data, which in fact contain more useful information. In this article, we propose a new mine landslide risk prediction model using knowledge graph. The gradient boosting decision tree is applied to further exploit the crossed features within the historical data, and then a landslide semantic network is constructed using knowledge graph with the consideration of correlation information between historical slope data. In this way, the model is suitable to deal with the small set of high-dimensional data, and it makes a joint use of the features within the landslide factor values and correlations between historical slope data. We conduct a set of experiments on real historical slope data from the real-world open pit mining scene. The experiment results validate the effectiveness and efficiency of the proposed model in predicting landslide risks with a small set of high-dimensional historical data.},
  archive      = {J_TAI},
  author       = {Lianbo Ma and Jingwei Wang and Jian Cheng and Xingwei Wang and Wancheng Zhu},
  doi          = {10.1109/TAI.2021.3114652},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {78-87},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {MLRP-KG: Mine landslide risk prediction based on knowledge graph},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). New artificial intelligence approach to inclination
measurement based on MEMS accelerometer. <em>TAI</em>, <em>3</em>(1),
67–77. (<a href="https://doi.org/10.1109/TAI.2021.3105494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article presents a research of angular orientation based on a microelectromechanical system (MEMS) accelerometer by using machine learning (ML) and deep learning (DL) model with architectures of deep neural networks (DNNs). In the industrial environment, artificial intelligence (AI) plays a crucial role in automation which is a potential solution for better performance of inclinometer. This article was carried out to apply this intelligent model on the inertial measurement unit to accomplish the angular position. The experiment shows that the ML model correctly learns the relationship between acceleration and tracking angles via polynomial regression with an R-square of 0.98. The employed DL model with four hidden layers of ten neurons achieves an accuracy of 99.99 &amp;#x0025; and almost a nonerror performance. The acceleration acquisitions were obtained from MEMS accelerometer LSM9DS1 at a frequency of 50 Hz via microcontroller STM32F401RE. The ML and DNN models were designed based on the platform Tensorflow with high processing accuracy. The Pan-Tilt Unit was used as the angle reference for static and dynamic tests. The traditional technique is used for comparison as well as verification of the proposed models. The DL model has better precision over the ML model due to its high structure level with updating weight and error optimization from the neural network structure. Meanwhile, ML shows more stable results in dynamic circumstances.},
  archive      = {J_TAI},
  author       = {Minh Long Hoang and Antonio Pietrosanto},
  doi          = {10.1109/TAI.2021.3105494},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {67-77},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {New artificial intelligence approach to inclination measurement based on MEMS accelerometer},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performance assessment of fuzzy logic control approach for
MR-damper based-transfemoral prosthetic leg. <em>TAI</em>,
<em>3</em>(1), 53–66. (<a
href="https://doi.org/10.1109/TAI.2021.3106884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfemoral amputation commonly occurs due to some stroke, diabetes, physical or mental trauma, which reduces the person&#39;s movement capability. Therefore, an efficient prosthetic leg is essential to improve the life of an amputee by replacing the lost limb. This letter addresses the fuzzy logic-based control strategy for magneto-rheological damper based prosthetic leg for transfemoral amputees. The primary focus of this letter is to present the performance analysis of the control to achieve the entire gait cycle (both swing and stance phases) for a transfemoral prosthesis. The performance and robustness analysis of the developed prosthetic leg is tested for real-time gait data to obtain the precise and more natural gait by the transfemoral amputee.},
  archive      = {J_TAI},
  author       = {Richa Sharma and Prerna Gaur and Shaurya Bhatt and Deepak Joshi},
  doi          = {10.1109/TAI.2021.3106884},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {53-66},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Performance assessment of fuzzy logic control approach for MR-damper based-transfemoral prosthetic leg},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Domain adaptation and autoencoder-based unsupervised speech
enhancement. <em>TAI</em>, <em>3</em>(1), 43–52. (<a
href="https://doi.org/10.1109/TAI.2021.3119927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a category of transfer learning, domain adaptation plays an important role in generalizing the model trained in one task and applying it to other similar tasks or settings. In speech enhancement, a well-trained acoustic model can be exploited to obtain the speech signal in the context of other languages, speakers, and environments. Recent domain-adaptation research was developed more effectively with various neural networks and high-level abstract features. However, the related studies are more likely to transfer the well-trained model from a rich and more diverse domain to a limited and similar domain. Therefore, in this article, the domain-adaptation method is proposed in unsupervised speech enhancement for the opposite circumstance, that is, transferring to a larger and richer domain. On the one hand, the importance-weighting (IW) approach is exploited with a variance-constrained autoencoder to reduce the shift of shared weights between the source and target domains. On the other hand, in order to train the classifier with the worst-case weights and minimize the risk, the minimax method is proposed. Both the proposed IW and minimax methods are evaluated from the VOICE BANK and IEEE datasets to the TIMIT dataset. The experimental results show that the proposed methods outperform the state-of-the-art approaches.},
  archive      = {J_TAI},
  author       = {Yi Li and Yang Sun and Kirill Horoshenkov and Syed Mohsen Naqvi},
  doi          = {10.1109/TAI.2021.3119927},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {43-52},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Domain adaptation and autoencoder-based unsupervised speech enhancement},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bipartite cooperative coevolution for energy-aware coverage
path planning of UAVs. <em>TAI</em>, <em>3</em>(1), 29–42. (<a
href="https://doi.org/10.1109/TAI.2021.3103143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coverage path planning of unmanned aerial vehicles (UAVs) is a complex optimization problem in practice, especially for those involving multiple target areas. It is challenging to comprehensively plan the inter-area visiting order and the intra-area coverage paths simultaneously. Due to the battery limitation, usually the task can hardly be finished by a single UAV, and instead a fleet of UAVs are required. In this article, we first formulate an energy-aware multi-UAV multi-area coverage path planning (EM &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$^2$&lt;/tex-math&gt;&lt;/inline-formula&gt; CPP) model, in order to characterize the practical path planning requirements of UAVs in complex conditions. Subsequently, to accomplish the optimization task, we propose a bipartite cooperative coevolution (BiCC) algorithm that coevolves an inter-area path planning and an intra-area path planning components to obtain good solutions. The basic operators in BiCC, such as the initialization and the reproduction operators, are tailored for the task of EM &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$^2$&lt;/tex-math&gt;&lt;/inline-formula&gt; CPP. Besides, we also develop a fast heuristic algorithm for EM &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$^2$&lt;/tex-math&gt;&lt;/inline-formula&gt; CPP, which is able to produce approximate solutions in a short time. Simulations on real-world datasets validate the good performance of the proposed methods.},
  archive      = {J_TAI},
  author       = {Xian-Xin Shao and Yue-Jiao Gong and Zhi-Hui Zhan and Jun Zhang},
  doi          = {10.1109/TAI.2021.3103143},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {29-42},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Bipartite cooperative coevolution for energy-aware coverage path planning of UAVs},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Playing against deep-neural-network-based object detectors:
A novel bidirectional adversarial attack approach. <em>TAI</em>,
<em>3</em>(1), 20–28. (<a
href="https://doi.org/10.1109/TAI.2021.3107807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the fields of deep learning and computer vision, the security of object detection models has received extensive attention. Revealing the security vulnerabilities resulting from adversarial attacks has become one of the most important research directions. Existing studies show that object detection models can also be threatened by adversarial examples, just like other deep-neural-network-based models, e.g., those for classification. In this article, we propose a bidirectional adversarial attack method. First, the added perturbation pushes the prediction results given by the object detectors far away from the ground-truth class while getting close to the background class. Second, a confidence loss function is designed for the region proposal network to reduce the foreground scores. Third, the adversarial examples are generated by a pretrained autoencoder, and the model is trained using an adversarial approach, which can enhance the similarity between the adversarial examples and the original image and speed up algorithm convergence. The proposed method was verified on the most popular two-stage detection framework (Faster R-CNN), and 55.1% drop in the mean average precision (mAP-drop) was obtained. In addition, the adversarial examples have superior transferability, migrating which to the common one-stage detection framework (YOLOv3) gets a 39.5% mAP-drop.},
  archive      = {J_TAI},
  author       = {Xiang Li and Yuchen Jiang and Chenglin Liu and Shaochong Liu and Hao Luo and Shen Yin},
  doi          = {10.1109/TAI.2021.3107807},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {20-28},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Playing against deep-neural-network-based object detectors: A novel bidirectional adversarial attack approach},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kolmogorov–smirnov test-based actively-adaptive thompson
sampling for non-stationary bandits. <em>TAI</em>, <em>3</em>(1), 11–19.
(<a href="https://doi.org/10.1109/TAI.2021.3121653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the nonstationary multiarmed bandit framework and propose a Kolmogorov–Smirnov (KS) test based Thompson sampling (TS) algorithm named TS-KS that actively detects change points and resets the TS parameters once a change is detected. In particular, for the two-armed bandit case, we derive bounds on the number of samples of the reward distribution to detect the change once it occurs. Consequently, we show that the proposed algorithm has sublinear regret. Contrary to existing works, our algorithm is able to detect a change when the underlying reward distribution changes even though the mean reward remains the same. Finally, to test the efficacy of the proposed algorithm, we employ it in the following two case-studies: first, task-offloading scenario in wireless edge-computing, and second, portfolio optimization. Our results show that the proposed TS-KS algorithm outperforms not only the static TS algorithm but also it performs better than other bandit algorithms designed for nonstationary environments. Moreover, the performance of TS-KS is at par with the state-of-the-art forecasting algorithms such as Facebook-PROPHET and ARIMA .},
  archive      = {J_TAI},
  author       = {Gourab Ghatak and Hardhik Mohanty and Aniq Ur Rahman},
  doi          = {10.1109/TAI.2021.3121653},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {11-19},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Kolmogorov–Smirnov test-based actively-adaptive thompson sampling for non-stationary bandits},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AutoFac: The perpetual robot machine. <em>TAI</em>,
<em>3</em>(1), 2–10. (<a
href="https://doi.org/10.1109/TAI.2021.3104789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotics currently lacks fully autonomous capabilities, especially where task knowledge is incomplete and optimal robotic solutions cannot be pre-engineered. The intersection of evolutionary robotics , artificial life , and embodied artificial intelligence presents a promising paradigm for generating multitask problem-solvers suitable for adapting over extended periods in unexplored, remote, and hazardous environments. To address the automation of evolving robotic systems, we propose fully autonomous embodied artificial-life factories and laboratories, situated in various environments as multitask problem solvers. Such integrated factories and laboratories would be adaptive solution designers, producing fit-for-purpose physical robots with accelerated artificial evolution that experiment to continually discover new tasks. Such tasks would be stepping-stones toward accomplishing given mission objectives over extended periods (days to decades). Rather than being purely speculative, prerequisite technologies to realize such factories have been experimentally demonstrated. Currently, vast scientific and enterprise opportunities await in applications such as asteroid mining, terraforming, space, and deep-sea exploration, though no suitable solution exists. The proposed embodied artificial-life factories and laboratories, termed AutoFac , use robot production equipment run by artificial evolution controllers to collect and synthesize environmental information (from robotic sensory systems). Such information is merged with current needs and mission objectives to create new robot embodiment and task definitions that are environmentally adapted and balance task-oriented behavior with exploration. AutoFac is, thus, generalist (deployable in many environments) but continually produces specialist solutions within such environments—a perpetual robot machine.},
  archive      = {J_TAI},
  author       = {Geoff Nitschke and David Howard},
  doi          = {10.1109/TAI.2021.3104789},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {2-10},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {AutoFac: The perpetual robot machine},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
