<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAFFC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taffc---166">TAFFC - 166</h2>
<ul>
<li><details>
<summary>
(2022). Modeling real-world affective and communicative nonverbal
vocalizations from minimally speaking individuals. <em>IEEE Transactions
on Affective Computing</em>, <em>13</em>(4), 2238–2253. (<a
href="https://doi.org/10.1109/TAFFC.2022.3208233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Nonverbal vocalizations from non- and minimally speaking individuals who speak fewer than 20 words (mv* individuals) convey important communicative and affective information. While nonverbal vocalizations that occur amidst typical speech and infant vocalizations have been studied extensively in the literature, there is limited prior work on vocalizations by mv* individuals. Our work is among the first studies of the communicative and affective information expressed in nonverbal vocalizations by mv* children and adults. We collected labeled vocalizations in real-world settings with eight mv* communicators, with communicative and affective labels provided in-the-moment by a close family member. Using evaluation strategies suitable for messy, real-world data, we show that nonverbal vocalizations can be classified by function (with 4- and 5-way classifications) with F1 scores above chance for all participants. We analyze labeling and data collection practices for each participating family, and discuss the classification results in the context of our novel real-world data collection protocol. The presented work includes results from the largest classification experiments with nonverbal vocalizations from mv* communicators to date.},
  archive  = {J},
  author   = {Jaya Narain and Kristina T. Johnson and Thomas F. Quatieri and Rosalind W. Picard and Pattie Maes},
  doi      = {10.1109/TAFFC.2022.3208233},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2238-2253},
  title    = {Modeling real-world affective and communicative nonverbal vocalizations from minimally speaking individuals},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ER-chat: A text-to-text open-domain dialogue framework for
emotion regulation. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(4), 2229–2237. (<a
href="https://doi.org/10.1109/TAFFC.2022.3191973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotions are essential for constructing social relationships between humans and interactive systems. Although emotional and empathetic dialogue generation methods have been proposed for dialogue systems, appropriate dialogue involves not only mirroring emotions and always being empathetic but also complex factors such as context. This paper proposes Emotion Regulation Chat (ER-Chat) as an end-to-end dialogue framework for emotion regulation. Emotion regulation is concerned with actions to approach appropriate emotional states. Learning appropriate emotion and intent when responding on the basis of the context of the dialogue enables the generation of more human-like dialogue. We conducted automatic and human evaluations to demonstrate the superiority of ER-Chat over the baseline system. The results show that inclusion of emotion and intent prediction mechanisms enable generation of dialogues with greater fluency, diversity, emotion awareness, and emotion appropriateness, which are greatly preferred by humans.},
  archive  = {J},
  author   = {Shin Katayama and Shunsuke Aoki and Takuro Yonezawa and Tadashi Okoshi and Jin Nakazawa and Nobuo Kawaguchi},
  doi      = {10.1109/TAFFC.2022.3191973},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2229-2237},
  title    = {ER-chat: A text-to-text open-domain dialogue framework for emotion regulation},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A dual-branch dynamic graph convolution based adaptive
TransFormer feature fusion network for EEG emotion recognition. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(4), 2218–2228. (<a
href="https://doi.org/10.1109/TAFFC.2022.3199075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Electroencephalograph (EEG) emotion recognition plays an important role in the brain-computer interface (BCI) field. However, most of recent methods adopted shallow graph neural networks using a single temporal feature, leading to the limited emotion classification performance. Furthermore, the existing methods generally ignore the individual divergence between different subjects, resulting in poor transfer performance. To address these deficiencies, we propose a dual-branch dynamic graph convolution based adaptive transformer feature fusion network with adapter-finetuned transfer learning (DBGC-ATFFNet-AFTL) for EEG emotion recognition. Specifically, a dual-branch graph convolution network (DBGCN) is firstly designed to effectively capture the temporal and spectral characterizations of EEG simultaneously. Second, the adaptive Transformer feature fusion network (ATFFNet) is conducted by integrating the obtained feature maps with the channel-weight unit, leading to significant difference between different channels. Finally, the adapter-finetuned transfer learning method (AFTL) is applied in cross-subject emotion recognition, which proves to be parameter-efficient with few samples of the target subject. The competitive experimental results on three datasets have shown that our proposed method achieves the promising emotion classification performance compared with the state-of-the-art methods. The code of our proposed method will be available at: https://github.com/smy17/DANet .},
  archive  = {J},
  author   = {Mingyi Sun and Weigang Cui and Shuyue Yu and Hongbin Han and Bin Hu and Yang Li},
  doi      = {10.1109/TAFFC.2022.3199075},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2218-2228},
  title    = {A dual-branch dynamic graph convolution based adaptive TransFormer feature fusion network for EEG emotion recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward robust stress prediction in the age of wearables:
Modeling perceived stress in a longitudinal study with information
workers. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(4), 2201–2217. (<a
href="https://doi.org/10.1109/TAFFC.2022.3188006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Given the widespread adverse outcomes of stress – exacerbated by the current pandemic – wearable sensing provides unique opportunities for automated stress tracking to inform well-being interventions. However, its success in the wild and at scale depends on the robustness and validity of automated stress inference, which is limited in current systems. In this work, we enumerate the properties of robustness and validity necessary for achieving viable automated stress inference using wearable sensors, and we underscore present challenges to constructing and evaluating these systems. Using these criteria as guiding principles, we present automated stress inference results from a large (N=606) in situ longitudinal wearable and contextual sensing study of information workers. Using a multimodal approach encompassing a wearable sensor, relative location tracking, smartphone usage, and environmental sensing, we trained regression models to predict daily self-reported perceived stress in a participant-independent fashion. Our models significantly outperformed baseline variants with shuffled stress scores and were consistent with small-to-moderate effects. Our findings highlight the performance disparity between robust and valid approaches to automated perceived stress inference and current approaches and suggest that further performance gains might require additional sensing modalities and enhanced contextual awareness than existing approaches.},
  archive  = {J},
  author   = {Brandon M. Booth and Hana Vrzakova and Stephen M. Mattingly and Gonzalo J. Martinez and Louis Faust and Sidney K. D’Mello},
  doi      = {10.1109/TAFFC.2022.3188006},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2201-2217},
  title    = {Toward robust stress prediction in the age of wearables: Modeling perceived stress in a longitudinal study with information workers},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PARSE: Pairwise alignment of representations in
semi-supervised EEG learning for emotion recognition. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(4), 2185–2200. (<a
href="https://doi.org/10.1109/TAFFC.2022.3210441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose pairwise alignment of representations for semi-supervised Electroencephalogram (EEG) learning (PARSE), a novel semi-supervised architecture for learning reliable EEG representations for emotion recognition. To reduce the potential distribution mismatch between large amounts of unlabeled data and a limited number of labeled data, PARSE uses pairwise representation alignment. First, our model performs data augmentation followed by label guessing for large amounts of original and augmented unlabeled data. The model is then followed by sharpening the guessed labels and convex combinations of the unlabeled and labeled data. Finally, it performs representation alignment and emotion classification. To rigorously test our model, we compare PARSE to several state-of-the-art semi-supervised approaches, which we implement and adapt for EEG learning. We perform these experiments on four public EEG-based emotion recognition datasets, SEED, SEED-IV, SEED-V and AMIGOS (valence and arousal). The experiments show that our proposed framework achieves the overall best results with varying amounts of limited labeled samples in SEED, SEED-IV and AMIGOS (valence), while approaching the overall best result (reaching the second-best) in SEED-V and AMIGOS (arousal). The analysis shows that our pairwise representation alignment considerably improves the performance by performing the distribution alignment between unlabeled and labeled data, especially when only 1 sample per class is labeled. The source code of our article is publicly available at https://github.com/guangyizhangbci/PARSE .},
  archive  = {J},
  author   = {Guangyi Zhang and Vandad Davoodnia and Ali Etemad},
  doi      = {10.1109/TAFFC.2022.3210441},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2185-2200},
  title    = {PARSE: Pairwise alignment of representations in semi-supervised EEG learning for emotion recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The arousal video game AnnotatIoN (AGAIN) dataset. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(4), 2171–2184. (<a
href="https://doi.org/10.1109/TAFFC.2022.3188851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {How can we model affect in a general fashion, across dissimilar tasks, and to which degree are such general representations of affect even possible? To address such questions and enable research towards general affective computing, this paper introduces The Arousal video Game AnnotatIoN (AGAIN) dataset. AGAIN is a large-scale affective corpus that features over 1,100 in-game videos (with corresponding gameplay data) from nine different games, which are annotated for arousal from 124 participants in a first-person continuous fashion. Even though AGAIN is created for the purpose of investigating the generality of affective computing across dissimilar tasks, affect modelling can be studied within each of its 9 specific interactive games. To the best of our knowledge AGAIN is the largest—over 37 hours of annotated video and game logs—and most diverse publicly available affective dataset based on games as interactive affect elicitors.},
  archive  = {J},
  author   = {David Melhart and Antonios Liapis and Georgios N. Yannakakis},
  doi      = {10.1109/TAFFC.2022.3188851},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2171-2184},
  title    = {The arousal video game AnnotatIoN (AGAIN) dataset},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust audiovisual emotion recognition: Aligning modalities,
capturing temporal information, and handling missing features. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(4), 2156–2170. (<a
href="https://doi.org/10.1109/TAFFC.2022.3216993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion recognition using audiovisual features is a challenging task for human-machine interaction systems. Under ideal conditions (perfect illumination, clean speech signals, and non-occluded visual data) many systems are able to achieve reliable results. However, few studies have considered developing multimodal systems and training strategies to build systems that can perform well under non ideal conditions. Audiovisual models still face challenging problems such as misalignment of modalities, lack of temporal modeling, and missing features due to noise or occlusions. In this article, we implement a model that combines auxiliary networks, a transformer architecture, and an optimized training mechanism to achieve a robust system for audiovisual emotion recognition that addresses, in a principled way, these challenges. Our evaluation analyzes how well this model performs in ideal conditions and when modalities are missing. We contrast this method with other multimodal fusion methods for emotion recognition. Our experimental results based on two audiovisual databases demonstrate that the proposed framework achieves: 1) improvements in emotion recognition accuracy, 2) better alignment and fusion of audiovisual features at the model level, 3) awareness of temporal information, and 4) robustness to non-ideal scenarios.},
  archive  = {J},
  author   = {Lucas Goncalves and Carlos Busso},
  doi      = {10.1109/TAFFC.2022.3216993},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2156-2170},
  title    = {Robust audiovisual emotion recognition: Aligning modalities, capturing temporal information, and handling missing features},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dimensional affect uncertainty modelling for apparent
personality recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(4), 2144–2155. (<a
href="https://doi.org/10.1109/TAFFC.2022.3189974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Despite achieving impressive performance, dimensional affect or emotion recognition from faces is largely based on uncertainty-unaware models that predict only point estimates. Modelling uncertainty is important to learn reliable facial emotion recognition models with the abilities to (a). holistically quantify predictive uncertainty estimates and (b). propagate those estimates to the benefit of downstream behavioural analysis tasks. In this work, we first quantify uncertainties in dimensional emotion recognition by adopting the framework of epistemic (model) and aleatoric (data) uncertainty categorisation. Then for evaluating the practical utility of uncertainty-aware emotion predictions, we introduce them in learning an important downstream task, apparent personality recognition. To this end, we ask two questions: how to effectively (a). use already known behavioural attributes (emotions) in a downstream task (personality recognition) and (b). summarise global temporal context from uncertainty-aware emotion predictions fused with image embeddings. Answering these questions, we learn a conditional latent variable model building on recently proposed neural latent variable models. Our experiments on two in-the-wild datasets, SEWA for emotion recognition and ChaLearn for personality recognition, demonstrate that fusion of epistemic and aleatoric emotion uncertainties significantly improves personality recognition performance, with $\sim$ 42% relative improvement in Pearson correlation coefficient, leading to a new state-of-the-art.},
  archive  = {J},
  author   = {Mani Kumar Tellamekala and Timo Giesbrecht and Michel Valstar},
  doi      = {10.1109/TAFFC.2022.3189974},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2144-2155},
  title    = {Dimensional affect uncertainty modelling for apparent personality recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Classifying emotions and engagement in online learning based
on a single facial expression recognition neural network. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(4), 2132–2143. (<a
href="https://doi.org/10.1109/TAFFC.2022.3188390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, behaviour of students in the e-learning environment is analyzed. The novel pipeline is proposed based on video facial processing. At first, face detection, tracking and clustering techniques are applied to extract the sequences of faces of each student. Next, a single efficient neural network is used to extract emotional features in each frame. This network is pre-trained on face identification and fine-tuned for facial expression recognition on static images from AffectNet using a specially developed robust optimization technique. It is shown that the resulting facial features can be used for fast simultaneous prediction of students’ engagement levels (from disengaged to highly engaged), individual emotions (happy, sad, etc.,) and group-level affect (positive, neutral or negative). This model can be used for real-time video processing even on a mobile device of each student without the need for sending their facial video to the remote server or teacher&#39;s PC. In addition, the possibility to prepare a summary of a lesson is demonstrated by saving short clips of different emotions and engagement of all students. The experimental study on the datasets from EmotiW (Emotion Recognition in the Wild) challenges showed that the proposed network significantly outperforms existing single models.},
  archive  = {J},
  author   = {Andrey V. Savchenko and Lyudmila V. Savchenko and Ilya Makarov},
  doi      = {10.1109/TAFFC.2022.3188390},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2132-2143},
  title    = {Classifying emotions and engagement in online learning based on a single facial expression recognition neural network},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multimodal approach for mania level prediction in bipolar
disorder. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(4), 2119–2131. (<a
href="https://doi.org/10.1109/TAFFC.2022.3193054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Bipolar disorder is a mental health disorder that causes mood swings that range from depression to mania. Clinical diagnosis of bipolar disorder is based on patient interviews and reports obtained from the relatives of the patients. Subsequently, the diagnosis depends on the experience of the expert, and there is co-morbidity with other mental disorders. Automated processing in the diagnosis of bipolar disorder can help providing quantitative indicators, and allow easier observations of the patients for longer periods. In this paper, we create a multimodal decision system for three level mania classification based on recordings of the patients in acoustic, linguistic, and visual modalities. The system is evaluated on the Turkish Bipolar Disorder corpus we have recently introduced to the scientific community. Comprehensive analysis of unimodal and multimodal systems, as well as fusion techniques, are performed. Using acoustic, linguistic, and visual features in a multimodal fusion system, we achieved a 64.8% unweighted average recall score, which advances the state-of-the-art performance on this dataset.},
  archive  = {J},
  author   = {Pınar Baki and Heysem Kaya and Elvan Çiftçi and Hüseyin Güleç and Albert Ali Salah},
  doi      = {10.1109/TAFFC.2022.3193054},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2119-2131},
  title    = {A multimodal approach for mania level prediction in bipolar disorder},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring self-attention graph pooling with EEG-based
topological structure and soft label for depression detection. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(4), 2106–2118. (<a
href="https://doi.org/10.1109/TAFFC.2022.3210958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Electroencephalogram (EEG) has been widely used in neurological disease detection, i.e., major depressive disorder (MDD). Recently, some deep EEG-based MDD detection attempts have been proposed and achieved promising performance. These works, however, still suffer from the following limitations, such as insufficient exploration of the EEG-based topological structure, information loss caused by high-dimensional data compression, and under-estimation of intra-class difference and inter-class similarity. To solve these issues, we propose an EEG-based MDD detection model named S elf-attention G raph P ooling with S oft L abel (SGP-SL). Specifically, we explore the local and global connections among EEG channels to construct an EEG-based graph in advance. By leveraging multiple self-attention graph pooling modules, the constructed graph is then gradually refined, followed by graph pooling, to aggregate information from less-important nodes to more-important ones. In this way, the feature representation with better discriminability can be learned from EEG signals. In addition, the soft label strategy is also adopted to build the loss function, aiming to further enhance the feature discriminability. Experimental results on the MODMA dataset demonstrate the superiority of the proposed method. What&#39;s more, extensive ablation studies are conducted to verify the effectiveness of the proposed elements in our model.},
  archive  = {J},
  author   = {Tao Chen and Yanrong Guo and Shijie Hao and Richang Hong},
  doi      = {10.1109/TAFFC.2022.3210958},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2106-2118},
  title    = {Exploring self-attention graph pooling with EEG-based topological structure and soft label for depression detection},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiview facial expression recognition, a survey. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(4), 2086–2105. (<a
href="https://doi.org/10.1109/TAFFC.2022.3184995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multiview Facial Expression Recognition (MFER) is a well-known interdisciplinary problem among computer science and related disciplines with promising and valuable applications. Recognizing the facial expression in pose variations, which is very common in real-world conditions, makes it very challenging. This paper aims to provide a comprehensive survey of the MFER progress, includingboth categories of traditional and deep approaches. In general, we sort each of these categories into three overall groups to meet the pose variations: Pose-Robust Features, Pose Normalization, and Pose-Specific Classification. While reviewing the traditional methods, a thorough study is proposed on the existing novel deep techniques. We also introduce the state-of-the-art and discuss the challenges, limitations, opportunities, and future trends that need to be addressed in this field. Moreover, we provide an extensive review of publicly available datasets for MFER, including the labs’ collections and the sets gathered from in the wild. Besides, we introduce the most popular protocols on each dataset to standardize comparisons in the future.},
  archive  = {J},
  author   = {Mahdi Jampour and Malihe Javidi},
  doi      = {10.1109/TAFFC.2022.3184995},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2086-2105},
  title    = {Multiview facial expression recognition, a survey},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised approach for facial movement based optical
flow. <em>IEEE Transactions on Affective Computing</em>, <em>13</em>(4),
2071–2085. (<a
href="https://doi.org/10.1109/TAFFC.2022.3197622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Computing optical flow is a fundamental problem in computer vision. However, deep learning-based optical flow techniques do not perform well for non-rigid movements such as those found in faces, primarily due to lack of the training data representing the fine facial motion. We hypothesize that learning optical flow on face motion data will improve the quality of predicted flow on faces. This work aims to: (1) exploring self-supervised techniques to generate optical flow ground truth for face images; (2) computing baseline results on the effects of using face data to train Convolutional Neural Networks (CNN) for predicting optical flow; and (3) using the learned optical flow in micro-expression recognition to demonstrate its effectiveness. We generate optical flow ground truth using facial key-points in the BP4D-Spontaneous dataset. This optical flow is used to train the FlowNetS architecture to test its performance on the Extended Cohn-Kanade dataset and a portion of the generated dataset. The performance of FlowNetS trained on face images surpassed that of other optical flow CNN architectures. Our optical flow features are further compared with other methods using the STSTNet micro-expression classifier, and the results indicate that the optical flow obtained using this work has promising applications in facial expression analysis.},
  archive  = {J},
  author   = {Muhannad Alkaddour and Usman Tariq and Abhinav Dhall},
  doi      = {10.1109/TAFFC.2022.3197622},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2071-2085},
  title    = {Self-supervised approach for facial movement based optical flow},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FLEPNet: Feature level ensemble parallel network for facial
expression recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(4), 2058–2070. (<a
href="https://doi.org/10.1109/TAFFC.2022.3208309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the advent of deep learning, the research on facial expression recognition (FER) has received a lot of interest. Different deep convolutional neural network (DCNN) architectures have been developed for real-time and efficient FER. One of the challenges in FER is obtaining trustworthy features that are strongly associated with facial expression changes. Furthermore, traditional DCNNs for FER problems have two significant issues: insufficient training data, which leads to overfitting, and intra-class facial appearance variations. FLEPNet, a texture-based feature-level ensemble parallel network for FER, is proposed in this study and proved to solve the aforementioned problems. Our parallel network FLEPNet uses multi-scale convolutional and multi-scale residual block-based DCNN as building blocks. First, we consider modified homomorphic filtering to normalize the illumination effectively, which minimizes the intra-class difference. The deep networks are then protected against having insufficient training data by using texture analysis on face expression images to identify multiple attributes. Four texture features are extracted and combined with the image&#39;s original characteristics. Finally, the integrated features retrieved by two networks are used to classify seven facial expressions. Experimental results reveal that the proposed technique achieves an average accuracy of 0.9914, 0.9894, 0.9796, 0.8756, and 0.8072 on Japanese Female Facial Expressions, Extended CohnKanade, Karolinska Directed Emotional Faces, Real-world Affective Face Database, and Facial Expression Recognition 2013 databases, respectively. Moreover, experimental outcomes depict significant reliability when compared to competing approaches.},
  archive  = {J},
  author   = {Mohan Karnati and Ayan Seal and Anis Yazidi and Ondrej Krejcar},
  doi      = {10.1109/TAFFC.2022.3208309},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2058-2070},
  title    = {FLEPNet: Feature level ensemble parallel network for facial expression recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Subjective fear in virtual reality: A linear mixed-effects
analysis of skin conductance. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(4), 2047–2057. (<a
href="https://doi.org/10.1109/TAFFC.2022.3197842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The investigation of the physiological and pathological processes involved in fear perception is complicated due to the difficulties in reliably eliciting and measuring the complex construct of fear. This study proposes a novel approach to induce and measure subjective fear and its physiological correlates combining virtual reality (VR) with a mixed-effects model based on skin conductance (SC). Specifically, we developed a new VR scenario applying specific guidelines derived from horror movies and video games. Such a VR environment was used to induce fear in eighteen volunteers in an experimental protocol, including two relaxation scenarios and a neutral virtual environment. The SC signal was acquired throughout the experiment, and after each virtual scenario, the emotional state and fear perception level were assessed using psychometric scales. We statistically evaluated the greatest sympathetic activation induced by the fearful scenario compared to the others, showing significant results for most SC-derived features. Finally, we developed a rigorous mixed-effects model to explain the perceived fear as a function of the SC features. Model-fitting results showed a significant relationship between the fear perception scores and a combination of features extracted from both fast- and slow-varying SC components, proposing a novel solution for a more objective fear assessment.},
  archive  = {J},
  author   = {Andrea Baldini and Sergio Frumento and Danilo Menicucci and Angelo Gemignani and Enzo Pasquale Scilingo and Alberto Greco},
  doi      = {10.1109/TAFFC.2022.3197842},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2047-2057},
  title    = {Subjective fear in virtual reality: A linear mixed-effects analysis of skin conductance},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning for micro-expression recognition: A survey.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(4),
2028–2046. (<a
href="https://doi.org/10.1109/TAFFC.2022.3205170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Micro-expressions (MEs) are involuntary facial movements revealing people&#39;s hidden feelings in high-stake situations and have practical importance in various fields. Early methods for Micro-expression Recognition (MER) are mainly based on traditional features. Recently, with the success of Deep Learning (DL) in various tasks, neural networks have received increasing interest in MER. Different from macro-expressions, MEs are spontaneous, subtle, and rapid facial movements, leading to difficult data collection and annotation, thus publicly available datasets are usually small-scale. Currently, various DL approaches have been proposed to solve the ME issues and improve MER performance. In this survey, we provide a comprehensive review of deep MER and define a new taxonomy for the field encompassing all aspects of MER based on DL, including datasets, each step of the deep MER pipeline, and performance comparisons of the most influential methods. The basic approaches and advanced developments are summarized and discussed for each aspect. Additionally, we conclude the remaining challenges and potential directions for the design of robust MER systems. Finally, ethical considerations in MER are discussed. To the best of our knowledge, this is the first survey of deep MER methods, and this survey can serve as a reference point for future MER research.},
  archive  = {J},
  author   = {Yante Li and Jinsheng Wei and Yang Liu and Janne Kauttonen and Guoying Zhao},
  doi      = {10.1109/TAFFC.2022.3205170},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2028-2046},
  title    = {Deep learning for micro-expression recognition: A survey},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inconsistency-based multi-task cooperative learning for
emotion recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(4), 2017–2027. (<a
href="https://doi.org/10.1109/TAFFC.2022.3197414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion recognition is an important part of affective computing. Human emotions can be described categorically or dimensionally. Accurate machine learning models for emotion classification and estimation usually depend on a large amount of annotated data. However, label acquisition in emotion recognition is costly: obtaining the ground-truth labels of an emotional sample usually requires multiple annotators’ assessments, which is expensive and time-consuming. To reduce the labeling effort in multi-task emotions recognition, the paper proposes an inconsistency measure that can indicate the difference between the labels estimated from the feature space and the label distribution of labeled dataset. Using the inconsistency as an indicator of sample informativeness, we further propose an inconsistency-based multi-task cooperative learning framework that integrates multi-task active learning and self-training semi-supervised learning. Experiments in two multi-task emotion recognition scenarios, multi-dimensional emotion estimation and simultaneous emotion classification and estimation, were conducted under this framework. The results demonstrated that the proposed multi-task active learning framework outperformed several single-task and multi-task active learning approaches.},
  archive  = {J},
  author   = {Yifan Xu and Yuqi Cui and Xue Jiang and Yingjie Yin and Jingting Ding and Liang Li and Dongrui Wu},
  doi      = {10.1109/TAFFC.2022.3197414},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2017-2027},
  title    = {Inconsistency-based multi-task cooperative learning for emotion recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Objective class-based micro-expression recognition under
partial occlusion via region-inspired relation reasoning network.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(4),
1998–2016. (<a
href="https://doi.org/10.1109/TAFFC.2022.3197785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Micro-expression recognition ( MER ) has attracted the attention of many researchers in the past decade. However, occlusion occurs for MER in real-world scenarios. In this paper, a challenging issue in MER that is interesting but unexplored, i.e., occlusion MER, is deeply investigated. First, to research MER under real-world occlusion conditions, synthetic occluded microexpression databases are created by using various community masks. Second, to suppress the influence of occlusion, a R egion-inspired R elation R easoning N etwork ( RRRN ) is proposed to model the relations between various facial regions. The RRRN consists of a backbone network, a region-inspired (RI) module and a relation reasoning (RR) module. More specifically, the backbone network aims to extract feature representations from different facial regions, the RI module is designed to compute the adaptive weight from the facial region itself based on the unobstructedness and importance of the region for suppressing the influence of occlusion using an attention mechanism, and the RR module exploits the progressive interactions among these regions by performing graph convolutions. Experiments are conducted on two tasks of MEGC 2018: the holdout-database evaluation task and the composite database evaluation task. Experimental results show that RRRN can be utilized to significantly explore the importance of facial regions and capture the cooperative complementary relationship of facial regions for MER. The results also demonstrate that RRRN outperforms the state-of-the-art approaches, especially with respect to occlusion, where RRRN is more robust.},
  archive  = {J},
  author   = {Qirong Mao and Ling Zhou and Wenming Zheng and Xiuyan Shao and Xiaohua Huang},
  doi      = {10.1109/TAFFC.2022.3197785},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1998-2016},
  title    = {Objective class-based micro-expression recognition under partial occlusion via region-inspired relation reasoning network},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Facial expression translation using landmark guided GANs.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(4),
1986–1997. (<a
href="https://doi.org/10.1109/TAFFC.2022.3207007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a simple yet powerful Landmark guided Generative Adversarial Network (LandmarkGAN) for the facial expression-to-expression translation using a single image, which is an important and challenging task in computer vision since the expression-to-expression translation is a non-linear and non-aligned problem. Moreover, it requires a high-level semantic understanding between the input and output images since the objects in images can have arbitrary poses, sizes, locations, backgrounds, and self-occlusions. To tackle this problem, we propose utilizing facial landmark information explicitly. Since it is a challenging problem, we split it into two sub-tasks, (i) category-guided landmark generation, and (ii) landmark-guided expression-to-expression translation. Two sub-tasks are trained in an end-to-end fashion that aims to enjoy the mutually improved benefits from the generated landmarks and expressions. Compared with current keypoint-guided approaches, the proposed LandmarkGAN only needs a single facial image to generate various expressions. Extensive experimental results on four public datasets demonstrate that the proposed LandmarkGAN achieves better results compared with state-of-the-art approaches only using a single image.},
  archive  = {J},
  author   = {Hao Tang and Nicu Sebe},
  doi      = {10.1109/TAFFC.2022.3207007},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1986-1997},
  title    = {Facial expression translation using landmark guided GANs},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Short and long range relation based spatio-temporal
transformer for micro-expression recognition. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(4), 1973–1985. (<a
href="https://doi.org/10.1109/TAFFC.2022.3213509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Being spontaneous, micro-expressions are useful in the inference of a person&#39;s true emotions even if an attempt is made to conceal them. Due to their short duration and low intensity, the recognition of micro-expressions is a difficult task in affective computing. The early work based on handcrafted spatio-temporal features which showed some promise, has recently been superseded by different deep learning approaches which now compete for the state of the art performance. Nevertheless, the problem of capturing both local and global spatio-temporal patterns remains challenging. To this end, herein we propose a novel spatio-temporal transformer architecture – to the best of our knowledge, the first purely transformer based approach (i.e., void of any convolutional network use) for micro-expression recognition. The architecture comprises a spatial encoder which learns spatial patterns, a temporal aggregator for temporal dimension analysis, and a classification head. A comprehensive evaluation on three widely used spontaneous micro-expression data sets, namely SMIC-HS, CASME II and SAMM, shows that the proposed approach consistently outperforms the state of the art, and is the first framework in the published literature on micro-expression recognition to achieve the unweighted F1-score greater than 0.9 on any of the aforementioned data sets. The source code is available at https://github.com/Vision-Intelligence-and-Robots-Group/SLSTT .},
  archive  = {J},
  author   = {Liangfei Zhang and Xiaopeng Hong and Ognjen Arandjelović and Guoying Zhao},
  doi      = {10.1109/TAFFC.2022.3213509},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1973-1985},
  title    = {Short and long range relation based spatio-temporal transformer for micro-expression recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised personalization of an emotion recognition
system: The unique properties of the externalization of valence in
speech. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(4), 1959–1972. (<a
href="https://doi.org/10.1109/TAFFC.2022.3187336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The prediction of valence from speech is an important, but challenging problem. The expression of valence in speech has speaker-dependent cues, which contribute to performances that are often significantly lower than the prediction of other emotional attributes such as arousal and dominance. A practical approach to improve valence prediction from speech is to adapt the models to the target speakers in the test set. Adapting a speech emotion recognition (SER) system to a particular speaker is a hard problem, especially with deep neural networks (DNNs), since it requires optimizing millions of parameters. This study proposes an unsupervised approach to address this problem by searching for speakers in the train set with similar acoustic patterns as the speaker in the test set. Speech samples from the selected speakers are used to create the adaptation set. This approach leverages transfer learning using pre-trained models, which are adapted with these speech samples. We propose three alternative adaptation strategies: unique speaker, oversampling and weighting approaches. These methods differ on the use of the adaptation set in the personalization of the valence models. The results demonstrate that a valence prediction model can be efficiently personalized with these unsupervised approaches, leading to relative improvements as high as 13.52%.},
  archive  = {J},
  author   = {Kusha Sridhar and Carlos Busso},
  doi      = {10.1109/TAFFC.2022.3187336},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1959-1972},
  title    = {Unsupervised personalization of an emotion recognition system: The unique properties of the externalization of valence in speech},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint feature adaptation and graph adaptive label
propagation for cross-subject emotion recognition from EEG signals.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(4),
1941–1958. (<a
href="https://doi.org/10.1109/TAFFC.2022.3189222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Though Electroencephalogram (EEG) could objectively reflect emotional states of our human beings, its weak, non-stationary, and low signal-to-noise properties easily cause the individual differences. To enhance the universality of affective brain-computer interface systems, transfer learning has been widely used to alleviate the data distribution discrepancies among subjects. However, most of existing approaches focused mainly on the domain-invariant feature learning, which is not unified together with the recognition process. In this paper, we propose a joint feature adaptation and graph adaptive label propagation model (JAGP) for cross-subject emotion recognition from EEG signals, which seamlessly unifies the three components of domain-invariant feature learning, emotional state estimation and optimal graph learning together into a single objective. We conduct extensive experiments on two benchmark SEED_IV and SEED_V data sets and the results reveal that 1) the recognition performance is greatly improved, indicating the effectiveness of the triple unification mode; 2) the emotion metric of EEG samples are gradually optimized during model training, showing the necessity of optimal graph learning, and 3) the projection matrix-induced feature importance is obtained based on which the critical frequency bands and brain regions corresponding to subject-invariant features can be automatically identified, demonstrating the superiority of the learned shared subspace.},
  archive  = {J},
  author   = {Yong Peng and Wenjuan Wang and Wanzeng Kong and Feiping Nie and Bao-Liang Lu and Andrzej Cichocki},
  doi      = {10.1109/TAFFC.2022.3189222},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1941-1958},
  title    = {Joint feature adaptation and graph adaptive label propagation for cross-subject emotion recognition from EEG signals},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient framework for constructing speech emotion
corpus based on integrated active learning strategies. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(4), 1929–1940. (<a
href="https://doi.org/10.1109/TAFFC.2022.3192899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speech emotion recognition has been developed rapidly in recent decades because of the appearance of machine learning. Nevertheless, lack of corpus remains a significant issue. For actual speech emotion corpus construction, many professional actors are required to perform voices with various emotions in specific scenes. In the process of data labelling, since the number of samples of different emotion categories is extremely imbalanced, it is difficult to efficiently label the samples. Hence, we proposed an integrated active learning sampling strategy and designed an efficient framework for constructing speech emotion corpora in order to address the problems presented above. Comparing experiments with other active learning algorithms on 13 datasets, our method was shown to improve sampling efficiency. In addition, it is able to select small category samples to be labelled with preference in imbalanced datasets. During the actual corpus construction experiments, our method can prioritize selecting small class emotion samples. As even when the amount of labelled data is less than 50%, the accuracy rate still can reach 90%. This greatly enhances the efficiency of constructing the speech emotion corpus and fills in the gaps.},
  archive  = {J},
  author   = {Fuji Ren and Zheng Liu and Xin Kang},
  doi      = {10.1109/TAFFC.2022.3192899},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1929-1940},
  title    = {An efficient framework for constructing speech emotion corpus based on integrated active learning strategies},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-stage graph fusion networks for major depressive
disorder diagnosis. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(4), 1917–1928. (<a
href="https://doi.org/10.1109/TAFFC.2022.3205652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Major depressive disorder (MDD) is a common and severe psychiatric illness marked by loss of interest and low energy, which result in the highest burden of disability among all mental disorders. Clinical MDD diagnosis still utilizes the phenomenological approach of syndrome-based interview, which leads to a high rate of misdiagnosis. Therefore, it is highly imperative to explore effective biomarkers to enable precise personalized diagnosis. There still exist two main challenges due to complexity of MDD and individual differences. On the one hand, discriminative features need to be investigated to better reflect the characteristics of MDD. On the other hand, the performance from shallow and static learning models is still not satisfactory. To overcome these issues, we propose a novel Multi-Stage Graph Fusion Networks (MSGFN) for major depressive disorder diagnosis. At first, functional connectivity is calculated to better characterize interactions between white matter and gray matter. Second, multi-stage features are obtained by a deep subspace learning model, and a number of graphs are constructed under the self-expression constraints at each stage. Finally, a novel graph convolutional fusion module is proposed with graph convolutional operations to integrate features and graph at each stage. Extensive experiments demonstrate the superior performance of the proposed framework. Our source code is available on: https://github.com/LIST-KONG/MSGFN-master .},
  archive  = {J},
  author   = {Youyong Kong and Shuyi Niu and Heren Gao and Yingying Yue and Huazhong Shu and Chunming Xie and Zhijun Zhang and Yonggui Yuan},
  doi      = {10.1109/TAFFC.2022.3205652},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1917-1928},
  title    = {Multi-stage graph fusion networks for major depressive disorder diagnosis},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic-rich facial emotional expression recognition.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(4),
1906–1916. (<a
href="https://doi.org/10.1109/TAFFC.2022.3201290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The ability to perceive human facial emotions is an essential feature of various multi-modal applications, especially in the intelligent human-computer interaction (HCI) area. In recent decades, considerable efforts have been put into researching automatic facial emotion recognition (FER). However, most of the existing FER methods only focus on either basic emotions such as the seven/eight categories (e.g., happiness, anger and surprise ) or abstract dimensions ( valence, arousal, etc. ), while neglecting the fruitful nature of emotion statements. In real-world scenarios, there is definitely a larger vocabulary for describing human&#39;s inner feelings as well as their reflection on facial expressions. In this work, we propose to address the semantic richness issue in the FER problem, with an emphasis on the granularity of the emotion concepts. Particularly, we take inspiration from former psycho-linguistic research, which conducted a prototypicality rating study and chose 135 emotion names from hundreds of English emotion terms. Based on the 135 emotion categories, we investigate the corresponding facial expressions by collecting a large-scale 135-class FER image dataset and propose a consequent facial emotion recognition framework. To demonstrate the accessibility of prompting FER research to a fine-grained level, we conduct extensive evaluations on the dataset credibility and the accompanying baseline classification model. The qualitative and quantitative results prove that the problem is meaningful and our solution is effective. To the best of our knowledge, this is the first work aimed at exploiting such a large semantic space for emotion representation in the FER problem.},
  archive  = {J},
  author   = {Keyu Chen and Xu Yang and Changjie Fan and Wei Zhang and Yu Ding},
  doi      = {10.1109/TAFFC.2022.3201290},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1906-1916},
  title    = {Semantic-rich facial emotional expression recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quality-aware bag of modulation spectrum features for robust
speech emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(4), 1892–1905. (<a
href="https://doi.org/10.1109/TAFFC.2022.3188223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatic speech emotion recognition (SER) has gained popularity over the last decade and numerous Challenges have emerged. While the latest Challenges have shown that deep neural networks achieve the best results, existing input features are still a bottleneck and cause severe performance degradation in realistic “in-the-wild” scenarios. In this paper, we propose two innovations to tackle this issue. First, we propose to combine the bag-of-audio-words methodology with modulation spectrum features for environmental robustness. Second, we take advantage of the inherent quality-awareness properties of modulation spectrum and propose the use of a quality feature as an additional feature to be used by the speech emotion recognizer. Experiments are conducted with three multi-lingual speech datasets used in recent SER Challenges degraded by different noise sources and levels, and room reverberation. Experimental results show the proposed features i) consistently outperforming benchmark systems, ii) providing complementary information to classical features, hence improving performance with feature fusion, and iii) showing robustness against environment and language mismatch. Moreover, we show that when the proposed system is provided with quality information, further improvements are obtained. Overall, the proposed bag of modulation spectrum features are shown to be a promising candidate for “in-the-wild” SER.},
  archive  = {J},
  author   = {Shruti Rajendra Kshirsagar and Tiago Henrik Falk},
  doi      = {10.1109/TAFFC.2022.3188223},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1892-1905},
  title    = {Quality-aware bag of modulation spectrum features for robust speech emotion recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards contrastive context-aware conversational emotion
recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(4), 1879–1891. (<a
href="https://doi.org/10.1109/TAFFC.2022.3212994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Conversational Emotion Recognition (CER) aims at classifying the emotion of each utterance in a conversation. For a target utterance, its emotion is jointly determined by multiple factors, such as conversation topics, emotion labels and intra/inter-speaker influences, in the conversational context of it. Then an important research question arises: can the effects of these contextual factors be sufficiently captured by the current CER models? To answer this question, we carry out an empirical study on four representative CER models by a context-replacement methodology. The results suggest that these models either exhibit a label-copying effect, or rely heavily on the intra/inter-speaker dependency structure within the conversation, but do not make a good use of the semantics carried by the conversational context. Thus, there is a high risk that they overfit certain single factors, yet lacking a holistic understanding of the semantic context. To tackle the problem, we propose a semantic-guided contrastive context-aware CER method, namely C3ER, to augment/regularize a backbone CER model, which can be any neural CER framework. Specifically, C3ER takes the hidden states of utterances from the CER model as input, extracts the contrast pairs consisting of relevant and irrelevant utterances to the conversational context of a target utterance, and uses contrastive learning to establish a soft semantic constraint between the target utterance and its context. It is then jointly trained with the main CER model, forcing the model to gain a semantic understanding of the context. Extensive experimental results show that C3ER can significantly boost the accuracy and improve the robustness of the representative CER models.},
  archive  = {J},
  author   = {Hanqing Zhang and Dawei Song},
  doi      = {10.1109/TAFFC.2022.3212994},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1879-1891},
  title    = {Towards contrastive context-aware conversational emotion recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Disentangling identity and pose for facial expression
recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(4), 1868–1878. (<a
href="https://doi.org/10.1109/TAFFC.2022.3197761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial expression recognition (FER) is a challenging problem because the expression component is always entangled with other irrelevant factors, such as identity and head pose. In this work, we propose an identity and pose disentangled facial expression recognition (IPD-FER) model to learn more discriminative feature representation. We regard the holistic facial representation as the combination of identity, pose and expression. These three components are encoded with different encoders. For identity encoder, a well pre-trained face recognition model is utilized and fixed during training, which alleviates the restriction on specific expression training data in previous works and makes the disentanglement practicable on in-the-wild datasets. At the same time, the pose and expression encoder are optimized with corresponding labels. Combining identity and pose feature, a neutral face of input individual should be generated by the decoder. When expression feature is added, the input image should be reconstructed. By comparing the difference between synthesized neutral and expressional images of the same individual, the expression component is further disentangled from identity and pose. Experimental results verify the effectiveness of our method on both lab-controlled and in-the-wild databases and we achieve state-of-the-art recognition performance.},
  archive  = {J},
  author   = {Jing Jiang and Weihong Deng},
  doi      = {10.1109/TAFFC.2022.3197761},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1868-1878},
  title    = {Disentangling identity and pose for facial expression recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A framework to model and control the state of presence in
virtual reality systems. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(4), 1854–1867. (<a
href="https://doi.org/10.1109/TAFFC.2022.3195697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This work is concerned with the modelling and control of the state of presence of a user in a virtual reality system (VRS). The manner how the state of a user interacting with the VRS is measured by some of its physiological signals is discussed. Based on the user-VRS interaction, a novel feedback control schema to manipulate the user state is proposed. In this, the error between the required and the actual user behavior is used by a controller to select the appropriate stimulus that must be executed by the VRS core scenario to reduce the error. The schema includes the modelling of the presence state by a discrete event strategy based on Petri Nets. Based on the obtained model, the control strategy to select the appropriate stimuli is devised. Finally, some real experiments with a VRS prototype are carried out to show the effectiveness of the proposal.},
  archive  = {J},
  author   = {Gustavo Hernández-Melgarejo and Alberto Luviano-Juárez and Rita Quetziquel Fuentes-Aguilar},
  doi      = {10.1109/TAFFC.2022.3195697},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1854-1867},
  title    = {A framework to model and control the state of presence in virtual reality systems},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Training socially engaging robots: Modeling backchannel
behaviors with batch reinforcement learning. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(4), 1840–1853. (<a
href="https://doi.org/10.1109/TAFFC.2022.3190233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A key aspect of social human-robot interaction is natural non-verbal communication. In this work, we train an agent with batch reinforcement learning to generate nods and smiles as backchannels in order to increase the naturalness of the interaction and to engage humans. We introduce the Sequential Random Deep Q-Network (SRDQN) method to learn a policy for backchannel generation, that explicitly maximizes user engagement. The proposed SRDQN method outperforms the existing vanilla Q-learning methods when evaluated using off-policy policy evaluation techniques. Furthermore, to verify the effectiveness of SRDQN, a human-robot experiment has been designed and conducted with an expressive 3d robot head. The experiment is based on a story-shaping game designed to create an interactive social activity with the robot. The engagement of the participants during the interaction is computed from user&#39;s social signals like backchannels, mutual gaze and adjacency pair. The subjective feedback from participants and the engagement values strongly indicate that our framework is a step forward towards the autonomous learning of a socially acceptable backchanneling behavior.},
  archive  = {J},
  author   = {Nusrah Hussain and Engin Erzin and T. Metin Sezgin and Yücel Yemez},
  doi      = {10.1109/TAFFC.2022.3190233},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1840-1853},
  title    = {Training socially engaging robots: Modeling backchannel behaviors with batch reinforcement learning},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EEG-video emotion-based summarization: Learning with EEG
auxiliary signals. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(4), 1827–1839. (<a
href="https://doi.org/10.1109/TAFFC.2022.3208259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Video summarization is the process of selecting a subset of informative keyframes to expedite storytelling with limited loss of information. In this article, we propose an EEG-Video Emotion-based Summarization (EVES) model based on a multimodal deep reinforcement learning (DRL) architecture that leverages neural signals to learn visual interestingness to produce quantitatively and qualitatively better video summaries. As such, EVES does not learn from the expensive human annotations but the multimodal signals. Furthermore, to ensure the temporal alignment and minimize the modality gap between the visual and EEG modalities, we introduce a Time Synchronization Module (TSM) that uses an attention mechanism to transform the EEG representations onto the visual representation space. We evaluate the performance of EVES on the TVSum and SumMe datasets. Based on the rank order statistics benchmarks, the experimental results show that EVES outperforms the unsupervised models and narrows the performance gap with supervised models. Furthermore, the human evaluation scores show that EVES receives a higher rating than the state-of-the-art DRL model DR-DSN by 11.4% on the coherency of the content and 7.4% on the emotion-evoking content. Thus, our work demonstrates the potential of EVES in selecting interesting content that is both coherent and emotion-evoking.},
  archive  = {J},
  author   = {Wai-Cheong Lincoln Lew and Di Wang and Kai Keng Ang and Joo-Hwee Lim and Chai Quek and Ah-Hwee Tan},
  doi      = {10.1109/TAFFC.2022.3208259},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1827-1839},
  title    = {EEG-video emotion-based summarization: Learning with EEG auxiliary signals},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic estimation of self-reported pain by trajectory
analysis in the manifold of fixed rank positive semi-definite matrices.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(4),
1813–1826. (<a
href="https://doi.org/10.1109/TAFFC.2022.3207001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose an automatic method to estimate self-reported pain intensity based on facial landmarks extracted from videos. For each video sequence, we decompose the face into four different regions and pain intensity is measured by modeling the dynamics of facial movement using the landmarks of these regions. A formulation based on Gram matrices is used to represent the trajectory of facial landmarks on the Riemannian manifold of symmetric positive semi-definite matrices of fixed rank. A curve fitting algorithm is then used to smooth the trajectories and a temporal alignment is performed to compute the similarity between the trajectories on the manifold. A Support Vector Regression classifier is then trained to encode the extracted trajectories into pain intensity levels consistent with the self-reported pain intensity measurement. Finally, a late fusion of the estimation for each region is performed to obtain the final predicted pain intensity level. The proposed approach is evaluated on two publicly available databases, the UNBCMcMaster Shoulder Pain Archive and the Biovid Heat Pain database. We compared our method to the state-of-the-art on both databases using different testing protocols, showing the competitiveness of the proposed approach.},
  archive  = {J},
  author   = {Benjamin Szczapa and Mohamed Daoudi and Stefano Berretti and Pietro Pala and Alberto Del Bimbo and Zakia Hammal},
  doi      = {10.1109/TAFFC.2022.3207001},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1813-1826},
  title    = {Automatic estimation of self-reported pain by trajectory analysis in the manifold of fixed rank positive semi-definite matrices},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). What lies beneath—a survey of affective theory use in
computational models of emotion. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(4), 1793–1812. (<a
href="https://doi.org/10.1109/TAFFC.2022.3197456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Studying and developing systems that can recognize, express, and “have” emotions is called affective computing . To create a Computational Model of Emotion (CME), one must first identify what kind of system to build, then find emotion theories that match its requirements. The relevant literature is vast. This survey aims to help design CMEs that generate emotions —separated into emotion representation and elicitation tasks—in computer agents and interfaces. We give an overview of 67 CMEs from different domains, and identify which emotion theories they use and why. To better understand why CMEs use some theories, we also analyze instances where these CMEs use theories to express emotion . Lastly we summarize how CMEs generally use each theory. The survey is meant to be a guideline for deciding which affective theories to use for new emotion-generating CME designs.},
  archive  = {J},
  author   = {Geneva M. Smith and Jacques Carette},
  doi      = {10.1109/TAFFC.2022.3197456},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1793-1812},
  title    = {What lies Beneath—A survey of affective theory use in computational models of emotion},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modulation of driver’s emotional states by manipulating
in-vehicle environment: Validation with biosignals recorded in an actual
car environment. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(4), 1783–1792. (<a
href="https://doi.org/10.1109/TAFFC.2022.3206222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A driver&#39;s emotional state can affect driving performance. According to the studies on the driving performance based on the circumplex (arousal-valence) model of affect, negative emotions such as anger and sadness can severely hinder safe driving. In this study, we developed a system to modulate drivers’ emotions to designated emotional states by manipulating in-vehicle environments, such as ambient lighting, background music, scent, ventilation, and rear curtains. The proposed system, named the “mood-modulator” system, consists of four different modes, designed to induce different emotional states. The feasibility of the “mood-modulator” system was evaluated using electroencephalogram (EEG) and photoplethysmogram (PPG) signals recorded from 48 drivers in an actual car environment. In the experiments, negative emotions were induced for each participant using short movie clips. Then, one of the four modes (different in-vehicle environments) was executed, during which both EEG and PPG data were acquired. We quantitatively evaluated whether each mode could effectively induce targeted emotional valence using machine learning classifier models, individually constructed from EEG data recorded during calibration sessions. The modulation of emotional arousal by each mode was also assessed using heart rate and respiration rate extracted from the PPG data. Our results demonstrated that the four modes could effectively increase the participant&#39;s emotional valence and modulate emotional arousal state to the intended direction. To the best of our knowledge, this is the first study to quantitatively evaluate a system that modulates a driver&#39;s emotional state using biosignals recorded in an actual car.},
  archive  = {J},
  author   = {Hodam Kim and Suhye Kim and Hongmin Kim and Youngsoo Ji and Chang-Hwan Im},
  doi      = {10.1109/TAFFC.2022.3206222},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1783-1792},
  title    = {Modulation of driver&#39;s emotional states by manipulating in-vehicle environment: Validation with biosignals recorded in an actual car environment},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving the performance of sentiment analysis using
enhanced preprocessing technique and artificial neural network. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(4), 1771–1782. (<a
href="https://doi.org/10.1109/TAFFC.2022.3206891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the presence of a massive amount of digitally recorded data, an automated computation can be preferable over the manual approach to evaluate sentiments within given textual fragments. Artificial neural network (ANN) is preferred for sentiment analysis (SA) because of its learning ability and adaptive nature towards diverse data. Handling negation in SA is a challenging task, and to address the same, we propose a specific order of preprocessing (PPR) steps to enhance the performance of SA using ANN. Typically, ANN weights are randomly initialized (R-ANN), which may not give the desired performance. As a potential solution, we propose a novel approach named Matching features with output label based Advanced Technique (MAT) to initialize the ANN weights (MAT-ANN). Simulation results conclude the superiority of the proposed approach PPR+MAT-ANN compared to the existing approach EPR+R-ANN i.e., integrating existing preprocessing (EPR) steps with R-ANN. Moreover, PPR+MAT-ANN architecture is significantly simpler than the existing deep learning-based approach named the NeuroSent tool and gives better performance when evaluated upon the Dranziera protocol.},
  archive  = {J},
  author   = {Ankit Thakkar and Dhara Mungra and Anjali Agrawal and Kinjal Chaudhari},
  doi      = {10.1109/TAFFC.2022.3206891},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1771-1782},
  title    = {Improving the performance of sentiment analysis using enhanced preprocessing technique and artificial neural network},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Immersion measurement in watching videos using eye-tracking
data. <em>IEEE Transactions on Affective Computing</em>, <em>13</em>(4),
1759–1770. (<a
href="https://doi.org/10.1109/TAFFC.2022.3209311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Immersion plays a crucial role in video watching, leading viewers to a positive experience, such as increased engagement and decreased fatigue. However, few studies measure immersion while watching videos, and questionnaires are typically used in the measurement of immersion for other applications. These methods may rely on the viewer&#39;s memory and cause biased results. Therefore, we propose an objective immersion detection model by leveraging people&#39;s gaze behavior while watching videos. In a lab study with 30 participants, an in-depth analysis is carried out on a number of gaze features and machine learning (ML) models to identify the immersion state. Several gaze features are highly indicative of immersion and ML models with these features are able to detect an immersion state of video watchers. Post-hoc interviews demonstrate that our approach is applicable to measure immersion in the middle of watching a video, where some practical issues are discussed as well.},
  archive  = {J},
  author   = {Youjin Choi and JooYeong Kim and Jin-Hyuk Hong},
  doi      = {10.1109/TAFFC.2022.3209311},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1759-1770},
  title    = {Immersion measurement in watching videos using eye-tracking data},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Causal narrative comprehension: A new perspective for
emotion cause extraction. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(4), 1743–1758. (<a
href="https://doi.org/10.1109/TAFFC.2022.3206960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion Cause Extraction (ECE) aims to reveal the cause clauses behind a given emotion expressed in a text, which has become an emerging topic in broad research communities, such as affective computing and natural language processing. Despite the fact that current methods about the ECE task have made great progress in text semantic understanding from lexicon- and sentence-level, they always ignore the certain causal narratives of emotion text. Significantly, these causal narratives are presented in the form of semantic structure and highly helpful for structure-level emotion cause understanding. Nevertheless, causal narrative is just an abstract narratological concept and its involving semantics is quite different from the common sequential information. Thus, how to properly model and utilize such particular narrative information to boost the ECE performance still remains an unresolved challenge. To this end, in this paper, we propose a novel Causal Narrative Comprehension Model (CNCM) for emotion cause extraction, which learns and leverages causal narrative information smartly to address the above problem. Specifically, we develop a Narrative-aware Causal Association (NCA) unit, which mines the narrative cue about emotional results and uses the semantic correlation between causes and results to model causal narratives of documents. Besides, we design a Result-aware Emotion Attention (REA) unit to make full use of the known result of causal narrative for multiple understanding about emotional causal associations. Through the ingenious combination and collaborative utilization of these two units, we could better identify the emotion cause in the text with causal narrative comprehension. Extensive experiments on the public English and Chinese benchmark datasets of ECE task have validated the effectiveness of CNCM with significant margin by comparing with the state-of-the-art baselines, which demonstrates the potential of narrative information in long text understanding.},
  archive  = {J},
  author   = {Wei Cao and Kun Zhang and Shulan Ruan and Hanqing Tao and Sirui Zhao and Hao Wang and Qi Liu and Enhong Chen},
  doi      = {10.1109/TAFFC.2022.3206960},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1743-1758},
  title    = {Causal narrative comprehension: A new perspective for emotion cause extraction},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). IAF-LG: An interactive attention fusion network with local
and global perspective for aspect-based sentiment analysis. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(4), 1730–1742. (<a
href="https://doi.org/10.1109/TAFFC.2022.3208216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {One of the interesting trending phenomena in sentiment analysis is the prediction of sentiment given by the user towards an aspect term. Till today, a considerable number of researchers have proposed varying methodologies for predicting aspect-based sentiments. But they mostly encapsulate the semantic information by manifesting themselves within a local boundary around each aspect term and overlook capturing the semantic concept that is conveyed within the entire review (global). Therefore, this study proposes a model, IAF-LG , that performs semantic learning at both local and global scales to discover aspect-based sentiments. IAF-LG first encodes the local semantics by fusing contextual-semantic dependencies between tokens and computing relational semantics between inter-aspects. Next, it develops the global semantics by formulating interactions between local semantics and review-based sentiment learning. Lastly, it conjoins the local and global interactive learning to earn credible semantics for predicting the accurate sentiment of aspect terms. Extensive experiments on publicly available datasets demonstrate the significantly improved performance of IAF-LG than competitive baselines.},
  archive  = {J},
  author   = {Ambreen Nazir and Yuan Rao and Lianwei Wu and Ling Sun},
  doi      = {10.1109/TAFFC.2022.3208216},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1730-1742},
  title    = {IAF-LG: An interactive attention fusion network with local and global perspective for aspect-based sentiment analysis},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Aspect-based sentiment quantification. <em>IEEE Transactions
on Affective Computing</em>, <em>13</em>(4), 1718–1729. (<a
href="https://doi.org/10.1109/TAFFC.2022.3218504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the current literature, many methods have been devised for sentiment quantification. In this work, we propose AspEntQuaNet, one of the first methods for aspect-based sentiment quantification. It extends the state-of-the-art QuaNet deep learning method for sentiment quantification in two ways. First, it considers aspects and ternary sentiment quantification concerning these aspects instead of binary sentiment quantification. Second, it improves on the results of QuaNet with an entropy-based sorting procedure instead of multisorting. Other sentiment quantification methods have also been adapted for ternary sentiment quantification instead of binary sentiment quantification. Using the modified version of the SemEval 2016 dataset for aspect-based sentiment quantification, we show that AspEntQuaNet is superior to all other considered existing methods based on obtained results for various aspect categories. In particular, AspEntQuaNet outperforms QuaNet often by a factor of 2 on all considered evaluation measures.},
  archive  = {J},
  author   = {Vladyslav Matsiiako and Flavius Frasincar and David Boekestijn},
  doi      = {10.1109/TAFFC.2022.3218504},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1718-1729},
  title    = {Aspect-based sentiment quantification},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Affective dynamics and cognition during game-based learning.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(4),
1705–1717. (<a
href="https://doi.org/10.1109/TAFFC.2022.3210755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Inability to regulate affective states can impact one&#39;s capacity to engage in higher-order thinking like scientific reasoning with game-based learning environments. Many efforts have been made to build affect-aware systems to mitigate the potentially detrimental effects of negative affect. Yet, gaps in research exist since accurately capturing and modeling affect as a state that changes dynamically over time is methodologically and analytically challenging. In this paper, we calculated multilevel mixed effects growth models to assess whether seventy-eight participants’ ( n = 78) time engaging in scientific reasoning (via logfiles and eye gaze) were related to time facially expressing confused, frustrated, and neutral states (via facial recognition software) during game-based learning with Crystal Island. The fitted model estimated significant positive relations between the time learners facially expressed confusion, frustration, and neutral states and time engaging in scientific-reasoning actions. The time individual learners facially expressed frustrated, confused, and neutral states explained a significant amount of variation in time engaging in scientific reasoning. Our finding emphasize that individual differences and agency may play a important role on relations between affective states, their dynamics, and higher-order cognition during game-based learning. Designing affect-aware game-based learning environments that track the dynamics within individual learners’ affective states may best support cognition.},
  archive  = {J},
  author   = {Elizabeth B. Cloude and Daryn A. Dever and Debbie L. Hahs-Vaughn and Andrew J. Emerson and Roger Azevedo and James Lester},
  doi      = {10.1109/TAFFC.2022.3210755},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1705-1717},
  title    = {Affective dynamics and cognition during game-based learning},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Aspect-opinion correlation aware and knowledge-expansion few
shot cross-domain sentiment classification. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(4), 1691–1704. (<a
href="https://doi.org/10.1109/TAFFC.2022.3205358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Cross-domain sentiment analysis has recently attracted significant attention, which can effectively alleviate the problem of lacking large-scale labeled data for deep neural network based methods. However, most of the existing cross-domain sentiment classification models neglect the domain-specific features, which limits their performance especially when the domain discrepancy becomes larger. Meanwhile, the relations between the aspect and opinion terms cannot be effectively modeled and thus the sentiment transfer error problem is suffered in the existing unsupervised domain-adaptation methods. To address these two issues, we propose an aspect-opinion correlation aware and knowledge-expansion few shot cross-domain sentiment classification model. Sentiment classification can be effectively conducted with only a few support instances of the target domain. Extensive experiments are conducted and the experimental results show the effectiveness of our proposed model.},
  archive  = {J},
  author   = {Haopeng Ren and Yi Cai and Yushi Zeng and Jinghui Ye and Ho-fung Leung and Qing Li},
  doi      = {10.1109/TAFFC.2022.3205358},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1691-1704},
  title    = {Aspect-opinion correlation aware and knowledge-expansion few shot cross-domain sentiment classification},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic prediction of group cohesiveness in images.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(3),
1677–1690. (<a
href="https://doi.org/10.1109/TAFFC.2020.3026095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article discusses the prediction of cohesiveness of a group of people in images. The cohesiveness of a group is an essential indicator of the emotional state, structure, and success of the group. We study the factors that influence the perception of group-level cohesion and propose methods for estimating the human-perceived cohesion on the group cohesiveness scale. To identify the visual cues (attributes) for cohesion, we conducted a user survey. Image analysis is performed at a group-level via a multi-task convolutional neural network. A capsule network is explored for analyzing the contribution of facial expressions of the group members on predicting the Group Cohesion Score (GCS). We add GCS to the Group Affect database and propose the ‘GAF-Cohesion database’. The proposed model performs well on the database and achieves near human-level performance in predicting a group&#39;s cohesion score. It is interesting to note that group cohesion as an attribute, when jointly trained for group-level emotion prediction, helps in increasing the performance for the later task. This suggests that group-level emotion and cohesion are correlated. Further, we investigate the effect of face-level similarity, body pose and subset of a group on the task of automatic cohesion perception.},
  archive  = {J},
  author   = {Shreya Ghosh and Abhinav Dhall and Nicu Sebe and Tom Gedeon},
  doi      = {10.1109/TAFFC.2020.3026095},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1677-1690},
  title    = {Automatic prediction of group cohesiveness in images},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Abnormal attentional bias of non-drug reward in abstinent
heroin addicts: An ERP study. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(3), 1664–1676. (<a
href="https://doi.org/10.1109/TAFFC.2020.3025799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Drug addicts are characterized by difficulty neglecting monetary reward, but its underlying neural mechanisms remain unclear. The current study aimed to investigate the behavioral and electrophysiological signatures of abnormal attentional bias based on different amounts of reward in abstinent heroin addicts (AHAs). We used a modified attentional capture task while recording EEG in 18 AHAs and 18 age-, gander-, and education-matched healthy controls (HCs). We analyzed the attentional distribution of the relative positional changes in space of the target and reward-related stimulus. When targets integrated reward-related colors, participants were more responsive and deployed more attention to targets, especially those with high-value colors. When targets and reward-related distractors were spatially separated, high-value distractors captured the AHA&#39;s attention and slowed their responses. Moreover, AHAs had weaker attentional control than HCs, exhibiting an inability to suppress the attentional bias driven by high-value stimuli. Overall, these results demonstrated that AHAs was hypersensitive to task-irrelevant and previous reward-related stimuli, possibly due to damage to brain reward circuits caused by chronic heroin abuse. Our work provides novel behavioral and neurophysiological evidence that are closely associated with the maintenance and relapse of addiction.},
  archive  = {J},
  author   = {Yanrong Hao and Yonghui Li and Jianxiu Li and Hong Peng and Qinglin Zhao and Bin Hu},
  doi      = {10.1109/TAFFC.2020.3025799},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1664-1676},
  title    = {Abnormal attentional bias of non-drug reward in abstinent heroin addicts: An ERP study},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling vocal entrainment in conversational speech using
deep unsupervised learning. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(3), 1651–1663. (<a
href="https://doi.org/10.1109/TAFFC.2020.3024972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In interpersonal spoken interactions, individuals tend to adapt to their conversation partner&#39;s vocal characteristics to become similar, a phenomenon known as entrainment. A majority of the previous computational approaches are often knowledge driven and linear and fail to capture the inherent nonlinearity of entrainment. In this article, we present an unsupervised deep learning framework to derive a representation from speech features containing information relevant for vocal entrainment. We investigate both an encoding based approach and a more robust triplet network based approach within the proposed framework. We also propose a number of distance measures in the representation space and use them for quantification of entrainment. We first validate the proposed distances by using them to distinguish real conversations from fake ones. Then we also demonstrate their applications in relation to modeling several entrainment-relevant behaviors in observational psychotherapy, namely agreement, blame and emotional bond.},
  archive  = {J},
  author   = {Md Nasir and Brian Baucom and Craig Bryan and Shrikanth Narayanan and Panayiotis Georgiou},
  doi      = {10.1109/TAFFC.2020.3024972},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1651-1663},
  title    = {Modeling vocal entrainment in conversational speech using deep unsupervised learning},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhancement of movement intention detection using EEG
signals responsive to emotional music stimulus. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(3), 1637–1650. (<a
href="https://doi.org/10.1109/TAFFC.2020.3025004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent psychological and neurological studies suggest that human motor preparation and execution are largely affected by the subjective emotional state. Thus, external emotion stimuli can be a potential tool to enhance the detectability of movement intention from pre-movement neural signals. This article investigated whether emotion-evoking music stimulus could improve the performances of a fully predictive Brain-Computer Interface (BCI) system for movement intention detection. For this purpose, electro encephalo graphical (EEG) signals were recorded from twelve healthy subjects under three emotional conditions: happy, sad, and neutral. The emotions were elicited using external music stimuli while they performed a wrist extension action. Additionally, support vector machine-based offline and pseudo online testing schemes were employed to solve a binary classification problem for determining movement intention from pre-movement EEG. EEG power analysis showed that happy music stimulus resulted in an early occurrence of event-related desynchronization in alpha band compared to other emotional states. Happy emotional stimuli also resulted in comparatively better performances in both offline and pseudo online testing paradigms. The results of this article suggest that external happy music stimulus could enhance the early and accurate detectability of human self-paced movement intention and thus could contribute to the predictive capability of state-of-the-art assistive BCIs.},
  archive  = {J},
  author   = {S M Shafiul Hasan and Masudur R. Siddiquee and J. Sebastian Marquez and Ou Bai},
  doi      = {10.1109/TAFFC.2020.3025004},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1637-1650},
  title    = {Enhancement of movement intention detection using EEG signals responsive to emotional music stimulus},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reading personality preferences from motion patterns in
computer mouse operations. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(3), 1619–1636. (<a
href="https://doi.org/10.1109/TAFFC.2020.3023296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Personality not only plays essential roles in people&#39;s real lives, but also becomes an important factor for various online services. Traditional approaches to personality assessment usually ask users to answer a long list of questions and are thus not practical in many online systems. It is more desirable to use some universally available data in the system to perform personality assessment. This article reports a controlled study to investigate common mouse operations as a potential new type of data source for online personality assessment. We establish an elaborate personality-mouse behavior dataset from 146 subjects and propose kinematic and adjustment features to characterize mouse motion patterns. Statistical approaches and machine learning algorithms are employed to examine the connections between personality preferences and mouse motion features via correlation analysis, discernibility analysis, and personality recognition experiments. The results reveal some interesting expressions of cognitive personality perspectives reflected in mouse motion patterns, such as fast starting acceleration and better controller . Performance evaluation shows it is possible to recognize different personality preferences using mouse motion features with accuracies ranging from 60.6 to 78.3 percent. Our findings suggest a potential to use mouse operational behaviors as a new data source for personality assessment in various information systems.},
  archive  = {J},
  author   = {Yinghui Zhao and Danmin Miao and Zhongmin Cai},
  doi      = {10.1109/TAFFC.2020.3023296},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1619-1636},
  title    = {Reading personality preferences from motion patterns in computer mouse operations},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Facial depression recognition by deep joint label
distribution and metric learning. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(3), 1605–1618. (<a
href="https://doi.org/10.1109/TAFFC.2020.3022732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {While existing prediction models built on popular deep architectures have shown promising results in facial depression recognition, they still lack sufficient discriminative power due to the issues of 1) limited amount of labeled depression data for deep representation learning and, 2) large variation in facial expression across different persons of the same depression score and the subtle difference in facial expression across different depression levels. In this article, we formulate the facial depression recognition as a label distribution learning (LDL) problem, and propose a deep joint label distribution and metric learning (DJ-LDML) method to address these issues. In DJ-LDML, LDL exploits label relevance inherent in depression data to implicitly increase the amount of training data associated with each depression level without actually enlarging the dataset, while deep metric learning (DML) aims at learning a deep ordinal embedding with a specifically designed label-aware histogram loss, allowing semantics similarity between video sequences (described by ordinal labels) to be preserved for discriminative feature learning. The two learning modules in our DJ-LDML work collaboratively to enhance the representation ability and discriminative power of the deeply learned spatiotemporal feature, leading to improved depression prediction. We empirically evaluate our method on two benchmark datasets and the results demonstrate the effectiveness of our formulation.},
  archive  = {J},
  author   = {Xiuzhuang Zhou and Zeqiang Wei and Min Xu and Shan Qu and Guodong Guo},
  doi      = {10.1109/TAFFC.2020.3022732},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1605-1618},
  title    = {Facial depression recognition by deep joint label distribution and metric learning},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Persuasion-induced physiology as predictor of persuasion
effectiveness. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(3), 1593–1604. (<a
href="https://doi.org/10.1109/TAFFC.2020.3022109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Physiological responses to persuasion can help to increase our understanding of persuasive processes, and thereby the effectiveness of persuasive interventions. However, a clear relationship between psychophysiology and persuasion is not yet established. This article investigates if peripheral physiology predicts persuasion effectiveness, and whether peripheral physiology yields information that is not represented in other predictors of persuasion. We studied physiological reactions in the cardiovascular, electrodermal, facial muscle and respiratory systems of 75 participants while they read gain- or loss-framed persuasive messages advocating increased oral health care behavior. Persuasion effectiveness was measured as pre to post intervention changes in self-reported attitudes and intentions, as well as through changes in behavioral compliance over three weeks. Overall, participants showed stronger attitudes and intentions directly after the intervention (short-term persuasion), but did not show changes in behavior or attitudes two weeks later (no long-term persuasion). On an individual level, physiological reactivity parameters yielded additional information – next to self-report measures – to predict persuasion effectiveness on attitude, intention and behavioral compliance. To conclude, our findings suggested a positive relationship between physiological reactivity to persuasive messages and subsequent attitudes, intentions and behavior, and quantified the extra personalization that psychophysiological measures might bring to persuasive messaging.},
  archive  = {J},
  author   = {Hanne A. A. Spelt and Chao Zhang and Joyce H. D. M. Westerink and Jaap Ham and Wijnand A. IJsselsteijn},
  doi      = {10.1109/TAFFC.2020.3022109},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1593-1604},
  title    = {Persuasion-induced physiology as predictor of persuasion effectiveness},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep multiscale spatiotemporal network for assessing
depression from facial dynamics. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(3), 1581–1592. (<a
href="https://doi.org/10.1109/TAFFC.2020.3021755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, deep learning models have been successfully employed in many video-based affective computing applications (e.g., detecting pain, stress, and Alzheimer’s disease). One key application is automatic depression recognition – recognition of facial expressions associated with depressive behaviour. State-of-the-art deep learning algorithms to recognize depression typically explore spatial and temporal information individually, by using 2D convolutional neural networks (CNNs) to analyze appearance information, and then by either mapping facial feature variations or averaging the depression level over video frames. This approach has limitations in terms of its ability to represent dynamic information that can help to accurately discriminate between depression levels. In contrast, models based on 3D CNNs allow to directly encode the spatio-temporal relationships, although these models rely on temporal information with fixed range and single receptive field. This approach limits the ability to capture variations of facial expression with diverse ranges, and the exploitation of diverse facial areas. In this article, a novel 3D CNN architecture – the Multiscale Spatiotemporal Network (MSN) – is introduced to effectively represent facial information related to depressive behaviours from videos. The basic structure of the model is composed of parallel convolutional layers with different temporal depths and sizes of receptive field, which allows the MSN to explore a wide range of spatio-temporal variations in facial expressions. Experimental results on two benchmark datasets show that our MSN architecture is effective, outperforming state-of-the-art methods in automatic depression recognition.},
  archive  = {J},
  author   = {Wheidima Carneiro de Melo and Eric Granger and Abdenour Hadid},
  doi      = {10.1109/TAFFC.2020.3021755},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1581-1592},
  title    = {A deep multiscale spatiotemporal network for assessing depression from facial dynamics},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning continuous facial actions from speech for real-time
animation. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(3), 1567–1580. (<a
href="https://doi.org/10.1109/TAFFC.2020.3022017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speech conveys not only the verbal communication, but also emotions, manifested as facial expressions of the speaker. In this article, we present deep learning frameworks that directly infer facial expressions from just speech signals. Specifically, the time-varying contextual non-linear mapping between audio stream and micro facial movements is realized by our proposed recurrent neural networks to drive a 3D blendshape face model in real-time. Our models not only activate appropriate facial action units (AUs), defined as 3D expression blendshapes in the FaceWarehouse database, to depict different utterance generating actions in the form of lip movements, but also, without any assumption, automatically estimate emotional intensity of the speaker and reproduces her ever-changing affective states by adjusting strength of related facial unit activations. In the baseline models, conventional handcrafted acoustic features are utilized to predict facial actions. Furthermore, we show that it is more advantageous to learn meaningful acoustic feature representation from speech spectrograms with convolutional nets, which subsequently improves the accuracy of facial action synthesis. Experiments on diverse audiovisual corpora of different actors across a wide range of facial actions and emotional states show promising results of our approaches. Being speaker-independent, our generalized models are readily applicable to various tasks in human-machine interaction and animation.},
  archive  = {J},
  author   = {Hai X. Pham and Yuting Wang and Vladimir Pavlovic},
  doi      = {10.1109/TAFFC.2020.3022017},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1567-1580},
  title    = {Learning continuous facial actions from speech for real-time animation},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EmoSen: Generating sentiment and emotion controlled
responses in a multimodal dialogue system. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(3), 1555–1566. (<a
href="https://doi.org/10.1109/TAFFC.2020.3015491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {An essential skill for effective communication is the ability to express specific sentiment and emotion in a conversation. Any robust dialogue system should handle the combined effect of both sentiment and emotion while generating responses. This is expected to provide a better experience and concurrently increase users’ satisfaction. Previously, research on either emotion or sentiment controlled dialogue generation has shown great promise in developing the next generation conversational agents, but the simultaneous effect of both is still unexplored. The existing dialogue systems are majorly based on unimodal sources, predominantly the text, and thereby cannot utilize the information present in the other sources, such as video, audio, image, etc. In this article, we present at first a large scale benchmark Sentiment Emotion aware Multimodal Dialogue (SEMD) dataset for the task of sentiment and emotion controlled dialogue generation. The SEMD dataset consists of 55k conversations from 10 TV shows having text, audio, and video information. To utilize multimodal information, we propose multimodal attention based conditional variational autoencoder (M-CVAE) that outperforms several baselines. Quantitative and qualitative analyses show that multimodality, along with contextual information, plays an essential role in generating coherent and diverse responses for any given emotion and sentiment.},
  archive  = {J},
  author   = {Mauajama Firdaus and Hardik Chauhan and Asif Ekbal and Pushpak Bhattacharyya},
  doi      = {10.1109/TAFFC.2020.3015491},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1555-1566},
  title    = {EmoSen: Generating sentiment and emotion controlled responses in a multimodal dialogue system},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised ECG representation learning for emotion
recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(3), 1541–1554. (<a
href="https://doi.org/10.1109/TAFFC.2020.3014842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We exploit a self-supervised deep multi-task learning framework for electrocardiogram (ECG) -based emotion recognition. The proposed solution consists of two stages of learning a) learning ECG representations and b) learning to classify emotions. ECG representations are learned by a signal transformation recognition network. The network learns high-level abstract representations from unlabeled ECG data. Six different signal transformations are applied to the ECG signals, and transformation recognition is performed as pretext tasks. Training the model on pretext tasks helps the network learn spatiotemporal representations that generalize well across different datasets and different emotion categories. We transfer the weights of the self-supervised network to an emotion recognition network, where the convolutional layers are kept frozen and the dense layers are trained with labelled ECG data. We show that the proposed solution considerably improves the performance compared to a network trained using fully-supervised learning. New state-of-the-art results are set in classification of arousal, valence, affective states, and stress for the four utilized datasets. Extensive experiments are performed, providing interesting insights into the impact of using a multi-task self-supervised structure instead of a single-task model, as well as the optimum level of difficulty required for the pretext self-supervised tasks.},
  archive  = {J},
  author   = {Pritam Sarkar and Ali Etemad},
  doi      = {10.1109/TAFFC.2020.3014842},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1541-1554},
  title    = {Self-supervised ECG representation learning for emotion recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient LSTM network for emotion recognition from
multichannel EEG signals. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(3), 1528–1540. (<a
href="https://doi.org/10.1109/TAFFC.2020.3013711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Most previous EEG-based emotion recognition methods studied hand-crafted EEG features extracted from different electrodes. In this article, we study the relation among different EEG electrodes and propose a deep learning method to automatically extract the spatial features that characterize the functional relation between EEG signals at different electrodes. Our proposed deep model is called AT tention-based LSTM with D omain D iscriminator (ATDD-LSTM), a model based on Long Short-Term Memory (LSTM) for emotion recognition that can characterize nonlinear relations among EEG signals of different electrodes. To achieve state-of-the-art emotion recognition performance, the architecture of ATDD-LSTM has two distinguishing characteristics: (1) By applying the attention mechanism to the feature vectors produced by LSTM, ATDD-LSTM automatically selects suitable EEG channels for emotion recognition, which makes the learned model concentrate on the emotion related channels in response to a given emotion; (2) To minimize the significant feature distribution shift between different sessions and/or subjects, ATDD-LSTM uses a domain discriminator to modify the data representation space and generate domain-invariant features. We evaluate the proposed ATDD-LSTM model on three public EEG emotional databases (DEAP, SEED and CMEED) for emotion recognition. The experimental results demonstrate that our ATDD-LSTM model achieves superior performance on subject-dependent (for the same subject), subject-independent (for different subjects) and cross-session (for the same subject) evaluation.},
  archive  = {J},
  author   = {Xiaobing Du and Cuixia Ma and Guanhua Zhang and Jinyao Li and Yu-Kun Lai and Guozhen Zhao and Xiaoming Deng and Yong-Jin Liu and Hongan Wang},
  doi      = {10.1109/TAFFC.2020.3013711},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1528-1540},
  title    = {An efficient LSTM network for emotion recognition from multichannel EEG signals},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantitative personality predictions from a brief EEG
recording. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(3), 1514–1527. (<a
href="https://doi.org/10.1109/TAFFC.2020.3008775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The assessment of personality is crucial not only for scientific inquiries but also for real-world applications such as personnel selection. In this article, we propose and validate a novel implicit measure to predict an individual&#39;s levels in the Big Five personality traits from 5 minutes of electroencephalography (EEG) recordings. Participants viewed Chinese words with positive, negative, and neutral emotions. The multi-channel event-related potentials elicited by these emotional words were used to train a sparse regression model for personality prediction. Results from a large test sample of 196 participants indicated that the personality scores derived from the proposed measure reached significant correlations with a commonly used questionnaire (r = .50, .60, .49, .55, and .49 for agreeableness, conscientiousness, neuroticism, openness, and extraversion, respectively). The EEG-based personality scores showed good external validity as well, capable of predicting behavioral indices and psychological adjustment similar to self-reported scores. Besides, the EEG-based scores were relatively stable across time, as reflected by the test-retest reliability of .5 ∼ .7 for the five personality traits within a cohort of 33 participants 19-78 days later. These evaluations suggest that the proposed measure can serve as a viable alternative to conventional personality questionnaires in practice.},
  archive  = {J},
  author   = {Wenyu Li and Chengpeng Wu and Xin Hu and Jingjing Chen and Shimin Fu and Fei Wang and Dan Zhang},
  doi      = {10.1109/TAFFC.2020.3008775},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1514-1527},
  title    = {Quantitative personality predictions from a brief EEG recording},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Emotion dependent domain adaptation for speech driven
affective facial feature synthesis. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(3), 1501–1513. (<a
href="https://doi.org/10.1109/TAFFC.2020.3008456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Although speech driven facial animation has been studied extensively in the literature, works focusing on the affective content of the speech are limited. This is mostly due to the scarcity of affective audio-visual data. In this article, we improve the affective facial animation using domain adaptation by partially reducing the data scarcity. We first define a domain adaptation to map affective and neutral speech representations to a common latent space in which cross-domain bias is smaller. Then the domain adaptation is used to augment affective representations for each emotion category, including angry, disgust, fear, happy, sad, surprise, and neutral, so that we can better train emotion-dependent deep audio-to-visual (A2V) mapping models. Based on the emotion-dependent deep A2V models, the proposed affective facial synthesis system is realized in two stages: first, speech emotion recognition extracts soft emotion category likelihoods for the utterances; then a soft fusion of the emotion-dependent A2V mapping outputs form the affective facial synthesis. Experimental evaluations are performed on the SAVEE audio-visual dataset. The proposed models are assessed with objective and subjective evaluations. The proposed affective A2V system achieves significant MSE loss improvements in comparison to the recent literature. Furthermore, the resulting facial animations of the proposed system are preferred over the baseline animations in the subjective evaluations.},
  archive  = {J},
  author   = {Rizwan Sadiq and Engin Erzin},
  doi      = {10.1109/TAFFC.2020.3008456},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1501-1513},
  title    = {Emotion dependent domain adaptation for speech driven affective facial feature synthesis},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identifying cortical brain directed connectivity networks
from high-density EEG for emotion recognition. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(3), 1489–1500. (<a
href="https://doi.org/10.1109/TAFFC.2020.3006847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we investigate brain directed connectivity (BDC) networks for emotion recognition using electroencephalogram (EEG) source signals that were estimated from high-density sensor EEG signals, for the first time. Currently, a variety of features extracted from sensor EEG signals are used for emotion recognition. However, they cannot unambiguously describe the location of emotions associated with neural activities and information propagation or the interaction between brain regions. In addition, most current studies use low-density sensor EEG signals. Moreover, source signals estimated from high-density sensor EEG signal have not been employed for emotion recognition to date. We designed a BDC network-based framework using EEG source signals to investigate emotion recognition. The global cortex factor-based multivariate autoregressive (GCF-MVAR) method was utilized to extract emotion-related BDC features. Our study revealed that the combined BDC and DE features facilitated a recognition accuracy of up to 89.58 percent, which is higher than the rate obtained from BDC features and DE features alone. The sensor features derived from high-density EEG signals also exhibited higher recognition accuracy compared to low-density EEG signals. These findings suggest that BDC features derived from EEG source signals can better characterize human emotional states and are meaningful for emotion recognition.},
  archive  = {J},
  author   = {Hailing Wang and Xia Wu and Li Yao},
  doi      = {10.1109/TAFFC.2020.3006847},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1489-1500},
  title    = {Identifying cortical brain directed connectivity networks from high-density EEG for emotion recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conveying emotions through device-initiated touch. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(3), 1477–1488. (<a
href="https://doi.org/10.1109/TAFFC.2020.3008693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Humans have the ability to convey an array of emotions through complex and rich touch gestures. However, it is not clear how these touch gestures can be reproduced through interactive systems and devices in a remote mediated communication context. In this article, we explore the design space of device-initiated touch for conveying emotions with an interactive system reproducing a collection of human touch characteristics. For this purpose, we control a robotic arm to touch the forearm of participants with different force, velocity and amplitude characteristics to simulate human touch. In view of adding touch as an emotional modality in human-machine interaction, we have conducted two studies. After designing the touch device, we explore touch in a context-free setup and then in a controlled context defined by textual scenarios and emotional facial expressions of a virtual agent. Our results suggest that certain combinations of touch characteristics are associated with the perception of different degrees of valence and of arousal. Moreover, in the case of non-congruent mixed signals (touch, facial expression, textual scenario) not conveying a priori the same emotion, the message conveyed by touch seems to prevail over the ones displayed by the visual and textual signals.},
  archive  = {J},
  author   = {Marc Teyssier and Gilles Bailly and Catherine Pelachaud and Eric Lecolinet},
  doi      = {10.1109/TAFFC.2020.3008693},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1477-1488},
  title    = {Conveying emotions through device-initiated touch},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Phase space reconstruction driven spatio-temporal feature
learning for dynamic facial expression recognition. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(3), 1466–1476. (<a
href="https://doi.org/10.1109/TAFFC.2020.3007531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatic Dynamic Facial Expression Recognition (DFER) is a challenging task, since how to effectively capture facial temporal dynamics is still an open problem. In this article, we regard variations of facial expressions as a dynamic system in accord with certain rules, and try to explore the fundamental temporal properties for recognizing dynamic expressions. Inspired by the phase space reconstruction method for time series analysis, we propose a novel network named Phase Space Reconstruction Network (PSRNet) for learning spatio-temporal features of facial expressions. First, 3D convolutional neural networks are used to extract spatial and short-term temporal features, which indicate the state of each frame and are termed as observations in the phase space. All the observations compose the trajectory of the dynamical system. Then, a data-driven across-correlation matrix is inferred to reveal the relationship of the observations. With this matrix, the phase space reconstruction module reconstructs the trajectory by aggregating the observations adaptively in the phase space. Reconstructed observations represent the gradual process of dynamic facial expressions, which is beneficial to recognize these expressions. The experiment results on three databases (Oulu, MMI, and CK+) demonstrate that the proposed PSRNet can extract more informative and representative spatio-temporal features for DFER. Moreover, the visualization of intermediate features reveals that the reconstructed features have global consistency in facial regions and the underlying evolutionary pattern of dynamic facial expression.},
  archive  = {J},
  author   = {Shanmin Wang and Hui Shuai and Qingshan Liu},
  doi      = {10.1109/TAFFC.2020.3007531},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1466-1476},
  title    = {Phase space reconstruction driven spatio-temporal feature learning for dynamic facial expression recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RFAU: A database for facial action unit analysis in real
classrooms. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(3), 1452–1465. (<a
href="https://doi.org/10.1109/TAFFC.2020.3006392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion analysis of students plays an important role in teaching effect evaluation. To develop robust algorithms for emotion analysis of students, a database from real classrooms is required. However, most existing databases were collected from adults and constructed in laboratory settings. In this article, we present a manually-annotated facial action unit database from juveniles in real classrooms. Our database has three main characteristics: (1) it provides numerous education-related action units data from primary and high schools, complementing the vacancy of the publicly available educational action unit databases; (2) it contains 256,220 manually-annotated facial images of 1,796 juveniles, frame-by-frame annotated with 12 action units and 6-level intensities for each action unit; (3) it covers many challenges in the wild, including various head poses, low facial resolution, illuminations, and occlusions, supplementing action unit databases in the wild for research. The baselines for action unit detection and action unit intensity estimation are provided for future references. Especially, we apply the weighted balance loss to solve imbalances within and between labels. Our database will be available to the research community: http://www.dlc.sjtu.edu.cn/rfau .},
  archive  = {J},
  author   = {Qiaoping Hu and Chuanneng Mei and Fei Jiang and Ruimin Shen and Yitian Zhang and Ce Wang and Junpeng Zhang},
  doi      = {10.1109/TAFFC.2020.3006392},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1452-1465},
  title    = {RFAU: A database for facial action unit analysis in real classrooms},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Affective words and the company they keep: Studying the
accuracy of affective word lists in determining sentence and word
valence in a domain-specific corpus. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(3), 1440–1451. (<a
href="https://doi.org/10.1109/TAFFC.2020.3005613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we explore whether and how linguistic and pragmatic context can change individual word valence and emotionality in two parts. In the first part, we investigate whether sentence contexts retrieved from a domain-specific corpus (soccer) bias individual word affect. We then examine whether word valence with and without context accurately indicates sentence valence. In the second part, we compare word ratings from the first part to four different existing affective lexicons, with different levels of sensitivity to semantic and pragmatic context, and examine their accuracy in determining sentence valence. Results show a significant difference between words with and without context, the former more accurate in determining sentence valence than the latter. The preexisting lexicons were found to be similar to the individual word ratings collected in the first part of the study, with human-evaluated, context-sensitive lexicons being the most accurate in determining sentence valence. We discuss implications for emotion theory and bag-of-words approaches to sentiment analysis.},
  archive  = {J},
  author   = {Nadine Braun and Martijn Goudbeek and Emiel Krahmer},
  doi      = {10.1109/TAFFC.2020.3005613},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1440-1451},
  title    = {Affective words and the company they keep: Studying the accuracy of affective word lists in determining sentence and word valence in a domain-specific corpus},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adapted dynamic memory network for emotion recognition in
conversation. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(3), 1426–1439. (<a
href="https://doi.org/10.1109/TAFFC.2020.3005660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we address Emotion Recognition in Conversation (ERC) where conversational data are presented in a multimodal setting. Psychological evidence shows that self and inter-speaker influence are two central factors to emotion dynamics in conversation. State-of-the-art models do not effectively synthesise these two factors. Therefore, we propose an Adapted Dynamic Memory Network (A-DMN) where self and inter-speaker influences are modelled individually and further synthesised oriented towards the current utterance. Specifically, we model the dependency of the constituent utterances in a dialogue video using a global RNN to capture inter-speaker influence. Likewise, each speaker is assigned an RNN to capture their self influence. Afterwards, an Episodic Memory Module is devised to extract contexts for self and inter-speaker influence and synthesise them to update the memory. This process repeats itself for multiple passes until a refined representation is obtained and used for final prediction. Additionally, we explore cross-modal fusion in the context of multimodal ERC, and propose a convolution-based method which proves effective in extracting local interactions and computationally efficient. Extensive experiments demonstrate that A-DMN outperforms the state-of-the-art models on benchmark datasets.},
  archive  = {J},
  author   = {Songlong Xing and Sijie Mai and Haifeng Hu},
  doi      = {10.1109/TAFFC.2020.3005660},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1426-1439},
  title    = {Adapted dynamic memory network for emotion recognition in conversation},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stimulus sampling with 360-videos: Examining head movements,
arousal, presence, simulator sickness, and preference on a large sample
of participants and videos. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(3), 1416–1425. (<a
href="https://doi.org/10.1109/TAFFC.2020.3004617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As the public use of virtual reality (VR) scales, understanding how users engage across various sources of VR content is critical. 360-video is popular due to its ease of both creation and access. There are, however, few studies of 360-videos, and they suffer from three limitations. First, most studies rely on small and homogeneous samples of participants. Second, they tend to examine only a single 360-video, or a handful of them in a few exceptional cases. Third, very few studies trace participants’ VR use over multiple experiences. The current study examined a large sample of participants (511) and a large set of 360-videos (80). Each participant experienced 5 of the videos, and we tracked head movement in addition to self-report data on presence, arousal, simulator sickness, and future use intention for each video. This design allowed us to answer novel questions relating to individual differences of participants and changes in experience over time, and in general to present results of VR use at a scale not seen before in the literature. Moreover, the results suggest that looking at patterns across stimuli provide unique insights which are missed when looking only within a single piece of content.},
  archive  = {J},
  author   = {Hanseul Jun and Mark Roman Miller and Fernanda Herrera and Byron Reeves and Jeremy N. Bailenson},
  doi      = {10.1109/TAFFC.2020.3004617},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1416-1425},
  title    = {Stimulus sampling with 360-videos: Examining head movements, arousal, presence, simulator sickness, and preference on a large sample of participants and videos},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Affective video content analysis via multimodal deep quality
embedding network. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(3), 1401–1415. (<a
href="https://doi.org/10.1109/TAFFC.2020.3004114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The establishment of large video affective content analysis datasets, such as LIRIS-ACCEDE, opens up the possibility of utilizing the massive representation power of deep neural networks (DNNs) to model the complex process of eliciting affective responses from video viewers. However, label noise in these datasets poses a considerable challenge to both the training and evaluation of DNNs. The optimization of DNNs requires stochastic gradient descent (SGD), but label noise in the training set leads to an inaccurate estimate of the gradient, which may cause the model to converge to a nonoptima. In addition, label noise in the test set renders the results of model evaluation untrustworthy. In this article, we propose a multimodal deep quality embedding network (MMDQEN) for affective video content analysis. Specifically, MMDQEN can infer the latent label and label quality from the noisy training samples so that cleaner supervision signals are provided to the DNN-based affective classifier, and a tractable objective for MMDQEN is derived with variational inference and conditional independence assumption. In addition, to avoid model evaluation bias incurred by the annotation noise in the test set, new test sets based on the original LIRIS-ACCEDE database, which we name LIRIS-ACCEDE-RANK, are established where the samples are ranked according to their label uncertainty level, with corresponding evaluation metrics introduced accordingly to further reveal the performance of different models. Experiments conducted on both the LIRIS-ACCEDE and the LIRIS-ACCEDE-RANK datasets demonstrate the effectiveness of the proposed method.},
  archive  = {J},
  author   = {Yaochen Zhu and Zhenzhong Chen and Feng Wu},
  doi      = {10.1109/TAFFC.2020.3004114},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1401-1415},
  title    = {Affective video content analysis via multimodal deep quality embedding network},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Leaders and followers identified by emotional mimicry during
collaborative learning: A facial expression recognition study on
emotional valence. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(3), 1390–1400. (<a
href="https://doi.org/10.1109/TAFFC.2020.3003243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article explores the potential of emotional mimicry in identifying the leader and follower students in collaborative learning settings. Our data include video recorded interactions of 24 high school students who worked together in groups of three during a collaborative exam. A facial emotions recognition method was used to capture participants’ facial emotions during the collaborative work. Cross-recurrence quantification analysis was applied on the detected facial emotions to see the level and direction of emotional mimicry among the dyads in the same groups. In order to validate the cross-recurrence quantification analysis results, student interactions in terms of leading or following the task were video coded. Our findings showed that the leaders and followers identified by cross-recurrence quantification analysis findings matched the leaders and followers identified by the video coding in 70 percent of the dyadic interactions across the collaborating groups. The current findings show that video-based facial emotions recognition as a method can add to collaborative learning research, especially explaining some social, and affective dynamics about it. The study further discusses the possible variables that might confound the relationship between emotional mimicry and leader-follower interactions during collaboration.},
  archive  = {J},
  author   = {Muhterem Dindar and Sanna Järvelä and Sara Ahola and Xiaohua Huang and Guoying Zhao},
  doi      = {10.1109/TAFFC.2020.3003243},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1390-1400},
  title    = {Leaders and followers identified by emotional mimicry during collaborative learning: A facial expression recognition study on emotional valence},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). “Emotions are the great captains of our lives”: Measuring
moods through the power of physiological and environmental sensing.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(3),
1378–1389. (<a
href="https://doi.org/10.1109/TAFFC.2020.3003736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The article proposes the use of a smartwatch-based system for measuring the emotions of individuals in a classroom setting with respect to five mood variables: Activation, Tiredness, Pleasance, Quality of Presentation and Understanding. Internal (body) and external (environment) data such as movement, heart rate, noise, temperature and humidity were collected through the built-in sensors of the smartwatch. The system was verified by means of a longitudinal study that has been carried out in a series of workshops and lectures. Through experience-based sampling, participants were polled at periodic time intervals asking them to enter a self-assessment of the aforementioned mood states directly on the smartwatch. The goal was to demonstrate whether sensor data can be used to effectively predict the five moods. By resorting to a machine learning approach our system was able to predict the moods with an accuracy ranging between 89-95 percent for single-output classification, 92-99 percent for the chain classification task and of approximately 93 percent for the multi-output analysis. Our results showed also that body signals are better predictors compared to the external environmental variables. These results demonstrate and verify the potential of smartwatches in collecting and predicting human emotions, enabling dynamic feedback loops to enhance user experience.},
  archive  = {J},
  author   = {Keith April Araño and Peter Gloor and Carlotta Orsenigo and Carlo Vercellis},
  doi      = {10.1109/TAFFC.2020.3003736},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1378-1389},
  title    = {“Emotions are the great captains of our lives”: Measuring moods through the power of physiological and environmental sensing},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep temporal analysis for non-acted body affect
recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(3), 1366–1377. (<a
href="https://doi.org/10.1109/TAFFC.2020.3003816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the field of body affect recognition, the majority of literature is based on experiments performed on datasets where trained actors simulate emotional reactions. These acted and unnatural expressions differ from the more challenging genuine emotions, thus leading to less valuable results. In this article, a solution for basic non-acted emotion recognition based on 3D skeleton and Deep Neural Networks (DNNs) is provided. The proposed work introduces three majors contributions. First, temporal local movements performed by subjects are examined frame-by-frame, unlike the current state-of-the-art in non-acted body affect recognition where only static or global body features are considered. Second, an original set of global and time-dependent features for body movement description is provided. Third, this is one of the first works to use deep learning methods in the current non-acted body affect recognition literature. Due to the novelty of the topic, only the UCLIC dataset is currently considered the benchmark for comparative tests. On the latter, the proposed method outperforms all the competitors.},
  archive  = {J},
  author   = {Danilo Avola and Luigi Cinque and Alessio Fagioli and Gian Luca Foresti and Cristiano Massaroni},
  doi      = {10.1109/TAFFC.2020.3003816},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1366-1377},
  title    = {Deep temporal analysis for non-acted body affect recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adapting the interplay between personalized and generalized
affect recognition based on an unsupervised neural framework. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(3), 1349–1365. (<a
href="https://doi.org/10.1109/TAFFC.2020.3002657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent emotion recognition models, most of them being based on strongly supervised deep learning solutions, are rather successful in recognizing instantaneous emotion expressions. However, when applied to continuous interactions, these models show a weaker adaptation to a person-specific and long-term emotion appraisal. In this article, we present an unsupervised neural framework that improves emotion recognition by learning how to describe continuous affective behavior of individual persons. Our framework is composed of three self-organizing mechanisms: (1) a recurrent growing layer to cluster general emotion expressions, (2) a set of associative layers, acting as affective memories to model specific emotional behavior of individual persons, (3) and an online learning layer which provides contextual modeling of continuous emotion expressions. We propose different learning strategies to integrate all three mechanisms and to improve the performance on arousal and valence recognition of the OMG-Emotion dataset. We evaluate our model with a series of experiments ranging from ablation studies assessing the different contributions of each neural component to an objective comparison with state-of-the-art solutions. The results from the evaluations show a good performance on emotion recognition of continuous emotions on monologue videos. Furthermore, we discuss how the model self-regulates the interplay between generalized and personalized emotion perception and how this influences the model’s reliability when recognizing unseen emotion expressions.},
  archive  = {J},
  author   = {Pablo Barros and Emilia Barakova and Stefan Wermter},
  doi      = {10.1109/TAFFC.2020.3002657},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1349-1365},
  title    = {Adapting the interplay between personalized and generalized affect recognition based on an unsupervised neural framework},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lexicon-based sentiment convolutional neural networks for
online review analysis. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(3), 1337–1348. (<a
href="https://doi.org/10.1109/TAFFC.2020.2997769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the growing availability and popularity of sentiment-rich resources like blogs and online reviews, new opportunities and challenges have emerged regarding the identification, extraction, and organization of sentiments from user-generated documents or sentences. Recently, many studies have exploited lexicon-based methods or supervised learning algorithms to conduct sentiment analysis tasks separately; however, the former approaches ignore contextual information of sentences and the latter ones do not take sentiment information embedded in sentiment words into consideration. To tackle these limitations, we propose a new model named Sentiment Convolutional Neural Network (SentiCNN) to analyze the sentiments of sentences with both contextual and sentiment information of sentiment words, in which, contextual information is captured from word embeddings and sentiment information is identified using existing lexicons. We incorporate a Highway Network into our model to adaptively combine sentiment and contextual information from sentences by strengthening the connection between features of both sentences and their sentiment words. Furthermore, we propose three lexicon-based attention mechanisms (LBAMs) for our SentiCNN model to find the most important indicators of sentiments and make predictions more effectively. Experiments over two well-known datasets indicate that sentiment words, the Highway Network, and LBAMs contribute to sentiment analysis.},
  archive  = {J},
  author   = {Minghui Huang and Haoran Xie and Yanghui Rao and Yuwei Liu and Leonard K. M. Poon and Fu Lee Wang},
  doi      = {10.1109/TAFFC.2020.2997769},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1337-1348},
  title    = {Lexicon-based sentiment convolutional neural networks for online review analysis},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Facial expression recognition with active local shape
pattern and learned-size block representations. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(3), 1322–1336. (<a
href="https://doi.org/10.1109/TAFFC.2020.2995432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial expression recognition has been studied broadly, and several works using local micro-pattern descriptors have obtained significant results. There are, however, open questions: how to design a discriminative and robust feature descriptor?, how to select expression-related most influential features?, and how to represent the face descriptor exploiting the most salient parts of the face? In this article, we address these three issues to achieve better performance in recognizing facial expressions. First, we propose a new feature descriptor, namely Local Shape Pattern (LSP), that describes the local shape structure of a pixel’s neighborhood based on the prominent directional information by analyzing the statistics of the neighborhood gradient, which allows it to be robust against subtle local noise and distortion. Furthermore, we propose a selection strategy for learning the influential codes being active in the expression affiliated changes by selecting them exhibiting statistical dominance and high spatial variance. Lastly, we learn the size of the salient facial blocks to represent the facial description with the notion that changes in expressions vary in size and location. We conduct person-independent experiments in existing datasets after combining above three proposals, and obtain an improved performance for the facial expression recognition task.},
  archive  = {J},
  author   = {Md Tauhid Bin Iqbal and Byungyong Ryu and Adín Ramírez Rivera and Farkhod Makhmudkhujaev and Oksam Chae and Sung-Ho Bae},
  doi      = {10.1109/TAFFC.2020.2995432},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1322-1336},
  title    = {Facial expression recognition with active local shape pattern and learned-size block representations},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). What’s your laughter doing there? A taxonomy of the
pragmatic functions of laughter. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(3), 1302–1321. (<a
href="https://doi.org/10.1109/TAFFC.2020.2994533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Laughter is a crucial signal for communication and managing interactions. Until now no consensual approach has emerged for classifying laughter. We propose a new framework for laughter analysis and classification, based on the pivotal assumption that laughter has propositional content. We propose an annotation scheme to classify the pragmatic functions of laughter taking into account the form, the laughable, the social, situational, and linguistic context. We apply the framework and taxonomy proposed in a multilingual corpus study (French, Mandarin Chinese, and English), involving a variety of situational contexts. Our results give rise to novel generalizations about the range of meanings laughter exhibits, the placement of the laughable, and how placement and arousal relate to the functions of laughter. We have tested and refuted the validity of the commonly accepted assumption that laughter directly follows its laughable. In the concluding section, we discuss the implications our work has for spoken dialogue systems. We stress that laughter integration in spoken dialogue systems is not only crucial for emotional and affective computing aspects, but also for aspects related to natural language understanding and pragmatic reasoning. We formulate the emergent computational challenges for incorporating laughter in spoken dialogue systems.},
  archive  = {J},
  author   = {Chiara Mazzocconi and Ye Tian and Jonathan Ginzburg},
  doi      = {10.1109/TAFFC.2020.2994533},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1302-1321},
  title    = {What’s your laughter doing there? a taxonomy of the pragmatic functions of laughter},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EEG-based emotion recognition using regularized graph neural
networks. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(3), 1290–1301. (<a
href="https://doi.org/10.1109/TAFFC.2020.2994159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Electroencephalography (EEG) measures the neuronal activities in different brain regions via electrodes. Many existing studies on EEG-based emotion recognition do not fully exploit the topology of EEG channels. In this article, we propose a regularized graph neural network (RGNN) for EEG-based emotion recognition. RGNN considers the biological topology among different brain regions to capture both local and global relations among different EEG channels. Specifically, we model the inter-channel relations in EEG signals via an adjacency matrix in a graph neural network where the connection and sparseness of the adjacency matrix are inspired by neuroscience theories of human brain organization. In addition, we propose two regularizers, namely node-wise domain adversarial training (NodeDAT) and emotion-aware distribution learning (EmotionDL), to better handle cross-subject EEG variations and noisy labels, respectively. Extensive experiments on two public datasets, SEED, and SEED-IV, demonstrate the superior performance of our model than state-of-the-art models in most experimental settings. Moreover, ablation studies show that the proposed adjacency matrix and two regularizers contribute consistent and significant gain to the performance of our RGNN model. Finally, investigations on the neuronal activities reveal important brain regions and inter-channel relations for EEG-based emotion recognition.},
  archive  = {J},
  author   = {Peixiang Zhong and Di Wang and Chunyan Miao},
  doi      = {10.1109/TAFFC.2020.2994159},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1290-1301},
  title    = {EEG-based emotion recognition using regularized graph neural networks},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Facial action unit detection using attention and relation
learning. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(3), 1274–1289. (<a
href="https://doi.org/10.1109/TAFFC.2019.2948635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Attention mechanism has recently attracted increasing attentions in the field of facial action unit (AU) detection. By finding the region of interest of each AU with the attention mechanism, AU-related local features can be captured. Most of the existing attention based AU detection works use prior knowledge to predefine fixed attentions or refine the predefined attentions within a small range, which limits their capacity to model various AUs. In this paper, we propose an end-to-end deep learning based attention and relation learning framework for AU detection with only AU labels, which has not been explored before. In particular, multi-scale features shared by each AU are learned first, and then both channel-wise and spatial attentions are adaptively learned to select and extract AU-related local features. Moreover, pixel-level relations for AUs are further captured to refine spatial attentions so as to extract more relevant local features. Without changing the network architecture, our framework can be easily extended for AU intensity estimation. Extensive experiments show that our framework (i) soundly outperforms the state-of-the-art methods for both AU detection and AU intensity estimation on the challenging BP4D, DISFA, FERA 2015, and BP4D+ benchmarks, (ii) can adaptively capture the correlated regions of each AU, and (iii) also works well under severe occlusions and large poses.},
  archive  = {J},
  author   = {Zhiwen Shao and Zhilei Liu and Jianfei Cai and Yunsheng Wu and Lizhuang Ma},
  doi      = {10.1109/TAFFC.2019.2948635},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1274-1289},
  title    = {Facial action unit detection using attention and relation learning},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Normative emotional agents: A viewpoint paper. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(3), 1254–1273. (<a
href="https://doi.org/10.1109/TAFFC.2020.3028512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Human social relationships imply conforming to the norms, behaviors, and cultural values of the society, but also socialization of emotions, to learn how to interpret and show them. In multiagent systems, much progress has been made in the analysis and interpretation of both emotions and norms. Nonetheless, the relationship between emotions and norms has hardly been considered and most normative agents do not consider emotions, or vice-versa. In this article, we provide an overview of relevant aspects within the area of normative agents and emotional agents. First we focus on the concept of norm, the different types of norms, its life cycle and a review of multiagent normative systems. Second, we present the most relevant theories of emotions, the life cycle of an agent’s emotions, and how emotions have been included through computational models in multiagent systems. Next, we present an analysis of proposals that integrate emotions and norms in multiagent systems. From this analysis, four relationships are detected between norms and emotions, which we analyze in detail and discuss how these relationships have been tackled in the reviewed proposals. Finally, we present a proposal for an abstract architecture of a Normative Emotional Agent that covers these four norm-emotion relationships.},
  archive  = {J},
  author   = {Estefanía Argente and E. Del Val and D. Pérez-García and V. Botti},
  doi      = {10.1109/TAFFC.2020.3028512},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1254-1273},
  title    = {Normative emotional agents: A viewpoint paper},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ethics and good practice in computational paralinguistics.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(3),
1236–1253. (<a
href="https://doi.org/10.1109/TAFFC.2020.3021015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the advent of ‘heavy Artificial Intelligence’ – big data, deep learning, and ubiquitous use of the internet, ethical considerations are widely dealt with in public discussions and governmental bodies. Within Computational Paralinguistics with its manifold topics and possible applications (modelling of long-term, medium-term, and short-term traits and states such as personality, emotion, or speech pathology), we have not yet seen that many contributions. In this article, we try to set the scene by (1) giving a short overview of ethics and privacy, (2) describing the field of Computational Paralinguistics, its history and exemplary use cases, as well as (de-)anonymisation and peculiarities of speech and text data, and (3) proposing rules for good practice in the field, such as choosing the right performance measure, and accounting for representativity and interpretability.},
  archive  = {J},
  author   = {Anton Batliner and Simone Hantke and Björn Schuller},
  doi      = {10.1109/TAFFC.2020.3021015},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1236-1253},
  title    = {Ethics and good practice in computational paralinguistics},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Beyond mobile apps: A survey of technologies for mental
well-being. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(3), 1216–1235. (<a
href="https://doi.org/10.1109/TAFFC.2020.3015018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Mental health problems are on the rise globally and strain national health systems worldwide. Mental disorders are closely associated with fear of stigma, structural barriers such as financial burden, and lack of available services and resources which often prohibit the delivery of frequent clinical advice and monitoring. Technologies for mental well-being exhibit a range of attractive properties, which facilitate the delivery of state-of-the-art clinical monitoring. This review article provides an overview of traditional techniques followed by their technological alternatives, sensing devices, behaviour changing tools, and feedback interfaces. The challenges presented by these technologies are then discussed with data collection, privacy, and battery life being some of the key issues which need to be carefully considered for the successful deployment of mental health toolkits. Finally, the opportunities this growing research area presents are discussed including the use of portable tangible interfaces combining sensing and feedback technologies. Capitalising on the data these ubiquitous devices can record, state of the art machine learning algorithms can lead to the development of robust clinical decision support tools towards diagnosis and improvement of mental well-being delivery in real-time.},
  archive  = {J},
  author   = {Kieran Woodward and Eiman Kanjo and David J. Brown and T. M. McGinnity and Becky Inkster and Donald J. Macintyre and Athanasios Tsanas},
  doi      = {10.1109/TAFFC.2020.3015018},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1216-1235},
  title    = {Beyond mobile apps: A survey of technologies for mental well-being},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Deep facial expression recognition: A survey. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(3), 1195–1215. (<a
href="https://doi.org/10.1109/TAFFC.2020.2981446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the transition of facial expression recognition (FER) from laboratory-controlled to challenging in-the-wild conditions and the recent success of deep learning techniques in various fields, deep neural networks have increasingly been leveraged to learn discriminative representations for automatic FER. Recent deep FER systems generally focus on two important issues: overfitting caused by a lack of sufficient training data and expression-unrelated variations, such as illumination, head pose, and identity bias. In this survey, we provide a comprehensive review of deep FER, including datasets and algorithms that provide insights into these intrinsic problems. First, we introduce the available datasets that are widely used in the literature and provide accepted data selection and evaluation principles for these datasets. We then describe the standard pipeline of a deep FER system with the related background knowledge and suggestions for applicable implementations for each stage. For the state-of-the-art in deep FER, we introduce existing novel deep neural networks and related training strategies that are designed for FER based on both static images and dynamic image sequences and discuss their advantages and limitations. Competitive performances and experimental comparisons on widely used benchmarks are also summarized. We then extend our survey to additional related issues and application scenarios. Finally, we review the remaining challenges and corresponding opportunities in this field as well as future directions for the design of robust deep FER systems.},
  archive  = {J},
  author   = {Shan Li and Weihong Deng},
  doi      = {10.1109/TAFFC.2020.2981446},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1195-1215},
  title    = {Deep facial expression recognition: A survey},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-label and multimodal classifier for affective states
recognition in virtual rehabilitation. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(3), 1183–1194. (<a
href="https://doi.org/10.1109/TAFFC.2021.3055790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Computational systems that process multiple affective states may benefit from explicitly considering the interaction between the states to enhance their recognition performance. This work proposes the combination of a multi-label classifier, Circular Classifier Chain (CCC), with a multimodal classifier, Fusion using a Semi-Naive Bayesian classifier (FSNBC), to include explicitly the dependencies between multiple affective states during the automatic recognition process. This combination of classifiers is applied to a virtual rehabilitation context of post-stroke patients. We collected data from post-stroke patients, which include finger pressure, hand movements, and facial expressions during ten longitudinal sessions. Videos of the sessions were labelled by clinicians to recognize four states: tiredness, anxiety, pain, and engagement. Each state was modelled by the FSNBC receiving the information of finger pressure, hand movements, and facial expressions. The four FSNBCs were linked in the CCC to exploit the dependency relationships between the states. The convergence of CCC was reached by 5 iterations at most for all the patients. Results (ROC AUC) of CCC with the FSNBC are over $0.940 \pm 0.045$ ( $mean \pm std.\;deviation$ ) for the four states. Relationships of mutual exclusion between engagement and all the other states and co-occurrences between pain and anxiety were detected and discussed.},
  archive  = {J},
  author   = {Jesús Joel Rivas and María del Carmen Lara and Luis Castrejón and Jorge Hernández-Franco and Felipe Orihuela-Espina and Lorena Palafox and Amanda Williams and Nadia Bianchi-Berthouze and Luis Enrique Sucar},
  doi      = {10.1109/TAFFC.2021.3055790},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1183-1194},
  title    = {Multi-label and multimodal classifier for affective states recognition in virtual rehabilitation},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring individual differences of public speaking anxiety
in real-life and virtual presentations. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(3), 1168–1182. (<a
href="https://doi.org/10.1109/TAFFC.2020.3048299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Public speaking is a vital skill for making good impressions, effectively exchanging ideas, and influencing others. Yet, public speaking anxiety (PSA) ranks as a top social phobia. Recent advancements in wearable devices and ubiquitous virtual reality (VR) interfaces can help measure and mitigate PSA. This research quantifies PSA through bio-behavioral markers related to individuals’ physiological and acoustic characteristics. The effect of virtual reality (VR) training on alleviating PSA is measured through self-reported and bio-behavioral indices. Psychological (e.g., general trait anxiety, personality) and demographic (e.g., age, gender, highest education, native language) traits are examined as moderating factors between bio-behavioral indices and PSA, as well as moderating factors for measuring the VR effectiveness in mitigating PSA. These measures are also used as clustering criteria for stratifying participants in group-based models of PSA. Results indicate the significance of such traits to modeling PSA with the proposed group-based models yielding Spearman’s correlation of 0.55 ( $p&amp;lt;0.05$ ) between the actual and predicted outcome. Results further demonstrate that systematic exposure to public speaking in VR can alleviate PSA in terms of both self-reported ( $p&amp;lt;0.05$ ) and physiological ( $p&amp;lt;0.05$ ) indices. Findings from this study will enable researchers to better understand antecedents and causes of PSA and lay the foundation for personalized adaptive feedback for PSA interventions.},
  archive  = {J},
  author   = {Megha Yadav and Md Nazmus Sakib and Ehsanul Haque Nirjhar and Kexin Feng and Amir H. Behzadan and Theodora Chaspari},
  doi      = {10.1109/TAFFC.2020.3048299},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1168-1182},
  title    = {Exploring individual differences of public speaking anxiety in real-life and virtual presentations},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unraveling ML models of emotion with NOVA: Multi-level
explainable AI for non-experts. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(3), 1155–1167. (<a
href="https://doi.org/10.1109/TAFFC.2020.3043603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we introduce a next-generation annotation tool called NOVA for emotional behaviour analysis, which implements a workflow that interactively incorporates the ‘human in the loop’. A main aspect of NOVA is the possibility of applying semi-supervised active learning where Machine Learning techniques are used already during the annotation process by giving the possibility to pre-label data automatically. Furthermore, NOVA implements recent eXplainable AI (XAI) techniques to provide users with both, a confidence value of the automatically predicted annotations, as well as visual explanations. We investigate how such techniques can assist non-experts in terms of trust, perceived self-efficacy, cognitive workload as well as creating correct mental models about the system by conducting a user study with 53 participants. The results show that NOVA can easily be used by non-experts and lead to a high computer self-efficacy. Furthermore, the results indicate that XAI visualisations help users to create more correct mental models about the machine learning system compared to the baseline condition. Nevertheless, we suggest that explanations in the field of AI have to be more focused on user-needs as well as on the classification task and the model they want to explain.},
  archive  = {J},
  author   = {Alexander Heimerl and Katharina Weitz and Tobias Baur and Elisabeth André},
  doi      = {10.1109/TAFFC.2020.3043603},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1155-1167},
  title    = {Unraveling ML models of emotion with NOVA: Multi-level explainable AI for non-experts},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Doing and feeling: Relationships between moods, productivity
and task-switching. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(3), 1140–1154. (<a
href="https://doi.org/10.1109/TAFFC.2020.3029440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Digital technology influences behaviours, moods and wellbeing. The relationships are complex, but users are increasingly interested in finding how to balance a digital life with psychological wellbeing. We present an approach for investigating the relationship between lifestyle aspects and digital technology usage patterns that combines MindGauge, a mobile app enabling users collect and analyse their moods and behaviours, with a productivity tool (RescueTime). We then report a 16-month study in which we collected computer and smartphone usage and self-reports from 72 participants. We present methods for analysing the relationship between productivity, task-switching, mood and lifestyle, and more specifically how digital technology usage associates with productivity and task-switching. Our study also investigates how lifestyle aspects (sleep quality, physical activity, workload, social interaction and alcoholic drink consumption) relate to mood, task-switching and productivity. Results show that more frequent task-switching is associated with negative moods. A few lifestyle aspects, such as sleep quality and physical activity, had a significant relationship with positive moods. We also contribute a mood detection model that utilise both digital footprints and lifestyle contexts, yielding an accuracy of 87 percent. The study provides evidence that such methods can be used to understand the impact of technology on wellbeing.},
  archive  = {J},
  author   = {Muhammad Johan Alibasa and Rizka W. Purwanto and Kalina Yacef and Nick Glozier and Rafael A. Calvo},
  doi      = {10.1109/TAFFC.2020.3029440},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1140-1154},
  title    = {Doing and feeling: Relationships between moods, productivity and task-switching},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-componential approach to emotion recognition and the
effect of personality. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(3), 1127–1139. (<a
href="https://doi.org/10.1109/TAFFC.2020.3028109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotions are an inseparable part of human nature affecting our behavior in response to the outside world. Although most empirical studies have been dominated by two theoretical models including discrete categories of emotion and dichotomous dimensions, results from neuroscience approaches suggest a multi-processes mechanism underpinning emotional experience with a large overlap across different emotions. While these findings are consistent with the influential theories of emotion in psychology that emphasize a role for multiple component processes to generate emotion episodes, few studies have systematically investigated the relationship between discrete emotions and a full componential view. This article applies a componential framework with a data-driven approach to characterize emotional experiences evoked during movie watching. The results suggest that differences between various emotions can be captured by a few (at least 6) latent dimensions, each defined by features associated with component processes, including appraisal, expression, physiology, motivation, and feeling. In addition, the link between discrete emotions and component model is explored and results show that a componential model with a limited number of descriptors is still able to predict the level of experienced discrete emotion(s) to a satisfactory level. Finally, as appraisals may vary according to individual dispositions and biases, we also study the relationship between personality traits and emotions in our computational framework and show that the role of personality on discrete emotion differences can be better justified using the component model.},
  archive  = {J},
  author   = {Gelareh Mohammadi and Patrik Vuilleumier},
  doi      = {10.1109/TAFFC.2020.3028109},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1127-1139},
  title    = {A multi-componential approach to emotion recognition and the effect of personality},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unconstrained facial action unit detection via latent
feature domain. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(2), 1111–1126. (<a
href="https://doi.org/10.1109/TAFFC.2021.3091331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial action unit (AU) detection in the wild is a challenging problem, due to the unconstrained variability in facial appearances and the lack of accurate annotations. Most existing methods depend on either impractical labor-intensive labeling or inaccurate pseudo labels. In this paper, we propose an end-to-end unconstrained facial AU detection framework based on domain adaptation, which transfers accurate AU labels from a constrained source domain to an unconstrained target domain by exploiting labels of AU-related facial landmarks. Specifically, we map a source image with label and a target image without label into a latent feature domain by combining source landmark-related feature with target landmark-free feature. Due to the combination of source AU-related information and target AU-free information, the latent feature domain with transferred source label can be learned by maximizing the target-domain AU detection performance. Moreover, we introduce a novel landmark adversarial loss to disentangle the landmark-free feature from the landmark-related feature by treating the adversarial learning as a multi-player minimax game. Our framework can also be naturally extended for use with target-domain pseudo AU labels. Extensive experiments show that our method soundly outperforms lower-bounds and upper-bounds of the basic model, as well as state-of-the-art approaches on the challenging in-the-wild benchmarks. The code is available at https://github.com/ZhiwenShao/ADLD .},
  archive  = {J},
  author   = {Zhiwen Shao and Jianfei Cai and Tat-Jen Cham and Xuequan Lu and Lizhuang Ma},
  doi      = {10.1109/TAFFC.2021.3091331},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1111-1126},
  title    = {Unconstrained facial action unit detection via latent feature domain},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling feature representations for affective speech using
generative adversarial networks. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(2), 1098–1110. (<a
href="https://doi.org/10.1109/TAFFC.2020.2998118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion recognition is a classic field of research with a typical setup extracting features and feeding them through a classifier for prediction. On the other hand, generative models jointly capture the distributional relationship between emotions and the feature profiles. Recently, Generative Adversarial Networks (GANs) have surfaced as a new class of generative models and have shown considerable success in modeling distributions in the fields of computer vision and natural language understanding. In this article, we experiment with variants of GAN architectures to generate feature vectors corresponding to an emotion in two ways: (i) A generator is trained with samples from a mixture prior. Each mixture component corresponds to an emotional class and can be sampled to generate features from the corresponding emotion. (ii) A one-hot vector corresponding to an emotion can be explicitly used to generate the features. We perform analysis on such models and also propose different metrics used to measure the performance of the GAN models in their ability to generate realistic synthetic samples. Apart from evaluation on a given dataset of interest, we perform a cross-corpus study where we study the utility of the synthetic samples as additional training data in low resource conditions.},
  archive  = {J},
  author   = {Saurabh Sahu and Rahul Gupta and Carol Espy-Wilson},
  doi      = {10.1109/TAFFC.2020.2998118},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1098-1110},
  title    = {Modeling feature representations for affective speech using generative adversarial networks},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Measuring temporal distance focus from tweets and
investigating its association with psycho-demographic attributes.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(2),
1086–1097. (<a
href="https://doi.org/10.1109/TAFFC.2020.2992463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Temporal distance (TD) is a type of psychological distance which shows how an individual construes past and future. It is not explored with empirical research as to how an individual’s focus on temporal distance (near-past, far-past, near-future, and far-future) can be measured from human-written text and further used for studying human tendencies. Traditionally, focus on a Temporal Distance is studied by self-report measurements. In this article, we present a study on human focus on a temporal distance from their Twitter posts (English tweets). We first identify the tweet-level temporal focus by deep neural classifiers which make use of linguistic knowledge for classification. The model classifies each tweet into one of near-past , far-past , near-future or far-future . Classified tweets are then grouped by users to obtain the user-level temporal focus. Finally, we correlate the user’s focus on temporal distance ( near-past, far-past, near-future, and far-future ) with his/her demographic (age, gender, education, and relationship status) and psychological attributes (intelligence, optimism, joy, sadness, disgust, anger, surprise, and fear). Our empirical analysis reveals that users’ near-past focus is more positively correlated to their age. We also observe that users’ near-future focus is correlated to joy while users’ focus on far-past is associated with negative emotions like sadness, disgust, anger, and fear.},
  archive  = {J},
  author   = {Sabyasachi Kamila and Mohammad Hasanuzzaman and Asif Ekbal and Pushpak Bhattacharyya},
  doi      = {10.1109/TAFFC.2020.2992463},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1086-1097},
  title    = {Measuring temporal distance focus from tweets and investigating its association with psycho-demographic attributes},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Leveraging the dynamics of non-verbal behaviors for social
attitude modeling. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(2), 1072–1085. (<a
href="https://doi.org/10.1109/TAFFC.2020.2989262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {An Embodied Conversational Agent (ECA) is a virtual character designed to interact with humans in the most natural way. In the recent years, ECAs have been deployed in various contexts, such as commercial consulting and social training. In the context of social training, the virtual agent should be able to express different social attitudes in order to train the user in different situations, likely to occur in real life. Previous studies from psychology underlined the importance of considering the non-verbal behavior as well as its evolution over time, for efficient modeling of interpersonal attitudes. Inspired by these works as well as by advances from sequence mining, we propose to model attitude variation as a sequence of non-verbal signals, each being described by its starting time and duration. We demonstrate the efficiency of our model by integrating the sequences representing attitude variation in an ECA and assessing the obtained results based on the interpersonal circumplex, statistical tests and accuracy measures. To the best of our knowledge, this is the first attempt to study the relationship, in term of perception, between different attitude variations.},
  archive  = {J},
  author   = {Soumia Dermouche and Catherine Pelachaud},
  doi      = {10.1109/TAFFC.2020.2989262},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1072-1085},
  title    = {Leveraging the dynamics of non-verbal behaviors for social attitude modeling},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Facial expression recognition with deeply-supervised
attention network. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(2), 1057–1071. (<a
href="https://doi.org/10.1109/TAFFC.2020.2988264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial expression recognition (FER) is crucial for social communication. However, current studies present limitations when addressing facial expression difference due to demographic variation, such as race, gender, and age, etc. In this article, we first propose a deeply-supervised attention network (DSAN) to recognize human emotions based on facial images automatically. Based on DSAN, a two-stage training scheme is designed, taking full advantage of the race/gender/age-related information. In our DSAN framework, multi-scale features are leveraged to capture more discriminative information from the deep layers to the shallow layers. Furthermore, we adopt the attention block to highlight the essential local facial characteristics; it performs well when it is incorporated into the deeply-supervised framework. Finally, we combine the complementary characteristics of multiple convolutional layers in deeply-supervised manner and ensemble the intermediate predicted scores. Our experimental results have shown that our proposed framework can (i) effectively integrate demographic information in improving the performance of a variety of FER tasks, (ii) learn informative feature representations with a visual explanation by capturing the regions of interests (ROI), (iii) achieve superior performance for both the posed and the spontaneous FER databases, each containing pictures of human facial expressions varied in gender, age or race.},
  archive  = {J},
  author   = {Yingruo Fan and Victor O.K. Li and Jacqueline C.K. Lam},
  doi      = {10.1109/TAFFC.2020.2988264},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1057-1071},
  title    = {Facial expression recognition with deeply-supervised attention network},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multimodal non-intrusive stress monitoring from the
pleasure-arousal emotional dimensions. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(2), 1044–1056. (<a
href="https://doi.org/10.1109/TAFFC.2020.2988455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the increasing development of advanced unmanned aerial vehicles (UAVs), communication between operators and these intelligent systems is becoming more stressful. For the safety of UAV flights, automatic psychological stress detection is becoming a key research topic for successful missions. Stress can be reliably estimated via some biological markers which are not appropriate in many cases of human-machine-interaction setups. In this article, we propose a non-intrusive deep learning-based stress level estimation approach. The goal is to identify the region where the operator&#39;s emotional state projects in the space defined by the latent dimensional emotions of arousal and valence since the stress region is well delimited in this space. The proposed multimodal approach uses sequential temporal CNN and LSTM with an Attention Weighted Average layer in the vision modality. As a second modality, we investigate local and global descriptors such as Mel-frequency cepstral coefficients, i-vector embeddings as well as Fisher-vector encodings. The multimodal-fusion approach uses a strategy referred to as “late-fusion” that involves the combination of unimodal model outputs as inputs of the decision engine. Since we have to deal with more naturalistic behavior in operator-machine interaction contexts, the One minute Gradual Emotion Challenge dataset was used for predictive model validation.},
  archive  = {J},
  author   = {Mohamed Dahmane and Jahangir Alam and Pierre-Luc St-Charles and Marc Lalonde and Kevin Heffner and Samuel Foucher},
  doi      = {10.1109/TAFFC.2020.2988455},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1044-1056},
  title    = {A multimodal non-intrusive stress monitoring from the pleasure-arousal emotional dimensions},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic micro-expression recognition using knowledge
distillation. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(2), 1037–1043. (<a
href="https://doi.org/10.1109/TAFFC.2020.2986962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Micro-expression is a spontaneous expression that occurs when a person tries to mask his or her inner emotion, and can neither be forged nor suppressed. It is a kind of short-duration, low-intensity, and usually local-motion facial expression. However, owing to these characteristics of micro-expression, it is difficult to obtain micro-expression data, which is the bottleneck of applying deep learning methods to micro-expression recognition. In addition, micro-expression is still a type of expression, and it can also be encoded by the facial action coding system. Therefore, there is a certain correlation between action unit recognition and micro-expression recognition. Addressing those, we propose a novel knowledge transfer technique distills and transfers knowledge from action unit for micro-expression recognition, where knowledge from a pre-trained deep teacher neural network is distilled and transferred to a shallow student neural network. Specifically, a teacher-student correlative framework is designed with a novel objective function. And features extracted from the teacher network is used as prior knowledge to guide the student part to efficiently learning from the target micro-expression dataset. Experiments are conducted on four available published micro-expression datasets (SMIC2, CASME, CASME II, and SAMM). The experimental results show that our model outperforms the state-of-the-art systems.},
  archive  = {J},
  author   = {Bo Sun and Siming Cao and Dongliang Li and Jun He and Lejun Yu},
  doi      = {10.1109/TAFFC.2020.2986962},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1037-1043},
  title    = {Dynamic micro-expression recognition using knowledge distillation},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BReG-NeXt: Facial affect computing using adaptive residual
networks with bounded gradient. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(2), 1023–1036. (<a
href="https://doi.org/10.1109/TAFFC.2020.2986440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article introduces BReG-NeXt , a residual-based network architecture using a function wtih bounded derivative instead of a simple shortcut path ( a.k.a. identity mapping) in the residual units for automatic recognition of facial expressions based on the categorical and dimensional models of affect. Compared to ResNet, our proposed adaptive complex mapping results in a shallower network with less numbers of training parameters and floating point operations per second (FLOPs). Adding trainable parameters to the bypass function further improves fitting and training the network and hence recognizing subtle facial expressions such as contempt with a higher accuracy. We conducted comprehensive experiments on the categorical and dimensional models of affect on the challenging in-the-wild databases of AffectNet, FER2013, and Affect-in-Wild. Our experimental results show that our adaptive complex mapping approach outperforms the original ResNet consisting of a simple identity mapping as well as other state-of-the-art methods for Facial Expression Recognition (FER). Various metrics are reported in both affect models to provide a comprehensive evaluation of our method. In the categorical model, BReG-NeXt-50 with only 3.1M training parameters and 15 MFLOPs, achieves 68.50 and 71.53 percent accuracy on AffectNet and FER2013 databases, respectively. In the dimensional model, BReG-NeXt achieves 0.2577 and 0.2882 RMSE value on AffectNet and Affect-in-Wild databases, respectively.},
  archive  = {J},
  author   = {Behzad Hasani and Pooran Singh Negi and Mohammad H. Mahoor},
  doi      = {10.1109/TAFFC.2020.2986440},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1023-1036},
  title    = {BReG-NeXt: Facial affect computing using adaptive residual networks with bounded gradient},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A psychologically inspired fuzzy cognitive deep learning
framework to predict crowd behavior. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(2), 1005–1022. (<a
href="https://doi.org/10.1109/TAFFC.2020.2987021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In an intelligent surveillance system, detecting and predicting diverse collective crowd behaviors has emerged as a challenging problem for efficient crowd management. In real-world scenarios, potential disasters and hazards can be averted by considering crowd psychology for predicting crowd behaviors. This article proposes an approach that exploits the psychological and cognitive aspects of human behavior in determining nine diverse crowd behaviors. The proposed approach is a combination of two cognitive deep learning frameworks and a psychological fuzzy computational model that utilizes OCC theory of emotions, OCEAN five-factor model of personality and visual attention for detecting crowd behaviors. Experiments are performed on different datasets and the results prove that our approach is successful in detecting and predicting crowd behavior in confronting situations and also outperforms the state-of-the-art methods. In particular, considering psychological aspects and cognition in determining crowd behavior is beneficial for rectifying the semantic ambiguity in identifying crowd behaviors.},
  archive  = {J},
  author   = {Elizabeth B. Varghese and Sabu M. Thampi and Stefano Berretti},
  doi      = {10.1109/TAFFC.2020.2987021},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1005-1022},
  title    = {A psychologically inspired fuzzy cognitive deep learning framework to predict crowd behavior},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-task semi-supervised adversarial autoencoding for
speech emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(2), 992–1004. (<a
href="https://doi.org/10.1109/TAFFC.2020.2983669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Inspite the emerging importance of Speech Emotion Recognition (SER), the state-of-the-art accuracy is quite low and needs improvement to make commercial applications of SER viable. A key underlying reason for the low accuracy is the scarcity of emotion datasets, which is a challenge for developing any robust machine learning model in general. In this article, we propose a solution to this problem: a multi-task learning framework that uses auxiliary tasks for which data is abundantly available. We show that utilisation of this additional data can improve the primary task of SER for which only limited labelled data is available. In particular, we use gender identifications and speaker recognition as auxiliary tasks, which allow the use of very large datasets, e. g., speaker classification datasets. To maximise the benefit of multi-task learning, we further use an adversarial autoencoder (AAE) within our framework, which has a strong capability to learn powerful and discriminative features. Furthermore, the unsupervised AAE in combination with the supervised classification networks enables semi-supervised learning which incorporates a discriminative component in the AAE unsupervised training pipeline. This semi-supervised learning essentially helps to improve generalisation of our framework and thus leads to improvements in SER performance. The proposed model is rigorously evaluated for categorical and dimensional emotion, and cross-corpus scenarios. Experimental results demonstrate that the proposed model achieves state-of-the-art performance on two publicly available datasets.},
  archive  = {J},
  author   = {Siddique Latif and Rajib Rana and Sara Khalifa and Raja Jurdak and Julien Epps and Björn W. Schuller},
  doi      = {10.1109/TAFFC.2020.2983669},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {992-1004},
  title    = {Multi-task semi-supervised adversarial autoencoding for speech emotion recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bayesian deep learning framework for end-to-end prediction
of emotion from heartbeat. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(2), 985–991. (<a
href="https://doi.org/10.1109/TAFFC.2020.2981610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatic prediction of emotion promises to revolutionise human-computer interaction. Recent trends involve fusion of multiple data modalities $-$ audio, visual, and physiological $-$ to classify emotional state. However, in practice, collection of physiological data ‘in the wild’ is currently limited to heartbeat time series of the kind generated by affordable wearable heart monitors. Furthermore, real-world applications of emotion prediction often require some measure of uncertainty over model output, in order to inform downstream decision-making. We present here an end-to-end deep learning model for classifying emotional valence from unimodal heartbeat time series. We further propose a Bayesian framework for modelling uncertainty over these valence predictions, and describe a probabilistic procedure for choosing to accept or reject model output according to the intended application. We benchmarked our framework against two established datasets and achieved peak classification accuracy of 90 percent. These results lay the foundation for applications of affective computing in real-world domains such as healthcare, where a high premium is placed on non-invasive collection of data, and predictive certainty.},
  archive  = {J},
  author   = {Ross Harper and Joshua Southern},
  doi      = {10.1109/TAFFC.2020.2981610},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {985-991},
  title    = {A bayesian deep learning framework for end-to-end prediction of emotion from heartbeat},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised learning in reservoir computing for EEG-based
emotion recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(2), 972–984. (<a
href="https://doi.org/10.1109/TAFFC.2020.2982143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In real-world applications such as emotion recognition from recorded brain activity, data are captured from electrodes over time. These signals constitute a multidimensional time series. In this article, Echo State Network (ESN), a recurrent neural network with great success in time series prediction and classification, is optimized with different neural plasticity rules for classification of emotions based on electroencephalogram (EEG) time series. The developed network could automatically extract valid features from EEG signals. We use the filtered signals as the network input and do not take any feature extraction methods. Evaluated on two well-known benchmarks, the DEAP dataset, and the SEED dataset, the performance of the ESN with intrinsic plasticity greatly outperforms the feature-based methods and shows certain advantages compared with other existing methods. Thus, the proposed network can form a more complete and efficient representation, whilst retaining the advantages such as faster learning speed and more reliable performance.},
  archive  = {J},
  author   = {Rahma Fourati and Boudour Ammar and Javier Sanchez-Medina and Adel M. Alimi},
  doi      = {10.1109/TAFFC.2020.2982143},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {972-984},
  title    = {Unsupervised learning in reservoir computing for EEG-based emotion recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fusing of electroencephalogram and eye movement with group
sparse canonical correlation analysis for anxiety detection. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(2), 958–971. (<a
href="https://doi.org/10.1109/TAFFC.2020.2981440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Electroencephalogram (EEG) has been widely used for the detection of anxiety because of its ability to reflect the functional activities of the brain. However, EEG alone may not provide precision in the detection of anxiety because other emotional disorders usually trigger the same changes in brain function. To discover effective diagnostic indicators and to achieve more precise anxiety detection, we integrate eye movement information into EEG and divide the features into groups according to their respective characteristics. Then, we use group sparse canonical correlation analysis (GSCCA) to investigate group structure information among EEG and eye movement features and obtain an effective fusion representation of EEG and eye movement to achieve more precise detection of anxiety mood. The experimental results from 45 anxious subjects and 47 normal controls from the Healthy Brain Network (HBN) dataset showed that GSCCA could be effectively used to explore the correlation between EEG features within different scalp regions and eye movement features from several aspects. Visual behaviors, including saccades and fixation, are more linearly related to the power spectrum of EEG on the scalp area corresponding to the visual region of the brain. The ultimate fusion representation achieved an optimal classification accuracy of 82.70 percent with the support vector machine (SVM) classifier on the gamma band of EEG.},
  archive  = {J},
  author   = {Xiaowei Zhang and Jing Pan and Jian Shen and Zia ud Din and Junlei Li and Dawei Lu and Manxi Wu and Bin Hu},
  doi      = {10.1109/TAFFC.2020.2981440},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {958-971},
  title    = {Fusing of electroencephalogram and eye movement with group sparse canonical correlation analysis for anxiety detection},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic detection of reflective thinking in mathematical
problem solving based on unconstrained bodily exploration. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(2), 944–957. (<a
href="https://doi.org/10.1109/TAFFC.2020.2978069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {For technology (like serious games) that aims to deliver interactive learning, it is important to address relevant mental experiences such as reflective thinking during problem solving. To facilitate research in this direction, we present the weDraw-1 Movement Dataset of body movement sensor data and reflective thinking labels for 26 children solving mathematical problems in unconstrained settings where the body (full or parts) was required to explore these problems. Further, we provide qualitative analysis of behaviours that observers used in identifying reflective thinking moments in these sessions. The body movement cues from our compilation informed features that led to average F1 score of 0.73 for binary classification of problem-solving episodes by reflective thinking based on Long Short-Term Memory neural networks. We further obtained 0.79 average F1 score for end-to-end classification, i.e., based on raw sensor data. Finally, the algorithms resulted in 0.64 average F1 score for subsegments of these episodes as short as 4 seconds. Overall, our results show the possibility of detecting reflective thinking moments from body movement behaviours of a child exploring mathematical concepts bodily, such as within serious game play.},
  archive  = {J},
  author   = {Temitayo A. Olugbade and Joseph Newbold and Rose Johnson and Erica Volta and Paolo Alborno and Radoslaw Niewiadomski and Max Dillon and Gualtiero Volpe and Nadia Bianchi-Berthouze},
  doi      = {10.1109/TAFFC.2020.2978069},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {944-957},
  title    = {Automatic detection of reflective thinking in mathematical problem solving based on unconstrained bodily exploration},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ENGAGE-DEM: A model of engagement of people with dementia.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(2),
926–943. (<a href="https://doi.org/10.1109/TAFFC.2020.2980275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {One of the most effective ways to improve quality of life in dementia is by exposing people to meaningful activities. The study of engagement is crucial to identify which activities are significant for persons with dementia and customize them. Previous work has mainly focused on developing assessment tools and the only available model of engagement for people with dementia focused on factors influencing engagement or influenced by engagement. This article focuses on the internal functioning of engagement and presents the development and testing of a model specifying the components of engagement, their measures, and the relationships they entertain. We collected behavioral and physiological data while participants with dementia (N = 14) were involved in six sessions of play, three of game-based cognitive stimulation and three of robot-based free play. We tested the concurrent validity of the measures employed to gauge engagement and ran factorial analysis and Structural Equation Modeling to determine whether the components of engagement and their relationships were those hypothesized. The model we constructed, which we call the ENGAGE-DEM, achieved excellent goodness of fit and can be considered a scaffold to the development of affective computing frameworks for measuring engagement online and offline, especially in HCI and HRI.},
  archive  = {J},
  author   = {Giulia Perugia and Marta Díaz-Boladeras and Andreu Català-Mallofré and Emilia I. Barakova and Matthias Rauterberg},
  doi      = {10.1109/TAFFC.2020.2980275},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {926-943},
  title    = {ENGAGE-DEM: A model of engagement of people with dementia},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Are you really looking at me? A feature-extraction framework
for estimating interpersonal eye gaze from conventional video. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(2), 912–925. (<a
href="https://doi.org/10.1109/TAFFC.2020.2979440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Despite a revolution in the pervasiveness of video cameras in our daily lives, one of the most meaningful forms of nonverbal affective communication, interpersonal eye gaze, i.e., eye gaze relative to a conversation partner, is not available from common video. We introduce the Interpersonal-Calibrating Eye-gaze Encoder (ICE), which automatically extracts interpersonal gaze from video recordings without specialized hardware and without prior knowledge of participant locations. Leveraging the intuition that individuals spend a large portion of a conversation looking at each other enables the ICE dynamic clustering algorithm to extract interpersonal gaze. We validate ICE in both video chat using an objective metric with an infrared gaze tracker (F1 = 0.846, N = 8), as well as in face-to-face communication with expert-rated evaluations of eye contact (r = 0.37, N = 170). We then use ICE to analyze behavior in two different, yet important affective communication domains: interrogation-based deception detection, and communication skill assessment in speed dating. We find that honest witnesses break interpersonal gaze contact and look down more often than deceptive witnesses when answering questions (p = 0.004, d = 0.79). In predicting expert communication skill ratings in speed dating videos, we demonstrate that interpersonal gaze alone has more predictive power than facial expressions.},
  archive  = {J},
  author   = {Minh Tran and Taylan Sen and Kurtis Haut and Mohammad Rafayet Ali and Ehsan Hoque},
  doi      = {10.1109/TAFFC.2020.2979440},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {912-925},
  title    = {Are you really looking at me? a feature-extraction framework for estimating interpersonal eye gaze from conventional video},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling, recognizing, and explaining apparent personality
from videos. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(2), 894–911. (<a
href="https://doi.org/10.1109/TAFFC.2020.2973984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Explainability and interpretability are two critical aspects of decision support systems. Despite their importance, it is only recently that researchers are starting to explore these aspects. This paper provides an introduction to explainability and interpretability in the context of apparent personality recognition. To the best of our knowledge, this is the first effort in this direction. We describe a challenge we organized on explainability in first impressions analysis from video. We analyze in detail the newly introduced data set, evaluation protocol, proposed solutions and summarize the results of the challenge. We investigate the issue of bias in detail. Finally, derived from our study, we outline research opportunities that we foresee will be relevant in this area in the near future.},
  archive  = {J},
  author   = {Hugo Jair Escalante and Heysem Kaya and Albert Ali Salah and Sergio Escalera and Yağmur Güçlütürk and Umut Güçlü and Xavier Baró and Isabelle Guyon and Julio C. S. Jacques Junior and Meysam Madadi and Stephane Ayache and Evelyne Viegas and Furkan Gürpınar and Achmadnoer Sukma Wicaksana and Cynthia C. S. Liem and Marcel A. J. van Gerven and Rob van Lier},
  doi      = {10.1109/TAFFC.2020.2973984},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {894-911},
  title    = {Modeling, recognizing, and explaining apparent personality from videos},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). A deeper look at facial expression dataset bias. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(2), 881–893. (<a
href="https://doi.org/10.1109/TAFFC.2020.2973158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Datasets play an important role in the progress of facial expression recognition algorithms, but they may suffer from obvious biases caused by different cultures and collection conditions. To look deeper into this bias, we first conduct comprehensive experiments on dataset recognition and cross-dataset generalization tasks, and for the first time, explore the intrinsic causes of the dataset discrepancy. The results quantitatively verify that current datasets have a strong build-in bias, and corresponding analyses indicate that the conditional probability distributions between source and target datasets are different. However, previous researches are mainly based on shallow features with limited discriminative ability under the assumption that the conditional distribution remains unchanged across domains. To address these issues, we further propose a novel deep Emotion-Conditional Adaption Network (ECAN) to learn domain-invariant and discriminative feature representations, which can match not only the marginal distribution but also the class-conditional distribution across domains by exploring the underlying label information of the target dataset. Moreover, the largely ignored expression class distribution bias is also addressed so that the training and testing domains can share similar class distribution. Extensive cross-database experiments on both lab-controlled datasets (CK+, JAFFE, MMI, and Oulu-CASIA) and real-world databases (AffectNet, FER2013, RAF-DB 2.0, and SFEW 2.0) demonstrate that our ECAN can yield competitive performances across various cross-dataset facial expression recognition tasks and outperform the state-of-the-art methods.},
  archive  = {J},
  author   = {Shan Li and Weihong Deng},
  doi      = {10.1109/TAFFC.2020.2973158},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {881-893},
  title    = {A deeper look at facial expression dataset bias},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Affective dynamics: Principal motion analysis of temporal
dominance of sensations and emotions data. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(2), 871–880. (<a
href="https://doi.org/10.1109/TAFFC.2020.2971700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Temporal dominance (TD) methods can be used to record temporal changes in multiple sensory and affective responses. TD methods have found wide applications in the analysis of eating experiences of humans. However, extant analyses performed on TD data do not fully utilize the time-series properties of such data. The present study validates the prospect of principal motion analysis (PMA) of TD data. PMA is an extension of principal component analysis, and can be used to resolve multivariate motion data into base principal motions. In this study, panelists were asked to evaluate the tastes of ten types of pickled plums using the temporal dominance of sensations (TDS) and emotions (TDE) methods. Additionally, the panelists were asked to rate the plums using the semantic differential method. Results obtained using both methods were observed to demonstrate good agreement with each other in terms of the structures of reduced variable spaces. As realized in this article, implementation of the combined TD–PMA approach can potentially facilitate statistical discrimination of all food products, whereas conventional methods, such as principal component analysis of data provided via use of the semantic differential method, can at best discriminate only 67 percent of product pairs. PMA can, therefore, be considered as a suitable technique to reveal the characteristics of TD data.},
  archive  = {J},
  author   = {Shogo Okamoto and Yuki Ehara and Takumu Okada and Yoji Yamada},
  doi      = {10.1109/TAFFC.2020.2971700},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {871-880},
  title    = {Affective dynamics: Principal motion analysis of temporal dominance of sensations and emotions data},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Depression level prediction using deep spatiotemporal
features and multilayer bi-LTSM. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(2), 864–870. (<a
href="https://doi.org/10.1109/TAFFC.2020.2970418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Depression is a serious psychiatric disorder that restricts an individuals ability to work properly in both their daily and professional lives. Usually, the diagnosis of depression often needs a thorough assessment by an expert. Recently, significant consideration has been given to automatic depression prediction for more reliable and efficient depression investigation. In this article, we propose a novel framework to estimate the depression level from video data by employing a two-stream deep spatiotemporal network. Our approach extracts spatial information using the Inception-ResNet-v2 network. In contrast, we introduce a volume local directional number (VLDN) based dynamic feature descriptor to capture facial motions. Then, the feature map obtained from the VLDN is fed into a convolutional neural network (CNN) to obtain more discriminative features. Additionally, we designed a multilayer bidirectional long short-term memory (Bi-LSTM) model to obtain temporal information by integrating the temporal median pooling (TMP) approach into the model. The TMP approach is employed on the temporal fragments of spatial and temporal features. Finally, extensive experimental analysis of two challenging datasets, AVEC2013 and AVEC2014, demonstrates that the proposed approach shows promising performance compared to the existing approaches for depression level prediction.},
  archive  = {J},
  author   = {Md Azher Uddin and Joolekha Bibi Joolee and Young-Koo Lee},
  doi      = {10.1109/TAFFC.2020.2970418},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {864-870},
  title    = {Depression level prediction using deep spatiotemporal features and multilayer bi-LTSM},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Issues and challenges of aspect-based sentiment analysis: A
comprehensive survey. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(2), 845–863. (<a
href="https://doi.org/10.1109/TAFFC.2020.2970399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The domain of Aspect-based Sentiment Analysis, in which aspects are extracted, their sentiments are analysed and sentiments are evolved over time, is getting much attention with increasing feedback of public and customers on social media. The immense advancements in this field urged the researchers to devise new techniques and approaches, each sermonizing a different research analysis/question, that cope with upcoming issues and complex scenarios of Aspect-based Sentiment Analysis. Therefore, this survey emphasized on the issues and challenges that are related to extraction of different aspects and their relevant sentiments, relational mapping between aspects, interactions, dependencies, and contextual-semantic relationships between different data objects for improved sentiment accuracy, and prediction of sentiment evolution dynamicity. A rigorous overview of the recent progress is summarized based on whether they contributed towards highlighting and mitigating the issue of Aspect Extraction, Aspect Sentiment Analysis or Sentiment Evolution. The reported performance for each scrutinized study of Aspect Extraction and Aspect Sentiment Analysis is also given, showing the quantitative evaluation of the proposed approach. Future research directions are proposed and discussed, by critically analysing the presented recent solutions, that will be helpful for researchers and beneficial for improving sentiment classification at aspect-level.},
  archive  = {J},
  author   = {Ambreen Nazir and Yuan Rao and Lianwei Wu and Ling Sun},
  doi      = {10.1109/TAFFC.2020.2970399},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {845-863},
  title    = {Issues and challenges of aspect-based sentiment analysis: A comprehensive survey},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spectral representation of behaviour primitives for
depression analysis. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(2), 829–844. (<a
href="https://doi.org/10.1109/TAFFC.2020.2970712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Depression is a serious mental disorder affecting millions of people all over the world. Traditional clinical diagnosis methods are subjective, complicated and require extensive participation of clinicians. Recent advances in automatic depression analysis systems promise a future where these shortcomings are addressed by objective, repeatable, and readily available diagnostic tools to aid health professionals in their work. Yet there remain a number of barriers to the development of such tools. One barrier is that existing automatic depression analysis algorithms base their predictions on very brief sequential segments, sometimes as little as one frame. Another barrier is that existing methods do not take into account what the context of the measured behaviour is. In this article, we extract multi-scale video-level features for video-based automatic depression analysis. We propose to use automatically detected human behaviour primitives as the low-dimensional descriptor for each frame. We also propose two novel spectral representations, i.e., spectral heatmaps and spectral vectors, to represent video-level multi-scale temporal dynamics of expressive behaviour. Constructed spectral representations are fed to Convolution Neural Networks (CNNs) and Artificial Neural Networks (ANNs) for depression analysis. We conducted experiments on the AVEC 2013 and AVEC 2014 benchmark datasets to investigate the influence of interview tasks on depression analysis. In addition to achieving state of the art accuracy in severity of depression estimation, we show that the task conducted by the user matters, that fusion of a combination of tasks reaches highest accuracy, and that longer tasks are more informative than shorter tasks, up to a point.},
  archive  = {J},
  author   = {Siyang Song and Shashank Jaiswal and Linlin Shen and Michel Valstar},
  doi      = {10.1109/TAFFC.2020.2970712},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {829-844},
  title    = {Spectral representation of behaviour primitives for depression analysis},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep multi-task multi-label CNN for effective facial
attribute classification. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(2), 818–828. (<a
href="https://doi.org/10.1109/TAFFC.2020.2969189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial Attribute Classification (FAC) has attracted increasing attention in computer vision and pattern recognition. However, state-of-the-art FAC methods perform face detection/alignment and FAC independently. The inherent dependencies between these tasks are not fully exploited. In addition, most methods predict all facial attributes using the same CNN network architecture, which ignores the different learning complexities of facial attributes. To address the above problems, we propose a novel deep multi-task multi-label CNN, termed DMM-CNN, for effective FAC. Specifically, DMM-CNN jointly optimizes two closely-related tasks (i.e., facial landmark detection and FAC) to improve the performance of FAC by taking advantage of multi-task learning. To deal with the diverse learning complexities of facial attributes, we divide the attributes into two groups: objective attributes and subjective attributes. Two different network architectures are respectively designed to extract features for two groups of attributes, and a novel dynamic weighting scheme is proposed to automatically assign the loss weight to each facial attribute during training. Furthermore, an adaptive thresholding strategy is developed to effectively alleviate the problem of class imbalance for multi-label learning. Experimental results on the challenging CelebA and LFWA datasets show the superiority of the proposed DMM-CNN method compared with several state-of-the-art FAC methods.},
  archive  = {J},
  author   = {Longbiao Mao and Yan Yan and Jing-Hao Xue and Hanzi Wang},
  doi      = {10.1109/TAFFC.2020.2969189},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {818-828},
  title    = {Deep multi-task multi-label CNN for effective facial attribute classification},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-stage fuzzy fusion based-convolution neural network for
dynamic emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(2), 805–817. (<a
href="https://doi.org/10.1109/TAFFC.2020.2966440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The two-stage fuzzy fusion based-convolution neural network is proposed for dynamic emotion recognition by using both facial expression and speech modalities, which not only can extract discriminative emotion features which contain spatio-temporal information, but also can effectively fuse facial expression and speech modalities. Moreover, the proposal is able to handle situations where the contributions of each modality data to emotion recognition are very imbalanced. The local binary patterns coming from three orthogonal planes and spectrogram are considered first to extract low-level dynamic emotion, so that the spatio-temporal information of these modalities can be obtained. To reveal more discriminative features, two deep convolution neural networks are constructed to extract high-level emotion semantic features. Moreover, the two stage fuzzy fusion strategy is developed by integrating canonical correlation analysis and fuzzy broad learning system, so as to take into account the correlation and difference between different modal features, as well as handle the ambiguity of emotional state information. The experimental results obtained on benchmark databases show that the accuracies of the proposed method are higher than those of existing methods (such as the hybrid deep model, and the rule-based and machine learning method) on SAVEE, eNTERFACE’05, and AFEW databases.},
  archive  = {J},
  author   = {Min Wu and Wanjuan Su and Luefeng Chen and Witold Pedrycz and Kaoru Hirota},
  doi      = {10.1109/TAFFC.2020.2966440},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {805-817},
  title    = {Two-stage fuzzy fusion based-convolution neural network for dynamic emotion recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). State-specific and supraordinal components of facial
response to pain. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(2), 793–804. (<a
href="https://doi.org/10.1109/TAFFC.2020.2965105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Pain inadequate treatment is frequent in modern society, with major medical, ethical, and financial implications. In many healthcare environments, pain is quantified prevalently through subjective measures, such as self-reports from patients or health care providers’ personal experience. Recently, automatic diagnostic tools have been developed to detect and quantify pain more “objectively” from facial expressions. However, it is still unclear if these approaches can distinguish pain from other aversive (but painless) states. In this article, we analyzed the facial responses from a database of video-recorded facial reactions evoked by comparably-unpleasant painful and disgusting stimuli. We modeled this information as function of subjective unpleasantness, as well as the specific state evoked by the stimuli (pain vs . disgust). Results show that a machine learning algorithm could predict subjective pain unpleasantness from facial information, but mistakenly detected unpleasant disgust, especially in those models relying in great extent on the brow lowerer. Importantly, pain and disgust could be disentangled using an ad hoc algorithm that rely on combined information from the eyes and the mouth. Overall, the facial expression of pain contains both specific and unpleasantness-related information shared with disgust. Automatic diagnostic tools should be guided to account for this confounding effect.},
  archive  = {J},
  author   = {Giada Dirupo and Paolo Garlasco and Cyrielle Chappuis and Gil Sharvit and Corrado Corradi-Dell&#39;Acqua},
  doi      = {10.1109/TAFFC.2020.2965105},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {793-804},
  title    = {State-specific and supraordinal components of facial response to pain},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recognition of advertisement emotions with application to
computational advertising. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(2), 781–792. (<a
href="https://doi.org/10.1109/TAFFC.2020.2964549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Advertisements (ads) often contain strong emotions to capture audience attention and convey an effective message. Still, little work has focused on affect recognition (AR) from ads employing audiovisual or user cues. This work (1) compiles an affective video ad dataset which evokes coherent emotions across users; (2) explores the efficacy of content-centric convolutional neural network (CNN) features for ad AR vis-ã-vis handcrafted audio-visual descriptors; (3) examines user-centric ad AR from Electroencephalogram (EEG) signals, and (4) demonstrates how better affect predictions facilitate effective computational advertising via a study involving 18 users. Experiments reveal that (a) CNN features outperform handcrafted audiovisual descriptors for content-centric AR; (b) EEG features encode ad-induced emotions better than content-based features; (c) Multi-task learning achieves optimal ad AR among a slew of classifiers and (d) Pursuant to (b), EEG features enable optimized ad insertion onto streamed video compared to content-based or manual insertion, maximizing ad recall and viewing experience.},
  archive  = {J},
  author   = {Abhinav Shukla and Shruti Shriya Gullapuram and Harish Katti and Mohan Kankanhalli and Stefan Winkler and Ramanathan Subramanian},
  doi      = {10.1109/TAFFC.2020.2964549},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {781-792},
  title    = {Recognition of advertisement emotions with application to computational advertising},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Holistic affect recognition using PaNDA: Paralinguistic
non-metric dimensional analysis. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(2), 769–780. (<a
href="https://doi.org/10.1109/TAFFC.2019.2961881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Humans perceive emotion from each other using a holistic perspective, accounting for diverse personal, non-emotional variables, such as age and personality, that shape expression. In contrast, today’s algorithms are mainly designed to recognize emotion in isolation, and are usually demonstrated only within one relatively narrow database. In this article, we propose a multi-task learning approach to jointly learn the recognition of affective states from speech along with various speaker attributes. A problem with multi-task learning is that sometimes inductive transfer can negatively impact performance. To mitigate negative transfer, we introduce the Paralinguistic Non-metric Dimensional Analysis (PaNDA) method that systematically measures task relatedness and also enables visualizing the topology of affective phenomena as a whole. In addition, we present a generic framework that conflates the concepts of single-task and multi-task learning. Using this framework, we construct two models that demonstrate holistic affect recognition: one treats all tasks as equally related, whereas the other one incorporates the task correlations between a main task and its supporting tasks obtained from PaNDA. Both models employ a multi-task deep neural network, in which separate output layers are used to predict discrete and continuous attributes, while hidden layers are shared across different tasks. On average across 18 classification and regression tasks, the weighted multi-task learning with PaNDA significantly improves performance compared to single-task and unweighted multi-task learning.},
  archive  = {J},
  author   = {Yue Zhang and Felix Weninger and Björn Schuller and Rosalind W. Picard},
  doi      = {10.1109/TAFFC.2019.2961881},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {769-780},
  title    = {Holistic affect recognition using PaNDA: Paralinguistic non-metric dimensional analysis},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An active learning paradigm for online audio-visual emotion
recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(2), 756–768. (<a
href="https://doi.org/10.1109/TAFFC.2019.2961089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The advancement of Human-Robot Interaction (HRI) drives research into the development of advanced emotion identification architectures that fathom audio-visual (A-V) modalities of human emotion. State-of-the-art methods in multi-modal emotion recognition mainly focus on the classification of complete video sequences, leading to systems with no online potentialities. Such techniques are capable of predicting emotions only when the videos are concluded, thus restricting their applicability in practical scenarios. This article provides a novel paradigm for online emotion classification, which exploits both audio and visual modalities and produces a responsive prediction when the system is confident enough. We propose two deep Convolutional Neural Network (CNN) models for extracting emotion features, one for each modality, and a Deep Neural Network (DNN) for their fusion. In order to conceive the temporal quality of human emotion in interactive scenarios, we train in cascade a Long Short-Term Memory (LSTM) layer and a Reinforcement Learning (RL) agent –which monitors the speaker– thus stopping feature extraction and making the final prediction. The comparison of our results on two publicly available A-V emotional datasets viz., RML and BAUM-1s, against other state-of-the-art models, demonstrates the beneficial capabilities of our work.},
  archive  = {J},
  author   = {Ioannis Kansizoglou and Loukas Bampis and Antonios Gasteratos},
  doi      = {10.1109/TAFFC.2019.2961089},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {756-768},
  title    = {An active learning paradigm for online audio-visual emotion recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evoking physiological synchrony and empathy using social VR
with biofeedback. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(2), 746–755. (<a
href="https://doi.org/10.1109/TAFFC.2019.2958657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the advent of consumer grade virtual reality (VR) headsets and physiological measurement devices, new possibilities for mediated social interaction emerge enabling the immersion to environments where the visual features react to the users’ physiological activation. In this study, we investigated whether and how individual and interpersonally shared biofeedback (visualised respiration rate and frontal asymmetry of electroencephalography, EEG) enhance synchrony between the users’ physiological activity and perceived empathy towards the other during a compassion meditation exercise carried out in a social VR setting. The study was conducted as a laboratory experiment $(\text{N} = 72)$ employing a Unity3D-based Dynecom immersive social meditation environment and two amplifiers to collect the psychophysiological signals for the biofeedback. The biofeedback on empathy-related EEG frontal asymmetry evoked higher self-reported empathy towards the other user than the biofeedback on respiratory activation, but the perceived empathy was highest when both feedbacks were simultaneously presented. In addition, the participants reported more empathy when there was stronger EEG frontal asymmetry synchronization between the users. The presented results inform the field of affective computing on the possibilities that VR offers for different applications of empathic technologies.},
  archive  = {J},
  author   = {Mikko Salminen and Simo Järvelä and Antti Ruonala and Ville J. Harjunen and Juho Hamari and Giulio Jacucci and Niklas Ravaja},
  doi      = {10.1109/TAFFC.2019.2958657},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {746-755},
  title    = {Evoking physiological synchrony and empathy using social VR with biofeedback},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ACSEE: Antagonistic crowd simulation model with emotional
contagion and evolutionary game theory. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(2), 729–745. (<a
href="https://doi.org/10.1109/TAFFC.2019.2954394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Antagonistic crowd behaviors are often observed in cases of serious conflict. Antagonistic emotions, which is the typical psychological state of agents in different roles (i.e., cops, activists, and civilians) in crowd violence scenes, and the way they spread through contagion in a crowd are important causes of crowd antagonistic behaviors. Moreover, games, which refers to the interaction between opposing groups adopting different strategies to obtain higher benefits and less casualties, determine the level of crowd violence. We present an antagonistic crowd simulation model (ACSEE), which is integrated with antagonistic emotional contagion and evolutionary game theories. Our approach models the antagonistic emotions between agents in different roles using two components: mental emotion and external emotion. We combine enhanced susceptible-infectious-susceptible (SIS) and game approaches to evaluate the role of antagonistic emotional contagion in crowd violence. Our evolutionary game theoretic approach incorporates antagonistic emotional contagion through deterrent force, which is modelled by a mixture of emotional forces and physical forces defeating the opponents. Antagonistic emotional contagion and evolutionary game theories influence each other to determine antagonistic crowd behaviors. We evaluate our approach on real-world scenarios consisting of different kinds of agents. We also compare the simulated crowd behaviors with real-world crowd videos and use our approach to predict the trends of crowd movements in violence incidents. We investigate the impact of various factors (number of agents, emotion, strategy, etc.) on the outcome of crowd violence. We present results from user studies suggesting that our model can simulate antagonistic crowd behaviors similar to those seen in real-world scenarios.},
  archive  = {J},
  author   = {Chaochao Li and Pei Lv and Dinesh Manocha and Hua Wang and Yafei Li and Bing Zhou and Mingliang Xu},
  doi      = {10.1109/TAFFC.2019.2954394},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {729-745},
  title    = {ACSEE: Antagonistic crowd simulation model with emotional contagion and evolutionary game theory},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analyzing group-level emotion with global alignment kernel
based approach. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(2), 713–728. (<a
href="https://doi.org/10.1109/TAFFC.2019.2953664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {From the perspective of social science, understanding group emotion has become increasingly important for teams to considerably accomplish organizational work. Currently, automatically analyzing the perceived affect of a group of people has been received increasingly interest in affective computing community. The variability in group size makes difficulty for group-level emotion recognition to straightforwardly measure the feature distance of two group-level images. Recent works attempted to resolve the preceding problem by using feature encoding. However, the early works lack of efficiency. To alleviate this problem, this article aims to design a new method to effectively analyze the group behavior from a group-level image. Motivated by time-series kernel approaches explored in dynamic facial expression classification, this article mainly concentrates on global alignment kernel and design support vector machine with the combined global alignment kernels (SVM-CGAK) to better recognize group-level emotion. Specifically, we first propose to use global alignment kernel to explicitly measure the distance of two group-level images. For improving the performance of global alignment kernel, we use the global weight sort scheme based on their spatial relation information to sort the faces from group-level image, making an efficient data structure to the global alignment kernel. With this new global alignment kernel, we construct the backbone of SVM-CGAK, namely, support vector machine with global alignment kernel. Furthermore, considering the challenging environment, we construct two global alignment kernels based on Reisz-based Volume Local Binary Pattern and deep convolutional neural network features, respectively. Lastly, to make the robustness of group-level emotion recognition, we propose SVM-CGAK combining both global alignment kernels with multiple kernel learning approach. It can enhance the discriminative ability of each global alignment kernel. Intensive experiments are conducted on three challenging group-level emotion databases. The experimental results demonstrate that the proposed approach achieves promising performance for group-level emotion recognition compared with the recent state-of-the-art methods.},
  archive  = {J},
  author   = {Xiaohua Huang and Abhinav Dhall and Roland Goecke and Matti Pietikäinen and Guoying Zhao},
  doi      = {10.1109/TAFFC.2019.2953664},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {713-728},
  title    = {Analyzing group-level emotion with global alignment kernel based approach},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The change matters! Measuring the effect of changing the
leader in joint music performances. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(2), 700–712. (<a
href="https://doi.org/10.1109/TAFFC.2019.2951368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In a joint action, a group of individuals coordinate their movements to reach a shared goal. When a change–i.e., an event that affects group functioning–occurs, the group adopts strategies to face it. This article investigates how a change involving a strategic core role in a group affects interpersonal coordination, and ultimately group effectiveness in performing a joint action. Following the entrainment theory, interpersonal coordination is addressed in terms of the rhythmic cycles of the individuals and of the group and their adjustment. Music is used as an ideal ecological scenario for investigation. More specifically, this article focuses on orchestra playing. By adopting a computational approach, research is devoted to measure how a change of conductor (i.e., the leader) influences entrainment between players and its variation over time as well as the relationship between entrainment and external ratings of the orchestra performance. Results show that, whereas the change of conductor had a limited significant effect on entrainment, a significant effect was found when entrainment is used as a predictor of the external ratings. Both the obtained results and the techniques developed for measuring entrainment may open novel research directions in the area of automated analysis of group behavior, and particularly of emotion in groups.},
  archive  = {J},
  author   = {Giovanna Varni and Maurizio Mancini and Luciano Fadiga and Antonio Camurri and Gualtiero Volpe},
  doi      = {10.1109/TAFFC.2019.2951368},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {700-712},
  title    = {The change matters! measuring the effect of changing the leader in joint music performances},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamics of blink and non-blink cyclicity for affective
assessment: A case study for stress identification. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(2), 689–699. (<a
href="https://doi.org/10.1109/TAFFC.2019.2946829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Previous studies have shown that eye activities, including blinks, can indicate the psychological state of an individual. However, almost all previous studies analyzing blinks merely concentrated on traditional descriptive statistics, which are unable to reflect their dynamic processes. Furthermore, the states of non-blink (opening the eyes) and blink alternate with each other, forming a physiological cycle. If we only investigate blinks alone, it may be inadequate to describe how blinking works. Therefore, we attempted to recognize the affective state (“relaxation” vs. “stress”) of an individual through the dynamics of blink and non-blink cyclicity (BNBC), as one example, to illustrate this method. First, the “Stroop Test” was employed for emotion elicitation. Then, features were extracted from a categorical time series (0: non-blink; 1: blink), which was recorded by the eye-tracking system. Finally, the areas under the receiver operating characteristic curve (AUC) values were obtained via eight commonly used classifiers. The results show that, compared with the traditional approaches for blink analysis, BNBC exhibits more compelling proficiency to detect stress. In summation, BNBC can be considered a new type of psychophysiological measure, which could be widely applied in psychology, medicine, and engineering.},
  archive  = {J},
  author   = {Peng Ren and Armando Barreto and Xiaole Ma and Shengnan Liu and Min Zhang and Ying Wang and Yeyun Dong and Dezhong Yao},
  doi      = {10.1109/TAFFC.2019.2946829},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {689-699},
  title    = {Dynamics of blink and non-blink cyclicity for affective assessment: A case study for stress identification},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spontaneous speech emotion recognition using multiscale deep
convolutional LSTM. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(2), 680–688. (<a
href="https://doi.org/10.1109/TAFFC.2019.2947464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, emotion recognition in real sceneries such as in the wild has attracted extensive attention in affective computing, because existing spontaneous emotions in real sceneries are more challenging and difficult to identify than other emotions. Motivated by the diverse effects of different lengths of audio spectrograms on emotion identification, this paper proposes a multiscale deep convolutional long short-term memory (LSTM) framework for spontaneous speech emotion recognition. Initially, a deep convolutional neural network (CNN) model is used to learn deep segment-level features on the basis of the created image-like three channels of spectrograms. Then, a deep LSTM model is adopted on the basis of the learned segment-level CNN features to capture the temporal dependency among all divided segments in an utterance for utterance-level emotion recognition. Finally, different emotion recognition results, obtained by combining CNN with LSTM at multiple lengths of segment-level spectrograms, are integrated by using a score-level fusion strategy. Experimental results on two challenging spontaneous emotional datasets, i.e., the AFEW5.0 and BAUM-1s databases, demonstrate the promising performance of the proposed method, outperforming state-of-the-art methods.},
  archive  = {J},
  author   = {Shiqing Zhang and Xiaoming Zhao and Qi Tian},
  doi      = {10.1109/TAFFC.2019.2947464},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {680-688},
  title    = {Spontaneous speech emotion recognition using multiscale deep convolutional LSTM},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Investigation of speech landmark patterns for depression
detection. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(2), 666–679. (<a
href="https://doi.org/10.1109/TAFFC.2019.2944380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The massive and growing burden imposed on modern society by depression has motivated investigations into early detection through automated, scalable and non-invasive methods, including those based on speech. However, speech-based methods that capture articulatory information effectively across different recording devices and in naturalistic environments are still needed. This article proposes two feature sets associated with speech articulation events based on counts and durations of sequential landmark groups or n -grams. Statistical analysis of the duration-based features reveals that durations from several consecutive landmark bigrams and onset-offset landmark pairs are significant in discriminating depressed from non-depressed speakers. In addition to investigating different normalization approaches and values of n for landmark n -gram features, experiments across different elicitation tasks suggest that the features can be tailored to capture different articulatory aspects of depressed voices. Evaluations of both landmark duration features and landmark n -gram features on the DAIC-WOZ and SH2 datasets show that they are highly effective, either alone or fused, relative to existing approaches.},
  archive  = {J},
  author   = {Zhaocheng Huang and Julien Epps and Dale Joachim},
  doi      = {10.1109/TAFFC.2019.2944380},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {666-679},
  title    = {Investigation of speech landmark patterns for depression detection},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FaceEngage: Robust estimation of gameplay engagement from
user-contributed (YouTube) videos. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(2), 651–665. (<a
href="https://doi.org/10.1109/TAFFC.2019.2945014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Measuring user engagement in interactive tasks can facilitate numerous applications toward optimizing user experience, ranging from eLearning to gaming. However, a significant challenge is the lack of non-contact engagement estimation methods that are robust in unconstrained environments. We present FaceEngage, a non-intrusive engagement estimator leveraging user facial recordings during actual gameplay in naturalistic conditions. Our contributions are three-fold. First, we show the potential of using front-facing videos as training data to build the engagement estimator. We compile FaceEngage Dataset with over 700 picture-in-picture, realisitic, and user-contributed YouTube gaming videos (i.e., with both full-screen game scenes and time-synchronized user facial recordings in subwindows). Second, we develop FaceEngage system, that captures relevant gamer facial features from front-facing recordings to infer task engagement. We implement two FaceEngage pipelines: an estimator trained on user facial motion features inspired by prior psychological works, and a deep learning-enabled estimator. Lastly, we conduct extensive experiments and conclude: (i) certain user facial motion cues (e.g., blink rates, head movements) are engagement-indicative; (ii) our deep learning-enabled FaceEngage pipeline can automatically extract more informative features, outperforming the facial motion feature-based pipeline; (iii) FaceEngage is robust to various video lengths, users/game genres and interpretable. Despite the challenging nature of realistic videos, FaceEngage attains the accuracy of 83.8 percent and leave-one-user-out precision of 79.9 percent, both of which are superior to our face motion-based model.},
  archive  = {J},
  author   = {Xu Chen and Li Niu and Ashok Veeraraghavan and Ashutosh Sabharwal},
  doi      = {10.1109/TAFFC.2019.2945014},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {651-665},
  title    = {FaceEngage: Robust estimation of gameplay engagement from user-contributed (YouTube) videos},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Aspect-based sentiment analysis with new target
representation and dependency attention. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(2), 640–650. (<a
href="https://doi.org/10.1109/TAFFC.2019.2945028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Aspect-based sentiment analysis (ABSA) is crucial for exploring user feedbacks and preferences on produces or services. Although numerous classical deep learning-based methods have been proposed in previous literature, several useful cues (e.g., contextual, lexical, and syntactic) are still not fully considered and utilized. In this study, a new approach for ABSA is proposed through the guidance of contextual, lexical, and syntactic cues. First, a novel sub-network is introduced to represent a target in a sentence in ABSA by considering the whole context. Second, lexicon embedding is applied to incorporate additional lexical cues. Third, a new attention module, namely, dependency attention, is proposed to elaborate syntactic dependency cues between words in attention inference. Experimental results on four benchmark data sets demonstrate the effectiveness of our proposed approach to aspect-based sentiment analysis.},
  archive  = {J},
  author   = {Tao Yang and Qing Yin and Lei Yang and Ou Wu},
  doi      = {10.1109/TAFFC.2019.2945028},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {640-650},
  title    = {Aspect-based sentiment analysis with new target representation and dependency attention},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Affective dynamics: Causality modeling of temporally
evolving perceptual and affective responses. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(2), 628–639. (<a
href="https://doi.org/10.1109/TAFFC.2019.2942931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Human perceptual and affective responses change dynamically when stimuli are experienced. In this study, we developed a method for modeling the causal structures of affective dynamics using time-series data. Using the temporal dominance of sensations method, perceptual and affective data were collected from individuals eating strawberries, and the resulting time-series data were mathematically represented using a vector auto-regression model. Multihierarchical and multidimensional causality structures that explain the temporal evolution of perceptual and affective responses were then established based on Granger causality and the information criterion. The established model suggests how affective and preferential responses are triggered following exposure to stimuli. We also assessed the quantitative and semantic validity of the model.},
  archive  = {J},
  author   = {Takumu Okada and Shogo Okamoto and Yoji Yamada},
  doi      = {10.1109/TAFFC.2019.2942931},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {628-639},
  title    = {Affective dynamics: Causality modeling of temporally evolving perceptual and affective responses},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ICA-evolution based data augmentation with ensemble deep
neural networks using time and frequency kernels for emotion recognition
from EEG-data. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(2), 616–627. (<a
href="https://doi.org/10.1109/TAFFC.2019.2942587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The aim of this study is to recognize human emotions from electroencephalographic (EEG) signals using deep neural networks. Large training data is an important prerequisite for successful implementation of deep neural networks. In this view, we propose an independent component analysis (ICA) - evolution based data augmentation method. This method performs ICA to extract and accumulate clean independent components (ICs) of each class. The new ICs are generated by selection which uses a fitness function such as mutual information (MI) and crossover in component space. Data augmentation is done by performing mutation, and crossover on generated data in sensor space. Since EEG signals are non-stationary, with time-varying frequency contents, emotional patterns associated with EEG are detected in the time-frequency (TF) domain using a spectrogram. To extract emotion related features from a spectrogram, we train an ensemble convolutional neural networks (CNNs) with convolutional kernels in time and frequency axes. The information integrated over both the axes is concatenated and fed to long short-term memory (LSTM). We used the benchmark DEAP dataset for emotion classification to evaluate our approach. The results highlight the potential of proposed ICA-evolution based data augmentation and an ensemble CNNs with LSTM model for emotion recognition.},
  archive  = {J},
  author   = {Jun-Su Kang and Swathi Kavuri and Minho Lee},
  doi      = {10.1109/TAFFC.2019.2942587},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {616-627},
  title    = {ICA-evolution based data augmentation with ensemble deep neural networks using time and frequency kernels for emotion recognition from EEG-data},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Emotion prediction with weighted appraisal models – towards
validating a psychological theory of affect. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(2), 604–615. (<a
href="https://doi.org/10.1109/TAFFC.2019.2940937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Appraisal theories are a prominent approach for the explanation and prediction of emotions. According to these theories, the subjective perception of an emotion results from a series of specific event evaluations. To validate and extend one of the most known representatives of appraisal theory, the Component Process Model by Klaus Scherer, we implemented four computational appraisal models that predicted emotion labels based on prototype similarity calculations. Different weighting algorithms, mapping the models’ input to a distinct emotion label, were integrated in the models. We evaluated the plausibility of the models’ structure by assessing their predictive power and comparing their performance to a baseline model and a highly predictive machine learning algorithm. Model parameters were estimated from empirical data and validated out-of-sample. All models were notably better than the baseline model and able to explain part of the variance in the emotion labels. The preferred model, yielding a relatively high performance and stable parameter estimations, was able to predict a correct emotion label with an accuracy of 40.2 percent and a correct emotion family with an accuracy of 76.9 percent. The weighting algorithm of this favored model corresponds to the weighting complexity implied by the Component Process Model, but uses differing weighting parameters.},
  archive  = {J},
  author   = {Laura S. F. Israel and Felix D. Schönbrodt},
  doi      = {10.1109/TAFFC.2019.2940937},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {604-615},
  title    = {Emotion prediction with weighted appraisal models – towards validating a psychological theory of affect},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the influence of shot scale on film mood and narrative
engagement in film viewers. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(2), 592–603. (<a
href="https://doi.org/10.1109/TAFFC.2019.2939251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The perceived distance of the camera from the subject of a filmed scene, namely shot scale , is a prominent formal feature of any filmic product, endowed with both stylistic and narrative functions. To measure how shot scale affects both lower and higher complexity responses in viewers, we first investigate how the distribution and rotation of Close, Medium, and Long Shots relate to viewers’ rating on film mood , assessed in terms of hedonic tone, energetic arousal, and tense arousal on an extensive set of 50 film clips. Then we examine the effect of shot scale on viewers of violent scenes in terms of narrative engagement and its sub-scales: narrative understanding, attentional focus, emotional engagement, and narrative presence. To enable such analysis, shot scale classification is automatized by means of Convolutional Neural Networks trained on the filmographies by six directors for a total of 120 full-length movies analysed at one frame per second. Based on large corpora, this study provides methods for further investigating the relationship between shot scale and the viewers’ emotional involvement. Beyond stylistic analysis, gaining valuable insight into the mechanism of narrative effects of film stories could be useful for purposes such as movie recommendation and film therapy.},
  archive  = {J},
  author   = {Sergio Benini and Mattia Savardi and Katalin Bálint and András Bálint Kovács and Alberto Signoroni},
  doi      = {10.1109/TAFFC.2019.2939251},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {592-603},
  title    = {On the influence of shot scale on film mood and narrative engagement in film viewers},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EmoLabel: Semi-automatic methodology for emotion annotation
of social media text. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(2), 579–591. (<a
href="https://doi.org/10.1109/TAFFC.2019.2927564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The exponential growth of the amount of subjective information on the Web 2.0. has caused an increasing interest from researchers willing to develop methods to extract emotion data from these new sources. One of the most important challenges in textual emotion detection is the gathering of data with emotion labels because of the subjectivity of assigning these labels. Basing on this rationale, the main objective of our research is to contribute to the resolution of this important challenge. This is tackled by proposing EmoLabel: a semi-automatic methodology based on pre-annotation, which consists of two main phases: (1) an automatic process to pre-annotate the unlabelled English sentences; and (2) a manual process of refinement where human annotators determine which is the dominant emotion. Our objective is to assess the influence of this automatic pre-annotation method on manual emotion annotation from two points of view: agreement and time needed for annotation. The evaluation performed demonstrates the benefits of pre-annotation processes since the results on annotation time show a gain of near 20 percent when the pre-annotation process is applied (Pre-ML) without reducing annotator performance. Moreover, the benefits of pre-annotation are higher in those contributors whose performance is low (inaccurate annotators).},
  archive  = {J},
  author   = {Lea Canales and Walter Daelemans and Ester Boldrini and Patricio Martínez-Barco},
  doi      = {10.1109/TAFFC.2019.2927564},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {579-591},
  title    = {EmoLabel: Semi-automatic methodology for emotion annotation of social media text},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). From regional to global brain: A novel hierarchical
spatial-temporal neural network model for EEG emotion recognition.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(2),
568–578. (<a href="https://doi.org/10.1109/TAFFC.2019.2922912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we propose a novel Electroencephalograph (EEG) emotion recognition method inspired by neuroscience with respect to the brain response to different emotions. The proposed method, denoted by R2G-STNN, consists of spatial and temporal neural network models with regional to global hierarchical feature learning process to learn discriminative spatial-temporal EEG features. To learn the spatial features, a bidirectional long short term memory (BiLSTM) network is adopted to capture the intrinsic spatial relationships of EEG electrodes within brain region and between brain regions, respectively. Considering that different brain regions play different roles in the EEG emotion recognition, a region-attention layer into the R2G-STNN model is also introduced to learn a set of weights to strengthen or weaken the contributions of brain regions. Based on the spatial feature sequences, BiLSTM is adopted to learn both regional and global spatial-temporal features and the features are fitted into a classifier layer for learning emotion-discriminative features, in which a domain discriminator working corporately with the classifier is used to decrease the domain shift between training and testing data. Finally, to evaluate the proposed method, we conduct both subject-dependent and subject-independent EEG emotion recognition experiments on SEED database, and the experimental results show that the proposed method achieves state-of-the-art performance.},
  archive  = {J},
  author   = {Yang Li and Wenming Zheng and Lei Wang and Yuan Zong and Zhen Cui},
  doi      = {10.1109/TAFFC.2019.2922912},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {568-578},
  title    = {From regional to global brain: A novel hierarchical spatial-temporal neural network model for EEG emotion recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Study on horse-rider interaction based on body sensor
network in competitive equitation. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(1), 553–567. (<a
href="https://doi.org/10.1109/TAFFC.2019.2936814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Horse-rider interaction analysis by wearable sensors is a promising tool for monitoring equestrian training. In this paper, a body sensor network (BSN) based equestrian motion analysis system is developed, which combines bespoke inertial measurement units (IMU) and MindWave electroencephalography (EEG) acquisition equipment. To fuse the mechanical and EEG signals collected from the system, emotional and attitude information can be obtained to analyze the interaction between the rider and horse in equestrian training. For motion data fusion, a novel method, exercise intensity extend kalman filter (EID-EKF), is proposed, which can also reconstruct the riders’ posture in different gaits by establishing a biomechanical model. The accuracy of our method is verified with the optical system Vicon to support the motion capture for four riding styles (walking, sitting troth, rising trot, canter). Finally, the emotion changes of the riders with different levels are quantified, and kinematic analysis is carried out by combining with inertial and emotional information. It is concluded from the experiment results that the estimation errors are well controlled, and motion patterns acquired according to the kinematic analysis are consistent with the actual situation.},
  archive  = {J},
  author   = {Jie Li and Zhelong Wang and Sen Qiu and Hongyu Zhao and Jiaxin Wang and Xin Shi and Long Liu and Ning Yang},
  doi      = {10.1109/TAFFC.2019.2936814},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {553-567},
  title    = {Study on horse-rider interaction based on body sensor network in competitive equitation},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic recognition methods supporting pain assessment: A
survey. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(1), 530–552. (<a
href="https://doi.org/10.1109/TAFFC.2019.2946774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Pain is a complex phenomenon, involving sensory and emotional experience, that is often poorly understood, especially in infants, anesthetized patients, and others who cannot speak. Technology supporting pain assessment has the potential to help reduce suffering; however, advances are needed before it can be adopted clinically. This survey paper assesses the state of the art and provides guidance for researchers to help make such advances. First, we overview pain’s biological mechanisms, physiological and behavioral responses, emotional components, as well as assessment methods commonly used in the clinic. Next, we discuss the challenges hampering the development and validation of pain recognition technology, and we survey existing datasets together with evaluation methods. We then present an overview of all automated pain recognition publications indexed in the Web of Science as well as from the proceedings of the major conferences on biomedical informatics and artificial intelligence, to provide understanding of the current advances that have been made. We highlight progress in both non-contact and contact-based approaches, tools using face, voice, physiology, and multi-modal information, the importance of context, and discuss challenges that exist, including identification of ground truth. Finally, we identify underexplored areas such as chronic pain and connections to treatments, and describe promising opportunities for continued advances.},
  archive  = {J},
  author   = {Philipp Werner and Daniel Lopez-Martinez and Steffen Walter and Ayoub Al-Hamadi and Sascha Gruss and Rosalind W. Picard},
  doi      = {10.1109/TAFFC.2019.2946774},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {530-552},
  title    = {Automatic recognition methods supporting pain assessment: A survey},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The recognition of multiple anxiety levels based on
electroencephalograph. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(1), 519–529. (<a
href="https://doi.org/10.1109/TAFFC.2019.2936198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Anxiety is a complex emotional state that has a great impact on people&amp;#x0027;s physical and mental health. Effectively identifying different anxiety states is very important. By inducing various anxiety states of 12 healthy college students with electroencephalograph (EEG) recording, comprehensive EEG features, including not only commonly used frequency domain features but also the time domain, statistical and nonlinear features were extracted from different EEG bands and brain locations. Next, correlation analysis was performed between various features and anxiety level changes that were predetermined at each stage of the experiment using a 5-point Likert scale, and the most relevant features were collected. Then, different classifiers were applied to classify four anxiety levels using different features alone or together to explore their anxiety recognition ability. Based on our dataset, the highest accuracy of identifying four anxiety states reached approximately 62.56 percent using the Support Vector Machine (SVM), which improved the classification accuracy compared with previous studies. The results also revealed the importance of EEG linear features (especially for features including total power, mean square and variance) in anxiety recognition. Furthermore, it suggested that EEG features in the beta band and the frontal lobe contributed to anxiety recognition more than the features in the other bands or other brain locations. In short, this study improves the accuracy of multi-level anxiety recognition and helps in choosing better features for anxiety recognition, which lay the foundation for the detection of continuous anxiety changes.},
  archive  = {J},
  author   = {Ziyu Li and Xia Wu and Xueyuan Xu and Hailing Wang and Zhenghao Guo and Zhichao Zhan and Li Yao},
  doi      = {10.1109/TAFFC.2019.2936198},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {519-529},
  title    = {The recognition of multiple anxiety levels based on electroencephalograph},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-label multi-task deep learning for behavioral coding.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(1),
508–518. (<a href="https://doi.org/10.1109/TAFFC.2019.2952113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a methodology for estimating human behaviors in psychotherapy sessions using multi-label and multi-task learning paradigms. We discuss the problem of behavioral coding in which data of human interactions are annotated with labels to describe relevant human behaviors of interest. We describe two related, yet distinct, corpora consisting of therapist-client interactions in psychotherapy sessions. We experimentally compare the proposed learning approaches for estimating behaviors of interest in these datasets. Specifically, we compare single and multiple label learning approaches, single and multiple task learning approaches, and evaluate the performance of these approaches when incorporating turn context. We demonstrate that the best multi-label, multi-task learning model with turn context achieves 18.9 and 19.5 percent absolute improvements with respect to a logistic regression classifier (for each behavioral coding task respectively) and 6.4 and 6.1 percent absolute improvements with respect to the best single-label, single-task deep neural network models. Lastly, we discuss the insights these modeling paradigms provide into these complex interactions including key commonalities and differences of behaviors within and between the two prevalent psychotherapy approaches–Motivational Interviewing and Cognitive Behavioral Therapy–considered.},
  archive  = {J},
  author   = {James Gibson and David C. Atkins and Torrey A. Creed and Zac Imel and Panayiotis Georgiou and Shrikanth Narayanan},
  doi      = {10.1109/TAFFC.2019.2952113},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {508-518},
  title    = {Multi-label multi-task deep learning for behavioral coding},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DepecheMood++: A bilingual emotion lexicon built through
simple yet powerful techniques. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(1), 496–507. (<a
href="https://doi.org/10.1109/TAFFC.2019.2934444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Several lexica for sentiment analysis have been developed; while most of these come with word polarity annotations (e.g., positive/negative), attempts at building lexica for finer-grained emotion analysis (e.g., happiness, sadness) have recently attracted significant attention. They are often exploited as a building block for developing emotion recognition learning models, and/or used as baselines to which the performance of the models can be compared. In this work, we contribute two new resources, that we call DepecheMood++ (DM++): a) an extension of an existing and widely used emotion lexicon for English; and b) a novel version of the lexicon, targeting Italian. Furthermore, we show how simple techniques can be used, both in supervised and unsupervised experimental settings, to boost performance on datasets and tasks of varying degree of domain-specificity. Also, we report an extensive comparative analysis against other available emotion lexica and state-of-the-art supervised approaches, showing that DepecheMood++ emerges as the best-performing non-domain-specific lexicon in unsupervised settings. We also observe that simple learning models on top of DM++ can provide more challenging baselines. We finally introduce embedding-based methodologies to perform a) vocabulary expansion to address data scarcity and b) vocabulary porting to new languages in case training data is not available.},
  archive  = {J},
  author   = {Oscar Araque and Lorenzo Gatti and Jacopo Staiano and Marco Guerini},
  doi      = {10.1109/TAFFC.2019.2934444},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {496-507},
  title    = {DepecheMood++: A bilingual emotion lexicon built through simple yet powerful techniques},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting evolutionary algorithms to model nonverbal
reactions to conversational interruptions in user-agent interactions.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(1),
485–495. (<a href="https://doi.org/10.1109/TAFFC.2019.2947054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In social interactions between humans and Embodied Conversational Agents (ECAs) conversational interruptions may occur. ECAs should be prepared to detect, manage and react to such interruptions in order to keep the interaction smooth, natural and believable. In this paper, we examined nonverbal reactions exhibited by an interruptee during conversational interruptions and we propose a novel technique driven by an evolutionary algorithm to build a computational model for ECAs to manage user&amp;#x0027;s interruptions. We propose a taxonomy of conversational interruptions adapted from social psychology, an annotation schema for semi-automatic detection of user&amp;#x0027;s interruptions and a corpus-based observational analysis of human nonverbal reactions to interruptions. Then we present a methodology for building an ECA behavioral model including the design and realization of an interactive study driven by an evolutionary algorithm, where participants interactively built the most appropriate set of multimodal reactive behaviours for an ECA to display interpersonal attitudes (friendly/hostile) through nonverbal reactions to a conversational interruption.},
  archive  = {J},
  author   = {Angelo Cafaro and Brian Ravenet and Catherine Pelachaud},
  doi      = {10.1109/TAFFC.2019.2947054},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {485-495},
  title    = {Exploiting evolutionary algorithms to model nonverbal reactions to conversational interruptions in user-agent interactions},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint multi-dimensional model for global and time-series
annotations. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(1), 473–484. (<a
href="https://doi.org/10.1109/TAFFC.2020.3006418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Crowdsourcing is a popular approach to collect annotations for unlabeled data instances. It involves collecting a large number of annotations from several, often naive untrained annotators for each data instance which are then combined to estimate the ground truth. Further, annotations for constructs such as affect are often multi-dimensional with annotators rating multiple dimensions, such as valence and arousal, for each instance. Most annotation fusion schemes however ignore this aspect and model each dimension separately. In this article we address this by proposing a generative model for multi-dimensional annotation fusion, which models the dimensions jointly leading to more accurate ground truth estimates. The model we propose is applicable to both global and time series annotation fusion problems and treats the ground truth as a latent variable distorted by the annotators. The model parameters are estimated using the Expectation-Maximization algorithm and we evaluate its performance using synthetic data and real emotion corpora as well as on an artificial task with human annotations.},
  archive  = {J},
  author   = {Anil Ramakrishna and Rahul Gupta and Shrikanth Narayanan},
  doi      = {10.1109/TAFFC.2020.3006418},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {473-484},
  title    = {Joint multi-dimensional model for global and time-series annotations},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Psychophysiological reactions to persuasive messages
deploying persuasion principles. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(1), 461–472. (<a
href="https://doi.org/10.1109/TAFFC.2019.2931689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Measurement of physiological reactions to persuasive messages can improve our understanding of psychological processes of persuasion, and potentially further enhance and personalize current persuasion interventions. However, little is known about the relationship between psychophysiology and persuasive processes. This study focused on four persuasion principles: scarcity, commitment, consensus, and authority, and people&#39;s susceptibility to them. Physiological measures included the cardiovascular, respiratory, and electrodermal system, as well as facial motor systems. Psychological measures consisted of self-reported attitude towards oral care and susceptibility to persuasion (STPS). We performed a randomized within-subject experiment in which fifty-six participants viewed persuasive messages deploying the aforementioned persuasion principles to improve their oral care. Results indicated different physiological patterns during persuasion versus rest. We found no different physiological patterns in exposure to distinct persuasion principles, nor a clear correlation with susceptibility to individual persuasion principles. However, mixed model analysis illustrated that overall STPS scores help explain variance in reactivity of skin conductance level and skin conductance response, and reactivity in the zygomaticus major: lower susceptibility relates to higher reactivity. Summarizing, we have found no conclusive support for distinct psychophysiological patterns associated with different persuasion principles, although overall susceptibility seems to be reflected in physiology to some extent.},
  archive  = {J},
  author   = {Hanne A. A. Spelt and Joyce H. D. M. Westerink and Jaap Ham and Wijnand A. IJsselsteijn},
  doi      = {10.1109/TAFFC.2019.2931689},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {461-472},
  title    = {Psychophysiological reactions to persuasive messages deploying persuasion principles},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Review on psychological stress detection using biosignals.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(1),
440–460. (<a href="https://doi.org/10.1109/TAFFC.2019.2927337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This review investigates the effects of psychological stress on the human body measured through biosignals. When a potentially threatening stimulus is perceived, a cascade of physiological processes occurs mobilizing the body and nervous system to confront the imminent threat and ensure effective adaptation. Biosignals that can be measured reliably in relation to such stressors include physiological (EEG, ECG, EDA, EMG) and physical measures (respiratory rate, speech, skin temperature, pupil size, eye activity). A fundamental objective in this area of psychophysiological research is to establish reliable biosignal indices that reveal the underlying physiological mechanisms of the stress response. Motivated by the lack of comprehensive guidelines on the relationship between the multitude of biosignal features used in the literature and their corresponding behaviour during stress, in this paper, the impact of stress to multiple bodily responses is surveyed. Emphasis is put on the efficiency, robustness and consistency of biosignal data features across the current state of knowledge in stress detection. It is also explored multimodal biosignal analysis and modelling methods for deriving accurate stress correlates. This paper aims to provide a comprehensive review on biosignal patterns caused during stress conditions and reliable practical guidelines towards more efficient detection of stress.},
  archive  = {J},
  author   = {Giorgos Giannakakis and Dimitris Grigoriadis and Katerina Giannakaki and Olympia Simantiraki and Alexandros Roniotis and Manolis Tsiknakis},
  doi      = {10.1109/TAFFC.2019.2927337},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {440-460},
  title    = {Review on psychological stress detection using biosignals},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Leveraging the deep learning paradigm for continuous affect
estimation from facial expressions. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(1), 426–439. (<a
href="https://doi.org/10.1109/TAFFC.2019.2944603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Continuous affect estimation from facial expressions has attracted increased attention in the affective computing research community. This paper presents a principled framework for estimating continuous affect from video sequences. Based on recent developments, we address the problem of continuous affect estimation by leveraging the Bayesian filtering paradigm, i.e., considering affect as a latent dynamical system corresponding to a general feeling of pleasure with a degree of arousal, and recursively estimating its state using a sequence of visual observations. To this end, we advance the state-of-the-art as follows: (i) Canonical face representation (CFR): a novel algorithm for two-dimensional face frontalization, (ii) Convex unsupervised representation learning (CURL): a novel frequency-domain convex optimization algorithm for unsupervised training of deep convolutional neural networks (CNN)s, and (iii) Deep extended Kalman filtering (DEKF): an extended Kalman filtering-based algorithm for affect estimation from a sequence of CNN observations. The performance of the resulting CFR-CURL-DEKF algorithmic framework is empirically evaluated on publicly available benchmark datasets for facial expression recognition (CK+) and continuous affect estimation (AVEC 2012 and 2014).},
  archive  = {J},
  author   = {Meshia Cédric Oveneke and Yong Zhao and Ercheng Pei and Abel Díaz Berenguer and Dongmei Jiang and Hichem Sahli},
  doi      = {10.1109/TAFFC.2019.2944603},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {426-439},
  title    = {Leveraging the deep learning paradigm for continuous affect estimation from facial expressions},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MES-p: An emotional tonal speech dataset in mandarin with
distal and proximal labels. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(1), 408–425. (<a
href="https://doi.org/10.1109/TAFFC.2019.2945322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion shapes all aspects of our interpersonal and intellectual experiences. Its automatic analysis has therefore many applications. In this paper, we propose an emotional tonal speech dataset, Mandarin Chinese Emotional Speech Dataset-Portrayed (MES-P), with both distal and proximal labels. In contrast with state of the art datasets which only focused on perceived emotions, MES-P includes not only perceived emotions (proximal labels) but also intended emotions (distal labels), to make it possible to study human emotional intelligence, i.e., emotion expression/understanding ability, and emotional misunderstandings in real life. Furthermore, MES-P also captures a main feature of tonal languages, and provides emotional speech samples matching the tonal distribution in real life Mandarin. MES-P dataset also features emotion intensity variations, by introducing both moderate and intense versions for joy, anger, and sadness, in addition to neutral. Ratings of the collected speech samples are made in valence-arousal space through continuous coordinate locations, resulting in an emotional distribution pattern in 2D VA space. High consistency between the speakers emotional intentions and the listeners perceptions is also proved by Cohens Kappa coefficients. Finally, extensive experiments are carried out as a baseline on MES-P for automatic emotion recognition and with comparison to human emotion intelligence.},
  archive  = {J},
  author   = {Zhongzhe Xiao and Ying Chen and Weibei Dou and Zhi Tao and Liming Chen},
  doi      = {10.1109/TAFFC.2019.2945322},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {408-425},
  title    = {MES-P: An emotional tonal speech dataset in mandarin with distal and proximal labels},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple instance learning for emotion recognition using
physiological signals. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(1), 389–407. (<a
href="https://doi.org/10.1109/TAFFC.2019.2954118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The problem of continuous emotion recognition has been the subject of several studies. The proposed affective computing approaches employ sequential machine learning algorithms for improving the classification stage, accounting for the time ambiguity of emotional responses. Modeling and predicting the affective state over time is not a trivial problem because continuous data labeling is costly and not always feasible. This is a crucial issue in real-life applications, where data labeling is sparse and possibly captures only the most important events rather than the typical continuous subtle affective changes that occur. In this work, we introduce a framework from the machine learning literature called Multiple Instance Learning, which is able to model time intervals by capturing the presence or absence of relevant states, without the need to label the affective responses continuously (as required by standard sequential learning approaches). This choice offers a viable and natural solution for learning in a weakly supervised setting, taking into account the ambiguity of affective responses. We demonstrate the reliability of the proposed approach in a gold-standard scenario and towards real-world usage by employing an existing dataset (DEAP) and a purposely built one (Consumer). We also outline the advantages of this method with respect to standard supervised machine learning algorithms.},
  archive  = {J},
  author   = {Luca Romeo and Andrea Cavallo and Lucia Pepa and Nadia Bianchi-Berthouze and Massimiliano Pontil},
  doi      = {10.1109/TAFFC.2019.2954118},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {389-407},
  title    = {Multiple instance learning for emotion recognition using physiological signals},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GCB-net: Graph convolutional broad network and its
application in emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(1), 379–388. (<a
href="https://doi.org/10.1109/TAFFC.2019.2937768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, emotion recognition has become a research focus in the area of artificial intelligence. Due to its irregular structure, EEG data can be analyzed by applying graphical based algorithms or models much more efficiently. In this work, a Graph Convolutional Broad Network (GCB-net) was designed for exploring the deeper-level information of graph-structured data. It used the graph convolutional layer to extract features of graph-structured input and stacks multiple regular convolutional layers to extract relatively abstract features. The final concatenation utilized the broad concept, which preserves the outputs of all hierarchical layers, allowing the model to search features in broad spaces. To improve the performance of the proposed GCB-net, the broad learning system (BLS) was applied to enhance its features. For comparison, two individual experiments were conducted to examine the efficiency of the proposed GCB-net based on the SJTU emotion EEG dataset (SEED) and DREAMER dataset respectively. In SEED, compared with other state-of-art methods, the GCB-net could better promote the accuracy (reaching 94.24 percent) on the DE feature of the all-frequency band. In DREAMER dataset, GCB-net performed better than other models with the same setting. Furthermore, the GCB-net reached high accuracies of 86.99, 89.32 and 89.20 percent on dimensions of Valence, Arousal and Dominance respectively. The experimental results showed the robust classifying ability of the GCB-net and BLS in EEG emotion recognition.},
  archive  = {J},
  author   = {Tong Zhang and Xuehan Wang and Xiangmin Xu and C. L. Philip Chen},
  doi      = {10.1109/TAFFC.2019.2937768},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {379-388},
  title    = {GCB-net: Graph convolutional broad network and its application in emotion recognition},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rating vs. Paired comparison for the judgment of dominance
on first impressions. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(1), 367–378. (<a
href="https://doi.org/10.1109/TAFFC.2020.3022982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents a contest between the rating and the paired comparison voting in judging the perceived dominance of virtual characters, the aim being to select the voting mode that is the most convenient for voters while staying reliable. The comparison consists of an experiment where human subjects vote on a set of virtual characters generated by randomly altering a set of physical attributes. The minimum number of participants has been determined via numerical simulation. The outcome is a sequence of stereotypes ordered along their conveyed amount of submissiveness or dominance. Results show that the two voting modes result in equivalently expressive models of dominance. Further analysis of the voting procedure shows that, despite an initial slower learning phase, after about 30 votes the two modes exhibit the same judging speed. Finally, a subjective questionnaire reports a higher (63.8 percent) preference for the paired comparison mode.},
  archive  = {J},
  author   = {Fabrizio Nunnari and Alexis Heloir},
  doi      = {10.1109/TAFFC.2020.3022982},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {367-378},
  title    = {Rating vs. paired comparison for the judgment of dominance on first impressions},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Affect in multimedia: Benchmarking violent scenes detection.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(1),
347–366. (<a href="https://doi.org/10.1109/TAFFC.2020.2986969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we report on the creation of a publicly available, common evaluation framework for Violent Scenes Detection (VSD) in Hollywood and YouTube videos. We propose a robust data set, the VSD96, with more than 96 hours of video of various genres, annotations at different levels of detail (e.g., shot-level, segment-level), annotations of mid-level concepts (e.g., blood, fire), various pre-computed multi-modal descriptors, and over 230 system output results as baselines. This is the most comprehensive data set available to this date tailored to the VSD task and was extensively validated during the MediaEval benchmarking campaigns. Furthermore, we provide an in-depth analysis of the crucial components of VSD algorithms, by reviewing the capabilities and the evolution of existing systems (e.g., overall trends and outliers, the influence of the employed features and fusion techniques, the influence of deep learning approaches). Finally, we discuss the possibility of going beyond state-of-the-art performance via an ad-hoc late fusion approach. Experimentation is carried out on the VSD96 data. We provide the most important lessons learned and gained insights. The increasing number of publications using the VSD96 data underline the importance of the topic. The presented and published resources are a practitioner&#39;s guide and also a strong baseline to overcome, which will help researchers for the coming years in analyzing aspects of audio-visual affect and violence detection in movies and videos.},
  archive  = {J},
  author   = {Mihai Gabriel Constantin and Liviu-Daniel Ştefan and Bogdan Ionescu and Claire-Hélène Demarty and Mats Sjöberg and Markus Schedl and Guillaume Gravier},
  doi      = {10.1109/TAFFC.2020.2986969},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {347-366},
  title    = {Affect in multimedia: Benchmarking violent scenes detection},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Facial expressions of comprehension (FEC). <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(1), 335–346. (<a
href="https://doi.org/10.1109/TAFFC.2019.2954498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {While the relationship between facial expressions and emotion has been a productive area of inquiry, research is only recently exploring whether a link exists between facial expressions and cognitive processes. Using findings from psychology and neuroscience to guide predictions of affectation during a cognitive task, this article aimed to study facial dynamics as a mean to understand comprehension. We present a new multimodal facial expression database, named Facial Expressions of Comprehension (FEC), consisting of the videos recorded during a computer-mediated task in which each trial consisted of reading, answering, and feedback to general knowledge true and false statements. To identify the level of engagement with the corresponding stimuli, we present a new methodology using animation units (AnUs) from the Kinect v2 device to explore the changes in facial configuration caused by an event: Event-Related Intensities (ERIs). To identify dynamic facial configurations, we used ERIs in statistical analyses with generalized additive models. To identify differential facial dynamics linked to knowing vs. guessing and true vs. false responses, we employed an SVM classifier with facial appearance information extracted using LPQ-TOP. Results of ERIs in sentence comprehension show that facial dynamics are promising to help understand affective and cognitive states of the mind.},
  archive  = {J},
  author   = {Cigdem Turan and Karl David Neergaard and Kin-Man Lam},
  doi      = {10.1109/TAFFC.2019.2954498},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {335-346},
  title    = {Facial expressions of comprehension (FEC)},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-fusion residual memory network for multimodal human
sentiment comprehension. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(1), 320–334. (<a
href="https://doi.org/10.1109/TAFFC.2020.3000510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multimodal human sentiment comprehension refers to recognizing human affection from multiple modalities. There exist two key issues for this problem. First, it is difficult to explore time-dependent interactions between modalities and focus on the important time steps. Second, processing the long fused sequence of utterances is susceptible to the forgetting problem due to the long-term temporal dependency. In this article, we introduce a hierarchical learning architecture to classify utterance-level sentiment. To address the first issue, we perform time-step level fusion to generate fused features for each time step, which explicitly models time-restricted interactions by incorporating information across modalities at the same time step. Furthermore, based on the assumption that acoustic features directly reflect emotional intensity, we pioneer emotion intensity attention to focus on the time steps where emotion changes or intense affections take place. To handle the second issue, we propose Residual Memory Network (RMN) to process the fused sequence. RMN utilizes some techniques such as directly passing the previous state into the next time step, which helps to retain the information from many time steps ago. We show that our method achieves state-of-the-art performance on multiple datasets. Results also suggest that RMN yields competitive performance on sequence modeling tasks.},
  archive  = {J},
  author   = {Sijie Mai and Haifeng Hu and Jia Xu and Songlong Xing},
  doi      = {10.1109/TAFFC.2020.3000510},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {320-334},
  title    = {Multi-fusion residual memory network for multimodal human sentiment comprehension},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal deception detection using real-life trial data.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(1),
306–319. (<a href="https://doi.org/10.1109/TAFFC.2020.3015684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Hearings of witnesses and defendants play a crucial role when reaching court trial decisions. Given the high-stakes nature of trial outcomes, developing computational models that assist the decision-making process is an important research venue. In this article, we address the identification of deception in real-life trial data. We use a dataset consisting of videos collected from public court trials. We explore the use of verbal and non-verbal modalities to build a multimodal deception detection system that aims to discriminate between truthful and deceptive statements provided by defendants and witnesses. In particular, three complementary modalities (visual, acoustic and linguistic) are evaluated for the classification of deception at the subject level. The final classifier is obtained by combining the three modalities via score-level classification, achieving 83.05 percent accuracy in subject-level deceit detection. To place our results in perspective, we present a human deception detection study where we evaluate the human capability of detecting deception using different modalities and compare the results to the developed system. The results show that our system outperforms the average non-expert human capability of identifying deceit.},
  archive  = {J},
  author   = {M. Umut Şen and Verónica Pérez-Rosas and Berrin Yanikoglu and Mohamed Abouelenien and Mihai Burzo and Rada Mihalcea},
  doi      = {10.1109/TAFFC.2020.3015684},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {306-319},
  title    = {Multimodal deception detection using real-life trial data},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PersEmoN: A deep network for joint analysis of apparent
personality, emotion and their relationship. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(1), 298–305. (<a
href="https://doi.org/10.1109/TAFFC.2019.2951656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Apparent personality and emotion analysis are both central to affective computing. Existing works solve them individually. In this paper we investigate if such high-level affect traits and their relationship can be jointly learned from face images in the wild. To this end, we introduce PersEmoN , an end-to-end trainable and deep Siamese-like network. It consists of two convolutional network branches, one for emotion and the other for apparent personality. Both networks share their bottom feature extraction module and are optimized within a multi-task learning framework. Emotion and personality networks are dedicated to their own annotated dataset. Furthermore, an adversarial-like loss function is employed to promote representation coherence among heterogeneous dataset sources. Based on this, we also explore the emotion-to-apparent-personality relationship. Extensive experiments demonstrate the effectiveness of PersEmoN .},
  archive  = {J},
  author   = {Le Zhang and Songyou Peng and Stefan Winkler},
  doi      = {10.1109/TAFFC.2019.2951656},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {298-305},
  title    = {PersEmoN: A deep network for joint analysis of apparent personality, emotion and their relationship},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). All-in-one: Emotion, sentiment and intensity prediction
using a multi-task ensemble framework. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(1), 285–297. (<a
href="https://doi.org/10.1109/TAFFC.2019.2926724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a multi-task ensemble framework that jointly learns multiple related problems. The ensemble model aims to leverage the learned representations of three deep learning models (i.e., CNN, LSTM and GRU) and a hand-crafted feature representation for the predictions. Through multi-task framework, we address four problems of emotion and sentiment analysis, i.e., “emotion classification &amp; intensity ”, “ valence , arousal &amp; dominance for emotion”, “ valence &amp; arousal for sentiment”, and “ 3-class categorical &amp; 5-class ordinal classification for sentiment”. The underlying problems cover two granularity (i.e., coarse-grained and fine-grained ) and a diverse range of domains (i.e., tweets , Facebook posts , news headlines , blogs , letters etc.). Experimental results suggest that the proposed multi-task framework outperforms the single-task frameworks in all experiments.},
  archive  = {J},
  author   = {Md Shad Akhtar and Deepanway Ghosal and Asif Ekbal and Pushpak Bhattacharyya and Sadao Kurohashi},
  doi      = {10.1109/TAFFC.2019.2926724},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {285-297},
  title    = {All-in-one: Emotion, sentiment and intensity prediction using a multi-task ensemble framework},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Affective impression: Sentiment-awareness POI suggestion via
embedding in heterogeneous LBSNs. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(1), 272–284. (<a
href="https://doi.org/10.1109/TAFFC.2019.2925077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Location-based social networks (LBSNs) add geographical information into traditional social networks and link people&amp;#x2019;s virtual and physical lives. As an important application of LBSNs, point-of-interest (POI) suggestion has become an important method to help users explore interesting and attractive locations in LBSNs. The main problems of POI suggestion include data sparsity and cold start, which have been paid much attention by existing techniques. There are two major challenges which can greatly influence the performance of suggestion accuracy. One is the fuzzy boundary between sentiments, i.e., the fine distinction between sentiments makes it difficult to classify words and texts after word-sentiment mapping operation. The other challenge is the unreliability of data quality represented by similarity metrics, which relies on data integrity and path reachability of a heterogeneous network. To cope with the above two challenges, we present a novel framework called Community-based Sentiment Extraction and Network Embedding for POI Recommendation (CENTER) for suggesting impressive POIs to a specific user in an effective fashion. The CENTER framework contains two essential techniques: (1) a latent probabilistic generative model called Community-based Sentiment Extraction (CSE), which can accurately capture the sentiments from review content in LBSNs by taking into consideration the characteristics of social communities. The parameters of the CSE model can be inferred effectively by the Gibbs sampling method. The primary sentiments are obtained based on the distribution of sentiments; (2) a network embedding model called Sentiment-aware Nework Embedding for POI Recommendation (SNER) is employed to learn the representation of the factors including POIs, users and textual sentiments in a low-dimensional embedding space. The joint training is utilized to alternatively sample all sets of edges in a heterogeneous information network. Extensive experiments were conducted on two large-scale real datasets, in order to evaluate the performance of the proposed CENTER framework. The results demonstrate that CENTER is superior to the state-of-the-art baseline methods in the effectiveness and efficiency of POI suggestion.},
  archive  = {J},
  author   = {Xi Xiong and Shaojie Qiao and Nan Han and Yuanyuan Li and Fei Xiong and Ling He},
  doi      = {10.1109/TAFFC.2019.2925077},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {272-284},
  title    = {Affective impression: Sentiment-awareness POI suggestion via embedding in heterogeneous LBSNs},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved empirical mode decomposition of
electroencephalogram signals for depression detection. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(1), 262–271. (<a
href="https://doi.org/10.1109/TAFFC.2019.2934412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Depression is a mental disorder characterized by persistent low mood that affects a person’s thoughts, behavior, feelings, and sense of well-being. According to the World Health Organization (WHO), depression will become the second major life-threatening illness in 2020. Electroencephalogram (EEG) signals, which reflect the working status of human brain, are regarded as the best physiological tool for depression detection. Previous studies used the Empirical Mode Decomposition (EMD) method, which can deal with the highly complex, nonlinear and non-stationary nature of EEG, to extract features from EEG signals. However, for some special data, the neighboring components extracted through EMD could certainly have sections of data carrying the same frequency at different time durations. Thus, the Intrinsic Mode Functions (IMFs) of the data could be linearly dependent and the features coefficients of expansion based on IMFs could not be extracted, which can make the pre-proposed EMD-based feature extraction method impractical. In order to solve this problem, an improved EMD applying Singular Value Decomposition (SVD)-based feature extraction method was proposed in this study, which can extract the features coefficients of expansion based on all IMFs as accurately as possible, ignoring potentially linear dependence of IMFs. Experiments were conducted on four EEG databases for detecting depression. The improved EMD-based feature extraction method can extract feature from all three channels (Fp1, Fpz, and Fp2) on the four EEG databases. The average classification results of the proposed method on the four EEG databases including depressed patients and healthy subjects reached 83.27, 85.19, 81.98 and 88.07 percent, respectively, which were comparable with the pre-proposed EMD-based feature extraction method.},
  archive  = {J},
  author   = {Jian Shen and Xiaowei Zhang and Gang Wang and Zhijie Ding and Bin Hu},
  doi      = {10.1109/TAFFC.2019.2934412},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {262-271},
  title    = {An improved empirical mode decomposition of electroencephalogram signals for depression detection},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Participatory design of affective technology: Interfacing
biomusic and autism. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(1), 250–261. (<a
href="https://doi.org/10.1109/TAFFC.2019.2922911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The benefits of user-centered and participatory design have been widely acknowledged for the development of technologies that are likely to be appropriated by the product&amp;#x2019;s stakeholders. While participatory design has been applied to some affective technologies, the technical and algorithmic complexity of those based on semi-intelligent information filters (SIIFs) pose distinct challenges. Coincidentally, these technologies raise important and distinct ethical issues that make stakeholder input critical during product design. We present a framework for fostering genuine engagement from stakeholders through the case example of biomusic - a SIIF-based affective technology that translates emotion-related physiological changes into sound. During a 3-day workshop, ethnographic methods were used to collect data about the interface between biomusic and individuals on the autism spectrum. From these data, emergent themes, such as such as privacy, data security, conceptions of assistive technology and representation of emotions were analyzed using a grounded theory approach. In order to illuminate distinct design decisions implicated by these complex and interwoven ethical issues, we propose a design framework consisting of a technological, a human-centered and an ecological lens. This framework and recommendations provide a concrete praxis for engaging stakeholders in the complex issues associated with the design of SIIF-based emotion-oriented systems.},
  archive  = {J},
  author   = {Florian Grond and Rossio Motta-Ochoa and Natalie Miyake and Tamar Tembeck and Melissa Park and Stefanie Blain-Moraes},
  doi      = {10.1109/TAFFC.2019.2922911},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {250-261},
  title    = {Participatory design of affective technology: Interfacing biomusic and autism},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Affective audio annotation of public speeches with
convolutional clustering neural network. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(1), 238–249. (<a
href="https://doi.org/10.1109/TAFFC.2019.2937028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Public speaking is a critical skill in daily communication. While more practicing such as rehearsal is helpful to improve such a skill, lack of personalized feedback limits the effectiveness of practicing. Therefore, we formulate the task of personalized feedback as an affective audio annotation problem by learning knowledge from online public speech videos. Considering the great success of deep learning techniques such as convolutional neural networks in a wide range of applications including speech recognition and object recognition, we propose a novel convolutional clustering neural network (CCNN) to solve this multi-label classification problem. Instead of aggregating the features of different channels through pooling, we introduce a novel clustering layer to derive intermediate representation for improved annotation performance. In order to evaluate the performance of our proposed method, we purposely built an affective audio annotation dataset by collecting more than 2,000 video clips from the TED website. Experimental results on this dataset demonstrate that our proposed method outperforms traditional CNN-based approaches with a lower hamming loss for affective annotation.},
  archive  = {J},
  author   = {Jiahao Xu and Boyan Zhang and Zhiyong Wang and Yang Wang and Fang Chen and Junbin Gao and David Dagan Feng},
  doi      = {10.1109/TAFFC.2019.2937028},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {238-249},
  title    = {Affective audio annotation of public speeches with convolutional clustering neural network},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Facial expression recognition using a temporal ensemble of
multi-level convolutional neural networks. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(1), 226–237. (<a
href="https://doi.org/10.1109/TAFFC.2019.2946540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion recognition is indispensable in human-machine interaction systems. It comprises locating facial regions of interest in images and classifying them into one of seven classes: angry, disgust, fear, happy, neutral, sad, and surprise. Despite several breakthroughs in image classification, particularly in facial expression recognition, this research area is still challenging, as sampling in the wild is a demanding task. In this study, a two-stage method is proposed for recognizing facial expressions given a sequence of images. At the first stage, all face regions are extracted in each frame, and essential information that would be helpful and related to human emotion is obtained. Then, the extracted features from the previous step are considered temporal data and are assigned to one of the seven basic emotions. In addition, a study of multi-level features is conducted in a convolutional neural network for facial expression recognition. Moreover, various network connections are introduced to improve the classification task. By combining the proposed network connections, superior results are obtained compared to state-of-the-art methods on the FER2013 dataset. Furthermore, the performance of our temporal model is better than that of the single architecture of the 2017 EmotiW challenge winner on the AFEW 7.0 dataset.},
  archive  = {J},
  author   = {Hai-Duong Nguyen and Sun-Hee Kim and Guee-Sang Lee and Hyung-Jeong Yang and In-Seop Na and Soo-Hyung Kim},
  doi      = {10.1109/TAFFC.2019.2946540},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {226-237},
  title    = {Facial expression recognition using a temporal ensemble of multi-level convolutional neural networks},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development and cross-cultural evaluation of a scoring
algorithm for the biometric attachment test: Overcoming the challenges
of multimodal fusion with “small data.” <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(1), 211–225. (<a
href="https://doi.org/10.1109/TAFFC.2019.2921311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The Biometric Attachment Test (BAT) is a recently developed psychometric assessment that exposes adults to standardized picture and music stimuli-sets while simultaneously capturing their linguistic, behavioral and physiological responses, with the goal of objectively measuring their psychological attachment characteristics. Within this work, (I) we describe a new version of the BAT (v2) that implements a remote photoplethysmography method to obtain physiological measures from video alone. (II) We discuss the specific challenges we found when trying to develop an automatic scoring algorithm for the BAT v2 using machine learning: practicing multimodal fusion over a high-dimensional feature space with a small learning sample. We propose and evaluate an original combination of methods, including a three-step hybrid multimodal fusion procedure, that overcomes these challenges. (III) Using the proposed methodology, we train a scoring algorithm for the BAT v2 on a francophone sample, using the Adult Attachment Questionnaire as ground-truth. (IV) We then validate the scoring algorithm cross-culturally, testing its performance on an independent anglophone sample, showing low error and high correlation and serving as the BAT v2&#39;s first convergent validity evidence. We believe this work constitutes a breakthrough in the development of the first objective and automatic measure for adult attachment, and we hope that our “small data” learning methodology could be useful for other machine learning projects involving small samples coming from psychological research.},
  archive  = {J},
  author   = {Federico Parra and Stefan Scherer and Yannick Benezeth and Plamena Tsvetanova and Susana Tereno},
  doi      = {10.1109/TAFFC.2019.2921311},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {211-225},
  title    = {Development and cross-cultural evaluation of a scoring algorithm for the biometric attachment test: Overcoming the challenges of multimodal fusion with “Small data”},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Emotion recognition and EEG analysis using ADMM-based sparse
group lasso. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(1), 199–210. (<a
href="https://doi.org/10.1109/TAFFC.2019.2943551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This study presents an efficient sparse learning-based pattern recognition framework to recognize the discrete states of three emotions—happy, angry, and neutral emotion—using electroencephalogram (EEG) signals. In affective computing with massive spatiotemporal brainwave signals, a large number of features can be extracted to capture various information from multivariate brain data. However, it is often a challenge to model high-dimensional features efficiently in consideration of the intrinsic structure, such as channel location, feature group, time epoch, etc. In this study, features were extensively extracted from EEG signals and were applied on a structured sparse learning model to perform feature selection and classification simultaneously. An efficient ADMM-based algorithm with a closed-form solution was developed to solve the sparse group model. Experimental results show that the proposed method is capable of selecting a small number of important neural features to discriminate the three emotion states with high classification accuracy. With greatly enhanced interpretability and efficiency to learn neural signatures of brain activity from high-dimensional-feature, low-sample-size brain imaging data, the presented computational framework is promising for handling emotion recognition problems with high-dimensional brain imaging data.},
  archive  = {J},
  author   = {Kin Ming Puk and Shouyi Wang and Jay Rosenberger and Kellen C. Gandy and Haley Nicole Harris and Yuan Bo Peng and Anne Nordberg and Peter Lehmann and Jodi Tommerdahl and Jung-Chih Chiao},
  doi      = {10.1109/TAFFC.2019.2943551},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {199-210},
  title    = {Emotion recognition and EEG analysis using ADMM-based sparse group lasso},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Emotional conversation generation orientated syntactically
constrained bidirectional-asynchronous framework. <em>IEEE Transactions
on Affective Computing</em>, <em>13</em>(1), 187–198. (<a
href="https://doi.org/10.1109/TAFFC.2019.2923619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The field of open-domain conversation generation using deep neural networks has attracted increasing attention from researchers for several years. However, traditional neural language models tend to generate safe, generic reply with poor logic and no emotion. In this paper, an emotional conversation generation orientated syntactically constrained bidirectional-asynchronous framework called E-SCBA is proposed to generate meaningful (logical and emotional) reply. In E-SCBA, pre-generated emotion keyword and topic keyword are asynchronously introduced into the reply during the generation, and the process of decoding is much different from the most existing methods that generates reply from the first word to the end. A newly designed bidirectional-asynchronous decoder with the multi-stage strategy is proposed to support this idea, which ensures the fluency and grammaticality of reply by making full use of syntactic constraint. Through the experiments, the results show that our framework not only improves the diversity of replies, but gains a boost on both logic and emotion compared with baselines as well.},
  archive  = {J},
  author   = {Xiao Sun and Jingyuan Li and Jianhua Tao},
  doi      = {10.1109/TAFFC.2019.2923619},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {187-198},
  title    = {Emotional conversation generation orientated syntactically constrained bidirectional-asynchronous framework},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On emotions as features for speech overlaps classification.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(1),
175–186. (<a href="https://doi.org/10.1109/TAFFC.2019.2925795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Although being a frequently occurring phenomenon in spoken communication, speech overlaps did not obtain the deserved attention in research so far&amp;#x2014;in both Human-Human Interaction (HHI) and Human-Computer Interaction (HCI). It is common knowledge that overlaps can figure as a competitive, rude interruption as well as a cooperative, convenient feedback signal giving important insight on the course of the interaction&amp;#x2014;but how are they related to the internal state of the overlapping speaker or the overlapped speaker? In this paper, we investigate dyadic human-human interactions and focus on the relations between the emotional changes occurring around overlaps in both interaction participants. Further to an in-depth statistical analysis of the changes in control and valence levels with respect to the nature of the overlap, we also present a classification approach based on features derived from such emotional changes surrounding an overlap and compare the classification performance of these features to classic acoustic features. We show that the automatic classification of competitive and cooperative overlaps using the changes in valence and control levels of the overlapping speaker outperforms common approaches employing acoustic and linguistic features.},
  archive  = {J},
  author   = {Olga Egorow and Andreas Wendemuth},
  doi      = {10.1109/TAFFC.2019.2925795},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {175-186},
  title    = {On emotions as features for speech overlaps classification},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On-the-fly facial expression prediction using LSTM encoded
appearance-suppressed dynamics. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(1), 159–174. (<a
href="https://doi.org/10.1109/TAFFC.2019.2957465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Encoding the facial expression dynamics is efficient in classifying and recognizing facial expressions. Most facial dynamics-based methods assume that a sequence is temporally segmented before prediction. This requires the prediction to wait until a full sequence is available, resulting in prediction delay. To reduce the prediction delay and enable prediction “on-the-fly” (as frames are fed to the system), we propose new dynamics feature learning method that allows prediction with partial (incomplete) sequences. The proposed method utilizes the readiness of recurrent neural networks (RNNs) for on-the-fly prediction, and introduces novel learning constraints to induce early prediction with partial sequences. We further show that a delay in accurate prediction using RNNs could originate from the effect that the subject appearance has on the spatio-temporal features encoded by the RNN. We refer to that effect as “appearance bias”. We propose the appearance suppressed dynamics feature, which utilizes a static sequence to suppress the appearance bias. Experimental results have shown that the proposed method achieved higher recognition rates compared to the state-of-the-art methods on publicly available datasets. The results also verified that the proposed method improved on-the-fly prediction at subtle expression frames early in the sequence, using partial sequence inputs.},
  archive  = {J},
  author   = {Wissam J. Baddar and Sangmin Lee and Yong Man Ro},
  doi      = {10.1109/TAFFC.2019.2957465},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {159-174},
  title    = {On-the-fly facial expression prediction using LSTM encoded appearance-suppressed dynamics},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Micro and macro facial expression recognition using advanced
local motion patterns. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(1), 147–158. (<a
href="https://doi.org/10.1109/TAFFC.2019.2949559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we develop a new method that recognizes facial expressions, on the basis of an innovative Local Motion Patterns (LMP) feature. The LMP feature analyzes locally the motion distribution in order to separate consistent mouvement patterns from noise. Indeed, facial motion extracted from the face is generally noisy and without specific processing, it can hardly cope with expression recognition requirements especially for micro-expressions. Direction and magnitude statistical profiles are jointly analyzed in order to filter out noise. This work presents three main contributions. The first one is the analysis of the face skin temporal elasticity and face deformations during expression. The second one is a unified approach for both macro and micro expression recognition leading the way to supporting a wide range of expression intensities. The third one is the step forward towards in-the-wild expression recognition, dealing with challenges such as various intensity and various expression activation patterns, illumination variations and small head pose variations. Our method outperforms state-of-the-art methods for micro expression recognition and positions itself among top-ranked state-of-the-art methods for macro expression recognition.},
  archive  = {J},
  author   = {Benjamin Allaert and Ioan Marius Bilasco and Chaabane Djeraba},
  doi      = {10.1109/TAFFC.2019.2949559},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {147-158},
  title    = {Micro and macro facial expression recognition using advanced local motion patterns},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning pain from action unit combinations: A weakly
supervised approach via multiple instance learning. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(1), 135–146. (<a
href="https://doi.org/10.1109/TAFFC.2019.2949314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Patient pain can be detected highly reliably from facial expressions using a set of facial muscle-based action units (AUs) defined by the Facial Action Coding System (FACS). A key characteristic of facial expression of pain is the simultaneous occurrence of pain-related AU combinations, whose automated detection would be highly beneficial for efficient and practical pain monitoring. Existing general Automated Facial Expression Recognition (AFER) systems prove inadequate when applied specifically for detecting pain as they either focus on detecting individual pain-related AUs but not on combinations or they seek to bypass AU detection by training a binary pain classifier directly on pain intensity data but are limited by lack of enough labeled data for satisfactory training. In this paper, we propose a new approach that mimics the strategy of human coders of decoupling pain detection into two consecutive tasks: one performed at the individual video-frame level and the other at video-sequence level. Using state-of-the-art AFER tools to detect single AUs at the frame level, we propose two novel data structures to encode AU combinations from single AU scores. Two weakly supervised learning frameworks namely multiple instance learning (MIL) and multiple clustered instance learning (MCIL) are employed corresponding to each data structure to learn pain from video sequences. Experimental results show an 87 percent pain recognition accuracy with 0.94 AUC (Area Under Curve) on the UNBC-McMaster Shoulder Pain Expression dataset. Tests on long videos in a lung cancer patient video dataset demonstrates the potential value of the proposed system for pain monitoring in clinical settings.},
  archive  = {J},
  author   = {Zhanli Chen and Rashid Ansari and Diana J. Wilkie},
  doi      = {10.1109/TAFFC.2019.2949314},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {135-146},
  title    = {Learning pain from action unit combinations: A weakly supervised approach via multiple instance learning},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regression guided by relative ranking using convolutional
neural network (r<span
class="math inline"><sup>3</sup></span><!-- -->3CNN) for facial beauty
prediction. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(1), 122–134. (<a
href="https://doi.org/10.1109/TAFFC.2019.2933523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial beauty prediction (FBP) aims to automatically assess facial attractiveness consistently with judgements based on human perception. Most of previous methods formulate FBP as a classification, regression or ranking problem of machine learning. However, humans not only represent facial attractiveness as a score, but also perceive the relative aesthetics of faces. Inspired by this observation, we formulate FBP as a specific regression problem guided by ranking information. Specifically, we propose a general CNN architecture, called R $^3$ CNN, to integrate the relative ranking of faces in terms of aesthetics to improve performance of FBP. As R $^3$ CNN consists of both regression and ranking components, it is challenging to train and fine-tune it by existing techniques. To tackle this problem, we propose the following learning schemes for R $^3$ CNN: 1) a hard pair sampling strategy that generates challenging-to-predicted image pairs and pseudo ranking labels from true rating scores; 2) an assemble loss function that combines regression loss and pairwise ranking loss (PR-Loss), where PR-Loss can be a hinge-form loss or a log-sum-exp pairwise loss; 3) a cascaded fine-tuning method that further improves prediction. Moreover, we build a benchmark dataset, called SCUT-FBP5500, containing 5,500 facial images with diverse properties (male/female, Asian/Caucasian, ages) and labels (face landmarks, rating scores within [1, 5], rating score distribution). Experiments were performed on both the SCUT-FBP and the SCUT-FBP5500 benchmark datasets, where our method achieves state-of-the-art performance on different evaluation settings. Comparisons with related CNN models highlight the effectiveness of the R $^3$ CNN architecture for FBP.},
  archive  = {J},
  author   = {Luojun Lin and Lingyu Liang and Lianwen Jin},
  doi      = {10.1109/TAFFC.2019.2933523},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {122-134},
  title    = {Regression guided by relative ranking using convolutional neural network (R$^3$3CNN) for facial beauty prediction},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). What an “ehm” leaks about you: Mapping fillers into
personality traits with quantum evolutionary feature selection
algorithms. <em>IEEE Transactions on Affective Computing</em>,
<em>13</em>(1), 108–121. (<a
href="https://doi.org/10.1109/TAFFC.2019.2930695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This work shows that fillers - short utterances like “ehm” and “uhm” - allow one to predict whether someone is above median along the Big-Five personality traits. The experiments have been performed over a corpus of 2,988 fillers uttered by 120 different speakers in spontaneous conversations. The results show that the prediction accuracies range between 74 and 82 percent depending on the particular trait. The proposed approach includes a feature selection step - based on Quantum Evolutionary Algorithms - that has been used to detect the personality markers, i.e., the subset of the features that better account for the prediction outcomes and, indirectly, for the personality of the speakers. The results show that only a relatively few features tend to be consistently selected, thus acting as reliable personality markers.},
  archive  = {J},
  author   = {Mohammad Tayarani and Anna Esposito and Alessandro Vinciarelli},
  doi      = {10.1109/TAFFC.2019.2930695},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {108-121},
  title    = {What an “Ehm” leaks about you: Mapping fillers into personality traits with quantum evolutionary feature selection algorithms},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Utilizing deep learning towards multi-modal bio-sensing and
vision-based affective computing. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(1), 96–107. (<a
href="https://doi.org/10.1109/TAFFC.2019.2916015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, the use of bio-sensing signals such as electroencephalogram (EEG), electrocardiogram (ECG), etc. have garnered interest towards applications in affective computing. The parallel trend of deep-learning has led to a huge leap in performance towards solving various vision-based research problems such as object detection. Yet, these advances in deep-learning have not adequately translated into bio-sensing research. This work applies novel deep-learning-based methods to various bio-sensing and video data of four publicly available multi-modal emotion datasets. For each dataset, we first individually evaluate the emotion-classification performance obtained by each modality. We then evaluate the performance obtained by fusing the features from these modalities. We show that our algorithms outperform the results reported by other studies for emotion/valence/arousal/liking classification on DEAP and MAHNOB-HCI datasets and set up benchmarks for the newer AMIGOS and DREAMER datasets. We also evaluate the performance of our algorithms by combining the datasets and by using transfer learning to show that the proposed method overcomes the inconsistencies between the datasets. Hence, we do a thorough analysis of multi-modal affective data from more than 120 subjects and 2,800 trials. Finally, utilizing a convolution-deconvolution network, we propose a new technique towards identifying salient brain regions corresponding to various affective states.},
  archive  = {J},
  author   = {Siddharth and Tzyy-Ping Jung and Terrence J. Sejnowski},
  doi      = {10.1109/TAFFC.2019.2916015},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {96-107},
  title    = {Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). First impressions: A survey on vision-based apparent
personality trait analysis. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(1), 75–95. (<a
href="https://doi.org/10.1109/TAFFC.2019.2930058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Personality analysis has been widely studied in psychology, neuropsychology, and signal processing fields, among others. From the past few years, it also became an attractive research area in visual computing. From the computational point of view, by far speech and text have been the most considered cues of information for analyzing personality. However, recently there has been an increasing interest from the computer vision community in analyzing personality from visual data. Recent computer vision approaches are able to accurately analyze human faces, body postures and behaviors, and use these information to infer apparent personality traits. Because of the overwhelming research interest in this topic, and of the potential impact that this sort of methods could have in society, we present in this paper an up-to-date review of existing vision-based approaches for apparent personality trait recognition. We describe seminal and cutting edge works on the subject, discussing and comparing their distinctive features and limitations. Future venues of research in the field are identified and discussed. Furthermore, aspects on the subjectivity in data labeling/evaluation, as well as current datasets and challenges organized to push the research on the field are reviewed.},
  archive  = {J},
  author   = {Julio C. S. Jacques Junior and Yağmur Güçlütürk and Marc Pérez and Umut Güçlü and Carlos Andujar and Xavier Baró and Hugo Jair Escalante and Isabelle Guyon and Marcel A. J. van Gerven and Rob van Lier and Sergio Escalera},
  doi      = {10.1109/TAFFC.2019.2930058},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {75-95},
  title    = {First impressions: A survey on vision-based apparent personality trait analysis},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel sentiment polarity detection framework for chinese.
<em>IEEE Transactions on Affective Computing</em>, <em>13</em>(1),
60–74. (<a href="https://doi.org/10.1109/TAFFC.2019.2932061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Nowadays, mining opinions or sentiment from online user-generated text has become a research hot spot. Although a large amount of lexicon-based Chinese polarity detection works have been done, the existing methods have one common flaw: that even the same word can have opposite polarities among different seed lexicons. This is known as polarity fuzziness. To enhance the performance of Chinese sentiment polarity detection, we start from a two-aspect lexicon expansion so that the polarity fuzziness can be avoided. Specifically, we detect sentiment polarity for new words and revise sentiment polarity for words already defined in seed lexicons. Then, we formulate a novel sentiment polarity detection framework for Chinese (SPDFC) with more attention to fine-grained sentiment processing, which is involved in symmetrical mapping, sentiment feature pruning and text representation. In this way, words’ polarity can be directly taken as features, penetrating further in the polarity detection phase. According to our experimental results, the proposed SPDFC framework can achieve the best overall performance from the perspective of Chinese polarity detection, sentiment feature pruning, and text representation compared to other classical and state-of-the-art methods.},
  archive  = {J},
  author   = {Tinghuai Ma and Huan Rong and Yongsheng Hao and Jie Cao and Yuan Tian and Mznah Al-Rodhaan},
  doi      = {10.1109/TAFFC.2019.2932061},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {60-74},
  title    = {A novel sentiment polarity detection framework for chinese},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal self-assessed personality estimation during
crowded mingle scenarios using wearables devices and cameras. <em>IEEE
Transactions on Affective Computing</em>, <em>13</em>(1), 46–59. (<a
href="https://doi.org/10.1109/TAFFC.2019.2930605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper focuses on the automatic classification of self-assessed personality traits from the HEXACO inventory during crowded mingle scenarios. These scenarios provide rich study cases for social behavior analysis but are also challenging to analyze automatically as people in them interact dynamically and freely in an in-the-wild face-to-face setting. To do so, we leverage the use of wearable sensors recording acceleration and proximity, and video from overhead cameras. We use 3 different behavioral modality types (movement, speech and proximity) coming from 2 sensors (wearable and camera). Unlike other works, we extract an individual’s speaking status from a single body worn triaxial accelerometer instead of audio, which scales easily to large populations. Additionally, we study the effect of different combinations of modality types on the personality estimation, and how this relates to the nature of each trait. We also include an analysis of feature complementarity and an evaluation of feature importance for the classification, showing that combining complementary modality types further improves the classification performance. We estimate the self-assessed personality traits both using a binary classification (community’s standard) and as a regression over the trait scores. Finally, we analyze the impact of the accuracy of the speech detection on the overall performance of the personality estimation.},
  archive  = {J},
  author   = {Laura Cabrera-Quiros and Ekin Gedik and Hayley Hung},
  doi      = {10.1109/TAFFC.2019.2930605},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {46-59},
  title    = {Multimodal self-assessed personality estimation during crowded mingle scenarios using wearables devices and cameras},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comprehensive and context-sensitive neonatal pain
assessment using computer vision. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(1), 28–45. (<a
href="https://doi.org/10.1109/TAFFC.2019.2926710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Infants receiving care in the Neonatal Intensive Care Unit (NICU) experience several painful procedures during their hospitalization. Assessing neonatal pain is difficult because the current standard for assessment is subjective, inconsistent, and discontinuous. The intermittent and inconsistent assessment can induce poor treatment and, therefore, cause serious long-term outcomes. In this paper, we present a comprehensive pain assessment system that utilizes facial expressions along with crying sounds, body movement, and vital sign changes. The proposed automatic system generates a standardized pain assessment comparable to those obtained by conventional nurse-derived pain scores. The system achieved 95.56 percent accuracy using decision fusion of different pain responses that were recorded in a challenging clinical environment. In addition to the decision fusion, we present the performance of multimodal assessment using other fusion schemes as well as a unimodal assessment approach. We also discuss the impact of different factors (e.g., gestational age) on pain, propose several group-specific models for pain assessment (e.g., pre-term and full-term models), and compare the performance of these models with the performance of general models. While further research is needed, our results show that the automatic assessment of neonatal pain is a viable and more efficient alternative to the manual assessment.},
  archive  = {J},
  author   = {Ghada Zamzmi and Chih-Yun Pai and Dmitry Goldgof and Rangachar Kasturi and Terri Ashmeade and Yu Sun},
  doi      = {10.1109/TAFFC.2019.2926710},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {28-45},
  title    = {A comprehensive and context-sensitive neonatal pain assessment using computer vision},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Affect estimation in 3D space using multi-task active
learning for regression. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(1), 16–27. (<a
href="https://doi.org/10.1109/TAFFC.2019.2916040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Acquisition of labeled training samples for affective computing is usually costly and time-consuming, as affects are intrinsically subjective, subtle and uncertain, and hence multiple human assessors are needed to evaluate each affective sample. Particularly, for affect estimation in the 3D space of valence, arousal and dominance, each assessor has to perform the evaluations in three dimensions, which makes the labeling problem even more challenging. Many sophisticated machine learning approaches have been proposed to reduce the data labeling requirement in various other domains, but so far few have considered affective computing. This paper proposes two multi-task active learning for regression approaches, which select the most beneficial samples to label, by considering the three affect primitives simultaneously. Experimental results on the VAM corpus demonstrated that our optimal sample selection approaches can result in better estimation performance than random selection and several traditional single-task active learning approaches. Thus, they can help alleviate the data labeling problem in affective computing, i.e., better estimation performance can be obtained from fewer labeling queries.},
  archive  = {J},
  author   = {Dongrui Wu and Jian Huang},
  doi      = {10.1109/TAFFC.2019.2916040},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {16-27},
  title    = {Affect estimation in 3D space using multi-task active learning for regression},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Classification of video game player experience using
consumer-grade electroencephalography. <em>IEEE Transactions on
Affective Computing</em>, <em>13</em>(1), 3–15. (<a
href="https://doi.org/10.1109/TAFFC.2020.2992437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A growing body of literature has emerged that demonstrates the potential of neurogaming platforms for inter- facing with well-known video games. With the recent convergence of advances in consumer electronics, ubiquitous computing, and wearable sensor technologies real-time monitoring of neurocognitive and affective states can be studied in an objective manner. Whilst establishing the optimal relation among frequency bands, task engagement, and arousal states is a goal of neurogaming, a standardized method has yet to be established. Herein we aimed to test classifiers within the same context, group of participants, feature extraction methods, and protocol. Given the emphasis upon neurogaming, a commercial-grade electroencephalographic (EEG; Emotiv EPOC) headset was used to collect signals from 30 participants. The EEG data was then filtered to get separate frequency bands to train cognitive-affective classifiers with three classification techniques: Support Vector Machines (SVM), Naive Bayes (NB), and k-Nearest Neighbors (kNN). Results revealed that the NB classifier was the most robust classifier for identifying negative (e.g., character death) game-based events. The identification of general gameplay events is best identified using kNN and the Beta band. Results from this study suggest that a combination of classifiers is preferable over selection of a single classifier.},
  archive  = {J},
  author   = {Thomas D. Parsons and Timothy McMahan and Ian Parberry},
  doi      = {10.1109/TAFFC.2020.2992437},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {3-15},
  title    = {Classification of video game player experience using consumer-grade electroencephalography},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Transactions on affective computing – another
year in the shade of covid-19. <em>IEEE Transactions on Affective
Computing</em>, <em>13</em>(1), 1–2. (<a
href="https://doi.org/10.1109/TAFFC.2022.3148681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Presents the introductory editorial for this issue of the publication.},
  archive  = {J},
  author   = {Elisabeth André},
  doi      = {10.1109/TAFFC.2022.3148681},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {1-2},
  title    = {Editorial: Transactions on affective computing – another year in the shade of covid-19},
  volume   = {13},
  year     = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
