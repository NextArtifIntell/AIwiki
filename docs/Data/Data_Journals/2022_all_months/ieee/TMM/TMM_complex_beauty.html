<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmm---350">TMM - 350</h2>
<ul>
<li><details>
<summary>
(2022a). Robust learning from noisy web images via data purification
for fine-grained recognition. <em>TMM</em>, <em>24</em>, 1198–1209. (<a
href="https://doi.org/10.1109/TMM.2021.3134156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manually labeling fine-grained datasetsis laborious and typically requires domain-specific expert knowledge. Conversely, a vast amount of web data is relatively easy to obtain with nearly no human effort. Therefore, learning from noisy web data for fine-grained tasks is attracting increasing attention in recent years. However, the presence of noise in web images is a huge obstacle for training robust fine-grained recognition models. To this end, we propose a novel approach to identify noisy images as well as specifically distinguish in- and out-of-distribution samples. It can purify the noisy web training set by discarding out-of-distribution noise and relabeling in-distribution noisy samples. Then we can train the model on the purified dataset to alleviate the harmful effects of noise and make the most of web images to achieve better performance. Extensive experiments on three commonly used fine-grained datasets demonstrate that our approach is far superior to current state-of-the-art web-supervised methods. The data and source code of this work have been made publicly available at: https://github.com/NUST-Machine-Intelligence-Laboratory/Dataset-Purification .},
  archive      = {J_TMM},
  author       = {Chuanyi Zhang and Qiong Wang and Guosen Xie and Qi Wu and Fumin Shen and Zhenmin Tang},
  doi          = {10.1109/TMM.2021.3134156},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1198-1209},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust learning from noisy web images via data purification for fine-grained recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning the global descriptor for 3-d object recognition
based on multiple views decomposition. <em>TMM</em>, <em>24</em>,
188–201. (<a href="https://doi.org/10.1109/TMM.2020.3047762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key point of view based strategies for the analysis of 3D object is to obtain a global descriptor from a collection of its rendered views on 2D images. The views are always redundantly sampled as to ensure the completeness of the information. In this paper, we bring new insight into the study of multi-view object recognition, which models an object as a View Mixture Model (VMM). We argue that each object represented by the multiple views can be decomposed into just a few latent views. Based on the VMM, we introduce a decomposition module to mine the representations of these latent views for the construction of a compact and comprehensive descriptor. After that, we further propose a view alignment module to ensure the descriptor is robust to the variation of view permutation. We evaluate our method on the ModelNet-40, ModelNet-10 and ShapeNetCore55 datasets. The experimental results show that our method can learn efficient and comprehensive representation for 3D objects, and achieves state-of-the-art performance on both the 3D object classification and retrieval tasks. Lastly, experiments are conducted for benchmarking various popular CNN backbones on the 3D object recognition task, with a view to achieving fair comparisons and promoting the future research in this area. Codes for our paper are released: “ https://github.com/hjjpku/multi_view_sort ”.},
  archive      = {J_TMM},
  author       = {Jingjia Huang and Wei Yan and Thomas Li and Shan Liu and Ge Li},
  doi          = {10.1109/TMM.2020.3047762},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {188-201},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning the global descriptor for 3-D object recognition based on multiple views decomposition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized zero-shot learning via multi-modal aggregated
posterior aligning neural network. <em>TMM</em>, <em>24</em>, 177–187.
(<a href="https://doi.org/10.1109/TMM.2020.3047546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visual-semantic gap between the visual space (visual features) and semantic space (semantic attributes) is one of the main problems in the Generalized Zero-Shot Learning (GZSL) task. The essence of this problem is that the structure of manifolds in these two spaces is inconsistent, which makes it difficult to learn embeddings that unify visual features and semantic attributes for similarity measurement. In this work, we tackle this problem by proposing a multi-modal aggregated posterior aligning neural network based on Wasserstein Auto-encoders (WAE) which learns a shared latent space for visual features and semantic attributes. The key to our approach is that the aggregated posterior distribution of the latent representations encoded from visual features of each class is encouraged to be aligned with a Gaussian distribution predicted by the corresponding semantic attribute in the latent space. On one hand, requiring the latent manifolds of visual features and semantic attributes to be consistent preserves the inter-class association between seen and unseen classes. On the other hand, the aggregated posterior of each class is directly defined as a Gaussian in the latent space, which provides a reliable way to synthesize latent features for training classification models. Using the AWA1, AWA2, CUB, aPY, FLO, and SUN benchmark datasets, we extensively conducted comparative evaluations to demonstrate the advantages of our method over state-of-the-art approaches.},
  archive      = {J_TMM},
  author       = {Xingyu Chen and Jin Li and Xuguang Lan and Nanning Zheng},
  doi          = {10.1109/TMM.2020.3047546},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {177-187},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Generalized zero-shot learning via multi-modal aggregated posterior aligning neural network},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Differentiable spatial regression: A novel method for 3D
hand pose estimation. <em>TMM</em>, <em>24</em>, 166–176. (<a
href="https://doi.org/10.1109/TMM.2020.3047552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Hand pose estimation from a single depth image is an essential topic in computer vision and human-computer interaction. Although the rising of deep learning boosts the accuracy a lot, the problem is still hard to solve due to the complex structure of the human hand. Two existing types of methods with deep learning, i.e. the regression-based and detection-based methods, either lose spatial information of the hand structure or lack direct supervision of the joint coordinates. In this paper, we propose a novel Differentiable Spatial Regression method which combines the advantages of these two types of methods to overcome each other&amp;#x0027;s shortcomings. Our method uses spatial-form representation (SFR) to maintain spatial information and differentiable decoder to establish a direct supervision. Following the procedure suggested by our method, a particular model named SRNet is designed which uses a combination of 2D heatmaps and local offset maps as SFRs. Two modules named Plane Regression and Depth Regression are designed as differentiable decoder to regress plane coordinates and depth coordinates respectively. Ablation study demonstrates the superiority of our method over the two combined methods since the differentiable decoder leads to better SFRs learned by the network itself other than human design. Extensive experiments on four public datasets demonstrate that SRNet is comparable with the state-of-the-art models.},
  archive      = {J_TMM},
  author       = {Xingyuan Zhang and Fuhai Zhang},
  doi          = {10.1109/TMM.2020.3047552},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {166-176},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Differentiable spatial regression: A novel method for 3D hand pose estimation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low bitrate light field compression with geometry and
content consistency. <em>TMM</em>, <em>24</em>, 152–165. (<a
href="https://doi.org/10.1109/TMM.2020.3046860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field imaging can simultaneously record the position and direction information of light rays; thus, digital refocusing and full depth-of-field extension &amp;#x2014; functions that are inaccessible for conventional images &amp;#x2014; can be achieved using the structural consistency of light field data. To meet the challenges of limited bandwidth and storage, such vast numbers of light field data must be compressed to a low bitrate. However, current compression solutions ignore the intrinsic consistency of light fields in pursuit of a low bitrate, thereby leading to the loss of light field capabilities. To solve this issue, this work focuses on structural consistency to achieve efficient light field compression with a low bitrate. The proposed light field compression method encodes the sparsely selected sub-aperture images (SAIs) and the disparity maps corresponding to the unselected SAIs. From the perspective of geometry consistency, the consistency of the initially estimated disparity maps is improved by using a color-guided refinement algorithm, thereby reducing the bitrate of the disparity maps. From the perspective of content consistency, the consistency of the SAI-transformed pseudo sequence is improved by the proposed content-similarity-based arrangement algorithm along with a specific prediction structure; thereby, the bitrate of the sparsely selected SAIs is reduced. The experimental results show that the proposed compression method can reduce the total bitrate while preserving good structural consistency.},
  archive      = {J_TMM},
  author       = {Xinpeng Huang and Ping An and Yilei Chen and Deyang Liu and Liquan Shen},
  doi          = {10.1109/TMM.2020.3046860},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {152-165},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Low bitrate light field compression with geometry and content consistency},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Amorphous region context modeling for scene recognition.
<em>TMM</em>, <em>24</em>, 141–151. (<a
href="https://doi.org/10.1109/TMM.2020.3046877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene images are usually composed of foreground and background regional contents. Some existing methods propose to extract regional contents with dense grids or objectness region proposals. However, dense grids may split the object into several discrete parts, learning semantic ambiguity for the patches. The objectness methods may focus on particular objects but only pay attention to the foreground contents and do not exploit the background that is key to scene recognition. In contrast, we propose a novel scene recognition framework with amorphous region detection and context modeling. In the proposed framework, discriminative regions are first detected with amorphous contours that can tightly surround the targets through semantic segmentation techniques. In addition, both foreground and background regions are jointly embedded to obtain the scene representations with the graph model. Based on the graph modeling module, we explore the contextual relations between the regions in geometric and morphology aspects, and generate the discriminative representations for scene recognition. Experimental results on MIT67 and SUN397 demonstrate the effectiveness and generality of the proposed method.},
  archive      = {J_TMM},
  author       = {Haitao Zeng and Xinhang Song and Gongwei Chen and Shuqiang Jiang},
  doi          = {10.1109/TMM.2020.3046877},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {141-151},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Amorphous region context modeling for scene recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep domain adaptation based multi-spectral salient object
detection. <em>TMM</em>, <em>24</em>, 128–140. (<a
href="https://doi.org/10.1109/TMM.2020.3046868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient Object Detection (SOD) plays an important role in many image-related multimedia applications. Although there are many existing research works about the salient object detection in traditional RGB (visible-light spectrum) images, there are still many complex situations that regular RGB images cannot provide enough cues for the accurate SOD, such as the shadow effect, similar appearance between background and foreground, strong or insufficient illumination, etc. Because of the success of near-infrared spectrum in many computer vision tasks, we explore the multi-spectral SOD in the synchronized RGB images and near-infrared (NIR) images for the both simple and complex situations. We assume that the RGB SOD in the existing RGB image datasets could provide references for the multi-spectral SOD problem. In this paper, we mainly model this research problem as a deep learning based domain adaptation from the traditional RGB image data (source domain) to the multi-spectral data (target domain), and an adversarial deep domain adaptation model is proposed. We first collect and will publicize a large multi-spectral dataset, RGBN-SOD dataset, including 780 synchronized RGB and NIR image pairs for the multi-spectral SOD problem in the simple and complex situations. Intensive experimental results show the effectiveness and accuracy of the proposed deep domain adaptation for the multi-spectral SOD. Besides, due to the absence of research on the field of multi-spectral co-saliency detection, we also collect 200 synchronized RGB and NIR image pairs in addition to explore the multi-spectral co-saliency detection.},
  archive      = {J_TMM},
  author       = {Shaoyue Song and Zhenjiang Miao and Hongkai Yu and Jianwu Fang and Kang Zheng and Cong Ma and Song Wang},
  doi          = {10.1109/TMM.2020.3046868},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {128-140},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep domain adaptation based multi-spectral salient object detection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attribute restoration framework for anomaly detection.
<em>TMM</em>, <em>24</em>, 116–127. (<a
href="https://doi.org/10.1109/TMM.2020.3046884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the recent advances in deep neural networks, anomaly detection in multimedia has received much attention in the computer vision community. While reconstruction-based methods have recently shown great promise for anomaly detection, the information equivalence among input and supervision for reconstruction tasks can not effectively force the network to learn semantic feature embeddings. We here propose to break this equivalence by erasing selected attributes from the original data and reformulate it as a restoration task, where the normal and the anomalous data are expected to be distinguishable based on restoration errors. Through forcing the network to restore the original image, the semantic feature embeddings related to the erased attributes are learned by the network. During testing phases, because anomalous data are restored with the attribute learned from the normal data, the restoration error is expected to be large. Extensive experiments have demonstrated that the proposed method significantly outperforms several state-of-the-arts on multiple benchmark datasets, especially on ImageNet, increasing the AUROC of the top-performing baseline by 10.1%. We also evaluate our method on a real-world anomaly detection dataset MVTec AD.},
  archive      = {J_TMM},
  author       = {Fei Ye and Chaoqin Huang and Jinkun Cao and Maosen Li and Ya Zhang and Cewu Lu},
  doi          = {10.1109/TMM.2020.3046884},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {116-127},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Attribute restoration framework for anomaly detection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Focus your attention: A focal attention for multimodal
learning. <em>TMM</em>, <em>24</em>, 103–115. (<a
href="https://doi.org/10.1109/TMM.2020.3046855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key point in multimodal learning is to learn semantic alignment that finds the correspondence between sub-elements of instances from different modality data. Attention mechanism has shown its power in semantic alignment learning as it enables to densely associate sub-elements across different modalities. However, for each sub-element, existing attention aligns it with all the sub-elements from another modality, while most of them have no correspondence with it, i.e. irrelevant sub-elements. The irrelevant sub-elements will distract the semantic alignment if they are also attended. In this paper, we propose a novel focal attention mechanism to learn more accurate semantic alignment. The focal attention sparsely attends to a subset of sub-elements, which are identified as relevant ones according to their posterior probabilities given each sub-element from another modality. Based on the observation that relevant sub-elements mostly describe the same semantic, the posterior probability can precisely distinguish relevant and irrelevant ones by taking interactions within the same modality into consideration, such that relevant sub-elements get higher and closer posterior probabilities, while irrelevant ones get lower probabilities. Such a design learns better semantic alignment by preventing the interference of irrelevant sub-elements, and it facilitates subsequent multimodal tasks that demand semantic alignment. To validate the effectiveness of the focal attention, we conduct extensive experiments on image-text matching and text-to-image generation, and we propose a bidirectional and stacked version of focal attention for them, respectively. Experimental results on benchmarks show that the focal attention can significantly and consistently outperform state-of-the-arts.},
  archive      = {J_TMM},
  author       = {Chunxiao Liu and Zhendong Mao and Tianzhu Zhang and An-An Liu and Bin Wang and Yongdong Zhang},
  doi          = {10.1109/TMM.2020.3046855},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {103-115},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Focus your attention: A focal attention for multimodal learning},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Building and using personal knowledge graph to improve
suicidal ideation detection on social media. <em>TMM</em>, <em>24</em>,
87–102. (<a href="https://doi.org/10.1109/TMM.2020.3046867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large number of individuals are suffering from suicidal ideation in the world. There are a number of causes behind why an individual might suffer from suicidal ideation. As the most popular platform for self-expression, emotion release, and personal interaction, individuals may exhibit a number of symptoms of suicidal ideation on social media. Nevertheless, challenges from both data and knowledge aspects remain as obstacles, constraining the social media-based detection performance. Data implicitness and sparsity make it difficult to discover the inner true intentions of individuals based on their posts. Inspired by psychological studies, we build and unify a high-level suicide-oriented knowledge graph with deep neural networks for suicidal ideation detection on social media. We further design a two-layered attention mechanism to explicitly reason and establish key risk factors to individual&amp;#x0027;s suicidal ideation. The performance study on microblog and Reddit shows that: 1) with the constructed personal knowledge graph, the social media-based suicidal ideation detection can achieve over 93&amp;#x0025; accuracy; and 2) among the six categories of personal factors, post, personality, and experience are the top-3 key indicators. Under these categories, posted text, stress level, stress duration, posted image, and ruminant thinking contribute to one&amp;#x0027;s suicidal ideation detection.},
  archive      = {J_TMM},
  author       = {Lei Cao and Huijun Zhang and Ling Feng},
  doi          = {10.1109/TMM.2020.3046867},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {87-102},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Building and using personal knowledge graph to improve suicidal ideation detection on social media},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deeper look at image salient object detection: Bi-stream
network with a small training dataset. <em>TMM</em>, <em>24</em>, 73–86.
(<a href="https://doi.org/10.1109/TMM.2020.3046871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with the conventional hand-crafted approaches, the deep learning based ISOD (image salient object detection) models have achieved tremendous performance improvements by training exquisitely crafted fancy networks over large-scale training sets. However, do we really need large-scale training set for ISOD? In this article, we provide a deeper insight into the interrelationship between the ISOD performance and the training data. To alleviate the conventional demands for large-scale training data, we provide a feasible way to construct a novel small-scale training set, which only contains 4 K images. To take full advantage of this new set, we propose a novel bi-stream network consisting of two different feature backbones. Benefit from the proposed gate control unit, this bi-stream network is able to achieve complementary fusion status for its subbranches. To our best knowledge, this is the first attempt to use a small-scale training set to compete with other large-scale ones; nevertheless, our method can still achieve the leading SOTA performance on all tested benchmark datasets. Both the code and dataset are publicly available at https://github.com/wuzhenyubuaa/TSNet.},
  archive      = {J_TMM},
  author       = {Zhenyu Wu and Shuai Li and Chenglizhao Chen and Aimin Hao and Hong Qin},
  doi          = {10.1109/TMM.2020.3046871},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {73-86},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deeper look at image salient object detection: Bi-stream network with a small training dataset},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Raw image deblurring. <em>TMM</em>, <em>24</em>, 61–72. (<a
href="https://doi.org/10.1109/TMM.2020.3045303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based blind image deblurring plays an essential role in solving image blur since all existing kernels are limited in modeling the real world blur. Thus far, researchers focus on powerful models to handle the deblurring problem and achieve decent results. For this work, in a new aspect, we discover the great opportunity for image enhancement (e.g., deblurring) directly from RAW images and investigate novel neural network structures benefiting RAW-based learning. However, to the best of our knowledge, there is no available RAW image deblurring dataset. Therefore, we built a new dataset containing both RAW images and processed sRGB images and design a new model to utilize the unique characteristics of RAW images. The proposed deblurring model, trained solely from RAW images, achieves the state-of-art performance and outweighs those trained on processed sRGB images. Furthermore, with fine-tuning, the proposed model, trained on our new dataset, can generalize to other sensors. Additionally, by a series of experiments, we demonstrate that existing deblurring models can also be improved by training on the RAW images in our new dataset. Ultimately, we show a new venue for further opportunities based on the devised novel raw-based deblurring method and the brand-new Deblur-RAW dataset.},
  archive      = {J_TMM},
  author       = {Chih-Hung Liang and Yu-An Chen and Yueh-Cheng Liu and Winston H. Hsu},
  doi          = {10.1109/TMM.2020.3045303},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {61-72},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Raw image deblurring},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correlation graph convolutional network for pedestrian
attribute recognition. <em>TMM</em>, <em>24</em>, 49–60. (<a
href="https://doi.org/10.1109/TMM.2020.3045286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pedestrian attribute recognition aims at generating the structured description of pedestrian, which plays an important role in surveillance. However, it is difficult to achieve accurate recognition results due to diverse illumination, partial body occlusion and limited resolutions. Therefore, this paper proposes a comprehensive relationship framework for comprehensively describing and utilizing relations among attributes, describing different type of relations in the same dimension, and implementing complex transfers of relations in a GCN manner. This framework is named Correlation Graph Convolutional Network (CGCN). Based on the proposed framework, the feature vectors are built to associate attributes with image features and generate different relation matrices through self-attention among different feature vectors, describing different attribute relations. Then, we conduct multi-layer transfer of attribute relations by means of graph convolution, realizing complex utilization of attribute relations. In addition, the relations among attributes are fully exploited and two types of relations, namely the explicit and implicit relations, are proposed to be integrate into the proposed comprehensive relationship framework. The experimental results on RAP and PETA demonstrate that the recognition performance of the proposed CGCN can obviously outperform the state-of-the-arts, and moreover, the CGCN can achieve a better synergy with different relations.},
  archive      = {J_TMM},
  author       = {Haonan Fan and Hai-Miao Hu and Shuailing Liu and Weiqing Lu and Shiliang Pu},
  doi          = {10.1109/TMM.2020.3045286},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {49-60},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Correlation graph convolutional network for pedestrian attribute recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LensCast: Robust wireless video transmission over MmWave
MIMO with lens antenna array. <em>TMM</em>, <em>24</em>, 33–48. (<a
href="https://doi.org/10.1109/TMM.2020.3045294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present LensCast, a novel cross-layer video transmission framework for wireless networks, which seamlessly integrates millimeter wave (mmWave) lens multiple-input multiple-output (MIMO) with robust video transmission. LensCast is designed to exploit the video content diversity at the application layer, together with the spatial path diversity of lens antenna array at the physical layer, to achieve graceful video transmission performance under varying channel conditions. In LensCast, a transmission distortion minimization problem is formulated with the consideration of video chunk scheduling, path matching and power allocation, which is an intractable mixed integer non-linear programming (MINLP) problem. The solution of this MINLP problem is converted into resource allocation (i.e., joint path matching and power allocation) plus chunk scheduling. First, resource allocation is investigated with given chunk scheduling results. By analyzing the optimality of the resource allocation problem, a winner-takes-all assignment is obtained to guide resource allocation. After that, a greedy water-filling algorithm is proposed as a near-optimal solution. Second, we propose a low-complexity chunk scheduling algorithm to schedule chunks for each transmission. Simulation results demonstrate that the proposed LensCast achieves an improved performance in terms of both peak signal-to-noise ratio and visual quality comparing with reference schemes.},
  archive      = {J_TMM},
  author       = {Yongqiang Gui and Hancheng Lu and Feng Wu and Chang Wen Chen},
  doi          = {10.1109/TMM.2020.3045294},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {33-48},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {LensCast: Robust wireless video transmission over MmWave MIMO with lens antenna array},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint contrast enhancement and noise reduction of low light
images via JND transform. <em>TMM</em>, <em>24</em>, 17–32. (<a
href="https://doi.org/10.1109/TMM.2020.3043106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low light images suffer from a low dynamic range and severe noise due to low signal-to-noise ratio (SNR). In this paper, we propose joint contrast enhancement and noise reduction of low light images via just-noticeable-difference (JND) transform. We adopt the JND transform to achieve both contrast enhancement and noise reduction based on human visual perception. First, we generate a JND map based on an the human visual system (HVS) response model from foreground and background luminance, called JND transform. Second, for base image, we perform perceptual contrast enhancement based on luminance adaptation to effectively allocate a dynamic range to each gray level while preventing under enhancement (tone distortion) and over-enhancement. Third, we refine the JND map using Weber&#39;s law, luminance adaptation and visual masking. Weber&#39;s law enhances the JND map based on the luminance variation after contrast enhancement. Luminance adaptation suppresses noise for smooth regions, while visual masking enforces detail enhancement for textural regions. Fourth, we perform inverse JND transform to generate the enhanced luma channel from the JND map and base image. Finally, we conduct chroma denoising by transferring texture information of the enhanced luma channel to the chroma channels with guided filtering. Experimental results show that the proposed method achieves both contrast enhancement and noise reduction for low light images as well as outperforms state-of-the-art methods in terms of quantitative measurements.},
  archive      = {J_TMM},
  author       = {Haonan Su and Long Yu and Cheolkon Jung},
  doi          = {10.1109/TMM.2020.3043106},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {17-32},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Joint contrast enhancement and noise reduction of low light images via JND transform},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quality assessment for omnidirectional video: A
spatio-temporal distortion modeling approach. <em>TMM</em>, <em>24</em>,
1–16. (<a href="https://doi.org/10.1109/TMM.2020.3044458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Omnidirectional video, also known as 360-degree video, has become increasingly popular nowadays due to its ability to provide immersive and interactive visual experiences. However, the ultra high resolution and the spherical observation space brought by the large spherical viewing range make omnidirectional video distinctly different from traditional 2D video. To date, the video quality assessment (VQA) for omnidirectional video is still an open issue. The existing VQA metrics for omnidirectional video only consider the spatial characteristics of distortions, but the temporal change of spatial distortions can also considerably influence human visual perception. In this paper, we propose a spatiotemporal modeling approach to evaluate the quality of the omnidirectional video. Firstly, we construct a spatioral quality assessment unit to evaluate the average distortion in temporal dimension at the eye fixation level, based upon which the smoothed distortion value is recursively calculated and consolidated by the characteristics of temporal variations. Then, we give a detailed solution of how to to integrate the three existing spatial VQA metrics into our approach. Besides, the cross-format omnidirectional video distortion measurement is also investigated. Finally, the spatiotemporal distortion of the whole video sequence is obtained by pooling. Based on the modeling approach, a full reference objective quality assessment metric for omnidirectional video is derived, namely OV-PSNR. The experimental results show that our proposed OV-PSNR greatly improves the prediction performance of the existing VQA metrics for omnidirectional video.},
  archive      = {J_TMM},
  author       = {Pan Gao and Pengwei Zhang and Aljosa Smolic},
  doi          = {10.1109/TMM.2020.3044458},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Quality assessment for omnidirectional video: A spatio-temporal distortion modeling approach},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distribution-preserving-based automatic data augmentation
for deep image steganalysis. <em>TMM</em>, <em>24</em>, 4538–4550. (<a
href="https://doi.org/10.1109/TMM.2021.3119994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning-based steganalyzers far outperformed handcrafted feature-based steganalyzers. However, a large amount of data is needed to train deep learning networks. For steganalysis tasks, the steganographic traces are subtle and the steganographic signals are difficult to be captured when the number of cover/stego pairs in the training set is insufficient. Data augmentation has been proved to be effective in improving accuracy and generalization for deep learning models. Yet not all data augmentation methods are universal for all tasks. When performing data augmentation, we argue that data distribution under the target tasks should be maintained. Since the steganalysis task is more concerned with the high-frequency signals of the images, if the high-frequency signals are unchanged, the data distribution from the perspective of steganalysis will remain largely unchanged. Based on this principle, we designed a neural network called cover augmentation network, which enriches the dataset by intelligently adding noise to the original cover to generate the augmented cover. Further, we designed a whole process of data augmentation based on the cover augmentation network. Experimental results show that the proposed data augmentation method can effectively improve the performance of steganalysis networks, and the advantage is significant at low payloads.},
  archive      = {J_TMM},
  author       = {Jiansong Zhang and Kejiang Chen and Chuan Qin and Weiming Zhang and Nenghai Yu},
  doi          = {10.1109/TMM.2021.3119994},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {4538-4550},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Distribution-preserving-based automatic data augmentation for deep image steganalysis},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Zero-shot learning based on quality-verifying adversarial
network. <em>TMM</em>, <em>24</em>, 4526–4537. (<a
href="https://doi.org/10.1109/TMM.2021.3119854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, generative adversarial network (GAN)-based zero-shot learning methods have attracted widespread attention. However, due to the randomness of GAN generation, most existing methods cannot well guarantee to generate sufficiently reliable features and have good generalization ability. Targeting at these problems, we propose an effective Quality-Verifying Adversarial Network (QVAN) that consists of one generator and double discriminators. Adversarial learning between the former discriminator and generator is to generate visual features, which can be partitioned into pseudo-generated features and reliable-generated features. The latter discriminator is used for quality-verifying that will guide the generator to generate more reliable features that are near the real visual features. To avoid over-fitting and ensure intra-class diversity, we set the threshold for each class to distinguish pseudo-generated features and reliable-generated features. To further preserve both compactness and discriminability of the samples, we introduce the class metric constraint, which are more conducive to classification. Moreover, we introduce $\ell _{1,2}$ -norm constraint to fully consider the specific distribution among different classes, thus making the generated features more discriminant. Extensive experiments on several real-world datasets show the effectiveness of the proposed approach, which demonstrate the advantage over the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Siyang Deng and Gang Xiang and Quanxue Gao and Wei Xia and Xinbo Gao},
  doi          = {10.1109/TMM.2021.3119854},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {4526-4537},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Zero-shot learning based on quality-verifying adversarial network},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Haptic signal reconstruction for cross-modal communications.
<em>TMM</em>, <em>24</em>, 4514–4525. (<a
href="https://doi.org/10.1109/TMM.2021.3119860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging multi-modal services, characterized as the integration of audio, visual, and haptic signals, will become the killer applications in 5 G and beyond 5 G era. In order to support multi-modal services, cross-modal communications come into being. However, when adopting cross-modal communications to haptic-dominant multi-modal services, there still face several technical challenges. On the one hand, haptic signals are very sensitive to interference and easy to be damaged or even missing during transmission. On the other hand, it needs to generate virtual haptic signals when real touch sensory information is hard to be gathered. To get over the dilemma, this paper proposes a haptic signal reconstruction strategy for cross-modal communications. First, a cloud-edge collaboration-based cross-modal communication architecture is constructed. Then, an audio-visual-aided haptic signal reconstruction (AVHR) approach under this architecture is designed by leveraging the potential correlation among modalities. It can be further divided into three components: feature extraction by cloud-edge transfer, shared semantic learning by multi-modal fusion, and haptic signal generation by semantic constraints. Finally, experiments on a standard audio-visual-haptic dataset and a practical cross-modal communication platform show that the proposed AVHR approach has better reconstruction performance when compared with the competing schemes.},
  archive      = {J_TMM},
  author       = {Xin Wei and Yingying Shi and Liang Zhou},
  doi          = {10.1109/TMM.2021.3119860},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {4514-4525},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Haptic signal reconstruction for cross-modal communications},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Motion estimation and coding structure for inter-prediction
of LiDAR point cloud geometry. <em>TMM</em>, <em>24</em>, 4504–4513. (<a
href="https://doi.org/10.1109/TMM.2021.3119872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate two fundamental problems of inter-prediction for Light Detection and Ranging (LiDAR) point cloud geometries: motion estimation (ME) and coding structure under the inter-exploration model of geometry-based point cloud compression (G-PCC). Under the inter-exploration model of G-PCC, the key to a good ME algorithm is to design an accurate criterion for estimating the bit cost of an octree node. In the previous work, a logarithmic relationship between the prediction distortion and the bit cost was used as the criterion. We first note that the multiscale binary prediction residue, instead of the prediction distortion, is the key factor in determining the bit cost. Then, a linear relationship between the number of 1s and 0s in the multiscale binary prediction residue and the bit cost is built and used as the ME criterion. In terms of the coding structure, only the IPPP coding structure is investigated in all previous geometry inter-prediction algorithms. The use of the hierarchical coding structure is first investigated in this paper. We further propose determining the use of the IPPP or hierarchical coding structure at the group of pictures (GoP)-level based on rate distortion optimization to improve the performance. The proposed algorithms are implemented in the inter-exploration model of G-PCC. The experimental results show that compared with the inter-exploration model of G-PCC, the proposed algorithms can provide an average of 2.1% bitrate savings.},
  archive      = {J_TMM},
  author       = {Li Li and Zhu Li and Shan Liu and Houqiang Li},
  doi          = {10.1109/TMM.2021.3119872},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {4504-4513},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Motion estimation and coding structure for inter-prediction of LiDAR point cloud geometry},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). View-invariant human action recognition via view
transformation network (VTN). <em>TMM</em>, <em>24</em>, 4493–4503. (<a
href="https://doi.org/10.1109/TMM.2021.3119177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the human body is non-rigid, actions captured in different views always involve action occlusion and information loss. Recently, view-variation-related human action recognition is still a challenging problem. To address the problem, we propose a View Transformation Network (VTN) that realizes the view normalization by transforming arbitrary-view action samples to a base view to seek for a view-invariant representation. an attention learning module is designed to learn a co-attention for action samples of different views, that contributes to output a similar feature representation to erase the view diversity in different views. Extensive and fair evaluations are performed on the UESTC varying-view RGB-D dataset, the NTU RGB-D 60 dataset, and the NTU RGB-D 120 dataset, where three evaluation types, i.e. X-subject, X-view, and A-view recognition, are performed. Experiments illustrate that our VTN model achieves outstanding performance.},
  archive      = {J_TMM},
  author       = {Lingling Gao and Yanli Ji and Kumie Gedamu and Xiaofeng Zhu and Xing Xu and Heng Tao Shen},
  doi          = {10.1109/TMM.2021.3119177},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {4493-4503},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {View-invariant human action recognition via view transformation network (VTN)},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Apparel-invariant feature learning for person
re-identification. <em>TMM</em>, <em>24</em>, 4482–4492. (<a
href="https://doi.org/10.1109/TMM.2021.3119133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of deep learning methods, person Re-Identification (ReID) performance has been improved tremendously in many public datasets. However, most public ReID datasets are collected in a short time window in which persons’ appearance rarely changes. In real-world applications such as in a shopping mall, the same person may change their wearings, and different persons may wear similar apparel. It reveals a critical problem that current ReID models heavily rely on a person’s apparel, resulting in an inconsistent ReID performance. Therefore, it is crucial to learn an apparel-invariant person representation under clothes changing or several persons wearing similar clothes cases. In this work, we tackle this problem from the viewpoint of invariant feature representation learning. The main contributions of this work are as follows. (1) We propose the semi-supervised Apparel-invariant Feature Learning (AIFL) framework to learn an apparel-invariant pedestrian representation using images of the same person wearing different clothes. (2) To obtain images of the same person wearing different clothes, we propose an unsupervised apparel-simulation GAN (AS-GAN) to synthesize cloth-changing images according to the target cloth embedding. It is worth noting that the images used in ReID tasks were cropped from real-world low-quality CCTV videos, making it more challenging to synthesize cloth-changing images. Extensive experiments demonstrate that our proposal can improve the ReID performance of the baseline models.},
  archive      = {J_TMM},
  author       = {Zhengxu Yu and Yilun Zhao and Bin Hong and Zhongming Jin and Jianqiang Huang and Deng Cai and Xian-Sheng Hua},
  doi          = {10.1109/TMM.2021.3119133},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {4482-4492},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Apparel-invariant feature learning for person re-identification},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). I-GCN: Incremental graph convolution network for
conversation emotion detection. <em>TMM</em>, <em>24</em>, 4471–4481.
(<a href="https://doi.org/10.1109/TMM.2021.3118881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis and emotion detection in conversation are becoming hot topics in regard to several applications. With the development of the social robot, social network, and intelligent voice assistant, emotion detection is attracting more attention as a key component in these research fields. Many approaches have been proposed to handle this problem in recent years. However, these previous approaches focus on either the temporal change information of the conversation or the semantic correlation information of the dialogue but ignore the combination of temporal information and semantic correlation information. In this paper, we propose an incremental graph convolution network (I-GCN) to handle emotion detection in conversation. We first utilize the graph structure to represent conversation at different times, which can represent the semantic correlation information of utterances. Then, we apply the incremental graph structure to imitate the process of dynamic conversation, which can preserve the temporal change information of conversation. Especially, for the first step of the process, we creatively propose utterance-level GCN (U-GCN) and speaker-level GCN (S-GCN) to learn the features of utterances for emotion detection. U-GCN focuses on the correlations among utterances and applies the multi-head attention model to find latent correlation information among utterances, which aims to further enhance the guidance of semantic relevance for feature learning. S-GCN focuses on the correlation between speaker and utterances, which can provide a different angle to guide the feature learning of utterances. In the learning of model parameters, we constantly utilize the new utterances to fine-tune the parameters of GNN for enhancement of the contribution of temporal change information. Detailed evaluations of the proposed method on three published conversation corpuses demonstrate the great effectiveness of our approach over several conventional competitive baselines.},
  archive      = {J_TMM},
  author       = {Weizhi Nie and Rihao Chang and Minjie Ren and Yuting Su and Anan Liu},
  doi          = {10.1109/TMM.2021.3118881},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {4471-4481},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {I-GCN: Incremental graph convolution network for conversation emotion detection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A commonality modeling framework for enhanced video coding
leveraging on the cuboidal partitioning based representation of frames.
<em>TMM</em>, <em>24</em>, 4446–4457. (<a
href="https://doi.org/10.1109/TMM.2021.3117397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video coding algorithms attempt to minimize the significant commonality that exists within a video sequence. Each new video coding standard contains tools that can perform this task more efficiently compared to its predecessors. Modern video coding systems are block-based wherein commonality modeling is carried out only from the perspective of the block that need be coded next. In this work, we argue for a commonality modeling approach that can provide a seamless blending between global and local homogeneity information. For this purpose, at first the frame that need be coded, is recursively partitioned into rectangular regions based on the homogeneity information of the entire frame. After that each obtained rectangular region’s feature descriptor is taken to be the average value of all the pixels’ intensities encompassing the region. In this way, the proposed approach generates a coarse representation of the current frame by minimizing both global and local commonality. This coarse frame is computationally simple and has a compact representation. It attempts to preserve important structural properties of the current frame which can be viewed subjectively as well as from improved rate-distortion performance of a reference scalable HEVC coder that employs the coarse frame as a reference frame for encoding the current frame.},
  archive      = {J_TMM},
  author       = {Ashek Ahmmed and Manzur Murshed and Manoranjan Paul and David Taubman},
  doi          = {10.1109/TMM.2021.3117397},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {4446-4457},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A commonality modeling framework for enhanced video coding leveraging on the cuboidal partitioning based representation of frames},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph-based multimodal sequential embedding for sign
language translation. <em>TMM</em>, <em>24</em>, 4433–4445. (<a
href="https://doi.org/10.1109/TMM.2021.3117124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign language translation (SLT) is a challenging weakly supervised task without word-level annotations. An effective method of SLT is to leverage multimodal complementarity and to explore implicit temporal cues. In this work, we propose a graph-based multimodal sequential embedding network (MSeqGraph), in which multiple sequential modalities are densely correlated. Specifically, we build a graph structure to realize the intra-modal and inter-modal correlations. First, we design a graph embedding unit (GEU), which embeds a parallel convolution with channel-wise and temporal-wise learning into the graph convolution to learn the temporal cues in each modal sequence and cross-modal complementarity. Then, a hierarchical GEU stacker with a pooling-based skip connection is proposed. Unlike the state-of-the-art methods, to obtain a compact and informative representation of multimodal sequences, the GEU stacker gradually compresses the channel $d$ with multi-modalities $m$ rather than the temporal dimension $t$ . Finally, we adopt the connectionist temporal decoding strategy to explore the entire video’s temporal transition and translate the sentence. Extensive experiments on the USTC-CSL and BOSTON-104 datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_TMM},
  author       = {Shengeng Tang and Dan Guo and Richang Hong and Meng Wang},
  doi          = {10.1109/TMM.2021.3117124},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {4433-4445},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Graph-based multimodal sequential embedding for sign language translation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LR-GCN: Latent relation-aware graph convolutional network
for conversational emotion recognition. <em>TMM</em>, <em>24</em>,
4422–4432. (<a href="https://doi.org/10.1109/TMM.2021.3117062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an intersection of artificial intelligence and human communication analysis, Emotion Recognition in Conversation (ERC) has attracted much research attention in recent years. Existing studies, however, are limited in adequately exploiting latent relations among the constituent utterances. In this paper, we address this issue by proposing a novel approach named Latent Relation-Aware Graph Convolutional Network (LR-GCN), where both speaker dependency of the interlocutors is leveraged and latent correlations among the utterances are captured for ERC. Specifically, we first establish a graph model to incorporate the context information and speaker dependency of the conversation. Afterward, the multi-head attention mechanism is introduced to explore the latent correlations among the utterances and generate a set of all-linked graphs. Here, aiming to simultaneously exploit the original modeled speaker dependency and the explored correlation information, we introduce a dense connection layer to capture more structural information of the generated graphs. Through a multi-branch graph network, we achieve a unified representation of each utterance for final prediction. Detailed evaluations on two benchmark datasets demonstrate LR-GCN outperforms the state-of-the-art approaches.},
  archive      = {J_TMM},
  author       = {Minjie Ren and Xiangdong Huang and Wenhui Li and Dan Song and Weizhi Nie},
  doi          = {10.1109/TMM.2021.3117062},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {4422-4432},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {LR-GCN: Latent relation-aware graph convolutional network for conversational emotion recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhancing mixture-of-experts by leveraging attention for
fine-grained recognition. <em>TMM</em>, <em>24</em>, 4409–4421. (<a
href="https://doi.org/10.1109/TMM.2021.3117064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differentiating subcategories of a common visual category is challenging because of the similar appearance shared among different classes in fine-grained recognition. Existing mixture-of-expert based methods divide the fine-grained space into some specific regions and solve the integrated problem by conquering subspace ones. However, it is not feasible to learn diverse experts directly through data partition strategy because of limited data available for fine-grained recognition problems. To address the issue, we leverage visual attention to learn an enhanced experts’ mixture. Specifically, we introduce a gradually-enhanced learning strategy from model attention. The strategy promotes diversity among experts by feeding each expert with full-size data distinct in granularity. We further promote expert’s learning by providing it with a larger data space, which is achieved by swapping attentive regions within positive pairs. Our method learns new experts on the dataset with the prior knowledge from former experts sequentially and enforces the experts to learn more diverse but discriminative representation. These enhanced experts are finally combined to make stronger predictions. We conduct extensive experiments on fine-grained benchmarks. The results show that our method consistently outperforms the state-of-the-art method in both weakly supervised localization and fine-grained image classification. Our code is publicly available at https://github.com/lbzhang/Enhanced-Expert-FGVC-Pytorch.git .},
  archive      = {J_TMM},
  author       = {Lianbo Zhang and Shaoli Huang and Wei Liu},
  doi          = {10.1109/TMM.2021.3117064},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {4409-4421},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhancing mixture-of-experts by leveraging attention for fine-grained recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GeoPose: Dense reconstruction guided 6D object pose
estimation with geometric consistency. <em>TMM</em>, <em>24</em>,
4394–4408. (<a href="https://doi.org/10.1109/TMM.2021.3117092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {6D object pose estimation for texture-less objects from RGB images remains challenging, especially in occlusion scenarios. Instead of localizing sparse keypoints by regressing their image coordinates or heatmaps, which are sensitive to occlusion, we introduce GeoPose, a novel reconstruction guided pose estimation pipeline that predicts dense correspondences and leverages geometric consistency effectively. We first design a dense reconstruction network (ReconNet) to reconstruct pixel-wise object coordinates in normalization space. Dense 2D-3D correspondences are generated intuitively by our explicit parameterization for 3D object models, which dismisses keypoint selection efforts. These 2D-3D correspondences are then utilized to estimate 6D poses by the PnP algorithm with RANSAC iterations. Furthermore, a novel Cycle Loss is proposed to provide 3D prior supervision, which significantly correlates with the pose estimation task by guiding geometric consistency between reconstruction (pixel to 3D) and reprojection (3D to pixel). In addition, a training data augmentation method is proposed to handle the insufficiency of 6D datasets, the acquisition of which is error-prone and time-consuming. Extensive experiments demonstrate that, compared with existing RGB-based methods, our GeoPose can achieve state-of-the-art (SOTA) 6D pose estimation performance on the LINEMOD, Occlusion LINEMOD and T-LESS datasets.},
  archive      = {J_TMM},
  author       = {Deming Wang and Guangliang Zhou and Yi Yan and Huiyi Chen and Qijun Chen},
  doi          = {10.1109/TMM.2021.3117092},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {4394-4408},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {GeoPose: Dense reconstruction guided 6D object pose estimation with geometric consistency},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CroMM-VSR: Cross-modal memory augmented visual speech
recognition. <em>TMM</em>, <em>24</em>, 4342–4355. (<a
href="https://doi.org/10.1109/TMM.2021.3115626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Speech Recognition (VSR) is a task that recognizes speech from external appearances of the face ( ${\it i}.{\it e}.$ , lips) into text. Since the information from the visual lip movements is not sufficient to fully represent the speech, VSR is considered as one of the challenging problems. One possible way to resolve this problem is additionally utilizing audio which contains rich information for speech recognition. However, the audio information could not be always available such as in crowded situations. Thus, it is necessary to find a way that successfully provides enough information for speech recognition with visual inputs only. In this paper, we alleviate the information insufficiency of visual lip movement by proposing a cross-modal memory augmented VSR with Visual-Audio Memory (VAM). The proposed framework tries to utilize the complementary information of audio even when the audio inputs are not provided at the inference time. Concretely, the proposed VAM learns to imprint audio features of short clip-level into a memory network using the corresponding visual features. To this end, the VAM contains two memories, lip-video key and audio value. We guide the audio value memory to imprint the audio feature and the lip-video key memory to memorize the location of the imprinted audio. By doing this, the VAM can exploit rich audio information by accessing the memory using visual inputs only. Experimental results show that the proposed method achieves state-of-the-art performance on both word- and sentence-level VSR. In addition, we verify the learned representations inside the VAM contain meaningful information for VSR.},
  archive      = {J_TMM},
  author       = {Minsu Kim and Joanna Hong and Se Jin Park and Yong Man Ro},
  doi          = {10.1109/TMM.2021.3115626},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {4342-4355},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CroMM-VSR: Cross-modal memory augmented visual speech recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TC-net: Detecting noisy labels via transform consistency.
<em>TMM</em>, <em>24</em>, 4328–4341. (<a
href="https://doi.org/10.1109/TMM.2021.3115635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is crucial to distinguish mislabeled samples for dealing with noisy labels. Previous methods such as “Co-teaching” and “JoCoR” introduce two different networks to select clean samples out of the noisy ones and only use these clean samples to train the deep models. Different from these methods which require to train two networks simultaneously, we propose a simple and effective method to identify clean samples only using one single network. We discover that the clean samples prefer to reach consistent predictions for the original images and the transformed images while noisy samples usually suffer from inconsistent predictions. Motivated by this observation, we propose a noisy label detection approach, named Transform Consistency Network (TC-Net), which constrains the transform consistency ( i.e., category consistency and visual attention consistency) between the original images and the transformed images for network training. Then we can select small-loss samples to update the parameters of the network. Furthermore, in order to mitigate the negative influence of noisy labels, we design a classification loss by using the off-line hard labels and on-line soft labels to provide more reliable supervisions for training a robust model. We conduct comprehensive experiments on CIFAR-10, CIFAR-100 and Clothing1M datasets. Compared with the clean sample selection baselines, we achieve the state-of-the-art performance. Especially, in most cases, our proposed method outperforms the baselines by a large margin.},
  archive      = {J_TMM},
  author       = {Rumeng Yi and Yaping Huang},
  doi          = {10.1109/TMM.2021.3115635},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {4328-4341},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TC-net: Detecting noisy labels via transform consistency},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A 3D mesh-based lifting-and-projection network for human
pose transfer. <em>TMM</em>, <em>24</em>, 4314–4327. (<a
href="https://doi.org/10.1109/TMM.2021.3115628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose transfer has typically been modeled as a 2D image-to-image translation problem. This formulation ignores the human body shape prior in 3D space and inevitably causes implausible artifacts, especially when facing occlusion. To address this issue, we propose a lifting-and-projection framework to perform pose transfer in the 3D mesh space. The core of our framework is a foreground generation module, that consists of two novel networks: a lifting-and-projection network (LPNet) and an appearance detail compensating network (ADCNet). To leverage the human body shape prior, LPNet exploits the topological information of the body mesh to learn an expressive visual representation for the target person in the 3D mesh space. To preserve texture details, ADCNet is further introduced to enhance the feature produced by LPNet with the source foreground image. Such design of the foreground generation module enables the model to better handle difficult cases such as those with occlusions. Experiments on the iPER and Fashion datasets empirically demonstrate that the proposed lifting-and-projection framework is effective and outperforms the existing image-to-image-based and mesh-based methods on human pose transfer task in both self-transfer and cross-transfer settings.},
  archive      = {J_TMM},
  author       = {Jinxiang Liu and Yangheng Zhao and Siheng Chen and Ya Zhang},
  doi          = {10.1109/TMM.2021.3115628},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {4314-4327},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A 3D mesh-based lifting-and-projection network for human pose transfer},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). WAFP-net: Weighted attention fusion based progressive
residual learning for depth map super-resolution. <em>TMM</em>,
<em>24</em>, 4113–4127. (<a
href="https://doi.org/10.1109/TMM.2021.3118282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the remarkable progresses achieved in depth map super-resolution (DSR), it remains a major challenge to tackle with real-world degradation of low-resolution (LR) depth maps. Synthetic datasets are mainly used in existing DSR approaches, which is quite different from what would get from a real depth sensor. Besides, the enhancements of features in existing DSR approaches are not sufficiently enough, which also limit the performance. To alleviate these problems, we first propose two types of degradation models to describe the generation of LR depth maps, including bi-cubic down-sampling with noise and interval down-sampling, and different DSR models are learned correspondingly. Then, we propose a weighted attention fusion strategy that is embedded into a progressive residual learning framework, which guarantees that the high-resolution (HR) depth maps can be well recovered in a coarse-to-fine manner. The weighted attention fusion strategy can enhance the features with abundant high-frequency components in both global and local manners, thus better HR depth maps can be expected. Besides, to re-use the effective information in the progressive process sufficiently, a multi-stage fusion module is combined into the proposed framework, and the Total Generalized Variation (TGV) regularization and input loss are exploited to further improve the performance of our method. Extensive experiments of different benchmarks demonstrate the superiority of our approach over the state-of-the-art (SOTA) approaches.},
  archive      = {J_TMM},
  author       = {Xibin Song and Dingfu Zhou and Wei Li and Yuchao Dai and Liu Liu and Hongdong Li and Ruigang Yang and Liangjun Zhang},
  doi          = {10.1109/TMM.2021.3118282},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {4113-4127},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {WAFP-net: Weighted attention fusion based progressive residual learning for depth map super-resolution},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic training data dropout for robust deep face
recognition. <em>TMM</em>, <em>24</em>, 1186–1197. (<a
href="https://doi.org/10.1109/TMM.2021.3123478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning with noise is a practically challenging problem in deep face recognition. Despite the success of large margin softmax loss functions, these methods are designed for clean face databases. Considering the inevitable noise in the large scale databases, we first analyze the performance of noise in the training databases. For noise-robust deep face recognition, we propose a dynamic training data dropout (DTDD) method to dynamically filter the noise in the training database and gradually form a stable refined database for model learning. Specifically, we leverage the information provided by the model predictions of accumulated training epochs, which can distinguish regular samples and noise effectively and accurately. The proposed DTDD method is easy and stable for implementation, and can be combined with existing state-of-the-art loss functions and network architectures. Extensive experiments on CASIA-WebFace, VGGFace2, and MS-Celeb-1 M databases empirically demonstrate that our proposed method can robustly train deep face recognition models in the presence of label noise and low quality images.},
  archive      = {J_TMM},
  author       = {Yaoyao Zhong and Weihong Deng and Han Fang and Jiani Hu and Dongyue Zhao and Xian Li and Dongchao Wen},
  doi          = {10.1109/TMM.2021.3123478},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {1186-1197},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dynamic training data dropout for robust deep face recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning from web recipe-image pairs for food recognition:
Problem, baselines and performance. <em>TMM</em>, <em>24</em>,
1175–1185. (<a href="https://doi.org/10.1109/TMM.2021.3123474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal recipe retrieval has recently been explored for food recognition and understanding. Text-rich recipe provides not only visual content information (e.g., ingredients, dish presentation) but also procedure of food preparation (cutting and cooking styles). The paired data is leveraged to train deep models to retrieve recipes for food images. Most recipes on the Web include sample pictures as the references. The paired multimedia data is not noise-free, due to errors such as pairing of images containing partially prepared dishes with recipes. The content of recipes and food images are not always consistent due to free-style writing and preparation of food in different environments. As a consequence, the effectiveness of learning cross-modal deep models from such noisy web data is questionable. This paper conducts an empirical study to provide insights whether the features learnt with noisy pair data are resilient and could capture the modality correspondence between visual and text.},
  archive      = {J_TMM},
  author       = {Bin Zhu and Chong-Wah Ngo and Wing-Kwong Chan},
  doi          = {10.1109/TMM.2021.3123474},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {1175-1185},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning from web recipe-image pairs for food recognition: Problem, baselines and performance},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Annealing genetic GAN for imbalanced web data learning.
<em>TMM</em>, <em>24</em>, 1164–1174. (<a
href="https://doi.org/10.1109/TMM.2021.3120642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is one of the most basic and important problems of web data. The key to overcoming the class imbalance problems is to increase the effective instances of the minority, that is, data augmentation. Generative Adversarial Networks (GANs), which have recently been successfully applied in the field of image generation, can be used for data augmentation because they can learn the data distribution given ample training data instances and generate more data. However, learning the distributions from the imbalanced data can make GANs easily get stuck in a local optimum. In this work, we propose a new training strategy called Annealing Genetic GAN (AGGAN), which incorporates simulated annealing genetic algorithm into the training process of GANs. And this can help GANs avoid the local optimum trapping problem, which easily occurs when the training set is imbalanced. Unlike existing GANs, which use a fixed adversarial learning objective alternately training a generator, we use multiple adversarial learning objectives to train a set of generators and use the Metropolis criterion in simulated annealing to decide whether the generator should update. More specifically, the Metropolis criterion accepts worse solutions with a certain probability, so it can make our AGGAN escape from the local optimum and find a better solution. Theory and mathematical analysis provide strong theoretical support for the proposed training strategy. And experiments on several datasets demonstrate that AGGAN achieves convincing ability to solve the class imbalanced problem and reduces the training problems inherent in existing GANs.},
  archive      = {J_TMM},
  author       = {Jingyu Hao and Chengjia Wang and Guang Yang and Zhifan Gao and Jinglin Zhang and Heye Zhang},
  doi          = {10.1109/TMM.2021.3120642},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {1164-1174},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Annealing genetic GAN for imbalanced web data learning},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to recognize human actions from noisy skeleton data
via noise adaptation. <em>TMM</em>, <em>24</em>, 1152–1163. (<a
href="https://doi.org/10.1109/TMM.2021.3120631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have made great progress on skeleton-based action recognition. However, most of them are developed with relatively clean skeletons without the presence of intensive noise. We argue that the models learned from relatively clean data are not well generalizable to handle noisy skeletons commonly appeared in the real world. In this paper, we address the challenge of recognizing human actions from noisy skeletons, which is seldom explored by previous methods. Beyond exploring the new problem, we further take a new perspective to address it, i.e. , noise adaptation, which gets rid of explicit skeleton noise modeling and reliance on skeleton ground truths. Specifically, we develop regression-based and generation-based adaptation models according to whether pairs of noisy skeletons are available. The regression-based model aims to learn noise-suppressed intrinsic feature representations by mapping pairs of noisy skeletons into a noise-robust space. When only unpaired skeletons are accessible, the generation-based model aims to adapt the features from noisy skeletons to a low-noise space by adversarial learning. To verify our proposed model and facilitate research on noisy skeletons, we collect a new dataset Noisy Skeleton Dataset (NSD), the skeletons of which are with much noise and more similar to daily-life data than previous datasets. Extensive experiments are conducted on the NSD, VV-RGBD and N-UCLA datasets, and results consistently show the outstanding performance of our proposed model.},
  archive      = {J_TMM},
  author       = {Sijie Song and Jiaying Liu and Lilang Lin and Zongming Guo},
  doi          = {10.1109/TMM.2021.3120631},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {1152-1163},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning to recognize human actions from noisy skeleton data via noise adaptation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regularized two granularity loss function for weakly
supervised video moment retrieval. <em>TMM</em>, <em>24</em>, 1141–1151.
(<a href="https://doi.org/10.1109/TMM.2021.3120545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised video moment retrieval or weakly supervised language moment retrieval aims to search the most relevant moment given a language query. In order to guide the model to capture the most matching video segments with the text description, we design a two-granularity loss function that simultaneously considers both video-level and instance-level relationships. Specifically, we first generate coarse video segments and regard each video segment as an instance. For video-level regularized multiple instance loss (MIL), we leverage the latent alignment between all intra-video segments ( ie. , positive bag) and text descriptions. Then, we classify these segments by regarding this procedure as a supervised learning task under noisy labels. With the instance-level regularized loss function, our model can learn to correct noisy instance-level labels so as to locate the more accurate frame boundary from all the positive instances. Comprehensive experimental results on ActivityNet and DiDeMo demonstrate that the proposed loss function sets a new state-of-the-art.},
  archive      = {J_TMM},
  author       = {Junya Teng and Xiankai Lu and Yongshun Gong and Xinfang Liu and Xiushan Nie and Yilong Yin},
  doi          = {10.1109/TMM.2021.3120545},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {1141-1151},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Regularized two granularity loss function for weakly supervised video moment retrieval},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ensemble learning with manifold-based data splitting for
noisy label correction. <em>TMM</em>, <em>24</em>, 1127–1140. (<a
href="https://doi.org/10.1109/TMM.2021.3119861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label noise in training data can significantly degrade a model’s generalization performance for supervised learning tasks. Here we focus on the problem that noisy labels are primarily caused by mislabeled confusing samples, which tend to be concentrated near decision boundaries rather than uniformly distributed, and whose features should be equivocal. To address the problem, we propose an ensemble learning method to correct noisy labels by exploiting the local structures of feature manifolds. Different from typical ensemble strategies that increase the prediction diversity among sub-models via certain loss terms, our method trains sub-models on disjoint subsets, each being a union of randomly selected seed samples’ nearest-neighbors of the same class on the data manifold. As a result, only a limited number of sub-models will be affected by locally-concentrated noisy labels, and each sub-model can learn a coarse representation of the data manifold along with a corresponding graph. The constructed graphs are used to suggest a set of label correction candidates, and accordingly, our method determines label correction results by majority decisions. Our experiments on real-world noisy label datasets demonstrate the superiority of the proposed method over existing state-of-the-arts.},
  archive      = {J_TMM},
  author       = {Hao-Chiang Shao and Hsin-Chieh Wang and Weng-Tai Su and Chia-Wen Lin},
  doi          = {10.1109/TMM.2021.3119861},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {1127-1140},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Ensemble learning with manifold-based data splitting for noisy label correction},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep co-image-label hashing for multi-label image retrieval.
<em>TMM</em>, <em>24</em>, 1116–1126. (<a
href="https://doi.org/10.1109/TMM.2021.3119868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep supervised hashing has greatly improved retrieval performance with the powerful learning capability of deep neural network. In multi-label image retrieval, existing deep hashing simply indicates whether two images are similar by constructing a similarity matrix. However, it ignores the dependency among multiple labels that has been shown important in multi-label application. To fulfill this gap, this paper proposes Deep Co-Image-Label Hashing (DCILH) to discover label dependency. Specifically, DCILH regards image and label as two views, and maps the two views into a common deep Hamming space. DCILH proposes to learn prototype for each label, and preserve similarity among images, labels, and prototypes. To exploit label dependency, DCILH further employs the label-correlation aware loss on the predicted labels, such that predicted output on positive label is enforced to be larger than that on negative label. Extensive experiments on several multi-label benchmarks demonstrate the proposed DCILH outperforms state-of-the-art deep supervised hashing on large-scale multi-label image retrieval.},
  archive      = {J_TMM},
  author       = {Xiaobo Shen and Guohua Dong and Yuhui Zheng and Long Lan and Ivor W. Tsang and Quan-Sen Sun},
  doi          = {10.1109/TMM.2021.3119868},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {1116-1126},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep co-image-label hashing for multi-label image retrieval},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting web images for fine-grained visual recognition
via dynamic loss correction and global sample selection. <em>TMM</em>,
<em>24</em>, 1105–1115. (<a
href="https://doi.org/10.1109/TMM.2021.3118216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To distinguish subtle differences among fine-grained categories, a large amount of well-labeled images are typically required. However, acquiring manual annotations for fine-grained categories is an extremely difficult task as it usually has a high demand for professional knowledge. To this end, directly leveraging web images for learning fine-grained models becomes a natural choice. Nevertheless, due to the existence of label noise, this learning paradigm tends to have a poor performance. In this work, we propose an end-to-end approach by combining dynamic loss correction and global sample selection to alleviate the problem of label noise. Specifically, we leverage the network to predict all samples, record the predictions of recent several epochs, and calculate the uncertainly-based dynamic loss for global sample selection. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our proposed approach. The source code of our approach has been released on the website: https://github.com/NUST-Machine-Intelligence-Laboratory/dlc .},
  archive      = {J_TMM},
  author       = {Huafeng Liu and Haofeng Zhang and Jianfeng Lu and Zhenmin Tang},
  doi          = {10.1109/TMM.2021.3118216},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {1105-1115},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Exploiting web images for fine-grained visual recognition via dynamic loss correction and global sample selection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LR-SVM+: Learning using privileged information with noisy
labels. <em>TMM</em>, <em>24</em>, 1080–1092. (<a
href="https://doi.org/10.1109/TMM.2021.3116417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paradigm of Learning Using Privileged Information (LUPI) always assumes that labels are annotated precisely. However, in practice, this assumption may be violated, as the labels may be heavily noisy, which inevitably degenerates the performance of learning algorithms in the LUPI paradigm. To handle the side effect of noisy labels, we propose a novel Label Noise Robust SVM+ (LR-SVM+) algorithm. Specifically, as the privileged information contains rich information of the latent labels, we first utilize it to infer underlying clean labels. Then we use the inference to modify the noisy labels. Comprehensive experiments demonstrate the necessity of studying label noise robust SVM+ and the effectiveness of the proposed method.},
  archive      = {J_TMM},
  author       = {Zhengning Wu and Xiaobo Xia and Ruxin Wang and Jiatong Li and Jun Yu and Yinian Mao and Tongliang Liu},
  doi          = {10.1109/TMM.2021.3116417},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {1080-1092},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {LR-SVM+: Learning using privileged information with noisy labels},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust audio patch attacks using physical sample simulation
and adversarial patch noise generation. <em>TMM</em>, <em>24</em>,
4381–4393. (<a href="https://doi.org/10.1109/TMM.2021.3116426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network (DNNs) based Automatic Speech Recognition (ASR) systems are known vulnerable to adversarial attacks that are maliciously implemented by adding small but powerful distortions to the original audio input. However, most existing methods that generate audio adversarial examples targeting ASR models cannot achieve successful robust attacks against defense methods. This paper proposes a novel framework for robust audio patch attacks using Physical Sample Simulation (PSS) and Adversarial Patch Noise Generation (APNG). First, the proposed PSS simulated real-audio with selected room impulse response for training the adversarial patches. Second, the proposed APNG generates the imperceptible audio adversarial patch examples using the voice activity detector to hide the adversarial patch noise into the non-silent locations of the input audio. Furthermore, the design Sounds Pressure Level-based adaptive noise minimization algorithm helps us further reduce the perturbation during the attack. The experimental results show that our proposed method can achieve the highest attack success rates and SNRs in various cases, comparing with other state-of-the-art attacks.},
  archive      = {J_TMM},
  author       = {Xia Du and Chi-Man Pun},
  doi          = {10.1109/TMM.2021.3116426},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4381-4393},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust audio patch attacks using physical sample simulation and adversarial patch noise generation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep continual learning for emerging emotion recognition.
<em>TMM</em>, <em>24</em>, 4367–4380. (<a
href="https://doi.org/10.1109/TMM.2021.3116434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding an unknown facial emotion that emerges in the future underpins significant impacts in various domains. Knowing the fact that emotional states grow in vocabulary, new emotional states need to be adapted while the existing knowledge of known emotional states is preserved. While human beings spontaneously perform this task, the challenge is, how to devise a deep learning technique that can effectively recognize an unknown emotion category in the future. Although the deep convolutional neural network has shown excellent emotion recognition performances in the past, it is conventionally a predefined multi-way classifier showing little resilience towards adding a new emotion class. Considering the aforementioned challenge, in this paper, we propose a generic deep convolutional neural network-based architecture that constantly absorbs the upcoming emotion categories and recognizes them effectively. We further propose an indicator loss, which is associated with the distillation mechanism that preserves the existing knowledge. In order to demonstrate the feasibility of our proposed method, we evaluated our model using benchmark emotion datasets. The results confirm that the proposed approach is superior in recognizing unknown emotional states compared to continual learning benchmarks. Further, our proposed method demonstrates higher accuracy, compared to the transfer learning baselines.},
  archive      = {J_TMM},
  author       = {Selvarajah Thuseethan and Sutharshan Rajasegarar and John Yearwood},
  doi          = {10.1109/TMM.2021.3116434},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4367-4380},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep continual learning for emerging emotion recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Knowledge-driven generative adversarial network for
text-to-image synthesis. <em>TMM</em>, <em>24</em>, 4356–4366. (<a
href="https://doi.org/10.1109/TMM.2021.3116416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-Image (T2I) synthesis is a challenging task that aims to convert natural language descriptions to real images. It remains an open problem mainly due to the diversity of text descriptions, which poses a huge obstacle in generating vivid and relevant images. Moreover, the existing evaluation metrics in T2I synthesis are mainly used to evaluate the visual quality of the generated images, while the semantic consistency between the two modalities is often ignored. To address these issues, we present a novel Knowledge-Driven Generative Adversarial Network , termed KD-GAN, and a new evaluation system, named Pseudo Turing Test (PTT for short). Concretely, KD-GAN takes a further step in imitating the behavior of human painting, i.e. , drawing an image according to reference knowledge. The introduction of reference knowledge in KD-GAN not only improves the quality of the generated images but also enhances the semantic consistency between them and the input texts. In addition, KD-GAN can also greatly avoid some flaws against common sense during image generation, e.g. , skiing in the blue sky. The proposed PTT is an important supplement to the existing evaluation system of T2I synthesis. It includes a set of pseudo-experts of different multimedia tasks to evaluate the semantic consistency between the given texts and the generated images. To validate the proposed KD-GAN, we conducted extensive experiments on two benchmark datasets, i.e. , Caltech-UCSD Birds (CUB), and MS-COCO (COCO). The experimental results demonstrate that KD-GAN outperforms state-of-the-art methods on IS, FID, and the proposed PTT metrics. 1 1The codes of KD-GAN are at [Online]. Available: https://github.com/pengjunn/KD-GAN and the codes and models of PTT are at [Online]. Available: https://github.com/pengjunn/PTT.},
  archive      = {J_TMM},
  author       = {Jun Peng and Yiyi Zhou and Xiaoshuai Sun and Liujuan Cao and Yongjian Wu and Feiyue Huang and Rongrong Ji},
  doi          = {10.1109/TMM.2021.3116416},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4356-4366},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Knowledge-driven generative adversarial network for text-to-image synthesis},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Underwater image enhancement with lightweight cascaded
network. <em>TMM</em>, <em>24</em>, 4301–4313. (<a
href="https://doi.org/10.1109/TMM.2021.3115442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to light scatter and absorption in waterbody, underwater imaging can be easily impaired with low contrast and visual distortion. The resulting images are often unable to meet the quality requirements of human perception and computer processing. Therefore, Underwater Image Enhancement (UIE) has been attracting extensive research efforts. Although deep learning has demonstrated its great success in many vision tasks, its huge amounts of parameters and computations are not conducive to UIE in resource-limited scenarios. In this paper, we address this issue by proposing a Lightweight Cascaded Network (LCNet) based on Laplacian image pyramids. At each pyramid level, we implement cascaded blocks upon a residual network. Specifically, high quality residuals can be progressively predicted with significantly reduced complexity in a coarse-to-fine fashion. Furthermore, these sub-networks are recursively nested to build our LCNet, thereby reducing the overall computational complexity with reused parameters. Extensive experiments demonstrate that the proposed method performs favorably against the state-of-the-arts in terms of visual quality, model parameters and complexity.},
  archive      = {J_TMM},
  author       = {Nanfeng Jiang and Weiling Chen and Yuting Lin and Tiesong Zhao and Chia-Wen Lin},
  doi          = {10.1109/TMM.2021.3115442},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4301-4313},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Underwater image enhancement with lightweight cascaded network},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep shape-aware person re-identification for overcoming
moderate clothing changes. <em>TMM</em>, <em>24</em>, 4285–4300. (<a
href="https://doi.org/10.1109/TMM.2021.3114539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although person re-identification (person re-id) has advanced substantially in recent years, most methods are based on the assumption that the identities would not change clothes. This assumption might not hold in practice considering criminals who intentionally change clothes. In this work, we attempt to solve person re-id under moderate clothing change. Since the human body shape is considered as relatively more invariant under moderate clothing changes, we propose to learn a reliable shape-aware feature representation by mutually learning both colorful images and contour images. Instead of directly extracting shape features from contour images, we utilize contour feature learning as regularization and excavate more effective shape-aware feature representations from colorful images. We propose a multi-scale appearance and contour deep infomax (MAC-DIM) to maximize mutual information between colorful appearance features and contour shape features, and in this way, the extracted appearance features are constrained to be shape-aware in terms of both low-level visual properties and high-level semantics. To better model the long-range human body shape and explicitly capture contour segment relations, we introduce hierarchical graph modeling as aggregation headers, propagating structural context through graph convolutional networks (GCNs). The extensive results on benchmarks under clothing changes demonstrate the effectiveness of our shape-aware feature learning scheme.},
  archive      = {J_TMM},
  author       = {Jiaxing Chen and Wei-Shi Zheng and Qize Yang and Jingke Meng and Richang Hong and Qi Tian},
  doi          = {10.1109/TMM.2021.3114539},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4285-4300},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep shape-aware person re-identification for overcoming moderate clothing changes},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Boundary information progressive guidance network for
salient object detection. <em>TMM</em>, <em>24</em>, 4236–4249. (<a
href="https://doi.org/10.1109/TMM.2021.3115344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the use of boundary information in saliency detection has been receiving increasing attention. In some cases, existing methods can output saliency maps with clear object boundaries by learning boundary information. However, their boundary prediction structures are generally separated from the prediction branches of the salient regions, and the resulting boundary features may not match the salient objects. We propose a simple saliency detection unit (SDU) to learn more accurate boundary features, and apply multiple such units to construct a boundary information progressive guidance network (BIPGNet). The SDU cascades the salient region and boundary detections, where the boundary features are directly extracted from the salient regions. In the BIPGNet, semantic and boundary features are progressively merged to produce complementary features. We use the complementary features of each stage in one SDU for detecting the salient objects. In addition, a novel boundary information guidance (BIG) module is designed that focuses on the boundary information in a feature layer. We apply multiple BIG modules to the complementary features at different stages. The quality of output saliency map is improved by modifying the complementary features. Experimental results demonstrate that our method can achieve better performance on five benchmark datasets, consistently surpassing 15 state-of-the-art methods. Our source code is publicly available at https://github.com/CKYiu/BIPG .},
  archive      = {J_TMM},
  author       = {Zhaojian Yao and Luping Wang},
  doi          = {10.1109/TMM.2021.3115344},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4236-4249},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Boundary information progressive guidance network for salient object detection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heterogeneous contrastive learning: Encoding spatial
information for compact visual representations. <em>TMM</em>,
<em>24</em>, 4224–4235. (<a
href="https://doi.org/10.1109/TMM.2021.3115335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised pretraining is of great significance for visual representation. Especially, contrastive learning has achieved great success recently, but existing approaches mostly ignored spatial information which is often crucial for visual representation. Strong semantic embedding has an inherent advantage for classification, but dense prediction tasks require more spatial and low-level representation. This paper presents heterogeneous contrastive learning (HCL), an effective approach that adds spatial information to the encoding stage to alleviate the learning inconsistency between the contrastive objective and strong data augmentation operations. We demonstrate the effectiveness of HCL by showing that (i) it achieves higher accuracy in instance discrimination, (ii) it surpasses existing pre-training methods in a series of downstream tasks (iii) and it shrinks the pre-training costs by half for almost 800 GPU-hours. More importantly, we show that our approach achieves higher efficiency in visual representations, and thus delivers a key message to inspire the future research of self-supervised visual representation learning.},
  archive      = {J_TMM},
  author       = {Xinyue Huo and Lingxi Xie and Longhui Wei and Xiaopeng Zhang and Xin Chen and Hao Li and Zijie Yang and Wengang Zhou and Houqiang Li and Qi Tian},
  doi          = {10.1109/TMM.2021.3115335},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4224-4235},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Heterogeneous contrastive learning: Encoding spatial information for compact visual representations},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TANet: Target attention network for video bit-depth
enhancement. <em>TMM</em>, <em>24</em>, 4212–4223. (<a
href="https://doi.org/10.1109/TMM.2021.3115039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video bit-depth enhancement (VBDE) reconstructs high-bit-depth (HBD) frames from a low-bit-depth (LBD) video sequence. As neighboring frames contain a considerable amount of complementary information related to the center frame, it is vital for VBDE to exploit neighboring frames as much as possible. Conventional VBDE algorithms with explicit alignment across frames attempt to warp each neighboring frame to the center frame with estimated optical flow, taking into account only pairwise correlation. Most spatiotemporal fusion approaches involve direct concatenation or 3D convolution and treat all features equally, failing to focus on information related to the center frame. Therefore, in this paper, we introduce an improved nonlocal block as a global attentive alignment (GAA) module, which takes the whole input video sequence into consideration to capture features that are globally correlated, to perform implicit alignment. Furthermore, given the bulk of features extracted from the center and neighboring frames, we propose target-guided attention (TGA). TGA can exploit more center-frame-related details and facilitate feature fusion. The proposed network (dubbed TANet) is capable of effectively eliminating false contours and recovering the center frame in high quality, as demonstrated by the experimental results. TANet outperforms state-of-the-art models in terms of both PSNR and SSIM with low time consumption.},
  archive      = {J_TMM},
  author       = {Jing Liu and Ziwen Yang and Yuting Su and Xiaokang Yang},
  doi          = {10.1109/TMM.2021.3115039},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4212-4223},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TANet: Target attention network for video bit-depth enhancement},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel rank learning based no-reference image quality
assessment method. <em>TMM</em>, <em>24</em>, 4197–4211. (<a
href="https://doi.org/10.1109/TMM.2021.3114551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, applying deep learning to no-reference image quality assessment (NR-IQA) has received significant attention. Especially in the last five years, an increasing interest has been drawn to the studies of rank learning since it can help mitigate the problem of small IQA datasets. However, on one hand, existing rank learning is not suitable for the authentically distorted images due to the lack of generated rank samples. On the other hand, the output of existing rank loss functions is uncontrollable, resulting in reduced performance. Motivated by these two limitations, we propose a novel rank learning based NR-IQA method, termed controllable list-wise ranking IQA (CLRIQA) in this paper. To be specific, we first present an imaging-heuristic approach, in which the over- and under-exposure is formulated as an inverse of the Weber-Fechner law, and fusion strategy and compression are adopted, to simulate the authentic distortion and generate the rank image samples. These samples are label-free yet associated with quality ranking information. Then we design a controllable list-wise ranking (CLR) loss function by setting an upper and lower bound of rank range and introducing an adaptive margin to tune rank interval. Finally, both the generated rank samples and proposed CLR are used to pre-train a convolutional neural network. Moreover, to obtain a more accurate prediction model, we take advantage of the IQA datasets to fine-tune the pre-trained network further. Various experiments are conducted on the IQA benchmark datasets, and experimental results demonstrate the effectiveness of the proposed CLRIQA method. The source code and network model can be downloaded at the following web address: https://github.com $/$ GZHU-DVL $/$ CLRIQA .},
  archive      = {J_TMM},
  author       = {Fu-Zhao Ou and Yuan-Gen Wang and Jin Li and Guopu Zhu and Sam Kwong},
  doi          = {10.1109/TMM.2021.3114551},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4197-4211},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A novel rank learning based no-reference image quality assessment method},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bridging the gap between semantic segmentation and instance
segmentation. <em>TMM</em>, <em>24</em>, 4183–4196. (<a
href="https://doi.org/10.1109/TMM.2021.3114541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained instance segmentation is considerably more complicated and challenging than semantic segmentation. Most existing instance segmentation methods only focus on accuracy without paying much attention to inference latency, which, is critical to real-time applications, such as autonomous driving. In this paper, we aim to bridge the gap between semantic segmentation and instance segmentation by presenting a novel real-time model for instance segmentation, Sem2Ins, which effectively generates instance boundaries according to a semantic segmentation by leveraging conditional generative adversarial networks (cGANs) coupled with deep supervision and a weighted fusion layer. Specifically, supervision is imposed on each output layer, and features from different levels are fused to produce a well-generated boundary map. Sem2Ins has the following desirable features: 1) Combined with some fast semantic segmentation methods, Sem2Ins runs at a real-time speed that is fairly well-balanced against accuracy; 2) Sem2Ins works flexibly with any semantic segmentation model for instance segmentation, and if the given semantic segmentation is sufficiently good, Sem2Ins even achieves state-of-the-art in terms of accuracy; 3) deep supervision and weighted fusion can be leveraged to generate high-quality boundaries; and 4) Sem2Ins can be easily extended to panoptic segmentation. Extensive experiments performed on the Cityscapes, WildDash, KITTI and COCO benchmarks have demonstrated that 1) Sem2Ins, when combined with PSPNet and DDRNet-23-Slim, consistently outperforms the state-of-the-art real-time solution (Box2Pix) in terms of both speed and accuracy; and 2) Sem2Ins combined with DPC performs comparably to some powerful detect-and-segment approaches.},
  archive      = {J_TMM},
  author       = {Chengxiang Yin and Jian Tang and Tongtong Yuan and Zhiyuan Xu and Yanzhi Wang},
  doi          = {10.1109/TMM.2021.3114541},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4183-4196},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Bridging the gap between semantic segmentation and instance segmentation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling sequential listening behaviors with attentive
temporal point process for next and next new music recommendation.
<em>TMM</em>, <em>24</em>, 4170–4182. (<a
href="https://doi.org/10.1109/TMM.2021.3114545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems, which aim to provide personalized suggestions for users, have proven to be an effective approach to cope with the information overload problem existing in many online applications and services. In this paper, we target two specific sequential recommendation tasks, next music recommendation and next new music recommendation , to predict the next (new) music piece that users would like based on their historical listening records. In current music recommender systems, various kinds of auxiliary/side information, e.g., item contents and users’ contexts, have been taken into account to facilitate user/item preference modeling and have yielded comparable performance improvement. Despite the gained benefits, it is still a challenging and important problem to fully exploit sequential music listening records due to the complexity and diversity of interactions and temporal contexts among users and music, as well as the dynamics of users’ preferences. To this end, this paper proposes a novel A ttentive T emporal P oint P rocess ( ATPP ) approach for sequential music recommendation, which is mainly composed of a temporal point process model and an attention mechanism. Our ATPP can effectively capture the long- and short-term preferences from the sequential behaviors of users for sequential music recommendation. Specifically, ATPP is able to discover the complex sequential patterns from the interaction between users and music with the temporal point process, as well as model the dynamic impact of historical music listening records on next (new) music pieces adaptively with an attention mechanism. Comprehensive experiments on four real-world music datasets demonstrate that the proposed approach ATPP outperforms state-of-the-art baselines in both next and next new music recommendation tasks.},
  archive      = {J_TMM},
  author       = {Dongjing Wang and Xin Zhang and Yao Wan and Dongjin Yu and Guandong Xu and Shuiguang Deng},
  doi          = {10.1109/TMM.2021.3114545},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4170-4182},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Modeling sequential listening behaviors with attentive temporal point process for next and next new music recommendation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Beyond triplet loss: Meta prototypical n-tuple loss for
person re-identification. <em>TMM</em>, <em>24</em>, 4158–4169. (<a
href="https://doi.org/10.1109/TMM.2021.3115451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Re-identification (ReID) aims at matching a person of interest across images. In convolutional neural network (CNN) based approaches, loss design plays a vital role in pulling closer features of the same identity and pushing far apart features of different identities. In recent years, triplet loss achieves superior performance and is predominant in ReID. However, triplet loss considers only three instances of two classes in per-query optimization (with an anchor sample as query) and it is actually equivalent to a two-class classification. There is a lack of loss design which enables the joint optimization of multiple instances (of multiple classes) within per-query optimization for person ReID. In this paper, we introduce a multi-class classification loss, i . e ., N-tuple loss, to jointly consider multiple ( $N$ ) instances for per-query optimization. This in fact aligns better with the ReID test/inference process, which conducts the ranking/comparisons among multiple instances. Furthermore, for more efficient multi-class classification, we propose a new meta prototypical N-tuple loss. With the multi-class classification incorporated, our model achieves the state-of-the-art performance on the benchmark person ReID datasets},
  archive      = {J_TMM},
  author       = {Zhizheng Zhang and Cuiling Lan and Wenjun Zeng and Zhibo Chen and Shih-Fu Chang},
  doi          = {10.1109/TMM.2021.3115451},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4158-4169},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Beyond triplet loss: Meta prototypical N-tuple loss for person re-identification},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning-based scalable image compression with
latent-feature reuse and prediction. <em>TMM</em>, <em>24</em>,
4143–4157. (<a href="https://doi.org/10.1109/TMM.2021.3114548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, learning-based image compression model has attracted much attention due to its impressive performance and ease of optimization, compared with traditional DCT and wavelet-based image compression standards. Most learning-based image compression models are trained to minimize joint rate-distortion (RD) loss on one single RD trade-off point. However, in many multimedia applications, due to communication constraints, or display adaptation needs for different spatial formats, bit rates or power, it is necessary to provide a variety of image versions for different client devices. To fulfill this requirement, typical end-to-end image compression methods have to compress an image into several bit streams independently by a number of pre-trained networks, which are resource-consuming because of redundancy among these streams. To address this problem, inspired by traditional scalable video coding framework, we propose a learning-based end-to-end quality and spatial scalable image compression (QSSIC) model in multi-layer structure, in which each layer could generate one bitstream corresponding to a specified resolution and image fidelity. This scalability is achieved by exploring the potential of feature-domain representation prediction and reuse. To be specific, firstly, bitstreams of previous layers are used to predict the current layer representations which contains the enhancement information, and then only prediction residuals need to be coded in enhancement layers. Secondly, previous bitstreams are reused in image reconstruction in higher layers to provide basic information. The proposed model could be optimized in an end-to-end manner. Extensive experiments show that our method outperforms state-of-art deep neural networks (DNN)-based auto-encoders in simulcast scenarios. In addition, our method has a better performance than the traditional scalable image compression method scalable extension of H.264/AVC (SVC) and is comparable to scalable extension of H.265/HEVC (SHVC).},
  archive      = {J_TMM},
  author       = {Yixin Mei and Li Li and Zhu Li and Fan Li},
  doi          = {10.1109/TMM.2021.3114548},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4143-4157},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning-based scalable image compression with latent-feature reuse and prediction},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint distribution alignment via adversarial learning for
domain adaptive object detection. <em>TMM</em>, <em>24</em>, 4102–4112.
(<a href="https://doi.org/10.1109/TMM.2021.3114550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptive object detection aims to adapt a well-trained detector from its original source domain with rich labeled data to a new target domain with unlabeled data. Recently, mainstream approaches perform this task through adversarial learning, yet still suffer from two limitations. First, they mainly align marginal distribution by unsupervised cross-domain feature matching, and ignore each feature&#39;s categorical and positional information that can be exploited for conditional alignment; Second, they treat all classes as equally important for transferring cross-domain knowledge and ignore that different classes usually have different transferability. In this article, we propose a joint adaptive detection framework (JADF) to address the above challenges. First, an end-to-end joint adversarial adaptation framework for object detection is proposed, which aligns both marginal and conditional distributions between domains without introducing any extra hyper-parameter. Next, to consider the transferability of each object class, a metric for class-wise transferability assessment is proposed, which is incorporated into the JADF objective for domain adaptation. Further, an extended study from unsupervised domain adaptation (UDA) to unsupervised few-shot domain adaptation (UFDA) is conducted, where only a few unlabeled training images are available in unlabeled target domain. Extensive experiments validate that JADF is effective in both the UDA and UFDA settings, achieving significant performance gains over existing state-of-the-art cross-domain detection methods.},
  archive      = {J_TMM},
  author       = {Bo Zhang and Tao Chen and Bin Wang and Ruoyao Li},
  doi          = {10.1109/TMM.2021.3114550},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4102-4112},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Joint distribution alignment via adversarial learning for domain adaptive object detection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RGB-d DSO: Direct sparse odometry with RGB-d cameras for
indoor scenes. <em>TMM</em>, <em>24</em>, 4092–4101. (<a
href="https://doi.org/10.1109/TMM.2021.3114546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual odometry (VO) is a fundamental technique for many robotics and augmented reality (AR) applications. However, most existing RGB-D VO systems suffer from large performance degradation when large occlusions are present and/or a large portion of depth values are invalid due to the limited range of an RGB-D camera, prohibiting the usage of most systems in practical applications. To address above two problems, we present RGB-D DSO, an RGB-D direct sparse odometry with the core part being sliding-window optimization with occlusion removal and a depth refinement module. Occlusion removal excludes negative effects arising from occluded objects when minimizing the final energy function for camera pose tracking. Depth refinement ensures sufficient valid depth values uniformly distributed for the depth map of a keyframe. Experimental results on three public datasets demonstrate that our method achieves smaller tracking error than most existing state-of-the-art methods. Meanwhile, our system takes only 21.93 ms to track a frame, which is faster than most existing methods.},
  archive      = {J_TMM},
  author       = {Zikang Yuan and Ken Cheng and Jinhui Tang and Xin Yang},
  doi          = {10.1109/TMM.2021.3114546},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4092-4101},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RGB-D DSO: Direct sparse odometry with RGB-D cameras for indoor scenes},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AniGAN: Style-guided generative adversarial networks for
unsupervised anime face generation. <em>TMM</em>, <em>24</em>,
4077–4091. (<a href="https://doi.org/10.1109/TMM.2021.3113786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel framework to translate a portrait photo-face into an anime appearance. Different from existing translation methods which do not designate specific styles, we aim to synthesize anime-faces which are style-consistent with a given reference anime-face. However, unlike typical translation tasks, such anime-face translation is particularly challenging due to the large and complex variations of appearances among anime-faces. Existing methods often fail to transfer the styles of reference anime-faces to the generated anime-faces, or introduce noticeable artifacts/distortions in the local shapes of their generated anime-faces. We propose a novel GAN-based anime-face translator, called AniGAN, to synthesize high-quality anime-faces. Specifically, a new generator architecture is proposed to simultaneously transfer color/texture styles and transform local facial shapes into anime-like counterparts based on the style of a reference anime-face, while preserving the global structure of the source photo-face. New normalization functions are designed for the generator to further improve local shape transformation and color/texture style transfer. Besides, we propose a double-branch discriminator to learn domain-specific distributions through individual branches and learn cross-domain shared distributions via shared layers, helping generate visually pleasing anime-faces and effectively mitigate artifacts/distortions. Extensive experiments on benchmark datasets qualitatively and quantitatively demonstrate the superiority of our method over state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Bing Li and Yuanlue Zhu and Yitong Wang and Chia-Wen Lin and Bernard Ghanem and Linlin Shen},
  doi          = {10.1109/TMM.2021.3113786},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4077-4091},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AniGAN: Style-guided generative adversarial networks for unsupervised anime face generation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contrastive attention for video anomaly detection.
<em>TMM</em>, <em>24</em>, 4067–4076. (<a
href="https://doi.org/10.1109/TMM.2021.3112814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider weakly-supervised video anomaly detection in this work. This task aims to learn to localize video frames containing anomaly events with only binary video-level annotation, i.e. , anomaly vs. normal. Traditional approaches usually formulate it as a multiple instance learning problem, which ignore the intrinsic data imbalance issue that positive samples are very scarce compared to negative ones. In this paper, we focus on addressing this issue to boost detection performance further. We develop a new light-weight anomaly detection model that fully utilizes enough normal videos to train a classifier with a good discriminative ability for normal videos, and we employ it to improve the selectivity for anomalous segments and filter out normal segments. Specifically, in addition to boosting anomalous prediction, a novel contrastive attention module additionally produces a converted normal feature from anomalous video to refined anomalous predictions by maximizing the classifier making a mistake. Moreover, to remove the stubborn normal segments selected by the attention module, we also design an attention consistency loss to employ the classifier with high confidence for normal features to guide the attention module. Extensive experiments on two large-scale datasets, UCF-Crime, ShanghaiTech and XD-Violence, clearly demonstrate that our model largely improves frame-level AUC over the state-of-the-art. Code is released at https://github.com/changsn/Contrastive-Attention-for-Video-Anomaly-Detection .},
  archive      = {J_TMM},
  author       = {Shuning Chang and Yanchao Li and Shengmei Shen and Jiashi Feng and Zhiying Zhou},
  doi          = {10.1109/TMM.2021.3112814},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4067-4076},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Contrastive attention for video anomaly detection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Self-paced enhanced low-rank tensor kernelized multi-view
subspace clustering. <em>TMM</em>, <em>24</em>, 4054–4066. (<a
href="https://doi.org/10.1109/TMM.2021.3112230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the multi-view subspace clustering problem and proposes the self-paced enhanced low-rank tensor kernelized multi-view subspace clustering (SETKMC) method, which is based on two motivations: (1) singular values of the representations and multiple instances should be treated differently. The reasons are that larger singular values of the representations usually quantify the major information and should be less penalized; samples with different degrees of noise may have various reliability for clustering. (2) many existing methods may cause the degraded performance when multi-view features reside in different nonlinear subspaces. This is because they usually assumed that multiple features lie within the union of several linear subspaces. SETKMC integrates the nonconvex tensor norm, self-paced learning, and kernel trick into a unified model for multi-view subspace clustering. The nonconvex tensor norm imposes different weights on different singular values. The self-paced learning gradually involves instances from more reliable to less reliable ones while the kernel trick aims to handle the multi-view data in nonlinear subspaces. One iterative algorithm is proposed based on the alternating direction method of multipliers. Extensive results on seven real-world datasets show the effectiveness of the proposed SETKMC compared to fifteen state-of-the-art multi-view clustering methods.},
  archive      = {J_TMM},
  author       = {Yongyong Chen and Shuqin Wang and Xiaolin Xiao and Youfa Liu and Zhongyun Hua and Yicong Zhou},
  doi          = {10.1109/TMM.2021.3112230},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4054-4066},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Self-paced enhanced low-rank tensor kernelized multi-view subspace clustering},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Controllable facial caricaturization with localized
deformation and personalized semantic attentions. <em>TMM</em>,
<em>24</em>, 4041–4053. (<a
href="https://doi.org/10.1109/TMM.2021.3111711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The facial caricature shows the distinct characteristics of a person via exaggerations of both shape and appearance. This paper presents a novel framework that automatically generates vivid facial caricatures by encoding personalized semantic information. To this end, we first design a part-based scheme for geometry warping, which composes local semantic deformation into a global warping field, equipped with sufficient warping freedom of different facial components. Second, under the scheme of Part-based Warping, we design a photo-to-caricature translation network called PbWarpGAN, and adopt several novel losses to capture the personalized characteristics of each input face and preserve its identity better. Third, based on PbWarpGAN, we develop a user-friendly interface by introducing an attention scheme on each facial component, allowing ordinary users to adjust the automatically generated caricature by PbWarpGAN according to their preference conveniently. Experimental results show that our PbWarpGAN is more effective in capturing personalized characteristics than counterparts, and provides an efficient tool for caricature designing application.},
  archive      = {J_TMM},
  author       = {Ming Zeng and Yinglin Zheng and Jinpeng Lin and Xuan Cheng and Jing Liao and Zizhao Wu and Wenjin Deng},
  doi          = {10.1109/TMM.2021.3111711},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4041-4053},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Controllable facial caricaturization with localized deformation and personalized semantic attentions},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prior-guided multi-view 3D head reconstruction.
<em>TMM</em>, <em>24</em>, 4028–4040. (<a
href="https://doi.org/10.1109/TMM.2021.3111485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recovery of a 3D head model including the complete face and hair regions is still a challenging problem in computer vision and graphics. In this paper, we consider this problem using only a few multi-view portrait images as input. Previous multi-view stereo methods that have been based, either on optimization strategies or deep learning techniques, suffer from low-frequency geometric structures such as unclear head structures and inaccurate reconstruction in hair regions. To tackle this problem, we propose a prior-guided implicit neural rendering network. Specifically, we model the head geometry with a learnable signed distance field (SDF) and optimize it via an implicit differentiable renderer with the guidance of some human head priors, including the facial prior knowledge, head semantic segmentation information and 2D hair orientation maps. The utilization of these priors can improve the reconstruction accuracy and robustness, leading to a high-quality integrated 3D head model. Extensive ablation studies and comparisons with state-of-the-art methods demonstrate that our method can generate high-fidelity 3D head geometries with the guidance of these priors.},
  archive      = {J_TMM},
  author       = {Xueying Wang and Yudong Guo and Zhongqi Yang and Juyong Zhang},
  doi          = {10.1109/TMM.2021.3111485},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4028-4040},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Prior-guided multi-view 3D head reconstruction},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep generative model for image inpainting with local binary
pattern learning and spatial attention. <em>TMM</em>, <em>24</em>,
4016–4027. (<a href="https://doi.org/10.1109/TMM.2021.3111491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) has demonstrated its powerful capabilities in the field of image inpainting. The DL-based image inpainting approaches can produce visually plausible results, but often generate various unpleasant artifacts, especially in the boundary and highly textured regions. To tackle this challenge, in this work, we propose a new end-to-end, two-stage (coarse-to-fine) generative model through combining a local binary pattern (LBP) learning network with an actual inpainting network. Specifically, the first LBP learning network using U-Net architecture is designed to accurately predict the structural information of the missing region, which subsequently guides the second image inpainting network for better filling the missing pixels. Furthermore, an improved spatial attention mechanism is integrated into the image inpainting network, by considering the consistency not only between the known region with the generated one, but also within the generated region itself. Extensive experiments on public datasets including CelebA-HQ , Places and Paris StreetView demonstrate that our model generates better inpainting results than the state-of-the-art competing algorithms, both quantitatively and qualitatively. The source code and trained models are available at https://github.com/HighwayWu/ImageInpainting .},
  archive      = {J_TMM},
  author       = {Haiwei Wu and Jiantao Zhou and Yuanman Li},
  doi          = {10.1109/TMM.2021.3111491},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4016-4027},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep generative model for image inpainting with local binary pattern learning and spatial attention},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-density sketch-to-image translation network.
<em>TMM</em>, <em>24</em>, 4002–4015. (<a
href="https://doi.org/10.1109/TMM.2021.3111501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketch-to-image (S2I) translation plays an important role in image synthesis and manipulation tasks, such as photo editing and colorization. Some specific S2I translations, including sketch-to-photo and sketch-to-painting, can be used as powerful tools in the art design industry. However, previous methods only support S2I translation with a single level of density, which gives less flexibility to users for controlling the input sketches. In this work, we propose the first multi-level density sketch-to-image translation framework, which allows the input sketch to cover a wide range from rough object outlines to microstructures. Moreover, to tackle the problem of noncontinuous representation of multi-level density input sketches, we project the density level into a continuous latent space, which can then be linearly controlled by a parameter. This allows users to conveniently control the densities of input sketches and the generation of images. Moreover, our method has been successfully verified on various datasets for different applications, including face editing, multi-modal sketch-to-photo translation, and anime colorization, providing coarse-to-fine levels of controls to these applications.},
  archive      = {J_TMM},
  author       = {Jialu Huang and Liao Jing and Zhifeng Tan and Sam Kwong},
  doi          = {10.1109/TMM.2021.3111501},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {4002-4015},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-density sketch-to-image translation network},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning representation on optimized high-order manifold for
visual classification. <em>TMM</em>, <em>24</em>, 3989–4001. (<a
href="https://doi.org/10.1109/TMM.2021.3111500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) and graph neural networks (GNNs) have demonstrated convincing performance on many tasks by learning the intrinsic structure of the data. However, it is still valuable and challenging to consider the complex and complete correlations of objects, i.e., high-order manifold structures, for representation learning. In this paper, we present a novel representation learning method that utilizes the optimized high-order manifold of the data for classification tasks of nonstructural data and graph-structure data. In the method, we fully explore the complicated relationship of samples by highlighting the high-order manifold information in a hypergraph. Specifically, we incorporate high-order manifold information by graph $p$ -Laplacian into a hypergraph and propose $p$ -Laplacian-based hypergraph neural networks (pLapHGNN) to significantly learn hidden layer representations that encode both the high-order structure of data and the high-order manifold geometrical information. Confronting the difficulties of obtaining optimized high-order manifolds of the data, we propose an effective approximate approach by graph $p$ -Laplacian representing the relationship of hyperedges in the hypergraph. Furthermore, we study the weights of hyperedges in a hypergraph with high-order manifold information. Experiments on the ModelNet40 dataset and NTU dataset demonstrate that the proposed method is more effective than the other popular methods for 3D shape recognition. Extensive experiments on other visual classification tasks and citation networks also show the superiority of our proposed method for representation learning.},
  archive      = {J_TMM},
  author       = {Xueqi Ma and Weifeng Liu and Qi Tian and Yue Gao},
  doi          = {10.1109/TMM.2021.3111500},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3989-4001},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning representation on optimized high-order manifold for visual classification},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tear the image into strips for style transfer. <em>TMM</em>,
<em>24</em>, 3978–3988. (<a
href="https://doi.org/10.1109/TMM.2021.3111515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Deep Convolutional Neural Networks (DCNNs) have achieved remarkable progress in computer vision community, including in style transfer tasks. Normally, most methods feed the full image to the DCNN. Although high-quality results can be achieved in this manner, several underlying problems arise. For one, with the increase in image resolution, the memory footprint will increase dramatically, leading to high latency and massive power consumption. Furthermore, these methods are usually unable to integrate with the commercial image signal processor (ISP), which processes the image in a line-sequential manner. To solve the above problems, we propose a novel ISP-friendly deep learning-based style transfer algorithm: SequentialStyle. A brand new line-sequential processing mode is proposed, where the image is torn into strips, and each strip is sequentially processed, contributing to less memory demand. We further propose a Spatial-Temporal Synergistic (STS) mechanism that decouples the previously simplex 2-D image style transfer into spatial feature processing (in-strip) and temporal correlation transmission (in-between strips). Compared with the SOTA style transfer algorithms, experimental results show that our SequentialStyle is competitive. Besides, SequentialStyle has less demand for memory consumption, even for the images whose resolutions are 4 k or higher.},
  archive      = {J_TMM},
  author       = {Yujie Huang and Yuhao Liu and Minge Jing and Xiaoyang Zeng and Yibo Fan},
  doi          = {10.1109/TMM.2021.3111515},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3978-3988},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Tear the image into strips for style transfer},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph signal processing for geometric data and beyond:
Theory and applications. <em>TMM</em>, <em>24</em>, 3961–3977. (<a
href="https://doi.org/10.1109/TMM.2021.3111440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geometric data acquired from real-world scenes, e.g. , 2D depth images, 3D point clouds, and 4D dynamic point clouds, have found a wide range of applications including immersive telepresence, autonomous driving, surveillance, etc . Due to irregular sampling patterns of most geometric data, traditional image/video processing methodologies are limited, while Graph Signal Processing (GSP)—a fast-developing field in the signal processing community—enables processing signals that reside on irregular domains and plays a critical role in numerous applications of geometric data from low-level processing to high-level analysis. To further advance the research in this field, we provide the first timely and comprehensive overview of GSP methodologies for geometric data in a unified manner by bridging the connections between geometric data and graphs, among the various geometric data modalities, and with spectral/nodal graph filtering techniques. We also discuss the recently developed Graph Neural Networks (GNNs) and interpret the operation of these networks from the perspective of GSP. We conclude with a brief discussion of open problems and challenges.},
  archive      = {J_TMM},
  author       = {Wei Hu and Jiahao Pang and Xianming Liu and Dong Tian and Chia-Wen Lin and Anthony Vetro},
  doi          = {10.1109/TMM.2021.3111440},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3961-3977},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Graph signal processing for geometric data and beyond: Theory and applications},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-classes and motion properties for concurrent visual
SLAM in dynamic environments. <em>TMM</em>, <em>24</em>, 3947–3960. (<a
href="https://doi.org/10.1109/TMM.2021.3110667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Working in a dynamic environment is a challenging problem for visual simultaneous localization and mapping (visual SLAM). Most of the existing visual SLAM algorithms fail resulting in significant error or losing in tracking when moving objects dominate the scene. We found two reasons cause these issues: (i) Previous approaches use information from all regions in the image; (ii) Existing algorithms use just two groups and block all feature points from moveable objects. In this paper, we propose a novel Multi-classes and motion properties for Concurrent Visual SLAM (MCV-SLAM) algorithm, which defines classes into five categories and concurrently fuses prior knowledge and observation of moving objects with semantic segmentation to ensure visual SLAM works properly for dynamic environments in real time. We also propose an adaptive method to optimize camera pose by using more potential inlier feature points with continuous weights, while eliminating the impact of moving objects. Our experiments are performed on public datasets of both indoor and outdoor scenes with moving objects in dynamic environments. The experimental results demonstrate that our method outperforms previous works with greater robustness and smaller tracking errors, and our MCV-SLAM can deal with the situations (i.e., the dominance of moving objects, lack of matching points), which lead misestimating occurs in existing SLAMs.},
  archive      = {J_TMM},
  author       = {Bohong Yang and Wu Ran and Lin Wang and Hong Lu and Yi-Ping Phoebe Chen},
  doi          = {10.1109/TMM.2021.3110667},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3947-3960},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-classes and motion properties for concurrent visual SLAM in dynamic environments},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Joint contrast enhancement and exposure fusion for
real-world image dehazing. <em>TMM</em>, <em>24</em>, 3934–3946. (<a
href="https://doi.org/10.1109/TMM.2021.3110483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the complexity of real environment and potential defects of current simulation datasets, either prior-based or deep learning-based single image dehazing methods may not work well in certain scenarios. In this work, we propose an efficient joint contrast enhancement and exposure fusion (CEEF) framework to formulate image dehazing task as a problem of enhancing local visibility and global contrast. In the contrast enhancement stage, several intermediate images are generated through two pre-processing steps. Specifically, gamma correction (GC) is used to adjust local visibility of an input hazy image. To address the issue of applying adaptive histogram equalization (AHE) to each color channel independently, we introduce color-preserving AHE (CP-AHE) to improve global contrast of the input hazy image. In the fusion stage, we develop a fast structural patch decomposition-based fusion strategy with an adaptive kernel size to fuse the inputs obtained by GC and CP-AHE. Extensive experiments on the real-world datasets demonstrate superiority of the proposed method to state-of-the-art methods in terms of visual and quantitative evaluation. Particularly for nighttime hazy scenes, our approach is shown to retain fine details and reduce color artifacts against three latest nighttime defogging methods. Moreover, we discuss potential applications of our CP-AHE in low-light enhancement and image editing.},
  archive      = {J_TMM},
  author       = {Xiaoning Liu and Hui Li and Ce Zhu},
  doi          = {10.1109/TMM.2021.3110483},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3934-3946},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Joint contrast enhancement and exposure fusion for real-world image dehazing},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constrained tensor representation learning for multi-view
semi-supervised subspace clustering. <em>TMM</em>, <em>24</em>,
3920–3933. (<a href="https://doi.org/10.1109/TMM.2021.3110098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view subspace clustering is an effective method to partition data into their corresponding categories. Nevertheless, existing multi-view subspace clustering approaches generally operate in a purely unsupervised manner, while ignoring the valuable weakly supervised information that can be readily obtained in many practical applications. In this paper, we consider the weakly supervised form of sample pair constraints, and devote to promoting the performance of multi-view subspace clustering with the aid of such prior knowledge. To achieve this goal, inspired by the intrinsic block diagonal structure of ideal low-rank representation (LRR), we propose a novel regularization to integrate must-link, cannot-link and normalization constraints into a unified formulation. The proposed regularization can be regarded as a general description for sample pairwise constraints, and thus provides a flexible framework for multi-view semi-supervised subspace clustering task. Furthermore, we devise a contrained tensor representation learning (CTRL) model that takes advantage of our proposed regularization to facilitate the learning of the desired representation tensor. An efficient optimization algorithm based on alternating direction minimization strategy is carefully designed to solve the proposed CTRL model. Extensive experiments on eight challenging real-world datasets are conducted, and the results validate the effectiveness of our designed pairwise constraints regularization, as well as the superiority of the proposed CTRL model.},
  archive      = {J_TMM},
  author       = {Yongqiang Tang and Yuan Xie and Chenyang Zhang and Wensheng Zhang},
  doi          = {10.1109/TMM.2021.3110098},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3920-3933},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Constrained tensor representation learning for multi-view semi-supervised subspace clustering},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PiSLTRc: Position-informed sign language transformer with
content-aware convolution. <em>TMM</em>, <em>24</em>, 3908–3919. (<a
href="https://doi.org/10.1109/TMM.2021.3109665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the superiority of Transformer in learning long-term dependency, the sign language Transformer model achieves remarkable progress in Sign Language Recognition (SLR) and Translation (SLT). However, there are several issues with the Transformer that prevent it from better sign language understanding. The first issue is that the self-attention mechanism learns sign video representation in a frame-wise manner, neglecting the temporal semantic structure of sign gestures. Secondly, the attention mechanism with absolute position encoding is direction and distance unaware, thus limiting its ability. To address these issues, we propose a new model architecture, namely PiSLTRc, with two distinctive characteristics: (i) content-aware and position-aware convolution layers. Specifically, we explicitly select relevant features using a novel content-aware neighborhood gathering method. Then we aggregate these features with position-informed temporal convolution layers, thus generating robust neighborhood-enhanced sign representation. (ii) injecting the relative position information to the attention mechanism in the encoder, decoder, and even encoder-decoder cross attention. Compared with the vanilla Transformer model, our model performs consistently better on three large-scale sign language benchmarks: PHOENIX-2014, PHOENIX-2014-T and CSL. Furthermore, extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on translation quality with $+1.6$ BLEU improvements.},
  archive      = {J_TMM},
  author       = {Pan Xie and Mengyi Zhao and Xiaohui Hu},
  doi          = {10.1109/TMM.2021.3109665},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3908-3919},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PiSLTRc: Position-informed sign language transformer with content-aware convolution},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention guided relation detection approach for video
visual relation detection. <em>TMM</em>, <em>24</em>, 3896–3907. (<a
href="https://doi.org/10.1109/TMM.2021.3109430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Visual Relation Detection (VidVRD) aims at detecting the relation instances between two observed objects in the form of $&amp;lt; $ subject-predicate-object $&amp;gt;$ . Unlike image visual relation detection, due to the introduction of the time dimensions, the various predicates and spatial-temporal locations are both required to be tackled, making the task challenging. To balance these challenges, most existing works perform this task in two phases: first predicting relationships in segmented clips to capture the motions, and then associating them into the relation instances with proper locations in videos. These works detect different relationships by collecting the cues from multi-aspects, but treat them equally without distinction. Furthermore, due to the dynamic scenes and drifting problem in object tracking, the rigid spatial overlap used to determine the association in previous works is insufficient, which leads to missing associations. To address the problems, in this paper, we propose a novel attention guided relation detection approach for VidVRD. In order to model the distinction among different cues and strengthen the salient characteristics, we assign these cues the attention weights for relationship prediction and association decision-making. In addition, to comprehensively measure whether merging the relationships, we put forward a customized network to take both visual appearance and geometric location into account. Extensive experiment results on ImageNet-VidVRD dataset and VidOR dataset demonstrate the effectiveness of our proposed approach. And abundant ablation studies verify the component designed in the approach is essential.},
  archive      = {J_TMM},
  author       = {Qianwen Cao and Heyan Huang},
  doi          = {10.1109/TMM.2021.3109430},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3896-3907},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Attention guided relation detection approach for video visual relation detection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Orthogonal low-rank projection learning for robust image
feature extraction. <em>TMM</em>, <em>24</em>, 3882–3895. (<a
href="https://doi.org/10.1109/TMM.2021.3109442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projecting the original data into a low-dimensional target space for feature extraction is a common method. Recently, presentation-based approaches have been widely concerned and many feature extraction algorithms based on this have been proposed. However, in the process of acquiring real data, the pollution of complex noise cannot always be avoided, which will greatly increase the difficulty of feature extraction and even lead to failed feature extraction results. Thus, a robust image feature extraction model based on Orthogonal Low-Rank Projection Learning (OLRPL) is proposed, in which the introduction of orthogonal matrix can encourage the preservation of the main components of the sample. Particularly, the row sparsity constraint introduced on the projection matrix can encourage the features to be more compact, discriminative and interpretable. In particular, the Weighted Truncated Schatten p -norm (WTSN) is proposed to better solve the optimization problem of low-rank constraints. At the same time, the correntropy is applied in OLRPL to suppress the complex noise in the data and thus improve the robustness of the model. Finally, we specially design a robust classification loss function so that our model can be fitted the supervised scene effectively. Experiments on five general databases have proved that OLRPL has better effectiveness and robustness than existing advanced methods.},
  archive      = {J_TMM},
  author       = {Xiaoqian Zhang and Zhen Tan and Huaijiang Sun and Zungang Wang and Mingwei Qin},
  doi          = {10.1109/TMM.2021.3109442},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3882-3895},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Orthogonal low-rank projection learning for robust image feature extraction},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image-to-image translation: Methods and applications.
<em>TMM</em>, <em>24</em>, 3859–3881. (<a
href="https://doi.org/10.1109/TMM.2021.3109419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-to-image translation (I2I) aims to transfer images from a source domain to a target domain while preserving the content representations. I2I has drawn increasing attention and made tremendous progress in recent years because of its wide range of applications in many computer vision and image processing problems, such as image synthesis, segmentation, style transfer, restoration, and pose estimation. In this paper, we provide an overview of the I2I works developed in recent years. We will analyze the key techniques of the existing I2I works and clarify the main progress the community has made. Additionally, we will elaborate on the effect of I2I on the research and industry community and point out remaining challenges in related fields.},
  archive      = {J_TMM},
  author       = {Yingxue Pang and Jianxin Lin and Tao Qin and Zhibo Chen},
  doi          = {10.1109/TMM.2021.3109419},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3859-3881},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Image-to-image translation: Methods and applications},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BVI-DVC: A training database for deep video compression.
<em>TMM</em>, <em>24</em>, 3847–3858. (<a
href="https://doi.org/10.1109/TMM.2021.3108943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods are increasingly being applied in the optimisation of video compression algorithms and can achieve significantly enhanced coding gains, compared to conventional approaches. Such approaches often employ Convolutional Neural Networks (CNNs) which are trained on databases with relatively limited content coverage. In this paper, a new extensive and representative video database, BVI-DVC,is presented for training CNN-based video compression systems, with specific emphasis on machine learning tools that enhance conventional coding architectures, including spatial resolution and bit depth up-sampling, post-processing and in-loop filtering. BVI-DVC contains 800 sequences at various spatial resolutions from 270p to 2160p and has been evaluated on ten existing network architectures for four different coding tools. Experimental results show that this database produces significant improvements in terms of coding gains over five existing (commonly used) image/video training databases under the same training and evaluation configurations. The overall additional coding improvements by using the proposed database for all tested coding modules and CNN architectures are up to 10.3% based on the assessment of PSNR and 8.1% based on VMAF.},
  archive      = {J_TMM},
  author       = {Di Ma and Fan Zhang and David R. Bull},
  doi          = {10.1109/TMM.2021.3108943},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3847-3858},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {BVI-DVC: A training database for deep video compression},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Soft warping based unsupervised domain adaptation for stereo
matching. <em>TMM</em>, <em>24</em>, 3835–3846. (<a
href="https://doi.org/10.1109/TMM.2021.3108900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo matching is a practical method to estimate depth information and retrieve 3D world in robot perception and autonomous driving scenarios. With the development of convolution neural networks (CNNs), deep-learning based stereo matching algorithms have significantly improved the accuracy and dominated most of the online benchmarks. However, limited labels in real world, especially in challenging weather conditions, still hinder the technology from practical usage. In this paper, we propose a new unsupervised learning mechanism for stereo matching, utilizing adversarial iterative learning and novel soft warping loss to promote the effectiveness of the networks in unseen environments. The experiments transferring the stereo matching module from synthetic domain to real-world domain demonstrate the superiority of our proposed method. Extensive experiments in challenging weathers further prove that our method shows great practical potential in strait environments.},
  archive      = {J_TMM},
  author       = {Haoyuan Zhang and Lap-Pui Chau and Danwei Wang},
  doi          = {10.1109/TMM.2021.3108900},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3835-3846},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Soft warping based unsupervised domain adaptation for stereo matching},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pasadena: Perceptually aware and stealthy adversarial
denoise attack. <em>TMM</em>, <em>24</em>, 3807–3822. (<a
href="https://doi.org/10.1109/TMM.2021.3108009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising can remove natural noise that widely exists in images captured by multimedia devices due to low-quality imaging sensors, unstable image transmission processes, or low light conditions. Recent works also find that image denoising benefits the high-level vision tasks, e.g ., image classification. In this work, we try to challenge this common sense and explore a totally new problem, i.e ., whether the image denoising can be given the capability of fooling the state-of-the-art deep neural networks (DNNs) while enhancing the image quality. To this end, we initiate the very first attempt to study this problem from the perspective of adversarial attack and propose the adversarial denoise attack . More specifically, our main contributions are three-fold: First , we identify a new task that stealthily embeds attacks inside the image denoising module widely deployed in multimedia devices as an image post-processing operation to simultaneously enhance the visual image quality and fool DNNs. Second , we formulate this new task as a kernel prediction problem for image filtering and propose the adversarial-denoising kernel prediction that can produce adversarial-noiseless kernels for effective denoising and adversarial attacking simultaneously. Third , we implement an adaptive perceptual region localization to identify semantic-related vulnerability regions with which the attack can be more effective while not doing too much harm to the denoising. We name the proposed method as Pasadena (Perceptually Aware and Stealthy Adversarial DENoise Attack) and validate our method on the NeurIPS’17 adversarial competition dataset, CVPR2021-AIC-VI: unrestricted adversarial attacks on ImageNet, and Tiny-ImageNet-C dataset. The comprehensive evaluation and analysis demonstrate that our method not only realizes denoising but also achieves a significantly higher success rate and transferability over state-of-the-art attacks.},
  archive      = {J_TMM},
  author       = {Yupeng Cheng and Qing Guo and Felix Juefei-Xu and Shang-Wei Lin and Wei Feng and Weisi Lin and Yang Liu},
  doi          = {10.1109/TMM.2021.3108009},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3807-3822},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Pasadena: Perceptually aware and stealthy adversarial denoise attack},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Co-LDL: A co-training-based label distribution learning
method for tackling label noise. <em>TMM</em>, <em>24</em>, 1093–1104.
(<a href="https://doi.org/10.1109/TMM.2021.3116430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performances of deep neural networks are prone to be degraded by label noise due to their powerful capability in fitting training data. Deeming low-loss instances as clean data is one of the most promising strategies in tackling label noise and has been widely adopted by state-of-the-art methods. However, prior works tend to drop high-loss instances directly, neglecting their valuable information. To address this issue, we propose an end-to-end framework named Co-LDL, which incorporates the low-loss sample selection strategy with label distribution learning. Specifically, we simultaneously train two deep neural networks and let them communicate useful knowledge by selecting low-loss and high-loss samples for each other. Low-loss samples are leveraged conventionally for updating network parameters. On the contrary, high-loss samples are trained in a label distribution learning manner to update network parameters and label distributions concurrently. Moreover, we propose a self-supervised module to further boost the model performance by enhancing the learned representations. Comprehensive experiments on both synthetic and real-world noisy datasets are provided to demonstrate the superiority of our Co-LDL method over state-of-the-art approaches in learning with noisy labels. The source code and models have been made available at https://github.com/NUST-Machine-Intelligence-Laboratory/CoLDL .},
  archive      = {J_TMM},
  author       = {Zeren Sun and Huafeng Liu and Qiong Wang and Tianfei Zhou and Qi Wu and Zhenmin Tang},
  doi          = {10.1109/TMM.2021.3116430},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1093-1104},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Co-LDL: A co-training-based label distribution learning method for tackling label noise},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-modal variational graph auto-encoder for
recommendation systems. <em>TMM</em>, <em>24</em>, 1067–1079. (<a
href="https://doi.org/10.1109/TMM.2021.3111487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph embedding based methods have been used in recommendation systems recently, owing to their advances in modeling nodes as embeddings in a low-dimensional space. By effective neighborhood aggregation, graph convolutional networks can exploit high-order connections of neighbors such that the learned embeddings could be more informative thus improve the recommendation performance. However, user and item representations learned by graph aggregation inherently contain uncertainty due to sparsity of user-item interactions and noise of item features. To address these challenges, we propose a multi-modal variational graph auto-encoder (MVGAE) method. Specifically, we design modality-specific variational encoders that learn a Gaussian variable for each node whereas the mean vector represents semantic information and the variance vector denotes the noise level of the corresponding modality. Moreover, with the conditional independence assumption, the modality-specific Gaussian node embeddings are fused according to the product-of-experts principle, where the semantic information in each modality is weighted based on the estimated uncertainty level. Extensive experiments on three public datasets, Amazon Movies, Amazon Electronics and AliShop-7 C, demonstrate that our proposed method achieves competitive performance when compared with the state-of-the-art algorithms.},
  archive      = {J_TMM},
  author       = {Jing Yi and Zhenzhong Chen},
  doi          = {10.1109/TMM.2021.3111487},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1067-1079},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-modal variational graph auto-encoder for recommendation systems},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized large margin <span
class="math inline"><em>k</em></span>NN for partial label learning.
<em>TMM</em>, <em>24</em>, 1055–1066. (<a
href="https://doi.org/10.1109/TMM.2021.3109438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To deal with noises in partial label learning (PLL), existing approaches try to perform disambiguation either by identifying the ground-truth label or by averaging the candidate labels. However, these methods can be easily misled by the false-positive noisy labels in the candidate set, and fail to generalize well in testing. When labeling information is ambiguous, learning paradigms should depend more on underlying data structure. Large margin nearest neighbour (LMNN) is a popular strategy to consider instance and class correlations in supervised learning, but can not be directly used in weakly-supervised PLL due to the ambiguity of labeling information. In this paper, we first define similarly and differently labeled pairs as well as the similarity weight to evaluate the similarties between any two instances. We then propose a novel PLL method called Generalized Large Margin $k$ NN for Partial Label Learning (GLMNN-PLL), which adapts the framework of LMNN to PLL by modifying the constraint from ‘the same class’ to ‘similarly-labeled’. GLMNN-PLL aims to learn a new metric and perform disambiguation by reorganizing the underlying data structure, that is, making similarly labeled instances closer to each other while making differently labeled instances seperated by a large margin. As two close instances with shared labels do not necessarily belong to the same class, we put a weight on each instance pair. An efficient algorithm is designed to optimize the proposed method and the convergence is analyzed in this paper. Moreover, we present a theoretical analysis of the generalization error bound for GLMNN-PLL. Comprehensive experiments on controlled UCI datasets as well as real-world partial label datasets from various domains demonstrate the superiorities of the proposed method.},
  archive      = {J_TMM},
  author       = {Xiuwen Gong and Jiahui Yang and Dong Yuan and Wei Bao},
  doi          = {10.1109/TMM.2021.3109438},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1055-1066},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Generalized large margin $k$NN for partial label learning},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Facial chirality: From visual self-reflection to robust
facial feature learning. <em>TMM</em>, <em>24</em>, 4275–4284. (<a
href="https://doi.org/10.1109/TMM.2022.3197365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a fundamental vision task, facial expression recognition has made substantial progress recently. However, the recognition performance often degrades significantly in real-world scenarios due to the lack of robust facial features. In this paper, we propose an effective facial feature learning method that takes the advantage of facial chirality to discover the discriminative features for facial expression recognition. Most previous studies implicitly assume that human faces are symmetric. However, our work reveals that the facial asymmetric effect can be a crucial clue. Given a face image and its reflection without additional labels, we decouple the emotion-invariant facial features from the input image pair to better capture the emotion-related facial features. Moreover, as our model aligns emotion-related features of the image pair to enhance the recognition performance, the value of precise facial landmark alignment as a pre-processing step is reconsidered in this paper. Experiments demonstrate that the learned emotion-related features outperform the state of the art methods on several facial expression recognition benchmarks as well as real-world occlusion datasets, which manifests the effectiveness and robustness of the proposed model.},
  archive      = {J_TMM},
  author       = {Ling Lo and Hongxia Xie and Hong-Han Shuai and Wen-Huang Cheng},
  doi          = {10.1109/TMM.2022.3197365},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {4275-4284},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Facial chirality: From visual self-reflection to robust facial feature learning},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Texture preserving photo style transfer network.
<em>TMM</em>, <em>24</em>, 3823–3834. (<a
href="https://doi.org/10.1109/TMM.2021.3108401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photo style transfer aims to change the style of a given photo to a reference style image with the constraint by retaining the broad and faithful conservation of the content of the input image. Most previous algorithms still have challenging issues on how to exactly extract and represent the style of the image to avoid the interruption of human visual perception. In this paper, we present a texture preserving photo style transfer algorithm by separating the input image into texture and structure and then applying the deep structure style transfer network to effectively change the extracted style characteristics of the structure. The texture preserving photo style transfer overcomes the main drawback of the previous approaches like distortion and saturation of the boundary of the objects. The quantitative and qualitative experimental results including user study prove that the proposed photo style transfer is universally applicable comparing to remarkable previous approaches.},
  archive      = {J_TMM},
  author       = {Hwanbok Mun and Gang-Joon Yoon and Jinjoo Song and Sang Min Yoon},
  doi          = {10.1109/TMM.2021.3108401},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3823-3834},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Texture preserving photo style transfer network},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SEcure similar image matching (SESIM): An improved privacy
preserving image retrieval protocol over encrypted cloud database.
<em>TMM</em>, <em>24</em>, 3794–3806. (<a
href="https://doi.org/10.1109/TMM.2021.3107681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of cloud computing provides new dimension for the user to perform computations and store huge amount of data say images, video, audio etc,. However, the benefits of outsourcing the tasks bring privacy issues for the data that are outsourced. Consequently, to ensure privacy, multimedia data is encrypted and offloaded to the cloud database. Though images are encrypted, during retrieval, cloud server performs similarity computation on plaintext features. Fully homomorphic shows great results in computation over encrypted data yet due to its computation burden it is not applicable for practical usage. Thus, to guarantee the secrecy of outsourced image features, the proposed paper introduced an efficient SEcure Similar Image Matching (SESIM) protocol under encrypted domain. The computation overhead of the proposed SESIM protocol is compared with the existing secure distance metrics. The experiments and performance analysis show the effectiveness and security of the proposed scheme under encrypted cloud database.},
  archive      = {J_TMM},
  author       = {T. Janani and M. Brindha},
  doi          = {10.1109/TMM.2021.3107681},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3794-3806},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SEcure similar image matching (SESIM): An improved privacy preserving image retrieval protocol over encrypted cloud database},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single-image specular highlight removal via real-world
dataset construction. <em>TMM</em>, <em>24</em>, 3782–3793. (<a
href="https://doi.org/10.1109/TMM.2021.3107688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Specular reflections pose great challenges on various multimedia and computer vision tasks, e.g. , image segmentation, detection and matching. In this paper, we build a large-scale Paired Specular-Diffuse (PSD) image dataset, where the images are carefully captured by using real-world objects and the ground-truth specular-free diffuse images are provided. To the best of our knowledge, this is the first real-world benchmark dataset for specular highlight removal task, which is useful for evaluating and encouraging new deep learning-based approaches. Given this dataset, we present a novel Generative Adversarial Network (GAN) for specular highlight removal from a single image by introducing the detection of specular reflection information as a guidance. Our network also makes full use of the attention mechanism and is able to directly model the mapping relation between the diffuse area and the specular highlight area without any explicit estimation of the illumination. Experimental results demonstrate that the proposed network is more effective to remove specular reflection components with the guidance of specular highlight detection than recent state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Zhongqi Wu and Chuanqing Zhuang and Jian Shi and Jianwei Guo and Jun Xiao and Xiaopeng Zhang and Dong-Ming Yan},
  doi          = {10.1109/TMM.2021.3107688},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3782-3793},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Single-image specular highlight removal via real-world dataset construction},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quaternion-based dictionary learning and saturation-value
total variation regularization for color image restoration.
<em>TMM</em>, <em>24</em>, 3769–3781. (<a
href="https://doi.org/10.1109/TMM.2021.3107162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color image restoration is a critical task in imaging sciences. Most variational methods regard the color image as a Euclidean vector or the direct combination of three monochrome images and completely ignore the inherent color structures within channels. To better describe the relationship of color channels, we represent the color image as the so-called pure quaternion matrix. Note that the celebrated dictionary learning method has attracted considerable attention for image recovery in the past decade. Following this idea, we propose a novel quaternion-based color image recovery method. This model combines the advantages of dictionary learning and the total variation method for color image restoration. The new strategy used in the proposed model manages to handle the color image restoration problem in the quaternion space. Moreover, the new proposed model can be easily solved by the classical alternating direction method of multipliers (ADMM) algorithm. Numerical results demonstrate clearly that the performance of our proposed dictionary learning method is better than some state-of-the-art color image dictionary learning and total variation methods in terms of some criteria and visual quality.},
  archive      = {J_TMM},
  author       = {Chaoyan Huang and Michael K. Ng and Tingting Wu and Tieyong Zeng},
  doi          = {10.1109/TMM.2021.3107162},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3769-3781},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Quaternion-based dictionary learning and saturation-value total variation regularization for color image restoration},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video quality assessment with serial dependence modeling.
<em>TMM</em>, <em>24</em>, 3754–3768. (<a
href="https://doi.org/10.1109/TMM.2021.3107148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video quality assessment (VQA) is much more challenging than image quality assessment, due to the difficulty of modeling temporal influence among frames. Most of the existing VQA methods usually isolate each moment within the video (i.e., it neglects the sequential nature), leading to a large gap from the subjective perception. Recent research on neuroscience suggests a serially dependent perception (SDP) mechanism in the human visual system (HVS). Namely, the HVS tends to incorporate the recent past visual experience to predict the present perception. Inspired by the SDP, we suggest that the HVS prefers stable and continuous degradations in videos due to their predictability, and exhibits less tolerance to interrupted and unpredictable disturbances. Thus, we introduce a novel serial dependence modeling (SDM) framework for full-reference VQA in this paper. Firstly, the instantaneous degradation is measured on both the static appearance and motion information for each glimpse of scenes. Since motion plays an important role in videos, two types of structures are extracted for motion representation, namely, an explicit content-based 3D structure and an implicit feature-based 2D structure. Next, an assessment-directed long-short term memory (A-LSTM) is proposed to capture the serial dependence among instantaneous degradations. With the consideration of the perceptual effect from the previous moment on the current one, especially the effect from the perceptually worst moment, the serially dependent degradation is characterized. Finally, by mimicking the subjective rating for video-viewing, an attention-based quality decision procedure is presented to acquire the final video quality. Experimental results on publicly available VQA databases demonstrate that the proposed method maintains good consistency with the subjective perception.},
  archive      = {J_TMM},
  author       = {Yongxu Liu and Jinjian Wu and Aobo Li and Leida Li and Weisheng Dong and Guangming Shi and Weisi Lin},
  doi          = {10.1109/TMM.2021.3107148},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3754-3768},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Video quality assessment with serial dependence modeling},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tensor product and tensor-singular value decomposition based
multi-exposure fusion of images. <em>TMM</em>, <em>24</em>, 3738–3753.
(<a href="https://doi.org/10.1109/TMM.2021.3106789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering multidimensional structure of the multi-exposure images, a new Tensor product and Tensor-singular value decomposition based Multi-Exposure image Fusion (TT-MEF) method is proposed. The main innovation of this work is to explore a new feature representation of multi-exposure images in the new tensor domain and design the fusion strategy on this basis. Specifically, the luminance and the chrominance channels are fused separately to maintain color consistency. For the luminance fusion, the luminance channel of multi-exposure images is divided into two parts, that is, de-mean term and mean term. The de-mean term is represented as a tensor to extract the feature. Then, the tensor product and tensor-singular value decomposition (T-SVD) are used to design a tensor feature extractor. Furthermore, a fusion strategy of the de-mean term is presented according to the visual saliency model, and a fusion strategy of the mean term is defined by the local and the global visual weights to control counterpoise between the local and global luminance. For the chrominance fusion, a new fusion strategy is also designed by the tensor product and T-SVD, similar to the luminance fusion. Finally, the fused image is obtained by combining the luminance and chrominance fusion. Experimental results show that the proposed TT-MEF method generally outperforms the existing state-of-the-art in terms of subjective visual quality and objective evaluation.},
  archive      = {J_TMM},
  author       = {Haiyong Xu and Gangyi Jiang and Mei Yu and Zhongjie Zhu and Yongqiang Bai and Yang Song and Huifang Sun},
  doi          = {10.1109/TMM.2021.3106789},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3738-3753},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Tensor product and tensor-singular value decomposition based multi-exposure fusion of images},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep light field super-resolution using frequency domain
analysis and semantic prior. <em>TMM</em>, <em>24</em>, 3722–3737. (<a
href="https://doi.org/10.1109/TMM.2021.3106775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field (LF) camera can simultaneously capture the intensity and direction information of light rays, which has been widely concerned. However, limited by the size of the imaging sensor, the captured LF image (LFI) has a trade-off between spatial and angular resolutions. To this end, this paper proposes a new LF super-resolution method using frequency domain analysis and semantic prior, which designs a two-stage learning framework to enhance the spatial and angular resolutions of LFI. Specifically, the proposed method first decomposes the spatial and angular information to explore the 4D structure of LFI by using frequency domain transformation, and formulates the LF super-resolution as a frequency restoration process. Then, the decomposed frequency components are recovered in a progressive restoration manner, with new cascaded 2D and 3D convolutional neural networks. To further improve the quality of the reconstructed LFI, especially at the object boundary, the semantic prior is incorporated into the designed network to enhance its representation ability. Finally, the super-resolved LFI is reconstructed by inverse frequency domain transformation. Experimental results show that the proposed method can effectively generate high-resolution LFI, and outperforms other state-of-the-art methods in terms of both subjective visual perception and objective quality evaluation. Moreover, the proposed method can enhance the performance of LF applications such as depth estimation.},
  archive      = {J_TMM},
  author       = {Yeyao Chen and Gangyi Jiang and Zhidi Jiang and Mei Yu and Yo-Sung Ho},
  doi          = {10.1109/TMM.2021.3106775},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3722-3737},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep light field super-resolution using frequency domain analysis and semantic prior},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Progress and opportunities in modelling just-noticeable
difference (JND) for multimedia. <em>TMM</em>, <em>24</em>, 3706–3721.
(<a href="https://doi.org/10.1109/TMM.2021.3106503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Just-Noticeable Difference (JND) is the minimal amount of signal change that the human being is able to perceive. The human has five major sensing organs, namely, eyes, ears, nose, skin and tongue, and therefore JND exists for the corresponding five signal modalities and their derivatives. JND can play an important role in many multimedia applications and services, because these imperfect human perceptual characteristics may be turned into advantages for relevant system design, development and optimization. This paper starts off by giving a general description for JND concepts and the related statistical processes. Then, existing computational models for visual JND, which represent the majority of the related research so far, are to be reviewed systematically, with both handcrafted modeling and machine learning approaches. Furthermore, research attempts will be surveyed for JNDs for audio, smell, haptics and gustatory signals, as well as cross-modality/media efforts. Finally, possible future directions and opportunities are analysed and discussed.},
  archive      = {J_TMM},
  author       = {Weisi Lin and Gheorghita Ghinea},
  doi          = {10.1109/TMM.2021.3106503},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3706-3721},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Progress and opportunities in modelling just-noticeable difference (JND) for multimedia},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ForestDet: Large-vocabulary long-tailed object detection and
instance segmentation. <em>TMM</em>, <em>24</em>, 3693–3705. (<a
href="https://doi.org/10.1109/TMM.2021.3106096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection and instance segmentation with a large number of object categories and long-tailed data distribution are challenging for most existing deep learning models. As the number of classes increases, the outputs of a classifier become sensitive to likely noisy logits, which can easily result in an incorrect recognition. To alleviate the large-vocabulary problem, we cluster fine-grained classes into coarser parent classes and then build a classification tree to classify an object into a fine-grained class via its parent class. Because the number of parent class is much fewer, their logits are more stable to suppress the wrong/noisy logits existed in the fine-grained class nodes. Due to a variety of ways for clustering fine-grained classes into parent classes, we can further construct multiple trees to build a classification forest where each single tree contributes its vote to the fine-grained classification. Moreover, a simple yet effective resampling method, termed as NMS Resampling, is proposed aiming at solving the long tail (data imbalance) problem. Our method, coined as ForestDet, serves as a plug-and-play module, which can be readily employed in both one-stage and two-stage object recognition models for recognizing more than 1000 categories. Extensive experiments are conducted on the large vocabulary dataset LVIS. Compared to the Mask R-CNN baseline, our two-stage counterpart Forest R-CNN significantly boosts the performance by 11.5% and 3.9% AP improvements on the rare categories and overall categories, respectively. Compared to the RetinaNet baseline, our one-stage counterpart Forest RetinaNet improves 2.1% AP on overall categories. Moreover, we achieve state-of-the-art results on the LVIS dataset. Code and models are available at https://github.com/JialianW/Forest_RCNN.},
  archive      = {J_TMM},
  author       = {Jialian Wu and Liangchen Song and Qian Zhang and Ming Yang and Junsong Yuan},
  doi          = {10.1109/TMM.2021.3106096},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3693-3705},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ForestDet: Large-vocabulary long-tailed object detection and instance segmentation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Temporal self-ensembling teacher for semi-supervised object
detection. <em>TMM</em>, <em>24</em>, 3679–3692. (<a
href="https://doi.org/10.1109/TMM.2021.3105807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the semi-supervised object detection (SSOD) which makes good use of unlabeled data to boost performance. We face the following obstacles when adapting the knowledge distillation (KD) framework in SSOD. (1) The teacher model serves a dual role as a teacher and a student, such that the teacher predictions on unlabeled images may limit the upper bound of the student. (2) The data imbalance issue caused by the large quantity of consistent predictions between the teacher and student hinders an efficient knowledge transfer between them. To mitigate these issues, we propose a novel SSOD model called Temporal Self-Ensembling Teacher (TSET). Our teacher model ensembles its temporal predictions for unlabeled images under stochastic perturbations. Then, our teacher model ensembles its model weights with those of the student model by an exponential moving average. These ensembling strategies ensure data and model diversity, and lead to better teacher predictions for unlabeled images. In addition, we adapt the focal loss to formulate the consistency loss for handling the data imbalance issue. Together with a thresholding method, the focal loss automatically reweights the inconsistent predictions, which preserves the knowledge for difficult objects to detect in the unlabeled images. The mAP of our model reaches 80.73% and 40.52% on the VOC2007 test set and the COCO2014 minival5k set, respectively, and outperforms a strong fully supervised detector by 2.37% and 1.49%, respectively. Furthermore, the mAP of our model (80.73%) sets a new state-of-the-art performance in SSOD on the VOC2007 test set.},
  archive      = {J_TMM},
  author       = {Cong Chen and Shouyang Dong and Ye Tian and Kunlin Cao and Li Liu and Yuanhao Guo},
  doi          = {10.1109/TMM.2021.3105807},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3679-3692},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Temporal self-ensembling teacher for semi-supervised object detection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymmetric correlation quantization hashing for cross-modal
retrieval. <em>TMM</em>, <em>24</em>, 3665–3678. (<a
href="https://doi.org/10.1109/TMM.2021.3105824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, cross-modal hashing (CMH) has attracted considerable attention due to its ability to learn across different modalities and its high efficiency for similarity retrieval applications. This procedure is computationally inexpensive when dealing with large-scale multi-modalities datasets. However, they do not form the ideal representative model to fully exploit multi-modal data’s underlying properties despite their successful performance. We identify that: (i) most CMH models in their current forms transform the real data points into discrete compact binary codes, which can limit their ability to prevent the loss of important information and thereby produce suboptimal results. (ii) the discrete-binary constraint model is hard to implement, and relaxing the binary constraints is a common property in most existing methods, which often leads to significant quantization errors. (iii) handling the CMH in a symmetry domain leads to a complex and inefficient optimization problem. This paper addresses the above challenges and proposes a novel Asymmetric Correlation Quantization Hashing (ACQH) method. ACQH learns a projection matrix for each heterogeneous modality to map the data point into a low-dimensional semantic space and constructs a compositional quantization to generate hash codes, using the pairwise semantic similarity preservation and the pointwise label regression. As a specific instantiation of our model, we use discrete iterative optimization to obtain the unified hash codes across different modalities. Extensive experiments show that ACQH outperforms state-of-the-art methods on several diverse datasets.},
  archive      = {J_TMM},
  author       = {Lu Wang and Masoumeh Zareapoor and Jie Yang and Zhonglong Zheng},
  doi          = {10.1109/TMM.2021.3105824},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3665-3678},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Asymmetric correlation quantization hashing for cross-modal retrieval},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relation-aware compositional zero-shot learning for
attribute-object pair recognition. <em>TMM</em>, <em>24</em>, 3652–3664.
(<a href="https://doi.org/10.1109/TMM.2021.3104411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel model for recognizing images with composite attribute-object concepts, notably for composite concepts that are unseen during model training. We aim to explore the three key properties required by the task — relation-aware, consistent, and decoupled—to learn rich and robust features for primitive concepts that compose attribute-object pairs. To this end, we propose the Blocked Message Passing Network (BMP-Net). The model consists of two modules. The concept module generates semantically meaningful features for primitive concepts, whereas the visual module extracts visual features for attributes and objects from input images. A message passing mechanism is used in the concept module to capture the relations between primitive concepts. Furthermore, to prevent the model from being biased towards seen composite concepts and reduce the entanglement between attributes and objects, we propose a blocking mechanism that equalizes the information available to the model for both seen and unseen concepts. Extensive experiments and ablation studies on two benchmarks show the efficacy of the proposed model.},
  archive      = {J_TMM},
  author       = {Ziwei Xu and Guangzhi Wang and Yongkang Wong and Mohan S. Kankanhalli},
  doi          = {10.1109/TMM.2021.3104411},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3652-3664},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Relation-aware compositional zero-shot learning for attribute-object pair recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Style normalization and restitution for domain
generalization and adaptation. <em>TMM</em>, <em>24</em>, 3636–3651. (<a
href="https://doi.org/10.1109/TMM.2021.3104379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many computer vision applications, the learned models usually have high performance on the training datasets but suffer from significant performance degradation when deployed in new environments, where there are usually style differences between the training images and the testing images. For high-level vision tasks, an effective domain generalizable model is expected to be able to learn feature representations that are both generalizable and discriminative. In this paper, we design a novel Style Normalization and Restitution module (SNR) to simultaneously ensure high generalization and discrimination capability of the networks. In SNR, particularly, we filter out the style variations ( e.g ., illumination, color contrast) by performing Instance Normalization (IN) to obtain style normalized features, where the discrepancy among different samples/domains is reduced. However, such a process is task-ignorant and inevitably removes some task-relevant discriminative information, which may hurt the performance. To remedy this, we propose to distill task-relevant discriminative features from the residual ( i . e ., the difference between the original feature and the style normalized feature) and add them back to the network to ensure high discrimination. Moreover, for better disentanglement, we enforce a dual restitution loss constraint to encourage the better separation of task-relevant and task-irrelevant features. We validate the effectiveness of our SNR on different vision tasks, including classification, semantic segmentation, and object detection. Experiments demonstrate that our SNR is capable of improving the performance of networks for domain generalization (DG) and unsupervised domain adaptation (UDA).},
  archive      = {J_TMM},
  author       = {Xin Jin and Cuiling Lan and Wenjun Zeng and Zhibo Chen},
  doi          = {10.1109/TMM.2021.3104379},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3636-3651},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Style normalization and restitution for domain generalization and adaptation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised temporal action proposal generation via
exploiting 2-d proposal map. <em>TMM</em>, <em>24</em>, 3624–3635. (<a
href="https://doi.org/10.1109/TMM.2021.3104398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action proposal generation aims to generate temporal video segments containing human actions in untrimmed videos, which is always a preliminary for such video understanding tasks as action localization and temporally description grounding, etc . Fully-supervised solutions, though proven to be effective, suffer much from heavy data annotation overhead. To address this problem, this paper focuses on a rarely investigated yet practical problem of semi-supervised learning for temporal action proposal generation. Firstly, we propose a Proposal Map oriented Mean-Teacher (PM-MT) model, which can use both labeled and unlabeled data for end-to-end model training. Secondly, a Suppression-and-Re-Generation (SRG) strategy is designed to generate high-quality pseudo labels for unlabeled data, which are then used to finetune the model. Extensive experiments demonstrate the effectiveness of our proposed method, by achieving the state-of-the-art results on two public benchmark datatsets on the task of semi-supervised action proposal generation and outperforming fully-supervised learning methods with only a portion of labeled data.},
  archive      = {J_TMM},
  author       = {Weining Wang and Tianwei Lin and Dongliang He and Fu Li and Shilei Wen and Liang Wang and Jing Liu},
  doi          = {10.1109/TMM.2021.3104398},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3624-3635},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semi-supervised temporal action proposal generation via exploiting 2-D proposal map},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attribute-aware feature encoding for object recognition and
segmentation. <em>TMM</em>, <em>24</em>, 3611–3623. (<a
href="https://doi.org/10.1109/TMM.2021.3103605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing multi-task models for object recognition and segmentation have verified the effectiveness of joint optimization of two semantic tasks. However, learning discriminative representations with insufficient training data and redundant contextual information from the background remains challenging. Semantic attributes are designed as powerful and informative mid-level features that 1) share information across categories to model the interclass correlation and that 2) can be localized in the object region to benefit foreground extraction. This paper introduces a novel attribute-aware feature encoding (AFE) module to a multi-task network for object recognition and segmentation with the aim of improving both semantic tasks by regularizing feature encoding with auxiliary attribute learning. Intuitively, attribute learning in our method not only provides extra supervision signals to capture interclass correlation in object classification but also refines the output of object segmentation via weakly supervised attribute localization. The experimental results on two public benchmarks show that our method yields remarkable improvement in both semantic tasks and auxiliary attribute estimation over existing methods.},
  archive      = {J_TMM},
  author       = {Shu Yang and Yaowei Wang and Ke Chen and Wei Zeng and Zesong Fei},
  doi          = {10.1109/TMM.2021.3103605},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3611-3623},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Attribute-aware feature encoding for object recognition and segmentation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low quality and recognition of image content. <em>TMM</em>,
<em>24</em>, 3595–3610. (<a
href="https://doi.org/10.1109/TMM.2021.3103394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessment of visual encryption of video and image content requires a reliable estimation of content recognizability and low quality. As pointed out in the literature, current methods are insufficient and research into this topic, as well as into the relation between low quality and recognizability, is still lacking. This lack of research is primarily due to a lack of data. To improve on the status-quo we have taken a recognizability database and performed a subjective quality evaluation on a subset of the images. This gives us a new database with both subjective recognizability and quality information and allows to delve into the relation between low quality and recognizability. We analyze the relationship between quality and recognizability as well as the predictive quality of state of the art visual quality indices. We show that the visual quality indices are poor indicators for the estimation of recognizability. Furthermore, we show that they must be a poor fit because of the disparity between two distinct perceptual tasks: quality and recognizability.},
  archive      = {J_TMM},
  author       = {Heinz Hofbauer and Florent Autrusseau and Andreas Uhl},
  doi          = {10.1109/TMM.2021.3103394},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3595-3610},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Low quality and recognition of image content},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Families in wild multimedia: A multimodal database for
recognizing kinship. <em>TMM</em>, <em>24</em>, 3582–3594. (<a
href="https://doi.org/10.1109/TMM.2021.3103074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kinship, a soft biometric detectable in media, is fundamental for a myriad of use-cases. Despite the difficulty of detecting kinship, annual data challenges using still-images have consistently improved performances and attracted new researchers. Now, systems reach performance levels unforeseeable a decade ago, closing in on performances acceptable to deploy in practice. Like other biometric tasks, we expect systems can receive help from other modalities. We hypothesize that adding modalities to Families In the Wild (FIW), which has only still-images, will improve performance. Thus, to narrow the gap between research and reality and enhance the power of kinship recognition systems, we extend FIW with multimedia (MM) data ( i . e ., video, audio, and text captions). Specifically, we introduce the first publicly available multi-task MM kinship dataset. To build FIW in Multimedia (FIW MM), we developed machinery to automatically collect, annotate, and prepare the data, requiring minimal human input and no financial cost. The proposed MM corpus allows the problem statements to be more realistic template-based protocols. We show significant improvements in all benchmarks with the added modalities. The results highlight edge cases to inspire future research with different areas of improvement. FIW MM supplies the data needed to increase the potential of automated systems to detect kinship in MM. It also allows experts from diverse fields to collaborate in novel ways.},
  archive      = {J_TMM},
  author       = {Joseph P. Robinson and Zaid Khan and Yu Yin and Ming Shao and Yun Fu},
  doi          = {10.1109/TMM.2021.3103074},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3582-3594},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Families in wild multimedia: A multimodal database for recognizing kinship},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning-based quality assessment for image
super-resolution. <em>TMM</em>, <em>24</em>, 3570–3581. (<a
href="https://doi.org/10.1109/TMM.2021.3102401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image Super-Resolution (SR) techniques improve visual quality by enhancing the spatial resolution of images. Quality evaluation metrics play a critical role in comparing and optimizing SR algorithms, but current metrics achieve only limited success, largely due to the lack of large-scale quality databases, which are essential for learning accurate and robust SR quality metrics. In this work, we first build a large-scale SR image database using a novel semi-automatic labeling approach, which allows us to label a large number of images with manageable human workload. The resulting SR Image quality database with Semi-Automatic Ratings (SISAR), so far the largest of SR-IQA database, contains 12 600 images of 100 natural scenes. We train an end-to-end Deep Image SR Quality (DISQ) model by employing two-stream Deep Neural Networks (DNNs) for feature extraction, followed by a feature fusion network for quality prediction. Experimental results demonstrate that the proposed method outperforms state-of-the-art metrics and achieves promising generalization performance in cross-database tests. The SISAR database and DISQ model will be made publicly available to facilitate reproducible research.},
  archive      = {J_TMM},
  author       = {Tiesong Zhao and Yuting Lin and Yiwen Xu and Weiling Chen and Zhou Wang},
  doi          = {10.1109/TMM.2021.3102401},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3570-3581},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning-based quality assessment for image super-resolution},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). APMC: Adjacent pixels based measurement coding system for
compressively sensed images. <em>TMM</em>, <em>24</em>, 3558–3569. (<a
href="https://doi.org/10.1109/TMM.2021.3102394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressed sensing is now regarded as an effective method for dimension reduction during signal acquisition. One significant, yet under-addressed issue regarding the transmission of compressively sensed images is how to compress and code measurements, the output of a compressed sensing sensor. As the spatially adjacent correlation in the measurement domain is weak, conventional encoding algorithms cannot be applied directly for measurements. In this paper, we propose the adjacent pixels based measurement coding system (APMC) to generate compressed image bit-streams. Firstly, an adjacent pixels based measurement matrix (APMM) is applied to embed the pixel-domain boundary information of each block to the measurement domain. By adopting APMM, the pixel-domain information can be efficiently used for measurement-domain intra prediction. Moreover, to avoid the interference of pixels that are far apart and achieve a high prediction accuracy, we employ boundary measurements of neighboring blocks as references for prediction. Finally, we propose a rate control algorithm to process the residuals between measurements and predictions, to generate a coded bit sequence for transmitting. Experimental results demonstrated superiority in rate-distortion performance and bandwidth costs in transmitting as compared to previous schemes. Compared to the state-of-the-art, this work achieves a 24% decrease in bitrate and a 1.68 dB increase in Peak Signal-to-Noise Ratio (PSNR) on average.},
  archive      = {J_TMM},
  author       = {Rentao Wan and Jinjia Zhou and Bowen Huang and Hui Zeng and Yibo Fan},
  doi          = {10.1109/TMM.2021.3102394},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3558-3569},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {APMC: Adjacent pixels based measurement coding system for compressively sensed images},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive semantic-spatio-temporal graph convolutional
network for lip reading. <em>TMM</em>, <em>24</em>, 3545–3557. (<a
href="https://doi.org/10.1109/TMM.2021.3102433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this work is to recognize words, phrases, and sentences being spoken by a talking face without given the audio. Current deep learning approaches for lip reading focus on exploring the appearance and optical flow information of videos. However, these methods do not fully exploit the characteristics of lip motion. In addition to appearance and optical flow, the mouth contour deformation usually conveys significant information that is complementary to others. However, the modeling of dynamic mouth contour has received little attention than that of appearance and optical flow. In this work, we propose a novel model of dynamic mouth contours called Adaptive Semantic-Spatio-Temporal Graph Convolution Network (ASST-GCN), to go beyond previous methods by automatically learning both the spatial and temporal information from videos. To combine the complementary information from appearance and mouth contour, a two-stream visual front-end network is proposed. Experimental results demonstrate that the proposed method significantly outperforms the state-of-the-art lip reading methods on several large-scale lip reading benchmarks.},
  archive      = {J_TMM},
  author       = {Changchong Sheng and Xinzhong Zhu and Huiying Xu and Matti Pietikäinen and Li Liu},
  doi          = {10.1109/TMM.2021.3102433},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3545-3557},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive semantic-spatio-temporal graph convolutional network for lip reading},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep metric learning with manifold class variability
analysis. <em>TMM</em>, <em>24</em>, 3533–3544. (<a
href="https://doi.org/10.1109/TMM.2021.3101944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In deep metric learning (DML) techniques, understanding both the local and global characteristics of embedding space is essential. However, conventional DML techniques have two limitations as follows: First, Euclidean distance-based metrics never imply global information such as class variability because they only depend on the physical distance of samples. Second, they assume that the embedding space is simply a vector space which cannot represent complex data features. Therefore, we propose a novel loss function which can fully utilize characteristics of embedding space by using discriminant analysis and nonlinear mapping. With theoretical analysis, the superior performance of the proposed method is verified for the fine-grained retrieval datasets such as Cars196, CUB200-2011, Stanford online products, and In-shop clothes. Source code is available at https://github.com/kdhht2334/MCVA .},
  archive      = {J_TMM},
  author       = {Dae Ha Kim and Byung Cheol Song},
  doi          = {10.1109/TMM.2021.3101944},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3533-3544},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep metric learning with manifold class variability analysis},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive label-aware graph convolutional networks for
cross-modal retrieval. <em>TMM</em>, <em>24</em>, 3520–3532. (<a
href="https://doi.org/10.1109/TMM.2021.3101642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cross-modal retrieval task has raised continuous attention in recent years with the increasing scale of multi-modal data, which has broad application prospects including multimedia data management and intelligent search engine. Most existing methods mainly project data of different modalities into a common representation space where label information is often exploited to distinguish samples from different semantic categories. However, they typically treat each label as an independent individual and ignore the underlying semantic structure of labels. In this paper, we propose an end-to-end adaptive label-aware graph convolutional network (ALGCN) by designing both the instance representation learning branch and the label representation learning branch, which can obtain modality-invariant and discriminative representations for cross-modal retrieval. Firstly, we construct an instance representation learning branch to transform instances of different modalities into a common representation space. Secondly, we adopt Graph Convolutional Network (GCN) to learn inter-dependent classifiers in the label representation learning branch. In addition, a novel adaptive correlation matrix is proposed to efficiently explore and preserve the semantic structure of labels in a data-driven manner. Together with a robust self-supervision loss for GCN, the GCN model can be supervised to learn an effective and robust correlation matrix for feature propagation. Comprehensive experimental results on three benchmark datasets, NUS-WIDE, MIRFlickr and MS-COCO, demonstrate the superiority of ALGCN, compared with the state-of-the-art methods in cross-modal retrieval.},
  archive      = {J_TMM},
  author       = {Shengsheng Qian and Dizhan Xue and Quan Fang and Changsheng Xu},
  doi          = {10.1109/TMM.2021.3101642},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3520-3532},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive label-aware graph convolutional networks for cross-modal retrieval},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Harmonious textual layout generation over natural images via
deep aesthetics learning. <em>TMM</em>, <em>24</em>, 3416–3428. (<a
href="https://doi.org/10.1109/TMM.2021.3097900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic typography is important because it helps designers avoid highly repetitive tasks and amateur users achieve high-quality textual layout designs. However, there are often many parameters and complicated aesthetic rules that need to be adjusted in automatic typography work. In this paper, we propose an efficient deep aesthetics learning approach to generate harmonious textual layout over natural images, which can be decomposed into two stages, saliency-aware text region proposal and aesthetics-based textual layout selection. Our method incorporates both semantic features and visual perception principles. First, we propose a semantic visual saliency detection network combined with a text region proposal algorithm to generate candidate text anchors with various positions and sizes. Second, a discriminative deep aesthetics scoring model is developed to assess the aesthetic quality of the candidate textual layouts. We build a new Textual Layout Aesthetics dataset with dense annotations of each image and design a reasonable evaluation metric to compare our method with richer baselines. The results demonstrate that our method can generate harmonious textual layouts in various actual scenarios with better performance.},
  archive      = {J_TMM},
  author       = {Chenhui Li and Peiying Zhang and Changbo Wang},
  doi          = {10.1109/TMM.2021.3097900},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3416-3428},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Harmonious textual layout generation over natural images via deep aesthetics learning},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep modality assistance co-training network for
semi-supervised multi-label semantic decoding. <em>TMM</em>,
<em>24</em>, 3287–3299. (<a
href="https://doi.org/10.1109/TMM.2021.3104980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label semantic decoding is a challenging task with great scientific significance and application value. The existing methods mainly focus on label learning and ignore the amount of information contained in the sample itself, especially non-image sample, which may limit their performance. To address these issues, we propose a novel semi-supervised modality assistance co-training network, which utilizes image modality to assist non-image modality for multi-label learning. In real application, there are two thorny issues: (i) non-image modality tends to be missing owing to the difficulty in obtaining them; (ii) although the image modality is easy to obtain from the Internet, image label annotation is still time-consuming and expensive. Therefore, the proposed method utilizes a small number of paired &amp; labeled images and non-image modalities, and a large number of unpaired &amp; unlabeled images from web sources to improve results. It consists of the modality-specific feature generators, the feature translators and the label relationship network. Specifically, the modality-specific feature generators are used to generate different features (views) for each modality. Semantic translators are employed to capture the relationship between the paired modalities and impute the missing modality feature by using unpaired &amp; unlabeled images. Label relation network is a graph convolution network (GCN) aiming to capture the correlation between labels. To mine the information in unlabeled features, the co-training mechanism is considered. With this mechanism, we introduce a multi-view orthogonality constraint and a multi-label co-regularization constraint. Extensive experiments on three computer vision and neuroscience datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_TMM},
  author       = {Dan Li and Changde Du and Haibao Wang and Qiongyi Zhou and Huiguang He},
  doi          = {10.1109/TMM.2021.3104980},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3287-3299},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep modality assistance co-training network for semi-supervised multi-label semantic decoding},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Enhanced feature alignment for unsupervised domain
adaptation of semantic segmentation. <em>TMM</em>, <em>24</em>,
1042–1054. (<a href="https://doi.org/10.1109/TMM.2021.3106095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation for semantic segmentation aims to transfer knowledge from a labeled source domain to another unlabeled target domain. However, due to the label noise and domain mismatch, learning directly from source domain data tends to have poor performance. Though adversarial learning methods strive to reduce domain discrepancies by aligning feature distributions, traditional methods suffer from the training imbalance and feature distortion problems. Besides, due to the absence of target domain labels, the classifier is blind to features from the target domain during training. Consequently, the final classifier overfits the source domain features and usually fails to predict the structured outputs of the target domain. To alleviate these problems, we focus on enhancing the adversarial learning based feature alignment from three perspectives. First, a classification constrained discriminator is proposed to balance the adversarial training and alleviate the feature distortion problem. Next, to alleviate the classifier overfitting problem, self-training is collaboratively used to learn a domain robust classifier with target domain pseudo labels. Moreover, an efficient class centroid calculation module is proposed and the domain discrepancy is further reduced by aligning the feature centroids of the same class from different domains. Experimental evaluations on GTA5 $\rightarrow$ Cityscapes and SYNTHIA $\rightarrow$ Cityscapes demonstrate state-of-the-art results compared to other counterpart methods. The source code and models have been made available at. 1 1[Online]. Available: https://github.com/NUST-Machine-Intelligence-Laboratory/EFA.},
  archive      = {J_TMM},
  author       = {Tao Chen and Shui-Hua Wang and Qiong Wang and Zheng Zhang and Guo-Sen Xie and Zhenmin Tang},
  doi          = {10.1109/TMM.2021.3106095},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1042-1054},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhanced feature alignment for unsupervised domain adaptation of semantic segmentation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inter-domain adaptation label for data augmentation in
vehicle re-identification. <em>TMM</em>, <em>24</em>, 1031–1041. (<a
href="https://doi.org/10.1109/TMM.2021.3104141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle re-identification (Re-ID) methods often fail to achieve robust performance due to insufficient training data and domain diversities. Although state-of-the-art methods apply image-to-image translation or web data to achieve data augmentation, the construct of new datasets will not only introduce noise, but also undergo a mismatch issue with the source domain. Moreover, the label noise of cross-domain data in existing label distribution technologies cannot be alleviated. In this paper, a multi-domain joint learning with inter-domain adaptation label smoothing regularization (IALSR) is proposed using a semi-supervised learning framework. The overall framework consists of two parts. In one part, a multi-domain joint network (MJNet) is proposed to learn multiple vehicle attributes simultaneously. The output of the training model is employed to group several inter-domain subsets, which are regarded as different domains. To adapt to domain diversities, style transfer models are learned for each pair of subsets to generate free and rich data as a novel data augmentation approach. In the other part, IALSR, which preserves self-similarity and domain-transitivity, is designed to smooth the noise of style-transferred data. Upon our basis, we further introduce the web data to verify the superiority of the IALSR. The results of extensive experimental on two large-scale vehicle Re-ID datasets demonstrate that the proposed approach is superior to other state-of-the-art ones.},
  archive      = {J_TMM},
  author       = {Qi Wang and Weidong Min and Qing Han and Qian Liu and Cheng Zha and Haoyu Zhao and Zitai Wei},
  doi          = {10.1109/TMM.2021.3104141},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1031-1041},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Inter-domain adaptation label for data augmentation in vehicle re-identification},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast video saliency detection via maximally stable region
motion and object repeatability. <em>TMM</em>, <em>24</em>, 4458–4470.
(<a href="https://doi.org/10.1109/TMM.2021.3094356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion information is one important cue in unsupervised video salient object detection. In order to estimate motion in videos, most of the methods adopt time-consuming algorithms such as large displacement optical flow estimation (needs more than 8-40s with 640X480 size per frame), which leads to saliency detection with only 0.01-0.1 FPS speed and limits its application. In human visual system, the motion of one object is usually considered as a whole. Therefore, we need not compute the motion of each pixel. Instead, it is desirable to estimate the probability of each pixel belonging to a well identifiable object, which is proposed as maximally stable region ( MSR ) in recent work, and compute object-level motion. Motivated by this intuition, we firstly propose one fast object-level video motion model based on MSR , which only needs 49 ms for 640X480 size frame. Next, we present spatial-temporal boundary connectivity ( BndCon ) and spatial-temporal Minimum Barrier Distance ( MBD ) to estimate background probability and saliency. Then, we propose the repeatability saliency which means the frequency of the object recurs in all video sequences. Besides, we propose one simple yet effective method to combine our unsupervised method and deep learning model to further boost performance. Compared with the state-of-the-art unsupervised methods, our method shows significantly better performance with 12 FPS speed on normal CPU hardware.},
  archive      = {J_TMM},
  author       = {Xiaoming Huang and Yu-Jin Zhang},
  doi          = {10.1109/TMM.2021.3094356},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {4458-4470},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fast video saliency detection via maximally stable region motion and object repeatability},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CDFKD-MFS: Collaborative data-free knowledge distillation
via multi-level feature sharing. <em>TMM</em>, <em>24</em>, 4262–4274.
(<a href="https://doi.org/10.1109/TMM.2022.3192663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the compression and deployment of powerful deep neural networks (DNNs) on resource-limited edge devices to provide intelligent services have become attractive tasks. Although knowledge distillation (KD) is a feasible solution for compression, its requirement on the original dataset raises privacy concerns. In addition, it is common to integrate multiple pretrained models to achieve satisfactory performance. How to compress multiple models into a tiny model is challenging, especially when the original data are unavailable. To tackle this challenge, we propose a framework termed collaborative data-free knowledge distillation via multi-level feature sharing (CDFKD-MFS), which consists of a multi-header student module, an asymmetric adversarial data-free KD module, and an attention-based aggregation module. In this framework, the student model equipped with a multi-level feature-sharing structure learns from multiple teacher models and is trained together with a generator in an asymmetric adversarial manner. When some real samples are available, the attention module adaptively aggregates predictions of the student headers, which can further improve performance. We conduct extensive experiments on three popular computer visual datasets. In particular, compared with the most competitive alternative, the accuracy of the proposed framework is 1.18% higher on the CIFAR-100 dataset, 1.67% higher on the Caltech-101 dataset, and 2.99% higher on the mini-ImageNet dataset.},
  archive      = {J_TMM},
  author       = {Zhiwei Hao and Yong Luo and Zhi Wang and Han Hu and Jianping An},
  doi          = {10.1109/TMM.2022.3192663},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {4262-4274},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CDFKD-MFS: Collaborative data-free knowledge distillation via multi-level feature sharing},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MIG-net: Multi-scale network alternatively guided by
intensity and gradient features for depth map super-resolution.
<em>TMM</em>, <em>24</em>, 3506–3519. (<a
href="https://doi.org/10.1109/TMM.2021.3100766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The studies of previous decades have shown that the quality of depth maps can be significantly lifted by introducing the guidance from intensity images describing the same scenes. With the rising of deep convolutional neural network, the performance of guided depth map super-resolution is further improved. The variants always consider deep structure, optimized gradient flow and feature reusing. Nevertheless, it is difficult to obtain sufficient and appropriate guidance from intensity features without any prior. In fact, features in the gradient domain, e.g., edges, present strong correlations between the intensity image and the corresponding depth map. Therefore, the guidance in the gradient domain can be more efficiently explored. In this paper, the depth features are iteratively upsampled by 2×. In each upsampling stage, the low-quality depth features and the corresponding gradient features are iteratively refined by the guidance from the intensity features via two parallel streams. Then, to make full use of depth features in the image and gradient domains, the depth features and gradient features are alternatively complemented with each other. Compared with state-of-the-art counterparts, the sufficient experimental results show improvements according to the objective and subjective assessments. The code is available at https://github.com/Yifan-Zuo/MIG-net-gradient_guided_depth_enhancement .},
  archive      = {J_TMM},
  author       = {Yifan Zuo and Hao Wang and Yuming Fang and Xiaoshui Huang and Xiwu Shang and Qiang Wu},
  doi          = {10.1109/TMM.2021.3100766},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3506-3519},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MIG-net: Multi-scale network alternatively guided by intensity and gradient features for depth map super-resolution},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Game theory based dynamic adaptive video streaming for
multi-client over NDN. <em>TMM</em>, <em>24</em>, 3491–3505. (<a
href="https://doi.org/10.1109/TMM.2021.3100768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of Dynamic Adaptive Streaming (DAS) in multi-client scenarios can be improved by taking advantage of the aggregation capability of Named Data Networking (NDN). In this paper, we propose a client-side game theory based (GB) ABR algorithm for NDN that can achieve proactive aggregation of requests among clients as much as possible without requiring coordinating with other clients or scheduling by a central controller. We model the interaction between a DAS client and network as an incomplete information non-cooperative game. Then, this game is transformed into a complete but imperfect information game by Harsanyi transformation, and each client can issue an appropriate bitrate request by solving the Bayesian Nash Equilibrium (BNE) problem respectively. By designing the payoff function pair elaborately, the equilibrium point of the game can correspond to the situation that multiple clients issuing the same video bitrate request, that is, requests aggregation, which will reduce the repeated traffic and also achieve fairness. Compared with the existing solutions, through simulation and real-world experiments in multi-client video distribution scenarios, the GB algorithm outperforms the comparison algorithms in terms of overall Quality of Experience (QoE), fairness, and network bandwidth utilization, etc.},
  archive      = {J_TMM},
  author       = {Xiaobin Tan and Lei Xu and Jiawei Ni and Simin Li and Xiaofeng Jiang and Quan Zheng},
  doi          = {10.1109/TMM.2021.3100768},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3491-3505},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Game theory based dynamic adaptive video streaming for multi-client over NDN},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Speech driven talking face generation from a single image
and an emotion condition. <em>TMM</em>, <em>24</em>, 3480–3490. (<a
href="https://doi.org/10.1109/TMM.2021.3099900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual emotion expression plays an important role in audiovisual speech communication. In this work, we propose a novel approach to rendering visual emotion expression in speech-driven talking face generation. Specifically, we design an end-to-end talking face generation system that takes a speech utterance, a single face image, and a categorical emotion label as input to render a talking face video synchronized with the speech and expressing the conditioned emotion. Objective evaluation on image quality, audiovisual synchronization, and visual emotion expression shows that the proposed system outperforms a state-of-the-art baseline system. Subjective evaluation of visual emotion expression and video realness also demonstrates the superiority of the proposed system. Furthermore, we conduct a human emotion recognition pilot study using generated videos with mismatched emotions among the audio and visual modalities. Results show that humans respond to the visual modality more significantly than the audio modality on this task.},
  archive      = {J_TMM},
  author       = {Sefik Emre Eskimez and You Zhang and Zhiyao Duan},
  doi          = {10.1109/TMM.2021.3099900},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3480-3490},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Speech driven talking face generation from a single image and an emotion condition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards multi-domain face synthesis via domain-invariant
representations and multi-level feature parts. <em>TMM</em>,
<em>24</em>, 3469–3479. (<a
href="https://doi.org/10.1109/TMM.2021.3099297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain face synthesis plays a positive role in the real world. It is challenging to synthesize high-quality faces across multiple domains based on limited paired data because the multiple mappings between different domains may interfere with each other. Cognitive science investigates that the brain can recognize the same person with multiple different expressions by extracting invariant information on the face and we humans perceive instances by decomposing them into parts. Motivated by these cognition, we propose a unified semi-supervised framework for multi-domain face synthesis by extracting a domain-invariant representation and exploiting parts of multi-level features. Specifically, realized by adversarial training with additional ability to utilize domain-specific information, a encoder is trained to remove domain-specific information and extract the domain-invariant representation from multiple inputs. Then, we utilize the multi-level feature parts extracted from inputs and reconstructed faces via a pre-trained recognition model to ensure that the domain-invariant representation contains enough useful semantic information. we also utilize the feature parts extracted from inputs and limited paired data to compose pseudo features in target domain for supervising the synthesis, which makes our framework suitable for large amounts of unpaired training data. By exploiting this framework, we can achieve face synthesis between multiple domains using some paired data together with a large training database without ground truth target faces. Experimental results demonstrate our framework achieves great performances on qualitative and quantitative evaluations under both artificial and uncontrolled environments, and our framework has competitive performances in single translation compared with specialized methods for translation between two specific domains.},
  archive      = {J_TMM},
  author       = {Dawei Zhou and Nannan Wang and Chunlei Peng and Yi Yu and Xi Yang and Xinbo Gao},
  doi          = {10.1109/TMM.2021.3099297},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3469-3479},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards multi-domain face synthesis via domain-invariant representations and multi-level feature parts},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Entity-oriented multi-modal alignment and fusion network for
fake news detection. <em>TMM</em>, <em>24</em>, 3455–3468. (<a
href="https://doi.org/10.1109/TMM.2021.3098988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of social media enables fake news to be expressed in a multi-modal form, which is disseminated on various social platforms and brings harmful social impacts. To handle this challenge, the fake news detection task was proposed to examine whether false information is contained in multi-modal news. Existing methods exploit various approaches with cross-modal interaction and fusion, which have proven to be effective in detecting common fake news. However, although the description of multi-modal news is narrated around entities, the previously developed methods pay less attention to this characteristic. They do not explore its benefits to the detection task and underperform with respect to the detection of fake news that requires entity-centric comparisons. To make up for this omission, we explore a novel paradigm to detect fake news by aligning and fusing multi-modal entities and propose the Entity-oriented Multi-modal Alignment and Fusion network (EMAF). Our work adopts entity-centric cross-modal interaction, which can reserve semantic integrity and capture the details of multi-modal entities. Specifically, we design an Alignment module with the improved dynamic routing algorithm and introduce a Fusion module based on the comparison, the former aligns and captures the important entities and the latter compares and aggregates entity-centric features. Comparative experiments conducted on multiple public datasets, including Weibo, Twitter, and Reddit, reveal the superiority of the proposed EMAF method, and extensive analytical experiments demonstrate the effectiveness of our proposed modules.},
  archive      = {J_TMM},
  author       = {Peiguang Li and Xian Sun and Hongfeng Yu and Yu Tian and Fanglong Yao and Guangluan Xu},
  doi          = {10.1109/TMM.2021.3098988},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3455-3468},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Entity-oriented multi-modal alignment and fusion network for fake news detection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Subjective assessment experiments that recruit few observers
with repetitions (FOWR). <em>TMM</em>, <em>24</em>, 3442–3454. (<a
href="https://doi.org/10.1109/TMM.2021.3098450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that it is possible to characterize subject bias and variance in subjective assessment tests. Apparent differences among subjects can, for the most part, be explained by random factors. Building on that theory, we propose a subjective test design where four to six team members each rate the stimuli multiple times. The results are comparable to a high performing objective metric. This provides a quick and simple way to analyze new technologies and perform pre-tests for subjective assessment.},
  archive      = {J_TMM},
  author       = {Pablo Pérez and Lucjan Janowski and Narciso García and Margaret Pinson},
  doi          = {10.1109/TMM.2021.3098450},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3442-3454},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Subjective assessment experiments that recruit few observers with repetitions (FOWR)},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Anti-forensics for face swapping videos via adversarial
training. <em>TMM</em>, <em>24</em>, 3429–3441. (<a
href="https://doi.org/10.1109/TMM.2021.3098422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating falsified faces by artificial intelligence, widely known as DeepFake, has attracted attention worldwide since 2017. Given the potential threat brought by this novel technique, forensics researchers dedicated themselves to detect the video forgery. Except for exposing falsified faces, there could be extended research directions for DeepFake such as anti-forensics. It can disclose the vulnerability of current DeepFake forensics methods. Besides, it could also enable DeepFake videos as tactical weapons if the falsified faces are more subtle to be detected. In this paper, we propose a GAN model to behave as an anti-forensics tool. It features a novel architecture with additional supervising modules for enhancing image visual quality. Besides, a loss function is designed to improve the efficiency of the proposed model. After experimental evaluations, we show that the DeepFake forensics detectors are susceptible to attacks launched by the proposed method. Besides, the proposed method can efficiently produce anti-forensics videos in satisfying visual quality without noticeable artifacts. Compared with the other anti-forensics approaches, this is tremendous progress achieved for DeepFake anti-forensics. The attack launched by our proposed method can be truly regarded as DeepFake anti-forensics as it can fool detecting algorithms and human eyes simultaneously.},
  archive      = {J_TMM},
  author       = {Feng Ding and Guopu Zhu and Yingcan Li and Xinpeng Zhang and Pradeep K. Atrey and Siwei Lyu},
  doi          = {10.1109/TMM.2021.3098422},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3429-3441},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Anti-forensics for face swapping videos via adversarial training},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Suppressing biased samples for robust VQA. <em>TMM</em>,
<em>24</em>, 3405–3415. (<a
href="https://doi.org/10.1109/TMM.2021.3097502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing visual question answering (VQA) models strongly rely on language bias to answer questions, i.e., they always tend to fit question-answer pairs on the train split and perform poorly on the test spilt when the answer distributions are different. This behavior makes them hard to be applied in real scenarios. To reduce the language biases, previous studies mainly integrate modules to overcome language priors (ensemble-based methods) or generate additional training data to balance dataset biases (data-balanced methods). However, all the existing ensemble-based methods drop their accuracies on the VQA v2 dataset, while data-balanced methods may introduce new biases and cannot guarantee the quality of the generated data. In this paper, we propose a model-agnostic training scheme called Suppressing Biased Samples (SBS) to overcome language priors. SBS consists of two collaborative parts, i.e., a Data Classifier Module to divide the dataset into biased samples and unbiased samples by utilizing the similarity in the semantic space, and a Bias Penalty Module to suppress the biased samples to weaken their influence. As a new way of balancing data to address language bias, SBS overcomes the shortcomings of previous data-balanced methods. Experimental results show that our method can be merged into other bias-reduction methods and achieves a new state-of-the-art performance on the commonly used VQA-CP v2 dataset.},
  archive      = {J_TMM},
  author       = {Ninglin Ouyang and Qingbao Huang and Pijian Li and Yi Cai and Bin Liu and Ho-fung Leung and Qing Li},
  doi          = {10.1109/TMM.2021.3097502},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3405-3415},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Suppressing biased samples for robust VQA},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Targeted attack of deep hashing via prototype-supervised
adversarial networks. <em>TMM</em>, <em>24</em>, 3392–3404. (<a
href="https://doi.org/10.1109/TMM.2021.3097506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its powerful capability of representation learning and efficient computation, deep hashing has made significant progress in large-scale image retrieval. It has been recognized that deep neural networks are vulnerable to adversarial examples, which is a practical secure problem but seldom studied in deep hashing-based retrieval field. In this paper, we propose a novel prototype-supervised adversarial network (ProS-GAN), which formulates a flexible generative architecture for efficient and effective targeted hashing attack. To the best of our knowledge , this is one of the first generation-based methods to attack deep hashing networks. Generally, our proposed framework consists of three parts, i.e. , a PrototypeNet, a Generator and a Discriminator. Specifically, the designed PrototypeNet embeds the target label into the semantic representation and learns the prototype code as the category-level representative of the target label. Moreover, the semantic representation and the original image are jointly fed into the generator for flexible targeted attack. Particularly, the prototype code is adopted to supervise the generator to construct the targeted adversarial example by minimizing the Hamming distance between the hash code of the adversarial example and the prototype code. Furthermore, the generator fools the discriminator to simultaneously encourage the adversarial examples visually realistic and the semantic representation informative. Extensive experiments demonstrate that the proposed framework can efficiently produce adversarial examples with better targeted attack performance and transferability over state-of-the-art targeted attack methods of deep hashing. The source code is available at https://github.com/xunguangwang/ProS-GAN_Trans .},
  archive      = {J_TMM},
  author       = {Zheng Zhang and Xunguang Wang and Guangming Lu and Fumin Shen and Lei Zhu},
  doi          = {10.1109/TMM.2021.3097506},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3392-3404},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Targeted attack of deep hashing via prototype-supervised adversarial networks},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Annular-graph attention model for personalized sequential
recommendation. <em>TMM</em>, <em>24</em>, 3381–3391. (<a
href="https://doi.org/10.1109/TMM.2021.3097186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendations aim to predict the user’s next behaviors items based on their successive historical behaviors sequence. It has been widely applied in lots of online services. However, current sequential recommendations use the adjacent behaviors to capture the features of the sequence, ignoring the features among nonadjacent sequential items and the summarized features of the sequence. To address the above problems, in this paper, we propose an annular-graph attention based sequential recommendation (AGSR) model by exploring user’s long-term and short-term preferences for the personalized sequential recommendation. For user’s short-term preferences, AGSR builds an annular-graph on the sequence of user behavior. Then, AGSR proposes an annular-graph attention applying on the sub annular-graph to explore local features and applying annular-graph attention on entire annular-graph to explore the global features and the skip features. For user’s long-term preferences, the latent factor model are introduced in AGSR. The experimental results on two public datasets show that our model outperforms the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Junmei Hao and Yujie Dun and Guoshuai Zhao and Yuxia Wu and Xueming Qian},
  doi          = {10.1109/TMM.2021.3097186},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3381-3391},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Annular-graph attention model for personalized sequential recommendation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DualVGR: A dual-visual graph reasoning unit for video
question answering. <em>TMM</em>, <em>24</em>, 3369–3380. (<a
href="https://doi.org/10.1109/TMM.2021.3097171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video question answering is a challenging task, which requires agents to be able to understand rich video contents and perform spatial-temporal reasoning. However, existing graph-based methods fail to perform multi-step reasoning well, neglecting two properties of VideoQA: (1) Even for the same video, different questions may require different amount of video clips or objects to infer the answer with relational reasoning; (2) During reasoning, appearance and motion features have complicated interdependence which are correlated and complementary to each other. Based on these observations, we propose a Dual-Visual Graph Reasoning Unit (DualVGR) which reasons over videos in an end-to-end fashion. The first contribution of our DualVGR is the design of an explainable Query Punishment Module, which can filter out irrelevant visual features through multiple cycles of reasoning. The second contribution is the proposed Video-based Multi-view Graph Attention Network, which captures the relations between appearance and motion features. Our DualVGR network achieves state-of-the-art performance on the benchmark MSVD-QA and SVQA datasets, and demonstrates competitive results on benchmark MSRVTT-QA datasets. Our code is available at https://github.com/MM-IR/DualVGR-VideoQA .},
  archive      = {J_TMM},
  author       = {Jianyu Wang and Bing-Kun Bao and Changsheng Xu},
  doi          = {10.1109/TMM.2021.3097171},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3369-3380},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DualVGR: A dual-visual graph reasoning unit for video question answering},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust character labeling in movie videos: Data resources
and self-supervised feature adaptation. <em>TMM</em>, <em>24</em>,
3355–3368. (<a href="https://doi.org/10.1109/TMM.2021.3096155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust face clustering is a vital step in enabling computational understanding of visual character portrayal in media. Face clustering for long-form content is challenging because of variations in appearance and lack of supporting large-scale labeled data. Our work in this paper focuses on two key aspects of this problem: the lack of domain-specific training or benchmark datasets, and adapting face embeddings learned on web images to long-form content, specifically movies. First, we present a dataset of over 169000 face tracks curated from 240 Hollywood movies with weak labels on whether a pair of face tracks belong to the same or a different character. We propose an offline algorithm based on nearest-neighbor search in the embedding space to mine hard-examples from these tracks. We then investigate triplet-loss and multiview correlation-based methods for adapting face embeddings to hard-examples. Our experimental results highlight the usefulness of weakly labeled data for domain-specific feature adaptation. Overall, we find that multiview correlation-based adaptation yields more discriminative and robust face embeddings. Its performance on downstream face verification and clustering tasks is comparable to that of the state-of-the-art results in this domain. We also present the SAIL-Movie Character Benchmark corpus developed to augment existing benchmarks. It consists of racially diverse actors and provides face-quality labels for subsequent error analysis. We hope that the large-scale datasets developed in this work can further advance automatic character labeling in videos. All resources are available freely at https://sail.usc.edu/~ccmi/multiface .},
  archive      = {J_TMM},
  author       = {Krishna Somandepalli and Rajat Hebbar and Shrikanth Narayanan},
  doi          = {10.1109/TMM.2021.3096155},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3355-3368},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust character labeling in movie videos: Data resources and self-supervised feature adaptation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-attention-based multiscale feature learning optical
flow with occlusion feature map prediction. <em>TMM</em>, <em>24</em>,
3340–3354. (<a href="https://doi.org/10.1109/TMM.2021.3096083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Even though optical flow approaches based on convolutional neural networks have achieved remarkable performance with respect to both accuracy and efficiency, large displacements and motion occlusions remain challenges for most existing learning-based models. To address the abovementioned issues, we propose in this paper a self-attention-based multiscale feature learning optical flow computation method with occlusion feature map prediction. First, we exploit a self-attention mechanism-based multiscale feature learning module to compensate for large displacement optical flows, and the presented module is able to capture long-range dependencies from the input frames. Second, we design a simple but effective self-learning module to acquire an occlusion feature map, in which the predicted occlusion map is utilized to correct the optical flow estimation in occluded areas. Third, we explore a hybrid loss function that integrates the photometric and smoothness losses into the classical endpoint error (EPE)-based loss to ensure the accuracy and robustness of the presented network. Finally, we compare the proposed method with some state-of-the-art approaches using the MPI-Sintel and KITTI test databases. The experimental results demonstrate that the proposed method achieved competitive performance with respect to both accuracy and robustness, and it produced the better results compared to other methods under large displacements and motion occlusions.},
  archive      = {J_TMM},
  author       = {Congxuan Zhang and Zhongkai Zhou and Zhen Chen and Weiming Hu and Ming Li and Shaofeng Jiang},
  doi          = {10.1109/TMM.2021.3096083},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3340-3354},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Self-attention-based multiscale feature learning optical flow with occlusion feature map prediction},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identity-aware facial expression recognition via deep metric
learning based on synthesized images. <em>TMM</em>, <em>24</em>,
3327–3339. (<a href="https://doi.org/10.1109/TMM.2021.3096068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person-dependent facial expression recognition has received considerable research attention in recent years. Unfortunately, different identities can adversely influence recognition accuracy, and the recognition task becomes challenging. Other adverse factors, including limited training data and improper measures of facial expressions, can further contribute to the above dilemma. To solve these problems, a novel identity-aware method is proposed in this study. Furthermore, this study also represents the first attempt to fulfill the challenging person-dependent facial expression recognition task based on deep metric learning and facial image synthesis techniques. Technically, a StarGAN is incorporated to synthesize facial images depicting different but complete basic emotions for each identity to augment the training data. Then, a deep-convolutional-neural-network-based network is employed to automatically extract latent features from both real facial images and all synthesized facial images. Next, a Mahalanobis metric network trained based on extracted latent features outputs a learned metric that measures facial expression differences between images, and the recognition task can thus be realized. Extensive experiments based on several well-known publicly available datasets are carried out in this study for performance evaluations. Person-dependent datasets, including CK+, Oulu (all 6 subdatasets), MMI, ISAFE, ISED, etc., are all incorporated. After comparing the new method with several popular or state-of-the-art facial expression recognition methods, its superiority in person-dependent facial expression recognition can be proposed from a statistical point of view.},
  archive      = {J_TMM},
  author       = {Wei Huang and Siyuan Zhang and Peng Zhang and Yufei Zha and Yuming Fang and Yanning Zhang},
  doi          = {10.1109/TMM.2021.3096068},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3327-3339},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Identity-aware facial expression recognition via deep metric learning based on synthesized images},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Infrared and visible image fusion based on deep
decomposition network and saliency analysis. <em>TMM</em>, <em>24</em>,
3314–3326. (<a href="https://doi.org/10.1109/TMM.2021.3096088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional image fusion focuses on selecting an effective decomposition approach to extract representative features from the source image and attempts to find appropriate fusion rules to merge extracted features respectively. However, the existing image decomposition tools are mostly based on kernels or global energy-optimized functions limiting the performance of the wide range of image contents. This paper proposes a novel infrared and visible image fusion method based on deep decomposition network and saliency analysis (named DDNSA). First, the modified residual dense network (MRDN) is trained with a publicly available dataset to learn the decomposition process. Second, the structure and texture features of source images are separated by the trained decomposition network. Then, according to the characteristics of the above features, we construct the combination of local and global saliency maps by using stacked sparse autoencoder and visual saliency mechanism to fuse the structural features. Besides, we propose a bi-direction edge-strength fusion strategy for merging the texture features. Finally, the resultant image is reconstructed by combining the fused structure and texture features. The experimental results confirm that our proposed method outperforms the state-of-the-art methods in both visual perception and objective evaluation.},
  archive      = {J_TMM},
  author       = {Lihua Jian and Rakiba Rayhana and Ling Ma and Shaowu Wu and Zheng Liu and Huiqin Jiang},
  doi          = {10.1109/TMM.2021.3096088},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3314-3326},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Infrared and visible image fusion based on deep decomposition network and saliency analysis},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatio-temporal pain estimation network with measuring
pseudo heart rate gain. <em>TMM</em>, <em>24</em>, 3300–3313. (<a
href="https://doi.org/10.1109/TMM.2021.3096080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pain is a significant indicator that shows people are suffering from an unwell experience and its automatic estimation has attracted much interest in recent years. Of late, most estimation methods are designed to capture the dynamic pain information from visual signals while a few physiological-signal based methods can provide extra potential cues to analyze the pain more accurately. However, it is still challenging to capture the physiological data from patients as it requires contact devices and patients’ cooperation. In this paper, we propose to leverage the pseudo physiological information by generating new modal data from the original visual videos and jointly estimating the pain by an end-to-end network. To extract the representations from bi-modal data, we design a spatio-temporal pain estimation network, which employs a dual-branch framework for extracting pain-aware visual and pseudo physiological features separately and fuses the features in a probabilistic way. The inherent vital sign, i.e., heart rate gain (HRG), from pseudo physiological information can be utilized as an auxiliary signal and integrated with the visual pain estimation framework. Moreover, specially-designed 3D convolution filters and attention structures are employed to extract spatio-temporal features for both branches. To use the HRG as an auxiliary way for pain estimation, we propose a probabilistic inference model by jointly considering the visual branch and physiological branch, which makes our model estimate the pain comprehensively. Experiments on two publicly-available datasets show the effectiveness of introducing the pseudo modality, and the proposed method can outperform the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Dong Huang and Xiaoyi Feng and Haixi Zhang and Zitong Yu and Jinye Peng and Guoying Zhao and Zhaoqiang Xia},
  doi          = {10.1109/TMM.2021.3096080},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3300-3313},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spatio-temporal pain estimation network with measuring pseudo heart rate gain},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly supervised temporal adjacent network for language
grounding. <em>TMM</em>, <em>24</em>, 3276–3286. (<a
href="https://doi.org/10.1109/TMM.2021.3096087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal language grounding (TLG) is a fundamental and challenging problem for vision and language understanding. Existing methods mainly focus on fully supervised setting with temporal boundary labels for training, which, however, suffers expensive cost of annotation. In this work, we are dedicated to weakly supervised TLG, where multiple description sentences are given to an untrimmed video without temporal boundary labels. In this task, it is critical to learn a strong cross-modal semantic alignment between sentence semantics and visual content. To this end, we introduce a novel weakly supervised temporal adjacent network (WSTAN) for temporal language grounding. Specifically, WSTAN learns cross-modal semantic alignment by exploiting temporal adjacent network in a multiple instance learning (MIL) paradigm, with a whole description paragraph as input. Moreover, we integrate a complementary branch into the framework, which explicitly refines the predictions with pseudo supervision from the MIL stage. An additional self-discriminating loss is devised on both the MIL branch and the complementary branch, aiming to enhance semantic discrimination by self-supervising. Extensive experiments are conducted on three widely used benchmark datasets, i.e. , ActivityNet-Captions, Charades-STA, and DiDeMo, and the results demonstrate the effectiveness of our approach.},
  archive      = {J_TMM},
  author       = {Yuechen Wang and Jiajun Deng and Wengang Zhou and Houqiang Li},
  doi          = {10.1109/TMM.2021.3096087},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3276-3286},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Weakly supervised temporal adjacent network for language grounding},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gait recognition based on local graphical skeleton
descriptor with pairwise similarity network. <em>TMM</em>, <em>24</em>,
3265–3275. (<a href="https://doi.org/10.1109/TMM.2021.3095809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition aims to identify a human through a walking sequence. It is a challenging task in computer vision since monocular camera loses most of the 3D information. Previous works described gait features with the contours of shape or the global geometrical characters of skeleton. So little work is researched on the local patterns of gait skeleton. In this paper, to resist the dress changes and speed changes, a Local Graphical Skeleton Descriptor (LGSD) is proposed to describe both the inner and intra local graphical patterns of a human gait skeleton. The gait features from the same or different identities are paired up and a Pairwise Similarity Network (PSN) is proposed to maximize the similarity of True matched pairs and minimize the similarity of False matched pairs. The contributions of our method are: 1) LGSD is proposed to describe human gait by computing four novel local geometrical patterns of skeleton sequences, which makes use of the intuitive cognition of gait based on the prior knowledge of mankind. 2) PSN is implemented by a two-stream CNN structure to build the gait model, which fused two popular gait recognition strategies. 3) The robustness of our method to dress changes and speed changes is proved on the public datasets. We have also achieved some state-of-the-art results on these datasets. The proposed method is examined on three public gait datasets which have RGB or infrared frames for evaluation: the CASIA-B dataset, the NLPR gait database, and the CASIA-C dataset. The performers in these datasets are walking under different views, speeds or dresses. The results are further compared with previous approaches to confirm the effectiveness and the advantages of our method.},
  archive      = {J_TMM},
  author       = {Ke Xu and Xinghao Jiang and Tanfeng Sun},
  doi          = {10.1109/TMM.2021.3095809},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3265-3275},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Gait recognition based on local graphical skeleton descriptor with pairwise similarity network},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quality evaluation of holographic images coded with standard
codecs. <em>TMM</em>, <em>24</em>, 3256–3264. (<a
href="https://doi.org/10.1109/TMM.2021.3096059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a larger interest in the different plenoptic formats, including digital holograms, has emerged. Aside from other challenges that several steps of the holographic pipeline, from digital acquisition to display, have to face, visual quality assessment of compressed holograms is particularly demanding due to the distinct nature of this 3D image modality when compared to regular 2D imaging. There are few studies on holographic data quality assessment, particularly with respect to perceptual effects of lossy compression. This work aims to study the quality evaluation of digital hologram reconstructions, presented on regular 2D displays, in the presence of compression distortions. As there is no established or generally agreed compression methodology for digital holograms compression on the hologram plane with available implementations, a set of state-of-the-art compression codecs, namely HEVC, AV1, and JPEG2000, were used for compression of the digital holograms on the object plane. Both computer generated and optically generated holograms were considered. Two subjective tests were conducted to evaluate distortions caused by compression. The first subjective test was conducted on the reconstructed amplitude images of central views, while the second test was conducted on pseudo-videos generated from the reconstructed amplitudes of different views. The subjective quality assessment was based on mean opinion scores. A selection of objective quality metrics was evaluated, and their correlations with mean opinion scores were computed. The VIFp metrics appeared to have the highest correlation.},
  archive      = {J_TMM},
  author       = {Hadi Amirpour and Antonio M. G. Pinheiro and Elsa Fonseca and Mohammad Ghanbari and Manuela Pereira},
  doi          = {10.1109/TMM.2021.3096059},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3256-3264},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Quality evaluation of holographic images coded with standard codecs},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PR-RL: Portrait relighting via deep reinforcement learning.
<em>TMM</em>, <em>24</em>, 3240–3255. (<a
href="https://doi.org/10.1109/TMM.2021.3096009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a portrait relighting method based on deep reinforcement learning (called PR-RL). Our PR-RL model could conduct portrait relighting by sequentially predicting local light editing strokes, and use strokes to conduct dodge and burn operations on the image lightness, simulating image editing by artists using brush strokes. Reinforcement learning with Deep Deterministic Policy Gradient is introduced to design our PR-RL model, defining the action (stroke parameters) in a continuous space, through which a reward can be designed to guide the agent to learn and relight a portrait image like an artist. To optimize the relighting effect, we further enable the reward to be location relevant and hence a coarse-to-fine strategy can be applied to select corresponding actions and maximize the performance of the proposed method. In comparison with the existing efforts, our proposed PR-RL method is locally effective, scale-invariant and interpretable. We apply the proposed method to tasks of portrait relighting based on both SH-lighting and reference images. The experiments show that our PR-RL method outperforms state-of-the-art methods in generating locally effective and interpretable high resolution relighting results for wild portrait images.},
  archive      = {J_TMM},
  author       = {Xiaoyan Zhang and Yukai Song and Zhuopeng Li and Jianmin Jiang},
  doi          = {10.1109/TMM.2021.3096009},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3240-3255},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PR-RL: Portrait relighting via deep reinforcement learning},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust label rectifying with consistent contrastive-learning
for domain adaptive person re-identification. <em>TMM</em>, <em>24</em>,
3229–3239. (<a href="https://doi.org/10.1109/TMM.2021.3096014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptive person re-identification (Re-ID) is challenging due to the domain gap between the source and target domains. Existing methods have recently shown great promise by training models with contrastive learning and assigning pseudo labels by clustering, in which a memory bank is utilized to keep features for contrast. However, two main problems lead to sub-optimal generalization ability for existing methods. First, there is no constraint on the updating for memory kept features in existing methods, resulting in inaccurate contrastive learning. Second, the inevitable noisy labels during clustering are usually ignored. To alleviate these problems, we propose a Label Rectifying with Consistent Contrastive-learning (LRCC) framework with two strategies. (1) The consistent contrastive-learning (CC) strategy works with a memory bank which stores the source domain class centroids and all the target domain image features. With the CC strategy, the contrast is conducted across the source and target domains simultaneously. More specifically, we design and maintain consistent clustering during model iteration, thus the classes of memory kept target features are invariable in one epoch. (2) The label rectifying (LR) strategy introduces an auxiliary classifier into the LRCC framework. Thus the pseudo labels are rectified by minimizing the prediction variance between the primary classifier and the auxiliary classifier. To verify the effectiveness of LRCC, we conduct experiments on three public person Re-ID datasets under the domain adaptive setting, DukeMTMC-reID, Market-1501, and MSMT17. The experimental results demonstrate that the proposed LRCC can obtain reliable pseudo labels and achieves state-of-the-art adaptation performance.},
  archive      = {J_TMM},
  author       = {Xulin Song and Zhong Jin},
  doi          = {10.1109/TMM.2021.3096014},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3229-3239},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust label rectifying with consistent contrastive-learning for domain adaptive person re-identification},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PH-GCN: Person retrieval with part-based hierarchical graph
convolutional network. <em>TMM</em>, <em>24</em>, 3218–3228. (<a
href="https://doi.org/10.1109/TMM.2021.3095789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compact feature representation of person image is important for person re-identification (Re-ID) task. Recently, part-based representation models have been widely studied for extracting the more compact and robust feature representation for person image to improve person Re-ID results. However, existing part-based representation models mostly extract the features of different parts independently which ignore the spatial relationship information among different parts. To address this issue, in this paper we propose a novel deep learning framework, named Part-based Hierarchical Graph Convolutional Network (PH-GCN) for person Re-ID problem. Given a person image, PH-GCN first constructs a hierarchical graph to represent the spatial relationships among different parts. Then, both local and global feature learning is achieved by the feature information passing in PH-GCN, which takes the information of other parts into account for part feature representation. Finally, a perceptron layer is adopted for the final person part label prediction and re-identification. The proposed framework provides a general solution that integrates local , global and structural feature learning simultaneously in a unified end-to-end network representation and learning. Extensive experiments on several widely used benchmark datasets demonstrate the effectiveness and benefits of the proposed PH-GCN approach for person Re-ID task.},
  archive      = {J_TMM},
  author       = {Bo Jiang and Xixi Wang and Aihua Zheng and Jin Tang and Bin Luo},
  doi          = {10.1109/TMM.2021.3095789},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3218-3228},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PH-GCN: Person retrieval with part-based hierarchical graph convolutional network},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consensus feature network for scene parsing. <em>TMM</em>,
<em>24</em>, 3208–3217. (<a
href="https://doi.org/10.1109/TMM.2021.3094333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene parsing is challenging as it aims to assign one of the semantic categories to each pixel in scene images. Thus, pixel-level features are desired for scene parsing. However, classification networks are dominated by the discriminative portion, so directly applying classification networks to scene parsing will result in inconsistent parsing predictions within one instance and among instances of the same category. To address this problem, we propose two transform units to learn pixel-level consensus features. One is an Instance Consensus Transform (ICT) unit to learn the instance-level consensus features by aggregating features within the same instance. The other is a Category Consensus Transform (CCT) unit to pursue category-level consensus features through keeping the consensus of features among instances of the same category in scene images. The proposed ICT and CCT units are lightweight, data-driven and end-to-end trainable. The features learned by the two units are more coherent in both instance-level and category-level. Furthermore, we present the Consensus Feature Network (CFNet) based on the proposed ICT and CCT units, and demonstrate the effectiveness of each component in our method by performing extensive ablation experiments. Finally, our proposed CFNet achieves competitive performance on four datasets, including Cityscapes, Pascal Context, CamVid, and COCO Stuff.},
  archive      = {J_TMM},
  author       = {Tianyi Wu and Sheng Tang and Rui Zhang and Guodong Guo},
  doi          = {10.1109/TMM.2021.3094333},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3208-3217},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Consensus feature network for scene parsing},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Objective quality assessment of lenslet light field image
based on focus stack. <em>TMM</em>, <em>24</em>, 3193–3207. (<a
href="https://doi.org/10.1109/TMM.2021.3096071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large amount of complex scene information recorded by light field imaging has the potential for immersive media applications. Compression and reconstruction algorithms are crucial for the transmission, storage, and display of such massive data. Most of the existing quality evaluation indexes do not effectively account for light field characteristics. To accurately evaluate the distortions caused by compression and reconstruction algorithms, it is necessary to construct an image evaluation index that reflects the angular-spatial characteristics of the light field. This work proposes a full-reference light field image quality evaluation index that attempts to extract less information from the focus stack to accurately evaluate the entire light field quality. The proposed framework includes three specific steps. First, we construct a key refocused image extraction framework by the maximal spatial information contrast and the minimal angular information variation. Specifically, the gradient and phase congruency operators are used in the extraction framework. Second, a novel light field quality evaluation index is built based on the angular-spatial characteristics of the key refocused images. In detail, the features used in the key refocused image extraction framework and the chrominance feature are combined to construct the union feature. Third, the similarity of the union feature is pooled by the relevant visual saliency map to obtain the predicted score. Finally, the overall quality of the light field is measured by applying the proposed index to the key refocused images. The high efficiency and precision of the proposed method are shown by extensive comparison experiments.},
  archive      = {J_TMM},
  author       = {Chunli Meng and Ping An and Xinpeng Huang and Chao Yang and Liquan Shen and Bin Wang},
  doi          = {10.1109/TMM.2021.3096071},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3193-3207},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Objective quality assessment of lenslet light field image based on focus stack},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised graph convolutional network for multi-view
clustering. <em>TMM</em>, <em>24</em>, 3182–3192. (<a
href="https://doi.org/10.1109/TMM.2021.3094296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the promising preliminary results, existing graph convolutional network (GCN) based multi-view learning methods directly use the graph structure as view descriptor, which may inhibit the ability of multi-view learning for multimedia data. The major reason is that, in real multimedia applications, the graph structure may contain outliers. Moreover, they fail to take advantage of the information embedded in the inaccurate clustering labels obtained from their proposed methods, resulting in inferior clustering results. These observations motivate us to study whether there is a better alternative GCN based framework for multi-view clustering. To this end, in this paper, we propose an end-to-end self-supervised graph convolutional network for multi-view clustering (SGCMC). Specifically, SGCMC constructs a new view descriptor for graph-structured data by mapping the raw node content into the complex space via Euler transformation, which not only suppresses outliers but also reveals non-linear patterns embedded in data. Meanwhile, the proposed SGCMC uses the clustering labels to guide the learning of the latent representation and coefficient matrix, and the latter in turn is used to conduct the subsequent node clustering. By this way, clustering and representation learning are seamlessly connected, with the aim to achieve better clustering results. Extensive experimental results indicate that the proposed SGCMC outperforms the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Wei Xia and Qianqian Wang and Quanxue Gao and Xiangdong Zhang and Xinbo Gao},
  doi          = {10.1109/TMM.2021.3094296},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3182-3192},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Self-supervised graph convolutional network for multi-view clustering},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards analysis-friendly face representation with scalable
feature and texture compression. <em>TMM</em>, <em>24</em>, 3169–3181.
(<a href="https://doi.org/10.1109/TMM.2021.3094300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compactly representing visual information plays a fundamental role in optimizing the ultimate utility of myriad visual data-centered applications. Numerous approaches have been proposed to efficiently compress the texture and visual features for human visual perception and machine intelligence, respectively; however, much less work has been dedicated to studying the interactions between them. Here, we investigate the integration of feature and texture compression and show that a universal and collaborative visual information representation can be achieved in a hierarchical way. In particular, we study feature and texture compression in a scalable coding framework, where the base layer serves as the deep learning feature and the enhancement layer targets to perfectly reconstruct the texture. Based on the strong generative capability of deep neural networks, the gap between the base feature layer and enhancement layer is further filled with feature-level texture reconstruction, with the goal of further constructing texture representations from features. As such, the residuals between the original and reconstructed texture could be further conveyed in the enhancement layer. To improve the efficiency of the proposed framework, the base layer neural network is trained in a multitask manner such that the learned features enjoy both high-quality reconstruction and high-accuracy analysis. The framework and optimization strategies are further applied in face image compression, and promising coding performance has been achieved in terms of both rate-fidelity and rate-accuracy evaluations.},
  archive      = {J_TMM},
  author       = {Shurun Wang and Shiqi Wang and Wenhan Yang and Xinfeng Zhang and Shanshe Wang and Siwei Ma and Wen Gao},
  doi          = {10.1109/TMM.2021.3094300},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3169-3181},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards analysis-friendly face representation with scalable feature and texture compression},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DBDnet: A deep boosting strategy for image denoising.
<em>TMM</em>, <em>24</em>, 3157–3168. (<a
href="https://doi.org/10.1109/TMM.2021.3094058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new deep network architecture named deep boosting denoising net (DBDnet) for image denoising. It is a residual learning network that can generate a noise map from a noisy observation. In detail, it first generates a coarse noise map via a simple structure, and then updates the noise map gradually via a boosting function. The motivation of our DBDnet stems from the observation that the noise map recovered by any algorithm cannot ideally equal the ground-truth noise map, which typically contains noise. We call this noise NoN, i.e. , noise of noise map. Based on this observation, we formulate the denoising as a process of reducing NoN, and the role of DBDnet is to eliminate the NoN from the coarse noise map. In particular, we analyze the process of reducing NoN theoretically, and propose an NoN eliminating module to simulate it accordingly. We evaluate the proposed DBDnet on images polluted by different levels of additive white Gaussian noise and real noise. Experiment results demonstrate that our DBDnet can attain better denoising performance compared with state-of-the-art methods on several kinds of image denoising tasks. In particular, for the Gaussian denoising and real image denoising tasks, the average improvements of the PSNR values brought by our DBDnet are about 0.25 dB and 1.01 dB, respectively. In addition, we find and verify that the deep boosting insight can be easily introduced into the state-of-the-art image denoising network, and promotes its denoising performance. Our code is publicly available at https://github.com/jiayi-ma/DBDNet .},
  archive      = {J_TMM},
  author       = {Jiayi Ma and Chengli Peng and Xin Tian and Junjun Jiang},
  doi          = {10.1109/TMM.2021.3094058},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3157-3168},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DBDnet: A deep boosting strategy for image denoising},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Geometry-constrained scale estimation for monocular visual
odometry. <em>TMM</em>, <em>24</em>, 3144–3156. (<a
href="https://doi.org/10.1109/TMM.2021.3093771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a robust geometry-constrained scale estimation approach for monocular visual odometry, which takes the camera height as an absolute reference. Visual odometry is an essential module for robot self-localization and autonomous navigation in unexplored environments. Scale recovery is an indispensable requirement for monocular visual odometry, since it compensates for the metric information lost by a single camera and helps to reduce the scale drift. When the camera height is considered the absolute reference, the precision of scale recovery depends on the accuracy of the road point selection and road geometric model calculation. However, most of the previous approaches solve these two problems sequentially, and their road point selection is based on the color model of the road or prior-knowledge-based fixed region. In this paper, we propose combining and iteratively solving these two problems. We adopt the geometric model, instead of the color model, of the road to select the road points. Furthermore, the selected road feature points are used to estimate the road model, which limits the road point selection. In detail, we segment our feature points with Delaunay triangulation and select road points based on the depth consistency and road model consistency. The experiments on the KITTI dataset show that our method achieves the best performance among state-of-the-art monocular visual odometry methods.},
  archive      = {J_TMM},
  author       = {Hui Zhang and Xiangwei Wang and Xiaochuan Yin and Mingxiao Du and Chengju Liu and Qijun Chen},
  doi          = {10.1109/TMM.2021.3093771},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3144-3156},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Geometry-constrained scale estimation for monocular visual odometry},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient and accurate multi-scale topological network for
single image dehazing. <em>TMM</em>, <em>24</em>, 3114–3128. (<a
href="https://doi.org/10.1109/TMM.2021.3093724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image dehazing is a challenging ill-posed problem that has drawn significant attention in the last few years. Recently, convolutional neural networks have achieved great success in image dehazing. However, it is still difficult for these increasingly complex models to recover accurate details from the hazy image. In this paper, we pay attention to the feature extraction and utilization of the input image itself. To achieve this, we propose a Multi-scale Topological Network (MSTN) to fully explore the features at different scales. Meanwhile, we design a Multi-scale Feature Fusion Module (MFFM) and an Adaptive Feature Selection Module (AFSM) to achieve the selection and fusion of features at different scales, so as to achieve progressive image dehazing. This topological network provides a large number of search paths that enable the network to extract abundant image features as well as strong fault tolerance and robustness. In addition, ASFM and MFFM can adaptively select important features and ignore interference information when fusing different scale representations. Extensive experiments are conducted to demonstrate the superiority of our method compared with state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Qiaosi Yi and Juncheng Li and Faming Fang and Aiwen Jiang and Guixu Zhang},
  doi          = {10.1109/TMM.2021.3093724},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3114-3128},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Efficient and accurate multi-scale topological network for single image dehazing},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Subjective evaluation of visual quality and simulator
sickness of short 360<span class="math inline"><sup>∘</sup></span>
videos: ITU-t rec. p.919. <em>TMM</em>, <em>24</em>, 3087–3100. (<a
href="https://doi.org/10.1109/TMM.2021.3093717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently an impressive development in immersive technologies, such as Augmented Reality (AR), Virtual Reality (VR) and 360 ${^\circ }$ video, has been witnessed. However, methods for quality assessment have not been keeping up. This paper studies quality assessment of 360 ${^\circ }$ video from the cross-lab tests (involving ten laboratories and more than 300 participants) carried out by the Immersive Media Group (IMG) of the Video Quality Experts Group (VQEG). These tests were addressed to assess and validate subjective evaluation methodologies for 360 ${^\circ }$ video. Audiovisual quality, simulator sickness symptoms, and exploration behavior were evaluated with short (from 10 seconds to 30 seconds) 360 ${^\circ }$ sequences. The following factors’ influences were also analyzed: assessment methodology, sequence duration, Head-Mounted Display (HMD) device, uniform and non-uniform coding degradations, and simulator sickness assessment methods. The obtained results have demonstrated the validity of Absolute Category Rating (ACR) and Degradation Category Rating (DCR) for subjective tests with 360 ${^\circ }$ videos, the possibility of using 10-second videos (with or without audio) when addressing quality evaluation of coding artifacts, as well as any commercial HMD (satisfying minimum requirements). Also, more efficient methods than the long Simulator Sickness Questionnaire (SSQ) have been proposed to evaluate related symptoms with 360 ${^\circ }$ videos. These results have been instrumental for the development of the ITU-T Recommendation P.919. Finally, the annotated dataset from the tests is made publicly available for the research community.},
  archive      = {J_TMM},
  author       = {Jesús Gutiérrez and Pablo Pérez and Marta Orduna and Ashutosh Singla and Carlos Cortés and Pramit Mazumdar and Irene Viola and Kjell Brunnström and Federica Battisti and Natalia Cieplińska and Dawid Juszka and Lucjan Janowski and Mikołaj Leszczuk and Anthony Adeyemi-Ejeye and Yaosi Hu and Zhenzhong Chen and Glenn Van Wallendael and Peter Lambert and César Díaz and John Hedlund and Omar Hamsis and Stephan Fremerey and Frank Hofmeyer and Alexander Raake and Pablo César and Marco Carli and Narciso García},
  doi          = {10.1109/TMM.2021.3093717},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {3087-3100},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Subjective evaluation of visual quality and simulator sickness of short 360$^\circ$ videos: ITU-T rec. p.919},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Seek common ground while reserving differences: A
model-agnostic module for noisy domain adaptation. <em>TMM</em>,
<em>24</em>, 1020–1030. (<a
href="https://doi.org/10.1109/TMM.2021.3097495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noisy domain adaptation aims to solve the problem that the source dataset contains noisy labels in domain adaptation. Previous methods handle noisy labels by selecting the small-loss samples with inconsistent predictions between two models and discarding the consistent samples, resulting in many noises contained in the selected samples. By jointly considering the consistent and inconsistent samples, we propose a model-agnostic module, named Seek Common Ground While Reserving Differences (SCGWRD), to reduce the impact of noisy samples. The proposed SCGWRD module consists of Seek Common Ground (SCG) component and Reserve Differences (RD) component by utilizing the outputs of two symmetrical domain adaptation models. As the common samples with consistent predictions between two models are more likely to be clean samples, the SCG component applies the small-loss strategy to select the reliable samples with consistent predictions. Unlike SCG, the RD component maintains the divergences between two models with mutual learning and reduces the effect of noisy data using the samples with different predictions and small losses. Evaluations on three benchmarks demonstrate the effectiveness and robustness of the proposed SCGWRD module for noisy domain adaptation.},
  archive      = {J_TMM},
  author       = {Yukun Zuo and Hantao Yao and Liansheng Zhuang and Changsheng Xu},
  doi          = {10.1109/TMM.2021.3097495},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1020-1030},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Seek common ground while reserving differences: A model-agnostic module for noisy domain adaptation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sampling and re-weighting: Towards diverse frame aware
unsupervised video person re-identification. <em>TMM</em>, <em>24</em>,
4250–4261. (<a href="https://doi.org/10.1109/TMM.2022.3186177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video person re-identification (re-ID) methods extract richer features from video tracklets than image-based ones and have received growing attention. However, existing supervised methods require numerous cross-camera identity labels, which is impractical for large-scale data. Although clustering-based unsupervised methods have been exploited to obtain pseudo labels and train the models iteratively for video person re-ID, they remain in their infancy due to the diversity of person images and uncertainty in the image quality of video tracklets. In this work, we employ two strategies of S ampling and R e-weighting for C lustering (SRC) to obtain robust and discriminative person feature representations. This method considers the influence of two kinds of frames in the tracklet: 1) Detection errors or heavy occlusions generate noisy frames in the tracklet. These tracklets with noisy frames may be assigned with unreliable annotations during clustering. 2) Different frames are identified by the model with varying degrees of difficulty, caused by pose changes or partial occlusions. We call them hard frames, which are hard to identify but informative. To alleviate these problems, we propose a dynamic noise trimming module and diverse frame re-weighting module for sampling and re-weighting. The dynamic noise trimming module strengthens the dependability of the tracklet representation by removing noisy frames to enhance the clustering accuracy. The diverse frame re-weighting module focuses on training hard frames to enhance the learning of rich information from tracklet. Experiments on three video datasets, i.e. DukeMTMC-VideoReID, MARS and PRID2011, demonstrate the effectiveness of the proposed SRC under the unsupervised re-ID setting.},
  archive      = {J_TMM},
  author       = {Pengyu Xie and Xin Xu and Zheng Wang and Toshihiko Yamasaki},
  doi          = {10.1109/TMM.2022.3186177},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {4250-4261},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Sampling and re-weighting: Towards diverse frame aware unsupervised video person re-identification},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Boundary-aware arbitrary-shaped scene text detector with
learnable embedding network. <em>TMM</em>, <em>24</em>, 3129–3143. (<a
href="https://doi.org/10.1109/TMM.2021.3093727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from the popularity of deep learning theory, scene text detection algorithms have developed rapidly in recent years. Methods representing text region by text segmentation map are proved to capture arbitrary-shaped text in a more flexible and accurate way. However, such segmentation-based methods are prone to be disturbed by the text-like background patterns (like the fence, grass, etc.), which generally suffer from imprecise boundary detail problem. In this paper, LEMNet is proposed to handle the imprecise boundary problem by guiding the generation of text boundary based on a priori constraint. In the training stage, Boundary Segmentation Branch is firstly constructed to predict coarse boundary mask for each text instance. Then, through mapping pixels into an embedding space, the proposed Pixel Embedding Branch makes the embedding representation of boundary points learn to be more similar, meanwhile enlarging the characteristic distance between background points and boundary points. During inference, noise in the coarse boundary segmentation map can be effectively suppressed by a Noisy Point Suppression Algorithm among pixel embedding vectors. In this way, LEMNet can generate a more precise boundary description of text regions. To further enhance the distinguishability of boundary features, we propose a Context Enhancement Module to capture feature interactions in different representation subspaces, in which features are parallelly performed attention and concatenated to generate enhanced features. Extensive experiments are conducted over four challenging datasets, which demonstrate the effectiveness of LEMNet. Specifically, LEMNet achieves F-measure of 85.2%, 87.6% and 85.2% on CTW1500, Total-Text and MSRA-TD500 respectively, which is the latest SOTA.},
  archive      = {J_TMM},
  author       = {Mengting Xing and Hongtao Xie and Qingfeng Tan and Shancheng Fang and Yuxin Wang and Zhengjun Zha and Yongdong Zhang},
  doi          = {10.1109/TMM.2021.3093727},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {3129-3143},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Boundary-aware arbitrary-shaped scene text detector with learnable embedding network},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Exploring pairwise relationships adaptively from linguistic
context in image captioning. <em>TMM</em>, <em>24</em>, 3101–3113. (<a
href="https://doi.org/10.1109/TMM.2021.3093725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For image captioning, recent works start to focus on exploring visual relationships for generating high-quality interactive words (i.e. verbs and prepositions). However, many existing works only focus on semantic level by analysing the feature similarity between objects in the visual domain but ignore the linguistic context included in the caption decoder. When captioning is being carried out, the entity words can be inferred based on visual information of objects. The interactive words representing the relationships between entity words can only be inferred based on high-level language meaning generated in the process of captioning decoding. Such high-level language meaning is called linguistic context, which refers to the relational context between words or phrases in the caption sentences. The linguistic context can be used as strong guidance to explore related visual relationships between different objects effectively. To achieve this, we propose a novel context-adaptive attention module that is strongly driven by the linguistic context from the caption decoder. In this module, a novel design of visual relationship attention is proposed based on a bilinear self-attention model to explore related visual relationships and encode more discriminative features under the linguistic context. To achieve the adaptive process of attending to related visual relationships for generating interactive words or related visual objects for entity words, an attention modulator is integrated as an attention channel controller responding to the changing linguistic context of the caption decoder dynamically. Experimented on MSCOCO dataset, our model achieves promising performances compared with all counterpart models that explore visual relationships.},
  archive      = {J_TMM},
  author       = {Zongjian Zhang and Qiang Wu and Yang Wang and Fang Chen},
  doi          = {10.1109/TMM.2021.3093725},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {3101-3113},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Exploring pairwise relationships adaptively from linguistic context in image captioning},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross view capture for stereo image super-resolution.
<em>TMM</em>, <em>24</em>, 3074–3086. (<a
href="https://doi.org/10.1109/TMM.2021.3092571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo image super-resolution exploits additional features from cross view image pairs for high resolution (HR) image reconstruction. Recently, several new methods have been proposed to investigate cross view features along epipolar lines to enhance the visual perception of recovered HR images. Despite the impressive performance of these methods, global contextual features from cross view images are left unexplored. In this paper, we propose a cross view capture network (CVCnet) for stereo image super-resolution by using both global contextual and local features extracted from both views. Specifically, we design a cross view block to capture diverse feature embeddings from the views in stereo vision. In addition, a cascaded spatial perception module is proposed to redistribute each location in feature maps according to the weight it occupies to make the extraction of features more effective. Extensive experiments demonstrate that our proposed CVCnet outperforms the state-of-the-art image super-resolution methods to achieve the best performance for stereo image super-resolution tasks. The source code is available at https://github.com/xyzhu1/CVCnet .},
  archive      = {J_TMM},
  author       = {Xiangyuan Zhu and Kehua Guo and Hui Fang and Liang Chen and Sheng Ren and Bin Hu},
  doi          = {10.1109/TMM.2021.3092571},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {3074-3086},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross view capture for stereo image super-resolution},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-modal context propagation for person re-identification
with wireless positioning. <em>TMM</em>, <em>24</em>, 3060–3073. (<a
href="https://doi.org/10.1109/TMM.2021.3092579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing person re-identification methods mainly rely on the visual appearance captured by cameras for identity matching. However, dueto the sensitivity of visual data to occlusion, blur, clothing change, etc. , existing methods struggle to distinguish pedestrians in challenging scenarios. Inspired by the fact that most pedestrians carry around smart wireless devices, e.g. mobile phones that can be sensed by WiFi or cellular networks as wireless positioning signals, we propose to exploit the free yet informative wireless signals to assist person re-identification. It is well recognized that wireless signals are robust to visual noises mentioned above, which perform as a good complement to the visual data. To make full use of these multi-modal clues for person re-identification, we propose a multi-modal context propagation framework MCPF that contains a recurrent context propagation module RCPM and an unsupervised multi-modal cross-domain method UMM-ReID. RCPM enables context information to be continuously propagated and fused between visual data and wireless data. UMM-ReID utilizes wireless signals to constrain the estimation of pseudo labels. We contribute a new wireless positioning person re-identification dataset WP-ReID to evaluate our approach. Extensive experiments demonstrate the effectiveness of the proposed method. Benefiting from the collaboration of RCPM and UMM-ReID, the proposed framework MCPF achieves a significant performance improvement over existing methods.},
  archive      = {J_TMM},
  author       = {Yiheng Liu and Wengang Zhou and Mao Xi and Sanjing Shen and Houqiang Li},
  doi          = {10.1109/TMM.2021.3092579},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {3060-3073},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-modal context propagation for person re-identification with wireless positioning},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Jointly learning the attributes and composition of shots for
boundary detection in videos. <em>TMM</em>, <em>24</em>, 3049–3059. (<a
href="https://doi.org/10.1109/TMM.2021.3092143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In film making, shot has a profound influence on how the movie content is delivered and how the audiences are echoed, where different emotions and contents can be delivered through well-designed camera movements or shot editing. Therefore, in pursuit of high-level understanding of long videos, accurate shot detection from untrimmed videos should be considered as the first and the most fundamental step. Existing approaches address this problem based on the visual differences and content transitions between consecutive frames, while ignoring intrinsic shot attributes, viz. , camera movements, scales, and viewing angles, which essentially reveal how each shot is created. In this work, we propose a new learning framework (SCTSNet) for shot boundary detection by jointly recognizing the attributes and composition of shots in videos. To facilitate the analysis of shots and the evaluation of shot detection models, we collect a large-scale shot boundary dataset MovieShots2 , which contains $\text{15}\,K$ shots from 282 movie clips. It is richly annotated with the temporal boundary between consecutive shots and individual shot attributes, including camera movements, scales, and viewing angles, which are the three most distinct shot attributes. Our experiments show that the joint learning framework can significantly boost the boundary detection performance, surpassing the previous scores by a large margin. SCTSNet improves shot boundary detection AP from 0.65 to 0.77, pushing the performance to a new level.},
  archive      = {J_TMM},
  author       = {Xuekun Jiang and Libiao Jin and Anyi Rao and Linning Xu and Dahua Lin},
  doi          = {10.1109/TMM.2021.3092143},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {3049-3059},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Jointly learning the attributes and composition of shots for boundary detection in videos},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Horizontal-to-vertical video conversion. <em>TMM</em>,
<em>24</em>, 3036–3048. (<a
href="https://doi.org/10.1109/TMM.2021.3092202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At this blooming age of social media and mobile platform, mass consumers are migrating from horizontal video to vertical contents delivered on hand-held devices. Accordingly, revitalizing the exposure of horizontal video becomes vital and urgent, which is hereby tackled by our automated horizontal-to-vertical (abbreviated as H2V ) video conversion framework. Essentially, the H2V framework performs subject-preserving video cropping instantiated in the proposed Rank-SS module. Rank-SS incorporates object detection to discover the candidate subjects, from which we select the primary subject-to-preserve leveraging location, appearance, and salient cues in a convolutional neural network. In addition to converting horizontal videos vertically by cropping around the selected subject, automatic shot detection and multi-object tracking are integrated into the H2V framework to accommodate long and complex videos. To develop H2V systems, we collect an H2V-142 K dataset containing 125 videos (132 K frames) and 9500 cover images annotated with primary subject bounding boxes. On H2V-142 K and public object detection datasets, our method demonstrates promising results on the subject selection comparing to the related solutions. Furthermore, our H2V framework is industrially deployed hosting millions of daily active users and exhibits favorable H2V conversion performance. By making this dataset as well as our approach publicly available, we wish to pave the way for more horizontal-to-vertical video conversion research. Our collected H2V-142 K dataset is available at https://tianchi.aliyun.com/dataset/dataDetail?dataId=93339 .},
  archive      = {J_TMM},
  author       = {Tun Zhu and Daoxin Zhang and Yao Hu and Tianran Wang and Xiaolong Jiang and Jianke Zhu and Jiawei Li},
  doi          = {10.1109/TMM.2021.3092202},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {3036-3048},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Horizontal-to-vertical video conversion},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Confidence-based 6D object pose estimation. <em>TMM</em>,
<em>24</em>, 3025–3035. (<a
href="https://doi.org/10.1109/TMM.2021.3092149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this paper is to estimate the six-degree-of-freedom (6-DOF) poses of objects from a single RGB image in which the target objects are partially occluded. Most recent studies have formulated methods for predicting the projected 2-D locations of 3-D keypoints through a deep neural network and then used a PnP algorithm to compute the 6-DOF poses. Several researchers have pointed out the uncertainty of the predicted locations and modelled it according to predefined rules or functions, but the performance of such approaches may still be degraded if occlusion is present. To address this problem, we formulated 2-D keypoint locations as probabilistic distributions in our novel loss function and developed a confidence-based pose estimation network. This network not only predicts the 2-D keypoint locations from each visible patch of a target object but also provides the corresponding confidence values in an unsupervised fashion. Through the proper fusion of the most reliable local predictions, the proposed method can improve the accuracy of pose estimation when target objects are partially occluded. Experiments demonstrated that our method outperforms state-of-the-art methods on a main occlusion data set used for estimating 6-D object poses. Moreover, this framework is efficient and feasible for realtime multimedia applications.},
  archive      = {J_TMM},
  author       = {Wei-Lun Huang and Chun-Yi Hung and I-Chen Lin},
  doi          = {10.1109/TMM.2021.3092149},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {3025-3035},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Confidence-based 6D object pose estimation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhancing neural machine translation with dual-side
multimodal awareness. <em>TMM</em>, <em>24</em>, 3013–3024. (<a
href="https://doi.org/10.1109/TMM.2021.3092187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal machine translation (MMT) aims to translate a sentence in the source language into the target language with the context of an associated image. According to where the visual information is employed, previous approaches can be categorized into two types: directly injecting the visual information at the input side or exploiting it as a visual constraint at the objective side. In this work, we propose an IO-MMT model which exploits the visual assistance in dual sides to fully exploit the visual information for multimodal machine translation. It contains a relation-aware multimodal transformer to simultaneously exploit the objects and their spatial relationships in the image with a graph at the input-side and a novel visual assistance structure to further improve visual consistency of the translation at the objective-side. Experimental results under both normal setting and input degradation settings on the Multi30k benchmark dataset show that combining the visual assistance in dual sides consistently outperforms single-side MMT models and achieves the state-of-the-art results on EN-DE and EN-FR translation tasks. We will release the codes and models at https://github.com/syuqings/MMT .},
  archive      = {J_TMM},
  author       = {Yuqing Song and Shizhe Chen and Qin Jin and Wei Luo and Jun Xie and Fei Huang},
  doi          = {10.1109/TMM.2021.3092187},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {3013-3024},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhancing neural machine translation with dual-side multimodal awareness},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The model may fit you: User-generalized cross-modal
retrieval. <em>TMM</em>, <em>24</em>, 2998–3012. (<a
href="https://doi.org/10.1109/TMM.2021.3091888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world applications, a cross-model retrieval model trained on multimodal instances without considering differences in data distributions among users, termed as user domain shift, usually cannot generalize well to unknown user domains. In this paper, we define a new task of user-generalized cross-modal retrieval, and propose a novel Meta-Learning Multimodal User Generalization (MLMUG) method to solve it. MLMUG simulates the user domain shift with meta-optimization, which aims to embed multimodal data effectively and generalize the cross-modal retrieval model to any unknown user domains. We design a cross-modal embedding network with a learnable meta covariant attention module to encode transferable knowledge among different user domains. A user-adaptive meta-optimization scheme is proposed to adaptively aggregate gradients and meta-gradients for fast and stable meta-optimization. We build two benchmarks for user-generalized cross-modal retrieval evaluation. Experiments on the proposed benchmarks validate the generalization of our method compared with several state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Xinhong Ma and Xiaoshan Yang and Junyu Gao and Changsheng Xu},
  doi          = {10.1109/TMM.2021.3091888},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2998-3012},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {The model may fit you: User-generalized cross-modal retrieval},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Explicit cross-modal representation learning for visual
commonsense reasoning. <em>TMM</em>, <em>24</em>, 2986–2997. (<a
href="https://doi.org/10.1109/TMM.2021.3091882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a question about an image, Visual Commonsense Reasoning (VCR) needs to provide not only a correct answer, but also a rationale to justify the answer. VCR is a challenging task due to the requirement of proper semantic alignment and reasoning between the image and linguistic expression. Recent approaches offer a great promise by exploring holistic attention mechanisms or graph-based networks, but most of them do implicit reasoning and ignore the semantic dependencies among the linguistic expression. In this paper, we propose a novel explicit cross-modal representation learning network for VCR by incorporating syntactic information into the visual reasoning and natural language understanding. The proposed method enjoys several merits. First, based on a two-branch neural module network, we can do explicit cross-modal reasoning guided by the high-level syntactic structure of linguistic expression. Second, the semantic structure of the linguistic expression is incorporated into a syntactic GCN to facilitate language understanding. Third, our explicit cross-modal representation learning network can provide a traceable reasoning-flow, which offers visible fine-grained evidence of the answer and rationale. Quantitative and qualitative evaluations on the public VCR dataset demonstrate that our approach performs favorably against state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Xi Zhang and Feifei Zhang and Changsheng Xu},
  doi          = {10.1109/TMM.2021.3091882},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2986-2997},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Explicit cross-modal representation learning for visual commonsense reasoning},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic regularized class-conditional GANs for
semi-supervised fine-grained image synthesis. <em>TMM</em>, <em>24</em>,
2975–2985. (<a href="https://doi.org/10.1109/TMM.2021.3091859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning effective generative models for natural image synthesis is a promising way to reduce the dependence of deep models on massive training data. This work focuses on Fine-Grained Image Synthesis (FGIS) in the semi-supervised setting where a small number of training instances are labeled. Different from generic image synthesis tasks, the available fine-grained data may be inadequate, and the differences among the object categories are typically subtle. To address these issues, we propose a Semantic Regularized class-conditional Generative Adversarial Network, which is referred to as SReGAN. We incorporate an additional discriminator and classifier into the generator-discriminator minimax game. Competing with two discriminators enforces the generator to model both marginal and class-conditional data distributions, which alleviates the problem of limited training data and labels. However, the discriminators may overlook the class separability. To induce the generator to discover the distinctions between classes, we construct semantically congruent and incongruent pairs in the generation process, and further regularize the generator by encouraging high similarities of congruent pairs, while penalizing that of incongruent ones in the classifier’s feature space. We have conducted extensive experiments to verify the capability of SReGAN in generating high-fidelity images on a variety of FGIS benchmarks.},
  archive      = {J_TMM},
  author       = {Tianyi Chen and Si Wu and Xuhui Yang and Yong Xu and Hau-San Wong},
  doi          = {10.1109/TMM.2021.3091859},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2975-2985},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semantic regularized class-conditional GANs for semi-supervised fine-grained image synthesis},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Total generate: Cycle in cycle generative adversarial
networks for generating human faces, hands, bodies, and natural scenes.
<em>TMM</em>, <em>24</em>, 2963–2974. (<a
href="https://doi.org/10.1109/TMM.2021.3091847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel and unified Cycle in CycleGenerative Adversarial Network (C2GAN) for generating human faces, hands, bodies, and natural scenes. Our proposed C2GAN is a cross-modal model exploring the joint exploitation of the input image data and guidance data in an interactive manner. C2GAN contains two different generators, i.e., an image-generation generator and a guidance-generation generator. Both generators are mutually connected and trained in an end-to-end fashion and explicitly form three cycled subnets, i.e., one image generation cycle and two guidance generation cycles. Each cycle aims at reconstructing the input domain and simultaneously produces a useful output involved in the generation of another cycle. In this way, the cycles constrain each other implicitly providing complementary information from both image and guidance modalities and bringing an extra supervision gradient across the cycles, facilitating a more robust optimization of the whole model. Extensive results on four guided image-to-image translation subtasks demonstrate that the proposed C2GAN is effective in generating more realistic images compared with state-of-the-art models.},
  archive      = {J_TMM},
  author       = {Hao Tang and Nicu Sebe},
  doi          = {10.1109/TMM.2021.3091847},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2963-2974},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Total generate: Cycle in cycle generative adversarial networks for generating human faces, hands, bodies, and natural scenes},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal learning for temporally coherent talking face
generation with articulator synergy. <em>TMM</em>, <em>24</em>,
2950–2962. (<a href="https://doi.org/10.1109/TMM.2021.3091863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Talking face generation is a demanding task to synthesize a high quality video with accurate lip synchronization and rhythmic head motion. However, existing methods always suffer from unrealistic facial animations, because 1) they only take single-mode input, but ignore the complementarity of multimodal inputs for lip-sync improvement; 2) they only explore lip movements, but ignore the articulator synergy between lips and jaw; 3) they generate each video frame in a temporal-independent way, but ignore the temporal continuity among the entire video. To address these limitations, in this paper, we present a novel method to generate realistic and temporally coherent talking heads by considering multimodal inputs , articulator synergy , inter-frame consistency and intra-frame consistency . Firstly, for landmark prediction, a novel Multiple Synergy Network (MSN) is proposed to improve the accuracy of landmark prediction by incorporating multimodal inputs (i.e., audio and text inputs). Besides, instead of merely considering lip landmarks, we also explore the jaw movements to ensure articulator synergy among lips and jaw. Secondly, for realistic video generation, a Video Consistency Network (VCN) is proposed conditioned on the predicted landmarks. In VCN, the optical flow is adopted to model the temporal continuity between frames to ensure inter-frame consistency . Meanwhile, a mouth generation branch is proposed to enhance mouth texture and the corresponding mouth mask is employed to ensure intra-frame consistency between the mouth area and the others. Extensive experiments demonstrate that our approach exhibits excellent superiority on lip-sync and can generate photo-realistic facial animations. Project is available at http://imcc.ustc.edu.cn/project/tfgen/ .},
  archive      = {J_TMM},
  author       = {Lingyun Yu and Hongtao Xie and Yongdong Zhang},
  doi          = {10.1109/TMM.2021.3091863},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2950-2962},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multimodal learning for temporally coherent talking face generation with articulator synergy},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised monocular depth estimation using attention and
multi-warp reconstruction. <em>TMM</em>, <em>24</em>, 2938–2949. (<a
href="https://doi.org/10.1109/TMM.2021.3091308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth estimation has become one of the most studied topics in computer vision. Most approaches treat depth prediction as a fully supervised regression problem requiring vast amounts of corresponding ground-truth depth and image pairs for training. Unsupervised monocular depth estimation has emerged as a promising alternative that eliminates dataset limitations. This paper proposes an end-to-end unsupervised deep learning framework integrating attention blocks and multi-warp loss for monocular depth estimation. In this framework, to explore more general contextual information among the feature volumes, an attention block that sequentially refines the feature maps along the channel and spatial dimensions is inserted after the first and last stages of the network encoder. Additionally, to further utilize the errors in the original disparity estimation from the network, a novel multi-warp reconstruction strategy is designed for the loss function. The experimental results evaluated on the KITTI, CityScapes and Make3D datasets demonstrate the state-of-the-art performance and satisfactory generalization ability of our proposed method.},
  archive      = {J_TMM},
  author       = {Chuanwu Ling and Xiaogang Zhang and Hua Chen},
  doi          = {10.1109/TMM.2021.3091308},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2938-2949},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Unsupervised monocular depth estimation using attention and multi-warp reconstruction},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cryptanalysis of reversible data hiding in encrypted images
by block permutation and co-modulation. <em>TMM</em>, <em>24</em>,
2924–2937. (<a href="https://doi.org/10.1109/TMM.2021.3090588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible data hiding in encrypted images (RDH-EI) technology is commonly used in cloud storage images for privacy protection. Most existing RDH-EI techniques reported in the literature applied block permutation and co-modulation (BPCM) encryption to generate encrypted images. This work analyses the security of the RDH-EI algorithm based on BPCM encryption under known plaintext attacks (KPAs). Different from the existing KPAs, this paper considers that attackers can perform KPAs based on marked encrypted images and shows that BPCM encryption has the risk of information leakage. To find the constant features of a block before and after co-modulation, the first-pixel difference block (FDB) of a block is first defined. Then, a pseudo cypher difference image of the cyphertext image is constructed to eliminate the changed FDBs so that the differences in the cyphertext FDBs are the same as the FDBs in the corresponding plaintext difference image. Finally, we design an FDB-based block permutation key estimation method according to the plaintext difference image and pseudocyphertext difference image. The influence of block size on key estimation accuracy and the time complexity of the proposed KPA algorithm are analysed and discussed. Experimental results show that the correct rate of key estimation is positively correlated with the block size and the number of plain-cyphertext pairs. The average correct rate of key estimation reaches 63% when the block size is greater than 3×3.},
  archive      = {J_TMM},
  author       = {Lingfeng Qu and Fan Chen and Shanjun Zhang and Hongjie He},
  doi          = {10.1109/TMM.2021.3090588},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2924-2937},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cryptanalysis of reversible data hiding in encrypted images by block permutation and co-modulation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial-temporal graphs for cross-modal Text2Video
retrieval. <em>TMM</em>, <em>24</em>, 2914–2923. (<a
href="https://doi.org/10.1109/TMM.2021.3090595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal text to video retrieval aims to find relevant videos given text queries, which is crucial for various real-world applications. The key to address this task is to build the correspondence between video and text such that related samples from different modalities can be aligned. As the text (sentence) contains both nouns and verbs representing objects as well as their interactions, retrieving relevant videos requires a fine-grained understanding of video contents—not only the semantic concepts (i.e., objects) but also the interactions between them. Nevertheless, current approaches mostly represent videos with aggregated frame-level features for the learning of joint space and ignore the information of object interactions, which usually results in suboptimal retrieval performance. To improve the performance of cross-modal video retrieval, this paper proposes a framework that models videos as spatial-temporal graphs where nodes correspond to visual objects and edges correspond to the relations/interactions between objects. With the spatial-temporal graphs, object interactions in frame sequences can be captured to enrich the video representations for joint space learning. Specifically, Graph Convolutional Network is introduced to learn the representations on spatial-temporal graphs, aiming to encode spatial-temporal interactions between objects; while BERT is introduced to dynamically encode the sentence according to the context for cross-modal retrieval. Extensive experiments verify the effectiveness of the proposed framework and it achieves promising performances on both MSR-VTT and LSMDC datasets.},
  archive      = {J_TMM},
  author       = {Xue Song and Jingjing Chen and Zuxuan Wu and Yu-Gang Jiang},
  doi          = {10.1109/TMM.2021.3090595},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2914-2923},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spatial-temporal graphs for cross-modal Text2Video retrieval},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Learning scale-consistent attention part network for
fine-grained image recognition. <em>TMM</em>, <em>24</em>, 2902–2913.
(<a href="https://doi.org/10.1109/TMM.2021.3090274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discriminative region localization and feature learning are crucial for fine-grained visual recognition. Existing approaches solve this issue by attention mechanism or part based methods while neglecting consistency between attention and local parts, as well as the rich relation information among parts. This paper proposes a Scale-consistent Attention Part Network (SCAPNet) to address that issue, which seamlessly integrates three novel modules: grid gate attention unit (gGAU), scale-consistent attention part selection (SCAPS), and part relation modeling (PRM). The gGAU module represents the grid region at a certain fine-scale with middle layer CNN features and produces hard attention maps with the lightweight Gumbel-Max based gate. The SCAPS module utilizes attention to guide part selection across multi-scales and keep the selection scale-consistent. The PRM module utilizes the self-attention mechanism to build the relationship among parts based on their appearance and relative geo-positions. SCAPNet can be learned in an end-to-end way and demonstrates state-of-the-art accuracy on several publicly available fine-grained recognition datasets (CUB-200-2011, FGVC-Aircraft, Veg200, and Fru92).},
  archive      = {J_TMM},
  author       = {Huabin Liu and Jianguo Li and Dian Li and John See and Weiyao Lin},
  doi          = {10.1109/TMM.2021.3090274},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2902-2913},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning scale-consistent attention part network for fine-grained image recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Multi-scale grid network for image deblurring with
high-frequency guidance. <em>TMM</em>, <em>24</em>, 2890–2901. (<a
href="https://doi.org/10.1109/TMM.2021.3090206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been demonstrated that the blurring process reduces the high-frequency information of the original sharp image, so the main challenge for image deblurring is to reconstruct high-frequency information from the blurry image. In this paper, we propose a novel image deblurring framework to focus on the reconstruction of high-frequency information, which consists of two main subnetworks: a high-frequency reconstruction subnetwork (HFRSN) and a multi-scale grid subnetwork (MSGSN). The HFRSN is built to reconstruct latent high-frequency information from multiple scale blurry images. The MSGSN performs deblurring processes with high-frequency guidance at different scales simultaneously. Besides, in order to better use high-frequency information to restore sharpening images, we designed a high-frequency information aggregation (HFAG) module and a high-frequency information attention (HFAT) module in MSGSN. The HFAG module is designed to fuse high-frequency features and image features at the feature extraction stage, and the HFAT module is built to enhance the feature reconstruction stage. Extensive experiments on different datasets show the effectiveness and efficiency of our method.},
  archive      = {J_TMM},
  author       = {Yang Liu and Faming Fang and Tingting Wang and Juncheng Li and Yun Sheng and Guixu Zhang},
  doi          = {10.1109/TMM.2021.3090206},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2890-2901},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-scale grid network for image deblurring with high-frequency guidance},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SRDRL: A blind super-resolution framework with degradation
reconstruction loss. <em>TMM</em>, <em>24</em>, 2877–2889. (<a
href="https://doi.org/10.1109/TMM.2021.3090166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the remarkable success of deep learning-based single image super-resolution (SISR) methods. However, most of the existing SISR methods assume that low-resolution (LR) images are purely bicubic downsampled from high-resolution (HR) images. Once the actual degradation is not bicubic, their outstanding performance is hard to maintain. Since the real-world image degradation process can be modeled by a combination of downsampling, blurring, and noise, several SR methods have been proposed to super-resolve LR images with multiple blur kernels and noise levels. However, these SR methods require prior knowledge of the degradation process, which is difficult to obtain in practical applications. To address these issues, we propose a degradation reconstruction loss (DRL), which captures the degradation-wise differences between SR images and HR images via a degradation simulator. Empowered by the degradation simulator, the proposed loss, and an efficient SR network, a blind SR framework (SRDRL) without prior knowledge that can handle multiple degradations is formed. Extensive experimental results demonstrate that the proposed SRDRL outperforms the state-of-the-art blind SR methods and denosing+SR methods on multi-degraded datasets. The degradation reconstruction loss can be a plug-and-play loss for existing SR methods to handle multiple degradations. The source code can be found at https://github.com/FVL2020/SRDRL .},
  archive      = {J_TMM},
  author       = {Zongyao He and Zhi Jin and Yao Zhao},
  doi          = {10.1109/TMM.2021.3090166},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2877-2889},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SRDRL: A blind super-resolution framework with degradation reconstruction loss},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video-based point cloud compression artifact removal.
<em>TMM</em>, <em>24</em>, 2866–2876. (<a
href="https://doi.org/10.1109/TMM.2021.3090148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photo-realistic point cloud capture and transmission are the fundamental enablers for immersive visual communication. The coding process of dynamic point clouds, especially video-based point cloud compression (V-PCC) developed by the MPEG standardization group, is now delivering state-of-the-art performance in compression efficiency. V-PCC is based on the projection of the point cloud patches to 2D planes and encoding the sequence as 2D texture and geometry patch sequences. However, the resulting quantization errors from coding can introduce compression artifacts, which can be very unpleasant for the quality of experience (QoE). In this work, we developed a novel out-of-the-loop point cloud geometry artifact removal solution that can significantly improve reconstruction quality without additional bandwidth cost. Our novel framework consists of a point cloud sampling scheme, an artifact removal network, and an aggregation scheme. The point cloud sampling scheme employs a cube-based neighborhood patch extraction to divide the point cloud into patches. The geometry artifact removal network then processes these patches to obtain artifact-removed patches. The artifact-removed patches are then merged together using an aggregation scheme to obtain the final artifact-removed point cloud. We employ 3D deep convolutional feature learning for geometry artifact removal that jointly recovers both the quantization direction and the quantization noise level by exploiting projection and quantization prior. The simulation results demonstrate that the proposed method is highly effective and can considerably improve the quality of the reconstructed point cloud.},
  archive      = {J_TMM},
  author       = {Anique Akhtar and Wen Gao and Li Li and Zhu Li and Wei Jia and Shan Liu},
  doi          = {10.1109/TMM.2021.3090148},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2866-2876},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Video-based point cloud compression artifact removal},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EFRNet: Efficient feature reconstructing network for
real-time scene parsing. <em>TMM</em>, <em>24</em>, 2852–2865. (<a
href="https://doi.org/10.1109/TMM.2021.3089422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a light-weight and powerful convolutional neural network, termed as efficient feature reconstructing network (EFRNet), for real-time scene parsing. Our key idea is to decompose the process of learning high-resolution representations into two stages: i) bottom-up codebook/coding matrix learning and ii) top-down feature reconstructing. Specifically, the bottom-up process focuses on learning image-specific codewords (codebook) using deep-layer features and generating a coding matrix with the shallow-layer feature map. In the top-down process, the learned codebook and coding matrix are used to rebuild high-resolution features via a lightweight feature reconstructing operator (FRO). In addition, our EFRNet is constructed on a new building block, named efficient adaptive abstraction (EAA) block, to further reduce the overall network parameters and achieve a significant speed up. Extensive experiments are conducted on challenging benchmarks, such as CamVid and Cityscapes. The results show that EFRNet demonstrates state-of-the-art performance with an optimal balance between accuracy and speed.},
  archive      = {J_TMM},
  author       = {Xin Li and Fan Yang and Ao Luo and Zhicheng Jiao and Hong Cheng and Zicheng Liu},
  doi          = {10.1109/TMM.2021.3089422},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2852-2865},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {EFRNet: Efficient feature reconstructing network for real-time scene parsing},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two exposure fusion using prior-aware generative adversarial
network. <em>TMM</em>, <em>24</em>, 2841–2851. (<a
href="https://doi.org/10.1109/TMM.2021.3089324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Producing a high dynamic range (HDR) image from two low dynamic range (LDR) images with extreme exposures is challenging due to the lack of well-exposed contents. Existing works either use pixel fusion based on weighted quantization or conduct feature fusion using deep learning techniques. In contrast to these methods, our core idea is to progressively incorporate the pixel domain knowledge of LDR images into the feature fusion process. Specifically, we propose a novel Prior-Aware Generative Adversarial Network (PA-GAN), along with a new dual-level loss for two exposure fusion. The proposed PA-GAN is composed of a content-prior-guided encoder and a detail-prior-guided decoder, respectively in charge of content fusion and detail calibration. We further train the network using a dual-level loss that combines the semantic-level loss and pixel-level loss. Extensive qualitative and quantitative evaluations on diverse image datasets demonstrate that our proposed PA-GAN has superior performance than state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Jia-Li Yin and Bo-Hao Chen and Yan-Tsung Peng},
  doi          = {10.1109/TMM.2021.3089324},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2841-2851},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Two exposure fusion using prior-aware generative adversarial network},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Disentangling semantic-to-visual confusion for zero-shot
learning. <em>TMM</em>, <em>24</em>, 2828–2840. (<a
href="https://doi.org/10.1109/TMM.2021.3089017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using generative models to synthesize visual features from semantic distribution is one of the most popular solutions to ZSL image classification in recent years. The triplet loss (TL) is popularly used to generate realistic visual distributions from semantics by automatically searching discriminative representations. However, the traditional TL cannot search reliable unseen disentangled representations due to the unavailability of unseen classes in ZSL. To alleviate this drawback, we propose in this work a multi-modal triplet loss (MMTL) which utilizes multi-modal information to search a disentangled representation space. As such, all classes can interplay which can benefit learning disentangled class representations in the searched space. Furthermore, we develop a novel model called Disentangling Class Representation Generative Adversarial Network (DCR-GAN) focusing on exploiting the disentangled representations in training, feature synthesis, and final recognition stages. Benefiting from the disentangled representations, DCR-GAN could fit a more realistic distribution over both seen and unseen features. Extensive experiments show that our proposed model can lead to superior performance to the state-of-the-arts on four benchmark datasets.},
  archive      = {J_TMM},
  author       = {Zihan Ye and Fuyuan Hu and Fan Lyu and Linyan Li and Kaizhu Huang},
  doi          = {10.1109/TMM.2021.3089017},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2828-2840},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Disentangling semantic-to-visual confusion for zero-shot learning},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A real-time semi-supervised deep tone mapping network.
<em>TMM</em>, <em>24</em>, 2815–2827. (<a
href="https://doi.org/10.1109/TMM.2021.3089019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tone mapping operators (TMOs) can compress the range of high dynamic range (HDR) images so that they can be displayed normally on the low dynamic range (LDR) devices. Recent TMOs based on deep neural networks can produce impressive results, but there are still some shortcomings. On the one hand, their supervised learning procedure requires a high-quality paired dataset which is hard to be accessed. On the other hand, they are too slow and heavy to meet the needs of practical applications. This paper proposes a real-time deep semi-supervised learning TMO to solve the above problems. The proposed method learns in a semi-supervised manner by combining the adversarial loss, cycle consistency loss, and the pixel-wise loss. The first two can simulate the image distributions in the real world from the unpaired LDR data and the latter can learn the guidance of paired LDR labels. In this way, the proposed method only requires HDR sources, unpaired high-quality LDR images, and a few well tone-mapped HDR-LDR pairs as training data. Furthermore, the proposed method divides tone mapping into luminance mapping and saturation adjustment and then processes them simultaneously. By this strategy, we can reconstruct each component more precisely. Based on the aforementioned improvements, we propose a lightweight tone mapping network that is efficient in tone mapping task (up to 5000x parameters-saving and 27x time-saving compared to the learning-based TMOs). Both quantitative and qualitative results demonstrate that the proposed method performs favorable against state-of-the-art TMOs.},
  archive      = {J_TMM},
  author       = {Ning Zhang and Yang Zhao and Chao Wang and Ronggang Wang},
  doi          = {10.1109/TMM.2021.3089019},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2815-2827},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A real-time semi-supervised deep tone mapping network},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SPGNet: Serial and parallel group network. <em>TMM</em>,
<em>24</em>, 2804–2814. (<a
href="https://doi.org/10.1109/TMM.2021.3088639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural-network Processing Units (NPU), which specializes in the acceleration of deep neural networks (DNN), is of great significance to latency-sensitive areas like robotics or edge computing. However, there are few works focusing on the network design for NPU in recent studies. Most of the popular lightweight structures (e.g. MobileNet) are designed with depthwise convolution, which has less computation in theory but is not friendly to existing hardwares, and the speed tested on NPU is not always satisfactory. Even under similar FLOPs (the number of multiply-accumulates), vanilla convolution operation is always faster than depthwise one. In this paper, we will propose a novel architecture named Serial and Parallel Group Network (SPGNet), which can capture discriminative multi-scale information and at the same time keep the structure compact. Extensive evaluations have been conducted on different computer vision tasks, e.g. image classification (CIFAR and ImageNet), object detection (PASCAL VOC and MS COCO) and person re-identification (Market-1501 and DukeMTMC-ReID). The experimental results show that our proposed SPGNet can achieve comparable performance with the state-of-the-art networks while the speed is 120% faster than MobileNetV2 under similar FLOPS and over 300% faster than GhostNet with similar accuracy on NPU.},
  archive      = {J_TMM},
  author       = {Xuan Wang and Shenqi Lai and Zhenhua Chai and Xingjun Zhang and Xueming Qian},
  doi          = {10.1109/TMM.2021.3088639},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2804-2814},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SPGNet: Serial and parallel group network},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning temporal-correlated and channel- decorrelated
siamese networks for visual tracking. <em>TMM</em>, <em>24</em>,
2791–2803. (<a href="https://doi.org/10.1109/TMM.2021.3087340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Siamese network based trackers have attracted growing popularity in visual tracking, which tackle the tracking by template matching between the initial template and successive search regions. The initial template patch is generally encoded into a convolutional feature for matching. However, the limited representational capability of the template feature limits the tracking accuracy. Besides, this fixed representation also fails to adapt to the target appearance changes. To alleviate these issues, we improve the Siamese trackers by introducing temporal correlation and channel decorrelation mechanisms. On the one hand, we consider the channel-wise correlations between the initial and historical template features to adaptively aggregate informative channel-wise representations for template update. On the other hand, we propose a decorrelation regularization to weaken the channel-wise correlations of individual template features. By end-to-end training, we learn a more complete and adaptive template for accurate object tracking. We demonstrate the generality of our approach by applying it to two prevalent Siamese trackers, i.e., SiamFC and SiamRPN. Extensive experiments on seven benchmark datasets verify the effectiveness of our method.},
  archive      = {J_TMM},
  author       = {Mao Xi and Wengang Zhou and Ning Wang and Houqiang Li},
  doi          = {10.1109/TMM.2021.3087340},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2791-2803},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning temporal-correlated and channel- decorrelated siamese networks for visual tracking},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep enhanced weakly-supervised hashing with iterative tag
refinement. <em>TMM</em>, <em>24</em>, 2779–2790. (<a
href="https://doi.org/10.1109/TMM.2021.3087356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On image-sharing websites, images are usually associated with user-generated tags which contain semantic information and are more easily accessible than accurate labels. It is beneficial to utilize such tags as supervised information to learn image feature representation. However, some tags are not related with the image content and disturb the feature learning process. In this paper, we are dedicated to refining such noisy tags and upgrading the image feature learning. To this end, we propose a novel deep enhanced weakly-supervised hashing method, in which tags are adaptively refined according to image content. In our approach, we first map the deep image feature representation into the tag embedding space, and learn the discriminative as well as compact feature representations with the corresponding tags. After that, by referring to the learned feature representation in the first step, we refine the tags to become consistent with image content. The above two steps are alternated until convergence. Finally, we can obtain more content-relevant tags, better image features and binary hashing functions. The experiments on two image datasets prove that the proposed method outperforms the state-of-the-art weakly-supervised deep hashing methods on image retrieval task.},
  archive      = {J_TMM},
  author       = {Min Wang and Wengang Zhou and Qi Tian and Houqiang Li},
  doi          = {10.1109/TMM.2021.3087356},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2779-2790},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep enhanced weakly-supervised hashing with iterative tag refinement},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discriminative siamese complementary tracker with flexible
update. <em>TMM</em>, <em>24</em>, 2766–2778. (<a
href="https://doi.org/10.1109/TMM.2021.3087347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The offline generative Siamese trackers are equipped with the pre-defined anchors and the fixed target template. They overlook the target-background discriminative information, and lack the flexible target-specific update strategy. To overcome above drawbacks, we propose an adaptive and discriminative Siamese complementary tracking network with flexible update scheme. It consists of three collaborate subnetworks: anchor-free Siamese attention classification and regression subnetwork, online discriminative learning with multi-attention and multi-peak suppression, classifier guided template update subnetwork. All of them are interdependent and complementary to enhance each other for accurate target location. Specifically, an anchor-free multi-attention Siamese tracking subnetwork directly classifies the corresponding image patches with reliability assessment, and cascaded regresses the bounding boxes to progressively refine the predicting accuracy. Its evaluation is flexible and general with both proposal and anchor free in per-pixel prediction manner. Then, we integrate an online discriminative classifier optimizing module as a complementary subnetwork. It introduces spatial-temporal attention mechanism to fully explore multi-view multi-scale target-specific features, and evaluates multi-peak suppression to obtain a single centered peak response map. Its classified results can be fused with Siamese classification branch for accurate target location. Finally, the template update subnetwork is guided by the online discriminative classification scores. Extensive experiments on recent tracking datasets verify its top-ranked tracking accuracy and robustness against some state-of-the-art trackers.},
  archive      = {J_TMM},
  author       = {Baojie Fan and Jiandong Tian and Yan Peng and Yandong Tang},
  doi          = {10.1109/TMM.2021.3087347},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2766-2778},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Discriminative siamese complementary tracker with flexible update},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring zero-shot emotion recognition in speech using
semantic-embedding prototypes. <em>TMM</em>, <em>24</em>, 2752–2765. (<a
href="https://doi.org/10.1109/TMM.2021.3087098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech Emotion Recognition (SER) makes it possible for machines to perceive affective information. Our previous research differed from conventional SER endeavours in that it focused on recognising unseen emotions in speech autonomously through machine learning. Such a step would enable the automatic leaning of unknown emerging emotional states. This type of learning framework, however, still relied on manual annotations to obtain multiple samples of each emotion. In order to reduce this additional workload, herein, we propose a zero-shot SER framework employing a per-emotion semantic-embedding paradigm to describe emotions in zero-shot SER, instead of using the sample-wise descriptors. Aiming to optimise the relationship between emotions, prototypes, and speech samples, this framework includes two types of learning strategies: Sample-wise learning and emotion-wise learning. These strategies apply a novel learning process to speech samples and emotions, respectively, via specifically designed semantic-embedding prototypes. We verify the utility of these approaches by performing an extensive experimental evaluation on two corpora on three aspects, namely the influence of different types of learning strategies, emotional-pair comparison, and the selections of semantic-embedding prototypes and paralinguistic features. The experimental results indicate that it is applicable to use semantic-embedding prototypes for zero-shot emotion recognition in speech, despite the influence of choosing optimal strategies and prototypes.},
  archive      = {J_TMM},
  author       = {Xinzhou Xu and Jun Deng and Nicholas Cummins and Zixing Zhang and Li Zhao and Björn W. Schuller},
  doi          = {10.1109/TMM.2021.3087098},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2752-2765},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Exploring zero-shot emotion recognition in speech using semantic-embedding prototypes},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ToF and stereo data fusion using dynamic search range stereo
matching. <em>TMM</em>, <em>24</em>, 2739–2751. (<a
href="https://doi.org/10.1109/TMM.2021.3087017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-of-Flight (ToF) sensors and stereo vision systems are both widely used for capturing depth data. They have some complementary strengths and limitations, which have been exploited in prior research to produce more accurate depth maps by fusing data from the two sources. However, among these diverse data fusion approaches, none of them provides an end-to-end neural network solution. In this work, we propose the first end-to-end ToF and stereo data fusion network using the coarse-to-fine matching framework, where the prior of ToF depth is integrated into the stereo matching process by constraining the search range of stereo matching within an interval around the ToF camera depth measurement. We adopt a dynamic search range for each pixel according to an estimated ToF error map, which is more efficient and effective than a constant one when handling various errors. The ToF error map is estimated by the ToF error estimator branching out from the stereo matching network. Both ToF error estimation and stereo matching are performed in a joint framework, with the two tasks assisting each other mutually. We also propose an upsampling module to replace the naive bilinear upsampling in the coarse-to-fine stereo matching network, which reduces the error caused by the upsampling. The proposed deep network is trained end-to-end on synthetic datasets and generalizable to real-world datasets without further fine-tuning. Experimental results show that our fusion method achieves higher accuracy than either ToF or stereo alone, and outperforms state-of-the-art fusion methods on both synthetic and real data.},
  archive      = {J_TMM},
  author       = {Yong Deng and Jimin Xiao and Steven Zhiying Zhou},
  doi          = {10.1109/TMM.2021.3087017},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2739-2751},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ToF and stereo data fusion using dynamic search range stereo matching},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prior-induced information alignment for image matting.
<em>TMM</em>, <em>24</em>, 2727–2738. (<a
href="https://doi.org/10.1109/TMM.2021.3087007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image matting is an ill-posed problem that aims to estimate the opacity of foreground pixels in an image. However, most existing deep learning-based methods still suffer from the coarse-grained details. In general, these algorithms are incapable of felicitously distinguishing the degree of exploration between deterministic domains ( e.g. certain FG and BG pixels) and undetermined domains ( e.g. uncertain in-between pixels), or inevitably lose information in the continuous sampling process, leading to a sub-optimal result. In this paper, we propose a novel network named Prior-Induced Information Alignment Matting Network (PIIAMatting), which can efficiently model the distinction of pixel-wise response maps and the correlation of layer-wise feature maps. It mainly consists of a Dynamic Gaussian Modulation mechanism (DGM) and an Information Alignment strategy (IA). Specifically, the DGM can dynamically acquire a pixel-wise domain response map learned from the prior distribution. The response map can present the relationship between the opacity variation and the convergence process during training. On the other hand, the IA comprises an Information Match Module (IMM) and an Information Aggregation Module (IAM), jointly scheduled to match and aggregate the adjacent layer-wise features adaptively. Besides, we also develop a Multi-Scale Refinement (MSR) module to integrate multi-scale receptive field information at the refinement stage to recover the fluctuating appearance details. Extensive quantitative and qualitative evaluations demonstrate that the proposed PIIAMatting performs favourably against state-of-the-art image matting methods on the Alphamatting.com , Composition-1 K and Distinctions-646 dataset.},
  archive      = {J_TMM},
  author       = {Yuhao Liu and Jiake Xie and Yu Qiao and Yong Tang and Xin Yang},
  doi          = {10.1109/TMM.2021.3087007},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2727-2738},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Prior-induced information alignment for image matting},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep arbitrary HDRI: Inverse tone mapping with controllable
exposure changes. <em>TMM</em>, <em>24</em>, 2713–2726. (<a
href="https://doi.org/10.1109/TMM.2021.3087034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) have recently made significant advances in the inverse tone mapping technique, which generates a high dynamic range (HDR) image from a single low dynamic range (LDR) image that has lost information in over- and under-exposed regions. The end-to-end inverse tone mapping approach specifies the dynamic range in advance, thereby limiting dynamic range expansion. In contrast, the method of generating multiple exposure LDR images from a single LDR image and subsequently merging them into an HDR image enables flexible dynamic range expansion. However, existing methods for generating multiple exposure LDR images require an additional network for each exposure value to be changed or a process of recursively inferring images that have different exposure values. Therefore, the number of parameters increases significantly due to the use of additional networks, and an error accumulation problem arises due to recursive inference. To solve this problem, we propose a novel network architecture that can control arbitrary exposure values without adding networks or applying recursive inference. The training method of the auxiliary classifier-generative adversarial network structure is employed to generate the image conditioned on the specified exposure. The proposed network uses a newly designed spatially-adaptive normalization to address the limitation of existing methods that cannot sufficiently restore image detail due to the spatially equivariant nature of the convolution. Spatially-adaptive normalization facilitates restoration of the high frequency component by applying different normalization parameters to each element in the feature map according to the characteristics of the input image. Experimental results show that the proposed method outperforms state-of-the-art methods, yielding a 5.48dB higher average peak signal-to-noise ratio, a 0.05 higher average structure similarity index, a 0.28 higher average multi-scale structure similarity index, and a 7.36 higher average HDR-VDP-2 for various datasets.},
  archive      = {J_TMM},
  author       = {So Yeon Jo and Siyeong Lee and Namhyun Ahn and Suk-Ju Kang},
  doi          = {10.1109/TMM.2021.3087034},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2713-2726},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep arbitrary HDRI: Inverse tone mapping with controllable exposure changes},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical user intent graph network for multimedia
recommendation. <em>TMM</em>, <em>24</em>, 2701–2712. (<a
href="https://doi.org/10.1109/TMM.2021.3088307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding user preference on item context is the key to acquire a high-quality multimedia recommendation. Typically, the pre-existing features of items are derived from pre-trained models (e.g. visual features of micro-videos extracted from some neural networks), and then introduced into the recommendation framework (e.g. collaborative filtering) to capture user preference. However, we argue that such a paradigm is insufficient to output satisfactory user representations, which hardly profile personal interests well. The key reason is that present works largely leave user intents untouched, then failing to encode such informative representation of users. In this work, we aim to learn multi-level user intents from the co-interacted patterns of items, so as to obtain high-quality representations of users and items and further enhance the recommendation performance. Towards this end, we develop a novel framework, Hierarchical User Intent Graph Network , which exhibits user intents in a hierarchical graph structure, from the fine-grained to coarse-grained intents. In particular, we get the multi-level user intents by recursively performing two operations: 1) intra-level aggregation, which distills the signal pertinent to user intents from co-interacted item graphs; and 2) inter-level aggregation, which constitutes the supernode in higher levels to model coarser-grained user intents via gathering the nodes’ representations in the lower ones. Then, we refine the user and item representations as a distribution over the discovered intents, instead of simple pre-existing features. To demonstrate the effectiveness of our model, we conducted extensive experiments on three public datasets. Our model achieves significant improvements over the state-of-the-art methods, including MMGCN and DisenGCN. Furthermore, by visualizing the item representations, we provide the semantics of user intents.},
  archive      = {J_TMM},
  author       = {Yinwei Wei and Xiang Wang and Xiangnan He and Liqiang Nie and Yong Rui and Tat-Seng Chua},
  doi          = {10.1109/TMM.2021.3088307},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2701-2712},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical user intent graph network for multimedia recommendation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling instant user intent and content-level transition
for sequential fashion recommendation. <em>TMM</em>, <em>24</em>,
2687–2700. (<a href="https://doi.org/10.1109/TMM.2021.3088281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fashion recommendation, aiming to explore specific user preference in fashion, has become an important research topic for its practical significance to the fashion business sector. However, little work has been done on an important sub-task called sequential fashion recommendation, which aims to capture additional short-term fashion interest of users by modeling the item-to-item transitions. In this paper, we propose a novel Attentional Content-level Translation-based Recommender (ACTR) framework, which simultaneously models the instant user intent of each transition and the intent-specific transition probability. Specifically, we define instant intent with the relationships between adjacent items that the users interacted, which are the three fundamental domain-specific relationships of: match , substitute and others . To further exploit the characteristics of fashion domain and alleviate the item transition sparsity problem, we augment the item-level transition modeling with multiple sub-transitions using various content-level attributes. An attention mechanism is further devised to effectively aggregate multiple content-level transitions. To the best of our knowledge, this is the first work that specifies the implicit user actions in online fashion shopping with explicit instant intent, which enhances the connectivity of fashion items and boosts the recommendation performance. Extensive experiments on two real-world fashion E-commerce datasets demonstrate the effectiveness of the proposed method in sequential fashion recommendation.},
  archive      = {J_TMM},
  author       = {Yujuan Ding and Yunshan Ma and Wai Keung Wong and Tat-Seng Chua},
  doi          = {10.1109/TMM.2021.3088281},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2687-2700},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Modeling instant user intent and content-level transition for sequential fashion recommendation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CariMe: Unpaired caricature generation with multiple
exaggerations. <em>TMM</em>, <em>24</em>, 2673–2686. (<a
href="https://doi.org/10.1109/TMM.2021.3086722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Caricature generation aims to translate real photos into caricatures with artistic styles and shape exaggerations while maintaining the identity of the subject. Different from generic image-to-image translation, drawing caricatures automatically is a more challenging task due to the existence of various spatial deformations. Previous caricature generation methods are obsessed with predicting definite image warping from a given photo while ignoring the intrinsic representation and distribution of geometric exaggerations in caricatures. This limits their ability on diverse exaggeration generation. In this paper, we generalize the caricature generation problem from instance-level warping prediction to distribution-level deformation modeling. Based on this assumption, we present the first exploration for unpaired CARIcature generation with Multiple Exaggerations (CariMe) . Technically, we propose a Multi-exaggeration Warper network to learn the distribution-level mapping from photos to facial exaggerations. This makes it possible to generate diverse and reasonable exaggerations from randomly sampled warp codes given one input photo. To better represent the facial exaggeration and produce fine-grained warping, a deformation-field-based warping method is also proposed, which captures more detailed exaggerations than previous point-based warping methods. Experiments and two perceptual studies prove the superiority of our method comparing with other state-of-the-art methods, showing the improvement of our work on caricature generation. The source code is available at https://github.com/edward3862/CariMe-pytorch .},
  archive      = {J_TMM},
  author       = {Zheng Gu and Chuanqi Dong and Jing Huo and Wenbin Li and Yang Gao},
  doi          = {10.1109/TMM.2021.3086722},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2673-2686},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CariMe: Unpaired caricature generation with multiple exaggerations},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional sentence generation and cross-modal reranking
for sign language translation. <em>TMM</em>, <em>24</em>, 2662–2672. (<a
href="https://doi.org/10.1109/TMM.2021.3087006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign Language Translation (SLT) aims to generate spoken language translations from sign language videos. Currently, the available sign language datasets are relatively too small to learn the linguistic properties of spoken language. In this paper, towards effective SLT, we propose a novel framework which takes the advantage of the spoken language grammar learnt from a large corpus of text sentences. Our framework consists of three key modules: word existence verification, conditional sentence generation and cross-modal re-ranking. We first check the existence of words in the vocabulary by a series of binary classification in parallel. After that, the appearing words are assembled and guided by a pretrained spoken language generator to produce multiple candidate sentences in spoken language manner. Last but not least, we select the sentence most semantically similar to the input sign video as the translation result with a crossmodal re-ranking model. We evaluate our framework on two large scale continuous SLT benchmarks, i.e. , CSL and RWTHPHOENIX-Weather 2014 T. Experimental results demonstrate that the proposed framework achieves promising performance on both datasets.},
  archive      = {J_TMM},
  author       = {Jian Zhao and Weizhen Qi and Wengang Zhou and Nan Duan and Ming Zhou and Houqiang Li},
  doi          = {10.1109/TMM.2021.3087006},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2662-2672},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Conditional sentence generation and cross-modal reranking for sign language translation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LAGA-net: Local-and-global attention network for skeleton
based action recognition. <em>TMM</em>, <em>24</em>, 2648–2661. (<a
href="https://doi.org/10.1109/TMM.2021.3086758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition has attracted significant attention and obtained widespread applications due to the robustness of 3D skeleton data. One of the key challenges is how to extract discriminative and robust spatio-temporal features from sparse skeleton data to describe actions and improve recognition accuracy. To address this issue, this paper combines convolutions with attention mechanisms and proposes a deep network for skeleton-based action recognition, termed as local-and-global attention network (LAGA-Net). First, we encode skeleton sequences into joint feature evolution maps to compactly describe the spatial and temporal characteristics of skeleton sequences. Then, a motion guided channel attention module (MGCAM) is proposed to model the interdependencies between feature channels by calculating temporal frame-level motion and enhance motion-salient features in a channel-wise way. Further, a spatio-temporal attention module (STAM) is proposed to model spatio-temporal context-aware collaboration at sequence level and extract spatio-temporal attention features that involve long-range dependencies. Together, MGCAM and STAM are combined to form LAGA-Net, which extracts discriminative features integrating both local and global representations of skeleton sequences. Moreover, a two-stream architecture is proposed to learn complementary features from joint and bone aspects. We conduct extensive experiments to verify the effectiveness and superiority of our proposed method over state-of-the-art approaches on several benchmarks (e.g., NTU RGB+D, Northwestern-UCLA, UTD-MHAD and NTU RGB+D 120).},
  archive      = {J_TMM},
  author       = {Rongjie Xia and Yanshan Li and Wenhan Luo},
  doi          = {10.1109/TMM.2021.3086758},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2648-2661},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {LAGA-net: Local-and-global attention network for skeleton based action recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Crowd counting via perspective-guided fractional-dilation
convolution. <em>TMM</em>, <em>24</em>, 2633–2647. (<a
href="https://doi.org/10.1109/TMM.2021.3086709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting is critical for numerous video surveillance scenarios. One of the main issues in this task is how to handle the dramatic scale variations of pedestrians caused by the perspective effect. To address this issue, this paper proposes a novel convolution neural network-based crowd counting method, termed Perspective-guided Fractional-Dilation Network (PFDNet). By modeling the continuous scale variations, the proposed PFDNet is able to select the proper fractional-dilation kernels for adapting to different spatial locations. It significantly improves the flexibility of the state-of-the-arts that only consider the discrete representative scales. In addition, by avoiding the multi-scale or multi-column architecture that used in other methods, it is computationally more efficient. In practice, the proposed PFDNet is constructed by stacking multiple Perspective-guided Fractional-Dilation Convolutions (PFC) on a VGG16-BN backbone. By introducing a novel generalized dilation convolution operation, the PFC can handle fractional dilation ratios in the spatial domain under the guidance of perspective annotations, achieving continuous scales modeling of pedestrians. To deal with the problem of unavailable perspective information in some cases, we further introduce an effective perspective estimation branch to the proposed PFDNet, which can be trained in either supervised or weakly-supervised setting once the branch has been pre-trained. Extensive experiments show that the proposed PFDNet outperforms state-of-the-art methods on ShanghaiTech A, ShanghaiTech B, WorldExpo’10, UCF-QNRF, UCF_CC_50 and TRANCOS dataset, achieving MAE 53.8, 6.5, 6.8, 84.3205.8, and 3.06 respectively.},
  archive      = {J_TMM},
  author       = {Zhaoyi Yan and Ruimao Zhang and Hongzhi Zhang and Qingfu Zhang and Wangmeng Zuo},
  doi          = {10.1109/TMM.2021.3086709},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2633-2647},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Crowd counting via perspective-guided fractional-dilation convolution},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep-PCAC: An end-to-end deep lossy compression framework
for point cloud attributes. <em>TMM</em>, <em>24</em>, 2617–2632. (<a
href="https://doi.org/10.1109/TMM.2021.3086711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large data volume of point clouds poses severe challenges for efficient storage and transmission in recent years. In this paper, we propose the first -- to our best knowledge -- end-to-end deep framework for compressing point cloud attributes. Specifically, we propose a point cloud lossy attribute autoencoder, which directly encodes and decodes point cloud attributes with the help of geometry, instead of voxelizing or projecting the points. In the autoencoder, we propose a second-order point convolution that utilizes the spatial correlations between more points and the nonlinear relationship between attribute features. We introduce a dense point-inception block, which derives from a combination of an inception-style block and a dense block, to improve feature propagation. In addition, we devise a multiscale loss to guide the autoencoder in focusing attention on the coarse-grained points with better coverage of the entire point cloud, which makes it easier for the autoencoder to obtain better optimization of the qualities of all points. Experimental results show that our proposed framework still has a performance gap compared with the state-of-the-art algorithms in the MPEG G-PCC reference software TMC13. However, it does outperform the RAHT-RLGR, which is one of the core transforms used in TMC13 without many well-designed techniques that make TMC13 what it is today. It outperforms RAHT-RLGR by 2.63 dB, 1.77 dB, and 3.40 dB on average in terms of the BD-PSNR for the Y, U, and V components. A subjective quality comparison demonstrates that our framework can preserve more textures and reduce blocking and color noise artifacts.},
  archive      = {J_TMM},
  author       = {Xihua Sheng and Li Li and Dong Liu and Zhiwei Xiong and Zhu Li and Feng Wu},
  doi          = {10.1109/TMM.2021.3086711},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2617-2632},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep-PCAC: An end-to-end deep lossy compression framework for point cloud attributes},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TaoHighlight: Commodity-aware multi-modal video highlight
detection in e-commerce. <em>TMM</em>, <em>24</em>, 2606–2616. (<a
href="https://doi.org/10.1109/TMM.2021.3087001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In e-commerce, product related video is important content to introduce product characteristics and attract consumers. Especially in the recommendation system of e-commerce platform, video highlight detection methods are usually adopted to capture the most attractive clips for showing to consumers, so as to improve the click through rate of products. However, the effect of the current research methods applied to the actual scene is not satisfactory. Compared with other video understanding tasks, video highlight detection is relatively abstract and subjective, and it is difficult to make accurate judgment only by using visual information. Consequently, we put forward multi-modal video highlight detection task, which introduces video related linguistic information as supervised information. And we propose a graph-based commodity-aware model to solve multi-modal video highlight detection in e-commerce scene. Our model consists of multi-modal highlight detection stage and graph-based fine-tuning stage, in which we adopt graph aggregation method to fuse multi-source natural language information and introduce effective visual feature composition method for graph convolution network based highlight detection. Besides, we release the largest e-commerce video highlight detection dataset, TaoHighlight, in which the videos and related data are collected from Taobao e-commerce platform. Our model achieves state-of-art in all separate categories and overall dataset of TaoHighlight, which shows the superiority of our model.},
  archive      = {J_TMM},
  author       = {Zhaoyu Guo and Zhou Zhao and Weike Jin and Dazhou Wang and Ruitao Liu and Jun Yu},
  doi          = {10.1109/TMM.2021.3087001},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2606-2616},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TaoHighlight: Commodity-aware multi-modal video highlight detection in E-commerce},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discover micro-influencers for brands via better
understanding. <em>TMM</em>, <em>24</em>, 2595–2605. (<a
href="https://doi.org/10.1109/TMM.2021.3087038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the influencer marketing industry in recent years, the cooperation between brands and micro-influencers on marketing has achieved much attention. As a key sub-task of influencer marketing, micro-influencer recommendation is gaining momentum. However, in influencer marketing campaigns, it is not enough to only consider marketing effectiveness. Towards this end, we propose a concept-based micro-influencer ranking framework, to address the problems of marketing effectiveness and self-development needs for the task of micro-influencer recommendation. Marketing effectiveness is improved by concept-based social media account representation and a micro-influencer ranking function. We conduct social media account representation from the perspective of historical activities and marketing direction. And two adaptive learned metrics, endorsement effect score and micro-influencer influence score, are defined to learn the micro-influencer ranking function. To meet self-development needs, we design a bi-directional concept attention mechanism to focus on brands’ and micro-influencers’ marketing direction over social media concepts. Interpretable concept-based parameters are utilized to help brands and micro-influencers make marketing decisions. Extensive experiments conducted on a real-world dataset demonstrate the advantage of our proposed method compared with the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Shaokun Wang and Tian Gan and Yuan Liu and Li Zhang and JianLong Wu and Liqiang Nie},
  doi          = {10.1109/TMM.2021.3087038},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2595-2605},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Discover micro-influencers for brands via better understanding},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Instance GNN: A learning framework for joint symbol
segmentation and recognition in online handwritten diagrams.
<em>TMM</em>, <em>24</em>, 2580–2594. (<a
href="https://doi.org/10.1109/TMM.2021.3087000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online handwritten diagram recognition (OHDR) has attracted considerable attention for its potential applications in many areas, but it is a challenging task due to the complex 2D structure, writing style variation, and lack of annotated data. Existing OHDR methods often have limitations in modeling and learning complex contextual relationships. To overcome these challenges, we propose an OHDR method based on graph neural networks (GNNs) in this paper. In particular, we formulate symbol segmentation and symbol recognition as node clustering and node classification problems on stroke graphs and solve the problems jointly under a unified learning framework with a GNN model. This GNN model is denoted as Instance GNN since it gives the symbol instance label as well as the semantic label. Extensive experiments on two flowchart datasets and a finite automata dataset show that our method consistently outperforms previous methods with large margins and achieves state-of-the-art performance. In addition, we release a large-scale annotated online handwritten flowchart dataset, CASIA-OHFC, and provide initial experimental results as a baseline.},
  archive      = {J_TMM},
  author       = {Xiao-Long Yun and Yan-Ming Zhang and Fei Yin and Cheng-Lin Liu},
  doi          = {10.1109/TMM.2021.3087000},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2580-2594},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Instance GNN: A learning framework for joint symbol segmentation and recognition in online handwritten diagrams},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Projective multiple kernel subspace clustering.
<em>TMM</em>, <em>24</em>, 2567–2579. (<a
href="https://doi.org/10.1109/TMM.2021.3086727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple kernel subspace clustering (MKSC), as an important extension for handling multi-view non-linear subspace data, has shown notable success in a wide variety of machine learning tasks. The key objective of MKSC is to build a flexible and appropriate graph for clustering from the kernel space. However, existing MKSC methods apply a mechanism utilizing the kernel trick to the traditional self-expressive principle, where the similarity graphs are built on the respective high-dimensional (or even infinite) reproducing kernel Hilbert space (RKHS). Regarding this strategy, we argue that the original high-dimensional spaces usually include noise and unreliable similarity measures and, therefore, output a low-quality graph matrix, which degrades clustering performance. In this paper, inspired by projective clustering, we propose the utilization of a complementary similarity graph by fusing the multiple kernel graphs constructed in the low-dimensional partition space, termed projective multiple kernel subspace clustering (PMKSC). By incorporating intrinsic structures with multi-view data, PMKSC alleviates the noise and redundancy in the original kernel space and obtains high-quality similarity to uncover the underlying clustering structures. Furthermore, we design a three-step alternate algorithm with proven convergence to solve the proposed optimization problem. The experimental results on ten multiple kernel benchmark datasets validate the effectiveness of our proposed PMKSC, compared to the state-of-the-art multiple kernel and kernel subspace clustering methods, by a large margin. Our code is available at https://github.com/MengjingSun/PMKSC-code .},
  archive      = {J_TMM},
  author       = {Mengjing Sun and Siwei Wang and Pei Zhang and Xinwang Liu and Xifeng Guo and Sihang Zhou and En Zhu},
  doi          = {10.1109/TMM.2021.3086727},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2567-2579},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Projective multiple kernel subspace clustering},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-level temporal dilated dense prediction for action
recognition. <em>TMM</em>, <em>24</em>, 2553–2566. (<a
href="https://doi.org/10.1109/TMM.2021.3087023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D convolutional neural networks have achieved great success for action recognition. However, large variations of temporal dynamics have not been properly processed and low-level features have not been fully exploited in most existing works. To solve these two problems, we present a general and flexible framework, namely multi-level temporal dilated dense prediction network, which can incorporate with most of existing methods as backbone to improve the temporal modeling capacity. In the proposed method, a novel temporal dilated dense prediction block is designed to fully utilize temporal features with various temporal dilated rates for dense prediction while maintaining relatively low computational cost. To fuse information from low to high levels, our method combines the predictions from multiple such blocks inserted at different stages of the backbone network. In-depth analysis is given to show that short- to long-term temporal dependencies can be captured and multi-level spatio-temporal features are effectively fused for video action recognition by the proposed method. Experimental results demonstrate that our method achieves impressive performance improvement on four publicly available action recognition benchmarks including Charades, Kinetics, Something-Something-V1 and HMDB51.},
  archive      = {J_TMM},
  author       = {Jinpeng Wang and Yiqi Lin and Manlin Zhang and Yuan Gao and Andy J. Ma},
  doi          = {10.1109/TMM.2021.3087023},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2553-2566},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-level temporal dilated dense prediction for action recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Speaker-independent speech animation using perceptual loss
functions and synthetic data. <em>TMM</em>, <em>24</em>, 2539–2552. (<a
href="https://doi.org/10.1109/TMM.2021.3087020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a real-time speaker-independent speech-to-facial animation system that predicts lip and jaw movements on a reference face for audio speech taken from any speaker. Our approach is motivated by two key observations; 1) Speakerindependent facial animation can be generated from phoneme labels, but to perform this automatically a speech recogniser is needed which, due to contextual look-ahead, introduces too much time lag. 2) Audio-driven speech animation can be performed in real-time but requires large, multi-speaker audio-visual speech datasets of which there are few. We adopt a novel threestage training procedure that leverages the advantages of each approach. First we train a phoneme -to-visual speech model from a large single-speaker audio-visual dataset. Next, we use this model to generate the synthetic visual component of a large multi-speaker audio dataset of which the video is not available. Finally, we learn an audio -to-visual speech mapping using the synthetic visual features as the target. Furthermore, we increase the realism of the predicted facial animation by introducing two perceptually-based loss functions that aim to improve mouth closures and openings. The proposed method and loss functions are evaluated objectively using mean square error, global variance and a new metric that measures the extent of mouth opening. Subjective tests show that our approach produces facial animation comparable to those produced from phoneme sequences and that improved mouth closures, particularly for bilabial closures, are achieved.},
  archive      = {J_TMM},
  author       = {Danny Websdale and Sarah Taylor and Ben Milner},
  doi          = {10.1109/TMM.2021.3087020},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2539-2552},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Speaker-independent speech animation using perceptual loss functions and synthetic data},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MFFENet: Multiscale feature fusion and enhancement network
for RGB–thermal urban road scene parsing. <em>TMM</em>, <em>24</em>,
2526–2538. (<a href="https://doi.org/10.1109/TMM.2021.3086618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with traditional handcrafted features, deep learning has greatly improved the performance of scene parsing. However, it remains challenging under various environmental conditions caused by imaging limitations. Thermal imaging cameras have several advantages over cameras for the visible spectrum, such as operation in total darkness, robustness to shadow effects, insensitivity to illumination variations, and strong ability to penetrate smog and haze. These advantages of thermal imaging cameras make them ideal for the scene parsing of semantic objects in daytime and nighttime. In this paper, we propose a novel multiscale feature fusion and enhancement network (MFFENet) for accurate parsing of RGB–thermal urban road scenes even when the quality of the available RGB data is compromised. The proposed MFFENet consists of two encoders, a feature fusion layer, and a multi-label supervision layer. We concatenate the multi-scale features with the features that contain global semantic information. Furthermore, we explore the cross-modal fusion of RGB and thermal features at multiple stages, rather than fusing them once at the low or high stage. Then, we propose a spatial attention mechanism module that provides a higher weight to (focuses more on) the foreground area, allowing MFFENet to emphasize foreground objects. Finally, multi-label supervision is introduced to optimize parameters of the proposed MFFENet. Experimental results confirm that the proposed MFFENet outperforms similar high-performing methods.},
  archive      = {J_TMM},
  author       = {Wujie Zhou and Xinyang Lin and Jingsheng Lei and Lu Yu and Jenq-Neng Hwang},
  doi          = {10.1109/TMM.2021.3086618},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2526-2538},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MFFENet: Multiscale feature fusion and enhancement network for RGB–Thermal urban road scene parsing},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Broad-to-narrow registration and identification of 3D
objects in partially scanned and cluttered point clouds. <em>TMM</em>,
<em>24</em>, 2230–2245. (<a
href="https://doi.org/10.1109/TMM.2021.3089838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The new generation 3D scanner devices have revolutionized the way information from 3D objects is acquired, making the process of scene capturing and digitization straightforward. However, the effectiveness and robustness of conventional algorithms for real scene analysis are usually deteriorated due to challenging conditions, such as noise, low resolution, and bad perceptual quality. In this work, we present a methodology for identifying and registering partially-scanned and noisy 3D objects, lying in arbitrary positions in a 3D scene, with corresponding high-quality models. The methodology is assessed on point cloud scenes with multiple objects with large missing parts. The proposed approach does not require connectivity information and is thus generic and computationally efficient, thereby facilitating computationally demanding applications, like augmented reality. The main contributions of this work are the introduction of a layered joint registration and indexing scheme of cluttered partial point clouds using a novel multi-scale saliency extraction technique to identify distinctive regions, and an enhanced similarity criterion for object-to-model matching. The processing time of the process is also accelerated through 3D scene segmentation. Comparisons of the proposed methodology with other state-of-the-art approaches highlight its superiority under challenging conditions.},
  archive      = {J_TMM},
  author       = {Gerasimos Arvanitis and Evangelia I. Zacharaki and Libor Váŝa and Konstantinos Moustakas},
  doi          = {10.1109/TMM.2021.3089838},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2230-2245},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Broad-to-narrow registration and identification of 3D objects in partially scanned and cluttered point clouds},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-modal food retrieval: Learning a joint embedding of
food images and recipes with semantic consistency and attention
mechanism. <em>TMM</em>, <em>24</em>, 2515–2525. (<a
href="https://doi.org/10.1109/TMM.2021.3083109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food retrieval is an important task to perform analysis of food-related information, where we are interested in retrieving relevant information about the queried food item such as ingredients, cooking instructions, etc. In this paper, we investigate cross-modal retrieval between food images and cooking recipes. The goal is to learn an embedding of images and recipes in a common feature space, such that the corresponding image-recipe embeddings lie close to one another. Two major challenges in addressing this problem are 1) large intra-variance and small inter-variance across cross-modal food data; and 2) difficulties in obtaining discriminative recipe representations. To address these two problems, we propose Semantic-Consistent and Attention-based Networks (SCAN), which regularize the embeddings of the two modalities through aligning output semantic probabilities. Besides, we exploit a self-attention mechanism to improve the embedding of recipes. We evaluate the performance of the proposed method on the large-scale Recipe1M dataset, and show that we can outperform several state-of-the-art cross-modal retrieval strategies for food images and cooking recipes by a significant margin.},
  archive      = {J_TMM},
  author       = {Hao Wang and Doyen Sahoo and Chenghao Liu and Ke Shu and Palakorn Achananuparp and Ee-peng Lim and Steven C. H. Hoi},
  doi          = {10.1109/TMM.2021.3083109},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2515-2525},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross-modal food retrieval: Learning a joint embedding of food images and recipes with semantic consistency and attention mechanism},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncertainty-aware unsupervised domain adaptation in object
detection. <em>TMM</em>, <em>24</em>, 2502–2514. (<a
href="https://doi.org/10.1109/TMM.2021.3082687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptive object detection aims to adapt detectors from a labelled source domain to an unlabelled target domain. Most existing works take a two-stage strategy that first generates region proposals and then detects objects of interest, where adversarial learning is widely adopted to mitigate the inter-domain discrepancy in both stages. However, adversarial learning may impair the alignment of well-aligned samples as it merely aligns the global distributions across domains. To address this issue, we design an uncertainty-aware domain adaptation network (UaDAN) that introduces conditional adversarial learning to align well-aligned and poorly-aligned samples separately in different manners. Specifically, we design an uncertainty metric that assesses the alignment of each sample and adjusts the strength of adversarial learning for well-aligned and poorly-aligned samples adaptively. In addition, we exploit the uncertainty metric to achieve curriculum learning that first performs easier image-level alignment and then more difficult instance-level alignment progressively. Extensive experiments over four challenging domain adaptive object detection datasets show that UaDAN achieves superior performance as compared with state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Dayan Guan and Jiaxing Huang and Aoran Xiao and Shijian Lu and Yanpeng Cao},
  doi          = {10.1109/TMM.2021.3082687},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2502-2514},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Uncertainty-aware unsupervised domain adaptation in object detection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A unimodal representation learning and recurrent
decomposition fusion structure for utterance-level multimodal embedding
learning. <em>TMM</em>, <em>24</em>, 2488–2501. (<a
href="https://doi.org/10.1109/TMM.2021.3082398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning a unified embedding for utterance-level video attracts significant attention recently due to the rapid development of social media and its broad applications. An utterance normally contains not only spoken language but also the nonverbal behaviors such as facial expressions and vocal patterns. Instead of directly learning utterance embedding based on low-level features, we firstly explore high-level representation for each modality separately via an unimodal representation learning gyroscope structure. In this way, the learnt unimodal representations are more representative and contain more abstract semantic information. In the gyroscope structure, we introduce multi-scale kernel learning, ‘channel expansion’ and ‘channel fusion’ operations to explore high-level features both spatially and channelwise. Another insight of our method lies in that we fuse representations of all modalities to obtain a unified embedding by interpreting fusion procedure as the flow of inter-modality information between various modalities, which is more specialized in terms of the information to be fused and the fusion process. Specifically, considering that each modality carries modality-specific and cross-modality interactions, we innovate to decompose unimodal representations into intra- and inter-modality dynamics using gating mechanism, and further fuse the inter-modality dynamics by passing them from previous modalities to the following one using a recurrent neural fusion architecture. Extensive experiments demonstrate that our method achieves state-of-the-art performance on multiple benchmark datasets.},
  archive      = {J_TMM},
  author       = {Sijie Mai and Haifeng Hu and Songlong Xing},
  doi          = {10.1109/TMM.2021.3082398},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2488-2501},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A unimodal representation learning and recurrent decomposition fusion structure for utterance-level multimodal embedding learning},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph-based visual-semantic entanglement network for
zero-shot image recognition. <em>TMM</em>, <em>24</em>, 2473–2487. (<a
href="https://doi.org/10.1109/TMM.2021.3082292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning uses semantic attributes to connect the search space of unseen objects. In recent years, although the deep convolutional network brings powerful visual modeling capabilities to the ZSL task, its visual features have severe pattern inertia and lack of representation of semantic relationships, which leads to severe bias and ambiguity. In response to this, we propose the Graph-based Visual-Semantic Entanglement Network to conduct graph modeling of visual features, which is mapped to semantic attributes by using a knowledge graph, it contains several novel designs: 1. it establishes a multi-path entangled network with the convolutional neural network (CNN) and the graph convolutional network (GCN), which input the visual features from CNN to GCN to model the implicit semantic relations, then GCN feedback the graph modeled information to CNN features; 2. it uses attribute word vectors as the target for the graph semantic modeling of GCN, which forms a self-consistent regression for graph modeling and supervise GCN to learn more personalized attribute relations; 3. it fuses and supplements the hierarchical visual-semantic features refined by graph modeling into visual embedding. Our method outperforms state-of-the-art approaches on multiple representative ZSL datasets: AwA2, CUB, and SUN by promoting the semantic linkage modelling of visual features.},
  archive      = {J_TMM},
  author       = {Yang Hu and Guihua Wen and Adriane Chapman and Pei Yang and Mingnan Luo and Yingxue Xu and Dan Dai and Wendy Hall},
  doi          = {10.1109/TMM.2021.3082292},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2473-2487},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Graph-based visual-semantic entanglement network for zero-shot image recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consensus graph learning for multi-view clustering.
<em>TMM</em>, <em>24</em>, 2461–2472. (<a
href="https://doi.org/10.1109/TMM.2021.3081930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering, which exploits the multi-view information to partition data into their clusters, has attracted intense attention. However, most existing methods directly learn a similarity graph from original multi-view features, which inevitably contain noises and redundancy information. The learned similarity graph is inaccurate and is insufficient to depict the underlying cluster structure of multi-view data. To address this issue, we propose a novel multi-view clustering method that is able to construct an essential similarity graph in a spectral embedding space instead of the original feature space. Concretely, we first obtain multiple spectral embedding matrices from the view-specific similarity graphs, and reorganize the gram matrices constructed by the inner product of the normalized spectral embedding matrices into a tensor. Then, we impose a weighted tensor nuclear norm constraint on the tensor to capture high-order consistent information among multiple views. Furthermore, we unify the spectral embedding and low rank tensor learning into a unified optimization framework to determine the spectral embedding matrices and tensor representation jointly. Finally, we obtain the consensus similarity graph from the gram matrices via an adaptive neighbor manner. An efficient optimization algorithm is designed to solve the resultant optimization problem. Extensive experiments on six benchmark datasets are conducted to verify the efficacy of the proposed method. The code is implemented by using MATLAB R2018a and MindSpore library [1] : https://github.com/guanyuezhen/CGL .},
  archive      = {J_TMM},
  author       = {Zhenglai Li and Chang Tang and Xinwang Liu and Xiao Zheng and Wei Zhang and En Zhu},
  doi          = {10.1109/TMM.2021.3081930},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2461-2472},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Consensus graph learning for multi-view clustering},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). MFDNet: Collaborative poses perception and matrix fisher
distribution for head pose estimation. <em>TMM</em>, <em>24</em>,
2449–2460. (<a href="https://doi.org/10.1109/TMM.2021.3081873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head pose estimation suffers from several problems, including low pose tolerance under different disturbances and ambiguity arising from common head pose representation. In this study, a robust three-branch model with triplet module and matrix Fisher distribution module is proposed to address these problems. Based on metric learning, the triplet module employs triplet architecture and triplet loss. It is implemented to maximize the distance between embeddings with different pose pairs and minimize the distance between embeddings with same pose pairs. It can learn a highly discriminate and robust embedding related to head pose. Moreover, the rotation matrix instead of Euler angle and unit quaternion is utilized to represent head pose. An exponential probability density model based on the rotation matrix (referred to as the matrix Fisher distribution) is developed to model head rotation uncertainty. The matrix Fisher distribution can further analyze the head pose, and its maximum likelihood obtained using singular value decomposition provides enhanced accuracy. Extensive experiments executed over AFLW2000 and BIWI datasets demonstrate that the proposed model achieves state-of-the-art performance in comparison with traditional methods.},
  archive      = {J_TMM},
  author       = {Hai Liu and Shuai Fang and Zhaoli Zhang and Duantengchuan Li and Ke Lin and Jiazhang Wang},
  doi          = {10.1109/TMM.2021.3081873},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2449-2460},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MFDNet: Collaborative poses perception and matrix fisher distribution for head pose estimation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-modality fusion and progressive integration network
for saliency prediction on stereoscopic 3D images. <em>TMM</em>,
<em>24</em>, 2435–2448. (<a
href="https://doi.org/10.1109/TMM.2021.3081260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional 2D image-based saliency prediction models suffer from unsatisfactory performance when dealing with stereoscopic 3D (S3D) images because eye movements in the case of freely viewing S3D images are demonstrated to be guided by both RGB and depth features. This paper studies the problem of saliency prediction on S3D images, where the interactions between RGB and depth modalities are both taken into account. Specifically, we design a novel deep neural network named Cross-modality Fusion and Progressive Integration Network (CFPI-Net) to address this problem. It consists of a Multi-level Cross-modality Feature Fusion (MCFF) module and a Multi-stage Progressive Feature Integration (MPFI) module. The MCFF module first captures hierarchical contexture features from each modality and then effectively fuses the hierarchical contexture features from different modalities at each level. The MPFI module involves multiple cascaded deeply supervised feature integration (DSFI) blocks in which the low-level and high-level cross-modality features are progressively integrated using the integrated features in the previous stage as a guidance. Our proposed CFPI-Net benefits from the advantages of multi-level feature representation, cross-modality feature fusion, and multi-stage progressive feature integration, which hereby fully boost the performance. Experimental results on two benchmark datasets demonstrate that CFPI-Net outperforms state-of-the-art saliency prediction methods both quantitatively and qualitatively. All the results and relevant codes will be made available to the public.},
  archive      = {J_TMM},
  author       = {Yudong Mao and Qiuping Jiang and Runmin Cong and Wei Gao and Feng Shao and Sam Kwong},
  doi          = {10.1109/TMM.2021.3081260},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2435-2448},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross-modality fusion and progressive integration network for saliency prediction on stereoscopic 3D images},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Combining retargeting quality and depth perception measures
for quality evaluation of retargeted stereopairs. <em>TMM</em>,
<em>24</em>, 2422–2434. (<a
href="https://doi.org/10.1109/TMM.2021.3081259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereoscopic Image Retargeting (SIR) aims to adapt stereoscopic images and videos to 3D display devices with various aspect ratios by emphasizing the important content while retaining surrounding context with minimal visual distortion. To address the issue of SIR evaluation, this paper presents a new objective quality assessment method for retargeted stereopairs by combining image quality and depth perception measures. Specifically, the image quality measure is conducted between the source and retargeted intermediate views generated by the view synthesis method to characterize the geometric distortion and content loss of the retargeted stereopair, while several depth-aware features are extracted to measure the visual comfort/discomfort and depth sensation when human views a 3D scene. Then, the extracted features are integrated into an overall perceptual quality prediction. Experiment results on NBU SIRQA and SIRD databases verify the superiority of our method.},
  archive      = {J_TMM},
  author       = {Xuejin Wang and Feng Shao and Qiuping Jiang and Zhenqi Fu and Xiangchao Meng and Ke Gu and Yo-Sung Ho},
  doi          = {10.1109/TMM.2021.3081259},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2422-2434},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Combining retargeting quality and depth perception measures for quality evaluation of retargeted stereopairs},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Informative feature disentanglement for unsupervised domain
adaptation. <em>TMM</em>, <em>24</em>, 2407–2421. (<a
href="https://doi.org/10.1109/TMM.2021.3080516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation (UDA) aims at learning a classifier for an unlabeled target domain by transferring knowledge from a labeled source domain with a related but different distribution. The strategy of aligning the two domains in latent feature space via metric discrepancy or adversarial learning has achieved considerable progress. However, these existing approaches mainly focus on adapting the entire image and ignore the bottleneck that occurs when forced adaptation of uninformative domain-specific variations undermines the effectiveness of learned features. To address this problem, we propose a novel component called Informative Feature Disentanglement (IFD), which is equipped with the adversarial network or the metric discrepancy model, respectively. Accordingly, the new network architectures, named IFDAN and IFDMN, enable informative feature refinement before the adaptation. The proposed IFD is designed to disentangle informative features from the uninformative domain-specific variations, which are produced by a Variational Autoencoder (VAE) with lateral connections from the encoder to the decoder. We cooperatively apply the IFD to conduct supervised disentanglement for the source domain and unsupervised disentanglement for the target domain. In this way, informative features are disentangled from the domain-specific details before the adaptation. Extensive experimental results on three gold-standard domain adaptation datasets, e.g., Office31, Office-Home and VisDA-C, demonstrate the effectiveness of the proposed IFDAN and IFDMN models for UDA.},
  archive      = {J_TMM},
  author       = {Wanxia Deng and Lingjun Zhao and Qing Liao and Deke Guo and Gangyao Kuang and Dewen Hu and Matti Pietikäinen and Li Liu},
  doi          = {10.1109/TMM.2021.3080516},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2407-2421},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Informative feature disentanglement for unsupervised domain adaptation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scene recognition mechanism for service robot adapting
various families: A CNN-based approach using multi-type cameras.
<em>TMM</em>, <em>24</em>, 2392–2406. (<a
href="https://doi.org/10.1109/TMM.2021.3080076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key challenges of scene recognition for service robots in various family environments are the view shortage of holistic scenes and poor adaptation. To address these problems, a family scene recognition mechanism for the service robot is proposed in this paper. A comprehensive application of fish-eye, pinhole, and depth cameras is provided to guarantee the sufficient view of robot. A selective CNN features fusion for the recognition of fish-eye scene images is designed to improve the training efficiency and the recognition accuracy. The mechanism is deployed in a designed hybrid cloud including public and private clouds. The proposed family scene recognition model is trained by large-scale datasets in the public cloud and runs in the private cloud. Besides, the recognition skill can be reinforced and increased by matching human guidance and CNN features to help the robot learn new scenes and improve the adaptation in different family environments. Extensive experiments are implemented to evaluate the proposed method using real scene images from six families. The experiment results show the validity and good performance of our method for the service robot scene recognition in various family environments.},
  archive      = {J_TMM},
  author       = {Shaopeng Liu and Guohui Tian and Ying Zhang and Peng Duan},
  doi          = {10.1109/TMM.2021.3080076},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2392-2406},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Scene recognition mechanism for service robot adapting various families: A CNN-based approach using multi-type cameras},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FVV live: A real-time free-viewpoint video system with
consumer electronics hardware. <em>TMM</em>, <em>24</em>, 2378–2391. (<a
href="https://doi.org/10.1109/TMM.2021.3079711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {FVV Live is a novel end-to-end free-viewpoint video system, designed for real-time operation, using consumer-grade cameras and hardware, which enables low deployment costs and easy installation for immersive event-broadcasting or videoconferencing. All the blocks of the system have been designed to maximize perceptual video quality, overcoming the limitations imposed by hardware and network, which impact directly the accuracy of depth data and thus the quality of virtual view synthesis. Therefore, it does not sacrifice perceptual video quality with respect to high-end counterparts. The results presented in this paper correspond to an implementation with nine stereo-based depth cameras. However, the design of the acquisition block of FVV Live allows scalability for an arbitrary number of cameras. In addition, FVV Live presents low motion-to-photon and end-to-end delays, which enables a responsive free-viewpoint navigation and bilateral immersive communications. Moreover, the visual quality of FVV Live has been assessed through subjective assessment with satisfactory results, and additional comparative tests show that it is preferred over state-of-the-art DIBR alternatives.},
  archive      = {J_TMM},
  author       = {Pablo Carballeira and Carlos Carmona and César Díaz and Daniel Berjón and Daniel Corregidor and Julián Cabrera and Francisco Morán and Carmen Doblado and Sergio Arnaldo and María del Mar Martín and Narciso García},
  doi          = {10.1109/TMM.2021.3079711},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2378-2391},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FVV live: A real-time free-viewpoint video system with consumer electronics hardware},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards fast and robust real image denoising with attentive
neural network and PID controller. <em>TMM</em>, <em>24</em>, 2366–2377.
(<a href="https://doi.org/10.1109/TMM.2021.3079697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of deep learning technologies, recent research on real-world noisy image denoising has achieved a considerable improvement in performance. However, a common limitation for existing approaches is the imbalanced trade-off between denoising accuracy and efficiency. To address this problem, we propose a robust and efficient denoiser, called a hierarchical-based PID-attention denoising network (HPDNet), to flexibly deal with the sophisticated noise. The core of our algorithm is the PID-attentive recurrent network (PAR-Net) whose framework mainly consists of the LSTM network and PID controller. PAR-Net inherits the advantages of both the attentive recurrent network and control action, which can encourage more discriminatory feature representations. This learning procedure is implemented within a feedback control system, allowing a faster and more robust means to enhance feature discriminability. Furthermore, by decomposing the noisy image and stacking the PAR-Nets, our PAR-Net can work on a progressively hierarchical framework, and hence obtain multi-scale features and manageable successive refinements. On several widely used datasets, the proposed HPDNet demonstrates high efficiency, while delivering a better perceptually appealing image quality over state-of-the-art image denoising methods.},
  archive      = {J_TMM},
  author       = {Ruijun Ma and Shuyi Li and Bob Zhang and Zhengming Li},
  doi          = {10.1109/TMM.2021.3079697},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2366-2377},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards fast and robust real image denoising with attentive neural network and PID controller},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convolutional neural network-based occupancy map accuracy
improvement for video-based point cloud compression. <em>TMM</em>,
<em>24</em>, 2352–2365. (<a
href="https://doi.org/10.1109/TMM.2021.3079698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In video-based point cloud compression (V-PCC), a dynamic point cloud is projected onto geometry and attribute videos patch by patch for compression. In addition to the geometry and attribute videos, an occupancy map video is compressed into a V-PCC bitstream to indicate whether a two-dimensional (2D) point in the projected geometry video corresponds to any point in three-dimensional (3D) space. The occupancy map video is usually downsampled before compression to obtain a tradeoff between the bitrate and the reconstructed point cloud quality. Due to the accuracy loss in the downsampling process, some noisy points are generated, which leads to severe objective and subjective quality degradation of the reconstructed point cloud. To improve the quality of the reconstructed point cloud, we propose using a convolutional neural network (CNN) to improve the accuracy of the occupancy map video. We mainly make the following contributions. First, we improve the accuracy of the occupancy map video by formulating the problem as a binary segmentation problem since the pixel values of the occupancy map video are either 0 or 1. Second, in addition to the downsampled occupancy map video, we introduce a reconstructed geometry video as the other input of the CNN to provide more useful information in order to indicate the occupancy map video. To the best of our knowledge, this is the first learning-based work to improve the performance of V-PCC. Compared to state-of-the-art schemes, our proposed CNN-based approach achieves much more accurate occupancy map videos and significant bitrate savings.},
  archive      = {J_TMM},
  author       = {Wei Jia and Li Li and Anique Akhtar and Zhu Li and Shan Liu},
  doi          = {10.1109/TMM.2021.3079698},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2352-2365},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Convolutional neural network-based occupancy map accuracy improvement for video-based point cloud compression},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Zero-shot single-microphone sound classification and
localization in a building via the synthesis of unseen features.
<em>TMM</em>, <em>24</em>, 2339–2351. (<a
href="https://doi.org/10.1109/TMM.2021.3079705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a learning-based approach to identify the type and position of sounds using a single microphone in a real-world building. We attempt to treat this problem as a joint classification problem in which we predict the exact positions of sounds while classifying the types that are assumed to be from pre-defined types of sounds. The most problematic issue is that while the types are readily classified under supervised learning frameworks with one-hot encoded labels, it is difficult to predict the exact positions of the sound from unseen positions during training. To address this potential discrepancy, we formulate the position identification problem as a zero-shot learning problem inspired by the human ability to perceive new concepts from previously learned concepts. We extract feature representations from audio data and vectorize the type and position of the sound source as ‘type/position-aware attributes,’ instead of labeling each class with a simple one-hot vector. We then train a promising generative model to bridge the extracted features and the attributes by learning the class-invariant structure to transfer the knowledge from seen to unseen classes through their attributes; generative adversarial networks are conditioned on the class-embeddings. Our proposed methods are evaluated on an indoor noise dataset, SNU-B36-EX, a real-world dataset collected inside a building.},
  archive      = {J_TMM},
  author       = {Seungjun Lee and Haesang Yang and Hwiyong Choi and Woojae Seong},
  doi          = {10.1109/TMM.2021.3079705},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2339-2351},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Zero-shot single-microphone sound classification and localization in a building via the synthesis of unseen features},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Push &amp; pull: Transferable adversarial examples with
attentive attack. <em>TMM</em>, <em>24</em>, 2329–2338. (<a
href="https://doi.org/10.1109/TMM.2021.3079723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Targeted attack aims to mislead the classification model to a specific class, and it can be further divided into black-box and white-box targeted attack depending on whether the classification model is known. A growing number of approaches rely on disrupting the image representations to craft adversarial examples. However, this type of methods often suffer from either low white-box targeted attack success rate or poor black-box targeted attack transferability. To address these problems, we propose a Transferable Attentive Attack (TAA) method which adds perturbation to clean images based on the attended regions and features. This is motivated by one important observation that deep-learning based classification models (or even shallow-learning based models like SIFT) make the prediction mainly based on the informative and discriminative regions of an image. Specifically, the corresponding features of the informative regions are firstly extracted, and the anchor image’s features are iteratively “pushed” away from the source class and simultaneously “pulled” closer to the target class along with attacking. Moreover, we introduce a new strategy that the attack selects the centroids of source and target class cluster as the input of triplet loss to achieve high transferability. Experimental results demonstrate that our method improves the transferability of adversarial example, while maintaining higher success rate for white-box targeted attacks compared with the state-of-the-arts. In particular, TAA attacks on image-representation based task like VQA also result in a significant performance drop in terms of accuracy.},
  archive      = {J_TMM},
  author       = {Lianli Gao and Zijie Huang and Jingkuan Song and Yang Yang and Heng Tao Shen},
  doi          = {10.1109/TMM.2021.3079723},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2329-2338},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Push &amp; pull: Transferable adversarial examples with attentive attack},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An FCNN-based super-resolution mmwave radar framework for
contactless musical instrument interface. <em>TMM</em>, <em>24</em>,
2315–2328. (<a href="https://doi.org/10.1109/TMM.2021.3079695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a framework for contactless human-computer interaction (HCI) using novel tracking techniques based on deep learning-based super-resolution and tracking algorithms. Our system offers unprecedented high-resolution tracking of hand position and motion characteristics by leveraging spatial and temporal features embedded in the reflected radar waveform. Rather than classifying samples from a predefined set of hand gestures, as common in existing work on deep learning with mmWave radar, our proposed imager employs a regressive full convolutional neural network (FCNN) approach to improve localization accuracy by spatial super-resolution. While the proposed techniques are suitable for a host of tracking applications, this article focuses on their application as a musical interface to demonstrate the robustness of the gesture sensing pipeline and deep learning signal processing chain. The user can control the instrument by varying the position and velocity of their hand above the vertically-facing sensor. By employing a commercially available multiple-input-multiple-output (MIMO) radar rather than a traditional optical sensor, our framework demonstrates the efficacy of the mmWave sensing modality for fine motion tracking and offers an elegant solution to a host of HCI tasks. Additionally, we provide a freely available software package and user interface for controlling the device, streaming the data to MATLAB in real-time, and increasing accessibility to the signal processing and device interface functionality utilized in this article.},
  archive      = {J_TMM},
  author       = {Josiah W. Smith and Orges Furxhi and Murat Torlak},
  doi          = {10.1109/TMM.2021.3079695},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2315-2328},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An FCNN-based super-resolution mmwave radar framework for contactless musical instrument interface},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Catching the moment with LoL<span
class="math inline"><sup>+</sup></span> in twitch-like low-latency live
streaming platforms. <em>TMM</em>, <em>24</em>, 2300–2314. (<a
href="https://doi.org/10.1109/TMM.2021.3079288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our earlier Low-on-Latency (dubbed as LoL) solution offered an accurate bandwidth prediction and rate adaptation algorithm tailored for live streaming applications that targeted an end-to-end latency of up to two seconds. While LoL was a significant step forward in multi-bitrate low-latency live streaming, further experimentation and testing showed that there was room for improvement in three areas. First, LoL used hard-coded parameters computed from an offline training process in the rate adaptation algorithm and this was seen as a significant barrier in LoL’s wide deployment. Second, LoL’s objective was to maximize a collective QoE function. Yet, certain use cases have specific objectives besides the singular QoE and this had to be accommodated. Third, the adaptive playback speed control failed to produce satisfying results in some scenarios. Our goal in this paper is to address these areas and make LoL sufficiently robust to deploy. We refer to the enhanced solution as LoL $^+$ , which has been integrated to the official dash.js player in v3.2.0.},
  archive      = {J_TMM},
  author       = {Abdelhak Bentaleb and Mehmet N. Akcay and May Lim and Ali C. Begen and Roger Zimmermann},
  doi          = {10.1109/TMM.2021.3079288},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2300-2314},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Catching the moment with LoL$^+$ in twitch-like low-latency live streaming platforms},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Leveraging multiple relations for fashion trend forecasting
based on social media. <em>TMM</em>, <em>24</em>, 2287–2299. (<a
href="https://doi.org/10.1109/TMM.2021.3078907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fashion trend forecasting is of great research significance in providing useful suggestions for both fashion companies and fashion lovers. Although various studies have been devoted to tackling this challenging task, they only studied limited fashion elements with highly seasonal or simple patterns, which could hardly reveal the real complex fashion trends. Moreover, the mainstream solutions for this task are still statistical-based and solely focus on time-series data modeling, which limit the forecast accuracy. Towards insightful fashion trend forecasting, previous work [1] proposed to analyze more fine-grained fashion elements which can informatively reveal fashion trends. Specifically, it focused on detailed fashion element trend forecasting for specific user groups based on social media data. In addition, it proposed a neural network-based method, namely KERN, to address the problem of fashion trend modeling and forecasting. In this work, to extend the previous work [1] , we propose an improved model named Relation Enhanced Attention Recurrent (REAR) network. Compared to KERN, the REAR model leverages not only the relations among fashion elements, but also those among user groups, thus capturing more types of correlations among various fashion trends. To further improve the performance of long-range trend forecasting, the REAR method devises a sliding temporal attention mechanism, which is able to capture temporal patterns on future horizons better. Extensive experiments and more analysis have been conducted on the FIT [1] and GeoStyle [2] datasets to evaluate the performance of REAR. Experimental and analytical results demonstrate the effectiveness of the proposed REAR model in fashion trend forecasting, which also show the improvement of REAR compared to the KERN.},
  archive      = {J_TMM},
  author       = {Yujuan Ding and Yunshan Ma and Lizi Liao and Wai Keung Wong and Tat-Seng Chua},
  doi          = {10.1109/TMM.2021.3078907},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2287-2299},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Leveraging multiple relations for fashion trend forecasting based on social media},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Holographic feature learning of egocentric-exocentric videos
for multi-domain action recognition. <em>TMM</em>, <em>24</em>,
2273–2286. (<a href="https://doi.org/10.1109/TMM.2021.3078882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though existing cross-domain action recognition methods successfully improve the performance on videos of one view ( e.g. , egocentric videos) by transferring the knowledge from videos of another view ( e.g. , exocentric videos), they have limitations in generality because the source and target domains need to be fixed aforehand. In this paper, we propose to solve a more practical task of multi-domain action recognition on egocentric-exocentric videos, which aims to learn a single model to recognize test videos from either egocentric perspective or exocentric perspective by transferring knowledge between two domains. Though previous cross-domain methods can also transfer knowledge from one domain to another one by learning view-invariant representations of two video domains, they are not suitable for the multi-domain action recognition task because they always suffer from the problem of losing view-specific visual information. As a solution to the multi-domain action recognition task, we propose to map a video from either egocentric perspective or exocentric perspective to a global feature space (we call it holographic feature space) that shares both view-invariant and view-specific visual knowledge of two views. Specially, we decompose the video feature into view-invariant component and view-specific component, where view-specific component is written into memory networks for saving view-specific visual knowledge. The final holographic feature combines view-invariant feature and view-specific features of two views based on the memory networks. We demonstrate the effectiveness of the proposed method with extensive experimental results on two public datasets. Moreover, the good performances under the semi-supervised setting show the generality of our model.},
  archive      = {J_TMM},
  author       = {Yi Huang and Xiaoshan Yang and Junyun Gao and Changsheng Xu},
  doi          = {10.1109/TMM.2021.3078882},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2273-2286},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Holographic feature learning of egocentric-exocentric videos for multi-domain action recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Iterative network for image super-resolution. <em>TMM</em>,
<em>24</em>, 2259–2272. (<a
href="https://doi.org/10.1109/TMM.2021.3078615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image super-resolution (SISR), as a traditional ill-conditioned inverse problem, has been greatly revitalized by the recent development of convolutional neural networks (CNN). These CNN-based methods generally map a low-resolution image to its corresponding high-resolution version with sophisticated network structures and loss functions, showing impressive performances. This paper provides a new insight on conventional SISR algorithm, and proposes a substantially different approach relying on the iterative optimization. A novel iterative super-resolution network (ISRN) is proposed on top of the iterative optimization. We first analyze the observation model of image SR problem, inspiring a feasible solution by mimicking and fusing each iteration in a more general and efficient manner. Considering the drawbacks of batch normalization, we propose a feature normalization (F-Norm, FN) method to regulate the features in network. Furthermore, a novel block with FN is developed to improve the network representation, termed as FNB. Residual-in-residual structure is proposed to form a very deep network, which groups FNBs with a long skip connection for better information delivery and stabling the training phase. Extensive experimental results on testing benchmarks with bicubic (BI) degradation show our ISRN can not only recover more structural information, but also achieve competitive or better PSNR/SSIM results with much fewer parameters compared to other works. Besides BI, we simulate the real-world degradation with blur-downscale (BD) and downscale-noise (DN). ISRN and its extension ISRN+ both achieve better performance than others with BD and DN degradation models.},
  archive      = {J_TMM},
  author       = {Yuqing Liu and Shiqi Wang and Jian Zhang and Shanshe Wang and Siwei Ma and Wen Gao},
  doi          = {10.1109/TMM.2021.3078615},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2259-2272},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Iterative network for image super-resolution},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Self-guided adaptation: Progressive representation
alignment for domain adaptive object detection. <em>TMM</em>,
<em>24</em>, 2246–2258. (<a
href="https://doi.org/10.1109/TMM.2021.3078141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) has achieved unprecedented success in improving the cross-domain robustness of object detection models. However, existing UDA methods largely ignore the instantaneous data distribution and the sampling strategy during model learning, which could deteriorate the feature representation given large domain shift. In this work, we propose a Self-Guided Adaptation (SGA) model, targeting at aligning feature representation and transferring object detection models across domains while considering the instantaneous alignment difficulty. The core of SGA is to calculate “hardness” factors for sample pairs indicating domain distance in a kernel space. With the hardness factor, the proposed SGA adaptively indicates the importance of samples and assigns them different constrains. Indicated by these hardness factors, Self-Guided Progressive Sampling (SPS) is implemented in an “easy-to-hard” way during model adaptation. Using multi-stage convolutional features, SGA is further aggregated to fully align hierarchical representations of detection models. Extensive experiments on commonly-used benchmarks show that SGA improves the state-of-the-art methods with significant margins especially on large domain shift cases.},
  archive      = {J_TMM},
  author       = {Chong Zhang and Zongxian Li and Jingjing Liu and Peixi Peng and Qixiang Ye and Shijian Lu and Tiejun Huang and Yonghong Tian},
  doi          = {10.1109/TMM.2021.3078141},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2246-2258},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Self-guided adaptation: Progressive representation alignment for domain adaptive object detection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast adaptive meta-learning for few-shot image generation.
<em>TMM</em>, <em>24</em>, 2205–2217. (<a
href="https://doi.org/10.1109/TMM.2021.3077729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) are capable of effectively synthesising new realistic images and estimating the potential distribution of samples utilising adversarial learning. Nevertheless, conventional GANs require a large amount of training data samples to produce plausible results. Inspired by the capacity for humans to quickly learn new concepts from a small number of examples, several meta-learning approaches for the few-shot datasets are presented. However, most of meta-learning algorithms are designed to tackle few-shot classification and reinforcement learning tasks. Moreover, the existing meta-learning models for image generation are complex, thereby affecting the length of training time required. Fast Adaptive Meta-Learning (FAML) based on GAN and the encoder network is proposed in this study for few-shot image generation. This model demonstrates the capability to generate new realistic images from previously unseen target classes with only a small number of examples required. With 10 times faster convergence, FAML requires only one-fourth of the trainable parameters in comparison baseline models by training a simpler network with conditional feature vectors from the encoder, while increasing the number of generator iterations. The visualisation results are demonstrated in the paper. This model is able to improve few-shot image generation with the lowest FID score, highest IS, and comparable LPIPS to MNIST, Omniglot, VGG-Faces, and mini ImageNet datasets. The source code is available on https://github.com/phaphuang/FAML .},
  archive      = {J_TMM},
  author       = {Aniwat Phaphuangwittayakul and Yi Guo and Fangli Ying},
  doi          = {10.1109/TMM.2021.3077729},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2205-2217},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fast adaptive meta-learning for few-shot image generation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CCAFNet: Crossflow and cross-scale adaptive fusion network
for detecting salient objects in RGB-d images. <em>TMM</em>,
<em>24</em>, 2192–2204. (<a
href="https://doi.org/10.1109/TMM.2021.3077767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the widespread adoption of depth sensors, salient object detection (SOD) supported by depth maps for reliable complementary information is being increasingly investigated. Existing SOD models mainly exploit the relation between an RGB image and its corresponding depth information across three fusion domains: input RGB-D images, extracted feature maps, and output salient object. However, these models do not leverage the crossflows between high- and low-level information well. Moreover, the decoder in these models uses conventional convolution that involves several calculations. To further improve RGB-D SOD, we propose a crossflow and cross-scale adaptive fusion network (CCAFNet) to detect salient objects in RGB-D images. First, a channel fusion module allows for effective fusing depth and high-level RGB features. This module extracts accurate semantic information features from high-level RGB features. Meanwhile, a spatial fusion module combines low-level RGB and depth features with accurate boundaries and subsequently extracts detailed spatial information from low-level depth features. Finally, a purification loss is proposed to precisely learn the boundaries of salient objects and obtain additional details of the objects. The results of comprehensive experiments on seven common RGB-D SOD datasets indicate that the performance of the proposed CCAFNet is comparable to those of state-of-the-art RGB-D SOD models.},
  archive      = {J_TMM},
  author       = {Wujie Zhou and Yun Zhu and Jingsheng Lei and Jian Wan and Lu Yu},
  doi          = {10.1109/TMM.2021.3077767},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2192-2204},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CCAFNet: Crossflow and cross-scale adaptive fusion network for detecting salient objects in RGB-D images},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HoloCast+: Hybrid digital-analog transmission for graceful
point cloud delivery with graph fourier transform. <em>TMM</em>,
<em>24</em>, 2179–2191. (<a
href="https://doi.org/10.1109/TMM.2021.3077772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud is an emerging data format useful for various applications such has holographic display, autonomous vehicle, and augmented reality. Conventionally, communications of point cloud data have relied on digital compression and digital modulation for three-dimensional (3D) data streaming. However, such digital-based delivery schemes have fundamental issues called cliff and leveling effects, where the 3D reconstruction quality is a step function in terms of wireless channel quality. We propose a novel scheme of point cloud delivery, called HoloCast $+$ , to overcome cliff and leveling effects. Specifically, our method utilizes hybrid digital-analog coding, integrating digital compression and analog coding based on graph Fourier transform (GFT), to gracefully improve 3D reconstruction quality with the improvement of channel quality. We demonstrate that HoloCast $+$ offers better 3D reconstruction quality in terms of the symmetric mean square error (sMSE) by up to 18.3 dB and 10.5 dB, respectively, compared to conventional digital-based and analog-based delivery methods in wireless fading environments.},
  archive      = {J_TMM},
  author       = {Takuya Fujihashi and Toshiaki Koike-Akino and Takashi Watanabe and Philip V. Orlik},
  doi          = {10.1109/TMM.2021.3077772},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2179-2191},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HoloCast+: Hybrid digital-analog transmission for graceful point cloud delivery with graph fourier transform},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiframe-to-multiframe network for video denoising.
<em>TMM</em>, <em>24</em>, 2164–2178. (<a
href="https://doi.org/10.1109/TMM.2021.3077140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing studies performed video denoising by using multiple adjacent noisy frames to recover one clean frame; however, despite achieving relatively good quality for each individual frame, these approaches may result in visual flickering when the denoised frames are considered in sequence. In this paper, instead of separately restoring each clean frame, we propose a multiframe-to-multiframe (MM) denoising scheme that simultaneously recovers multiple clean frames from consecutive noisy frames. The proposed MM denoising scheme uses a training strategy that optimizes the denoised video from both the spatial and temporal dimensions, enabling better temporal consistency in the denoised video. Furthermore, we present an MM network (MMNet), which adopts a spatiotemporal convolutional architecture that considers both the interframe similarity and single-frame characteristics. Benefiting from the underlying parallel mechanism of the MM denoising scheme, MMNet achieves a highly competitive denoising efficiency. Extensive analyses and experiments demonstrate that MMNet outperforms the state-of-the-art video denoising methods, yielding temporal consistency improvements of at least 13.3 $\%$ and running more than 2 times faster than the other methods.},
  archive      = {J_TMM},
  author       = {Huaian Chen and Yi Jin and Kai Xu and Yuxuan Chen and Changan Zhu},
  doi          = {10.1109/TMM.2021.3077140},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {2164-2178},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multiframe-to-multiframe network for video denoising},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guest editorial: Learning from noisy multimedia data.
<em>TMM</em>, <em>24</em>, 1247–1252. (<a
href="https://doi.org/10.1109/TMM.2022.3159014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special issue provides a premier forum for researchers in multimedia big data to share challenges and recent advancements in learning from noisy multimedia data. The multimedia age and its proliferation of devices and platforms is fueling exponential data growth. As computational power and deep learning algorithms rapidly evolve, the web has become a rich source of potential training data for robust machine learning, with search engines such as Google and Bing, Twitter, TikTok, Instagram, and short video sharing platforms offering large-scale data points in the hundreds of millions. The concurrent shift in the Internet to richer web data modalities such as text, audio, image, and video reveal further opportunities to leverage large-scale data for the automatic construction of a variety of datasets for model training and testing. However, the ubiquity of multimedia data means noise is a fundamental challenge, with ‘label noise’ and ‘domain mismatch’ the most critical issues in automatically collected datasets. Learning from noisy multimedia data tends towards poor performance, making it increasingly essential to address these challenges.},
  archive      = {J_TMM},
  author       = {Jian Zhang and Alan Hanjalic and Ramesh Jain and Xiansheng Hua and Shin&#39;ichi Satoh and Yazhou Yao and Dan Zeng},
  doi          = {10.1109/TMM.2022.3159014},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1247-1252},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Guest editorial: Learning from noisy multimedia data},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A reinforcement-learning-based energy-efficient framework
for multi-task video analytics pipeline. <em>TMM</em>, <em>24</em>,
2150–2163. (<a href="https://doi.org/10.1109/TMM.2021.3076612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep-learning-based video processing has yielded transformative results in recent years. However, the video analytics pipeline is energy-intensive due to high data rates and reliance on complex inference algorithms, which limits its adoption in energy-constrained applications. Motivated by the observation of high and variable spatial redundancy and temporal dynamics in video data streams, we design and evaluate an adaptive-resolution optimization framework to minimize the energy use of multi-task video analytics pipelines. Instead of heuristically tuning the input data resolution of individual tasks, our framework utilizes deep reinforcement learning to dynamically govern the input resolution and computation of the entire video analytics pipeline. By monitoring the impact of varying resolution on the quality of high-dimensional video analytics features, hence the accuracy of video analytics results, the proposed end-to-end optimization framework learns the best non-myopic policy for dynamically controlling the resolution of input video streams to globally optimize energy efficiency. Governed by reinforcement learning, optical flow is incorporated into the framework to minimize unnecessary spatio-temporal redundancy that leads to re-computation, while preserving accuracy. The proposed framework is applied to video instance segmentation which is one of the most challenging computer vision tasks, and achieves better energy efficiency than all baseline methods of similar accuracy on the YouTube-VIS dataset.},
  archive      = {J_TMM},
  author       = {Yingying Zhao and Mingzhi Dong and Yujiang Wang and Da Feng and Qin Lv and Robert P. Dick and Dongsheng Li and Tun Lu and Ning Gu and Li Shang},
  doi          = {10.1109/TMM.2021.3076612},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {2150-2163},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A reinforcement-learning-based energy-efficient framework for multi-task video analytics pipeline},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3DBodyNet: Fast reconstruction of 3D animatable human body
shape from a single commodity depth camera. <em>TMM</em>, <em>24</em>,
2139–2149. (<a href="https://doi.org/10.1109/TMM.2021.3076340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge about individual body shape has numerous applications in various domains such as healthcare, fashion and personalized entertainment. Most of the depth based whole body scanners need multiple cameras surrounding the user and requiring the user to keep a canonical pose strictly during capturing depth images. These scanning devices are expensive and need professional knowledge for operation. In order to make 3D scanning as easy-to-use and fast as possible, there is a great demand to simplify the process and to reduce the hardware requirements. In this paper, we propose a deep learning algorithm, dubbed 3DBodyNet, to rapidly reconstruct the 3D shape of human bodies using a single commodity depth camera. As easy-to-use as taking a photo using a mobile phone, our algorithm only needs two depth images of the front-facing and back-facing bodies. The proposed algorithm has strong operability since it is insensitive to the pose and the pose variations between the two depth images. It can also reconstruct an accurate body shape for users under tight/loose clothing. Another advantage of our method is the ability to generate an animatable human body model. Extensive experimental results show that the proposed method enables robust and easy-to-use animatable human body reconstruction, and outperforms the state-of-the-art methods with respect to running time and accuracy.},
  archive      = {J_TMM},
  author       = {Pengpeng Hu and Edmond Shu–Lim Ho and Adrian Munteanu},
  doi          = {10.1109/TMM.2021.3076340},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {2139-2149},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {3DBodyNet: Fast reconstruction of 3D animatable human body shape from a single commodity depth camera},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consolidated dataset and metrics for high-dynamic-range
image quality. <em>TMM</em>, <em>24</em>, 2125–2138. (<a
href="https://doi.org/10.1109/TMM.2021.3076298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing popularity of high-dynamic-range (HDR) image and video content brings the need for metrics that could predict the severity of image impairments as seen on displays of different brightness levels and dynamic range. Such metrics should be trained and validated on a sufficiently large subjective image quality dataset to ensure robust performance. As the existing HDR quality datasets are limited in size, we created a Unified Photometric Image Quality dataset (UPIQ) with over 4000 images by realigning and merging existing HDR and standard-dynamic-range (SDR) datasets. The realigned quality scores share the same unified quality scale across all datasets. Such realignment was achieved by collecting additional cross-dataset quality comparisons and re-scaling data with a psychometric scaling method. Images in the proposed dataset are represented in absolute photometric and colorimetric units, corresponding to light emitted from a display. We use the new dataset to retrain existing HDR metrics and show that the dataset is sufficiently large for training deep architectures. We show the utility of the dataset on brightness aware image compression.},
  archive      = {J_TMM},
  author       = {Aliaksei Mikhailiuk and María Pérez-Ortiz and Dingcheng Yue and Wilson Suen and Rafał K. Mantiuk},
  doi          = {10.1109/TMM.2021.3076298},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {2125-2138},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Consolidated dataset and metrics for high-dynamic-range image quality},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modality disentangled discriminator for text-to-image
synthesis. <em>TMM</em>, <em>24</em>, 2112–2124. (<a
href="https://doi.org/10.1109/TMM.2021.3075997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-image (T2I) synthesis aims at generating photo-realistic images from text descriptions, which is a particularly important task in bridging vision and language. Each generated image consists of two parts: the content part related to the text and the style part irrelevant to the text. The existing discriminator does not distinguish between the content part and the style part. This not only precludes the T2I synthesis models from generating the content part effectively but also makes it difficult to manipulate the style of the generated image. In this paper, we propose a modality disentangled discriminator that distinguishes between the content part and the style part at a specific layer. Specifically, we enforce the early layers of a certain number in the discriminator to become the disentangled representation extractor through two losses. The extracted common representation for the content part can make the discriminator more effective for capturing the text-image correlation, while the extracted modality-specific representation for the style part can be directly transferred to other images. The combination of these two representations can also improve the quality of the generated images. Our proposed discriminator is used to substitute the discriminator of each stage in the representative model AttnGAN and the SOTA model DM-GAN. Extensive experiments are conducted on three widely used datasets, i.e. CUB, Oxford-102, and COCO, for the T2I synthesis task, demonstrating the superior performance of the modality disentangled discriminator over the base models. Code for DM-GAN with our modality disentangled discriminator is available at https://github.com/FangxiangFeng/DM-GAN-MDD .},
  archive      = {J_TMM},
  author       = {Fangxiang Feng and Tianrui Niu and Ruifan Li and Xiaojie Wang},
  doi          = {10.1109/TMM.2021.3075997},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {2112-2124},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Modality disentangled discriminator for text-to-image synthesis},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bilateral weighted regression ranking model with
spatial-temporal correlation filter for visual tracking. <em>TMM</em>,
<em>24</em>, 2098–2111. (<a
href="https://doi.org/10.1109/TMM.2021.3075876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many discriminative correlation filter (DCF)-based methods have successfully leveraged the guidance for solving two problems (i.e., the boundary effect and temporal filtering degradation) as a model prior to visual tracking. The intuitive motivation of these methods is to control the degeneration of the updating loss of the objective function with a structural framework. While these methods rely mostly on various regularization items, they always ignore the loss from data fidelity term. Therefore, we propose a bilateral weighted regression ranking model termed as BWRR. Here, we resort to two procedures for solving the above problems. First, BWRR introduces a bilateral constraint into the data fidelity term to control the loss of rows and columns of the filter learning data term. The weighted matrices could impose an adaptive penalty for large data loss during the learning process to avoid the model degradation problem. Second, the data of the updated weighted matrices is not directly applied to the calculation of the filter during each iteration. Instead, a new weighted product matrix is obtained by ranking and numerical transformation for updating the filter. We show that the proposed model converts the original correlation filter regression problem into a regression-with-ranking problem, thus avoiding the problem of positive and negative sample imbalance. Overall, the BWRR model is iteratively solved by the alternating direction method of multipliers(ADMM). Qualitative and quantitative evaluations demonstrate the effectiveness and superiority of our proposed method by extensive and quantitative experiments on the OTB, VOT, and UAV datasets.},
  archive      = {J_TMM},
  author       = {Hu Zhu and Hao Peng and Guoxia Xu and Lizhen Deng and Yueying Cheng and Aiguo Song},
  doi          = {10.1109/TMM.2021.3075876},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {2098-2111},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Bilateral weighted regression ranking model with spatial-temporal correlation filter for visual tracking},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). A high-performance CNN-applied HEVC steganography based on
diamond-coded PU partition modes. <em>TMM</em>, <em>24</em>, 2084–2097.
(<a href="https://doi.org/10.1109/TMM.2021.3075858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High efficiency video coding (HEVC) is the latest high-performance video coding standard, and HEVC video steganography has become a new way to hide data for covert communication. This paper proposes a novel multilevel steganography algorithm based on diamond-encoded prediction unit (PU) partition modes. The PU modes of smaller $8\times 8$ and $16\times 16$ CUs are selected as carriers for information hiding. The diamond-coding rules are adopted to enhance the expressive ability of limited PU types, allowing them to carry more information under limited modification. Based on the encoded PU partition modes, three different embedding levels with different capacities are proposed. This paper’s most outstanding contribution is the introduction of convolutional neural networks (CNNs) for the first time to improve visual quality and reduce steganographic video bitrate increases. Experimental results show that the embedding capacity of the proposed algorithm is significantly higher than the state-of-the-art work at the same bitrate, whether in high- or low-resolution HEVC videos. At the same time, the visual quality of the steganographic videos is excellent, and the resistance to video steganalysis is strong.},
  archive      = {J_TMM},
  author       = {Jindou Liu and Zhaohong Li and Xinghao Jiang and Zhenzhen Zhang},
  doi          = {10.1109/TMM.2021.3075858},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {2084-2097},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A high-performance CNN-applied HEVC steganography based on diamond-coded PU partition modes},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time and accurate UAV pedestrian detection for social
distancing monitoring in COVID-19 pandemic. <em>TMM</em>, <em>24</em>,
2069–2083. (<a href="https://doi.org/10.1109/TMM.2021.3075566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus Disease 2019 (COVID-19) is a highly infectious virus that has created a health crisis for people all over the world. Social distancing has proved to be an effective non-pharmaceutical measure to slow down the spread of COVID-19. As unmanned aerial vehicle (UAV) is a flexible mobile platform, it is a promising option to use UAV for social distance monitoring. Therefore, we propose a lightweight pedestrian detection network to accurately detect pedestrians by human head detection in real-time and then calculate the social distancing between pedestrians on UAV images. In particular, our network follows the PeleeNet as backbone and further incorporates the multi-scale features and spatial attention to enhance the features of small objects, like human heads. The experimental results on Merge-Head dataset show that our method achieves 92.22% AP (average precision) and 76 FPS (frames per second), outperforming YOLOv3 models and SSD models and enabling real-time detection in actual applications. The ablation experiments also indicate that multi-scale feature and spatial attention significantly contribute the performance of pedestrian detection. The test results on UAV-Head dataset show that our method can also achieve high precision pedestrian detection on UAV images with 88.5% AP and 75 FPS. In addition, we have conducted a precision calibration test to obtain the transformation matrix from images (vertical images and tilted images) to real-world coordinate. Based on the accurate pedestrian detection and the transformation matrix, the social distancing monitoring between individuals is reliably achieved.},
  archive      = {J_TMM},
  author       = {Zhenfeng Shao and Gui Cheng and Jiayi Ma and Zhongyuan Wang and Jiaming Wang and Deren Li},
  doi          = {10.1109/TMM.2021.3075566},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {2069-2083},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Real-time and accurate UAV pedestrian detection for social distancing monitoring in COVID-19 pandemic},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deformable template network (DTN) for object detection.
<em>TMM</em>, <em>24</em>, 2058–2068. (<a
href="https://doi.org/10.1109/TMM.2021.3075323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objects often have different appearances because of viewpoint changes or part deformation. How to reasonably model these variations is still a big challenge for object detection. In this paper, we propose a novel Deformable Template Network (DTN), which exploits the pictorial structure to model possible variations of an object. DTN represents an object by virtue of a generated template in a deformable way. It has two key modules: the template generating module and the part matching module. The template generating module produces a template for a given object which defines the anchor positions of the $k{\times }k$ parts. Based on such a template, the part matching module aims to perform part alignment around the anchor positions. In terms of each part, the matching process makes a trade-off between maximizing the detection score and minimizing the deformation cost relative to the anchor position. Moreover, DTN is a fully convolutional network which means it is competitive in terms of detection efficiency. We evaluate DTN on both the PASCAL VOC and MSCOCO datasets, achieving the state-of-the-art results, an accuracy of 82.7% for PASCAL VOC and of 44.9% for MSCOCO.},
  archive      = {J_TMM},
  author       = {Shuai Wu and Yong Xu and Bob Zhang and Jian Yang and David Zhang},
  doi          = {10.1109/TMM.2021.3075323},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {2058-2068},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deformable template network (DTN) for object detection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A social condition-enhanced network for recognizing power
distance using expressive prosody and intrinsic brain connectivity.
<em>TMM</em>, <em>24</em>, 2046–2057. (<a
href="https://doi.org/10.1109/TMM.2021.3075091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Culture is the social norm that often dictates a person’s thoughts, decision-making, and social behaviors during interaction at an individual level. In this study, we present a computational framework that automatically assesses an individual culture attribute of power distance (PDI), i.e., the measure to describe one’s acceptance of social status, power and authority in organizations through multimodal modeling of a participant’s expressive prosodic structures and brain connectivity using a social condition-enhanced network. In specific, we propose a joint learning approach of center-loss embedding network architecture that learns to “centerize” the embedding space given a particular social interaction condition to enhance the PDI discriminability of the representation. Our proposed method achieves 88.5% and 73.1% in binary classification task of recognizing low versus high power distance on prosodic and fMRI modality separately. After performing multimodal fusion, it improves to 96.2% of 2-class recognition rate (7.7% relative improvement). Further analyses reveal that average and standard deviation of speech energy are significantly correlated with power distance index; the right middle cingulate cortex (MCC) of brain region achieves the best recognition accuracy demonstrating its role in processing a person’s belief about power distance.},
  archive      = {J_TMM},
  author       = {Fu-Sheng Tsai and Wei-Wen Chang and Chi-Chun Lee},
  doi          = {10.1109/TMM.2021.3075091},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {2046-2057},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A social condition-enhanced network for recognizing power distance using expressive prosody and intrinsic brain connectivity},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixed dish recognition with contextual relation and domain
alignment. <em>TMM</em>, <em>24</em>, 2034–2045. (<a
href="https://doi.org/10.1109/TMM.2021.3075037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed dish is a food category that contains different dishes mixed in one plate, and is popular in Eastern and Southeast Asia. Recognizing the individual dishes in a mixed dish image is important for health related applications, e.g. to calculate the nutrition values of the dish. However, most existing methods that focus on single dish classification are not applicable to the recognition of mixed dish images. The main challenge of mixed dish recognition comes from three aspects: a wide range of dish types, the complex dish combination with severe overlap between different dishes and the large visual variances of same dish type caused by different cooking/cutting methods applied in different canteens. In order to tackle these problems, we propose the contextual relation network that encodes the implicit and explicit contextual relations among multiple dishes from region-level features and label-level co-occurrence respectively. Besides, to address the visual variances of dish instances from different canteens, we introduce the domain adaption networks to align both local and global features, and eliminating domain gaps of dish features across different canteens. In addition, we collect a mixed dish image dataset containing 9254 mixed dish images from 6 canteens in Singapore. Extensive experiments on both our dataset and public one validate that our methods can achieve top performance for localizing and recognizing multiple dishes and solve the domain shift problem to a certain extent in mixed dish images.},
  archive      = {J_TMM},
  author       = {Lixi Deng and Jingjing Chen and Chong-Wah Ngo and Qianru Sun and Sheng Tang and Yongdong Zhang and Tat-Seng Chua},
  doi          = {10.1109/TMM.2021.3075037},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {2034-2045},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Mixed dish recognition with contextual relation and domain alignment},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-task center-of-pressure metrics estimation with graph
convolutional network. <em>TMM</em>, <em>24</em>, 2018–2033. (<a
href="https://doi.org/10.1109/TMM.2021.3075025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Center of pressure (CoP) metrics, including its path length, sway area, and position, are important measurements of postural and balance control in biomechanical studies. A computer-vision-based CoP metrics estimation system offers a portable solution to obtain these gold-standard metrics with 3D multi-joint coordination underlying body movements for real-time evaluation of balance control. In this paper, we propose an end-to-end framework for video-level estimation of CoP path length and sway area, as well as the frame-level estimation of CoP position, utilizing the spatial-temporal features and adaptive graph structure learned by graph convolution network. This work is the first step toward demonstrating that these gold-standard metrics can be obtained with a more comprehensive tool than current force plate technologies. We propose two single-task models for video-level and frame-level estimation, respectively, and a multi-task learning approach that jointly learns the two-temporal-level features. To facilitate this line of research, we release a novel computer-vision-based 3D body landmark dataset containing a wide variety of action patterns with synchronized CoP labels using pose estimation. We also adapt our framework on an existing kinematic dataset collected by wearable markers. The experiments on both datasets validate that our framework achieves state-of-the-art accuracies for all metric estimations, while the proposed multi-task approach yields the most accurate and robust performance on video-level estimation. 1},
  archive      = {J_TMM},
  author       = {Chen Du and Sarah Graham and Colin Depp and Truong Nguyen},
  doi          = {10.1109/TMM.2021.3075025},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {2018-2033},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-task center-of-pressure metrics estimation with graph convolutional network},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image difference captioning with instance-level fine-grained
feature representation. <em>TMM</em>, <em>24</em>, 2004–2017. (<a
href="https://doi.org/10.1109/TMM.2021.3074803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of image difference captioning aims at locating changed objects in similar image pairs and describing the difference with natural language. The key challenges of this task are to comprehend the context of image pairs sufficiently and locate the changed objects accurately in the presence of viewpoint change. Previous studies focus on pixel-level image features, neglecting rich explicit features of objects in an image pair which are beneficial to generate a fine-grained difference caption. Additionally, existing generative models suffer from accurately locate the differences in the interference of viewpoint change. To address these issues, we propose an Instance-Level Fine-Grained Difference Captioning (IFDC) model, which consists of a fine-grained feature extraction module, a multi-round feature fusion module, a similarity-based difference finding module, and a difference captioning module. To describe the changed objects comprehensively, we extract the fine-grained features, i.e., visual features, semantic features, and positional features at instance-level, as the objects’ representation. To enhance the model’s immunity to viewpoint change, we design a similarity-based difference finding module to locate the changed objects accurately. Extensive experiments show that our IFDC model achieves comparable performance with the state-of-the-art models on the datasets of CLEVR-Change and Spot-the-Diff, thus verifying the effectiveness of our proposed model. Our source code is available at https://github.com/VISLANG-Lab/IFDC .},
  archive      = {J_TMM},
  author       = {Qingbao Huang and Yu Liang and Jielong Wei and Yi Cai and Hanyu Liang and Ho-fung Leung and Qing Li},
  doi          = {10.1109/TMM.2021.3074803},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {2004-2017},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Image difference captioning with instance-level fine-grained feature representation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gated SwitchGAN for multi-domain facial image translation.
<em>TMM</em>, <em>24</em>, 1990–2003. (<a
href="https://doi.org/10.1109/TMM.2021.3074807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies on multi-domain facial image translation have achieved impressive results. The existing methods generally provide a discriminator with an auxiliary classifier to impose domain translation. However, these methods neglect important information regarding domain distribution matching. To solve this problem, we propose a switch generative adversarial network (SwitchGAN) with a more adaptive discriminator structure and a matched generator to perform delicate image translation among multiple domains. A feature-switching operation is proposed to achieve feature selection and fusion in our conditional modules. We demonstrate the effectiveness of our model. Furthermore, we also introduce a new capability of our generator that represents attribute intensity control and extracts content information without tailored training. Experiments on the Morph, RaFD and CelebA databases visually and quantitatively show that our extended SwitchGAN (i.e., Gated SwitchGAN) can achieve better translation results than StarGAN, AttGAN and STGAN. The attribute classification accuracy achieved using the trained ResNet-18 model and the FID score obtained using the ImageNet pretrained Inception-v3 model also quantitatively demonstrate the superior performance of our models.},
  archive      = {J_TMM},
  author       = {Xiaokang Zhang and Yuanlue Zhu and Wenting Chen and Wenshuang Liu and Linlin Shen},
  doi          = {10.1109/TMM.2021.3074807},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1990-2003},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Gated SwitchGAN for multi-domain facial image translation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Underwater image quality assessment: Subjective and
objective methods. <em>TMM</em>, <em>24</em>, 1980–1989. (<a
href="https://doi.org/10.1109/TMM.2021.3074825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement plays a critical role in marine industry. Various algorithms are applied to enhance underwater images, but their performance in terms of perceptual quality has been little studied. In this paper, we investigate five popular enhancement algorithms and their output image quality. To this end, we have created a benchmark, including images enhanced by different algorithms and ground truth image quality obtained by human perception experiments. We statistically analyse the impact of various enhancement algorithms on the perceived quality of underwater images. Also, the visual quality provided by these algorithms is evaluated objectively, aiming to inform the development of objective metrics for automatic assessment of the quality for underwater image enhancement. The image quality benchmark and its objective metric are made publicly available.},
  archive      = {J_TMM},
  author       = {Pengfei Guo and Lang He and Shuangyin Liu and Delu Zeng and Hantao Liu},
  doi          = {10.1109/TMM.2021.3074825},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1980-1989},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Underwater image quality assessment: Subjective and objective methods},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extended feature pyramid network for small object detection.
<em>TMM</em>, <em>24</em>, 1968–1979. (<a
href="https://doi.org/10.1109/TMM.2021.3074273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object detection remains an unsolved challenge because it is hard to extract the information of small objects with only a few pixels. While scale-level corresponding detection in feature pyramid network alleviates this problem, we find feature coupling of various scales still impairs the performance of small objects. In this paper, we propose an extended feature pyramid network (EFPN) with an extra high-resolution pyramid level specialized for small object detection. Specifically, we design a novel module, named feature texture transfer (FTT), which is used to super-resolve features and extract credible regional details simultaneously. Moreover, we introduce a cross resolution distillation mechanism to transfer the ability of perceiving details across the scales of the network, where a foreground-background-balanced loss function is designed to alleviate area imbalance of foreground and background. In our experiments, the proposed EFPN is efficient on both computation and memory, and yields state-of-the-art results on small traffic-sign dataset Tsinghua-Tencent 100 K and small category of general object detection dataset MS COCO.},
  archive      = {J_TMM},
  author       = {Chunfang Deng and Mengmeng Wang and Liang Liu and Yong Liu and Yunliang Jiang},
  doi          = {10.1109/TMM.2021.3074273},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1968-1979},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Extended feature pyramid network for small object detection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SiamCorners: Siamese corner networks for visual tracking.
<em>TMM</em>, <em>24</em>, 1956–1967. (<a
href="https://doi.org/10.1109/TMM.2021.3074239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current Siamese network based on region proposal network (RPN) has attracted great attention in visual tracking due to its excellent accuracy and high efficiency. However, the design of the RPN involves the selection of the number, scale, and aspect ratios of anchor boxes, which will affect the applicability and convenience of the model. Furthermore, these anchor boxes require complicated calculations, such as calculating their intersection-over-union (IoU) with ground truth bounding boxes. Due to the problems related to anchor boxes, we propose a simple yet effective anchor-free tracker (named Siamese corner networks, SiamCorners), which is end-to-end trained offline on large-scale image pairs. Specifically, we introduce a modified corner pooling layer to convert the bounding box estimate of the target into a pair of corner predictions (the bottom-right and the top-left corners). By tracking a target as a pair of corners, we avoid the need to design the anchor boxes. This will make the entire tracking algorithm more flexible and simple than anchor-based trackers. In our network design, we further introduce a layer-wise feature aggregation strategy that enables the corner pooling module to predict multiple corners for a tracking target in deep networks. We then introduce a new penalty term that is used to select an optimal tracking box in these candidate corners. Finally, SiamCorners achieves experimental results that are comparable to the state-of-art tracker while maintaining a high running speed. In particular, SiamCorners achieves a 53.7% AUC on NFS30 and a 61.4% AUC on UAV123, while still running at 42 frames per second (FPS).},
  archive      = {J_TMM},
  author       = {Kai Yang and Zhenyu He and Wenjie Pei and Zikun Zhou and Xin Li and Di Yuan and Haijun Zhang},
  doi          = {10.1109/TMM.2021.3074239},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1956-1967},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SiamCorners: Siamese corner networks for visual tracking},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geometric back-projection network for point cloud
classification. <em>TMM</em>, <em>24</em>, 1943–1955. (<a
href="https://doi.org/10.1109/TMM.2021.3074240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the basic task of point cloud analysis, classification is fundamental but always challenging. To address some unsolved problems of existing methods, we propose a network that captures geometric features of point clouds for better representations. To achieve this, on the one hand, we enrich the geometric information of points in low-level 3D space explicitly. On the other hand, we apply CNN-based structures in high-level feature spaces to learn local geometric context implicitly. Specifically, we leverage an idea of error-correcting feedback structure to capture the local features of point clouds comprehensively. Furthermore, an attention module based on channel affinity assists the feature map to avoid possible redundancy by emphasizing its distinct channels. The performance on both synthetic and real-world point clouds datasets demonstrate the superiority and applicability of our network. Comparing with other state-of-the-art methods, our approach balances accuracy and efficiency.},
  archive      = {J_TMM},
  author       = {Shi Qiu and Saeed Anwar and Nick Barnes},
  doi          = {10.1109/TMM.2021.3074240},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1943-1955},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Geometric back-projection network for point cloud classification},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attribute-induced bias eliminating for transductive
zero-shot learning. <em>TMM</em>, <em>24</em>, 1933–1942. (<a
href="https://doi.org/10.1109/TMM.2021.3074252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transductive zero-shot learning is designed to recognize unseen categories by aligning both visual and semantic information in a joint embedding space. Four types of domain biases exist in Transductive ZSL, i.e., visual bias and semantic bias in two domains, and two visual-semantic biases exist in the seen and unseen domains. However, the existing work has only focused on specific components of these topics, leading to severe semantic ambiguity during knowledge transfer. To solve this problem, we propose a novel attribute-induced bias eliminating (AIBE) module for Transductive ZSL. Specifically, for the visual bias between the two domains, the mean-teacher module is first used to bridge the visual representation discrepancy between the two domains using unsupervised learning and unlabeled images. Then, an attentional graph attribute embedding process is proposed to reduce the semantic bias between seen and unseen categories using a graph operation to describe the semantic relationship between categories. To reduce semantic-visual bias in the seen domain, we align the visual center of each category with the corresponding semantic attributes instead of with the individual visual data point, which preserves the semantic relationship in the embedding space. Finally, for the semantic-visual bias in the unseen domain, an unseen semantic alignment constraint is designed to align visual and semantic space using an unsupervised process. The evaluations on several benchmarks demonstrate the effectiveness of the proposed method, e.g., 82.8%/75.5%, 97.1%/82.5%, and 73.2%/52.1% for Conventional/Generalized ZSL settings for CUB, AwA2, and SUN datasets, respectively.},
  archive      = {J_TMM},
  author       = {Hantao Yao and Shaobo Min and Yongdong Zhang and Changsheng Xu},
  doi          = {10.1109/TMM.2021.3074252},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1933-1942},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Attribute-induced bias eliminating for transductive zero-shot learning},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Structured attention network for referring image
segmentation. <em>TMM</em>, <em>24</em>, 1922–1932. (<a
href="https://doi.org/10.1109/TMM.2021.3074008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring image segmentation aims at segmenting out the object or stuff referred to by a natural language expression. The challenge of this task lies in the requirement of understanding both vision and language. The linguistic structure of a referring expression can provide an intuitive and explainable layout for reasoning over visual and linguistic concepts. In this paper, we propose a structured attention network (SANet) to explore the multimodal reasoning over the dependency tree parsed from the referring expression. Specifically, SANet implements the multimodal reasoning using an attentional multimodal tree-structure recurrent module (AMTreeGRU) in a bottom-up manner. In addition, for spatial detail improvement, SANet further incorporates the semantics-guided low-level features into high-level ones using the proposed attentional skip connection module. Extensive experiments on four public benchmark datasets demonstrate the superiority of our proposed SANet with more explainable visualization examples.},
  archive      = {J_TMM},
  author       = {Liang Lin and Pengxiang Yan and Xiaoqian Xu and Sibei Yang and Kun Zeng and Guanbin Li},
  doi          = {10.1109/TMM.2021.3074008},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1922-1932},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Structured attention network for referring image segmentation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximate k-NN graph construction: A generic online
approach. <em>TMM</em>, <em>24</em>, 1909–1921. (<a
href="https://doi.org/10.1109/TMM.2021.3073811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nearest neighbor search and k -nearest neighbor graph construction are two fundamental issues that arise from many disciplines such as multimedia information retrieval, data-mining, and machine learning. They become more and more imminent given the big data emerge in various fields in recent years. In this paper, a simple but effective solution both for approximate k -nearest neighbor search and approximate k -nearest neighbor graph construction is presented. These two issues are addressed jointly in our solution. On one hand, the approximate k -nearest neighbor graph construction is treated as a search task. Each sample along with its k -nearest neighbors is joined into the k -nearest neighbor graph by performing the nearest neighbor search sequentially on the graph under construction. On the other hand, the built k -nearest neighbor graph is used to support k -nearest neighbor search. Since the graph is built online, the dynamic update on the graph, which is not possible for most of the existing solutions, is supported. This solution is feasible for various distance measures. Its effectiveness both as k -nearest neighbor construction and k -nearest neighbor search approaches is verified across different types of data in different scales, various dimensions, and under different metrics.},
  archive      = {J_TMM},
  author       = {Wan-Lei Zhao and Hui Wang and Chong-Wah Ngo},
  doi          = {10.1109/TMM.2021.3073811},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1909-1921},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Approximate k-NN graph construction: A generic online approach},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Zero-shot video event detection with high-order semantic
concept discovery and matching. <em>TMM</em>, <em>24</em>, 1896–1908.
(<a href="https://doi.org/10.1109/TMM.2021.3073624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimedia event detection aims to precisely retrieve videos that contain complex semantic events from a large pool. This work addresses this task under a zero-shot setting, where only brief event-specific textural information (such as event names, a few descriptive sentences, etc.) is known yet none positive video example is provided. Mainstream approaches to tackling this task are middle-level semantic concept-based, where meticulously-crafted concept banks (e.g., LSCOM) are adopted. We argue that these concept banks are still inadequate facing video semantic complexity. Existing semantic concepts are essentially first-order, mainly designed for atomic objects, scenes or human actions, etc. This work advocates the utilization of high-order concepts (such as subject-predicate-object triplets or adjective-object). The main contributions are two-fold. First, we harvest a comprehensive albeit compact high-order concept library through distilling information from three large public datasets (MS-COCO, Visual Genome, and Kinetics-600), mainly related to visual relations and human-object interactions. Secondly, zero-shot events are often only briefly and partially described via textual input. The resultant semantic ambiguity makes the pursuit of the most indicative high-order concepts challenging. We thus design a novel query-expanding scheme that enriches ambiguous event-specific keywords by searching over either large common knowledge bases ( e.g. , WikiHow) or top-ranked webpages retrieved from modern search engines. This way sets up a more faithful connection between zero-shot events and high-order concepts. To our best knowledge, this is the first work that strives for concept-based video search beyond first-order concepts. Extensive experiments have been conducted on several large video benchmarks (TRECVID 2013, TRECVID 2014, and ActivityNet-1.3). The evaluations clearly demonstrate the superiority of our constructed high-order concept library and its complementariness to existing concepts.},
  archive      = {J_TMM},
  author       = {Yang Jin and Wenhao Jiang and Yi Yang and Yadong Mu},
  doi          = {10.1109/TMM.2021.3073624},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1896-1908},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Zero-shot video event detection with high-order semantic concept discovery and matching},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accurate scene text detection via scale-aware data
augmentation and shape similarity constraint. <em>TMM</em>, <em>24</em>,
1883–1895. (<a href="https://doi.org/10.1109/TMM.2021.3073575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text detection has attracted increasing concerns with the rapid development of deep neural networks in recent years. However, existing scene text detectors may overfit on the public datasets due to the limited training data, or generate inaccurate localization for arbitrary-shape scene texts. This paper presents an arbitrary-shape scene text detection method that can achieve better generalization ability and more accurate localization. We first propose a Scale-Aware Data Augmentation (SADA) technique to increase the diversity of training samples. SADA considers the scale variations and local visual variations of scene texts, which can effectively relieve the dilemma of limited training data. At the same time, SADA can enrich the training minibatch, which contributes to accelerating the training process. Furthermore, a Shape Similarity Constraint (SSC) technique is exploited to model the global shape structure of arbitrary-shape scene texts and backgrounds from the perspective of the loss function. SSC encourages the segmentation of text or non-text in the candidate boxes to be similar to the corresponding ground truth, which is helpful to localize more accurate boundaries for arbitrary-shape scene texts. Extensive experiments have demonstrated the effectiveness of the proposed techniques, and state-of-the-art performances are achieved over public arbitrary-shape scene text benchmarks (e.g., CTW1500 , Total-Text and ArT ).},
  archive      = {J_TMM},
  author       = {Pengwen Dai and Yang Li and Hua Zhang and Jingzhi Li and Xiaochun Cao},
  doi          = {10.1109/TMM.2021.3073575},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1883-1895},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Accurate scene text detection via scale-aware data augmentation and shape similarity constraint},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discriminative invariant alignment for unsupervised domain
adaptation. <em>TMM</em>, <em>24</em>, 1871–1882. (<a
href="https://doi.org/10.1109/TMM.2021.3073258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most prevalent branches of transfer learning, domain adaptation is dedicated to generalizing the knowledge of a source domain to a target domain to perform machine learning tasks. In domain adaptation, the key strategy is to overcome the shift between different domains and learn shared features with domain invariance. However, most existing methods focus on extracting the common features of the source and target domains, and do not consider the shift problem of class center in the target domain caused by this process. Specifically, when we align the domain distributions, we often ignore the inherent feature attributes of the data, or under the guidance of false pseudo-labels, cause the target domain data to be far away from the class center after projection. This is not conducive to classification task. To address these problems, in this study, we propose a novel domain adaptation method, referred to as discriminative invariant alignment (DIA), for image representation. DIA enriches the knowledge matrix by combining the class discriminative information of the source domain and local data structure information of the target domain into a new framework. By introducing the maximum margin criterion of the source domain, the classification boundaries are expanded. To verify the performance of the proposed method, we compared DIA with several state-of-the-art methods on five benchmark databases. The experimental results show that DIA is superior to the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Yuwu Lu and Desheng Li and Wenjing Wang and Zhihui Lai and Jie Zhou and Xuelong Li},
  doi          = {10.1109/TMM.2021.3073258},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1871-1882},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Discriminative invariant alignment for unsupervised domain adaptation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Action coherence network for weakly-supervised temporal
action localization. <em>TMM</em>, <em>24</em>, 1857–1870. (<a
href="https://doi.org/10.1109/TMM.2021.3073235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly-supervised Temporal Action Localization (W-TAL) aims at simultaneously classifying and locating all action instances with only video-level supervision. However, current W-TAL methods have two limitations. First, they ignore the difference in video representations between an action instance and its surrounding background when generating and scoring action proposals. Second, the unique characteristics of the RGB frames and optical flow are largely ignored when fusing these two modalities. To address these problems, an Action Coherence Network (ACN) is proposed in this paper. Its core is a new coherence loss which exploits both classification predictions and video content representations to supervise action boundary regression and thus leads to more accurate action localization results. Besides, the proposed ACN explicitly takes into account the specific characteristics of RGB frames and optical flow by training two separate sub-networks, each of which is able to generate modality-specific action proposals independently. Finally, to take advantage of the complementary action proposals generated by two streams, a novel fusion module is introduced to reconcile them and obtain the final action localization results. Experiments on the THUMOS14 and ActivityNet datasets show that our ACN outperforms the state-of-the-art W-TAL methods, and is even comparable to some recent fully-supervised methods. Particularly, ACN achieves a mean average precision of 26.4% on the THUMOS14 dataset under the IoU threshold 0.5.},
  archive      = {J_TMM},
  author       = {Yuanhao Zhai and Le Wang and Wei Tang and Qilin Zhang and Nanning Zheng and Gang Hua},
  doi          = {10.1109/TMM.2021.3073235},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1857-1870},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Action coherence network for weakly-supervised temporal action localization},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature estimations based correlation distillation for
incremental image retrieval. <em>TMM</em>, <em>24</em>, 1844–1856. (<a
href="https://doi.org/10.1109/TMM.2021.3073279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning for fine-grained image retrieval in an incremental context is less investigated. In this paper, we explore this task to realize the model’s continuous retrieval ability. That means, the model enables to perform well on new incoming data and reduce forgetting of the knowledge learned on preceding old tasks. For this purpose, we distill semantic correlations knowledge among the representations extracted from the new data only so as to regularize the parameters updates using the teacher-student framework. In particular, for the case of learning multiple tasks sequentially, aside from the correlations distilled from the penultimate model, we estimate the representations for all prior models and further their semantic correlations by using the representations extracted from the new data. To this end, the estimated correlations are used as an additional regularization and further prevent catastrophic forgetting over all previous tasks, and it is unnecessary to save the stream of models trained on these tasks. Extensive experiments demonstrate that the proposed method performs favorably for retaining performance on the already-trained old tasks and achieving good accuracy on the current task when new data are added at once or sequentially.},
  archive      = {J_TMM},
  author       = {Wei Chen and Yu Liu and Nan Pu and Weiping Wang and Li Liu and Michael S. Lew},
  doi          = {10.1109/TMM.2021.3073279},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1844-1856},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Feature estimations based correlation distillation for incremental image retrieval},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal marketing intent analysis for effective targeted
advertising. <em>TMM</em>, <em>24</em>, 1830–1843. (<a
href="https://doi.org/10.1109/TMM.2021.3073267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People’s daily information sharing and acquisition through the Internet has become more and more popular. The comprehensive multimodal marketing advertorial generated by ‘We Media’ accounts besides the normal social news is gaining its importance on social media platforms. In order to achieve effective advertising, the marketing intent understanding is a key step towards generating targeted advertising strategies (push advertorials to specific people at a specific time). However, advertorials in real are usually designed to pretend as normal social news with a wide range of contents. This poses big challenges to the platforms on accurately recognizing and analyzing the marketing intents behind the advertorials. As a pioneering study, we address this new problem of multimodal-based marketing intent analysis and answer three core questions: (1) does a piece of social news contain marketing intent? (2) what is the topic of marketing intent? (3) what is the extent of marketing intent? Towards this end, we propose a novel Multimodal-based Marketing Intent Analysis scheme (MMIA) to estimate the marketing intent embedded in the multimodal contents. Specifically, a novel supervised neural autoregressive model (SmiDocNADE) is proposed to enhance the discriminative capacity of the learned hidden features so that a single system is capable of solving the three questions. In order to effectively model inter-correlations between images and text in advertorials, we fuse multimodal data and extract features by Graph Convolution Networks as an enhancement to SmiDocNADE. The extensive evaluations demonstrate the advantages of our proposed system in multimodal-based marketing intent analysis from multiple aspects.},
  archive      = {J_TMM},
  author       = {Lu Zhang and Jialie Shen and Jian Zhang and Jingsong Xu and Zhibin Li and Yazhou Yao and Litao Yu},
  doi          = {10.1109/TMM.2021.3073267},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1830-1843},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multimodal marketing intent analysis for effective targeted advertising},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Voxel structure-based mesh reconstruction from a 3D point
cloud. <em>TMM</em>, <em>24</em>, 1815–1829. (<a
href="https://doi.org/10.1109/TMM.2021.3073265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mesh reconstruction from a 3D point cloud is an important topic in the fields of computer graphic, computer vision, and multimedia analysis. In this paper, we propose a voxel structure-based mesh reconstruction framework. It provides the intrinsic metric to improve the accuracy of local region detection. Based on the detected local regions, an initial reconstructed mesh can be obtained. With the mesh optimization in our framework, the initial reconstructed mesh is optimized into an isotropic one with the important geometric features such as external and internal edges. The experimental results indicate that our framework shows great advantages over peer ones in terms of mesh quality, geometric feature keeping, and processing speed. The source code of the proposed method is publicly available 1 .},
  archive      = {J_TMM},
  author       = {Chenlei Lv and Weisi Lin and Baoquan Zhao},
  doi          = {10.1109/TMM.2021.3073265},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1815-1829},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Voxel structure-based mesh reconstruction from a 3D point cloud},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly-supervised facial expression recognition in the wild
with noisy data. <em>TMM</em>, <em>24</em>, 1800–1814. (<a
href="https://doi.org/10.1109/TMM.2021.3072786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) has attracted much attention in recent years due to its wide applications. While some progress has been achieved thanks to the emergence of deep learning, the challenge occasioned by pose variations remains. Therefore, most conventional approaches mainly perform FER under laboratory-controlled environment, and the FER in-the-wild has received relatively less attention. To implement the FER in-the-wild, the pose-invariant expression recognition model would be a possible solution but for a paucity of training data. Sufficient training data with reliable expression labels on FER tasks typically are unavailable. This paper devotes to addressing the problem of how to model pose variations in facial images, and how to leverage noisy data in the web to boost the FER performance. The proposed model is implemented in an end-to-end weakly supervised manner and enjoys several merits. First, the proposed model utilizes massive noisy labeled data to boost the performance of the FER classifier trained on a small set of clean labels. Second, we offer a novel pose modeling network to adaptively capture the discrepancy in the deep representation space of facial images under different head poses, and consequently, the pose-invariant expression representations can be learned in our model. Last, to exploit the reliable information in the noisy data, we formulate a noise modeling network, which is capable of learning the mapping from feature space to the residuals between clean labels and noisy labels. We validate the proposed approach on four public FER benchmarks: AffectNet, RAF-DB, SFEW, and BU-3DFE. Extensive experiments show that the proposed method performs favorably against state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Feifei Zhang and Mingliang Xu and Changsheng Xu},
  doi          = {10.1109/TMM.2021.3072786},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1800-1814},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Weakly-supervised facial expression recognition in the wild with noisy data},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decoupled representation learning for character glyph
synthesis. <em>TMM</em>, <em>24</em>, 1787–1799. (<a
href="https://doi.org/10.1109/TMM.2021.3072449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Character glyph synthesis is still an open challenging problem, which involves two related aspects, i.e. , font style transfer and content consistency. In this paper, we propose a novel model named FontGAN, which integrates the character structure stylization, de-stylization and texture transfer into a unified framework. Specifically, we decouple character images into style representation and content representation, which offers fine-grained control of these two types of variables, thus improving the quality of the generated results. To effectively capture the style information, a style consistency module (SCM) is introduced. Technically, SCM exploits category-guided Kullback-Leibler divergence to explicitly model the style representation into different prior distributions. In this way, our model is capable of implementing transformations between multiple domains in one framework. In addition, we propose content prior module (CPM) to provide content prior for the model to guide the content encoding process and alleviates the problem of stroke deficiency during structure de-stylization. Benefiting from the idea of decoupling and regrouping, our FontGAN suffices to achieve many-to-many translation tasks for glyph structure. Experimental results demonstrate that the proposed FontGAN achieves the state-of-the-art performance in character glyph synthesis.},
  archive      = {J_TMM},
  author       = {Xiyan Liu and Gaofeng Meng and Jianlong Chang and Ruiguang Hu and Shiming Xiang and Chunhong Pan},
  doi          = {10.1109/TMM.2021.3072449},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1787-1799},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Decoupled representation learning for character glyph synthesis},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual attention on pyramid feature maps for image captioning.
<em>TMM</em>, <em>24</em>, 1775–1786. (<a
href="https://doi.org/10.1109/TMM.2021.3072479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating natural sentences from images is a fundamental learning task for visual-semantic understanding in multimedia. In this paper, we propose to apply dual attention on pyramid image feature maps to fully explore the visual-semantic correlations and improve the quality of generated sentences. Specifically, with the full consideration of the contextual information provided by the hidden state of the RNN controller, the pyramid attention can better localize the visually indicative and semantically consistent regions in images. On the other hand, the contextual information can help re-calibrate the importance of feature components by learning the channel-wise dependencies, to improve the discriminative power of visual features for better content description. We conducted comprehensive experiments on three well-known datasets: Flickr8K, Flickr30 K and MS COCO, which achieved impressive results in generating descriptive and smooth natural sentences from images. Using either convolution visual features or more informative bottom-up attention features, the composite model can boost the performance of image-to-sentence translation, with a limited computational resource overhead. The proposed pyramid attention and dual attention methods are highly modular, which can be inserted into various image captioning modules to further improve the performance.},
  archive      = {J_TMM},
  author       = {Litao Yu and Jian Zhang and Qiang Wu},
  doi          = {10.1109/TMM.2021.3072479},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1775-1786},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual attention on pyramid feature maps for image captioning},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Disentangled representation learning for cross-modal
biometric matching. <em>TMM</em>, <em>24</em>, 1763–1774. (<a
href="https://doi.org/10.1109/TMM.2021.3071243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal biometric matching (CMBM) aims to determine the corresponding voice from a face, or identify the corresponding face from a voice. Recently, many CMBM methods have been proposed by forcing the distance between two modal features to be narrowed. However, these methods ignore the alignability between the two modal features. Because the feature is extracted under the supervision of identity information from single modal data, it can only reflect the identity information of single modal data. In order to address this problem, a disentangled representation learning method is proposed to disentangle the alignable latent identity factors and nonalignable the modality-dependent factors for CMBM. The proposed method consists of two main steps: 1) feature extraction and 2) disentangled representation learning. Firstly, an image feature extraction network is adopted to obtain face features, and a voice feature extraction network is applied to learn voice features. Secondly, a disentangled latent variable is explored to disentangle the latent identity factors that are shared across the modalities from the modality-dependent factors. The modality-dependent factors are filtered out, while the latent identity factors from the two modalities are enforced to be narrowed to align the same identity information. Then, the disentangled latent identity factors are considered as pure identity information to bridge the two modalities for cross-modal verification, 1: $N$ matching, and retrieval. Note that the proposed method learns the identity information from the input face images and voice segments with only identity label as supervised information. Extensive experiments on the challenging VoxCeleb dataset demonstrate the proposed method outperforms the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Hailong Ning and Xiangtao Zheng and Xiaoqiang Lu and Yuan Yuan},
  doi          = {10.1109/TMM.2021.3071243},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1763-1774},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Disentangled representation learning for cross-modal biometric matching},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comprehensive study on deep learning-based methods for
sign language recognition. <em>TMM</em>, <em>24</em>, 1750–1762. (<a
href="https://doi.org/10.1109/TMM.2021.3070438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a comparative experimental assessment of computer vision-based methods for sign language recognition is conducted. By implementing the most recent deep neural network methods in this field, a thorough evaluation on multiple publicly available datasets is performed. The aim of the present study is to provide insights on sign language recognition, focusing on mapping non-segmented video streams to glosses. For this task, two new sequence training criteria, known from the fields of speech and scene text recognition, are introduced. Furthermore, a plethora of pretraining schemes is thoroughly discussed. Finally, a new RGB+D dataset for the Greek sign language is created. To the best of our knowledge, this is the first sign language dataset where three annotation levels are provided (individual gloss, sentence and spoken language) for the same set of video captures.},
  archive      = {J_TMM},
  author       = {Nikolas Adaloglou and Theocharis Chatzis and Ilias Papastratis and Andreas Stergioulas and Georgios Th. Papadopoulos and Vassia Zacharopoulou and George J. Xydopoulos and Klimnis Atzakas and Dimitris Papazachariou and Petros Daras},
  doi          = {10.1109/TMM.2021.3070438},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1750-1762},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A comprehensive study on deep learning-based methods for sign language recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep-IRTarget: An automatic target detector in infrared
imagery using dual-domain feature extraction and allocation.
<em>TMM</em>, <em>24</em>, 1735–1749. (<a
href="https://doi.org/10.1109/TMM.2021.3070138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, convolutional neural networks (CNNs) have brought impressive improvements for object detection. However, detecting targets in infrared images still remains challenging, because the poor texture information, low resolution and high noise levels of the thermal imagery restrict the feature extraction ability of CNNs. In order to deal with these difficulties in the feature extraction, we propose a novel backbone network named Deep-IRTarget, composing of a frequency feature extractor, a spatial feature extractor and a dual-domain feature resource allocation model. Hypercomplex Infrared Fourier Transform is developed to calculate the infrared intensity saliency by designing hypercomplex representations in the frequency domain, while a convolutional neural network is invoked to extract feature maps in the spatial domain. Features from the frequency domain and spatial domain are stacked to construct Dual-domain features. To efficiently integrate and recalibrate them, we propose a Resource Allocation model for Features (RAF). The well-designed channel attention block and position attention block are used in RAF to respectively extract interdependent relationships among channel and position dimensions, and capture channel-wise and position-wise contextual information. Extensive experiments are conducted on three challenging infrared imagery databases. We achieve 10.14%, 9.1% and 8.05% improvement on mAP scores, compared to the current state of the art method on MWIR, BITIR and WCIR respectively.},
  archive      = {J_TMM},
  author       = {Ruiheng Zhang and Lixin Xu and Zhengyu Yu and Ye Shi and Chengpo Mu and Min Xu},
  doi          = {10.1109/TMM.2021.3070138},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1735-1749},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep-IRTarget: An automatic target detector in infrared imagery using dual-domain feature extraction and allocation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting 3D points of interest using projective neural
networks. <em>TMM</em>, <em>24</em>, 1637–1650. (<a
href="https://doi.org/10.1109/TMM.2021.3070977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting points of interest on 3D shapes is a fundamental research problem in geometry processing. Due to the complicated relationship between points of interest and their geometric features, detecting points of interest on any given 3D shape remains challenging. Due to the lack of training data, previous data-driven methods for detecting 3D points of interest mainly focus on utilizing hand-crafted geometric features to predict the probabilities of each point being a POI, which greatly limits detection performance. In this paper, we propose a novel algorithm for detecting 3D points of interest by using projective neural networks. Our method first projects the labeled training 3D shapes into multiple 2D views and then learns the required features from the 2D views in an end-to-end fashion. The points of interest on test 3D shapes are then automatically detected by applying the learned neural network and our improved density peak clustering. Our method relies neither on hand-crafted feature descriptors nor a large quantity of expensive 3D training data to obtain satisfactory results. Experimental results show significantly superior detection performance of our method over the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Zhenyu Shu and Sipeng Yang and Shiqing Xin and Chaoyi Pang and Xiaogang Jin and Ladislav Kavan and Ligang Liu},
  doi          = {10.1109/TMM.2021.3070977},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1637-1650},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Detecting 3D points of interest using projective neural networks},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Personalized image recoloring for color vision deficiency
compensation. <em>TMM</em>, <em>24</em>, 1721–1734. (<a
href="https://doi.org/10.1109/TMM.2021.3070108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several image recoloring methods have been proposed to compensate for the loss of contrast caused by color vision deficiency (CVD). However, these methods only work for dichromacy (a case in which one of the three types of cone cells loses its function completely), while the majority of CVD is anomalous trichromacy (another case in which one of the three types of cone cells partially loses its function). In this paper, a novel degree-adaptable recoloring algorithm is presented, which recolors images by minimizing an objective function constrained by contrast enhancement and naturalness preservation. To assess the effectiveness of the proposed method, a quantitative evaluation using common metrics and subjective studies involving 14 volunteers with varying degrees of CVD are conducted. The results of the evaluation experiment show that the proposed personalized recoloring method outperforms the state-of-the-art methods, achieving desirable contrast enhancement adapted to different degrees of CVD while preserving naturalness as much as possible.},
  archive      = {J_TMM},
  author       = {Zhenyang Zhu and Masahiro Toyoura and Kentaro Go and Kenji Kashiwagi and Issei Fujishiro and Tien-Tsin Wong and Xiaoyang Mao},
  doi          = {10.1109/TMM.2021.3070108},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1721-1734},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Personalized image recoloring for color vision deficiency compensation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual perception based algorithm for fast depth intra
coding of 3D-HEVC. <em>TMM</em>, <em>24</em>, 1707–1720. (<a
href="https://doi.org/10.1109/TMM.2021.3070106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D-HEVC (The 3D Extension of High Efficiency Video Coding) is the newest 3D video coding standard, which enriches multimedia applications with the video format of multi-view plus depth. For the depth map coding in 3D-HEVC, the advanced coding tools enhance the coding efficiency of the depth map and the quality of the synthesized view. However, the time consumption and complexity of 3D-HEVC also increase significantly. This paper utilizes the characteristics of human visual system to propose a fast algorithm based on visual perception for the acceleration of the depth intra coding of 3D-HEVC. The depth map is segmented into different regions by Otsu&#39;s auto-thresholding. The dominate edge direction is categorized for each prediction unit. We detect the perceptual edge based on just noticeable depth difference model to extract the area that may affect the visual perception. According to depth map segmentation and edge distribution, we reduce the corresponding intra angular modes and determine whether to perform depth modelling mode. We also incorporate the boundary continuity and rate-distortion cost thresholding to propose the fast coding unit decision. The experimental results show that the proposed algorithm eliminates 53.09% of the depth coding time with only 0.15% BD-BR on average. The coding performance of the proposed algorithm outperforms the previous works significantly.},
  archive      = {J_TMM},
  author       = {Jie-Ru Lin and Mei-Juan Chen and Chia-Hung Yeh and Yong-Ci Chen and Lih-Jen Kau and Chuan-Yu Chang and Min-Hui Lin},
  doi          = {10.1109/TMM.2021.3070106},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1707-1720},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Visual perception based algorithm for fast depth intra coding of 3D-HEVC},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-latency network-adaptive error control for interactive
streaming. <em>TMM</em>, <em>24</em>, 1691–1706. (<a
href="https://doi.org/10.1109/TMM.2021.3070134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel network-adaptive algorithm that is suitable for alleviating network packet losses for low-latency interactive communications between a source and a destination. Our network-adaptive algorithm estimates in real-time the best parameters of a recently proposed streaming code that uses forward error correction (FEC) to correct both arbitrary and burst losses, which cause a crackling noise and undesirable jitters, respectively in audio. In particular, the destination estimates appropriate coding parameters based on its observed packet loss pattern and sends them back to the source for updating the underlying code. Besides, a new explicit construction of practical low-latency streaming codes that achieve the optimal tradeoff between the capability of correcting arbitrary losses and the capability of correcting burst losses is used. Simulation evaluations based on statistical losses and real-world packet loss traces reveal the following: (i) Our proposed network-adaptive algorithm combined with our optimal streaming codes can achieve significantly higher performance compared to uncoded and non-adaptive FEC schemes over UDP (User Datagram Protocol); (ii) Our explicit streaming codes can significantly outperform traditional MDS (maximum-distance separable) streaming schemes when they are used along with our network-adaptive algorithm. In addition, we study different factors that can affect the performance of our network-adaptive algorithm.},
  archive      = {J_TMM},
  author       = {Salma Emara and Silas L. Fong and Baochun Li and Ashish Khisti and Wai-Tian Tan and Xiaoqing Zhu and John Apostolopoulos},
  doi          = {10.1109/TMM.2021.3070134},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1691-1706},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Low-latency network-adaptive error control for interactive streaming},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-localized sensitive autoencoder-attention-LSTM for
skeleton-based action recognition. <em>TMM</em>, <em>24</em>, 1678–1690.
(<a href="https://doi.org/10.1109/TMM.2021.3070127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of key challenges of skeleton-based action recognition (SAR) tasks is the complex nature of human motion patterns. Variations such as performers and viewpoints may impose negative effects to the action recognition accuracy. In this work, we propose the Multi-Localized Sensitive Autoencoder-Attention-LSTM (Multi-LiSAAL) for SAR. The Localized Stochastic Sensitive Autoencoder (LiSSA) encodes both spatial and temporal information, and extracts meaningful features from different parts (four limbs and a trunk) from the skeleton. The LiSSA is trained by minimizing the localized generalization error to enhance the robustness of autoencoders via reducing its sensitivity with respect to small variations in inputs. We apply an attention mechanism to assign different weights to different skeleton parts and focus more on informative sections. Then, a backbone classifier network takes weighted features as inputs to differentiates actions. Experimental results on five public benchmarking datasets show that the Multi-LiSAAL outperforms state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Wing Ng and Mingyang Zhang and Ting Wang},
  doi          = {10.1109/TMM.2021.3070127},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1678-1690},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-localized sensitive autoencoder-attention-LSTM for skeleton-based action recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Beyond triplet loss: Person re-identification with
fine-grained difference-aware pairwise loss. <em>TMM</em>, <em>24</em>,
1665–1677. (<a href="https://doi.org/10.1109/TMM.2021.3069562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Re-IDentification (ReID) aims at re-identifying persons from different viewpoints across multiple cameras. Capturing the fine-grained appearance differences is often the key to accurate person ReID, because many identities can be differentiated only when looking into these fine-grained differences. However, most state-of-the-art person ReID approaches, typically driven by a triplet loss, fail to effectively learn the fine-grained features as they are focused more on differentiating large appearance differences. To address this issue, we introduce a novel pairwise loss function that enables ReID models to learn the fine-grained features by adaptively enforcing an exponential penalization on the images of small differences and a bounded penalization on the images of large differences. The proposed loss is generic and can be used as a plugin to replace the triplet loss to significantly enhance different types of state-of-the-art approaches. Experimental results on four benchmark datasets show that the proposed loss substantially outperforms a number of popular loss functions by large margins; and it also enables significantly improved data efficiency.},
  archive      = {J_TMM},
  author       = {Cheng Yan and Guansong Pang and Xiao Bai and Changhong Liu and Xin Ning and Lin Gu and Jun Zhou},
  doi          = {10.1109/TMM.2021.3069562},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1665-1677},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Beyond triplet loss: Person re-identification with fine-grained difference-aware pairwise loss},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Employing bilinear fusion and saliency prior information for
RGB-d salient object detection. <em>TMM</em>, <em>24</em>, 1651–1664.
(<a href="https://doi.org/10.1109/TMM.2021.3069297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal feature fusion and saliency reasoning are two core sub-tasks of RGB-D salient object detection. However, most existing models employ linear fusion strategies (e.g., concatenation) for multi-modal feature fusion and use a simple coarse-to-fine structure for saliency reasoning. Despite their simpleness, they can neither fully capture the cross-modal complementary information nor exploit the multi-level complementary information among the cross-modal features at different levels. To address these issues, a novel RGB-D salient object detection model is presented, where we pay special attention to the aforementioned two sub-tasks. Concretely, a multi-modal feature interaction module is first presented to explore more interactions between the unimodal RGB and depth features. It helps to capture their cross-modal complementary information by jointly using some simple linear fusion strategies and bilinear fusion ones. Then, a saliency prior information guided fusion module is presented to exploit the multi-level complementary information among the fused cross-modal features at different levels. Instead of employing a simple convolutional layer for the final saliency prediction, a saliency refinement and prediction module is designed to better exploit those extracted multi-level cross-modal information for RGB-D saliency detection. Experimental results on several benchmark datasets verify the effectiveness and superiority of the proposed framework over some state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Nianchang Huang and Yang Yang and Dingwen Zhang and Qiang Zhang and Jungong Han},
  doi          = {10.1109/TMM.2021.3069297},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1651-1664},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Employing bilinear fusion and saliency prior information for RGB-D salient object detection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). End-to-end rain removal network based on progressive
residual detail supplement. <em>TMM</em>, <em>24</em>, 1622–1636. (<a
href="https://doi.org/10.1109/TMM.2021.3068833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods of rain removal based on deep learning have rapidly developed, and the image quality after rain removal is continuously improving. However, the results of most methods have some common problems, including a loss of details, a blurring of edges, and the existence of artifacts. To remove rain-related information more thoroughly and retain more edge details, this paper proposes an end-to-end rain removal network based on the progressive residual detail supplement (ERRN-PRDS) approach. The entire network structure is designed in an iterative manner to obtain higher-quality rain removal images from coarse to fine. In the network, a diamond residual block is constructed as the main module of iteration to learn the feature information of the background layer. Meanwhile, to keep more texture details in the background layer, a detail supplement mechanism is designed between the iterative layers to transfer more information to the next iterative operation. Experimental results show that this method can remove the rain information more completely and better retain the image edges compared with previous state-of-the-art methods. In addition, because of the sparsity of the detail injection, our network also achieves high-quality results for image denoising tasks.},
  archive      = {J_TMM},
  author       = {Yong Yang and Juwei Guan and Shuying Huang and Weiguo Wan and Yating Xu and Jiaxiang Liu},
  doi          = {10.1109/TMM.2021.3068833},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1622-1636},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {End-to-end rain removal network based on progressive residual detail supplement},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recurrent exposure generation for low-light face detection.
<em>TMM</em>, <em>24</em>, 1609–1621. (<a
href="https://doi.org/10.1109/TMM.2021.3068840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face detection from low-light images is challenging due to limited photons and inevitable noise, which, to make the task even harder, are often spatially unevenly distributed. A natural solution is to borrow the idea from multi-exposure , which captures multiple shots to obtain well-exposed images under challenging conditions. High-quality implementation/approximation of multi-exposure from a single image is however nontrivial. Fortunately, as shown in this paper, neither is such high-quality necessary since our task is face detection rather than image enhancement . Specifically, we propose a novel Recurrent Exposure Generation (REG) module and couple it seamlessly with a Multi-Exposure Detection (MED) module, and thus significantly improve face detection performance by effectively inhibiting non-uniform illumination and noise issues. REG produces progressively and efficiently intermediate images corresponding to various exposure settings, and such pseudo-exposures are then fused by MED to detect faces across different lighting conditions. The proposed method, named REGDet , is the first ‘detection-with-enhancement’ framework for low-light face detection. It not only encourages rich interaction and feature fusion across different illumination levels, but also enables effective end-to-end learning of the REG component to be better tailored for face detection. Moreover, as clearly shown in our experiments, REG can be flexibly coupled with different face detectors without extra low/normal-light image pairs for training. We tested REGDet on the DARK FACE low-light face benchmark with thorough ablation study, where REGDet outperforms previous state-of-the-arts by a significant margin, with only negligible extra parameters.},
  archive      = {J_TMM},
  author       = {Jinxiu Liang and Jingwen Wang and Yuhui Quan and Tianyi Chen and Jiaying Liu and Haibin Ling and Yong Xu},
  doi          = {10.1109/TMM.2021.3068840},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1609-1621},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Recurrent exposure generation for low-light face detection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). List-wise rank learning for stereoscopic image retargeting
quality assessment. <em>TMM</em>, <em>24</em>, 1595–1608. (<a
href="https://doi.org/10.1109/TMM.2021.3068814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereoscopic imageretargeting (SIR) techniques attempt to display stereoscopic images on stereoscopic devices of various resolutions and aspect ratios to provide the users with better viewing experience. However, new quality perceptual problems emerge in the retargeted stereoscopic images generated by current SIR operators are quite different from those in the retargeted 2D images. In this paper, we dedicate to exploring the perceptual quality-related factors (e.g., shape preservation, object preservation and visual comfort.) of retargeted stereoscopic images, and propose a novel quality evaluation metric for SIR to achieve a more consistent evaluation with 3D perception and image degradation mechanism in the SIR process. Moreover, image quality features and 3D perceptual features are integrated into one representation for an overall perceptual quality prediction using a list-wise ranking approach, which gives priority to the ranking among the SIR results generated from the same stereoscopic source. Experimental results demonstrate that the proposed method outperforms most quality models developed for retargeted 2D/stereoscopic images.},
  archive      = {J_TMM},
  author       = {Xuejin Wang and Feng Shao and Qiuping Jiang and Xiongli Chai and Xiangchao Meng and Yo-Sung Ho},
  doi          = {10.1109/TMM.2021.3068814},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1595-1608},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {List-wise rank learning for stereoscopic image retargeting quality assessment},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-scale sparse graph convolutional network for the
assessment of parkinsonian gait. <em>TMM</em>, <em>24</em>, 1583–1594.
(<a href="https://doi.org/10.1109/TMM.2021.3068609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated assessment of patients with Parkinson&#39;s disease (PD) is urgently required in clinical practice to improve the diagnostic efficiency and objectivity and to remotely monitor the motor disorder symptoms and general health of these patients, especially in view of the travel restrictions due to the recent coronavirus epidemic. Gait motor disorder is one of the critical manifestations of PD, and automated assessment of gait is vital to realize automated assessment of PD patients. To this end, we propose a novel two-stream spatial-temporal attention graph convolutional network (2s-ST-AGCN) for video assessment of PD gait motor disorder. Specifically, the skeleton sequence of human body is extracted from videos to construct spatial-temporal graphs of joints and bones, and a two-stream spatial-temporal graph convolutional network is then built to simultaneously model the static spatial information and dynamic temporal variations. The multi-scale spatial-temporal attention-aware mechanism is also designed to effectively extract the discriminative spatial-temporal features. The deep supervision strategy is then embedded to minimize classification errors, thereby guiding the weight update process of the hidden layer to promote significant discriminative features. Besides, two model-driven terms are integrated into this deep learning framework to strengthen multi-scale similarity in the deep supervision and realize sparsification of discriminative features. Extensive experiments on the clinical video dataset show that the proposed model exhibits good performance with an accuracy of 65.66% and an acceptable accuracy of 98.90%, which is much better than that of the existing sensor- and vision-based methods for Parkinsonian gait assessment. Thus, the proposed method is potentially useful for assessing PD gait motor disorder in clinical practice.},
  archive      = {J_TMM},
  author       = {Rui Guo and Xiangxin Shao and Chencheng Zhang and Xiaohua Qian},
  doi          = {10.1109/TMM.2021.3068609},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1583-1594},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-scale sparse graph convolutional network for the assessment of parkinsonian gait},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Alleviating modality bias training for infrared-visible
person re-identification. <em>TMM</em>, <em>24</em>, 1570–1582. (<a
href="https://doi.org/10.1109/TMM.2021.3067760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of infrared-visible person re-identification (IV-reID) is to recognize people across two modalities ( i.e. , RGB and IR). Existing cutting-edge approaches normally use a pair of images that have the same IDs ( i.e. , ID-tied cross-modality image pairs) and input them into an ImageNet-trained ResNet50. The ResNet50 backbone model can learn shared features across modalities to tolerate modality discrepancies between RGB and IR. This work will unveil a Modality Bias Training (MBT) problem that is less discussed in IV-reID, which will demonstrate that MBT significantly compromises the performance of IV-reID. Due to MBT, IR information can be overwhelmed by RGB information during training when the ResNet50 model is pretrained based on a large amount of RGB images from ImageNet. Thus, the trained models are more inclined to RGB information. Accordingly, the cross-modality generalization ability of the model is also compromised. To tackle this issue, we present a Dual-level Learning Strategy (DLS) that 1) enforces the focus of the network on ID-exclusive (rather than ID-tied) labels of cross-modality image pairs to mitigate the problem of MBT and 2) introduces third modality data that contain both RGB and IR information to further prevent the information from the IR modality from being overwhelmed during training. Our third modality images are generated by a generative adversarial network. A dynamic ID-exclusive Smooth (dIDeS) label is proposed for the generated third modality data. In experiments, comprehensive experiments are carried out to demonstrate the success of DLS in tackling the MBT issue exposed in IV-reID.},
  archive      = {J_TMM},
  author       = {Yan Huang and Qiang Wu and Jingsong Xu and Yi Zhong and Peng Zhang and Zhaoxiang Zhang},
  doi          = {10.1109/TMM.2021.3067760},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1570-1582},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Alleviating modality bias training for infrared-visible person re-identification},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bal-r<span class="math inline"><sup>2</sup></span>CNN: High
quality recurrent object detection with balance optimization.
<em>TMM</em>, <em>24</em>, 1558–1569. (<a
href="https://doi.org/10.1109/TMM.2021.3067439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a common practice to refine object detection results using recurrent detection paradigm. We evaluate the recurrent detection on Faster R-CNN, but the improvement is far away from expected. We consider that the performance bottleneck is from imbalance optimization caused by the biased distribution of training data. Low-IoU-skewed RPN proposals could suppress the contribution of High-IoU examples at the training stage. Besides, data imbalance and statistical discrepancy on regression targets between low-IoU and high-IoU examples are not considered in the regression task; this design could impede localization quality. In this work, we propose Bal-R $^2$ CNN for high-quality recurrent object detection. There are two new components in Bal-R $^2$ CNN. Self-iteration box sampling collects object boxes from recurrent steps and increases the number of high-IoU training examples. IoU-sensitive bounding-box regression sends proposal boxes with different IoUs to specified regression branches for more accurate bounding-box prediction. Both two new components could induce balanced optimization and be helpful. With the resulting Bal-R $^2$ CNN detector, evaluation on PASCAL VOC and MSCOCO reveal that our method has a significant improvement on the existing solution and could reach a better performance than several state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Xiaoyu Chen and Hongliang Li and Qingbo Wu and Fanman Meng and Heqian Qiu},
  doi          = {10.1109/TMM.2021.3067439},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1558-1569},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Bal-R$^2$CNN: High quality recurrent object detection with balance optimization},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A blind color separation model for faithful palette-based
image recoloring. <em>TMM</em>, <em>24</em>, 1545–1557. (<a
href="https://doi.org/10.1109/TMM.2021.3067463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Palette-based image recoloring provides a simple yet effective way for color adjustment, which allows users to interactively manipulate the color of an image by editing a compact color palette. While remarkable progress has been made by previous methods, they have the common limitations that may produce unfaithful image recoloring results i.e., the obtained result does not respond faithfully to the palette adjustment, and tend to induce visual artifacts such as color bleeding and distortion. To address these limitations, we in this paper present a novel color separation model for palette-based recoloring. Akin to previous methods, our color separation model is built upon the assumption that color of each pixel in an image can be formulated as a linear combination of a small set of same basis colors. However, different from previous palette-based recoloring methods which typically rely on heuristic rules to build the color separation model, we experimentally reveal the underlying relationship between the color separation and the palette-based recoloring, and summarize three specialized color separation priors that allow more faithful palette-based recoloring. Based on these priors, we devise a blind color separation model that not only does not require known palette as input as done in previous methods, but also enables more effective palette-based recoloring with much less visual artifacts. Experiments on two datasets demonstrate that our method outperforms the state-of-the-art palette-based recoloring methods. In addition, we show some applications enabled by the proposed color separation model, including automatic pattern coloring generation, green screen keying and region-controllable color transfer.},
  archive      = {J_TMM},
  author       = {Qing Zhang and Yongwei Nie and Lei Zhu and Chunxia Xiao and Wei-Shi Zheng},
  doi          = {10.1109/TMM.2021.3067463},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1545-1557},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A blind color separation model for faithful palette-based image recoloring},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and analysis of MEC- and proactive caching-based
<span class="math inline">360<sup>∘</sup></span> mobile VR video
streaming. <em>TMM</em>, <em>24</em>, 1529–1544. (<a
href="https://doi.org/10.1109/TMM.2021.3067205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, 360-degree mobile virtual reality video (MVRV) has become increasingly popular because it can provide users with an immersive experience. However, MVRV is usually recorded in a high resolution and is sensitive to latency, which indicates that broadband, ultra-reliable, and low-latency communication is necessary to guarantee the users’ quality of experience. In this paper, we propose a mobile edge computing (MEC)-based 360-degree MVRV streaming scheme with field-of-view (FoV) prediction, which jointly considers video coding, proactive caching, computation offloading, and data transmission. To meet the requirement of stringent end-to-end (E2E) latency, the user’s viewpoint prediction is utilized to cache video data proactively, and computing tasks are partially offloaded to the MEC server. In addition, we propose an analytical model based on diffusion process to study the packet transmission process of 360-degree MVRV in multihop wired/wireless networks and analyze the performance of the MEC-enabled scheme. The simulation results verify the accuracy of the analysis and the effectiveness of the proposed MVRV streaming scheme in reducing the E2E delay. Furthermore, the analytical framework sheds some light on the impacts of system parameters, e.g., FoV prediction accuracy and transmission rate, on the balance between computation delay and communication delay.},
  archive      = {J_TMM},
  author       = {Qi Cheng and Hangguan Shan and Weihua Zhuang and Lu Yu and Zhaoyang Zhang and Tony Q. S. Quek},
  doi          = {10.1109/TMM.2021.3067205},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1529-1544},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Design and analysis of MEC- and proactive caching-based $360^{\circ }$ mobile VR video streaming},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatiotemporal saliency representation learning for video
action recognition. <em>TMM</em>, <em>24</em>, 1515–1528. (<a
href="https://doi.org/10.1109/TMM.2021.3066775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) have achieved great success in human action recognition, however they are still limited in understanding complex and noisy videos owing to the difficulties of exploiting appearance and motion information. Most existing works have been devoted to designing CNN architectures, which overlook the quality of network inputs that is of great importance. This paper provides an alternative solution of action recognition improvement by focusing on the quality of network inputs. A multi-task video salient object detection approach with object-of-interest segmentation scheme, which takes into account both human and action-relevant cues, is proposed to immunize the input video from background clutter. Further, a simple spatiotemporal residual network architecture is presented, which operates on multiple high-quality inputs for long-term action representation learning. Empirical evaluations on various challenging datasets demonstrate that the proposed framework can perform competitively against state-of-the-art. Besides better performance, learning representations of saliency can help prevent the action recognition model from overfitting and speed up the convergence of training.},
  archive      = {J_TMM},
  author       = {Yongqiang Kong and Yunhong Wang and Annan Li},
  doi          = {10.1109/TMM.2021.3066775},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1515-1528},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spatiotemporal saliency representation learning for video action recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A discriminative vectorial framework for multi-modal feature
representation. <em>TMM</em>, <em>24</em>, 1503–1514. (<a
href="https://doi.org/10.1109/TMM.2021.3066118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rapid advancements of sensory and computing technology, multi-modal data sources that represent the same pattern or phenomenon have attracted growing attention. As a result, finding means to explore useful information from these multi-modal data sources has quickly become a necessity. In this paper, a discriminative vectorial framework is proposed for multi-modal feature representation in knowledge discovery by employing multi-modal hashing (MH) and discriminative correlation maximization (DCM) analysis. Specifically, the proposed framework is capable of minimizing the semantic similarity among different modalities by MH and exacting intrinsic discriminative representations across multiple data sources by DCM analysis jointly, enabling a novel vectorial framework of multi-modal feature representation. Moreover, the proposed feature representation strategy is analyzed and further optimized based on canonical and non-canonical cases, respectively. Consequently, the generated feature representation leads to effective utilization of the input data sources of high quality, producing improved, sometimes quite impressive, results in various applications. The effectiveness and generality of the proposed framework are demonstrated by utilizing classical features and deep neural network (DNN) based features with applications to image and multimedia analysis and recognition tasks, including data visualization, face recognition, object recognition; cross-modal (text-image) recognition and audio emotion recognition. Experimental results show that the proposed solutions are superior to state-of-the-art statistical machine learning (SML) and DNN algorithms.},
  archive      = {J_TMM},
  author       = {Lei Gao and Ling Guan},
  doi          = {10.1109/TMM.2021.3066118},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1503-1514},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A discriminative vectorial framework for multi-modal feature representation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rhythm-aware sequence-to-sequence learning for labanotation
generation with gesture-sensitive graph convolutional encoding.
<em>TMM</em>, <em>24</em>, 1488–1502. (<a
href="https://doi.org/10.1109/TMM.2021.3066115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Labanotation is a professional dance notation system widely used in dance education and choreography preservation. Automatically generatingLabanotation dance scores from motion capture data can save a huge amount of manual time and effort. Recently, the sequence-to-sequence (seq2seq) model is applied to the automatic Labanotation generation. This model is based on an encoder-decoder structure, which encodes the input motion sequence to a fixed-length vector and then decodes it to generate the target sequence. However, the encoding of spatial skeleton structure of motion data is not considered in the existing work. Besides, it is challenging to align between the input motion data and the output Laban symbol sequences due to the severe imbalance of sequence lengths. Therefore, in this paper, we present a new seq2seq model for more effective Labanotation generation. In the encoder, we propose a new gesture-sensitive graph convolutional network with learned adaptive joint weights and non-physical connections to learn both spatial and temporal patterns from motion data sequences. In the decoder, we exploit motion rhythm information and propose a novel rhythm-aware attention mechanism to learn a good alignment between motion sequences and Laban symbol sequences, so that we can focus on relevant parts of the input motion sequence without searching in the whole sequence when predicting a target Laban symbol. Extensive experiments on two real-world datasets show that the proposed method achieves a better performance compared with the state-of-the-art approaches on the task of automatic Labanotation generation.},
  archive      = {J_TMM},
  author       = {Min Li and Zhenjiang Miao and Xiao-Ping Zhang and Wanru Xu and Cong Ma and Ningwei Xie},
  doi          = {10.1109/TMM.2021.3066115},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1488-1502},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Rhythm-aware sequence-to-sequence learning for labanotation generation with gesture-sensitive graph convolutional encoding},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fine-grained attention and feature-sharing generative
adversarial networks for single image super-resolution. <em>TMM</em>,
<em>24</em>, 1473–1487. (<a
href="https://doi.org/10.1109/TMM.2021.3065731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional super-resolution (SR) methods by minimize the mean square error usually produce images with over-smoothed and blurry edges, due to the lack of high-frequency details. In this paper, we propose two novel techniques within the generative adversarial network framework to encourage generation of photo-realistic images for image super-resolution. Firstly, instead of producing a single score to discriminate real and fake images, we propose a variant, called Fine-grained Attention Generative Adversarial Network (FASRGAN), to discriminate each pixel of real and fake images. FASRGAN adopts a UNet-like network as the discriminator with two outputs: an image score and an image score map. The score map has the same spatial size as the HR/SR images, serving as the fine-grained attention to represent the degree of reconstruction difficulty for each pixel. Secondly, instead of using different networks for the generator and the discriminator, we introduce a feature-sharing variant (denoted as Fs-SRGAN) for both the generator and the discriminator. The sharing mechanism can maintain model express power while making the model more compact, and thus can improve the ability of producing high-quality images. Quantitative and visual comparisons with state-of-the-art methods on benchmark datasets demonstrate the superiority of our methods. We further apply our super-resolution images for object recognition, which further demonstrates the effectiveness of our proposed method. The code is available at https://github.com/Rainyfish/FASRGAN-and-Fs-SRGAN .},
  archive      = {J_TMM},
  author       = {Yitong Yan and Chuangchuang Liu and Changyou Chen and Xianfang Sun and Longcun Jin and Xinyi Peng and Xiang Zhou},
  doi          = {10.1109/TMM.2021.3065731},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1473-1487},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fine-grained attention and feature-sharing generative adversarial networks for single image super-resolution},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A total variation with joint norms for infrared and visible
image fusion. <em>TMM</em>, <em>24</em>, 1460–1472. (<a
href="https://doi.org/10.1109/TMM.2021.3065496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A single infrared image or visible image for the same scene is usually insufficient to simultaneously reveal the infrared objects and the scene details. Thus, image fusion techniques play an important role in producing a single image from the images captured by infrared and visible sensors. In this paper, we propose a novel total variation (TV)-based fusion for infrared and visible images. In our model, a weighted fidelity term is employed to fuse both the infrared objects in the infrared image and the salient scenes in the visible image. To this end, a weight estimation method is developed based on the global luminance contrast-based saliency. Also, to overcome the over-fitting, two constraints are further introduced to merge more details from the visible image and prevent the luminance degradation for the fused result, respectively. Moreover, joint norms are exploited to produce a better result. ${{\boldsymbol{l}}_{2,1,{\boldsymbol{rc}}}}$ provides the structural group sparseness for the fidelity term, whereas ${{\boldsymbol{l}}_{1/2}}$ presents the better gradient sparse for the detail preserving term and ${{\boldsymbol{l}}_2}$ is utilized for the luminance degradation preventing term. Experimental results indicate that the proposed method can give state-of-the-art performances both in visual perception and quantitative scores than other methods.},
  archive      = {J_TMM},
  author       = {Rencan Nie and Chaozhen Ma and Jinde Cao and Hongwei Ding and Dongming Zhou},
  doi          = {10.1109/TMM.2021.3065496},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1460-1472},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A total variation with joint norms for infrared and visible image fusion},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-modal meta multi-task learning for social media rumor
detection. <em>TMM</em>, <em>24</em>, 1449–1459. (<a
href="https://doi.org/10.1109/TMM.2021.3065498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of social media platforms and the increasing scale of the social media data, the rumor detection task has become vitally important since the authenticity of posts cannot be guaranteed. To date, Many approaches have been proposed to facilitate the rumor detection process by utilizing the multi-task learning mechanism, which aims to improve the performance of rumor detection task by leveraging the useful information in the stance detection task. However, most of the existing approaches suffer from three limitations: (1) only focus on the textual content and ignore the multi-modal information which is key component contained in social media data; (2) ignore the difference of feature space between the stance detection task and rumor detection task, resulting in the unsatisfactory usage of stance information; (3) largely neglect the semantic information hidden in the fine-grained stance labels. Therefore, in this paper, we design a Multi-modal Meta Multi-Task Learning (MM-MTL) framework for social media rumor detection. To make use of multiple modalities, we design a multi-modal post embedding layer which considers both textual and visual content. To overcome the feature-sharing problem of the stance detection task and rumor detection task, we propose a meta knowledge-sharing scheme to share some higher meta network-layers and capture the meta knowledge behind the multi-modal post. To better utilize the semantic information hidden in the fine-grained stance labels, we employ the attention mechanism to estimate the weight of each reply. Extensive experiments on two Twitter benchmark datasets demonstrate that our proposed method achieves state-of-the-art performance.},
  archive      = {J_TMM},
  author       = {Huaiwen Zhang and Shengsheng Qian and Quan Fang and Changsheng Xu},
  doi          = {10.1109/TMM.2021.3065498},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1449-1459},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-modal meta multi-task learning for social media rumor detection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised image-to-image translation via pre-trained
StyleGAN2 network. <em>TMM</em>, <em>24</em>, 1435–1448. (<a
href="https://doi.org/10.1109/TMM.2021.3065230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-to-Image (I2I) translation is an emerging topic in academia, and it also has been applied in real-world industry for tasks like image synthesis, super-resolution, and colorization. Traditional I2I translation methods usually train data in two or more domains together. This requires lots of computation resources. The results are of lower quality, and contain more artifacts. The training process could be unstable when the data in different domains are not balanced, and modal collapse is more likely to happen. In this paper, we propose a new I2I translation method that generates a new model in the target domain via a series of model transformations on a pre-trained StyleGAN2 model in the source domain. After that, we develop an inversion method to achieve the conversion between an image and its latent vector. By feeding the latent vector into the generated model, we can perform I2I translation between the source domain and target domain. Both qualitative and quantitative evaluations were conducted to verify that the proposed method can achieve better performance in terms of image quality, diversity and semantic similarity to the input and reference images compared to state-of-the-art works.},
  archive      = {J_TMM},
  author       = {Jialu Huang and Jing Liao and Sam Kwong},
  doi          = {10.1109/TMM.2021.3065230},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1435-1448},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Unsupervised image-to-image translation via pre-trained StyleGAN2 network},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Show, price and negotiate: A negotiator with online value
look-ahead. <em>TMM</em>, <em>24</em>, 1426–1434. (<a
href="https://doi.org/10.1109/TMM.2021.3065169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Negotiation, as an essential and complicated aspect of online shopping, is still challenging for an intelligent agent. To that end, we propose the Price Negotiator , a modular deep neural network that addresses the unsolved problems in recent studies by (1) considering images of the items as a crucial, though neglected, source of information in a negotiation, (2) heuristically finding the most similar items from an external online source to predict the potential value and an acceptable agreement price, (3) predicting a general price-based “action” at each turn which is fed into the language generator to output the supporting natural language, and (4) adjusting the prices based on the predicted actions. Empirically, we show that our model, that is trained in both supervised and reinforcement learning setting, significantly improves negotiation on the CraigslistBargain dataset, in terms of the agreement price, price consistency, and dialogue quality.},
  archive      = {J_TMM},
  author       = {Amin Parvaneh and Ehsan Abbasnejad and Qi Wu and Javen Qinfeng Shi and Anton van den Hengel},
  doi          = {10.1109/TMM.2021.3065169},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1426-1434},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Show, price and negotiate: A negotiator with online value look-ahead},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised image and text fusion for travel information
enhancement. <em>TMM</em>, <em>24</em>, 1415–1425. (<a
href="https://doi.org/10.1109/TMM.2021.3064408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of the shared information on social media platforms, people are increasingly interested in sharing and making their travel plans by referring to others’ travel experiences. However, different social media sources render the heterogeneity of these valuable data, bringing difficulties for data collection and fusion. Thus, facing massive information online, one of the biggest challenges to enhance travel information is how to integrate and match these multi-source data without clear labels. In this paper, we propose an unsupervised method to fuse and match images and travelogues. We first use the three textual components (title, tag, and description) of the descriptive texts of images as three criteria to embed travelogues and the descriptive texts of images, and further introduce images into our method by joint embedding texts and images. Finally, a multiple kernel clustering approach is adopted for matching travelogues and images. Extensive experiments conducted on the real dataset crawled from two websites (Flickr and TripAdvisor) demonstrate the effectiveness and robustness of our proposed method.},
  archive      = {J_TMM},
  author       = {Lu Zhang and Jingsong Xu and Yongshun Gong and Litao Yu and Jian Zhang and Jialie Shen},
  doi          = {10.1109/TMM.2021.3064408},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1415-1425},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Unsupervised image and text fusion for travel information enhancement},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multilevel anomaly detection through variational
autoencoders and bayesian models for self-aware embodied agents.
<em>TMM</em>, <em>24</em>, 1399–1414. (<a
href="https://doi.org/10.1109/TMM.2021.3065232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection constitutes a fundamental step in developing self-aware autonomous agents capable of continuously learning from new situations, as it enables to distinguish novel experiences from already encountered ones. This paper combines Dynamic Bayesian Networks (DBNs) and Neural Networks (NNs) and proposes a method for detecting anomalies in video data at different abstraction levels. We use a Variational Autoencoder (VAE) to reduce the dimensionality of video frames, and Optical Flows between subsequent images, generating a latent space that captures both visual and dynamical information and that is comparable to low-dimensional sensory data (e.g., positioning, steering angle). An Adapted Markov Jump Particle Filter is employed to predict the following frames and detect anomalies in video data. Our method’s evaluation is executed using different video data from a semi-autonomous vehicle performing different tasks in a closed environment. Tests on benchmark anomaly detection datasets have additionally been conducted.},
  archive      = {J_TMM},
  author       = {Giulia Slavic and Mohamad Baydoun and Damian Campo and Lucio Marcenaro and Carlo Regazzoni},
  doi          = {10.1109/TMM.2021.3065232},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1399-1414},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multilevel anomaly detection through variational autoencoders and bayesian models for self-aware embodied agents},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blind stereoscopic image quality evaluator based on
binocular semantic and quality channels. <em>TMM</em>, <em>24</em>,
1389–1398. (<a href="https://doi.org/10.1109/TMM.2021.3064240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human beings always evaluate the perceptual quality of an image coupled with identifying the semantic content of images. This paper addresses the correlation issue between stereoscopic image quality assessment (SIQA) and semantic recognition. In contrast to the previous SIQA methods that relied on binocular quality-aware features of a stereoscopic image, our approach also extracts binocular semantic features using a pre-trained deep convolutional neural network (DCNN) on a large dataset like ImageNet dataset, as well as the manually designed binocular quality-aware features. It can solve the problem of limited SIQA dataset size and facilitate better prediction on the quality. Experimental results demonstrate that the binocular semantic features are a good predictor for the stereoscopic image quality. The proposed method outperforms the state-of-the-art SIQA methods on four benchmark SIQA datasets. Significantly, all Spearman rank-order correlation coefficients (SROCCs) between the predicted scores and the subjective scores on the four datasets exceed 0.95. The MATLAB source code of the proposed method is available at https://github.com/kyohoonsim/Blind-Stereoscopic-Image-Quality-Evaluator-based-on-Binocular-Semantic-and-Quality-Channels https://github.com/kyohoonsim .},
  archive      = {J_TMM},
  author       = {Kyohoon Sim and Jiachen Yang and Wen Lu and Xinbo Gao},
  doi          = {10.1109/TMM.2021.3064240},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1389-1398},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Blind stereoscopic image quality evaluator based on binocular semantic and quality channels},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Disentangled feature networks for facial portrait and
caricature generation. <em>TMM</em>, <em>24</em>, 1378–1388. (<a
href="https://doi.org/10.1109/TMM.2021.3064273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial portrait is an artistic form which draws faces by emphasizing discriminative or prominent parts of faces via various kinds of drawing tools. However, the complex interplay between the different facial factors, such as facial parts, background, and drawing styles, and the significant domain gap between natural facial images and their portrait counterparts makes the task challenging. In this paper, a flexible four-stream Disentangled Feature Networks ( DFN ) is proposed to learn disentangled feature representation of different facial factors and generate plausible portraits with reasonable exaggerations and richness in style. Four factors are encoded as embedding features, and combined to reconstruct facial portraits. Meanwhile, to make the process fully automatic (without manually specifying either portrait style or exaggerating form), we propose a new Adversarial Portrait Mapping Module ( APMM ) to map noise to the embedding feature space, as proxies for portrait style and exaggerating. Thanks to the proposed DFN and APMM , we are able to manipulate the portrait style and facial geometric structures to generate a large number of portraits. Extensive experiments on two public datasets show that our proposed methods can generate a diverse set of artistic portraits.},
  archive      = {J_TMM},
  author       = {Kaihao Zhang and Wenhan Luo and Lin Ma and Wenqi Ren and Hongdong Li},
  doi          = {10.1109/TMM.2021.3064273},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1378-1388},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Disentangled feature networks for facial portrait and caricature generation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). COLA-net: Collaborative attention network for image
restoration. <em>TMM</em>, <em>24</em>, 1366–1377. (<a
href="https://doi.org/10.1109/TMM.2021.3063916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local and non-local attention-based methods have been well studied in various image restoration tasks while leading to promising performance. However, most of the existing methods solely focus on one type of attention mechanism (local or non-local). Furthermore, by exploiting the self-similarity of natural images, existing pixel-wise non-local attention operations tend to give rise to deviations in the process of characterizing long-range dependence due to image degeneration. To overcome these problems, in this paper we propose a novel collaborative attention network (COLA-Net) for image restoration, as the first attempt to combine local and non-local attention mechanisms to restore image content in the areas with complex textures and with highly repetitive details respectively. In addition, an effective and robust patch-wise non-local attention model is developed to capture long-range feature correspondences through 3D patches. Extensive experiments on synthetic image denoising, real image denoising and compression artifact reduction tasks demonstrate that our proposed COLA-Net is able to achieve state-of-the-art performance in both peak signal-to-noise ratio and visual perception, while maintaining an attractive computational complexity. The source code is available on https://github.com/MC-E/COLA-Net .},
  archive      = {J_TMM},
  author       = {Chong Mou and Jian Zhang and Xiaopeng Fan and Hangfan Liu and Ronggang Wang},
  doi          = {10.1109/TMM.2021.3063916},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1366-1377},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {COLA-net: Collaborative attention network for image restoration},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Zwei: A self-play reinforcement learning framework for video
transmission services. <em>TMM</em>, <em>24</em>, 1350–1365. (<a
href="https://doi.org/10.1109/TMM.2021.3063620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video transmission services adopt adaptive algorithms to ensure users’ demands. Existing techniques are often optimized and evaluated by a function that linearly combines several weighted metrics. Nevertheless, we observe that the given function often fails to describe the requirement accurately, resulting in the violation of generating the required methods. We propose Zwei , a self-play reinforcement learning framework that updates the policy by straightforwardly utilizing the actual requirement. Technically, Zwei effectively rolls out the trajectories from the same initial state, and instantly estimate the win rate w.r.t the competition outcome, where the outcome represents which trajectory is closer to the assigned requirement. We evaluate Zwei with different requirements on various video transmission tasks, including adaptive bitrate streaming, crowd-sourced live streaming scheduling, and real-time communication. Results indicate that Zwei optimizes itself according to the assigned requirement faithfully, outperforming the state-of-the-art methods under all considered scenarios. Moreover, we further propose Zwei$^+$ , which enables Zwei to learn the policies in the vanilla no-regret reinforcement learning scenario. We validate Zwei $^+$ in the adaptive bitrate streaming task and show the superiority of the proposed method over existing state-of-the-art approaches.},
  archive      = {J_TMM},
  author       = {Tianchi Huang and Rui-Xiao Zhang and Lifeng Sun},
  doi          = {10.1109/TMM.2021.3063620},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1350-1365},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Zwei: A self-play reinforcement learning framework for video transmission services},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Frame-wise cross-modal matching for video moment retrieval.
<em>TMM</em>, <em>24</em>, 1338–1349. (<a
href="https://doi.org/10.1109/TMM.2021.3063631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video moment retrieval targets at retrieving a golden moment in a video for a given natural language query. The main challenges of this task include 1) the requirement of accurately localizing (i.e., the start time and the end time of) the relevant moment in an untrimmed video stream, and 2) bridging the semantic gap between textual query and video contents. To tackle those problems, early approaches adopt the sliding window or uniform sampling to collect video clips first and then match each clip with the query to identify relevant clips. Obviously, these strategies are time-consuming and often lead to unsatisfied accuracy in localization due to the unpredictable length of the golden moment. To avoid the limitations, researchers recently attempt to directly predict the relevant moment boundaries without the requirement to generate video clips first. One mainstream approach is to generate a multimodal feature vector for the target query and video frames (e.g., concatenation) and then use a regression approach upon the multimodal feature vector for boundary detection. Although some progress has been achieved by this approach, we argue that those methods have not well captured the cross-modal interactions between the query and video frames. In this paper, we propose an Attentive Cross-modal Relevance Matching (ACRM) model which predicts the temporal boundaries based on an interaction modeling between two modalities. In addition, an attention module is introduced to automatically assign higher weights to query words with richer semantic cues, which are considered to be more important for finding relevant video contents. Another contribution is that we propose an additional predictor to utilize the internal frames in the model training to improve the localization accuracy. Extensive experiments on two public datasets TACoS and Charades-STA demonstrate the superiority of our method over several state-of-the-art methods. Ablation studies have been also conducted to examine the effectiveness of different modules in our ACRM model.},
  archive      = {J_TMM},
  author       = {Haoyu Tang and Jihua Zhu and Meng Liu and Zan Gao and Zhiyong Cheng},
  doi          = {10.1109/TMM.2021.3063631},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1338-1349},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Frame-wise cross-modal matching for video moment retrieval},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Alleviating domain shift via discriminative learning for
generalized zero-shot learning. <em>TMM</em>, <em>24</em>, 1325–1337.
(<a href="https://doi.org/10.1109/TMM.2021.3063616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In zero-shot learning (ZSL) tasks, especially in generalized zero-shot learning (GZSL), the model tends to classify unseen test samples into seen categories, which is well known as the domain shift problem, because the model is trained from seen samples without unseen samples. Recently, generative adversarial network (GAN) based methods have achieved good performance in GZSL, which replace real unseen features by synthesizing fake ones to mitigate the domain shift. However, the domain shift problem is still not well solved, due to the lacking of unseen samples in the training progress of the GAN generator. In this paper, we propose a generative model named discriminative learning GAN (DL-GAN) to alleviate the domain shift in GZSL. Specifically, the DL-GAN is designed with three novel components: a dual-stream embedding model that aligns features to the ground-truth attributes to extract discriminative latent attributes from features, an attribute-based generative model that generates high-quality unseen features from semantic attributes to guarantee inter-class discriminability and semantic consistency, and a seen/unseen classifier that leverages validation samples to distinguish seen samples from unseen ones. Experimental results on four widely used datasets verify that our proposed approach significantly outperforms the state-of-the-art methods under the GZSL protocol.},
  archive      = {J_TMM},
  author       = {Yalan Ye and Yukun He and Tongjie Pan and Jingjing Li and Heng Tao Shen},
  doi          = {10.1109/TMM.2021.3063616},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1325-1337},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Alleviating domain shift via discriminative learning for generalized zero-shot learning},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep auto-encoders with sequential learning for multimodal
dimensional emotion recognition. <em>TMM</em>, <em>24</em>, 1313–1324.
(<a href="https://doi.org/10.1109/TMM.2021.3063612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal dimensional emotion recognition has drawn a great attention from the affective computing community and numerous schemes have been extensively investigated, making a significant progress in this area. However, several questions still remain unanswered for most of existing approaches including: (i) how to simultaneously learn compact yet representative features from multimodal data, (ii) how to effectively capture complementary features from multimodal streams, and (iii) how to perform all the tasks in an end-to-end manner. To address these challenges, in this paper, we propose a novel deep neural network architecture consisting of a two-stream auto-encoder and a long short term memory for effectively integrating visual and audio signal streams for emotion recognition. To validate the robustness of our proposed architecture, we carry out extensive experiments on the multimodal emotion in the wild dataset: RECOLA. Experimental results show that the proposed method achieves state-of-the-art recognition performance.},
  archive      = {J_TMM},
  author       = {Dung Nguyen and Duc Thanh Nguyen and Rui Zeng and Thanh Thi Nguyen and Son N. Tran and Thin Nguyen and Sridha Sridharan and Clinton Fookes},
  doi          = {10.1109/TMM.2021.3063612},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1313-1324},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep auto-encoders with sequential learning for multimodal dimensional emotion recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Structure-guided arbitrary style transfer for artistic image
and video. <em>TMM</em>, <em>24</em>, 1299–1312. (<a
href="https://doi.org/10.1109/TMM.2021.3063605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, neural style transfer has become a popular task in both academic research and industrial applications. Although the existing methods made great progress in terms of quality and efficiency, most of them mainly focus on extracting high-level features. Therefore, it is still challenging to display the hierarchical structure of the content image due to lack of texture information, which causes blurred boundaries and distortion of the stylized image. In this paper, a novel neural image and video style transfer scheme is proposed to suppress distortion and preserve the semantic content of the content image, which is capable of yielding satisfactory stylized images and videos of a variety of scenarios. We first propose to assemble a refine network into an auto-encoder framework to guide style transfer, which can ensure that the stylized image have diverse levels of details. Then, we introduce the global content loss and the local region structure loss to train the model and enhance the robustness of the model. In addition, in order to produce a high-quality stylized video, our method not only preserves the image structure, but also introduces a temporal consistency loss and a cycle-temporal loss to avoid temporal incoherence and motion blur as far as possible. Our approach is also friendly for photographic and exposed image and video style transfer. Both quantitative and qualitative evaluation demonstrated the effectiveness of our method.},
  archive      = {J_TMM},
  author       = {Shiguang Liu and Ting Zhu},
  doi          = {10.1109/TMM.2021.3063605},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1299-1312},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Structure-guided arbitrary style transfer for artistic image and video},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High capacity reversible data hiding in encrypted image
based on adaptive MSB prediction. <em>TMM</em>, <em>24</em>, 1288–1298.
(<a href="https://doi.org/10.1109/TMM.2021.3062699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible data hiding in encrypted image (RDHEI) is a technique that can be adopted by cloud sever to embed additional data into the encrypted image with no permanent distortion. For RDHEI, it remains a challenging task to improve the embedding capacity under the premise of real reversibility. In this paper, a novel RDHEI method based on adaptive most significant bit (MSB) prediction is proposed. The cover image is first encrypted in block-wise manner such that the correlation of pixels within block is preserved. Next, all blocks are permuted to fulfill the final encryption. During data embedding, the upper-left pixel within block is used to predict others such that the embedding room is vacated. Then, available blocks are selected and all blocks are rearranged so as to ensure reversibility. By fully exploiting the correlation of pixels within block via adaptive MSB prediction, the proposed method successes to achieve desirable improvement in capacity. Experimental results show that the proposed method significantly outperforms previous methods. Moreover, real reversibility and real separability are also guaranteed. In a word, the proposed method is a practical method that can be adopted by cloud storage.},
  archive      = {J_TMM},
  author       = {Yaomin Wang and Wenguang He},
  doi          = {10.1109/TMM.2021.3062699},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1288-1298},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {High capacity reversible data hiding in encrypted image based on adaptive MSB prediction},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tripartite graph regularized latent low-rank representation
for fashion compatibility prediction. <em>TMM</em>, <em>24</em>,
1277–1287. (<a href="https://doi.org/10.1109/TMM.2021.3062736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, an increasing online shopping demand has greatly promoted the innovation and development of the fashion industry. Visual fashion analysis has become a prospective research topic in computer vision and multimedia fields. Among these studies, fashion compatibility analysis is required in many real applications, such as fashion recommendation, matching, and retrieval. However, learning fashion compatibility is nontrivial, not only due to the uncertain and sparse dependencies among fashion items but also the latent and mutual associations among multiple factors such as color, texture, style, and functionality. To better predict fashion compatibility, in this paper, we proposed a tripartite graph regularized latent low-rank representation method, named TGRLLR, for fashion compatibility prediction. In TGRLLR, to learn more low-dimensional and effective representations, we considered the latent low-rank representation by decomposing the original feature matrix in both the column and row directions to tackle the problem of insufficient observations. On this basis, we simultaneously exploited different regularization strategies to encode the structured correlations among features, the high-order relationships among items, and the geometrical structures of outfits for more informative representations. Extensive experiments conducted on a real-world dataset demonstrate the effectiveness of our proposed method compared with state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Peiguang Jing and Jing Zhang and Liqiang Nie and Shu Ye and Jing Liu and Yuting Su},
  doi          = {10.1109/TMM.2021.3062736},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1277-1287},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Tripartite graph regularized latent low-rank representation for fashion compatibility prediction},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to simulate complex scenes for street scene
segmentation. <em>TMM</em>, <em>24</em>, 1253–1265. (<a
href="https://doi.org/10.1109/TMM.2021.3062497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data simulation engines like Unity are becoming an increasingly important data source that allows us to acquire ground truth labels conveniently. Moreover, we can flexibly edit the content of an image in the engine, such as objects (position, orientation) and environments (illumination, occlusion). When using simulated data as training sets, its editable content can be leveraged to mimick the distribution of real-world data, and thus reduce the content difference between the synthetic and real domains. This paper explores content adaptation in the context of semantic segmentation, where the complex street scenes are fully synthesized using 19 classes of virtual objects from a first person driver perspective and controlled by 23 attributes. To optimize the attribute values and obtain a training set of similar content to real-world data, we propose a scalable discretization-and-relaxation (SDR) approach. Under a reinforcement learning framework, we formulate attribute optimization as a random-to-optimized mapping problem using a neural network. Our method has three characteristics. 1) Instead of editing attributes of individual objects, we focus on global attributes that have large influence on the scene structure, such as object density and illumination. 2) Attributes are quantized to discrete values, so as to reduce search space and training complexity. 3) Correlated attributes are jointly optimized in a group, so as to avoid meaningless scene structures and find better convergence points. Experiment shows our system can generate reasonable and useful scenes, from which we obtain promising real-world segmentation accuracy compared with existing synthetic training sets.},
  archive      = {J_TMM},
  author       = {Zhenfeng Xue and Weijie Mao and Liang Zheng},
  doi          = {10.1109/TMM.2021.3062497},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1253-1265},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning to simulate complex scenes for street scene segmentation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AdaCrowd: Unlabeled scene adaptation for crowd counting.
<em>TMM</em>, <em>24</em>, 1008–1019. (<a
href="https://doi.org/10.1109/TMM.2021.3062481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of image-based crowd counting. In particular, we propose a new problem called unlabeled scene-adaptive crowd counting . Given a new target scene, we would like to have a crowd counting model specifically adapted to this particular scene based on the target data that capture some information about the new scene. In this paper, we propose to use one or more unlabeled images from the target scene to perform the adaptation. In comparison with the existing problem setups (e.g. fully supervised), our proposed problem setup is closer to the real-world applications of crowd counting systems. We introduce a novel AdaCrowd framework to solve this problem. Our framework consists of a crowd counting network and a guiding network. The guiding network predicts some parameters in the crowd counting network based on the unlabeled images from a particular scene. This allows our model to adapt to different target scenes. The experimental results on several challenging benchmark datasets demonstrate the effectiveness of our proposed approach compared with other alternative methods. Code is available at https://github.com/maheshkkumar/adacrowd},
  archive      = {J_TMM},
  author       = {Mahesh Kumar Krishna Reddy and Mrigank Rochan and Yiwei Lu and Yang Wang},
  doi          = {10.1109/TMM.2021.3062481},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1008-1019},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AdaCrowd: Unlabeled scene adaptation for crowd counting},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A CRF-based framework for tracklet inactivation in online
multi-object tracking. <em>TMM</em>, <em>24</em>, 995–1007. (<a
href="https://doi.org/10.1109/TMM.2021.3062489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online multi-object tracking (MOT) is an active research topic in the domain of computer vision. Although many previously proposed algorithms have exhibited decent results, the issue of tracklet inactivation has not been sufficiently studied. Simple strategies such as using a fixed threshold on classification scores are adopted, yielding undesirable tracking mistakes and limiting the overall performance. In this paper, a conditional random field (CRF) based framework is put forward to tackle the tracklet inactivation issue in online MOT problems. A discrete CRF which exploits the intra-frame relationship between tracking hypotheses is developed to improve the robustness of tracklet inactivation. Separate sets of feature functions are designed for the unary and binary terms in the CRF, which take into account various tracking challenges in practical scenarios. To handle the problem of varying CRF nodes in the MOT context, two strategies named as hypothesis filtering and dummy nodes are employed. In the proposed framework, the inference stage is conducted by using the loopy belief propagation algorithm, and the CRF parameters are determined by utilizing the maximum likelihood estimation method followed by slight manual adjustment. Experimental results show that the tracker combined with the CRF-based framework outperforms the baseline on the MOT16 and MOT17 benchmarks. The extensibility of the proposed framework is further validated by an extensive experiment.},
  archive      = {J_TMM},
  author       = {Tianze Gao and Huihui Pan and Zidong Wang and Huijun Gao},
  doi          = {10.1109/TMM.2021.3062489},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {995-1007},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A CRF-based framework for tracklet inactivation in online multi-object tracking},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Audio matters in video super-resolution by implicit semantic
guidance. <em>TMM</em>, <em>24</em>, 4128–4142. (<a
href="https://doi.org/10.1109/TMM.2022.3152941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video super-resolution (VSR) aims to use multiple consecutive low-resolution frames to recover the corresponding high-resolution frames. However, existing VSR methods only consider videos as image sequences, ignoring another essential timing information-audio, while in fact, there is a semantic link between audio and vision, and extensive studies have shown that audio can provide supervisory information in visual networks. Meanwhile, the addition of semantic priors has been proven to be effective in super-resolution (SR) tasks, but a pretrained segmentation network is required to obtain semantic segmentation maps. By contrast, audio as the information contained in the video itself can be directly used. Therefore, in this study, we propose a novel and pluggable multiscale audiovisual fusion (MS-AVF) module to enhance VSR performance by exploiting the relevant audio information, which can be regarded as implicit semantic guidance compared with the kind of explicit segmentation priors. Specifically, we first fuse audiovisual features on the semantic feature maps of different granularities of the target frames, and then through a top-down multiscale fusion approach, feedback high-level semantics to the underlying global visual features layer by layer, thereby providing effective audio implicit semantic guidance for VSR. Experimental results show that audio can further improve the VSR effect. Moreover, by visualizing the learned attention mask, the proposed end-to-end model can automatically learn potential audiovisual semantic links, especially improving the accuracy and effectiveness of the SR of sound sources and their surrounding regions.},
  archive      = {J_TMM},
  author       = {Yanxiang Chen and Pengcheng Zhao and Meibin Qi and Yang Zhao and Wei Jia and Ronggang Wang},
  doi          = {10.1109/TMM.2022.3152941},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {4128-4142},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Audio matters in video super-resolution by implicit semantic guidance},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Align r-CNN: A pairwise head network for visual relationship
detection. <em>TMM</em>, <em>24</em>, 1266–1276. (<a
href="https://doi.org/10.1109/TMM.2021.3062543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene graphs connect individual objects with visual relationships. They serve as a comprehensive scene representation for downstream multimodal tasks. However, by exploring recent progress in Scene Graph Generation (SGG), we find that the performance of recent works is highly limited by the pairwise relationship modeling by naive feature concatenation. Such pairwise features lack sufficient object interaction due to the mis-aligned object parts, resulting in non-discriminative pairwise features for visual relationship prediction. For example, naive concatenated pairwise feature usually make the model fail to discriminate between riding and feeding for object pair person and horse . To this end, we design a meta-architecture— learning-to-align — for dynamic object feature concatenation. We call our model: Align R-CNN . Specifically, we introduce a novel attention-based multiple region alignment module that can be jointly optimized with SGG. Experiments on the large-scale SGG benchmark Visual Genome show that the proposed Align R-CNN can replace the naive feature concatenation and thus boost all the existing SGG methods.},
  archive      = {J_TMM},
  author       = {Mitra Tajrobehkar and Kaihua Tang and Hanwang Zhang and Joo-Hwee Lim},
  doi          = {10.1109/TMM.2021.3062543},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1266-1276},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Align R-CNN: A pairwise head network for visual relationship detection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online residual quantization via streaming data correlation
preserving. <em>TMM</em>, <em>24</em>, 981–994. (<a
href="https://doi.org/10.1109/TMM.2021.3062480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the online retrieval task has been receiving widespread attention, which is closely related to many real-world applications. However, existing online retrieval methods based on hashing suffer from two main problems: a) the models tend to be biased towards the current streaming data due to unavailable history streaming data; b) when new streaming data comes in and the hashing functions have been updated, all history binary codes should be recomputed, which takes much computation burden. To address the above two issues, we propose a novel Online Residual Quantization (ORQ) method that can achieve efficient streaming data quantization via the small-scale residual quantization codebooks. For the first problem, we design a residual quantization module by learning multiple residual codebooks to quantize the float streaming data, which effectively reduces the quantization error and enables the binary codes to be easily reconstructed back to original float data. Then, with the reconstructed history data, a balanced affinity matrix is developed to model the semantic relationship, e.g., similarity and difference, between the history and current data distributions, which can prevent the model from being biased towards the current data distribution. For the second problem, when inputting current streaming data, only the residual codebooks should be updated, instead of the whole history binary codes in hashing-based methods, which significantly reduces the computation burden. Comprehensive experiments on six benchmarks demonstrate that ORQ yields significant improvements ( i.e., 1.2% $\sim$ 4.9% in average mAP) compared to the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Pandeng Li and Hongtao Xie and Shaobo Min and Zheng-Jun Zha and Yongdong Zhang},
  doi          = {10.1109/TMM.2021.3062480},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {981-994},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Online residual quantization via streaming data correlation preserving},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantically meaningful class prototype learning for
one-shot image segmentation. <em>TMM</em>, <em>24</em>, 968–980. (<a
href="https://doi.org/10.1109/TMM.2021.3061816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-shot semantic image segmentation aims to segment the object regions for the novel class with only one annotated image. Recent works adopt the episodic training strategy to mimic the expected situation at testing time. However, these existing approaches simulate the test conditions too strictly during the training process, and thus cannot make full use of the given label information. Besides, these approaches mainly focus on the foreground-background target class segmentation setting. They only utilize binary mask labels for training. In this paper, we propose to leverage the multi-class label information during the episodic training. It will encourage the network to generate more semantically meaningful features for each category. After integrating the target class cues into the query features, we then propose a pyramid feature fusion module to mine the fused features for the final classifier. Furthermore, to take more advantage of the support image-mask pair, we propose a self-prototype guidance branch to support image segmentation. It can constrain the network for generating more compact features and a robust prototype for each semantic class. For inference, we propose a fused prototype guidance branch for the segmentation of the query image. Specifically, we leverage the prediction of the query image to extract the pseudo-prototype and combine it with the initial prototype. Then we utilize the fused prototype to guide the final segmentation of the query image. Extensive experiments demonstrate the superiority of our proposed approach. The source codes and models have been made available at https://github.com/NUST-Machine-Intelligence-Laboratory/SMCP .},
  archive      = {J_TMM},
  author       = {Tao Chen and Guo-Sen Xie and Yazhou Yao and Qiong Wang and Fumin Shen and Zhenmin Tang and Jian Zhang},
  doi          = {10.1109/TMM.2021.3061816},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {968-980},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semantically meaningful class prototype learning for one-shot image segmentation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TERA: Screen-to-camera image code with transparency,
efficiency, robustness and adaptability. <em>TMM</em>, <em>24</em>,
955–967. (<a href="https://doi.org/10.1109/TMM.2021.3061801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of digital devices, the issue of how to transmit information among different devices with multimedia carriers has drawn much attention from the research community. This paper focuses on the important user scenario of “screen-to-camera information transmission”. Along this direction, image coding-based techniques have been shown to be the most popular and effective methods in the past decades. However, after careful study, we find that none of the existing methods can satisfy the four important properties simultaneously, i.e., high transparency , high embedding efficiency , strong transmission robustness and high adaptability to device types . This is mainly because these properties are contradictory with each other. In this paper, we thus propose a screen-to-camera image code dubbed “TERA” ( t ransparency, e fficiency, r obustness and a daptability), which makes it possible to circumvent the contradiction among the above four properties for the first time. Generally, TERA adopts the color decomposition principle to ensure the visual quality and the superposition-based scheme to ensure embedding efficiency. BCH-coding-based information arrangement and a powerful attention-guided information decoding network are further designed to guarantee the robustness and adaptability. Through extensive experiments, the superiority and broad applications of our method are demonstrated.},
  archive      = {J_TMM},
  author       = {Han Fang and Dongdong Chen and Feng Wang and Zehua Ma and Honggu Liu and Wenbo Zhou and Weiming Zhang and Nenghai Yu},
  doi          = {10.1109/TMM.2021.3061801},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {955-967},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TERA: Screen-to-camera image code with transparency, efficiency, robustness and adaptability},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Audio-visual tracking of concurrent speakers. <em>TMM</em>,
<em>24</em>, 942–954. (<a
href="https://doi.org/10.1109/TMM.2021.3061800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-visual tracking of an unknown number of concurrent speakers in 3D is a challenging task, especially when sound and video are collected with a compact sensing platform. In this paper, we propose a tracker that builds on generative and discriminative audio-visual likelihood models formulated in a particle filtering framework. We localize multiple concurrent speakers with a de-emphasized acoustic map assisted by the image detection-derived 3D video observations. The 3D multi-modal observations are either assigned to existing tracks for discriminative likelihood computation or used to initialize new tracks. The generative likelihoods rely on color distribution of the target and the de-emphasized acoustic map value. Experiments on AV16.3 and CAV3D datasets show that the proposed tracker outperforms the uni-modal trackers and the state-of-the-art approaches both in 3D and on the image plane.},
  archive      = {J_TMM},
  author       = {Xinyuan Qian and Alessio Brutti and Oswald Lanz and Maurizio Omologo and Andrea Cavallaro},
  doi          = {10.1109/TMM.2021.3061800},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {942-954},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Audio-visual tracking of concurrent speakers},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collaborative learning with a multi-branch framework for
feature enhancement. <em>TMM</em>, <em>24</em>, 929–941. (<a
href="https://doi.org/10.1109/TMM.2021.3061810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature representation is highly important for many computer vision tasks. A broad range of prior studies have been proposed to strengthen representation ability of architectures via built-in blocks. However, during the forward propagation, the reduction in feature map scales still leads to the lack of representation ability. In this paper, we focus on boosting the representational power of a convolutional network by the multi-branch framework that we term the BranchNet. Each branch is directly supervised by label information to enrich the hierarchy features in BranchNet. Based on this framework, we further propose a collaborative learning loss and a soft target loss to transfer knowledge from deeper layers to shallow layers. BranchNet is an efficient training framework without extra parameters introduced in inference and can be integrated in existing networks, e.g., VGG, ResNet, and DenseNet. We evaluate BranchNet on all of these models and find that our method outperforms the baseline models on the widely-used CIFAR and ImageNet datasets. In particular, on the CIFAR-100 dataset, the classification error of ResNet-164 with BranchNet decreases by 4.51 percent. We also conduct experiments on the representative computer vision tasks of instance segmentation and class activation mapping, further verifying the superiority of BranchNet over the baseline models. Models and code are available at https://github.com/zyyupup/BranchNet/ .},
  archive      = {J_TMM},
  author       = {Xiao Luan and Yuanyuan Zhao and Weihua Ou and Linghui Liu and Weisheng Li and Yucheng Shu and Hongmin Geng},
  doi          = {10.1109/TMM.2021.3061810},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {929-941},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Collaborative learning with a multi-branch framework for feature enhancement},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fine-grained categorization from RGB-d images. <em>TMM</em>,
<em>24</em>, 917–928. (<a
href="https://doi.org/10.1109/TMM.2021.3061284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of computer vision, fine-grained visual categorization has attracted a lot of attention and made great progress due to convolutional neural networks and a large number of publicly available datasets. With next-generation sensing technology, RGB-D cameras can provide high-quality synchronized RGB and depth images for solving many computer vision problems. Although RGB-D cameras have been used in the context of multi-view object category detection and scene understanding, they have not been widely used in fine-grained classification. In this paper, we introduce a multiview RGB-D dataset RGBD-FG for fine-grained categorization. Currently, the dataset contains 93 051 RGB-D images covering 19 super-categories and 50 sub-categories of common vegetables and fruit, and is organized in a hierarchical manner. We provide extensive experimental results to establish state-of-the-art benchmarks for our dataset, illustrating its diversity and scope for improvement through future work. We also propose a novel modality-specific multimodal network called FS-Multimodal network, which can solve two limitations of multimodal networks trained based on fine-tuning techniques: over-fitting and lack of effective depth-specific features. We hope that our study lays the foundations for fine-grained categorization of RGB-D data.},
  archive      = {J_TMM},
  author       = {Yanhao Tan and Mohammad Muntasir Rahman and Yanfu Yan and Jian Xue and Ling Shao and Ke Lu},
  doi          = {10.1109/TMM.2021.3061284},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {917-928},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fine-grained categorization from RGB-D images},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unpaired image captioning with semantic-constrained
self-learning. <em>TMM</em>, <em>24</em>, 904–916. (<a
href="https://doi.org/10.1109/TMM.2021.3060948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning has been an emerging and fast-developing research topic. Nevertheless, most existing works heavily rely on large amounts of image-sentence pairs and therefore hinder the practical applications of captioning in the wild. In this paper, we present a novel Semantic-Constrained Self-learning (SCS) framework that explores an iterative self-learning strategy to learn an image captioner with only unpaired image and text data. Technically, SCS consists of two stages, i.e., pseudo pair generation and captioner re-training, iteratively producing &quot;pseudo&quot; image-sentence pairs via a pre-trained captioner and re-training the captioner with the pseudo pairs, respectively. Particularly, both stages are guided by the recognized objects in the image, that act as semantic constraint to strengthen the semantic alignment between the input image and the output sentence. We leverage a semantic-constrained beam search for pseudo pair generation to regularize the decoding process with the recognized objects via forcing the inclusion/exclusion of the recognized/irrelevant objects in output sentence. For captioner re-training, a self-supervised triplet loss is utilized to preserve the relative semantic similarity ordering among generated sentences with regard to the input image triplets. Moreover, an object inclusion reward and an adversarial reward are adopted to encourage the inclusion of the predicted objects in the output sentence and pursue the generation of more realistic sentences during self-critical training, respectively. Experiments conducted on both dependent and independent unpaired data validate the superiority of SCS. More remarkably, we obtain the best published CIDEr score to-date of 74.7\% on COCO Karpathy test split for unpaired image captioning.},
  archive      = {J_TMM},
  author       = {Huixia Ben and Yingwei Pan and Yehao Li and Ting Yao and Richang Hong and Meng Wang and Tao Mei},
  doi          = {10.1109/TMM.2021.3060948},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {904-916},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Unpaired image captioning with semantic-constrained self-learning},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GPS2Vec: Pre-trained semantic embeddings for worldwide GPS
coordinates. <em>TMM</em>, <em>24</em>, 890–903. (<a
href="https://doi.org/10.1109/TMM.2021.3060951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GPS coordinates are fine-grained location indicators that are difficult to be effectively utilized by classifiers in geo-aware applications. Previous GPS encoding methods concentrate on generating hand-crafted features for small areas of interest. However, many real world applications require a machine learning model, analogous to the pre-trained ImageNet model for images, that can efficiently generate semantically-enriched features for planet-scale GPS coordinates. To address this issue, we propose a novel two-level grid-based framework, termed GPS2Vec, which is able to extract geo-aware features in real-time for locations worldwide. The Earth’s surface is first discretized by the Universal Transverse Mercator (UTM) coordinate system. Each UTM zone is then considered as a local area of interest that is further divided into fine-grained cells to perform the initial GPS encoding. We train a neural network in each UTM zone to learn the semantic embeddings from the initial GPS encoding. The training labels can be automatically derived from large-scale geotagged documents such as tweets, check-ins, and images that are available from social sharing platforms. We conducted comprehensive experiments on three geo-aware applications, namely place semantic annotation, geotagged image classification, and next location prediction. Experimental results demonstrate the effectiveness of our approach, as prediction accuracy improves significantly based on a simple multi-feature early fusion strategy with deep neural networks, including both CNNs and RNNs.},
  archive      = {J_TMM},
  author       = {Yifang Yin and Ying Zhang and Zhenguang Liu and Sheng Wang and Rajiv Ratn Shah and Roger Zimmermann},
  doi          = {10.1109/TMM.2021.3060951},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {890-903},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {GPS2Vec: Pre-trained semantic embeddings for worldwide GPS coordinates},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). V-SVR+: Support vector regression with variational
privileged information. <em>TMM</em>, <em>24</em>, 876–889. (<a
href="https://doi.org/10.1109/TMM.2021.3060955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many regression tasks encounter an asymmetric distribution of information between training and testing phases where the additional information available in training, the so-called privileged information (PI), is often inaccessible in testing. In practice, the privileged information in training data might be expressed in different formats, such as continuous, ordinal, or binary values. However, most the existing learning using privileged information (LUPI) paradigms primarily deal with the continuous form of PI, preventing them from managing variational PI, which motivates this research. Therefore, in this paper, we propose a unified framework to systematically address the aforementioned three forms of privileged information. The proposed V-SVR+ method integrates continuous, ordinal, and binary PI into the learning process of support vector regression (SVR) via three losses. For continuous privileged information, we define a linear correcting (slack) function in the privileged information space to estimate slack variables in the standard SVR method using privileged information. For the ordinal relations of privileged information, we first rank the privileged information and then, regard this ordinal privileged information as auxiliary information used in the learning process of the SVR model. For the binary or Boolean privileged information, we infer a probabilistic dependency between the privileged information and labels from the summarized privileged information knowledge. Then, we transfer the privileged information knowledge to constraints and form a constrained optimization problem. We evaluate the proposed method in three applications: music emotion recognition from songs with the help of implicit information about music elements judged by composers; multiple object recognition from images with the help of implicit information about the object’s importance conveyed by the list of manually annotated image tags; and photo aesthetic assessment enhanced by high-level aesthetic attributes hidden in photos. Experiment results demonstrate that the proposed methods are superior to the classic learning paradigm when solving practical problems.},
  archive      = {J_TMM},
  author       = {Yangyang Shu and Qian Li and Chang Xu and Shaowu Liu and Guandong Xu},
  doi          = {10.1109/TMM.2021.3060955},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {876-889},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {V-SVR+: Support vector regression with variational privileged information},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CrossNet: Detecting objects as crosses. <em>TMM</em>,
<em>24</em>, 861–875. (<a
href="https://doi.org/10.1109/TMM.2021.3060278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the use of deep learning, object detection has achieved great breakthroughs. However, existing object detection methods still can not cope with challenging environments, such as dense objects, small objects, and object scale variations. To address these issues, this paper proposes a novel keypoint-based detection framework, called CrossNet, which significantly improves detection performance with minimal costs. In our approach, an object is modeled as a cross that consists of a center keypoint and a specific size, which eliminates the need of hand-craft anchor design. The proposed CrossNet outputs three types of maps: the center map, size map, and offset map, where both center map and offset map are to predict the center keypoints of objects and the size map is to estimate the sizes (width and height) of objects. Specifically, we first design a cascaded center prediction method that introduces a coarse-to-fine idea to improve center prediction. Furthermore, since center prediction considered as a classification task is easier than size regression relatively, we design a center-attention size regression module that uses the detection results of centers to assist the size prediction. In addition, a slightly modified hourglass network is designed to enhance the quality of feature maps for center and size prediction. Extensive experiments are conducted to demonstrate the effectiveness of CrossNet on the challenging PASCAL VOC, COCO, KITTI, and WiderFace datasets. Empirical studies show that CrossNet achieves competitive results with top-ranked one-stage and two-stage detectors while being time-efficient.},
  archive      = {J_TMM},
  author       = {Jiaxu Leng and Ying Liu and Zhihui Wang and Haibo Hu and Xinbo Gao},
  doi          = {10.1109/TMM.2021.3060278},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {861-875},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CrossNet: Detecting objects as crosses},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Associated spatio-temporal capsule network for gait
recognition. <em>TMM</em>, <em>24</em>, 846–860. (<a
href="https://doi.org/10.1109/TMM.2021.3060280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a challenging task to identify a person based on her/his gait patterns. State-of-the-art approaches rely on the analysis of temporal or spatial characteristics of gait, and gait recognition is usually performed on single modality data (such as images, skeleton joint coordinates, or force signals). Evidence has shown that using multi-modality data is more conducive to gait research. Therefore, we here establish an automated learning system, with an associated spatio-temporal capsule network (ASTCapsNet) trained on multi-sensor datasets, to analyze multimodal information for gait recognition. Specifically, we first design a low-level feature extractor and a high-level feature extractor for spatio-temporal feature extraction of gait with a novel recurrent memory unit and a relationship layer. Subsequently, a Bayesian model is employed for the decision-making of class labels. Extensive experiments on several public datasets (normal and abnormal gait) validate the effectiveness of the proposed ASTCapsNet, compared against several state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Aite Zhao and Junyu Dong and Jianbo Li and Lin Qi and Huiyu Zhou},
  doi          = {10.1109/TMM.2021.3060280},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {846-860},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Associated spatio-temporal capsule network for gait recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-modal semantic matching generative adversarial
networks for text-to-image synthesis. <em>TMM</em>, <em>24</em>,
832–845. (<a href="https://doi.org/10.1109/TMM.2021.3060291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesizing photo-realistic images based on text descriptions is a challenging image generation problem. Although many recent approaches have significantly advanced the performance of text-to-image generation, to guarantee semantic matchings between the text description and synthesized image remains very challenging. In this paper, we propose a new model, Cross-modal Semantic Matching Generative Adversarial Networks (CSM-GAN), to improve the semantic consistency between text description and synthesized image for a fine-grained text-to-image generation. Two new modules are proposed in CSM-GAN: Text Encoder Module (TEM) and Textual-Visual Semantic Matching Module (TVSMM). TVSMM is aimed at making the distance of the pairs of synthesized image and its corresponding text description closer, in global semantic embedding space, than those of mismatched pairs. This improves the semantic consistency and consequently, the generalizability of CSM-GAN. In TEM, we introduce Text Convolutional Neural Networks (Text_CNNs) to capture and highlight local visual features in textual descriptions. Thorough experiments on two public benchmark datasets demonstrated the superiority of CSM-GAN over other representative state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Hongchen Tan and Xiuping Liu and Baocai Yin and Xin Li},
  doi          = {10.1109/TMM.2021.3060291},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {832-845},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross-modal semantic matching generative adversarial networks for text-to-image synthesis},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). <span class="math inline"><em>A</em><sup>3</sup></span>-FKG:
Attentive attribute-aware fashion knowledge graph for outfit preference
prediction. <em>TMM</em>, <em>24</em>, 819–831. (<a
href="https://doi.org/10.1109/TMM.2021.3059514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the booming development of the online fashion industry, effective personalized recommender systems have become indispensable for the convenience they brought to the customers and the profits to the e-commercial platforms. Estimating the user’s preference towards the outfit is at the core of a personalized recommendation system. Existing works on fashion recommendation are largely centering on modelling the clothing compatibility without considering the user factor or characterizing the user’s preference over the single item. However, how to effectively model the outfits with either few or even none interactions, is yet under-explored. In this paper, we address the task of personalized outfit preference prediction via a novel A ttentive A ttribute- A ware F ashion K nowledge G raph ( $A^3$ -FKG), which is incorporated to build the association between different outfits with both outfit- and item- level attributes. Additionally, a two-level attention mechanism is developed to capture the user’s preference: 1) User-specific relation-aware attention layer, which captures the user’s fine-grained preferences with different focus on relations for learning outfit representation; 2) Target-aware attention layer, which characterizes the user’s latent diverse interests from his/her behavior sequences for learning user representation. Extensive experiments conducted on a large-scale fashion outfit dataset demonstrate significant improvements over other methods, which verify the excellence of our proposed framework.},
  archive      = {J_TMM},
  author       = {Huijing Zhan and Jie Lin and Kenan Emir Ak and Boxin Shi and Ling-Yu Duan and Alex C. Kot},
  doi          = {10.1109/TMM.2021.3059514},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {819-831},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {$A^3$-FKG: Attentive attribute-aware fashion knowledge graph for outfit preference prediction},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heterogeneous hierarchical feature aggregation network for
personalized micro-video recommendation. <em>TMM</em>, <em>24</em>,
805–818. (<a href="https://doi.org/10.1109/TMM.2021.3059508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-video recommendation has attracted extensive research attention with the increasing popularity of micro-video sharing platforms. Traditional approaches consider micro-video recommendation as a matching task and ignore the rich relationships among users and micro-videos from various modalities (e.g., visual, acoustic, and textual). Recently, GNN-based approaches show promising performance for the micro-video recommendation task. However, they mainly focus on the homogeneous graph which includes only one type of nodes or relations, and cannot be applied to the heterogeneous graph which consists of users, micro-videos, and related multi-modal information. In this paper, a novel Heterogeneous Hierarchical Feature Aggregation Network (HHFAN) is proposed for personalized micro-video recommendation. Our goal is to explore the highly complicated relationship information among users, micro-videos and related multi-modal information from a modality-aware Heterogeneous Information Graph (M-HIG), and thus generate high-quality user and micro-video embeddings for recommendation. The proposed model consists of two key components: (1) In data structure level, we build a heterogeneous graph and utilize a random walk based sampling strategy to sample neighbors for users and micro-videos. (2) In representation learning level, we design a hierarchical feature aggregation network including the intra- and inter-type feature aggregation networks to better capture the complex structure and rich semantic information in the heterogeneous graph. We evaluate our method on two real-world datasets and the results demonstrate that the proposed model outperforms the baseline methods.},
  archive      = {J_TMM},
  author       = {Desheng Cai and Shengsheng Qian and Quan Fang and Changsheng Xu},
  doi          = {10.1109/TMM.2021.3059508},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {805-818},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Heterogeneous hierarchical feature aggregation network for personalized micro-video recommendation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AgeGAN++: Face aging and rejuvenation with dual conditional
GANs. <em>TMM</em>, <em>24</em>, 791–804. (<a
href="https://doi.org/10.1109/TMM.2021.3059336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face aging and rejuvenation is applied to predict what a person looks like at different ages. While prior work brought about a significant progress in this topic, there are two central problems remaining to be solved : 1) most prior works require sequential data during training, while it is very rare in existing datasets; and 2) how to render an aging face and preserve personality at the same time. To deal with these problems, we develop a novel dual conditional GANs mechanism, thus aging faces can be trained with multiple sets of unlabeled facial images of different ages. Our basic architecture is AgeGAN, in which the primal conditional GAN converts input faces to other ages based on relevant age conditions, and the dual conditional GAN learns to invert the task. We further improve our networks, termed AgeGAN++, in which we share the weights between the primal part and the dual part to to streamline the model. Moreover, in order to get more sensible results, a representation disentanglement component is integrated with the latent facial representation, and an enhanced discriminator is applied on the generated process. In addition, we firstly perform an interpolation experiment to demonstrate that our generators are powerful and effective for face aging and rejuvenation. Experimental results on four public datasets demonstrate the appealing performance of the proposed methods by comparing with the state-of-the-art methods. Our code and a demo are released at https://github.com/Sherry-JQ/AgeGAN .},
  archive      = {J_TMM},
  author       = {Jingkuan Song and Jingqiu Zhang and Lianli Gao and Zhou Zhao and Heng Tao Shen},
  doi          = {10.1109/TMM.2021.3059336},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {791-804},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AgeGAN++: Face aging and rejuvenation with dual conditional GANs},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic emotion modeling with learnable graphs and graph
inception network. <em>TMM</em>, <em>24</em>, 780–790. (<a
href="https://doi.org/10.1109/TMM.2021.3059169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human emotion is expressed, perceived and captured using a variety of dynamic data modalities, such as speech (verbal), videos (facial expressions) and motion sensors (body gestures). We propose a generalized approach to emotion recognition that can adapt across modalities by modeling dynamic data as structured graphs. The motivation behind the graph approach is to build compact models without compromising on performance. To alleviate the problem of optimal graph construction, we cast this as a joint graph learning and classification task. To this end, we present the Learnable Graph Inception Network (L-GrIN) that jointly learns to recognize emotion and to identify the underlying graph structure in the dynamic data. Our architecture comprises multiple novel components: a new graph convolution operation, a graph inception layer, learnable adjacency, and a learnable pooling function that yields a graph-level embedding. We evaluate the proposed architecture on five benchmark emotion recognition databases spanning three different modalities (video, audio, motion capture), where each database captures one of the following emotional cues: facial expressions, speech and body gestures. We achieve state-of-the-art performance on all five databases outperforming several competitive baselines and relevant existing methods. Our graph architecture shows superior performance with significantly fewer parameters (compared to convolutional or recurrent neural networks) promising its applicability to resource-constrained devices. Our code is available at https://github.com/AmirSh15/graph_emotion_recognition .},
  archive      = {J_TMM},
  author       = {Amir Shirian and Subarna Tripathi and Tanaya Guha},
  doi          = {10.1109/TMM.2021.3059169},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {780-790},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dynamic emotion modeling with learnable graphs and graph inception network},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial-temporal multi-cue network for sign language
recognition and translation. <em>TMM</em>, <em>24</em>, 768–779. (<a
href="https://doi.org/10.1109/TMM.2021.3059098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the recent success of deep learning in video-related tasks, deep models typically focus on the most discriminative features, ignoring other potentially non-trivial and informative contents. Such characteristic heavily constrains their capability to learn implicit visual grammars in sign videos behind the collaboration of different visual cues (i.e., hand shape, facial expression and body posture). To this end, we approach video-based sign language understanding with multi-cue learning and propose a spatial-temporal multi-cue (STMC) network to solve the vision-based sequence learning problem. Our STMC network consists of a spatial multi-cue (SMC) module and a temporal multi-cue (TMC) module. The SMC module learns to spatial representation of different cues with a self-contained pose estimation branch. The TMC module models temporal corrections from intra-cue and inter-cue perspectives to explore the collaboration of multiple cues. A joint optimization strategy and a segmented attention mechanism are designed to make the best of multi-cue sources for SL recognition and translation. To validate the effectiveness, we perform experiments on three large-scale sign language benchmarks: PHOENIX-2014, CSL and PHOENIX-2014-T. Experimental results demonstrate that the proposed method achieves new state-of-the-art performance on all three benchmarks.},
  archive      = {J_TMM},
  author       = {Hao Zhou and Wengang Zhou and Yun Zhou and Houqiang Li},
  doi          = {10.1109/TMM.2021.3059098},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {768-779},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spatial-temporal multi-cue network for sign language recognition and translation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep RGB-d saliency detection without depth. <em>TMM</em>,
<em>24</em>, 755–767. (<a
href="https://doi.org/10.1109/TMM.2021.3058788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing saliency detection models based on RGB colors only leverage appearance cues to detect salient objects. Depth information also plays a very important role in visual saliency detection and can supply complementary cues for saliency detection. Although many RGB-D saliency models have been proposed, they require to acquire depth data, which is expensive and not easy to get. In this paper, we propose to estimate depth information from monocular RGB images and leverage the intermediate depth features to enhance the saliency detection performance in a deep neural network framework. Specifically, we first use an encoder network to extract common features from each RGB image and then build two decoder networks for depth estimation and saliency detection, respectively. The depth decoder features can be fused with the RGB saliency features to enhance their capability. Furthermore, we also propose a novel dense multiscale fusion model to densely fuse multiscale depth and RGB features based on the dense ASPP model. A new global context branch is also added to boost the multiscale features. Experimental results demonstrate that the added depth cues and the proposed fusion model can both improve the saliency detection performance. Finally, our model not only outperforms state-of-the-art RGB saliency models, but also achieves comparable results compared with state-of-the-art RGB-D saliency models.},
  archive      = {J_TMM},
  author       = {Yuan-fang Zhang and Jiangbin Zheng and Wenjing Jia and Wenfeng Huang and Long Li and Nian Liu and Fei Li and Xiangjian He},
  doi          = {10.1109/TMM.2021.3058788},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {755-767},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep RGB-D saliency detection without depth},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Probability-based framework to fuse temporal consistency and
semantic information for background segmentation. <em>TMM</em>,
<em>24</em>, 740–754. (<a
href="https://doi.org/10.1109/TMM.2021.3058770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fusion of temporal consistency and semantic information with limited foreground information for background segmentation using deep learning is an underinvestigated problem. In this paper, we explore the relation between temporal consistency and semantic information based on the law of total probability. A highly concise framework is proposed to fuse these two types of information. A theoretical proof is given to show that the proposed framework is more accurate than either the temporal consistency-based model or the semantic information-based model and that each model is a special case of the proposed framework. The proposed framework is a white-box framework that can easily be embedded into a deep neural network as a merging layer. In the proposed model, only a few parameters must be learned, which substantially reduces the need for a large dataset. In addition, these interpretable parameters reflect our understanding of the background and can be applied to a wide range of environments. Extensive evaluations indicate the promising performance of the proposed method. Our code and trained weights for the experiments are available at GitHub. 1 1https://github.com/zengzhi2015/SS_TC_BS (We encourage the reader to run the program for a better understanding of the proposed method).},
  archive      = {J_TMM},
  author       = {Zhi Zeng and Ting Wang and Fulei Ma and Liang Zhang and Peiyi Shen and Syed Afaq Ali Shah and Mohammed Bennamoun},
  doi          = {10.1109/TMM.2021.3058770},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {740-754},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Probability-based framework to fuse temporal consistency and semantic information for background segmentation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unreliable-to-reliable instance translation for
semi-supervised pedestrian detection. <em>TMM</em>, <em>24</em>,
728–739. (<a href="https://doi.org/10.1109/TMM.2021.3058546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating realistic pedestrian instances in a semi-supervised setting is promising but challenging due to the limited labeled data. We propose an unreliable-to-reliable instance translation model (Un2Reliab) conditioned on unreliable instances which poorly align with pedestrians. Un2Reliab mainly consists of an encoder-decoder-like generative network and a discriminative network, which are jointly trained in a minimax game. We adopt regularization to ensure that the synthesized instances are semantically similar to the corresponding ground truth. Furthermore, to preserve the identities of persons, we propose another regularization to ensure that the synthesized instances associated with the same person should be consistent in appearance. As a result, Un2Reliab learns to restore the missing parts of the original instances. As a side benefit, the synthesized instances are brought into better alignment. Inclusion of the synthesized data improves both the diversity and quality of training data, which eventually leads to better generalization performance. Extensive experiments indicate that Un2Reliab is able to synthesize high-fidelity pedestrian instances and improve the previous state-of-the-art results on multiple semi-supervised pedestrian detection benchmarks.},
  archive      = {J_TMM},
  author       = {Sihao Lin and Wenhao Wu and Si Wu and Yong Xu and Hau-San Wong},
  doi          = {10.1109/TMM.2021.3058546},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {728-739},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Unreliable-to-reliable instance translation for semi-supervised pedestrian detection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Emotion expression with fact transfer for video description.
<em>TMM</em>, <em>24</em>, 715–727. (<a
href="https://doi.org/10.1109/TMM.2021.3058555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Translating a video into natural language is a fundamental but challenging task in visual understanding, since there is a great gap between visual content and linguistic sentence. More attention has been paid to this research field and a number of state-of-the-art results are achieved in recent years. However, the emotions in videos are usually overlooked, leading to the generated description sentences being boring and colorless. In this work, we construct a new dataset for video description with emotion expression, which consists of two parts: a re-annotated subset of the MSVD dataset with emotion embedded and another subset annotated with long sentences and rich emotions based on a video emotion recognition dataset. A fact transfer based framework is designed, which incorporates a fact stream and an emotion stream to generate sentences with emotion expression for video description. In addition, we propose a novel approach for sentence evaluation by balancing facts and emotions. A group of experiments are conducted, and the experimental results demonstrate the effectiveness of the proposed methods, including the idea of dataset construction for video description with emotion expression, model training and testing, and the emotion evaluation metric. The project page (including the code and dataset) can be found in https://mic.tongji.edu.cn/ce/70/c9778a183920/page.htm .},
  archive      = {J_TMM},
  author       = {Hanli Wang and Pengjie Tang and Qinyu Li and Meng Cheng},
  doi          = {10.1109/TMM.2021.3058555},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {715-727},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Emotion expression with fact transfer for video description},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-light image restoration with short- and long-exposure
raw pairs. <em>TMM</em>, <em>24</em>, 702–714. (<a
href="https://doi.org/10.1109/TMM.2021.3058586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light imaging with handheld mobile devices is a challenging issue. Limited by the existing models and training data, most existing methods cannot be effectively applied in real scenarios. In this paper, we propose a new low-light image restoration method by using the complementary information of short- and long-exposure images. We first propose a novel data generation method to synthesize realistic short- and long-exposure raw images by simulating the imaging pipeline in low-light environment. Then, we design a new long-short-exposure fusion network (LSFNet) to deal with the problems of low-light image fusion, including high noise, motion blur, color distortion and misalignment. The proposed LSFNet takes pairs of short- and long-exposure raw images as input, and outputs a clear RGB image. Using our data generation method and the proposed LSFNet, we can recover the details and color of the original scene, and improve the low-light image quality effectively. Experiments demonstrate that our method can outperform the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Meng Chang and Huajun Feng and Zhihai Xu and Qi Li},
  doi          = {10.1109/TMM.2021.3058586},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {702-714},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Low-light image restoration with short- and long-exposure raw pairs},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human action recognition by discriminative feature pooling
and video segment attention model. <em>TMM</em>, <em>24</em>, 689–701.
(<a href="https://doi.org/10.1109/TMM.2021.3058050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We Introduce a simple yet effective network that embeds a novel Discriminative Feature Pooling (DFP) mechanism and a novel Video Segment Attention Model (VSAM), for video-based human action recognition from both trimmed and untrimmed videos. Our DFP module introduces an attentional pooling mechanism for 3D Convolutional Neural Networks that attentionally pools 3D convolutional feature maps to emphasize the most critical spatial, temporal, and channel-wise features related to the actions within a video segment, while our VSAM ensembles these most critical features from all video segments and learns (1) class-specific attention weights to classify the video segments into the corresponding action categories, and (2) class-agnostic attention weights to rank the video segments based on their relevance to the action class. Our action recognition network can be trained from both trimmed videos in a fully-supervised way and untrimmed videos in a weakly-supervised way. For untrimmed videos with weak labels, our network learns attention weights without the requirement of precise temporal annotations of action occurrences in videos. Evaluated on the untrimmed video datasets of THUMOS14 and ActivityNet1.2, and trimmed video datasets of HMDB51, UCF101, and HOLLYWOOD2, our network achieves promising performance, compared to the latest state-of-the-art method. The implementation code is available at https://github.com/MoniruzzamanMd/DFP-VSAM-Networks .},
  archive      = {J_TMM},
  author       = {Md Moniruzzaman and Zhaozheng Yin and Zhihai He and Ruwen Qin and Ming C Leu},
  doi          = {10.1109/TMM.2021.3058050},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {689-701},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Human action recognition by discriminative feature pooling and video segment attention model},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). TWGAN: Twin discriminator generative adversarial networks.
<em>TMM</em>, <em>24</em>, 677–688. (<a
href="https://doi.org/10.1109/TMM.2021.3057989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GAN) has become more and more popular these years. However, it is difficult to train and suffers from the training instability problem. To tackle this difficulty, this paper proposes a novel approach. Our idea is intuitive but proven to be very useful. In essence, it combines saturating loss and non-saturating loss into the loss function. Thus it will exploit the complementary statistical properties from two kinds of loss functions to effectively improve the training stability. We term our method twin discriminator Generative Adversarial Networks (TWGAN), which, unlike GAN, has a generator and a twin discriminator. The twin discriminator consists of two discriminators with identical architecture and both of them aim to distinguish whether the samples are from real data or fake data. We develop theoretical analysis to show that, given the optimal discriminators, optimizing the generator of TWGAN reduces to minimizing the Kullback-Leibler (KL) divergence between the distribution of generated data ( $P_g$ ) and the distribution of real data ( $P_data$ ), hence effectively addressing the training instability problem. Extensive experiments on MNIST, Fashion MNIST, CIFAR-10/100 and STL-10 datasets demonstrate that the competitive performance of our TWGAN in generating good quality and diverse samples over baselines. The obtained highest inception score (IS) and lowest Fr $\acute{e}$ chet Inception Distance (FID), compared with other state-of-the-art GANs, show the superiority of our TWGAN.},
  archive      = {J_TMM},
  author       = {Zhaoyu Zhang and Mengyan Li and Haonian Xie and Jun Yu and Tongliang Liu and Chang Wen Chen},
  doi          = {10.1109/TMM.2021.3057989},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {677-688},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TWGAN: Twin discriminator generative adversarial networks},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Temporal cross-layer correlation mining for action
recognition. <em>TMM</em>, <em>24</em>, 668–676. (<a
href="https://doi.org/10.1109/TMM.2021.3057503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neighboring frames are more correlated compared to frames from further temporal distances. In this paper, we aim to explore the temporal correlations among neighboring frames and exploit cross-layer multi-scale features for action recognition. First, we present a Temporal Cross-Layer Correlation (TCLC) framework for temporal correlation learning. The unified framework uncovers both local and global structures from video data, enabling a better exploration of temporal context and assisting cross-layer spatio-temporal feature learning. Second, we propose a novel cross-layer attention and a center-guided attention mechanism to integrate features with contextual knowledge from multiple scales. Our method is a two-stage process for effective cross-layer feature learning. The first stage incorporates the cross-layer attention module to decide the importance weight of the convolutional layers. The second stage leverages the center-guided attention mechanism to aggregate local features from each layer for the generation of a final video representation. We leverage global centers to extract shared semantic knowledge among videos. We evaluate TCLC on three action recognition datasets, i.e., UCF-101, HMDB-51 and Kinetics. Our experimental results demonstrate the superiority of our proposed temporal correlation mining method.},
  archive      = {J_TMM},
  author       = {Linchao Zhu and Hehe Fan and Yawei Luo and Mingliang Xu and Yi Yang},
  doi          = {10.1109/TMM.2021.3057503},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {668-676},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Temporal cross-layer correlation mining for action recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-focus image fusion based on multi-scale gradients and
image matting. <em>TMM</em>, <em>24</em>, 655–667. (<a
href="https://doi.org/10.1109/TMM.2021.3057493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-focus image fusion technology is to extract different focused regions of the same scene among partially focused images and merge them together to generate a composite image where all objects are clear. Two crucial points to multi-focus image fusion are the effective focus measurement method to evaluate the sharpness of the source images and the accurate segmentation method to extract the focused regions. In conventional multi-focus image fusion methods, the decision map obtained according to the focus measurement is sensitive to mis-registration, or produces an uneven boundary lines. In this paper, the maximum value in the top-hat transform and the bottom-hat transform is used as the gradient measurement value, and the complementary features between multiple scales are used to achieve accurate focus measurement for initial segmentation. In order to obtain a better fusion decision map, a robust image matting algorithm is used to refine the trimap generated by the initial segmentation. Then, make full use of the strong correlation between the source images to optimize the edge regions of the decision map to improve the image fusion quality. Finally, a fusion image is constructed based on the fusion decision map and the source images. We perform qualitative and quantitative experiments on publicly available databases to verify the effectiveness of the method. The results show that compared with several state-of-the-art algorithms, the proposed fusion method can obtain accurate decision maps and achieve better performance in visual perception and quantitative analysis.},
  archive      = {J_TMM},
  author       = {Jun Chen and Xuejiao Li and Linbo Luo and Jiayi Ma},
  doi          = {10.1109/TMM.2021.3057493},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {655-667},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-focus image fusion based on multi-scale gradients and image matting},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Signal-dependent noise estimation for a real-camera model
via weight and shape constraints. <em>TMM</em>, <em>24</em>, 640–654.
(<a href="https://doi.org/10.1109/TMM.2021.3056879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most computer vision algorithms require parameter adjustment according to the image noise level. Conventionally, the additive white Gaussian noise (AWGN) model is widely used in most noise estimation algorithms; however, this assumption does not hold in the real world where the noise from cameras is more complex, and it is more appropriate to assume the signal-dependent noise (SDN) model. In this paper, we focus on the SDN model while considering the nonlinear radiometric calibration inside an actual camera, and propose an algorithm to efficiently estimate the noise level function (NLF), which is defined as the noise standard deviation with respect to image intensity. First, the input image is divided into overlapping patches, and noise samples are estimated in the linear transform domain. The confidence levels of the noise samples and the prior of the camera response function are then employed as constraints for the recovery of the NLF. Finally, the noise samples and constraints are represented in a convex optimization problem. The experimental results using both real and synthetic noisy images demonstrate the superiority of the proposed method. In addition, the estimated NLFs are incorporated into two well-known denoising schemes, non-local means and BM3D, and shows significant improvements in denoising SDN-polluted images.},
  archive      = {J_TMM},
  author       = {Heng Yao and Mian Zou and Chuan Qin and Xinpeng Zhang},
  doi          = {10.1109/TMM.2021.3056879},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {640-654},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Signal-dependent noise estimation for a real-camera model via weight and shape constraints},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial-temporal action localization with hierarchical
self-attention. <em>TMM</em>, <em>24</em>, 625–639. (<a
href="https://doi.org/10.1109/TMM.2021.3056892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel architecture for spatial-temporal action localization in videos. The new architecture first employs a two-stream 3D convolutional neural network (3D-CNN) to provide initial action detection. Next, a new Hierarchical Self-Attention Network (HiSAN), the core of this architecture, is developed to learn the spatial-temporal relationships of key actors. Spatial Gaussian priors (SGP) are also imbued to the bidirectional self-attention to enhance HiSAN in modelling the relationships of neighboring actors. Such a combination of 3D-CNN and SGP augmented HiSAN allows us to effectively extract both of the spatial context information and the long-term temporal dependency to improve action localization accuracy. Afterwards, a new fusion strategy is employed, which first re-scores the bounding boxes to settle the inconsistent detection scores caused by background clutter or occlusion, and then aggregates the motion and appearance information from the two-stream network with the motion saliency to alleviate the impact of camera movement. Finally, a tube association network based on the self-similarity of the actors’ appearance and spatial information across frames is addressed to efficaciously construct the action tubes. Simulations on four widespread datasets reveal the efficacy of the new approach.},
  archive      = {J_TMM},
  author       = {Rizard Renanda Adhi Pramono and Yie-Tarng Chen and Wen-Hsien Fang},
  doi          = {10.1109/TMM.2021.3056892},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {625-639},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spatial-temporal action localization with hierarchical self-attention},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Drift-proof tracking with deep reinforcement learning.
<em>TMM</em>, <em>24</em>, 609–624. (<a
href="https://doi.org/10.1109/TMM.2021.3056896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object tracking is an essential and challenging sub-domain in the field of computer vision owing to its wide range of applications and complexities of real-life situations. It has been studied extensively over the last decade, leading to the proposal of several tracking frameworks and approaches. Recently, the introduction of reinforcement learning and the ‘Actor-Critic’ framework has effectively improved the tracking speed of deep learning trackers. However, most existing deep reinforcement learning trackers experience a slight performance degradation mainly owing to the drift issues. Drifts pose a threat to the tracking performance, which may lead to losing the tracked target. Herein, we propose a drift-proof tracker with deep reinforcement learning that aims to improve the tracking performance by counteracting drifts while maintaining its real-time advantage. We utilize a reward function with the Distance-IoU (DIoU) metric to guide the reinforcement learning to alleviate the drifts caused by the trained model. Furthermore, double negative samples (hard negative and drift samples) are constructed in tracking for network initialization, which is followed by calculating the loss by a small error-friendly loss function. Therefore, our tracker can better discriminate between the positive and negative samples and correct the predicted bounding boxes when the drift occurs. Meanwhile, a generative adversarial network is adopted for positive sample augmentation. Extensive experimental results on multiple popular benchmarks show that our algorithm effectively reduces the occurrences of drift and boosts the tracking performance, compared to those of other state-of-the-art trackers.},
  archive      = {J_TMM},
  author       = {Zhongze Chen and Jing Li and Jia Wu and Jun Chang and Yafu Xiao and Xiaoting Wang},
  doi          = {10.1109/TMM.2021.3056896},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {609-624},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Drift-proof tracking with deep reinforcement learning},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AVN: An adversarial variation network model for handwritten
signature verification. <em>TMM</em>, <em>24</em>, 594–608. (<a
href="https://doi.org/10.1109/TMM.2021.3056217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwritten signature verification is a crucial yet challenging problem. While previous studies have made great progress in this problem, they learn signature features passively from given existing data. In this paper, we propose a novel adversarial variation network (AVN) model for handwritten signature verification which mines effective features by actively varying existing data and generating new data. Powered by a proposed novel variation consistency mechanism, the AVN contains three different types of modules unified under one end-to-end framework: the extractor seeks to extract deep discriminative features of handwritten signatures, the discriminator aims to make verification decisions based on the extracted features, and the variator is designed to actively generate signature variants for constructing a more discriminative model. The proposed model is trained in an adversarial way with a min-max loss function, by which the three modules cooperate and compete to enhance the entire model’s ability and therefore the signature verification performance is improved. We test the proposed method on four challenging signature datasets of different languages: CEDAR, BHSig-Hindi, BHSig-Bengali, and GPDS Synthetic Signature. Extensive experiments with in-depth discussions validate the effectiveness of the proposed method.},
  archive      = {J_TMM},
  author       = {Huan Li and Ping Wei and Ping Hu},
  doi          = {10.1109/TMM.2021.3056217},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {594-608},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AVN: An adversarial variation network model for handwritten signature verification},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Global-local label correlation for partial multi-label
learning. <em>TMM</em>, <em>24</em>, 581–593. (<a
href="https://doi.org/10.1109/TMM.2021.3055959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial Multi-label Learning (PML) addresses the scenario where each instance is assigned with multiple candidate labels, while only a subset of the labels are relevant. This task is very challenging because the training procedure can be misguided by the noisy (irrelevant) labels. Exploiting label correlations is useful for partial multi-label learning. However, the existing PML methods often ignore to explicitly and sufficiently leverage the label correlation information for handling the noisy labels. To this end, in this paper, we propose a novel Global-Local Label Correlation (GLC) approach for partial multi-label learning . On one hand, we introduce a label coefficient matrix to explicitly exploit the global structure information of labels from multiple subspaces. On the other hand, we present a new label manifold regularizer to capture the local label correlations to further improve the performance of our method. By jointly taking advantage of the global and local label correlations, our proposed approach achieves superior performance on both the synthetic and real-world data sets from diverse domains.},
  archive      = {J_TMM},
  author       = {Lijuan Sun and Songhe Feng and Jun Liu and Gengyu Lyu and Congyan Lang},
  doi          = {10.1109/TMM.2021.3055959},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {581-593},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Global-local label correlation for partial multi-label learning},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting web images for fine-grained visual recognition by
eliminating open-set noise and utilizing hard examples. <em>TMM</em>,
<em>24</em>, 546–557. (<a
href="https://doi.org/10.1109/TMM.2021.3055024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Labeling objects at a subordinate level typically requires expert knowledge, which is not always available when using random annotators. As such, learning directly from web images for fine-grained recognition has attracted broad attention. However, the presence of label noise and hard examples in web images are two obstacles for training robust fine-grained recognition models. Therefore, in this paper, we propose a novel approach for removing irrelevant samples from real-world web images during training, while employing useful hard examples to update the network. Thus, our approach can alleviate the harmful effects of irrelevant noisy web images and hard examples to achieve better performance. Extensive experiments on three commonly used fine-grained datasets demonstrate that our approach is far superior to current state-of-the-art web-supervised methods. The data and source code of this work have been made publicly available at: https://github.com/NUST-Machine-Intelligence-Laboratory/Advanced-Softly-Update-Drop .},
  archive      = {J_TMM},
  author       = {Huafeng Liu and Chuanyi Zhang and Yazhou Yao and Xiu-Shen Wei and Fumin Shen and Zhenmin Tang and Jian Zhang},
  doi          = {10.1109/TMM.2021.3055024},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {546-557},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Exploiting web images for fine-grained visual recognition by eliminating open-set noise and utilizing hard examples},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic tagging by leveraging visual and annotated
features in social media. <em>TMM</em>, <em>24</em>, 2218–2229. (<a
href="https://doi.org/10.1109/TMM.2021.3055037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic image annotation is one of the research fields helping to extract the meaning of images, which aims at the production of a set of semantic annotations for an image to help better present the concept. Over the past few decades, researchers have developed many approaches for automatic image annotation. Nevertheless, previous studies have not fully accounted for visual features and annotated features. Therefore, it is still possible to achieve a better annotation performance by combining visual and annotated information. In this study, we aim to associate multiple semantic tags with a given image. In particular, we detect how to obtain the image annotation by utilizing visual and annotated information. To take advantage of visual information, we first designed a modified neural network method to acquire the features of the image content. In addition, to obtain the annotated features, we exploit an aggregated network embedding approach that consists of annotation embedding, social embedding, profile embedding, and semantic embedding. Finally, to produce an accurate image annotation, we integrate the two aforementioned methods, that is, combining the visual and annotated information, to build a unified cooperative training framework. The experimental results on three real-world datasets clarify that our presented method is superior to the currently popular image annotation approaches.},
  archive      = {J_TMM},
  author       = {Jinpeng Chen and Pinguang Ying and Xiangling Fu and Xiaopeng Luo and Hao Guan and Kaimin Wei},
  doi          = {10.1109/TMM.2021.3055037},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {2218-2229},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Automatic tagging by leveraging visual and annotated features in social media},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SPG-VTON: Semantic prediction guidance for multi-pose
virtual try-on. <em>TMM</em>, <em>24</em>, 1233–1246. (<a
href="https://doi.org/10.1109/TMM.2022.3143712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-based virtual try-on is challenging in fitting a target in-shop clothes onto a reference person under diverse human poses. Previous works focus on preserving clothing details ( e.g., texture, logos, patterns) when transferring desired clothes onto a target person under a fixed pose. However, the performances of existing methods significantly dropped when extending existing methods to multi-pose virtual try-on. In this paper, we propose an end-to-end Semantic Prediction Guidance multi-pose Virtual Try-On Network (SPG-VTON), which can fit the desired clothing into a reference person under arbitrary poses. Specifically, SPG-VTON is composed of three sub-modules. First, a Semantic Prediction Module (SPM) generates the desired semantic map. The predicted semantic map provides more abundant guidance to locate the desired clothing region and produce a coarse try-on image. Second, a Clothes Warping Module (CWM) warps in-shop clothes to the desired shape according to the predicted semantic map and the desired pose. Specifically, we introduce a conductible cycle consistency loss to alleviate the misalignment in the clothing warping process. Third, a Try-on Synthesis Module (TSM) combines the coarse result and the warped clothes to generate the final virtual try-on image, preserving details of the desired clothes and under the desired pose. In addition, we introduce a face identity loss to refine the facial appearance and maintain the identity of the final virtual try-on result at the same time. We evaluate the proposed method on the most massive multi-pose dataset (MPV) and the DeepFashion dataset. The qualitative and quantitative experiments show that SPG-VTON is superior to the state-of-the-art methods and is robust to data noise, including background and accessory changes, i.e. , hats and handbags, showing good scalability to the real-world scenario.},
  archive      = {J_TMM},
  author       = {Bingwen Hu and Ping Liu and Zhedong Zheng and Mingwu Ren},
  doi          = {10.1109/TMM.2022.3143712},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {1233-1246},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SPG-VTON: Semantic prediction guidance for multi-pose virtual try-on},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-modal dynamic networks for video moment retrieval with
text query. <em>TMM</em>, <em>24</em>, 1221–1232. (<a
href="https://doi.org/10.1109/TMM.2022.3142420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video moment retrieval with text query aims to retrieve the most relevant segment from the whole video based on the given text query. It is a challenging cross-modal alignment task due to the huge gap between visual and linguistic modalities and the noise generated by manual labeling of time segments. Most of the existing works only use language information in the cross-modal fusion stage, neglecting that language information plays an important role in the retrieval stage. Besides, these works roughly compress the visual information in the video clips to reduce the computation cost which loses subtle video information in the long video. In this paper, we propose a novel model termed Cross-modal Dynamic Networks (CDN) which dynamically generates convolution kernel by visual and language features. In the feature extraction stage, we also propose a frame selection module to capture the subtle video information in the video segment. By this approach, the CDN can reduce the impact of the visual noise without significantly increasing the computation cost and leads to a better video moment retrieval result. The experiments on two challenge datasets, i.e. , Charades-STA and TACoS, show that our proposed CDN method outperforms a bundle of state-of-the-art methods with more accurately retrieved moment video clips. The implementation code and extensive instruction of our proposed CDN method are provided at https://github.com/CFM-MSG/Code_CDN .},
  archive      = {J_TMM},
  author       = {Gongmian Wang and Xing Xu and Fumin Shen and Huimin Lu and Yanli Ji and Heng Tao Shen},
  doi          = {10.1109/TMM.2022.3142420},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {1221-1232},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross-modal dynamic networks for video moment retrieval with text query},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active gradual domain adaptation: Dataset and approach.
<em>TMM</em>, <em>24</em>, 1210–1220. (<a
href="https://doi.org/10.1109/TMM.2022.3142524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adapting deep neural networks to the changing environments is critical in practical utility, especially for online web applications, where the data distribution changes gradually due to the evolving environments. For instance, the web photos of cellphones change gradually over years due to appearance changes. This paper deals with such a problem via active gradual domain adaptation, where the learner continually and actively selects the most informative labels from the target to enhance labeling efficiency and utilizes both labeled and unlabeled samples to improve the model adaptation under gradual domain drift. We propose the active gradual self-training (AGST) algorithm with novel designs of active pseudolabeling and gradual semi-supervised domain adaptation. Specifically, AGST pseudolabels the samples with high confidence, and selects the most informative labels from the unconfident samples based on both uncertainty and diversity, and then gradually self-trains itself by confident pseudolabels and queried labels. To study the gradual domain shift problem in the web data and verify the proposed algorithm, we create a new dataset -- Evolving-Image-Search (EVIS), collected from the web search engine and covers a 12-years range. Since the appearance of the products evolves over these years, such dataset naturally contains gradual domain drift. We extensively evaluate AGST on the synthetic dataset, real-world dataset, and EVIS dataset. AGST achieves up to 62% accuracy improvement (absolute value) against unsupervised gradual self-training with only 5% additional labels, and 19% accuracy improvement against directly applying CLUE, demonstrating the effectiveness of the designs of active pseudolabel and gradual semi-supervised domain adaptation.},
  archive      = {J_TMM},
  author       = {Shiji Zhou and Lianzhe Wang and Shanghang Zhang and Zhi Wang and Wenwu Zhu},
  doi          = {10.1109/TMM.2022.3142524},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {1210-1220},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Active gradual domain adaptation: Dataset and approach},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal cross-layer bilinear pooling for RGBT tracking.
<em>TMM</em>, <em>24</em>, 567–580. (<a
href="https://doi.org/10.1109/TMM.2021.3055362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical deep features can provide multilevel abstractions of target objects, which play an important role in target localization and classification. Determining how to effectively aggregate abstract information from different levels in RGB and thermal modalities is the key to exploiting their complementary advantages for robust RGBT tracking. However, existing RGBT tracking algorithms either focus on the semantic information of the last layer or aggregate hierarchical deep features from each modal using simple operations (e.g., summation and concatenation), which limit the capability of the multimodal tracker. To address these issues, in this paper, we propose a novel multimodal cross-layer bilinear pooling network for RGBT tracking. In our network, firstly, to boost the performance of the tracker, we use a channel attention mechanism to implement the adaptive calibration of feature channels for all convolutional layer features before realizing hierarchical feature fusion. Then, a bilinear pooling operation is performed on any two layers through the cross product, which is a second-order computation that effectively aggregates the deep semantic and shallow texture information of the target. Finally, a quality-aware fusion module is designed to aggregate the bilinear pooling features of different layer interactions between different modalities in an adaptive manner. The results of a large number of experiments on two public benchmark datasets demonstrate the effectiveness of our tracker compared with other state-of-the-art tracking methods.},
  archive      = {J_TMM},
  author       = {Qin Xu and Yiming Mei and Jinpei Liu and Chenglong Li},
  doi          = {10.1109/TMM.2021.3055362},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {567-580},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multimodal cross-layer bilinear pooling for RGBT tracking},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generate and purify: Efficient person data generation for
re-identification. <em>TMM</em>, <em>24</em>, 558–566. (<a
href="https://doi.org/10.1109/TMM.2021.3054973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating person images has been a promising approach to enhance the input richness for re-identification (reID) tasks in recent works. A key challenge is that the generated data often contains noise, which is caused by identity inconsistency between the generated person and the original input and failure cases in generative adversarial networks (GAN). Directly training using generated images may greatly affect learning good feature embeddings, resulting in unsatisfactory reID performance. This work presents a two-stage framework that can generate high-quality person images and purify failure cases for reID training. Experimental results demonstrate that our proposed generative model can produce person images with superior appearance consistency comparing with other state-of-the-art methods. Furthermore, we show that our method yields a significant improvement in re-identification (reID) task on public datasets with insufficient training data.},
  archive      = {J_TMM},
  author       = {Jianjie Lu and Weidong Zhang and Haibing Yin},
  doi          = {10.1109/TMM.2021.3054973},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {558-566},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Generate and purify: Efficient person data generation for re-identification},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IDHashGAN: Deep hashing with generative adversarial nets for
incomplete data retrieval. <em>TMM</em>, <em>24</em>, 534–545. (<a
href="https://doi.org/10.1109/TMM.2021.3054503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from low storage costs and high retrieval efficiency, hash learning has been a widely adopted technology for approximating nearest neighbor in large-scale data retrieval. Deep learning to hash greatly improves image retrieval performance by integrating feature learning and hash coding into an end-to-end framework. However, subject to application scope, most existing deep hashing methods only apply to retrieval of complete data and have undesirable results when retrieving incomplete but valuable data. In this paper we propose IDHashGAN, a novel deep hashing model with generative adversarial networks to retrieve incomplete data, in which feature restoration, feature learning and hash coding are integrated into an unified end-to-end framework. The proposed model consists of four key components: (1) reconstructive and generative loss are used to generate continuous feature of incomplete data in generative network; (2) supervised manifold similarity is proposed to improve retrieval accuracy and obtain good user acceptance; (3) adversarial and classified loss are designed to distinguish authenticity and similarity in discriminative network; and (4) encoding and quantization loss are adopted to preserve similarity and control hash quality. Extensive experiments on benchmark datasets show that IDHashGAN is competitive on complete dataset and yields substantial boosts of 70% on incomplete datasets compared to state-of-the-art hashing methods.},
  archive      = {J_TMM},
  author       = {Liming Xu and Xianhua Zeng and Weisheng Li and Ling Bai},
  doi          = {10.1109/TMM.2021.3054503},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {534-545},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IDHashGAN: Deep hashing with generative adversarial nets for incomplete data retrieval},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MIEGAN: Mobile image enhancement via a multi-module cascade
neural network. <em>TMM</em>, <em>24</em>, 519–533. (<a
href="https://doi.org/10.1109/TMM.2021.3054509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual quality of images captured by mobile devices is often inferior to that of images captured by a Digital Single Lens Reflex (DSLR) camera. This paper presents a novel generative adversarial network-based mobile image enhancement method, referred to as MIEGAN. It consists of a novel multi-module cascade generative network and a novel adaptive multi-scale discriminative network. The multi-module cascade generative network is built upon a two-stream encoder, a feature transformer, and a decoder. In the two-stream encoder, a luminance-regularizing stream is proposed to help the network focus on low-light areas. In the feature transformation module, two networks effectively capture both global and local information of an image. To further assist the generative network to generate the high visual quality images, a multi-scale discriminator is used instead of a regular single discriminator to distinguish whether an image is fake or real globally and locally. To balance the global and local discriminators, an adaptive weight allocation is proposed. In addition, a contrast loss is proposed, and a new mixed loss function is developed to improve the visual quality of the enhanced images. Extensive experiments on the popular DSLR photo enhancement dataset and MIT-FiveK dataset have verified the effectiveness of the proposed MIEGAN.},
  archive      = {J_TMM},
  author       = {Zhaoqing Pan and Feng Yuan and Jianjun Lei and Wanqing Li and Nam Ling and Sam Kwong},
  doi          = {10.1109/TMM.2021.3054509},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {519-533},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MIEGAN: Mobile image enhancement via a multi-module cascade neural network},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). E-commerce storytelling recommendation using attentional
domain-transfer network and adversarial pre-training. <em>TMM</em>,
<em>24</em>, 506–518. (<a
href="https://doi.org/10.1109/TMM.2021.3054525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In e-commerce platforms, there is an emerging type of content that tells a “story” about some merchandise in the form of multimedia (text, images, video), which is named storytelling . Well told stories, like advertisements, can inspire users to purchase the related products. Thus, e-commerce service provider is keen to disseminate storytelling items to potentially interested users. We address this requirement by a cross-domain personalized recommendation approach. Because storytelling is a new type of content, its related user actions are much less, more sparse than product-related user actions, thus we propose to use product-domain user actions to assist the identification of user preferences and to make storytelling recommendations. Our method has two technical contributions. First, since the user behavior patterns are different across the storytelling domain and the product domain, we propose an attentional domain-transfer network, which effectively selects the relevant items in the two domains to characterize user preferences. Second, although storytelling is about product, between the two domains there is a large gap: product description is objective and categorical, like “keywords,” but storytelling is close to human language. To bridge the domain gap, we propose a dual-domain contrastive adversarial learning method to pre-train the feature extractors for storytelling and product simultaneously. We conduct experiments on two industrial datasets, and the results demonstrate the advantage of our proposed method that consistently outperforms the state-of-the-art methods. Besides, our method can be used to recommend storytelling to products, which is a desired functionality for product providers. Our code and models are publicly available.},
  archive      = {J_TMM},
  author       = {Xusong Chen and Chenyi Lei and Dong Liu and Guoxin Wang and Haihong Tang and Zheng-Jun Zha and Houqiang Li},
  doi          = {10.1109/TMM.2021.3054525},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {506-518},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {E-commerce storytelling recommendation using attentional domain-transfer network and adversarial pre-training},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image co-saliency detection and instance co-segmentation
using attention graph clustering based graph convolutional network.
<em>TMM</em>, <em>24</em>, 492–505. (<a
href="https://doi.org/10.1109/TMM.2021.3054526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-Saliency Detection (CSD) is to explore the concurrent patterns and salient objects from a group of relevant images, while Instance Co-Segmentation (ICS) aims to identify and segment out all of these co-salient instances, generating corresponding mask for each instance. To simultaneously tackle these two tasks, we present a novel adaptive graph convolutional network with attention graph clustering (GCAGC) for CSD and ICS, termed as GCAGC-CSD and GCAGC-ICS, respectively. The GCAGC-CSD contains three key model designs: first, we develop a graph convolutional network architecture to extract multi-scale representations to characterize the intra- and inter-image consistency. Second, we propose an attention graph clustering algorithm to distinguish the salient foreground objects from common areas in an unsupervised manner. Third, we present a unified framework with encoder-decoder structure to jointly train and optimize the graph convolutional network, attention graph cluster, and CSD decoder in an end-to-end fashion. Afterwards, we design a salient instance segmentation network for GCAGC-ICS, and combine the outputs of GCAGC-CSD and the instance segmentation branch to obtain instance-aware co-segmentation masks. The proposed GCAGC-CSD and GCAGC-ICS are extensively evaluated on four CSD benchmark datasets (iCoseg, Cosal2015, COCO-SEG and CoSOD3k) and five ICS benchmark datasets (CoSOD3k, COCO-NONVOC, COCO-VOC, VOC12 and SOC), and achieve superior performance over state-of-the-arts on both tasks.},
  archive      = {J_TMM},
  author       = {Tengpeng Li and Kaihua Zhang and Shiwen Shen and Bo Liu and Qingshan Liu and Zhu Li},
  doi          = {10.1109/TMM.2021.3054526},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {492-505},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Image co-saliency detection and instance co-segmentation using attention graph clustering based graph convolutional network},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). One-shot image-to-image translation via part-global learning
with a multi-adversarial framework. <em>TMM</em>, <em>24</em>, 480–491.
(<a href="https://doi.org/10.1109/TMM.2021.3053775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that humans can learn and recognize objects effectively from several limited image samples. However, learning from just a few images is still a tremendous challenge for existing main-stream deep neural networks. Inspired by analogical reasoning in the human mind, a feasible strategy is to &amp;#x201C;translate&amp;#x201D; the abundant images of a rich source domain to enrich the relevant yet different target domain with insufficient image data. To achieve this goal, we propose a novel, effective multi-adversarial framework (MA) based on part-global learning, which accomplishes the one-shot cross-domain image-to-image translation. In specific, we first devise a part-global adversarial training scheme to provide an efficient way for feature extraction and prevent discriminators from being overfitted. Then, a multi-adversarial mechanism is employed to enhance the image-to-image translation ability to unearth the high-level semantic representation. Moreover, a balanced adversarial loss function is presented, which aims to balance the training data and stabilize the training process. Extensive experiments demonstrate that the proposed approach can obtain impressive results on various datasets between two extremely imbalanced image domains and outperform state-of-the-art methods on one-shot image-to-image translation. Our code will be released with this paper at https://github.com/zhengziqiang/OST.},
  archive      = {J_TMM},
  author       = {Ziqiang Zheng and Zhibin Yu and Haiyong Zheng and Yang Yang and Heng Tao Shen},
  doi          = {10.1109/TMM.2021.3053775},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {480-491},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {One-shot image-to-image translation via part-global learning with a multi-adversarial framework},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Aggregation-based graph convolutional hashing for
unsupervised cross-modal retrieval. <em>TMM</em>, <em>24</em>, 466–479.
(<a href="https://doi.org/10.1109/TMM.2021.3053766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal hashing has sparked much attention in large-scale information retrieval for its storage and query efficiency. Despite the great success achieved by supervised approaches, existing unsupervised hashing methods still suffer from the lack of reliable learning guidance and cross-modal discrepancy. In this paper, we propose Aggregation-based Graph Convolutional Hashing (AGCH) to tackle these obstacles. First, considering that a single similarity metric can hardly represent data relationships comprehensively, we develop an efficient aggregation strategy that utilises multiple metrics to construct a more precise affinity matrix for learning. Specifically, we apply various similarity measures to exploit the structural information of multiple modalities from different perspectives and then aggregate the obtained information to produce a joint similarity matrix. Furthermore, a novel deep model is designed to learn unified binary codes across different modalities, where the key components include modality-specific encoders, Graph Convolutional Networks (GCNs) and a fusion module. The modality-specific encoders are tasked to learn feature embeddings for each individual modality. On this basis, we leverage GCNs to further excavate the semantic structure of data, along with a fusion module to correlate different modalities. Extensive experiments on three real-world datasets demonstrate that the proposed method significantly outperforms the state-of-the-art competitors.},
  archive      = {J_TMM},
  author       = {Peng-Fei Zhang and Yang Li and Zi Huang and Xin-Shun Xu},
  doi          = {10.1109/TMM.2021.3053766},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {466-479},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Aggregation-based graph convolutional hashing for unsupervised cross-modal retrieval},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep view synthesis via self-consistent generative network.
<em>TMM</em>, <em>24</em>, 451–465. (<a
href="https://doi.org/10.1109/TMM.2021.3053401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {View synthesis aims to produce unseen views from a set of views captured by two or more cameras at different positions. This task is non-trivial since it is hard to conduct pixel-level matching among different views. To address this issue, most existing methods seek to exploit the geometric information to match pixels. However, when the distinct cameras have a large baseline (i. e., far away from each other), severe geometry distortion issues would occur and the geometric information may fail to provide useful guidance, resulting in very blurry synthesized images. To address the above issues, in this paper, we propose a novel deep generative model, called Self-Consistent Generative Network (SCGN), which synthesizes novel views from the given input views without explicitly exploiting the geometric information. The proposed SCGN model consists of two main components, i. e., a View Synthesis Network (VSN) and a View Decomposition Network (VDN), both employing an Encoder-Decoder structure. Here, the VDN seeks to reconstruct input views from the synthesized novel view to preserve the consistency of view synthesis. Thanks to VDN, SCGN is able to synthesize novel views without using any geometric rectification before encoding, making it easier for both training and applications. Finally, adversarial loss is introduced to improve the photo-realism of novel views. Both qualitative and quantitative comparisons against several state-of-the-art methods on two benchmark tasks demonstrated the superiority of our approach.},
  archive      = {J_TMM},
  author       = {Zhuoman Liu and Wei Jia and Ming Yang and Peiyao Luo and Yong Guo and Mingkui Tan},
  doi          = {10.1109/TMM.2021.3053401},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {451-465},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep view synthesis via self-consistent generative network},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Affinity fusion graph-based framework for natural image
segmentation. <em>TMM</em>, <em>24</em>, 440–450. (<a
href="https://doi.org/10.1109/TMM.2021.3053393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an affinity fusion graph framework to effectively connect different graphs with highly discriminating power and nonlinearity for natural image segmentation. The proposed framework combines adjacency-graphs and kernel spectral clustering based graphs (KSC-graphs) according to a new definition named affinity nodes of multi-scale superpixels. These affinity nodes are selected based on a better affiliation of superpixels, namely subspace-preserving representation which is generated by sparse subspace clustering based on subspace pursuit. Then a KSC-graph is built via a novel kernel spectral clustering to explore the nonlinear relationships among these affinity nodes. Moreover, an adjacency-graph at each scale is constructed, which is further used to update the proposed KSC-graph at affinity nodes. The fusion graph is built across different scales, and it is partitioned to obtain final segmentation result. Experimental results on the Berkeley segmentation dataset and Microsoft Research Cambridge dataset show the superiority of our framework in comparison with the state-of-the-art methods. The code is available at https://github.com/Yangzhangcst/AF-graph.},
  archive      = {J_TMM},
  author       = {Yang Zhang and Moyun Liu and Jingwu He and Fei Pan and Yanwen Guo},
  doi          = {10.1109/TMM.2021.3053393},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {440-450},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Affinity fusion graph-based framework for natural image segmentation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video frame interpolation via generalized deformable
convolution. <em>TMM</em>, <em>24</em>, 426–439. (<a
href="https://doi.org/10.1109/TMM.2021.3052419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video frame interpolation aims at synthesizing intermediate frames from nearby source frames while maintaining spatial and temporal consistencies. The existing deep-learning-based video frame interpolation methods can be roughly divided into two categories: flow-based methods and kernel-based methods. The performance of flow-based methods is often jeopardized by the inaccuracy of flow map estimation due to oversimplified motion models, while that of kernel-based methods tends to be constrained by the rigidity of kernel shape. To address these performance-limiting issues, a novel mechanism named generalized deformable convolution is proposed, which can effectively learn motion information in a data-driven manner and freely select sampling points in space-time. We further develop a new video frame interpolation method based on this mechanism. Our extensive experiments demonstrate that the new method performs favorably against the state-of-the-art, especially when dealing with complex motions. Code is available at https://github.com/zhshi0816/GDConvNet .},
  archive      = {J_TMM},
  author       = {Zhihao Shi and Xiaohong Liu and Kangdi Shi and Linhui Dai and Jun Chen},
  doi          = {10.1109/TMM.2021.3052419},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {426-439},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Video frame interpolation via generalized deformable convolution},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intra-domain consistency enhancement for unsupervised person
re-identification. <em>TMM</em>, <em>24</em>, 415–425. (<a
href="https://doi.org/10.1109/TMM.2021.3052354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, unsupervised domain adaptation in person re-identification (ReID) has been widely studied to improve the generalization ability of the ReID model. Some existing methods focus on handling the intra-domain image variations caused by different camera configurations, pose, illumination, and background in target domain. However, they fail to fully mine the underlying consistency constraints contained in unlabeled target dataset. To comprehensively investigate the underlying constraints for unsupervised representation learning, we introduce two consistency constraints to deal with the intra-domain variations, namely instance-ensembling consistency and cross-granularity consistency. Specifically, the instance-ensembling consistency constraint aims to encourage similar features for a given instance and its positive samples. The cross-granularity consistency constraint is designed to enhance the collaboration of global clues and local clues in multi-granularity feature learning, which can overcome the negative effects caused by the noisy pseudo labels. By combining the advantages of the two constraints, we propose an iterative Intra-domain Consistency Enhancement (ICE) approach based on the Mean Teacher framework to fully mine the two underlying consistency constraints on multi-granularity features. The proposed ICE approach achieves significant improvement compared with the state-of-the-art, which demonstrates the superiority of the two consistency constraints.},
  archive      = {J_TMM},
  author       = {Yaoyu Li and Hantao Yao and Changsheng Xu},
  doi          = {10.1109/TMM.2021.3052354},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {415-425},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Intra-domain consistency enhancement for unsupervised person re-identification},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast intra mode decision algorithm for versatile video
coding. <em>TMM</em>, <em>24</em>, 400–414. (<a
href="https://doi.org/10.1109/TMM.2021.3052348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve higher coding efficiency, the latest Versatile Video Coding (VVC) standard adopts a series of new intra coding techniques, including the quadtree plus multi-type tree (QTMT), intra sub-partitions (ISP) and intra block copy (IBC). However, this makes the intra coding more complicated, as VVC needs to traverse all prediction modes and partition types of QTMT to find the optimal combination. In this paper, we propose a fast algorithm for VVC from two aspects of mode selection and prediction terminating to reduce coding complexity. For the mode selection, adaptive mode pruning (AMP) is proposed to remove non-promising modes. First, since the newly introduced modes (IBC and ISP) are not effective for all blocks, learning-based classifiers are designed to remove them intelligently. Second, for normal modes, an ensemble decision strategy is proposed to sort the candidate modes and increase the probability of being the optimal mode for the first few candidates; thus, we can remove redundant candidates more efficiently. In terms of prediction terminating, we find that different optimal modes of current depth level lead to different termination probabilities of remaining intra predictions. Therefore, mode-dependent termination (MDT) is proposed to select an appropriate model through the optimal mode and terminate unnecessary intra predictions of remaining depth levels. The proposed algorithm is implemented on VVC test model, and simulation results show that it can achieve 51% $\sim$ 53% time savings with only 0.93% $\sim$ 1.08% BDBR increases.},
  archive      = {J_TMM},
  author       = {Xinchao Dong and Liquan Shen and Mei Yu and Hao Yang},
  doi          = {10.1109/TMM.2021.3052348},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {400-414},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fast intra mode decision algorithm for versatile video coding},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Viewport-aware deep reinforcement learning approach for
360<span class="math inline"><sup>∘</sup></span> video caching.
<em>TMM</em>, <em>24</em>, 386–399. (<a
href="https://doi.org/10.1109/TMM.2021.3052339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {360$^{\circ }$ video is an essential component of VR/AR/MR systems that provides immersive experience to the users. However, 360$^{\circ }$ video is associated with high bandwidth requirements. The required bandwidth can be reduced by exploiting the fact that users are interested in viewing only a part of the video scene and that users request viewports that overlap with each other. Motivated by the findings of our recent works where the benefits of caching video tiles at edge servers instead of caching entire 360$^{\circ }$ videos were shown, in this paper, we introduce the concept of virtual viewports that have the same number of tiles with the original viewports. The tiles forming these viewports are the most popular ones for each video and are determined by the users&amp;#x2019; requests. Then, we propose a reactive caching scheme that assumes unknown videos&amp;#x2019; and viewports&amp;#x2019; popularity. Our scheme determines which videos to cache as well as which is the optimal virtual viewport per video. Virtual viewports permit to lower the dimensionality of the cache optimization problem. To solve the problem, we first formulate the content placement of 360$^{\circ }$ videos in edge cache networks as a Markov Decision Process (MDP), and then we determine the optimal caching placement using the Deep Q-Network (DQN) algorithm. The proposed solution aims at maximizing the overall quality of the 360$^{\circ }$ videos delivered to the end-users by caching the most popular 360$^{\circ }$ videos at base quality along with a virtual viewport in high quality. We extensively evaluate the performance of the proposed system and compare it with that of known systems such as Least Frequently Used (LFU), Least Recently Used (LRU), First In First Out (FIFO), over both synthetic and real 360$^{\circ }$ video traces. The results reveal the large benefits coming from reactive caching of virtual viewports instead of the original ones in terms of the overall quality of the rendered viewports, the cache hit ratio, and the servicing cost.},
  archive      = {J_TMM},
  author       = {Pantelis Maniotis and Nikolaos Thomos},
  doi          = {10.1109/TMM.2021.3052339},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {386-399},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Viewport-aware deep reinforcement learning approach for 360$^\circ$ video caching},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised face image manipulation by conditioning GAN
on face decomposition. <em>TMM</em>, <em>24</em>, 377–385. (<a
href="https://doi.org/10.1109/TMM.2021.3050672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel architecture for manipulating facial expressions, head poses, and lighting conditions from a single monocular image. Recent methods based on Generative Adversarial Networks show promising results in expression manipulation. However, the variation is either defined by a limited number of classes or not well suitable for explicit manipulation of different attributes such as pose and lighting conditions. Besides, state-of-the-art methods are mostly focused on frontal faces. Therefore, in this paper, a new Generative Adversarial Network architecture is proposed by explicitly conditioning on the appearance image space which is the product of direct manipulation of facial expressions, light and pose conditions of the face model in 3D space. In addition, the method only requires video sequences for training. Therefore, it is self-supervised. Unlike other face manipulation methods, the proposed method does not require target specific training. Large scale experiments show that our method outperforms state-of-the-art methods for different scenarios.},
  archive      = {J_TMM},
  author       = {Lê Minh Ngô and Sezer Karaoğlu and Theo Gevers},
  doi          = {10.1109/TMM.2021.3050672},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {377-385},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Self-supervised face image manipulation by conditioning GAN on face decomposition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interaction relational network for mutual action
recognition. <em>TMM</em>, <em>24</em>, 366–376. (<a
href="https://doi.org/10.1109/TMM.2021.3050642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person-person mutual action recognition (also referred to as interaction recognition) is an important research branch of human activity analysis. Current solutions in the field – mainly dominated by CNNs, GCNs and LSTMs – often consist of complicated architectures and mechanisms to embed the relationships between the two persons on the architecture itself, to ensure the interaction patterns can be properly learned. Our main contribution with this work is by proposing a simpler yet very powerful architecture, named Interaction Relational Network, which utilizes minimal prior knowledge about the structure of the human body. We drive the network to identify by itself how to relate the body parts from the individuals interacting. In order to better represent the interaction, we define two different relationships, leading to specialized architectures and models for each. These multiple relationship models will then be fused into a single and special architecture, in order to leverage both streams of information for further enhancing the relational reasoning capability. Furthermore we define important structured pair-wise operations to extract meaningful extra information from each pair of joints – distance and motion. Ultimately, with the coupling of an LSTM, our IRN is capable of paramount sequential relational reasoning. These important extensions we made to our network can also be valuable to other problems that require sophisticated relational reasoning. Our solution is able to achieve state-of-the-art performance on the traditional interaction recognition datasets SBU and UT, and also on the mutual actions from the large-scale dataset NTU RGB+D. Furthermore, it obtains competitive performance in the NTU RGB+D 120 dataset interactions subset.},
  archive      = {J_TMM},
  author       = {Mauricio Perez and Jun Liu and Alex C. Kot},
  doi          = {10.1109/TMM.2021.3050642},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {366-376},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Interaction relational network for mutual action recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reinforcement learning for logic recipe generation: Bridging
gaps from images to plans. <em>TMM</em>, <em>24</em>, 352–365. (<a
href="https://doi.org/10.1109/TMM.2021.3050090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a challenging task to produce recipes from images, due to the difficulty in bridging the gap from intuitive, static images to sequential, dynamic recipes. In this paper, we propose a novel recipe generation system for producing effective recipes from images. As medium steps, ingredient generation is introduced to guide recipe generation in our system. With potential information in ingredient lists, ingredient selection and ingredient sequence, the system is taught to generate effective recipes. For information representation, a hierarchical attention mechanism is designed to extract effective features for ingredient production and recipe generation. In order to guarantee the comprehensiveness and logic in recipes, a specific and explicit criterion around ingredients is designed under the framework of reinforcement learning. In ingredient generation, the system is required to generate ingredients with correct sequence in cooking procedures. And in recipe generation, ingredients in recipes are required to be consistent with produced ingredients. In experiments, the proposed method is compared with state-of-the-art methods to evaluate the feasibility. The results indicate that the proposed system achieves a better performance than other methods on both aspects of producing proper ingredients and effective recipes.},
  archive      = {J_TMM},
  author       = {Mengyang Zhang and Guohui Tian and Ying Zhang and Peng Duan},
  doi          = {10.1109/TMM.2021.3050090},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {352-365},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Reinforcement learning for logic recipe generation: Bridging gaps from images to plans},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarial-metric learning for audio-visual cross-modal
matching. <em>TMM</em>, <em>24</em>, 338–351. (<a
href="https://doi.org/10.1109/TMM.2021.3050089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-visual matching aims to learn the intrinsic correspondence between image and audio clip. Existing works mainly concentrate on learning discriminative features, while ignore the cross-modal heterogeneous issue between audio and visual modalities. To deal with this issue, we propose a novel Adversarial-Metric Learning (AML) model for audio-visual matching. AML aims to generate a modality-independent representation for each person in each modality via adversarial learning, while simultaneously learns a robust similarity measure for cross-modality matching via metric learning. By integrating the discriminative modality-independent representation and robust cross-modality metric learning into an end-to-end trainable deep network, AML can overcome the heterogeneous issue with promising performance for audio-visual matching. Experiments on the various audio-visual learning tasks, including audio-visual matching, audio-visual verification and audio-visual retrieval on benchmark dataset demonstrate the effectiveness of the proposed AML model. The implementation codes are available on https://github.com/MLanHu/AML .},
  archive      = {J_TMM},
  author       = {Aihua Zheng and Menglan Hu and Bo Jiang and Yan Huang and Yan Yan and Bin Luo},
  doi          = {10.1109/TMM.2021.3050089},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {338-351},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adversarial-metric learning for audio-visual cross-modal matching},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving robustness of DASH against unpredictable network
variations. <em>TMM</em>, <em>24</em>, 323–337. (<a
href="https://doi.org/10.1109/TMM.2021.3050086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most video players use adaptive bitrate (ABR) algorithms to provide good quality-of-experience (QoE) in dynamic network conditions. To deal with the adaptation challenges, many ABR algorithms select bitrate by optimizing a defined QoE function. Within the framework, various algorithms mainly differ in how the optimization problem is solved, including prediction-based approaches and learn-based approaches. However, these algorithms suffer from limited performance in the current popular mobile streaming which has limited resources and rapidly changing link rates. Existing machine-learning approaches face deployment difficulties on mobile devices, and prediction-based approaches that rely on throughput prediction experience large buffer occupancy variations in cellular networks, resulting in rebuffering frequently. To provide a robust and lightweight ABR algorithm for mobile streaming, this work improves the robustness of prediction-based scheme against unpredictable network variations and develops RBC (Robust Bitrate Controller) algorithm. Rather than optimizing QoE over the entire buffer capacity, RBC creates buffer margins to absorb the impact of throughput jitters and solves QoE maximization on the narrowed buffer range. The amount of buffer margin is dynamically adjusted based on the real-time throughput fluctuation to ensure sufficient de-jitter space. For online lightweight deployment, RBC provides a closed-form solution of the desired bitrate with small computation complexity by using adaptive control approach. Trace-driven experiments and real-world tests show that RBC effectively reduces the playback freezing and gains an improvement in overall QoE.},
  archive      = {J_TMM},
  author       = {Bo Wang and Mingwei Xu and Fengyuan Ren and Jianping Wu},
  doi          = {10.1109/TMM.2021.3050086},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {323-337},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Improving robustness of DASH against unpredictable network variations},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). SAM: Modeling scene, object and action with semantics
attention modules for video recognition. <em>TMM</em>, <em>24</em>,
313–322. (<a href="https://doi.org/10.1109/TMM.2021.3050058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video recognition aims at understanding semantic contents that normally involve the interactions of humans and related objects under certain scenes. A common practice to improve recognition accuracy is to combine object, scene and action features for classification directly, assuming that they are explicitly complementary. In this paper, we break down the fusion of three features into two pairwise feature relation modeling processes, which mitigates the difficulty of correlation learning in high dimensional features. Towards this goal, we introduce a Semantics Attention Module that captures the relations of a pair of features by refining the relatively “weak” feature with the guidance from the “strong” feature using attention mechanisms. The refined representation is further combined with the “strong” feature using a residual design for downstream tasks. Two SAMs are applied in a Semantics Attention Network (SAN) for improving video recognition. Extensive experiments are conducted on two large-scale video benchmarks, FCVID and ActivityNet v1.3—the proposed approach achieves better results while requiring much less computational effort than alternative methods.},
  archive      = {J_TMM},
  author       = {Xing Zhang and Zuxuan Wu and Yu-Gang Jiang},
  doi          = {10.1109/TMM.2021.3050058},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {313-322},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SAM: Modeling scene, object and action with semantics attention modules for video recognition},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust visual object tracking via adaptive attribute-aware
discriminative correlation filters. <em>TMM</em>, <em>24</em>, 301–312.
(<a href="https://doi.org/10.1109/TMM.2021.3050073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, attention mechanisms have been widely studied in Discriminative Correlation Filter (DCF) based visual object tracking. To realise spatial attention and discriminative feature mining, existing approaches usually apply regularisation terms to the spatial dimension of multi-channel features. However, these spatial regularisation approaches construct a shared spatial attention pattern for all multi-channel features, without considering the diversity across channels. As each feature map (channel) focuses on a specific visual attribute, a shared spatial attention pattern limits the capability for mining important information from different channels. To address this issue, we advocate channel-specific spatial attention for DCF-based trackers. The key ingredient of the proposed method is an Adaptive Attribute-Aware spatial attention mechanism for constructing a novel DCF-based tracker (A$^3$ DCF). To highlight the discriminative elements in each feature map, spatial sparsity is imposed in the filter learning stage, moderated by the prior knowledge regarding the expected concentration of signal energy. In addition, we perform a post processing of the identified spatial patterns to alleviate the impact of less significant channels. The net effect is that the irrelevant and inconsistent channels are removed by the proposed method. The results obtained on a number of well-known benchmarking datasets, including OTB2015, DTB70, UAV123, VOT2018, LaSOT, GOT-10 K and TrackingNet, demonstrate the merits of the proposed A$^3$ DCF tracker, with improved performance compared to the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Xue-Feng Zhu and Xiao-Jun Wu and Tianyang Xu and Zhen-Hua Feng and Josef Kittler},
  doi          = {10.1109/TMM.2021.3050073},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {301-312},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust visual object tracking via adaptive attribute-aware discriminative correlation filters},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Infrared action detection in the dark via cross-stream
attention mechanism. <em>TMM</em>, <em>24</em>, 288–300. (<a
href="https://doi.org/10.1109/TMM.2021.3050069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action detection plays an important role in video understanding and attracts considerable attention in the last decade. However, current action detection methods are mainly based on visible videos, and few of them consider scenes with low-light, where actions are difficult to be detected by existing methods, or even by human eyes. Compared with visible videos, infrared videos are more suitable for the dark environment and resistant to background clutter. In this paper, we investigate the temporal action detection problem in the dark by using infrared videos, which is, to the best of our knowledge, the first attempt in the action detection community. Our model takes the whole video as input, a Flow Estimation Network (FEN) is employed to generate the optical flow for infrared data, and it is optimized with the whole network to obtain action-related motion representations. After feature extraction, the infrared stream and flow stream are fed into a Selective Cross-stream Attention (SCA) module to narrow the performance gap between infrared and visible videos. The SCA emphasizes informative snippets and focuses on the more discriminative stream automatically. Then we adopt a snippet-level classifier to obtain action scores for all snippets and link continuous snippets into final detections. All these modules are trained in an end-to-end manner. We collect an Inf rared action Det ection (InfDet) dataset obtained in the dark and conduct extensive experiments to verify the effectiveness of the proposed method. Experimental results show that our proposed method surpasses state-of-the-art temporal action detection methods designed for visible videos, and it also achieves the best performance compared with other infrared action recognition methods on both InfAR and Infrared-Visible datasets.},
  archive      = {J_TMM},
  author       = {Xu Chen and Chenqiang Gao and Chaoyu Li and Yi Yang and Deyu Meng},
  doi          = {10.1109/TMM.2021.3050069},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {288-300},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Infrared action detection in the dark via cross-stream attention mechanism},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting informative video segments for temporal action
localization. <em>TMM</em>, <em>24</em>, 274–287. (<a
href="https://doi.org/10.1109/TMM.2021.3050067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method of exploiting informative video segments by learning segment weights for temporal action localization in untrimmed videos. Informative video segments represent the intrinsic motion and appearance of an action, and thus contribute crucially to action localization. The learned segment weights represent the informativeness of video segments to recognize actions and help infer the boundaries required to temporally localize actions. We build a supervised temporal attention network (STAN) that includes a supervised segment-level attention module to dynamically learn the weights of video segments, and a feature-level attention module to effectively fuse multiple features of segments. Through the cascade of the attention modules, STAN exploits informative video segments and generates descriptive and discriminative video representations. We use a proposal generator and a classifier to estimate the boundaries of actions and classify the classes of actions. Extensive experiments are conducted on two public benchmarks, i.e., THUMOS2014 and ActivityNet1.3. The results demonstrate that our proposed method achieves competitive performance compared with existing state-of-the-art methods. Moreover, compared with the baseline method that treats video segments equally, STAN achieves significant improvements with an increase of the mean average precision from 30.4% to 39.8% on the THUMOS2014 dataset, and from 31.4% to 35.9% on the ActivityNet1.3 dataset, demonstrating the effectiveness of learning informative video segments for temporal action localization.},
  archive      = {J_TMM},
  author       = {Che Sun and Hao Song and Xinxiao Wu and Yunde Jia and Jiebo Luo},
  doi          = {10.1109/TMM.2021.3050067},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {274-287},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Exploiting informative video segments for temporal action localization},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatiotemporal dilated convolution with uncertain matching
for video-based crowd estimation. <em>TMM</em>, <em>24</em>, 261–273.
(<a href="https://doi.org/10.1109/TMM.2021.3050059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel SpatioTemporal convolutional Dense Network (STDNet) to address the video-based crowd counting problem, which contains the decomposition of 3D convolution and the 3D spatiotemporal dilated dense convolution to alleviate the rapid growth of the model size caused by the Conv3D layer. Moreover, since the dilated convolution extracts the multiscale features, we combine the dilated convolution with the channel attention block to enhance the feature representations. Due to the error that occurs from the difficulty of labeling crowds, especially for videos, imprecise or standard-inconsistent labels may lead to poor convergence for the model. To address this issue, we further propose a new patch-wise regression loss (PRL) to improve the original pixel-wise loss. Experimental results on three video-based benchmarks, i.e., the UCSD, Mall and WorldExpo’10 datasets, show that STDNet outperforms both image- and video-based state-of-the-art methods. The source codes are released at https://github.com/STDNet/STDNet .},
  archive      = {J_TMM},
  author       = {Yu-Jen Ma and Hong-Han Shuai and Wen-Huang Cheng},
  doi          = {10.1109/TMM.2021.3050059},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {261-273},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spatiotemporal dilated convolution with uncertain matching for video-based crowd estimation},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Region-based dehazing via dual-supervised
triple-convolutional network. <em>TMM</em>, <em>24</em>, 245–260. (<a
href="https://doi.org/10.1109/TMM.2021.3050053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most physical model-based dehazing methods are subject to contrast degradation in a dark or shadow region because of the mismatch between the physical model and real haze. This degradation decreases the quality of dehazed images. Furthermore, the retinex-haze combined models can cause the brightness saturation problem in a haze region. For this reason, the retinex-haze combined approaches are not appropriate to enhance the real-world haze images. To solve these problems, we present a novel region-based dehazing method via dual-supervised triple-convolutional network (TCN). More specifically, the proposed network first simulates the mismatch problem based on the region-model. Next, we then train the proposed triple-convolutional network, which can estimate the degraded regions. We then present a novel dual-supervised learning method to efficiently train the networks using a non-ideal dataset. Experimental results show that the proposed method outperforms state-of-the-art approaches in solving complex haze problems. The output of the proposed network has a high-similarity index in most cases for various benchmark dataset. Our approach also produces high-quality images in real haze image datasets.},
  archive      = {J_TMM},
  author       = {Joongchol Shin and Hasil Park and Joonki Paik},
  doi          = {10.1109/TMM.2021.3050053},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {245-260},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Region-based dehazing via dual-supervised triple-convolutional network},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SmsNet: A new deep convolutional neural network model for
adversarial example detection. <em>TMM</em>, <em>24</em>, 230–244. (<a
href="https://doi.org/10.1109/TMM.2021.3050057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of adversarial examples has had a significant impact on the development and application of deep learning. In this paper, a novel convolutional neural network model, the stochastic multifilter statistical network (SmsNet), is proposed for the detection of adversarial examples. A feature statistical layer is constructed to collect statistical data of feature map output from each convolutional layer in SmsNet by combining manual features with a neural network. The entire model is an end-to-end detection model, so the feature statistical layer is not independent of the network, and its output is directly transmitted to the fully connected layer by a short-cut connection called the SmsConnection. Additionally, a dynamic pruning strategy is introduced to simplify the model structure for better performance. The experiments demonstrate the effectiveness of the network structure and pruning strategy, and the proposed model achieves high detection rates against state-of-the-art adversarial attacks.},
  archive      = {J_TMM},
  author       = {Jinwei Wang and Junjie Zhao and Qilin Yin and Xiangyang Luo and Yuhui Zheng and Yun-Qing Shi and Sunil Kr. Jha},
  doi          = {10.1109/TMM.2021.3050057},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {230-244},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SmsNet: A new deep convolutional neural network model for adversarial example detection},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LAG-net: Multi-granularity network for person
re-identification via local attention system. <em>TMM</em>, <em>24</em>,
217–229. (<a href="https://doi.org/10.1109/TMM.2021.3050082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) is a challenging research topic which aims to retrieve the pedestrian images of the same person that captured by non-overlapping cameras. Existing methods either assume the body parts of the same person are well-aligned, or use attention selection mechanisms to constrain the effective region of feature learning. But these methods concentrate only on coarse feature representation and cannot model complex real scenes effectively. We propose a novel Local Attention Guided Network (LAG-Net) to not only exploit the most salient area among different people, but also extract important local detail through a Local Attention System (LAS). LAS is an attention selection unit that could extract approximate semantic local features of human body parts without extra supervision. To learn discriminative attention feature representation, we explore an attention feature regularization scheme to enhance the relevance of body part features that belong to same personal identity. Considering the effectiveness of feature augmentation in the Re-ID task and the defect of the existing methods, we propose a Batch Attention DropBlock (BA-DropBlock) to further improve DropBlock by combining the attention selection mechanism. Results on mainstream datasets demonstrate the superiority of our model over the state-of-the-art. Especially, our approach exceeds the current best method by a large margin of 4.6 ${\%}$ on the most challenging dataset CUHK03.},
  archive      = {J_TMM},
  author       = {Xun Gong and Zu Yao and Xin Li and Yueqiao Fan and Bin Luo and Jianfeng Fan and Boji Lao},
  doi          = {10.1109/TMM.2021.3050082},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {217-229},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {LAG-net: Multi-granularity network for person re-identification via local attention system},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross parallax attention network for stereo image
super-resolution. <em>TMM</em>, <em>24</em>, 202–216. (<a
href="https://doi.org/10.1109/TMM.2021.3050092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo super-resolution (SR) aims to enhance the spatial resolution of one camera view using additional information from the other. Previous deep-learning-based stereo SR methods indeed improved the SR performance effectively by employing additional information, but they are unable to super-resolve stereo images where there are large disparities, or different types of epipolar lines. Moreover, in these methods, one model can only super-solve images of a particular view, and for one specific scale factor. This paper proposes a cross parallax attention stereo super-resolution network (CPASSRnet) which can perform stereo SR of multiple scale factors for both views, with a single model. To overcome the difficulties of large disparity and different types of epipolar lines, a cross parallax attention module (CPAM) is presented, which captures the global correspondence of additional information for each view, relative to the other. CPAM allows the two views to exchange additional information with each other according to the generated attention maps. Quantitative and qualitative results compared with the state of the arts illustrate the superiority of CPASSRnet. Ablation experiments demonstrate that the proposed components are effective and noise tests verify the robustness of CPASSRnet.},
  archive      = {J_TMM},
  author       = {Canqiang Chen and Chunmei Qing and Xiangmin Xu and Patrick Dickinson},
  doi          = {10.1109/TMM.2021.3050092},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {202-216},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross parallax attention network for stereo image super-resolution},
  volume       = {24},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
