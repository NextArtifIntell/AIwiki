<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TPDS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tpds---363">TPDS - 363</h2>
<ul>
<li><details>
<summary>
(2022). Outperforming sequential full-word long addition with
parallelization and vectorization. <em>TPDS</em>, <em>33</em>(12),
4974‚Äì4985. (<a href="https://doi.org/10.1109/TPDS.2022.3211937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article presents algorithms for parallel and vectorized full-word addition of big unsigned integers with carry propagation. Because of the propagation, software parallelization and vectorization of non-polynomial addition of big integers have long been considered impractical due to data dependencies between digits of the operands. The presented algorithms are based upon parallel and vectorized detection of carry origins within elements of vector operands, masking bits which correspond to those elements and subsequent scalar addition of the resulting integers. The acquired bits can consequently be taken into account to adjust the sum using the proposed generalization of the Kogge-Stone method. Essentially, the article formalizes and experimentally verifies parallel and vectorized implementation of carry-lookahead adders applied at arbitrary granularity of data. This approach is noticeably beneficial for manycore, CUDA and vectorized implementation using AVX-512 with masked instructions. Experiments show that the parallel and vectorized implementations of the proposed algorithms can be multiple times faster compared to a sequential ripple-carry adder or adders based on redundant number systems such as one used in the GNU Multiple Precision library.},
  archive      = {J_TPDS},
  author       = {Andrey Chusov},
  doi          = {10.1109/TPDS.2022.3211937},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4974-4985},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Outperforming sequential full-word long addition with parallelization and vectorization},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Votes-as-a-proof (VaaP): Permissioned blockchain consensus
protocol made simple. <em>TPDS</em>, <em>33</em>(12), 4964‚Äì4973. (<a
href="https://doi.org/10.1109/TPDS.2022.3211829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of Blockchain technology, permissioned Blockchains are getting more and more attention from researchers because applications based on permissioned Blockchains are more practical and easier to be carried out. This paper aims to design a dedicated consensus protocol for permissioned Blockchains. The existing consensus protocols applied to permissioned Blockchains are either derived from public Blockchains such as Proof of Work (PoW) or Proof of Stake (PoS), with full decentralization, resulting in low transaction processing efficiency; or derived from traditional Byzantine fault-tolerant (BFT) consensus protocols such as Practical BFT (PBFT) or HoneyBadgerBFT, with high communication complexity of the consensus process, resulting in low scalability. Therefore, we propose a dedicated consensus protocol for permissioned Blockchains called Votes-as-a-Proof (VaaP) with high transaction processing efficiency while ensuring high scalability. Every node in VaaP runs a simple consensus process based on voting in parallel. Faulty nodes will only deprive themselves of using consensus service. We present the comparison of VaaP and Sphinx, one of the state-of-the-art consensus protocols, analytically and experimentally (up to 500 nodes). The results indicate that VaaP outperforms Sphinx in throughput, latency and scalability.},
  archive      = {J_TPDS},
  author       = {Xiang Fu and Huaimin Wang and Peichang Shi},
  doi          = {10.1109/TPDS.2022.3211829},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4964-4973},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Votes-as-a-proof (VaaP): Permissioned blockchain consensus protocol made simple},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ùëìuncX: Federated function as a service for science.
<em>TPDS</em>, <em>33</em>(12), 4948‚Äì4963. (<a
href="https://doi.org/10.1109/TPDS.2022.3208767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {∆í unc X is a distributed function as a service (FaaS) platform that enables flexible, scalable, and high performance remote function execution. Unlike centralized FaaS systems, ∆í unc X decouples the cloud-hosted management functionality from the edge-hosted execution functionality. ∆í unc X&#39;s endpoint software can be deployed, by users or administrators, on arbitrary laptops, clouds, clusters, and supercomputers, in effect turning them into function serving systems. ∆í unc X&#39;s cloud-hosted service provides a single location for registering, sharing, and managing both functions and endpoints. It allows for transparent, secure, and reliable function execution across the federated ecosystem of endpoints‚Äîenabling users to route functions to endpoints based on specific needs. ∆í unc X uses containers (e.g., Docker, Singularity, and Shifter) to provide common execution environments across endpoints. ∆í unc X implements various container management strategies to execute functions with high performance and efficiency on diverse ∆í unc X endpoints. ∆í unc X also integrates with an in-memory data store and Globus for managing data that may span endpoints. We motivate the need for ∆í unc X, present our prototype design and implementation, and demonstrate, via experiments on two supercomputers, that ∆í unc X can scale to more than 130000 concurrent workers. We show that ∆í unc X&#39;s container warming-aware routing algorithm can reduce the completion time for 3,000 functions by up to 61\% compared to a randomized algorithm and the in-memory data store can speed up data transfers by up to 3x compared to a shared file system.},
  archive      = {J_TPDS},
  author       = {Zhuozhao Li and Ryan Chard and Yadu Babuji and Ben Galewsky and Tyler J. Skluzacek and Kirill Nagaitsev and Anna Woodard and Ben Blaiszik and Josh Bryan and Daniel S. Katz and Ian Foster and Kyle Chard},
  doi          = {10.1109/TPDS.2022.3208767},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4948-4963},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ùëìuncX: Federated function as a service for science},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Co-concurrency mechanism for multi-GPUs in distributed
heterogeneous environments. <em>TPDS</em>, <em>33</em>(12), 4935‚Äì4947.
(<a href="https://doi.org/10.1109/TPDS.2022.3208082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high concurrency and high throughput characteristics of graphics processing units (GPUs) have made researchers continue to use it to optimize distributed parallel computing architectures. With the upgrading of processor architecture, GPUs allow multiple kernels to execute concurrently through stream queues. However, due to the different hardware characteristics and kernel properties in distributed architectures, existing research lacks careful consideration of optimization schemes for concurrent streams and kernel block sizes. Unreasonable stream concurrency and kernel block size configuration will lead to prolonged execution time and waste of computing resources during application execution. Therefore, we propose a multi-GPU multi-stream co-concurrency mechanism (MGSC) in a distributed heterogeneous environment, dynamically adjusting the number of concurrent streams and exploring the optimal block size in task scheduling. According to the memory resources and startup overhead occupied in concurrent stream scheduling, a resource-aware concurrent stream adaptive adjustment mechanism is proposed, which can dynamically adjust the number of streams. To explore the optimal block size, we abstract it as a multi-armed bandit problem (MAB) and propose a block size adjustment algorithm based on the upper confidence bound (UCB). We implement MGSC in Spark 3.1.1 and NVIDIA CUDA 11.2. We conduct comparative experiments with multiple typical benchmarks to evaluate the performance of MGSC. The experimental results show that the algorithm can make full use of the computing power of the GPU and significantly reduce the execution time of tasks.},
  archive      = {J_TPDS},
  author       = {Xuedong Zhang and Zhuo Tang and Xiantao Zhang and Kenli Li},
  doi          = {10.1109/TPDS.2022.3208082},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4935-4947},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Co-concurrency mechanism for multi-GPUs in distributed heterogeneous environments},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enabling in-network floating-point arithmetic for efficient
computation offloading. <em>TPDS</em>, <em>33</em>(12), 4918‚Äì4934. (<a
href="https://doi.org/10.1109/TPDS.2022.3208425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Programmable switches are recently used for accelerating data-intensive distributed applications. Some computational tasks, traditionally performed on servers in data centers, are offloaded into the network on programmable switches. These tasks may require the support of on-the-fly floating-point operations. Unfortunately, programmable switches are restricted to simple integer arithmetic operations. Existing systems circumvent this restriction by converting floats to integers or relying on local CPUs of switches, incurring extra processing delayed and accuracy loss. To address this gap, we propose NetFC, a table-lookup method to achieve on-the-fly in-network floating-point arithmetic operations nearly without accuracy loss. Specifically, NetFC utilizes logarithm projection and transformation to convert the original huge table enumerating all operands and results into several much smaller tables that can fit into the data plane of programmable switches. To cope with the table inflation problem on 32-bit floats, we also propose an approximation method that further breaks the large tables into smaller ones. In addition, NetFC leverages two optimizations to improve accuracy and reduce on-chip memory consumption. We use both synthetic and real-life datasets to evaluate NetFC. The experimental results show that the average accuracy of NetFC is above 99.9\% with only 448KB memory consumption for 16-bit floats and 99.1\% with 496KB memory consumption for 32-bit floats. Furthermore, we integrate NetFC into two distributed applications and two in-network telemetry systems to show its effectiveness in further improving the performance.},
  archive      = {J_TPDS},
  author       = {Penglai Cui and Heng Pan and Zhenyu Li and Penghao Zhang and Tianhao Miao and Jianer Zhou and Hongtao Guan and Gaogang Xie},
  doi          = {10.1109/TPDS.2022.3208425},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4918-4934},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Enabling in-network floating-point arithmetic for efficient computation offloading},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DRAS: Deep reinforcement learning for cluster scheduling in
high performance computing. <em>TPDS</em>, <em>33</em>(12), 4903‚Äì4917.
(<a href="https://doi.org/10.1109/TPDS.2022.3205325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster schedulers are crucial in high-performance computing (HPC). They determine when and which user jobs should be allocated to available system resources. Existing cluster scheduling heuristics are developed by human experts based on their experience with specific HPC systems and workloads. However, the increasing complexity of computing systems and the highly dynamic nature of application workloads have placed tremendous burden on manually designed and tuned scheduling heuristics. More aggressive optimization and automation are needed for cluster scheduling in HPC. In this work, we present an automated HPC scheduling agent named DRAS (Deep Reinforcement Agent for Scheduling) by leveraging deep reinforcement learning. DRAS is built on a hierarchical neural network incorporating special HPC scheduling features such as resource reservation and backfilling. An efficient training strategy is presented to enable DRAS to rapidly learn the target environment. Once being provided a specific scheduling objective given by the system manager, DRAS automatically learns to improve its policy through interaction with the scheduling environment and dynamically adjusts its policy as workload changes. We implement DRAS into a HPC scheduling platform called CQGym. CQGym provides a common platform allowing users to flexibly evaluate DRAS and other scheduling methods such as heuristic and optimization methods. The experiments using CQGym with different production workloads demonstrate that DRAS outperforms the existing heuristic and optimization approaches by up to 50\%.},
  archive      = {J_TPDS},
  author       = {Yuping Fan and Boyang Li and Dustin Favorite and Naunidh Singh and Taylor Childers and Paul Rich and William Allcock and Michael E. Papka and Zhiling Lan},
  doi          = {10.1109/TPDS.2022.3205325},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4903-4917},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DRAS: Deep reinforcement learning for cluster scheduling in high performance computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerating backward aggregation in GCN training with
execution path preparing on GPUs. <em>TPDS</em>, <em>33</em>(12),
4891‚Äì4902. (<a href="https://doi.org/10.1109/TPDS.2022.3205642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging Graph Convolutional Network (GCN) has been widely used in many domains, where it is important to improve the efficiencies of applications by accelerating GCN trainings. Due to the sparsity nature and exploding scales of input real-world graphs, state-of-the-art GCN training systems (e.g., GNNAdvisor) employ graph processing techniques to accelerate the message exchanging (i.e., aggregations) among the graph vertices. Nevertheless, these systems treat both the aggregation stages of forward and backward propagation phases as all-active graph processing procedures that indiscriminately conduct computations on all vertices of an input graph. In this article, we first point out that in a GCN training problem with a given training set on an input graph, its aggregation stages of backward propagation phases (called as backward aggregations in this article) can be equivalently converted to partially-active graph processing procedures, which conduct computations on only partial vertices of the input graph. By leveraging such a finding, we propose an execution path preparing method that collects and coalesces the graph data used during different training layers of backward aggregations, and constructs their corresponding sub-graphs (called as execution paths in this article) as inputs to conduct the backward training on GPUs. Further, we propose a structural-aware strategy for the execution paths to compute their optimal group sizes, so as to gain as high as possible performances on GPUs during the backward aggregations. The experiment results by conducting GCN training in typical real-world graphs show that compared with GNNAdvisor, our approach improves the performance of backward aggregations by up to 5.68x on NVIDIA P100 GPU, and up to 6.57x on NVIDIA V100S GPU},
  archive      = {J_TPDS},
  author       = {Shaoxian Xu and Zhiyuan Shao and Ci Yang and Xiaofei Liao and Hai Jin},
  doi          = {10.1109/TPDS.2022.3205642},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4891-4902},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating backward aggregation in GCN training with execution path preparing on GPUs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Artemis: A latency-oriented naming and routing system.
<em>TPDS</em>, <em>33</em>(12), 4874‚Äì4890. (<a
href="https://doi.org/10.1109/TPDS.2022.3207189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, Internet service deployment is typically implemented with server replication at multiple locations. Domain name system (DNS), which translates human-readable domain names into network-routable IP addresses, is typically used for distributing users to different server replicas. However, DNS relies on several network-based queries and the queries delay the connection setup process between the client and the server replica. In this article, we propose Artemis, a practical low-latency naming and routing system that supports optimal server (replica) selection based on user-defined policies and provides lower query latencies than DNS. Artemis uses a DNS-like domain name-IP mapping for replica selection and achieves low query latency by combining the name resolution process with the transport layer handshake process. In Artemis, all server replicas at different locations share the same anycast IP address, called Service Address. Clients use the Service Address to establish a transport layer connection with the server. The client&#39;s initial handshake packet is routed over an overlay network to reach the optimal server. Then the server migrates the transport layer connection to its original unicast IP address after finishing the handshake process. After that, service discovery is completed, and the client communicates with the server directly via IP addresses. To validate the effectiveness of Artemis, we evaluate its performance via both real trace-driven simulation and real-world deployment. The result shows that Artemis can handle a large number of connections and reduce the connection setup latency compared with state-of-the-art solutions. More specifically, our deployment across 11 Google data centers shows that Artemis reduces the connection setup latency by 39.4\% compared with DNS.},
  archive      = {J_TPDS},
  author       = {Xuebing Li and Yang Chen and Mengying Zhou and Tiancheng Guo and Chenhao Wang and Yu Xiao and Junjie Wan and Xin Wang},
  doi          = {10.1109/TPDS.2022.3207189},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4874-4890},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Artemis: A latency-oriented naming and routing system},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Federated learning with nesterov accelerated gradient.
<em>TPDS</em>, <em>33</em>(12), 4863‚Äì4873. (<a
href="https://doi.org/10.1109/TPDS.2022.3206480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a fast-developing technique that allows multiple workers to train a global model based on a distributed dataset. Conventional FL (FedAvg) employs gradient descent algorithm, which may not be efficient enough. Momentum is able to improve the situation by adding an additional momentum step to accelerate the convergence and has demonstrated its benefits in both centralized and FL environments. It is well-known that Nesterov Accelerated Gradient (NAG) is a more advantageous form of momentum, but it is not clear how to quantify the benefits of NAG in FL so far. This motives us to propose FedNAG, which employs NAG in each worker as well as NAG momentum and model aggregation in the aggregator. We provide a detailed convergence analysis of FedNAG and compare it with FedAvg. Extensive experiments based on real-world datasets and trace-driven simulation are conducted, demonstrating that FedNAG increases the learning accuracy by 3‚Äì24\% and decreases the total training time by 11‚Äì70\% compared with the benchmarks under a wide range of settings.},
  archive      = {J_TPDS},
  author       = {Zhengjie Yang and Wei Bao and Dong Yuan and Nguyen H. Tran and Albert Y. Zomaya},
  doi          = {10.1109/TPDS.2022.3206480},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4863-4873},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Federated learning with nesterov accelerated gradient},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LosaTM: A hardware transactional memory integrated with a
low-overhead scenario-awareness conflict manager. <em>TPDS</em>,
<em>33</em>(12), 4849‚Äì4862. (<a
href="https://doi.org/10.1109/TPDS.2022.3206777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vigorous development of high compute-intensive applications has led to the demand for maximizing the concurrency of multicore processors. The best-effort hardware transactional memory(HTM) is an important technology adopted by vendors to improve the potential concurrency of multicore processors, but the HTM implementations on commercial products have some drawbacks for its simplicity and need some further optimizations to enable more exploitation of concurrency. In this article, we propose and evaluate a novel design of HTM, called LosaTM, which can provide a scenario-awareness conflict management strategy. By leveraging the proposed feature of multiple-grained coherency maintenance in the coherence protocol, LosaTM resolves most false conflicts at a half-cache-line granularity. Furthermore, we design a winner/aborter vector conflict management algorithm to improve the efficiency of LosaTM in handling friendly-fire and unfairness competition that we have newly defined. In order to coordinate these integrated conflict management strategies, a scheduling strategy is also proposed to adaptively select the appropriate management according to the specific conflict scenario. We use gem5 to simulate LosaTM in detail on an 8-core tiled CMP system, and the simulation result shows that it only causes 0.7\% of the L1 cache size hardware overhead while achieving a 38\% average execution time reduction on the native STAMP. The speedup also demonstrates that LosaTM outperforms the state-of-the-art designs in previous works.},
  archive      = {J_TPDS},
  author       = {Chao Fu and Li Wan and Jun Han},
  doi          = {10.1109/TPDS.2022.3206777},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4849-4862},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {LosaTM: A hardware transactional memory integrated with a low-overhead scenario-awareness conflict manager},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TDFL: Truth discovery based byzantine robust federated
learning. <em>TPDS</em>, <em>33</em>(12), 4835‚Äì4848. (<a
href="https://doi.org/10.1109/TPDS.2022.3205714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) enables data owners to train a joint global model without sharing private data. However, it is vulnerable to Byzantine attackers that can launch poisoning attacks to destroy model training. Existing defense strategies rely on the additional datasets to train trustable server models or trusted execution environments to mitigate attacks. Besides, these strategies can only tolerate a small number of malicious users or resist a few types of poisoning attacks. To address these challenges, we design a novel federated learning method TDFL , T ruth D iscovery based F ederated L earning, which can defend against multiple poisoning attacks without additional datasets even when the Byzantine users are $\geq 50\%$ . Specifically, the TDFL considers different scenarios with different malicious proportions. For Honest-majority setting (Byzantine $&amp;lt; 50\%$ ), we design a special robust truth discovery aggregation scheme to remove malicious model updates, which can assign weights according to users‚Äô contribution; for Byzantine-majority setting (Byzantine $\geq 50\%$ ), we use maximum clique-based filter to guarantee global model quality. To the best of our knowledge, this is the first study that uses truth discovery to defend against poisoning attacks. It is also the first scheme which can achieve strong robustness under multiple kinds of attacks launched by high proportion attackers without root datasets. Extensive comparative experiments are designed with five state-of-the-art aggregation rules under five types of classical poisoning attacks on different datasets. The experimental results demonstrate that TDFL is practical and achieves reasonable Byzantine-robustness.},
  archive      = {J_TPDS},
  author       = {Chang Xu and Yu Jia and Liehuang Zhu and Chuan Zhang and Guoxie Jin and Kashif Sharif},
  doi          = {10.1109/TPDS.2022.3205714},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4835-4848},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {TDFL: Truth discovery based byzantine robust federated learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). QoS-aware co-scheduling for distributed long-running
applications on shared clusters. <em>TPDS</em>, <em>33</em>(12),
4818‚Äì4834. (<a href="https://doi.org/10.1109/TPDS.2022.3202493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve a high degree of resource utilization, production clusters need to co-schedule diverse workloads ‚Äì including both batch analytic jobs with short-lived tasks and long-running applications (LRAs) that execute for a long time frame from hours to months ‚Äì onto the shared resources. Microservice architecture advances the manifestation of distributed LRAs (DLRAs), comprising multiple interconnected microservices that are executed in long-lived distributed containers and serve massive user requests. Detecting and mitigating QoS violation become even more intractable due to the network uncertainties and latency propagation across dependent microservices. However, current resource managers are only responsible for resource allocation among applications/jobs but agnostic to runtime QoS such as latency at application level. The state-of-the-art QoS-aware scheduling approaches are dedicated for monolithic applications, without considering the temporal-spatio performance variability across distributed microservices. In this paper, we present Toposch , a new scheduling and execution framework to prioritize the QoS of DLRAs whilst balancing the performance of batch jobs and maintaining high cluster utilization through harvesting idle resources. Toposch tracks footprints of every single request across microservices and uses critical path analysis, based on the end-to-end latency graph, to identify microservices that have high risk of QoS violation. Based on microservice and node level risk assessment, we intervene the batch scheduling by adaptively reducing the visible resources to batch tasks and thus delaying their execution to give way to DLRAs. We propose a prediction-based vertical resource auto-scaling mechanism, with the aid of resource-performance modeling and fine-grained resource inference and access control, for prompt recovery of QoS violation. A cost-effective task preemption is leveraged to ensure a low-cost task preemption and resource reclamation during the auto-scaling. Toposch is integrated with Apache YARN and experiments show that Toposch outperforms other baselines in terms of performance guarantee of DLRAs, at an acceptable cost of batch job slowdown. The tail latency of DLRAs is merely 1.12x of the case of executing alone on average in Toposch with a 26\% JCT increase of Spark analytic jobs.},
  archive      = {J_TPDS},
  author       = {Jianyong Zhu and Renyu Yang and Xiaoyang Sun and Tianyu Wo and Chunming Hu and Hao Peng and Junqing Xiao and Albert Y. Zomaya and Jie Xu},
  doi          = {10.1109/TPDS.2022.3202493},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4818-4834},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {QoS-aware co-scheduling for distributed long-running applications on shared clusters},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ScaleFlux: Efficient stateful scaling in NFV. <em>TPDS</em>,
<em>33</em>(12), 4801‚Äì4817. (<a
href="https://doi.org/10.1109/TPDS.2022.3204209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network function virtualization (NFV) enables elastic scaling to middlebox deployment and management. Therefore, efficient stateful scaling is an important task because operators often need to shift traffic and the associated flow states across VNF instances to deal with time-varying loads. Existing NFV scaling methods, however, typically focus on one aspect of the scaling pipeline and does not offer an end-to-end scaling framework. This article presents ScaleFlux, a complete stateful scaling system that efficiently reduces flow-level latency and achieves near-optimal resource usage. ScaleFlux (1) monitors traffic load for each VNF instance and adopts a queue-based mechanism to detect load burstiness timely, (2) deploys a flow bandwidth predictor to predict flow bandwidth time-series with the ABCNN-LSTM model, and (3) schedules the necessary flow and state migration using the simulated annealing algorithm to achieve both flow-level latency guarantee and resource usage minimization. Testbed evaluation with a five-machine cluster shows that ScaleFlux reduces flow completion time by at least 8.7√ó for all the workloads and achieves near-optimal CPU usage during scaling.},
  archive      = {J_TPDS},
  author       = {Libin Liu and Hong Xu and Zhixiong Niu and Jingzong Li and Wei Zhang and Peng Wang and Jiamin Li and Jason Chun Xue and Cong Wang},
  doi          = {10.1109/TPDS.2022.3204209},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4801-4817},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ScaleFlux: Efficient stateful scaling in NFV},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A decentralized federated learning framework via committee
mechanism with convergence guarantee. <em>TPDS</em>, <em>33</em>(12),
4783‚Äì4800. (<a href="https://doi.org/10.1109/TPDS.2022.3202887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning allows multiple participants to collaboratively train an efficient model without exposing data privacy. However, this distributed machine learning training method is prone to attacks from Byzantine clients, which interfere with the training of the global model by modifying the model or uploading the false gradient. In this article, we propose a novel serverless federated learning framework Committee Mechanism based Federated Learning (CMFL), which can ensure the robustness of the algorithm with convergence guarantee. In CMFL, a committee system is set up to screen the uploaded local gradients. The committee system selects the local gradients rated by the elected members for the aggregation procedure through the selection strategy, and replaces the committee member through the election strategy. Based on the different considerations of model performance and defense, two opposite selection strategies are designed for the sake of both accuracy and robustness. Extensive experiments illustrate that CMFL achieves faster convergence and better accuracy than the typical Federated Learning, in the meanwhile obtaining better robustness than the traditional Byzantine-tolerant algorithms, in the manner of a decentralized approach. In addition, we theoretically analyze and prove the convergence of CMFL under different election and selection strategies, which coincides with the experimental results.},
  archive      = {J_TPDS},
  author       = {Chunjiang Che and Xiaoli Li and Chuan Chen and Xiaoyu He and Zibin Zheng},
  doi          = {10.1109/TPDS.2022.3202887},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4783-4800},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A decentralized federated learning framework via committee mechanism with convergence guarantee},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive federated deep reinforcement learning for proactive
content caching in edge computing. <em>TPDS</em>, <em>33</em>(12),
4767‚Äì4782. (<a href="https://doi.org/10.1109/TPDS.2022.3201983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the aggravation of data explosion and backhaul loads on 5 G edge network, it is difficult for traditional centralized cloud to meet the low latency requirements for content access. The federated learning ( F L)-based p roactive content c aching (FPC) can alleviate the matter by placing content in local cache to achieve fast and repetitive data access while protecting the users‚Äô privacy. However, due to the non-independent and identically distributed (Non-IID) data across the clients and limited edge resources, it is unrealistic for FL to aggregate all participated devices in parallel for model update and adopt the fixed iteration frequency in local training process. To address this issue, we propose a distributed resources-efficient FPC policy to improve the content caching efficiency and reduce the resources consumption. Through theoretical analysis, we first formulate the FPC problem into a stacked autoencoders (SAE) model loss minimization problem while satisfying resources constraint. We then propose an adaptive FPC (AFPC) algorithm combined deep reinforcement learning (DRL) consisting of two mechanisms of client selection and local iterations number decision. Next, we show that when training data are Non-IID, aggregating the model parameters of all participated devices may be not an optimal strategy to improve the FL-based content caching efficiency, and it is more meaningful to adopt adaptive local iteration frequency when resources are limited. Finally, experimental results in three real datasets demonstrate that AFPC can effectively improve cache efficiency up to 38.4 $\%$ and 6.84 $\%$ , and save resources up to 47.4 $\%$ and 35.6 $\%$ , respectively, compared with traditional multi-armed bandit (MAB)-based and FL-based algorithms.},
  archive      = {J_TPDS},
  author       = {Dewen Qiao and Songtao Guo and Defang Liu and Saiqin Long and Pengzhan Zhou and Zhetao Li},
  doi          = {10.1109/TPDS.2022.3201983},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4767-4782},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Adaptive federated deep reinforcement learning for proactive content caching in edge computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Increasing the efficiency of massively parallel sparse
matrix-matrix multiplication in first-principles calculation on the
new-generation sunway supercomputer. <em>TPDS</em>, <em>33</em>(12),
4752‚Äì4766. (<a href="https://doi.org/10.1109/TPDS.2022.3202518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The first-principles approach based on density-functional theory (DFT)/density-functional perturbation theory (DFPT) is widely used in calculations of the systems‚Äô ground state energy, response properties (e.g., polarizability, phonon dispersions) and is playing an increasingly important role in chemistry, physics and materials science. For the large-scale calculations, the computation of the density matrix/response density matrix in DFT/DFPT has become the main performance bottleneck. One of the solutions is using the linear scaling method to get the density matrix and response density matrix. Here a massively parallel medium sparse matrix-matrix multiplication algorithm is designed for first-principle calculations and implemented on the new-generation Sunway supercomputer. Experiments show that the proposed method has obvious performance advantages compared to the original parallel version under moderate sparsity. The computing cores scale to 3,900,000 with strong scalability of 77.3 $\%$ .},
  archive      = {J_TPDS},
  author       = {Xin Chen and Yingxiang Gao and Honghui Shang and Fang Li and Zhiqian Xu and Xin Liu and Dexun Chen},
  doi          = {10.1109/TPDS.2022.3202518},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4752-4766},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Increasing the efficiency of massively parallel sparse matrix-matrix multiplication in first-principles calculation on the new-generation sunway supercomputer},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). COIN: A container workload prediction model focusing on
common and individual changes in workloads. <em>TPDS</em>,
<em>33</em>(12), 4738‚Äì4751. (<a
href="https://doi.org/10.1109/TPDS.2022.3202833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, containers have become the primary deployment form for cloud applications. Predicting container workload accurately is critical to ensure the quality of service (QoS) and cost-efficiency of the applications and meet service level agreements (SLAs) with users. However, facing multiple challenges, including model unavailability due to insufficient data, model maladaptation due to dynamic workload changes, and model non-generalization due to changeable workload patterns in container workload prediction, existing methods have not yet provided a united and effective solution. To this end, we propose a novel integrated forecasting model named COIN that combines COmmon and INdividual changes in container workloads to ensure the availability, adaptivity, and generality of the prediction model based on transfer learning and online learning. Besides, we present a container similarity calculation algorithm for real cloud scenarios, which combines the static and dynamic information of containers and comprehensively depicts the similarity between containers. Through experiments based on two public datasets, the COIN model achieves a higher accuracy than existing state-of-the-art solutions, demonstrating the effectiveness and robustness of our proposed model, which provides a new solution to container workload prediction.},
  archive      = {J_TPDS},
  author       = {Zhijun Ding and Binbin Feng and Changjun Jiang},
  doi          = {10.1109/TPDS.2022.3202833},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4738-4751},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {COIN: A container workload prediction model focusing on common and individual changes in workloads},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parallel dynamic sparse approximate inverse preconditioning
algorithm on GPU. <em>TPDS</em>, <em>33</em>(12), 4723‚Äì4737. (<a
href="https://doi.org/10.1109/TPDS.2022.3202214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamic sparse approximate inverse (SPAI) preconditioner has proven to be effective in accelerating the convergence of iterative methods for large linear systems. Recently, accelerating it on graphics processing unit (GPU) has attracted considerable attention due to the fact that the cost of constructing the preconditioner is high. However, the existing parallel dynamic SPAI preconditioning algorithms on GPU are usually ineffective because of the out-of-memory error for large matrices. This motivates us to investigate how to accelerate the construction of dynamic SPAI preconditioners on GPU. In this article, we propose an efficient dynamic SPAI preconditioning algorithm on GPU, called GDSPAI. For our proposed GDSPAI, there are the following novelties: (1) a well-known dynamic SPAI preconditioning algorithm is substantially modified to address the main challenges of parallelization on GPU, (2) a parallel framework of constructing the dynamic SPAI preconditioner on GPU is presented on the basis of the modified dynamic SPAI preconditioning algorithm; and (3) each component of the preconditioner is computed in parallel inside a group of threads. Experimental results show that the proposed GDSPAI is effective for large matrices, and outperforms the popular preconditioning algorithms in three public libraries, as well as a recent parallel static SPAI preconditioning algorithm.},
  archive      = {J_TPDS},
  author       = {Jiaquan Gao and Xinyue Chu and Xiaotong Wu and Jun Wang and Guixia He},
  doi          = {10.1109/TPDS.2022.3202214},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4723-4737},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Parallel dynamic sparse approximate inverse preconditioning algorithm on GPU},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PayDebt: Reduce buffer occupancy under bursty traffic on
large clusters. <em>TPDS</em>, <em>33</em>(12), 4707‚Äì4722. (<a
href="https://doi.org/10.1109/TPDS.2022.3202504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The average/tail Flow Completion Times (FCTs) are critical to many datacenter applications. Congestion control plays a central role in optimizing FCT. Inappropriate congestion control can exacerbate buffer occupancy, thus hurting the flow performance. Our observations are that current approaches are too aggressive in injecting packets into underlying networks. Instead of handling buffer explosion afterward, we reduce buffer occupancy in the first place. We propose PayDebt, a novel and readily-deployable proactive congestion control protocol. At its heart, a debt mechanism provides bandwidth coordination between the already-buffered and the forthcoming packets. We evaluate PayDebt both in a testbed and large-scale simulations. The buffer occupancy can be decreased by up to 8.0√ó-35.9√ó compared to DCQCN and Homa.},
  archive      = {J_TPDS},
  author       = {Kexin Liu and Chen Tian and Qingyue Wang and Yanqing Chen and Bingchuan Tian and Wenhao Sun and Ke Meng and Long Yan and Lei Han and Jie Fu and Wanchun Dou and Guihai Chen},
  doi          = {10.1109/TPDS.2022.3202504},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4707-4722},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {PayDebt: Reduce buffer occupancy under bursty traffic on large clusters},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimizing DNN compilation for distributed training with
joint OP and tensor fusion. <em>TPDS</em>, <em>33</em>(12), 4694‚Äì4706.
(<a href="https://doi.org/10.1109/TPDS.2022.3201531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes DisCo , an automatic deep learning compilation module for data-parallel distributed training. Unlike most deep learning compilers that focus on training or inference on a single device, DisCo optimizes a DNN model for distributed training over multiple GPU machines. Existing single-device compilation strategies do not work well in distributed training, due mainly to communication inefficiency that they incur. DisCo generates optimized, joint computation operator and communication tensor fusion strategies to enable highly efficient distributed training. A GNN-based simulator is built to effectively estimate per-iteration training time achieved by operator/tensor fusion candidates. A backtracking search algorithm is driven by the simulator, navigating efficiently in the large strategy space to identify good operator/tensor fusion strategies that minimize distributed training time. We compare DisCo with existing DL fusion schemes and show that it achieves good training speed-up close to the ideal, full computation-communication overlap case.},
  archive      = {J_TPDS},
  author       = {Xiaodong Yi and Shiwei Zhang and Lansong Diao and Chuan Wu and Zhen Zheng and Shiqing Fan and Siyu Wang and Jun Yang and Wei Lin},
  doi          = {10.1109/TPDS.2022.3201531},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4694-4706},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimizing DNN compilation for distributed training with joint OP and tensor fusion},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robustness of subsystem reliability of <span
class="math inline"><em>k</em></span>k-ary <span
class="math inline"><em>n</em></span>n-cube networks under probabilistic
fault model. <em>TPDS</em>, <em>33</em>(12), 4684‚Äì4693. (<a
href="https://doi.org/10.1109/TPDS.2022.3199251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of the Big Data era, as multiprocessor systems consisting of multiple processors play a vital role in big data analytics, we are prompted to explore the qualitative and quantitative metric to characterize the reliability of the systems. As the size of the multiprocessor systems grows, the probability of the occurrence of failing processors increases. One metric of the macroscopic reliability of a system is the measure of the collective effect when its subsystems are out of function. The subsystem reliability of a system is the quantitative metric that a fault-free subsystem of specific size is operational as before with the occurrence of individual faults. Although some networks have the same order and similar topologies, there are differences in their subsystem reliabilities. In this work, we focus on the comparison of two distinct topologies of $k$ -ary $n$ -cube networks with the same order and calculate the robustness of reliability bounds of $k$ -ary $n$ -cube networks. We analytically show that the subsystem reliability is negatively correlated with the dimension $n$ , even if two subsystems of $Q_{n}^{k}$ are of the same order. That is, the smaller $n$ is, the larger subsystem reliability of $Q_{n}^{k}$ will be. This work provides a theoretical methodology to choose the more dependable topology of $k$ -ary $n$ -cube networks with the same order. Finally, we apply some numerical simulations to validate the results we established.},
  archive      = {J_TPDS},
  author       = {Xiaoqing Liu and Shuming Zhou and Sun-Yuan Hsieh and Hong Zhang},
  doi          = {10.1109/TPDS.2022.3199251},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4684-4693},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Robustness of subsystem reliability of $k$k-ary $n$n-cube networks under probabilistic fault model},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Theoretical analysis of an adaptive periodic multi
installment scheduling with result retrieval for SAR image processing.
<em>TPDS</em>, <em>33</em>(12), 4672‚Äì4683. (<a
href="https://doi.org/10.1109/TPDS.2022.3194542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing a large-scale Synthetic Aperture Radar (SAR) image dataset on a distributed computing infrastructure poses a challenging problem. Large-scale load distribution strategies like multi-installment scheduling (MIS) assume that the size of the result is negligible compared to the input workloads and hence ignore it in their design. Similarly, numerical methods like particle swarm optimization and their variants are not practical for real-time applications, given their run-time complexities. As both the results retrieval and completion time are crucial for SAR image data processing, in this article, we attempt to provide a thorough theoretical analysis of an adaptive MIS that includes the result retrieval phase. We use the periodic nature of the internal installments to keep the strategy simple and fine-tune the last installment to avoid any idle times in the processors. We derive a closed-form solution for the load fractions and hence, the overall processing time, schedule feasibility criteria, and certain other properties that lead to adaptive scheduling. Finally, we validate our theoretical findings through rigorous simulation studies using a loosely connected virtual machines (VMs) topology for the SAR dataset.},
  archive      = {J_TPDS},
  author       = {Gokul Madathupalyam Chinnappan and Bharadwaj Veeravalli},
  doi          = {10.1109/TPDS.2022.3194542},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4672-4683},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Theoretical analysis of an adaptive periodic multi installment scheduling with result retrieval for SAR image processing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tenant-grained request scheduling in software-defined cloud
computing. <em>TPDS</em>, <em>33</em>(12), 4654‚Äì4671. (<a
href="https://doi.org/10.1109/TPDS.2022.3199031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud providers host various services for tenants‚Äô requests (e.g., software-as-a-service) and seek to serve as many requests as possible for revenue maximization. Considering a large number of requests, the previous works on fine-grained request scheduling may lead to poor system scalability (or high schedule overhead) and break tenant isolation. In this article, we design a tenant-grained request scheduling framework to conquer the above two disadvantages. We formulate the tenant-grained request scheduling problem as an integer linear programming and prove its NP-hardness. We consider two complementary cases: the offline case (where we know all request demands in advance), and the online case (where we have to make immediate scheduling decisions for requests arriving online). A normalization-based algorithm with an approximation factor of $ {O}(1)$ is proposed to solve the offline problem and a primal-dual-based algorithm with a competitive ratio of $[(1-\epsilon), {O}(\log 3\cdot n+\log (1/\epsilon))]$ is designed for the online scenario, where $\epsilon \in (0,1)$ and $n$ is the number of racks in the cloud. We also discuss how to integrate our proposed algorithms with the previous (fine-grained) request scheduling mechanism. Extensive simulation and experiment results show that our algorithms can obtain significant performance gains, e.g., the online algorithm reduces the scheduler&#39;s overhead more than $90\%$ and achieves tenant isolation, while obtaining similar network performance (e.g., throughput) compared with the fine-grained request scheduling methods.},
  archive      = {J_TPDS},
  author       = {Huaqing Tu and Gongming Zhao and Hongli Xu and Xianjin Fang},
  doi          = {10.1109/TPDS.2022.3199031},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4654-4671},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Tenant-grained request scheduling in software-defined cloud computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A parallel secure flow control framework for private data
sharing in mobile edge cloud. <em>TPDS</em>, <em>33</em>(12), 4638‚Äì4653.
(<a href="https://doi.org/10.1109/TPDS.2022.3200959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the rapid development of edge computing is accelerating the data sharing between cloud computing platforms and mobile users. These data often contain sensitive information, which faces severe leakage risks not only from the semi-trusted cloud servers but also from the malicious senders in the organizations. Fortunately, access control encryption (ACE) has been utilized to secure the data with access control policies, in which a sanitizer (e.g., the edge node) is employed to check all the communications between the sender and receiver, and drop illegal ciphertexts according to the access control policy. However, previous schemes have some limitations in mobile edge cloud, e.g., the sender&#39;s attributes are not strictly authenticated in the attribute-based access control policy, or the sanitization time is the bottleneck of fast data sharing. To this end, we introduce PSFlow, a parallel secure flow control framework for private data sharing in mobile edge cloud. First, we propose an attribute-based outsourced ACE (AOACE) scheme, which achieves secure fine-grained data read and write control, and reduces the computational cost of the sender and receiver with outsourced computations in edge nodes. Then, we propose a concrete construction of PSFlow from AOACE, and accelerate the sanitization process with parallel computing. Specifically, PSFlow parallelizes the sanitization operations with a multi-server model in each edge node, and optimizes the sanitization efficiency in each edge server by constructing a shared pool from the attribute universe. The experimental results show that PSFlow is more efficient and practical than previous schemes in mobile edge cloud.},
  archive      = {J_TPDS},
  author       = {Qinlong Huang and Lixuan Chen and Chao Wang},
  doi          = {10.1109/TPDS.2022.3200959},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4638-4653},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A parallel secure flow control framework for private data sharing in mobile edge cloud},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PS+: A simple yet effective framework for fast training on
parameter server. <em>TPDS</em>, <em>33</em>(12), 4625‚Äì4637. (<a
href="https://doi.org/10.1109/TPDS.2022.3200518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In distributed training, workers collaboratively refine the global model parameters by pushing their updates to the Parameter Server and pulling fresher parameters for the next iteration. This introduces high communication costs for training at scale, and incurs unproductive waiting time for workers. To minimize the waiting time, existing approaches overlap communication and computation for deep neural networks. Yet, these techniques not only require the layer-by-layer model structures, but also need significant efforts in runtime profiling and hyperparameter tuning. To make the overlapping optimization simple and generic , in this article, we propose a new Parameter Server framework. Our solution decouples the dependency between push and pull operations, and allows workers to eagerly pull the global parameters. This way, both push and pull operations can be easily overlapped with computations. Besides, the overlapping manner offers a different way to address the straggler problem, where the stale updates greatly retard the training process. In the new framework, with adequate information available to workers, they can explicitly modulate the learning rates for their updates. Thus, the global parameters can be less compromised by stale updates. We implement a prototype system in PyTorch and demonstrate its effectiveness on both CPU/GPU clusters. Experimental results show that our prototype saves up to 54\% less time for each iteration and up to 37\% fewer iterations for model convergence, achieving up to 2.86√ó speedup over widely-used synchronization schemes.},
  archive      = {J_TPDS},
  author       = {A-Long Jin and Wenchao Xu and Song Guo and Bing Hu and Kwan Yeung},
  doi          = {10.1109/TPDS.2022.3200518},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4625-4637},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {PS+: A simple yet effective framework for fast training on parameter server},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BARM: A batch-aware resource manager for boosting multiple
neural networks inference on GPUs with memory oversubscription.
<em>TPDS</em>, <em>33</em>(12), 4612‚Äì4624. (<a
href="https://doi.org/10.1109/TPDS.2022.3199806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern intelligent devices usually execute multiple neural networks to improve service quality. However, system performance degrades significantly when the working set exceeds the physical memory capability, a phenomenon called memory oversubscription. To support the execution of multiple independent neural networks with limited physical memory, this article explores resource management in GPUs with unified virtual memory and demand paging. We first analyze the relationship between the simultaneous execution of multiple neural networks from streaming multiprocessors (SM) assignment and page fault overhead from memory thrashing. To boost performance by reducing the page fault penalty, we propose a batch-aware resource management approach, BARM, including (1) batch-aware SM resource allocation to increase the batch size and (2) thrashing-preventing memory allocation to eliminate run-time thrashing. The performance of the proposed method was evaluated using a series of workloads, and response latency is reduced significantly over the state-of-the-art page fault prefetcher and batch-aware TLP management. The proposed framework was also implemented on the real platform and evaluated by a case study, and impressive results were obtained.},
  archive      = {J_TPDS},
  author       = {Zhao-Wei Qiu and Kun-Sheng Liu and Ya-Shu Chen},
  doi          = {10.1109/TPDS.2022.3199806},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4612-4624},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {BARM: A batch-aware resource manager for boosting multiple neural networks inference on GPUs with memory oversubscription},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fairness-aware VNF sharing and rate coordination for high
efficient service scheduling. <em>TPDS</em>, <em>33</em>(12), 4597‚Äì4611.
(<a href="https://doi.org/10.1109/TPDS.2022.3199392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network service provisioning becomes flexible and programmable with the help of Network Function Virfitualization (NFV), since NFV abstracts various service functions into software components called Virtual Network Function (VNF) and VNFs can be flexibly and quickly composed to form new services. It is commonly known that sharing the same VNF among different services can improve the resource utilization. However, we should be aware that such sharing also leads to serious resource preemption. In addition, VNF sharing aggravates the generation of the performance bottleneck, which then causes the rate mismatch problem between the upstream and downstream VNFs belonging to the same service chain. In this article, we propose a dynamic and flexible algorithm to jointly address the VNF sharing resource allocation and the rate coordination between the upstream and downstream VNFs. Specifically, 1) the VNFs are shared among different service chains with a fairness factor considered for the purpose of reducing the resource preemption probability and improving the resource utilization; 2) the backpressure indicator of each VNF is defined to judge its pressure condition, based on which we can dynamically adjust the processing rates between it and its downstream or upstream VNFs by maximizing the idle resource utilization. The experimental results indicate that the proposed algorithm outperforms the other methods in terms of the average delay, the flow completion time, the throughput and the backlog, etc. Meanwhile, the proposed algorithm achieves more stable performance than the other methods.},
  archive      = {J_TPDS},
  author       = {Bo Yi and Xingwei Wang and Min Huang and Sajal K. Das and Keqin Li},
  doi          = {10.1109/TPDS.2022.3199392},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4597-4611},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Fairness-aware VNF sharing and rate coordination for high efficient service scheduling},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint application placement and request routing optimization
for dynamic edge computing service management. <em>TPDS</em>,
<em>33</em>(12), 4581‚Äì4596. (<a
href="https://doi.org/10.1109/TPDS.2022.3195205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As mobile edge computing (MEC) hosting applications at the network edge with limited capacities, service providers are facing the new challenge of how to make full use of the scarce edge resources to maximize the system performance. Accommodating this challenge requires careful application placement and request routing to coordinate diverse MEC nodes. However, frequent application re-placement would greatly increase the system reconfiguration cost, indicating a performance-cost trade-off. In response, in this paper, we study the problem of joint optimization on application placement and request routing to maximize the system performance, under a long-term budget of the application reconfiguration cost. Solving this problem is non-trivial since the long-term budget is coupled with the future system states (e.g., user request arrivals) that are typically unpredictable. To address this challenge, we first advocate an approximated dynamic optimization framework to decompose the long-term optimization problem into a series of one-shot problems which do not require the future system states. Moreover, since the decomposed problem is a mixed integer linear program (MILP) which is proven to be NP-hard, we then devise an efficient dependent rounding based approximation algorithm, which can achieve the near-optimal performance in a fast manner. Both rigorous theoretical analysis and extensive trace-driven evaluations demonstrate the proposed framework can achieve superior performance gain over existing schemes.},
  archive      = {J_TPDS},
  author       = {Rui Li and Zhi Zhou and Xiaoxi Zhang and Xu Chen},
  doi          = {10.1109/TPDS.2022.3195205},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4581-4596},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Joint application placement and request routing optimization for dynamic edge computing service management},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Elastic and reliable bandwidth reservation based on
distributed traffic monitoring and control. <em>TPDS</em>,
<em>33</em>(12), 4563‚Äì4580. (<a
href="https://doi.org/10.1109/TPDS.2022.3196840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bandwidth reservation can effectively improve the service quality for data transfers because of dedicated network resources. However, it is difficult to achieve a desired tradeoff between resource utilization and reliable bandwidth guarantees for data transfers with time-varying traffic. In this article, we study a novel bandwidth reservation solution based on distributed traffic monitoring and control for applications that require reliable bandwidth guarantees. In the proposed solution, designated bandwidth is allocated for an application in advance according to its maximum traffic peak, and idle reserved bandwidth resources are dynamically shared according to regular traffic. Dynamic resource sharing evidently improves resource utilization and effectively eliminates the potential congestion caused by sudden traffic bursts. To ensure that the congestion that occurs occasionally can dissipate rapidly, our solution monitors and manages traffic by a distributed monitoring and control strategy. Hence, we study a delay-constrained and proxy-assisted traffic monitoring structure construction problem and propose an algorithm to solve it. The proposed algorithm can also be used to build a delay-constrained traffic control structure. In addition to the above algorithm, we propose a dynamic traffic control algorithm that can achieve a desirable tradeoff between resource utilization and congestion avoidance capability.},
  archive      = {J_TPDS},
  author       = {Xinchang Zhang and Tianyi Wang},
  doi          = {10.1109/TPDS.2022.3196840},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4563-4580},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Elastic and reliable bandwidth reservation based on distributed traffic monitoring and control},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bi-objective learn-and-deploy scheduling method for bursty
and stochastic requests on heterogeneous cloud servers. <em>TPDS</em>,
<em>33</em>(12), 4547‚Äì4562. (<a
href="https://doi.org/10.1109/TPDS.2022.3196475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider the dynamic allocation of bursty requests stochastically arriving at heterogeneous servers with uncertain setup times. Lower expected response time and less power consumption are desirable objectives of users and service providers respectively. However, sudden increase and decrease of cloud servers caused by bursty requests are rather challenging to get an appropriate trade-off between the two conflicting objectives which are closely related to the launched servers. The heterogeneity of the cloud servers further makes it more difficult to decide how to switch on and off servers and effectively and efficiently allocate bursty requests with balanced objectives. Based on a Markov decision process, a real-time bilevel decision-making model is constructed for unallocated requests which includes: whether to launch a server and which type of server to launch. A learn-and-deploy algorithm framework is proposed which contains two complementary stages. In the first stage, an effective offline bi-objective optimization algorithm is proposed to learn a set of policies, which provides helpful trade-off information for a decision-maker to choose a preferred policy a posteriori . In terms of the system status, a policy decides whether to launch a server according to a state-action table and which server to launch using a server priority sequence. In the second stage, a computationally efficient policy deployment method is proposed to search the corresponding action in the selected policy based on the current system status and apply it to the real-time system. Experimental studies over a large number of random and real instances have been conducted to validate the effectiveness of the proposed bilevel model and algorithm. Compared to the most recent existing method, the performance of the proposed approach can at most achieve an 80\% improvement on power consumption and 20\% improvement on response time.},
  archive      = {J_TPDS},
  author       = {Xinye Cai and Haiyang Xu and Xiaoping Li and Kang Wang and Long Chen and Rub√©n Ruiz Garc√≠a and Qingfu Zhang},
  doi          = {10.1109/TPDS.2022.3196475},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4547-4562},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A bi-objective learn-and-deploy scheduling method for bursty and stochastic requests on heterogeneous cloud servers},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving cache utilization of nested parallel programs by
almost deterministic work stealing. <em>TPDS</em>, <em>33</em>(12),
4530‚Äì4546. (<a href="https://doi.org/10.1109/TPDS.2022.3196192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nested (fork-join) parallelism eases parallel programming by enabling high-level expression of parallelism and leaving the mapping between parallel tasks and hardware to the runtime scheduler. A challenge in dynamic scheduling of nested parallelism is how to exploit data locality, which has become more demanding in the deep cache hierarchies of modern processors with a large number of cores. This paper introduces almost deterministic work stealing (ADWS) , which efficiently exploits data locality by deterministically planning a cache-hierarchy-aware schedule, while allowing a little scheduling variety to facilitate dynamic load balancing. Furthermore, we propose an extension of our prior work on ADWS to achieve better shared cache utilization. The improved version of the scheduler is called multi-level ADWS . The idea is that only part of a computation whose working set size is small enough to fit into a shared cache is scheduled by ADWS within the cache recursively, thus avoiding excessive capacity misses. Our evaluation on a benchmark of parallel decision tree construction demonstrated that multi-level ADWS outperformed the conventional random work stealing of Cilk Plus by 61\% and it showed a 40\% performance improvement over the previous ADWS design.},
  archive      = {J_TPDS},
  author       = {Shumpei Shiina and Kenjiro Taura},
  doi          = {10.1109/TPDS.2022.3196192},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4530-4546},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Improving cache utilization of nested parallel programs by almost deterministic work stealing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving federated learning with quality-aware user
incentive and auto-weighted model aggregation. <em>TPDS</em>,
<em>33</em>(12), 4515‚Äì4529. (<a
href="https://doi.org/10.1109/TPDS.2022.3195207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning enables distributed model training over various computing nodes, e.g., mobile devices, where instead of sharing raw user data, computing nodes can solely commit model updates without compromising data privacy. The quality of federated learning relies on the model updates contributed by computing nodes training with their local data. However, with various factors (e.g., training data size, mislabeled data samples, skewed data distributions), the model update qualities of computing nodes can vary dramatically, while inclusively aggregating low-quality model updates can deteriorate the global model quality. To achieve efficient federated learning, in this paper, we propose a novel framework named FAIR , i.e., F ederated le A rning with qual I ty awa R eness. Particularly, FAIR integrates three major components: 1) learning quality estimation: we adopt the model aggregation weight (learned in the third component) to reversely quantify the individual learning quality of nodes in a privacy-preserving manner, and leverage the historical learning records to infer the next-round learning quality; 2) quality-aware incentive mechanism: within the recruiting budget, we model a reverse auction problem to stimulate the participation of high-quality and low-cost computing nodes, and the method is proved to be truthful, individually rational, and computationally efficient; and 3) auto-weighted model aggregation: based on the gradient descent method, we devise an auto-weighted model aggregation algorithm to automatically learn the optimal aggregation weights to further enhance the global model quality. Based on real-world datasets and learning tasks, extensive experiments are conducted to demonstrate the efficacy of FAIR .},
  archive      = {J_TPDS},
  author       = {Yongheng Deng and Feng Lyu and Ju Ren and Yi-Chao Chen and Peng Yang and Yuezhi Zhou and Yaoxue Zhang},
  doi          = {10.1109/TPDS.2022.3195207},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4515-4529},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Improving federated learning with quality-aware user incentive and auto-weighted model aggregation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HiTDL: High-throughput deep learning inference at the hybrid
mobile edge. <em>TPDS</em>, <em>33</em>(12), 4499‚Äì4514. (<a
href="https://doi.org/10.1109/TPDS.2022.3195664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have become a critical component for inference in modern mobile applications, but the efficient provisioning of DNNs is non-trivial. Existing mobile- and server-based approaches compromise either the inference accuracy or latency. Instead, a hybrid approach can reap the benefits of the two by splitting the DNN at an appropriate layer and running the two parts separately on the mobile and the server respectively. Nevertheless, the DNN throughput in the hybrid approach has not been carefully examined, which is particularly important for edge servers where limited compute resources are shared among multiple DNNs. This article presents HiTDL, a runtime framework for managing multiple DNNs provisioned following the hybrid approach at the edge. HiTDL&#39;s mission is to improve edge resource efficiency by optimizing the combined throughput of all co-located DNNs, while still guaranteeing their SLAs. To this end, HiTDL first builds comprehensive performance models for DNN inference latency and throughout with respect to multiple factors including resource availability, DNN partition plan, and cross-DNN interference. HiTDL then uses these models to generate a set of candidate partition plans with SLA guarantees for each DNN. Finally, HiTDL makes global throughput-optimal resource allocation decisions by selecting partition plans from the candidate set for each DNN via solving a fairness-aware multiple-choice knapsack problem. Experimental results based on a prototype implementation show that HiTDL improves the overall throughput of the edge by $4.3\times$ compared with the state-of-the-art.},
  archive      = {J_TPDS},
  author       = {Jing Wu and Lin Wang and Qiangyu Pei and Xingqi Cui and Fangming Liu and Tingting Yang},
  doi          = {10.1109/TPDS.2022.3195664},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4499-4514},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HiTDL: High-throughput deep learning inference at the hybrid mobile edge},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerating tensor swapping in GPUs with self-tuning
compression. <em>TPDS</em>, <em>33</em>(12), 4484‚Äì4498. (<a
href="https://doi.org/10.1109/TPDS.2022.3193867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data swapping between CPUs and GPUs is widely used to address the GPU memory shortage issue when training deep neural networks (DNNs) requiring a larger amount of memory than that a GPU may have. Data swapping may become a bottleneck when its latency is longer than the latency of DNN computations. Tensor compression in GPUs can reduce the data swapping time. However, existing works on compressing tensors in the virtual memory of GPUs have three major issues: lack of portability because its implementation requires additional (de)compression units in memory controllers, sub-optimal compression performance for varying tensor compression ratios and sizes, and poor adaptation to dense tensors because they only focus on sparse tensors. We propose a self-tuning tensor compression framework, named CSwap+ , for improving the virtual memory management of GPUs. It uses GPUs for (de)compression directly and thus has high portability and is minimally dependent on GPU architecture features. Furthermore, it only applies compression on tensors that are deemed to be cost-effective considering their compression ratio, size, and the characteristics of compression algorithms at runtime. Finally, to adapt to DNN models with dense tensors, it also supports cost-effective lossy compression for dense tensors with nearly no model training accuracy degradation. We conduct the experiments through six representative memory-intensive DNN models. Compared to vDNN, CSwap+ reduces tensor swapping latency by up to 50.9\% and 46.1\% with NVIDIA V100 GPU, for DNN models with sparse and dense tensors, respectively.},
  archive      = {J_TPDS},
  author       = {Ping Chen and Shuibing He and Xuechen Zhang and Shuaiben Chen and Peiyi Hong and Yanlong Yin and Xian-He Sun},
  doi          = {10.1109/TPDS.2022.3193867},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4484-4498},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating tensor swapping in GPUs with self-tuning compression},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gaviss: Boosting the performance of GPU-accelerated NFV
systems via data sharing. <em>TPDS</em>, <em>33</em>(12), 4472‚Äì4483. (<a
href="https://doi.org/10.1109/TPDS.2022.3193368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GPUs have demonstrated the capability of significantly improving the performance of network functions (NF). In an Network Function Virtualization (NFV) system, multiple NFs form a service chain to provide services. However, NFs in state-of-the-art GPU-accelerated NFV systems still utilize a GPU independently where each NF needs to transfer data to the GPU memory for acceleration. As a result, a packet might be transferred into the GPU memory by each NF when it passes through the service chain. We find these expensive and repetitive transfers are the main factor that limits the overall performance of an NFV system. We propose Gaviss, a GPU-accelerated NFV system with effective data sharing. By sharing packets in the GPU memory among network functions, a packet needs to be transferred to the GPU only once, eliminating the performance overhead caused by repetitive transfers. Extensive experimental results show that Gaviss can improve the overall throughput by 2.6-13.2√ó and reduce the latency by up to 37.9\%, when compared with state-of-the-art approaches. Moreover, Gaviss also demonstrates up to 2.5√ó higher price-performance ratio than CPU-based implementations, making GPUs competitive for building NFV systems.},
  archive      = {J_TPDS},
  author       = {Liangchen Guo and Kai Zhang and X. Sean Wang},
  doi          = {10.1109/TPDS.2022.3193368},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4472-4483},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Gaviss: Boosting the performance of GPU-accelerated NFV systems via data sharing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Redesigning and optimizing UCSF DOCK3.7 on sunway
TaihuLight. <em>TPDS</em>, <em>33</em>(12), 4458‚Äì4471. (<a
href="https://doi.org/10.1109/TPDS.2022.3194916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular docking is the process of posing, scoring, and ranking small molecules at the binding sites of proteins to prioritize compounds for experimental testing. It is a widely-used computational method in the drug discovery process. However, it is a highly time-consuming procedure since a receptor may need to find favorable ligand orientations in billions of ligands. UCSF DOCK3.7 is one of the most widely used molecular docking applications. In this paper, we port and optimize UCSF DOCK3.7 on the Sunway TaihuLight supercomputer. To avoid the impact of load imbalance, we employ a producer-consumer strategy that can overlap I/O and computation in order to achieve high performance. Furthermore, we present a new binary file format to replace the mol2db2 file format for ligand storage and adopt xzip rather than gzip to compress ligand files. We show that our file format can reduce I/O time significantly while xzip saves significant storage. For the routines which determine the orientation of a ligand relative to the receptor, we present an improved algorithm to discard geometrically similar orientations. Furthermore, we fuse loops and compress memory usage to store data in fast Local Device Memory (LDM) in order to score ligand orientations with high efficiency. In addition, we propose a number of architecture-specific optimizations. Asynchronous data transfer and vectorization of computation are implemented to take full advantage of the SW26010 processor. Our experiments show that a speedup of 167 can be achieved by using the proposed strategies. Compared to a core of an Intel(R) Core(TM) i9-10900K CPU, our approach achieves speedups of 15 on a SW26010 core group. Furthermore, our implementation achieves strong scalability to hundreds of thousands of heterogeneous cores on the next-generation Sunway supercomputer.},
  archive      = {J_TPDS},
  author       = {Kai Xu and Jinxiao Zhang and Xiaohui Duan and Xiaobo Wan and Niu Huang and Bertil Schmidt and Weiguo Liu and Guangwen Yang},
  doi          = {10.1109/TPDS.2022.3194916},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4458-4471},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Redesigning and optimizing UCSF DOCK3.7 on sunway TaihuLight},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimizing error-bounded lossy compression for scientific
data with diverse constraints. <em>TPDS</em>, <em>33</em>(12),
4440‚Äì4457. (<a href="https://doi.org/10.1109/TPDS.2022.3194695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vast volumes of data are produced by today&#39;s scientific simulations and advanced instruments. These data cannot be stored and transferred efficiently because of limited I/O bandwidth, network speed, and storage capacity. Error-bounded lossy compression can be an effective method for addressing these issues: not only can it significantly reduce data size, but it can also control the data distortion based on user-defined error bounds. In practice, many scientific applications have specific requirements or constraints for lossy compression, in order to guarantee that the reconstructed data are valid for post hoc analysis. For example, some datasets contain irrelevant data that should be isolated in particular and users often have intuition regarding value ranges, geospatial regions, and other data subsets that are crucial for subsequent analysis. Existing state-of-the-art error-bounded lossy compressors, however, do not consider these constraints during compression, resulting in inferior compression ratios with respect to user&#39;s post hoc analysis, due to the fact that the data itself provides little or no value for post hoc analysis. In this work we address this issue by proposing an optimized framework that can preserve diverse constraints during the error-bounded lossy compression, e.g., cleaning the irrelevant data, efficiently preserving different precision for multiple value intervals, and allowing users to set diverse precision over both regular and irregular regions. We perform our evaluation on a supercomputer with up to 2,100 cores. Experiments with six real-world applications show that our proposed diverse constraints based error-bounded lossy compressor can obtain a higher visual quality or data fidelity on reconstructed data with the same or even higher compression ratios compared with the traditional state-of-the-art compressor SZ. Our experiments also demonstrate very good scalability in compression performance compared with the I/O throughput of the parallel file system.},
  archive      = {J_TPDS},
  author       = {Yuanjian Liu and Sheng Di and Kai Zhao and Sian Jin and Cheng Wang and Kyle Chard and Dingwen Tao and Ian Foster and Franck Cappello},
  doi          = {10.1109/TPDS.2022.3194695},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4440-4457},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimizing error-bounded lossy compression for scientific data with diverse constraints},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enabling large scale simulations for particle accelerators.
<em>TPDS</em>, <em>33</em>(12), 4425‚Äì4439. (<a
href="https://doi.org/10.1109/TPDS.2022.3192707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {International high-energy particle physics research centers, like CERN and Fermilab, require excessive studies and simulations to plan for the upcoming upgrades of the world&#39;s largest particle accelerators, and the design of future machines given the technological challenges and tight budgetary constraints. The Beam Longitudinal Dynamics (BLonD) simulator suite incorporates the most detailed and complex physics phenomena in the field of longitudinal beam dynamics, required for providing extremely accurate predictions. Modern challenges in beam dynamics dictate for longer, larger and numerous simulation studies to draw meaningful conclusions that will drive the baseline choices for the daily operation of current machines and the design choices of future projects. These studies are extremely time consuming, and would be impractical to perform without a High-Performance Computing oriented simulator framework. In this article, at first, we design and evaluate a highly-optimized distributed version of BLonD. We combine approximate computing techniques, and leverage a dynamic load-balancing scheme to relax synchronization and improve scalability. In addition, we employ GPUs to accelerate the distributed implementation. We evaluate the highly optimized distributed beam longitudinal dynamics simulator in a supercomputing system and demonstrate speedups of more than two orders of magnitude when run on 32 GPU platforms, w.r.t. the previous state-of-art. By driving a wide range of new studies, the proposed high performance beam longitudinal dynamics simulator forms an invaluable tool for accelerator physicists.},
  archive      = {J_TPDS},
  author       = {Konstantinos Iliakis and Helga Timko and Sotirios Xydis and Panagiotis Tsapatsaris and Dimitrios Soudris},
  doi          = {10.1109/TPDS.2022.3192707},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4425-4439},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Enabling large scale simulations for particle accelerators},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Power-aware checkpointing for multicore embedded systems.
<em>TPDS</em>, <em>33</em>(12), 4410‚Äì4424. (<a
href="https://doi.org/10.1109/TPDS.2022.3188568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing the number of cores integrated on a single chip offers a great potential for the implementation of fault-tolerant techniques to achieve high reliability in real-time embedded systems. Checkpointing with rollback-recovery is a well-established technique to tolerate transient faults in multicore platforms. To consider the worst-case fault occurrence scenario, checkpointing technique requires to re-execute some parts of the tasks, and that might lead to simultaneous execution of task parts with high power consumptions, which eventually might result in a peak power increase beyond the thermal design power (TDP). Exceeding TDP can elevate on-chip temperatures beyond safe limits, and thereby triggering countermeasures that throttle down the voltage and frequency levels or power gate the cores. Such countermeasures might lead to violating task deadlines and degrading the system&#39;s reliability. To avoid such severe scenarios, it is inevitable to consider the impact of applying fault-tolerant techniques on the power consumption and prevent violating the power constraint of the chip, i.e., TDP. This article presents for the first time, a peak-power-aware checkpointing (PPAC) technique that tolerates a given number of faults, k , while at the same time meets the power constraint in hard real-time embedded systems. To do this, our proposed technique (PPAC) adjusts the timing of the checkpoints, which have lower power consumption than the tasks to the execution time points that have power spikes beyond TDP. Moreover, PPAC exploits the available slack times on the cores to delay the execution of some tasks to avoid the remaining power spikes beyond TDP, which could not be mitigated by solely adjusting checkpoints. To evaluate our technique, we extend the state-of-the-art system-level simulator, gem5, with the state-of-the-art checkpointing module in Linux. Our experimental results show that our proposed technique is able to tolerate a given number of faults without exceeding the timing and power constraints in hard real-time embedded systems. The resulting peak power reduction achieved by our technique compared to state-of-the-art techniques is an average of 23\%. Moreover, our technique employs the Dynamic Power Management (DPM) during the slack times resulting at runtime in the case of fault-free scenarios, which provides energy savings with an average of 17.28\% and up to 61.1\%.},
  archive      = {J_TPDS},
  author       = {Mohsen Ansari and Sepideh Safari and Heba Khdr and Pourya Gohari-Nazari and J√∂rg Henkel and Alireza Ejlali and Shaahin Hessabi},
  doi          = {10.1109/TPDS.2022.3188568},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4410-4424},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Power-aware checkpointing for multicore embedded systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey of storage systems in the RDMA era. <em>TPDS</em>,
<em>33</em>(12), 4395‚Äì4409. (<a
href="https://doi.org/10.1109/TPDS.2022.3188656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote Direct Memory Access (RDMA) based network devices are increasingly being deployed in modern data centers. RDMA brings significant performance improvements over traditional network devices such as Ethernet due to its unique features: protocol offloading and memory semantics . In particular, it can achieve microsecond level latency, which is about 2 $\sim$ 3 orders of magnitude improvement. With such improvement in hardware, the software stack, including device drivers and programming libraries, is becoming a new performance bottleneck. Developers need to use new programming libraries to take full advantage of the performance of the underlying hardware. Storage systems are very important in modern data centers. This article surveys the current efforts to use RDMA for optimizing storage systems. We first present five classes of RDMA-based storage systems, including key-value stores, file systems, distributed memory systems, database systems, and systems using smart NICs, to demonstrate different design choices. Then, we examine the core modules of storage systems from different perspectives: communication mode, concurrency control, fault tolerance, caching, and resource management. Finally, we provide some design guidelines for new RDMA-based storage systems, as well as a discussion of opportunities and challenges.},
  archive      = {J_TPDS},
  author       = {Shaonan Ma and Teng Ma and Kang Chen and Yongwei Wu},
  doi          = {10.1109/TPDS.2022.3188656},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4395-4409},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A survey of storage systems in the RDMA era},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated scheduling algorithm selection and chunk parameter
calculation in OpenMP. <em>TPDS</em>, <em>33</em>(12), 4383‚Äì4394. (<a
href="https://doi.org/10.1109/TPDS.2022.3189270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing node and cores-per-node counts in supercomputers render scheduling and load balancing critical for exploiting parallelism. OpenMP applications can achieve high performance via careful selection of scheduling kind and chunk parameters on a per-loop, per-application, and per-system basis from a portfolio of advanced scheduling algorithms (Kornd√∂rfer et al. , 2022). This selection approach is time-consuming, challenging, and may need to change during execution. We propose Auto4OMP , a novel approach for automated load balancing of OpenMP applications. With Auto4OMP, we introduce three scheduling algorithm selection methods and an expert-defined chunk parameter for OpenMP&#39;s schedule clause&#39;s kind and chunk , respectively. Auto4OMP extends the OpenMP schedule(auto) and chunk parameter implementation in LLVM&#39;s OpenMP runtime library to automatically select a scheduling algorithm and calculate a chunk parameter during execution. Loop characteristics are inferred in Auto4OMP from the loop execution over the application&#39;s time-steps. The experiments performed in this work show that Auto4OMP improves applications performance by up to $11\%$ compared to LLVM&#39;s schedule(auto) implementation and outperforms manual selection. Auto4OMP improves MPI+OpenMP applications performance by explicitly minimizing thread- and implicitly reducing process-load imbalance.},
  archive      = {J_TPDS},
  author       = {Ali Mohammed and Jonas H. M√ºller Kornd√∂rfer and Ahmed Eleliemy and Florina M. Ciorba},
  doi          = {10.1109/TPDS.2022.3189270},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4383-4394},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Automated scheduling algorithm selection and chunk parameter calculation in OpenMP},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HEROv2: Full-stack open-source research platform for
heterogeneous computing. <em>TPDS</em>, <em>33</em>(12), 4368‚Äì4382. (<a
href="https://doi.org/10.1109/TPDS.2022.3189390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous computers integrate general-purpose host processors with domain-specific accelerators to combine versatility with efficiency and high performance. To realize the full potential of heterogeneous computers, however, many hardware and software design challenges have to be overcome. While architectural and system simulators can be used to analyze heterogeneous computers, they are faced with unavoidable compromises between simulation speed and performance modeling accuracy. In this work we present HEROv2, an FPGA-based research platform that enables accurate and fast exploration of heterogeneous computers consisting of accelerators based on clusters of 32-bit RISC-V cores and an application-class 64-bit ARMv8 or RV64 host processor. HEROv2 allows to seamlessly share data between 64-bit host s and 32-bit accelerators and comes with a fully open-source on-chip network, a unified heterogeneous programming interface, and a mixed-data-model, mixed-ISA heterogeneous compiler based on LLVM. We evaluate HEROv2 in four case studies from the application level over toolchain and system architecture down to accelerator microarchitecture. We demonstrate how HEROv2 enables effective research and development on the full stack of heterogeneous computing. For instance, the compiler can tile loops and infer data transfers to and from the accelerators, which leads to a speedup of up to 4.4 √ó compared to the original program and in most cases is only 15\% slower than a handwritten implementation, which requires 2.6 √ó more code.},
  archive      = {J_TPDS},
  author       = {Andreas Kurth and Bj√∂rn Forsberg and Luca Benini},
  doi          = {10.1109/TPDS.2022.3189390},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4368-4382},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HEROv2: Full-stack open-source research platform for heterogeneous computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context-aware online client selection for hierarchical
federated learning. <em>TPDS</em>, <em>33</em>(12), 4353‚Äì4367. (<a
href="https://doi.org/10.1109/TPDS.2022.3186960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has been considered as an appealing framework to tackle data privacy issues of mobile devices compared to conventional Machine Learning (ML). Using Edge Servers (ESs) as intermediaries to perform model aggregation in proximity can reduce the transmission overhead, and it enables great potential in low-latency FL, where the hierarchical architecture of FL (HFL) has been attracted more attention. Designing a proper client selection policy can significantly improve training performance, and it has been widely investigated in conventional FL studies. However, to the best of our knowledge, systematic client selection policies have not yet been fully studied for HFL. In addition, client selection for HFL faces more challenges than conventional FL (e.g., the time-varying connection of client-ES pairs and the limited budget of the Network Operator (NO)). In this article, we investigate a client selection problem for HFL, where the NO learns the number of successful participating clients to improve training performance (i.e., select as many clients in each round) as well as under the limited budget on each ES. An online policy, called Context-aware Online Client Selection (COCS), is developed based on Contextual Combinatorial Multi-Armed Bandit (CC-MAB). COCS observes the side-information (context) of local computing and transmission of client-ES pairs and makes client selection decisions to maximize NO&#39;s utility given a limited budget. Theoretically, COCS achieves a sublinear regret compared to an Oracle policy on both strongly convex and non-convex HFL. Simulation results also support the efficiency of the proposed COCS policy on real-world datasets.},
  archive      = {J_TPDS},
  author       = {Zhe Qu and Rui Duan and Lixing Chen and Jie Xu and Zhuo Lu and Yao Liu},
  doi          = {10.1109/TPDS.2022.3186960},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4353-4367},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Context-aware online client selection for hierarchical federated learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TriangleKV: Reducing write stalls and write amplification in
LSM-tree based KV stores with triangle container in NVM. <em>TPDS</em>,
<em>33</em>(12), 4339‚Äì4352. (<a
href="https://doi.org/10.1109/TPDS.2022.3188268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Popular LSM-tree based key-value stores suffer from suboptimal and unpredictable performance due to write amplification and write stalls that cause application performance to periodically drop to nearly zero. Our preliminary experimental studies reveal that (1) write stalls mainly stem from the significantly large amount of data involved in each compaction between $L_{0}$ - $L_{1}$ (i.e., the first two levels of LSM-tree), and (2) write amplification increases with the depth of LSM-trees. Existing work mainly focus on reducing write amplification, while only a couple of them target mitigating write stalls. In this paper, we exploit unique features of non-volatile memory (NVM) to address these two limitations and propose TriangleKV, a new LSM-tree based persistent KV store with multi-tier DRAM-NVM-SSD storage. TriangleKV&#39;s design principles include performing smaller and cheaper $L_{0}$ - $L_{1}$ compaction to reduce write stalls while reducing the depth of LSM-trees to mitigate write amplification. To this end, four novel techniques are proposed. First, we relocate and manage the $L_{0}$ level in NVM with our proposed triangle container . Second, the new right-angle side compaction is devised to compact $L_{0}$ to $L_{1}$ at fine-grained key ranges, thus substantially reducing the amount of compaction data. Third, TriangleKV increases the width of each level to decrease the depth of LSM-trees thus mitigating write amplification. Finally, the cross-row hint search is introduced for the triangle container to keep adequate read performance. We implement TriangleKV based on MatrixKV and evaluate it on a hybrid DRAM/NVM/SSD system using Intel&#39;s latest 3D Xpoint NVM device Optane DC PMM. Evaluation results show that, with the same amount of NVM, TriangleKV outperforms RocksDB, NoveLSM and MatrixKV in 99th-percentile latencies by $5.5\times$ , $2.1\times$ and $1.1\times$ , and random write throughput by $4.9\times$ , $3.5\times$ and $1.4\times$ respectively.},
  archive      = {J_TPDS},
  author       = {Chen Ding and Ting Yao and Hong Jiang and Qiu Cui and Liu Tang and Yiwen Zhang and Jiguang Wan and Zhihu Tan},
  doi          = {10.1109/TPDS.2022.3188268},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4339-4352},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {TriangleKV: Reducing write stalls and write amplification in LSM-tree based KV stores with triangle container in NVM},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AccTFM: An effective intra-layer model parallelization
strategy for training large-scale transformer-based models.
<em>TPDS</em>, <em>33</em>(12), 4326‚Äì4338. (<a
href="https://doi.org/10.1109/TPDS.2022.3187815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based deep neural networks have recently swept the field of natural language processing due to their outstanding performance, and are gradually spreading to more applications such as image/video processing. However, compared with general DNNs, training a sizeable transformer-based model is further time-consuming and memory-hungry. The existing distributed training strategies for general DNNs are not appropriate or can not efficiently handle transformer-based networks. In view of this, we propose an intra-layer model parallelization optimization strategy, AccTFM, which introduces a novel fine-grained pipeline execution and hybrid communication compression strategy to overcome the synchronization bottleneck. Specifically, on one hand, it first decouples the inter-layer computation and communication dependencies, and then searches for the optimal partitioning strategy to maximize the overlap of computation and communication. On the other hand, the hybrid communication compression module consists of token-level top- $k$ sparsification and piecewise quantization methods aiming at minimizing communication traffic. Experimental results show that AccTFM accelerates transformer-based DNNs training by up to 2.08x compared to state-of-the-art distributed training techniques.},
  archive      = {J_TPDS},
  author       = {Zihao Zeng and Chubo Liu and Zhuo Tang and Kenli Li and Keqin Li},
  doi          = {10.1109/TPDS.2022.3187815},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4326-4338},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {AccTFM: An effective intra-layer model parallelization strategy for training large-scale transformer-based models},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient and secure deep learning inference in trusted
processor enabled edge clouds. <em>TPDS</em>, <em>33</em>(12),
4311‚Äì4325. (<a href="https://doi.org/10.1109/TPDS.2022.3187772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge intelligence has emerged as a prevalent enabling technology to support various intelligent applications. Along with the prosperity, it also raises great concern on the security and privacy since the edge servers are usually shared and untrusted. The security-sensitive code (i.e., the pre-trained model) and data may be easily stolen by malicious tenants, and even untrusted infrastructure providers. To this end, Software Guard Extensions (SGX) is proposed to provide an isolated Trust Execution Environment (TEE) for security and privacy guarantee. However, we find that running tasks in SGX suffer certain performance degradation due to the limited Enclave Page Cache (EPC) size. This further leads to frequent page swapping operations and the high enclave call overhead, which are also influenced by the task (i.e., DNN layer) dispatching and scheduling. To this end, in this paper, we design Lasagna , as an SGX based secure DNN inference acceleration framework, which explores the layered-structure of DNN models to well balance the usage of the scarce EPC resources and the computation resources. Lasagna mainly consists of a global task balancer and a local task scheduler, responding for task dispatching across distributed edge servers and task scheduling in local server, respectively. We evaluate Lasagna over different well-known DNN models, and the results show that Lasagna effectively speeds up the inference performance by $1.11\times -1.51\times$ .},
  archive      = {J_TPDS},
  author       = {Yuepeng Li and Deze Zeng and Lin Gu and Quan Chen and Song Guo and Albert Zomaya and Minyi Guo},
  doi          = {10.1109/TPDS.2022.3187772},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4311-4325},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient and secure deep learning inference in trusted processor enabled edge clouds},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cloud object storage synchronization: Design, analysis, and
implementation. <em>TPDS</em>, <em>33</em>(12), 4295‚Äì4310. (<a
href="https://doi.org/10.1109/TPDS.2022.3185067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud storage synchronization among different computing terminals has attracted large-scale uses among enterprise and individual users. It enables users to maintain the same copy of data in real time, which eases users the tedious yet error-prone data management burden. However, existing cloud storage synchronization systems are in a closed form. Users are fixed to a certain cloud service provider, which makes it hard to transfer from one provider to another when balancing factors such as performance, cost, security, etc. To bridge this gap, this article proposes a new synchronization system based on standard cloud object storage. Specifically, we first formulate the cloud object storage synchronization problem by defining some useful concepts. We then use the idea of state encoding and a push-pull paradigm to propose a cloud object storage synchronization system. The proposed system supports real-time, multiple-terminal, and cloud-independent storage synchronization. We also prototyped the proposed system. The experimental results show that the proposed system is promising for practical usages.},
  archive      = {J_TPDS},
  author       = {Fei Chen and Zhipeng Li and Changkun Jiang and Tao Xiang and Yuanyuan Yang},
  doi          = {10.1109/TPDS.2022.3185067},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4295-4310},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cloud object storage synchronization: Design, analysis, and implementation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Eiffel: Efficient and fair scheduling in adaptive federated
learning. <em>TPDS</em>, <em>33</em>(12), 4282‚Äì4294. (<a
href="https://doi.org/10.1109/TPDS.2022.3187365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging machine learning (ML) technologies, in combination with the increasing computational power of mobile devices, lead to the extensive adoption of ML-based applications. Different from conventional model training that needs to collect all the user data in centralized cloud servers, federated learning (FL) has recently drawn increasing research attention as it enables privacy-preserving model training. With FL, decentralized edge devices in participation, train their model copies locally over their siloed datasets, and periodically synchronize the model parameters. However, model training is computationally extensive which easily drains the battery of mobile devices. In addition, due to the uneven distribution of siloed datasets, the shared model may become biased. To address the efficiency and fairness concerns in a resource-constrained federated learning setting, in this paper, we propose Eiffel to judiciously select mobile devices to participate in the global model aggregation, and adaptively adjust the frequency of local and global model updates. Eiffel aims to make scheduling and coordination for the federated learning towards both resource efficiency and model fairness. We have conducted theoretical analysis of Eiffel from the perspectives of fairness and convergence. Extensive experiments with a wide variety of real-world datasets and models, both on a networked prototype system and in a larger-scale simulated environment, have demonstrated that while maintaining similar accuracy performance, Eiffel outperforms existing baselines with respect to reducing communication overhead by up to 6√ó for higher efficiency and improving the fairness metric by up to 57\% compared to the state-of-the-art algorithms.},
  archive      = {J_TPDS},
  author       = {Abeda Sultana and Md. Mainul Haque and Li Chen and Fei Xu and Xu Yuan},
  doi          = {10.1109/TPDS.2022.3187365},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4282-4294},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Eiffel: Efficient and fair scheduling in adaptive federated learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Formulating cost-effective data distribution strategies
online for edge cache systems. <em>TPDS</em>, <em>33</em>(12),
4270‚Äì4281. (<a href="https://doi.org/10.1109/TPDS.2022.3185250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge Computing (EC) enables a new kind of caching system in close geographic proximity to end-users by allowing app vendors to cache popular data on edge servers deployed at base stations. This edge cache system can better support latency-sensitive applications. However, transmitting data from the centralized cloud to the edge servers without proper transmission strategies may cost app vendors dearly. Cost-effective data distribution strategies are of particular importance for applications, whose data to be cached at the edge often changes dynamically. In this paper, we study this Online Edge Data Distribution (OEDD) problem, aiming to minimize app vendors‚Äô total transmission cost, while ensuring low transmission latency in the long term. We first model this problem and prove its $\mathcal {NP}$ -hardness. We then combine Lyapunov optimization and game theory to propose a novel Latency-Aware Online (LAO) approach for solving this OEDD problem over time in a distributed manner with provable performance guarantees. The evaluation of LAO based on a real-world dataset demonstrates that it can help app vendors formulate cost-effective edge data distribution strategies in an online manner.},
  archive      = {J_TPDS},
  author       = {Xiaoyu Xia and Feifei Chen and Qiang He and John Grundy and Mohamed Abdelrazek and Jun Shen and Athman Bouguettaya and Hai Jin},
  doi          = {10.1109/TPDS.2022.3185250},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4270-4281},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Formulating cost-effective data distribution strategies online for edge cache systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PushBox: Making use of every bit of time to accelerate
completion of data-parallel jobs. <em>TPDS</em>, <em>33</em>(12),
4256‚Äì4269. (<a href="https://doi.org/10.1109/TPDS.2022.3182037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To minimize a job&#39;s completion time, we need to minimize the completion time of its final stage&#39;s last task. Scheduling of machine slots and networks largely dominates the variable part of each task&#39;s duration. Finding an optimal schedule is NP-hard even for offline and simplified scenarios. Previous work does lead to improved performance with various strategies. State-of-the-art task placement and network scheduling efforts are largely disjunctive. Without joint optimization, they are sub-optimal and myopic in many scenarios. Task placement usually treats the network as a black box. Thus, we use prioritized bandwidth allocation among tasks making the network both predictable and efficient to achieve joint scheduling. With this feature, joint scheduling can be transformed into a special bin-packing problem . Over this minimal yet power-enough abstraction, we propose PushBox to schedule data-parallel jobs in multi-tenant clusters. When designing the joint scheduling algorithm, we not only embrace the wisdom of prior art but also respect administrators‚Äô fairness intent, which is so far largely ignored. We implement PushBox on Hadoop 3. PushBox performs persistently well on both a small testbed and a trace-driven simulator.},
  archive      = {J_TPDS},
  author       = {Chen Tian and Yi Wang and Bingchuan Tian and Yang Zhao and Yuhang Zhou and Chenxu Wang and Haoran Guan and Wanchun Dou and Guihai Chen},
  doi          = {10.1109/TPDS.2022.3182037},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4256-4269},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {PushBox: Making use of every bit of time to accelerate completion of data-parallel jobs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting performance variance for parallel applications
without source code. <em>TPDS</em>, <em>33</em>(12), 4239‚Äì4255. (<a
href="https://doi.org/10.1109/TPDS.2022.3181799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For parallel applications, performance variance is a critical issue that can degrade performance and make applications‚Äô behavior difficult to explain. Therefore, users and application developers should be able to detect and diagnose performance variance. Previous detection methods either introduce too much overhead and slow down applications, or rely on nontrivial source code analysis, which is impractical for production-run parallel systems. In this article, we propose Vapro , a framework for detecting and diagnosing performance variance in production-run parallel systems. Our method is based on an observation that most parallel programs contain code snippets that are executed repeatedly with a fixed workload and can be utilized to detect performance variance. We present State Transition Graph (STG) to track program execution and then do light-weight workload analysis on STG to locate performance variance. Vapro is able to successfully identify these snippets at runtime even without program source code. To diagnose the discovered variation, Vapro uses a progressive diagnosis method based on a hybrid model combining variance breakdown and statistical analysis. According to evaluating results, Vapro &#39;s performance overhead is only 1.38\% on average. Vapro can identify performance variance in real applications caused by hardware issues, such as memory and IO. The standard deviation of the execution time is decreased by up to 73.5\% when the identified variance is fixed. Vapro achieves 30.0\% larger detection coverage than the state-of-the-art variance detection approach based on source code analysis.},
  archive      = {J_TPDS},
  author       = {Jidong Zhai and Liyan Zheng and Feng Zhang and Xiongchao Tang and Haojie Wang and Teng Yu and Yuyang Jin and Shuaiwen Leon Song and Wenguang Chen},
  doi          = {10.1109/TPDS.2022.3181799},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4239-4255},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Detecting performance variance for parallel applications without source code},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). STRETCH: Virtual shared-nothing parallelism for scalable and
elastic stream processing. <em>TPDS</em>, <em>33</em>(12), 4221‚Äì4238.
(<a href="https://doi.org/10.1109/TPDS.2022.3181979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stream processing applications extract value from raw data through Directed Acyclic Graphs of data analysis tasks. Shared-nothing (SN) parallelism is the de-facto standard to scale stream processing applications. Given an application, SN parallelism ins9tantiates several copies of each analysis task, making each instance responsible for a dedicated portion of the overall analysis, and relies on dedicated queues to exchange data among connected instances. On the one hand, SN parallelism can scale the execution of applications both up and out since threads can run task instances within and across processes/nodes. On the other hand, its lack of sharing can cause unnecessary overheads and hinder the scaling up when threads operate on data that could be jointly accessed in shared memory. This trade-off motivated us in studying a way for stream processing applications to leverage shared memory and boost the scale up (before the scale out) while adhering to the widely-adopted and SN-based APIs for stream processing applications. We introduce STRETCH , a framework that maximizes the scale up and offers instantaneous elastic reconfigurations (without state transfer) for stream processing applications. We propose the concept of Virtual Shared-Nothing (VSN) parallelism and elasticity and provide formal definitions and correctness proofs for the semantics of the analysis tasks supported by STRETCH , showing they extend the ones found in common Stream Processing Engines. We also provide a fully implemented prototype and show that STRETCH &#39;s performance exceeds that of state-of-the-art frameworks such as Apache Flink and offers, to the best of our knowledge, unprecedented ultra-fast reconfigurations, taking less than 40 ms even when provisioning tens of new task instances.},
  archive      = {J_TPDS},
  author       = {Vincenzo Gulisano and Hannaneh Najdataei and Yiannis Nikolakopoulos and Alessandro V. Papadopoulos and Marina Papatriantafilou and Philippas Tsigas},
  doi          = {10.1109/TPDS.2022.3181979},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4221-4238},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {STRETCH: Virtual shared-nothing parallelism for scalable and elastic stream processing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online orchestration of collaborative caching for
multi-bitrate videos in edge computing. <em>TPDS</em>, <em>33</em>(12),
4207‚Äì4220. (<a href="https://doi.org/10.1109/TPDS.2022.3182022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the traditional video streaming service provisioning paradigm, users typically request video contents through nearby Content Delivery Network (CDN) server(s). However, because of the uncertain wide area networks delays, the (remote) users usually suffer from long video streaming delay, which affects the quality of experience. Multi-Access Edge Computing (MEC) offers caching infrastructures in closer proximity to end users than conventional Content Delivery Networks (CDNs). Yet, for video caching, MEC&#39;s potential has not been fully unleashed as it overlooks the opportunities of collaborative caching and multi-bitrate video transcoding. In this paper, we model and formulate an Integer Linear Program (ILP) to capture the long-term cost minimization problem for caching videos at MEC, allowing joint exploitation of MEC with CDN and real-time video transcoding to satisfy arbitrary user demands. While this problem is intractable and couples the caching decisions for adjacent time slots, we design a polynomial-time online orchestration framework which first relaxes and carefully decomposes the problem into a series of subproblems solvable in each individual time slot and then converts the fractional solutions into integers without violating constraints. We have formally proved a parameterized-constant competitive ratio as the performance guarantee for our approach, and also conducted extensive evaluations to confirm its superior practical performance. Simulation results demonstrate that our proposed algorithm outperforms the state-of-the-art algorithms, with 13.6\% improvement on average in terms of total cost.},
  archive      = {J_TPDS},
  author       = {Song Yang and Lei Jiao and Ramin Yahyapour and Jiannong Cao},
  doi          = {10.1109/TPDS.2022.3182022},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4207-4220},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Online orchestration of collaborative caching for multi-bitrate videos in edge computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hydra: A decentralized file system for persistent memory and
RDMA networks. <em>TPDS</em>, <em>33</em>(12), 4192‚Äì4206. (<a
href="https://doi.org/10.1109/TPDS.2022.3180369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging byte-addressable persistent memory (PM) has the potential to disrupt the boundary between memory and storage. Combined with high-speed RDMA networks, distributed PM-based storage systems offer the opportunity to provide huge increases in storage performance by closely coupling PM and RDMA features. However, existing distributed file systems adopt the conventional centralized client-server architecture designed for traditional disks, leading to excessive access latency, limited scalability, and high recovery overhead. In this paper, we propose a fully decentralized PM-based file system, Hydra. By exploiting the performance advantages of local PM, Hydra leverages data access locality to achieve high performance. To accelerate file transmission among Hydra nodes, file metadata and data are decoupled and updated differentially through one-sided RDMA reads. Hydra also batches RDMA requests and classifies RPCs into synchronous and asynchronous types to minimize network overhead. Decentralization enables Hydra to tolerate node failures and achieve load balancing. Experimental results show that Hydra outperforms existing distributed file systems by a large margin, and shows good scalability on multi-threaded and parallel workloads.},
  archive      = {J_TPDS},
  author       = {Shengan Zheng and Jingyu Wang and Dongliang Xue and Jiwu Shu and Linpeng Huang},
  doi          = {10.1109/TPDS.2022.3180369},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4192-4206},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Hydra: A decentralized file system for persistent memory and RDMA networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Privacy-preserving deduplication of sensor compressed data
in distributed fog computing. <em>TPDS</em>, <em>33</em>(12), 4176‚Äì4191.
(<a href="https://doi.org/10.1109/TPDS.2022.3179992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed fog computing has received wide attention recently. It enables distributed computing and data management on the network nodes within the close vicinity of IoT devices. An important service of fog-cloud based systems is data deduplication. With the increasing concern of privacy, some privacy-preserving data deduplication schemes have been proposed. However, they cannot support lossless deduplication of encrypted similar data in the fog-cloud network. Meanwhile, no existing design can protect message equality information while resisting brute-force and frequency analysis attacks. In this paper, we propose a privacy-preserving and compression-based data deduplication system under the fog-cloud network, which supports lossless deduplication of similar data in the encrypted domain. Specifically, we first use the generalized deduplication technique and cryptographic primitives to implement secure deduplication over similar data. Then, we devise a two-level deduplication protocol that can perform secure and efficient deduplication at distributed fog nodes and the cloud. The proposed system can not only resist brute-force and frequency analysis attacks but also ensure that only the data operator can capture the message equality information. We formally analyze the security of our design. Performance evaluations demonstrate that our proposed design is efficient in computing, storage, and communication.},
  archive      = {J_TPDS},
  author       = {Chen Zhang and Yinbin Miao and Qingyuan Xie and Yu Guo and Hongwei Du and Xiaohua Jia},
  doi          = {10.1109/TPDS.2022.3179992},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4176-4191},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Privacy-preserving deduplication of sensor compressed data in distributed fog computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SconeKV: A scalable, strongly consistent key-value store.
<em>TPDS</em>, <em>33</em>(12), 4164‚Äì4175. (<a
href="https://doi.org/10.1109/TPDS.2022.3179903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For decades, relational databases provided a strong foundation for constructing applications due to their ACID properties. However, distributed applications reached a scale, both in terms of data volume and number of concurrent clients, that traditional databases cannot accommodate. NoSQL databases addressed this problem by trading consistency for scalability, namely through horizontal scalability schemes supported by optimistic replication protocols, which only guarantee eventual consistency. In this paper, we explore a novel design between the two extremes, which is able to scale to large deployments while still offering strong consistency guarantees in the form of serializable transactions. Our key insight is to leverage recent advances in membership services that provide strongly consistent views at scale. Those assurances from the membership layer simplify building efficient and consistent storage protocols. Our evaluation of the resulting system, SconeKV , in a realistic scenario shows that it scales and performs better than CockroachDB while being competitive with Cassandra.},
  archive      = {J_TPDS},
  author       = {Jo√£o Gon√ßalves and Miguel Matos and Rodrigo Rodrigues},
  doi          = {10.1109/TPDS.2022.3179903},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4164-4175},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SconeKV: A scalable, strongly consistent key-value store},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Content collaborative caching strategy in the edge
maintenance of communication network: A joint download delay and energy
consumption method. <em>TPDS</em>, <em>33</em>(12), 4148‚Äì4163. (<a
href="https://doi.org/10.1109/TPDS.2022.3179271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of Big Data technology and Internet, the surge of data in the network will cause network congestion and untimely task processing. Additionally, caching content in the core network may cause redundant access of content and backhaul bottlenecks. Due to the increasing requirements of users for task processing efficiency, the centralized maintenance system based on traditional cloud computing cannot meet the current computing requirements. In view of these problems, we propose a content collaborative caching mechanism based on joint decision of download delay and energy consumption. By integrating network coding and content caching technology, the work content maintained in the communication network is deployed near the edge of the network in the form of coding to reduce the redundant transmission of content and acquisition time of content. This article establishes a user QoE satisfaction model, which consists of two indexes that measure time delay and energy consumption. This article proposes a $\varepsilon$ -hybrid Q-learning algorithm to optimize the placement of cache files, and made the cache action selection based on the combination of improved heuristic greedy algorithm and simulated annealing algorithm. The experimental results show that the proposed cache strategy can reduce the delay of users downloading content and the energy consumption of content cache, so as to improve the quality of field maintenance work in communication network.},
  archive      = {J_TPDS},
  author       = {Lanlan Rui and Dai Song and Shiyou Chen and Yingtai Yang and Yang Yang and Zhipeng Gao},
  doi          = {10.1109/TPDS.2022.3179271},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4148-4163},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Content collaborative caching strategy in the edge maintenance of communication network: A joint download delay and energy consumption method},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding the impact of data staging for coupled
scientific workflows. <em>TPDS</em>, <em>33</em>(12), 4134‚Äì4147. (<a
href="https://doi.org/10.1109/TPDS.2022.3179989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rate of data generated by cutting-edge experimental science facilities and large-scale simulations enabled by current high-performance computing (HPC) systems has continued to grow at a far greater pace than the development of the network and storage capabilities on which these systems rely. To cope with this challenge, scientist are moving toward the creation of autonomous experiments and HPC simulations using machine learning. However, efficiently moving, storing, and processing large amounts of data away from the point of origin presents an incredible challenge. In-memory computing, in situ analysis, data staging, and data streaming are recognized viable alternatives to traditional file-based methods for transferring data between coupled workflows. However, the performance trade-offs and limitations for these methods are not fully understood when used in HPC applications. This article presents a comprehensive performance assessment of the current solutions for data staging when applied to applications that are not necessary I/O intensive which makes them not ideal candidates for these methods. Our study is based on experiments running at scale on Oak Ridge National Laboratory&#39;s Summit supercomputer using applications and simulations that cover typical computational motifs and patterns. We investigated the usability and cost/benefit trade-offs of staging algorithms for HPC applications under different scenarios and highlight opportunities for optimizing the dataflow between coupled simulation workflows.},
  archive      = {J_TPDS},
  author       = {Ana Gainaru and Lipeng Wan and Ruonan Wang and Eric Suchyta and Jieyang Chen and Norbert Podhorszki and James Kress and David Pugmire and Scott Klasky},
  doi          = {10.1109/TPDS.2022.3179989},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4134-4147},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Understanding the impact of data staging for coupled scientific workflows},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time scheduling of parallel task graphs with critical
sections across different vertices. <em>TPDS</em>, <em>33</em>(12),
4117‚Äì4133. (<a href="https://doi.org/10.1109/TPDS.2022.3179328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {All existing work on real-time scheduling of parallel task graph models with shared resources assumes that a critical section must be contained inside a single vertex. However, this assumption does not hold in many realistic parallel real-time software. In this work, we conduct the first study on real-time scheduling and analysis of parallel task graphs where critical sections are allowed to cross different vertices. We show that allowing this may potentially lead to deadlocks and the so-called resource unrelated blocking time problem. We formalize the conditions for the deadlocks and resource unrelated blocking time to happen, and propose two different solutions to address them and develop corresponding schedulability analysis techniques. We conduct comprehensive experiments to evaluate our method. The results indicate that there is a significant impact to the system schedulability when tasks incur deadlock and resource unrelated blocking. Moreover, the schedulability can benefit from the execution of workload in parallel with critical sections if tasks can be carefully designed so that all deadlocks and resource unrelated blocking time can be avoided, and our methods are efficient to determine the schedulability of systems where critical sections across different vertices exist.},
  archive      = {J_TPDS},
  author       = {Xu Jiang and Nan Guan and Maolin Yang and Yang Wang and Yue Tang and Wang Yi},
  doi          = {10.1109/TPDS.2022.3179328},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4117-4133},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Real-time scheduling of parallel task graphs with critical sections across different vertices},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SmartVM: A smart contract virtual machine for fast on-chain
DNN computations. <em>TPDS</em>, <em>33</em>(12), 4100‚Äì4116. (<a
href="https://doi.org/10.1109/TPDS.2022.3177405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain-based artificial intelligence (BC-AI) has been applied for protecting deep neural network (DNN) data from being tampered with, which is expected to further boost trusted distributed AI applications in many fields. However, due to smart contract execution environment architectural defects, it is challenging for previous BC-AI systems to support computing-intensive tasks on-chain performing such as DNN convolution operations. They have to offload computations and a large amount of data from blockchain to off-chain platforms to execute smart contracts as native code. This failure to take advantage of data locality has become one of the major critical performance bottlenecks in BC-AI system. To this end, in this article, we propose SmartVM with optimization methods to support on-chain DNN inference for BC-AI system. The key idea is to design and optimize the computing mechanism and storage structure of smart contract execution environment according to the characteristics of DNN such as high computational parallelism and large data volume. We decompose SmartVM into three components: 1) a compact DNN-oriented instruction set to describe computations in a short number of instructions to reduce interpretation time. 2) a memory management mechanism to make SmartVM memory dynamic free/allocated according to the size of DNN feature maps. 3) a block-based weight prefetching and parallel computing method to organize each layer&#39;s computing and weights prefetching in a pipelined manner. We perform the typical image classification in a private Ethereum blockchain testbed to evaluate SmartVM performance. Experimental results highlight that SmartVM can support DNN inference on-chain with roughly the same efficiency against the native code execution. Compared with the traditional off-chain computing, SmartVM can speed up the overall execution by 70√ó , 16√ó , 11√ó , and 12√ó over LeNet5, AlexNet, ResNet18, and MobileNet, respectively. The memory footprint can be reduced by 84\% , 90.8\% , 94.3\% , and 93.7\% over the above four models, while offering the same level model accuracy. This article sheds light on the design space of the smart contract virtual machine for DNN computation and is promising to further boost BC-AI applications.},
  archive      = {J_TPDS},
  author       = {Tao Li and Yaozheng Fang and Ye Lu and Jinni Yang and Zhaolong Jian and Zhiguo Wan and Yusen Li},
  doi          = {10.1109/TPDS.2022.3177405},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4100-4116},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SmartVM: A smart contract virtual machine for fast on-chain DNN computations},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy-aware non-preemptive task scheduling with deadline
constraint in DVFS-enabled heterogeneous clusters. <em>TPDS</em>,
<em>33</em>(12), 4083‚Äì4099. (<a
href="https://doi.org/10.1109/TPDS.2022.3181096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy conservation of large data centers for high performance computing workloads, such as deep learning with Big Data, is of critical significance, where cutting down a few percent of electricity translates into million-dollar savings. This work studies energy conservation on emerging CPU-GPU hybrid clusters through dynamic voltage and frequency scaling (DVFS). We aim at minimizing the total energy consumption of processing a batch of offline tasks or a sequence of real-time tasks under deadline constraints. We derive a fast and accurate analytical model to compute the appropriate voltage/frequency setting for each task, and assign multiple tasks to the cluster with heuristic scheduling algorithms. In particular, our model stresses the nonlinear relationship between task execution time and processor speed for GPU-accelerated applications, for more accurately capturing real-world GPU energy consumption. In performance evaluation driven by real-world power measurement traces, our scheduling algorithm shows comparable energy savings to the theoretical upper bound. With a GPU scaling interval where analytically at most 36\% of energy can be saved, we record 33-35\% of energy savings. Our results are applicable to energy management on modern heterogeneous clusters.},
  archive      = {J_TPDS},
  author       = {Qiang Wang and Xinxin Mei and Hai Liu and Yiu-Wing Leung and Zongpeng Li and Xiaowen Chu},
  doi          = {10.1109/TPDS.2022.3181096},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4083-4099},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Energy-aware non-preemptive task scheduling with deadline constraint in DVFS-enabled heterogeneous clusters},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Highly accurate clock synchronization with drift correction
for the controller area network. <em>TPDS</em>, <em>33</em>(12),
4071‚Äì4082. (<a href="https://doi.org/10.1109/TPDS.2022.3179316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern vehicles, that have to be considered as safety-critical cyber-physical systems, require highly accurate clock synchronization (CS) among their distributed computing devices. Since Controller Area Network (CAN) is the predominant in-vehicle communication bus, it is highly relevant to support CS for CAN. This article proposes an original CS method for distributed in-vehicle networks based on CAN with both offset and drift correction . While offset correction is performed based on timestamps in periodic reference messages (RMs), our new method benefits from the re-synchronization mechanism of the CAN bit timing to apply highly accurate drift correction. Our algorithm does not make any modifications to the CAN protocol but requires the measurement of the phase error from the CAN controller. We derive analytical bounds for the expected clock differences and further validate the practicability of the proposed method by comprehensive experiments. As the main result, our method achieves a clock accuracy below $2\;\mu$ s independent of important parameters such as the bit rate, RM period, bus utilization and time-varying clock drifts.},
  archive      = {J_TPDS},
  author       = {Murat Akpƒ±nar and Ece G√ºran Schmidt and Klaus Werner Schmidt},
  doi          = {10.1109/TPDS.2022.3179316},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4071-4082},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Highly accurate clock synchronization with drift correction for the controller area network},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring query processing on CPU-GPU integrated edge
device. <em>TPDS</em>, <em>33</em>(12), 4057‚Äì4070. (<a
href="https://doi.org/10.1109/TPDS.2022.3177811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Huge amounts of data have been generated on edge devices every day, which requires efficient data analytics and management. However, due to the limited computing capacity of these edge devices, query processing at the edge faces tremendous pressure. Fortunately, in recent years, hardware vendors have integrated heterogeneous coprocessors, such as GPUs, into the edge device, which can provide much more computing power. Furthermore, the CPU-GPU integrated edge device has shown significant benefits in a variety of situations. Therefore, the exploration of query processing on such CPU-GPU integrated edge devices becomes an urgent need. In this article, we develop a fine-grained query processing engine, called FineQuery, which can perform efficient query processing on CPU-GPU integrated edge devices. Particularly, FineQuery can take advantage of both architectural features of edge devices and query characteristics by performing fine-grained workload scheduling between the CPU and the GPU. Experiments show that on TPC-H workloads, FineQuery reduces 42.81\% latency and improves 2.39√ó bandwidth utilization on average compared to the implementation of using only GPU or CPU. Furthermore, query processing at the edge can bring significant performance-per-cost benefits and energy efficiency. On average, FineQuery at the edge brings 21√ó performance-per-cost ratio and 4√ó energy efficiency compared with processing the data on a discrete GPU platform.},
  archive      = {J_TPDS},
  author       = {Jiesong Liu and Feng Zhang and Hourun Li and Dalin Wang and Weitao Wan and Xiaokun Fang and Jidong Zhai and Xiaoyong Du},
  doi          = {10.1109/TPDS.2022.3177811},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4057-4070},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Exploring query processing on CPU-GPU integrated edge device},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CNNPC: End-edge-cloud collaborative CNN inference with joint
model partition and compression. <em>TPDS</em>, <em>33</em>(12),
4039‚Äì4056. (<a href="https://doi.org/10.1109/TPDS.2022.3177782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge Intelligence (EI) aims at addressing concerns like response latency risen by the conflict between predominating Cloud-based deployments of computationally intensive AI applications and the expensive uploading of explosive end data. Convolutional Neural Networks (CNNs) leading the latest flourish of AI inevitably suffer from the aforementioned conflict. There emerge increasing EI-driven attempts on fast CNN inference with high accuracy in the End-Edge-Cloud (EEC) collaborative computing paradigm, where, however, neither model compression approaches for on-device inference nor collaborative inference methods across devices can effectively achieve the trade-off between latency and accuracy of End-to-End (E2E) inference. In this article, we present CNNPC that jointly partitions and compresses CNNs for fast inference with high accuracy in collaborative EEC systems. We implemented CNNPC (source code available at https://github.com/IoTDATALab/CNNPC ) and evaluated its performance within extensive real-world EEC scenarios. Experimental results demonstrate that, compared with state-of-the-art single-end and collaborative approaches, without obvious accuracy loss, collaborative inference based on CNNPC is up to $1.6\times$ and $5.6\times$ faster, and requires as low as $4.30\%$ and $6.48\%$ communications, respectively. Besides, when determines the optimal strategy, CNNPC requires as low as $0.1\%$ actual compression operations that the traversal method (the only viable method providing the theoretically optimal strategy) requires.},
  archive      = {J_TPDS},
  author       = {Shusen Yang and Zhanhua Zhang and Cong Zhao and Xin Song and Siyan Guo and Hailiang Li},
  doi          = {10.1109/TPDS.2022.3177782},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4039-4056},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CNNPC: End-edge-cloud collaborative CNN inference with joint model partition and compression},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DAG scheduling and analysis on multi-core systems by
modelling parallelism and dependency. <em>TPDS</em>, <em>33</em>(12),
4019‚Äì4038. (<a href="https://doi.org/10.1109/TPDS.2022.3177046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With ever more complex functionalities being implemented in emerging real-time applications, multi-core systems are demanded for high performance, with directed acyclic graphs (DAG) being used to model functional dependencies. For a single DAG task, our previous work presented a concurrent provider and consumer (CPC) model that captures the node-level dependency and parallelism, which are the two key factors of a DAG. Based on the CPC, scheduling and analysis methods were constructed to reduce makespan and tighten the analytical bound of the task. However, the CPC-based methods cannot support multi-DAGs as the interference between DAGs (i.e., inter-task interference) is not taken into account. To address this limitation, this article proposes a novel multi-DAG scheduling approach which specifies the number of cores a DAG can utilise so that it does not incur the inter-task interference. This is achieved by modelling and understanding the workload distribution of the DAG and the system. By avoiding the inter-task interference, the constructed schedule provides full compatibility for the CPC-based methods to be applied on each DAG and reduces the pessimism of the existing analysis. Experimental results show that the proposed multi-DAG method achieves an improvement up to 80\% in schedulability against the original work that it extends, and outperforms the existing multi-DAG methods by up to 60\% for tightening the interference.},
  archive      = {J_TPDS},
  author       = {Shuai Zhao and Xiaotian Dai and Iain Bate},
  doi          = {10.1109/TPDS.2022.3177046},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4019-4038},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DAG scheduling and analysis on multi-core systems by modelling parallelism and dependency},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive vertical federated learning on unbalanced features.
<em>TPDS</em>, <em>33</em>(12), 4006‚Äì4018. (<a
href="https://doi.org/10.1109/TPDS.2022.3178443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the existing FL systems focus on a data-parallel architecture where training data are partitioned by samples among several parties. In some real-life applications, however, partitioning by features is also of practical relevance and the number of features is usually unbalanced among parties. The corresponding learning framework is referred to as Vertical Federated Learning (VFL). Though some pioneering work focused on VFL, the convergence properties of VFL on unbalanced features, especially when parties conduct different numbers of local updates concerning heterogeneous computational capabilities are still unknown. In this article, we propose a new learning framework to improve the training efficiency of VFL on unbalanced features. Given the number of features and the computational capability owned by each party, our thorough theoretical analysis exhibits that the number of local updates conducted by each party has a great effect on the convergence rate and the computational complexity, both of which jointly determine the overall training efficiency in an interrelated and sophisticated way. Based on our theoretical findings, we formulate an optimization problem and derive the optimal solution by selecting an adaptive number of local training rounds for each party. Extensive experiments on various datasets and models demonstrate that our approach significantly improves the training efficiency of VFL.},
  archive      = {J_TPDS},
  author       = {Jie Zhang and Song Guo and Zhihao Qu and Deze Zeng and Haozhao Wang and Qifeng Liu and Albert Y. Zomaya},
  doi          = {10.1109/TPDS.2022.3178443},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {4006-4018},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Adaptive vertical federated learning on unbalanced features},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimized page fault handling during RDMA. <em>TPDS</em>,
<em>33</em>(12), 3990‚Äì4005. (<a
href="https://doi.org/10.1109/TPDS.2022.3175666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote Direct Memory Access (RDMA) is widely used in High-Performance Computing (HPC) while making inroads in datacenters and accelerators. State-of-the-art RDMA engines typically do not endure page faults, therefore users are forced to pin their buffers, which complicates the programming model, limits the memory utilization, and moves the pressure to the Network Interface Cards (NICs). In this article we introduce a mechanism for handling dynamic page faults during RDMA, named PART, suitable for emerging processors that also integrate the Network Interface. PART leverages the IOMMU already present in modern processors for translations. PART avoids the pinning overheads, allows any buffer to be used for communication, and enables overlapping page fault handling with serving subsequent RDMA transfers. We implement and optimize PART for a cluster of ARMv8 cores with tightly-coupled network interfaces. Handling a minor page-fault of a small transfer at the destination takes approximately 38 $\mu$ secs, while there is no performance degradation when running three full MPI applications in 16 nodes and 64 cores. Detailed breakdown uncovers the hardware and system software components of this overhead and was used to further optimize the system. A 4MB RDMA transfer performs 1.46x better over pinning.},
  archive      = {J_TPDS},
  author       = {Antonis Psistakis and Nikos Chrysos and Fabien Chaix and Marios Asiminakis and Michalis Gianioudis and Pantelis Xirouchakis and Vassilis Papaefstathiou and Manolis Katevenis},
  doi          = {10.1109/TPDS.2022.3175666},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3990-4005},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimized page fault handling during RDMA},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VCSR: An efficient GPU memory-aware sparse format.
<em>TPDS</em>, <em>33</em>(12), 3977‚Äì3989. (<a
href="https://doi.org/10.1109/TPDS.2022.3177291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Sparse Matrix-Vector Multiplication (SpMV) kernel is used in a broad class of linear algebra computations. SpMV computations result in a performance bottleneck in many high performance applications, so optimizing SpMV performance is paramount. While implementing this kernel on a GPU can potentially boost performance significantly, current GPU libraries either provide modest performance gains or are burdened with high sparse format conversion overhead. In this paper we introduce the Vertical Compressed Sparse Row (VCSR) format, a novel memory-aware format that out-performs previous proposed formats on a GPU. We first motivate the design of our baseline VCSR format and then step through a series of enhancements that further improve VCSR&#39;s memory efficiency (VCSR-MEM) and performance (VCSR-INTRLV), while also considering conversion overhead. VCSR attempts to produce a high degree of thread-level parallelism and memory utilization by exploiting knowledge of GPU memory microarchitecture. VCSR can reduce the number of global memory transactions significantly, an issue not addressed by most other sparse formats. In addition, VCSR provides a novel reordering mechanism. It minimizes the size of the compressed matrix, handles both regular/irregular sparse matrices, and can be customized based on matrix size. VCSR also minimizes conversion overhead, as compared to full or partial row reordering. Our methodology is highly configurable and can be optimized for any sparse matrix. We have evaluated the VCSR format for the SpMV kernel when run on two different NVIDIA GPUs, the Kepler K40 and the Volta V100. We compare VCSR with NVIDIA&#39;s cuSPARSE library (the HYB format), a state-of-the-art sparse library. We also compare against other state-of-the-art CSR-based formats, including CSR5, merge-base SpMV and HOLA. We evaluate the benefits of VCSR over the entire University of Florida&#39;s SuiteSparse dataset collection. The VCSR-baseline format achieves an average speedup ranging from $1.10\times$ to $1.39\times$ when compared to the performance of the four state-of-the-art formats on an NVIDIA V100. While the VCSR-MEM format can save a significant amount of memory space, it is a bit slower than our VCSR-baseline. VCSR-INTRLV performs much better than the VCSR-baseline, and even when including the conversion overhead, achieves an average speedup of $1.08\times$ as compared to HOLA (the best performing format among the prior schemes).},
  archive      = {J_TPDS},
  author       = {Elmira Karimi and Nicolas Bohm Agostini and Shi Dong and David Kaeli},
  doi          = {10.1109/TPDS.2022.3177291},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3977-3989},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {VCSR: An efficient GPU memory-aware sparse format},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Run-time remapping algorithm of dataflow actors on NoC-based
heterogeneous MPSoCs. <em>TPDS</em>, <em>33</em>(12), 3959‚Äì3976. (<a
href="https://doi.org/10.1109/TPDS.2022.3177957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiprocessor system-on-chip (MPSoC) platforms have been emerging as the main solution to cope with processor frequency ceiling and power density issues while still improving performances. Then, network-on-chip (NoC) has been adopted to provide the increasing number of processors with the required communication bandwidth as well as with the necessary flexibility. Video processing and streaming applications are adopting dynamic dataflow model of computation as the need for high performance parallel computing is growing. Dataflow applications executed on modern MPSoC-based architectures are becoming increasingly dynamic and more data-dependent. Different tasks execute concurrently with significant modifications in their workloads and resource demanding over time depending on the input data. Hence, adopting any static or offline dynamic scheduling for mapping tasks will not cope with the computation variations. This article introduces an original run-time mapping algorithm based on the Move Based (MB) method targeting a dedicated heterogeneous NoC-based MPSoC architecture to achieve workload balancing and optimized communication traffic. The performance of the proposed algorithm is verified by conducting cycle-accurate SystemC simulations of the adopted NoC implementing a real MPEG4-SP decoder. The obtained results reveal the effectiveness of our proposed algorithm. For various real-life videos, the proposed algorithm systematically succeeded to enhance significantly the performance.},
  archive      = {J_TPDS},
  author       = {Mostafa Rizk and Kevin J. M. Martin and Jean-Philippe Diguet},
  doi          = {10.1109/TPDS.2022.3177957},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3959-3976},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Run-time remapping algorithm of dataflow actors on NoC-based heterogeneous MPSoCs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Busy-time scheduling on heterogeneous machines: Algorithms
and analysis. <em>TPDS</em>, <em>33</em>(12), 3942‚Äì3958. (<a
href="https://doi.org/10.1109/TPDS.2022.3176665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a generalized busy-time scheduling model on heterogeneous machines. The input to the model includes a set of jobs and a set of machine types. Each job has a size and a time interval during which it should be processed. Each job is to be placed on a machine for execution. Different types of machines have distinct capacities and cost rates. The total size of the jobs running on a machine must always be kept within the machine&#39;s capacity, giving rise to placement restrictions for jobs of various sizes among the machine types. Each machine used is charged according to the time duration in which it is busy, i.e., it is processing jobs. The objective is to schedule the jobs into machines to minimize the total cost of all the machines used. We develop an $O(1)$ -approximation algorithm in the offline setting and an $O(\mu)$ -competitive algorithm in the online setting (where $\mu$ is the max/min job length ratio), both of which are asymptotically optimal. This article significantly improves the analysis of the algorithms over our preliminary work.},
  archive      = {J_TPDS},
  author       = {Mozhengfu Liu and Xueyan Tang},
  doi          = {10.1109/TPDS.2022.3176665},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3942-3958},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Busy-time scheduling on heterogeneous machines: Algorithms and analysis},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PhaST: Hierarchical concurrent log-free skip list for
persistent memory. <em>TPDS</em>, <em>33</em>(12), 3929‚Äì3941. (<a
href="https://doi.org/10.1109/TPDS.2022.3173707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skip list (skiplist) is a competitive index structure that offers superior concurrency and excellent performance but with high memory overhead and low access locality. Emerging persistent memory (PM) technologies present an opportunity to mitigate the capacity constraint of DRAM. However, data consistency on PM typically results in excessive write overhead. In addition, fast concurrent access to an index is critical to the throughput on high-end contemporary computer systems. In this article, we propose a Partitioned HierArchical SkiplisT called PhaST , which can simultaneously reduce the skiplist height and improve its access locality, through its hierarchy of component structures, while enabling fast parallel recovery in case of failure. To ensure high concurrency and fast data consistency, we also have developed writelock-free concurrent insert and log-free atomic split. Furthermore, we have developed a durable lock-free concurrent search that can discern transient structural inconsistencies and deliver highly concurrent read operations. We have conducted an extensive evaluation of PhaST compared to state-of-the-art studies such as NV-Skiplist, wB+-Tree, FPTree, and FAST-FAIR. Our evaluation results show PhaST outperforms other indexing structures by up to 4.05√ó and 2.87√ó in single-threaded inserts and searches, and 1.56√ó and 2.62√ó in concurrent inserts and searches.},
  archive      = {J_TPDS},
  author       = {Zhenxin Li and Bing Jiao and Shuibing He and Weikuan Yu},
  doi          = {10.1109/TPDS.2022.3173707},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3929-3941},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {PhaST: Hierarchical concurrent log-free skip list for persistent memory},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SMURF: Efficient and scalable metadata access for
distributed applications. <em>TPDS</em>, <em>33</em>(12), 3915‚Äì3928. (<a
href="https://doi.org/10.1109/TPDS.2022.3175596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In parallel with big data processing and analysis dominating the usage of distributed and Cloud infrastructures, the demand for distributed metadata access and transfer has increased. The volume of data generated by many application domains exceeds petabytes, while the corresponding metadata amounts to terabytes or even more. This article proposes a novel solution for efficient and scalable metadata access for distributed applications across wide-area networks, dubbed SMURF. Our solution combines novel pipelining and concurrent transfer mechanisms with reliability, provides distributed continuum caching and semantic locality-aware prefetching strategies to sidestep fetching latency, and achieves scalable and high-performance metadata fetch/prefetch services in the Cloud. We incorporate the phenomenon of semantic locality awareness for increased prefetch prediction rate using real-life application I/O traces from Yahoo! Hadoop audit logs and propose a novel prefetch predictor. By effectively caching and prefetching metadata based on the access patterns, our continuum caching and prefetching mechanism significantly improves the local cache hit rate and reduces the average fetching latency. We replay approximately 20 Million metadata access operations from real audit traces, where SMURF achieves 90\% accuracy during prefetch prediction and reduced the average fetch latency by 50\% compared to the state-of-the-art mechanisms.},
  archive      = {J_TPDS},
  author       = {Bing Zhang and Tevfik Kosar},
  doi          = {10.1109/TPDS.2022.3175596},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3915-3928},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SMURF: Efficient and scalable metadata access for distributed applications},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An in-depth study of microservice call graph and runtime
performance. <em>TPDS</em>, <em>33</em>(12), 3901‚Äì3914. (<a
href="https://doi.org/10.1109/TPDS.2022.3174631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Loosely-coupled and light-weight microservices running in containers are replacing monolithic applications gradually. Understanding the characteristics of microservices is critical to make good use of microservice architectures. However, there is no comprehensive study about microservice and its related systems in production environments so far. In this paper, we present a solid analysis of large-scale deployments of microservices at Alibaba clusters. Our study focuses on the characterization of microservice dependency as well as its runtime performance. We conduct an in-depth anatomy of microservice call graphs to quantify the difference between them and traditional DAGs of data-parallel jobs. In particular, we observe that microservice call graphs are heavy-tail distributed and their topology is similar to a tree and moreover, many microservices are hot-spots. We also discover that the structure of call graphs for long-term developed applications is much simpler so as to provide better performance. Our investigation on microservice runtime performance indicates most microservices are much more sensitive to CPU interference than memory interference. Moreover, we design resource management policies to efficiently tune memory resources.},
  archive      = {J_TPDS},
  author       = {Shutian Luo and Huanle Xu and Chengzhi Lu and Kejiang Ye and Guoyao Xu and Liping Zhang and Jian He and Chengzhong Xu},
  doi          = {10.1109/TPDS.2022.3174631},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3901-3914},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An in-depth study of microservice call graph and runtime performance},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A resource-efficient predictive resource provisioning system
in cloud systems. <em>TPDS</em>, <em>33</em>(12), 3886‚Äì3900. (<a
href="https://doi.org/10.1109/TPDS.2022.3172493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud systems, demand-prediction based resource provisioning schemes help assure the SLOs (service level objectives) of cloud tenants. We notice that if a provisioning scheme does not exclude bursts from historical resource demands in normal demand prediction or always uses a large padding to correct under-prediction, it will lead to resource over-provisioning and low resource utilization. To improve the previous schemes, in this paper, we present a Resource-efficient Predictive Resource Provisioning system in cloud systems (RPRP) that excludes bursts in demand prediction and has algorithms to specifically handle bursts to avoid resource over-provisioning. Rather than setting padding to a possibly high value, RPRP has a load-dependent padding algorithm that adaptively determines padding based on predicted demands. To handle bursts, RPRP has a burst-resilient shared padding algorithm that reserves resource shared by multiple co-located VMs rather than for individual VMs. It also embodies a responsive padding algorithm that adaptively adjusts padding to recover from both under-provisioning and over-provisioning. We implemented RPRP on top of Xen and conducted both trace-driven simulation and real-world testbed experiments. The experimental results show that RPRP achieves higher resource utilization, more accurate demand predictions, and fewer SLO violations than previous schemes.},
  archive      = {J_TPDS},
  author       = {Haiying Shen and Liuhua Chen},
  doi          = {10.1109/TPDS.2022.3172493},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3886-3900},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A resource-efficient predictive resource provisioning system in cloud systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient renaming in sequence CRDTs. <em>TPDS</em>,
<em>33</em>(12), 3870‚Äì3885. (<a
href="https://doi.org/10.1109/TPDS.2022.3172570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve high availability, large-scale distributed systems have to replicate data and to minimise coordination between nodes. For these purposes, literature and industry increasingly adopt Conflict-free Replicated Data Types (CRDTs) to design such systems. Conflict-free Replicated Data Types (CRDTs) are new specifications of existing data types, e.g., Set or Sequence. While CRDTs have the same behaviour as previous specifications in sequential executions, they actually shine in distributed settings as they natively support concurrent updates. To this end, CRDTs embed in their specification conflict resolution mechanisms. These mechanisms usually rely on identifiers attached to elements of the data structure to resolve conflicts in a deterministic and coordination-free manner. Identifiers have to comply with several constraints, such as being unique or belonging to a dense total order. These constraints may hinder the identifier size from being bounded. Identifiers hence tend to grow as the system progresses, which increases the overhead of CRDTs over time and leads to performance issues. To address this issue, we propose a novel Sequence CRDT which embeds a renaming mechanism. It enables nodes to reassign shorter identifiers to elements in an uncoordinated manner. Experimental results demonstrate that this mechanism decreases the overhead of the replicated data structure and eventually minimises it.},
  archive      = {J_TPDS},
  author       = {Matthieu Nicolas and G√©rald Oster and Olivier Perrin},
  doi          = {10.1109/TPDS.2022.3172570},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3870-3885},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient renaming in sequence CRDTs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The state of the art of metadata managements in large-scale
distributed file systems ‚Äî scalability, performance and availability.
<em>TPDS</em>, <em>33</em>(12), 3850‚Äì3869. (<a
href="https://doi.org/10.1109/TPDS.2022.3170574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {File system metadata is the data in charge of maintaining namespace, permission semantics and location of file data blocks. Operations on the metadata can account for up to 80\% of total file system operations. As such, the performance of metadata services significantly impacts the overall performance of file systems. A large-scale distributed file system (DFS) is a storage system that is composed of multiple storage devices spreading across different sites to accommodate data files, and in most cases, to provide users with location independent access interfaces. Large-scale DFSs have been widely deployed as a substrate to a plethora of computing systems, and thus their metadata management efficiency is crucial to a massive number of applications, especially with the advent of the Big Data age, which poses tremendous pressure on underlying storage systems. This paper reports the state-of-the-art research on metadata services in large-scale distributed file systems, which is conducted from three indicative perspectives that are always used to characterize DFSs: high-scalability, high-performance, and high-availability, with special focus on their respective major challenges as well as their developed mainstream technologies. Additionally, the paper also identifies and analyzes several existing problems in the research, which could be used as a reference for related studies.},
  archive      = {J_TPDS},
  author       = {Hao Dai and Yang Wang and Kenneth B. Kent and Lingfang Zeng and Chengzhong Xu},
  doi          = {10.1109/TPDS.2022.3170574},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3850-3869},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {The state of the art of metadata managements in large-scale distributed file systems ‚Äî scalability, performance and availability},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Astrea: Auto-serverless analytics towards cost-efficiency
and QoS-awareness. <em>TPDS</em>, <em>33</em>(12), 3833‚Äì3849. (<a
href="https://doi.org/10.1109/TPDS.2022.3172069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the ability to simplify the code deployment with one-click upload and lightweight execution, serverless computing has emerged as a promising paradigm with increasing popularity. However, there remain open challenges when adapting data-intensive analytics applications to the serverless context, in which users of serverless analytics encounter the difficulty in coordinating computation across different stages and provisioning resources in a large configuration space. This paper presents our design and implementation of Astrea , which configures and orchestrates serverless analytics jobs in an autonomous manner, while taking into account flexibly-specified user requirements. Astrea relies on the modeling of performance and cost which characterizes the intricate interplay among multi-dimensional factors (e.g., function memory size, degree of parallelism at each stage). We formulate an optimization problem based on user-specific requirements towards performance enhancement or cost reduction, and develop a set of algorithms based on graph theory to obtain the optimal job execution. We deploy Astrea in the AWS Lambda platform and conduct real-world experiments over representative benchmarks, including Big Data analytics and machine learning workloads, at different scales. Extensive results demonstrate that Astrea can achieve the optimal execution decision for serverless data analytics, in comparison with various provisioning and deployment baselines. For example, when compared with three provisioning baselines, Astrea manages to reduce the job completion time by 21\% to 69\% under a given budget constraint, while saving cost by 20\% to 84\% without violating performance requirements.},
  archive      = {J_TPDS},
  author       = {Jananie Jarachanthan and Li Chen and Fei Xu and Bo Li},
  doi          = {10.1109/TPDS.2022.3172069},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3833-3849},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Astrea: Auto-serverless analytics towards cost-efficiency and QoS-awareness},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). QoS-aware scheduling of remote rendering for interactive
multimedia applications in edge computing. <em>TPDS</em>,
<em>33</em>(12), 3816‚Äì3832. (<a
href="https://doi.org/10.1109/TPDS.2022.3172121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging emerging edge computing and 5G networks, researchers proposed to offload the 3D rendering of interactive multimedia applications (e.g., virtual reality and cloud gaming) onto edge servers. For high resource utilization, multiple rendering tasks run in the same GPU server and compete against each other for the computation resource. Each task has its requirement for performance, i.e., QoS target. A significant problem is how to schedule tasks so that each preset QoS is met and the performance of all tasks are maximized. We make the following contributions. First, we formulate the problem into a QoS constrained max-min utility problem. Second, we find that using the common natural logarithm as a utility function overly promotes one performance but demotes another. To avoid this phenomenon, we design a special utility function. Third, we propose an efficient scheduling algorithm, consisting of a resolution adjustment algorithm and a frame rate fair scheduling algorithm, both of which interact with each other. The former selects resolutions for tasks and the latter decides which task to process. We evaluate our method with actual rendering data, and the simulations demonstrate that our method can effectively improve task performance as well as satisfy QoS simultaneously.},
  archive      = {J_TPDS},
  author       = {Ruitao Xie and Junhong Fang and Junmei Yao and Kai Liu and Xiaohua Jia and Kaishun Wu},
  doi          = {10.1109/TPDS.2022.3172121},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3816-3832},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {QoS-aware scheduling of remote rendering for interactive multimedia applications in edge computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and implementation of 2D convolution on x86/x64
processors. <em>TPDS</em>, <em>33</em>(12), 3800‚Äì3815. (<a
href="https://doi.org/10.1109/TPDS.2022.3171471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new method for accelerating the 2D direct Convolution operation on x86/x64 processors is presented. It includes efficient vectorization by using SIMD intrinsics, bit-twiddling optimizations, the optimization of the division operation, multi-threading using OpenMP, register blocking and the shortest possible bit-width value of the intermediate results. The proposed method, which is provided as open-source, is general and can be applied to other processor families too, e.g., Arm. The proposed method has been evaluated on two different multi-core Intel CPUs, by using twenty different image sizes, 8-bit integer computations and the most commonly used kernel sizes (3x3, 5x5, 7x7, 9x9). It achieves from $2.8\times$ to $40\times$ speedup over the Intel IPP library (OpenCV GaussianBlur and Filter2D routines), from $105 \times$ to $400 \times$ speedup over the gemm-based convolution method (by using Intel MKL int8 matrix multiplication routine), and from $8.5\times$ to $618\times$ speedup over the vslsConvExec Intel MKL direct convolution routine. The proposed method is superior as it achieves far fewer arithmetical and load/store instructions.},
  archive      = {J_TPDS},
  author       = {Vasilios Kelefouras and Georgios Keramidas},
  doi          = {10.1109/TPDS.2022.3171471},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3800-3815},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Design and implementation of 2D convolution on x86/x64 processors},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). State space model and queuing network based cloud resource
provisioning for meshed web systems. <em>TPDS</em>, <em>33</em>(12),
3787‚Äì3799. (<a href="https://doi.org/10.1109/TPDS.2022.3170834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functions provided by Web applications are increasingly diverse which make their structures complicated and meshed. Cloud computing platforms provide elastic computing capacities for these meshed Web systems to guarantee Service Level Agreement (SLA). Though workloads of meshed Web systems usually change steadily and periodically in total, sometimes there are sudden fluctuations. In this paper, a hybrid State-space-model-and-Queuing-network based Feedback control method (SQF) is developed for auto-scaling Virtual Machines (VMs) allocated to each tier of meshed Web systems. For the case with workloads changing steadily, a State-space-model based static Feedback Control method (SFC) is proposed in SQF to stabilize request response times near the reference time. For unsteadily changing workloads, a Queuing-network based multi-tier collaborative Feedback Control method (QFC) is proposed for effectively eliminating bottlenecks. QFC builds a control system for each tier individually and uses the queuing network to measure the interaction relationships among different tiers. Experimental results show that QFC is able to improve the efficiency of eliminating bottlenecks (decreasing upper-limit SLA violation ratios by 31.99\% $\sim$ 56.52\%) with similar or a little bit high VM rental costs compared to existing methods while SFC obtains more stable response times for requests with reasonable additional costs.},
  archive      = {J_TPDS},
  author       = {Yamin Lei and Zhicheng Cai and Xiaoping Li and Rajkumar Buyya},
  doi          = {10.1109/TPDS.2022.3170834},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3787-3799},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {State space model and queuing network based cloud resource provisioning for meshed web systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deadline and reliability aware multiserver configuration
optimization for maximizing profit. <em>TPDS</em>, <em>33</em>(12),
3772‚Äì3786. (<a href="https://doi.org/10.1109/TPDS.2022.3170305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximizing profit is a key goal for cloud service providers in the modern cloud business market. Service revenue and business cost are two major factors in determining profit and highly depend on multiserver configuration. Understanding the relationship between multiserver configuration and profit is important to service providers. Although existing articles have explored this issue, few of them consider deadline miss rate and soft error reliability of cloud services in multiserver configuration for profit maximization. Since deadline misses violate cloud services‚Äô real-time requirements and soft error prevents successful processing of cloud services, it is necessary to consider the impact of deadline miss rate and soft error reliability on service providers‚Äô profits when configuring the multiserver. This article introduces a deadline miss rate and soft error reliability aware multiserver configuration scheme for maximizing cloud service providers‚Äô profit. Specifically, we derive the deadline miss rate considering the heterogeneity of cloud service requests, and propose an analytical method to compute the soft error reliability of multiserver systems. Based on the new deadline miss rate and soft error reliability models, we formulate the multiserver configuration optimization problem and introduce an augmented Lagrange multiplier-based iterative method to find the optimal multiserver configuration. Extensive experiments evaluate the efficacy of the proposed multiserver configuration approach. Compared with the two state-of-the-art methods, the profit gained by our scheme can be up to 11.92\% higher.},
  archive      = {J_TPDS},
  author       = {Tian Wang and Junlong Zhou and Liying Li and Gongxuan Zhang and Keqin Li and Xiaobo Sharon Hu},
  doi          = {10.1109/TPDS.2022.3170305},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3772-3786},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Deadline and reliability aware multiserver configuration optimization for maximizing profit},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint coverage-reliability for budgeted edge application
deployment in mobile edge computing environment. <em>TPDS</em>,
<em>33</em>(12), 3760‚Äì3771. (<a
href="https://doi.org/10.1109/TPDS.2022.3166163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC), as an emerging technology, allows application vendors to deploy application instances on edge servers to deliver low-latency services to nearby end-users. However, due to hardware faults, software exceptions, or cyberattacks, edge servers are prone to failures in the highly distributed and dynamic MEC environment. Hence service reliability must be ensured when failures occur. This raises a critical and open problem - improving service reliability when deploying application instances in the MEC environment. In this article, we jointly consider both user coverage and service reliability when deploying application instances on edge servers with a given application deployment budget $\mathcal {K}$ . We formally define this joint C overage- R eliability for $\mathcal {K}$ - B udgeted E dge A pplication D eployment ( CR-BEAD ) problem and model it as a constrained optimization problem. Next, we propose an optimal approach (named BEAD-O ) based on integer programming to find optimal solutions to small-scale CR-BEAD problems. We also propose a greedy approach named BEAD-G with a constant approximation ratio of $1 - 1/e$ to solve large-scale CR-BEAD problems efficiently. Extensive experimental evaluation against three representative approaches illustrates the effectiveness and efficiency of our approaches.},
  archive      = {J_TPDS},
  author       = {Lu Zhao and Bo Li and Wenan Tan and Guangming Cui and Qiang He and Xiaolong Xu and Lida Xu and Yun Yang},
  doi          = {10.1109/TPDS.2022.3166163},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3760-3771},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Joint coverage-reliability for budgeted edge application deployment in mobile edge computing environment},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online thread auto-tuning for performance improvement and
resource saving. <em>TPDS</em>, <em>33</em>(12), 3746‚Äì3759. (<a
href="https://doi.org/10.1109/TPDS.2022.3169410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-threading is a common way for programs to benefit from the multi/many-core design. However, the performance of some parallel programs does not increase/even decrease as the number of cores/threads increases. Our study shows that the performance of a parallel program is impacted by the number of cores/threads , the thread placement , the inputs of the program . It is nontrivial to identify the optimal number of cores and the corresponding thread placement to maximize the performance, when the input of a program is determined online and the workload of different iterations may not be identical. To resolve the above problem, we propose Otter , a thread auto-tuning system at runtime for iterative parallel programs. Otter collects the runtime information in the first few iterations and makes decisions on the number of threads and thread placement policy to achieve the goal of improving performance or saving resources. It considers the characteristics of dynamic workload in the iteration process and reduces the time overhead through a migration method. Experiments on a 96-core machine show that Otter improves the performance of the benchmarks by 20.7\% and reduces core hours by 51.3\% on average compared to the case of running them with all the CPU cores.},
  archive      = {J_TPDS},
  author       = {Guangqiang Luan and Pu Pang and Quan Chen and Shuai Xue and Zhuo Song and Minyi Guo},
  doi          = {10.1109/TPDS.2022.3169410},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3746-3759},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Online thread auto-tuning for performance improvement and resource saving},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TaiChi: A hybrid compression format for binary sparse
matrix-vector multiplication on GPU. <em>TPDS</em>, <em>33</em>(12),
3732‚Äì3745. (<a href="https://doi.org/10.1109/TPDS.2022.3170501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary Sparse Matrix-Vector Multiplication (SpMV) is a heavy computational kernel in weblink analysis, integer factorization, compressed sensing, spectral graph theory, and other domains. Testing several popular GPU-based SpMV implementations on 400 sparse matrices, we observed that data transfer to GPU memory accounts for a large part of the total computation time. The transfer of constant value ‚Äú1‚Äùs can be easily eliminated for binary sparse matrices. However, compressing index arrays has always been a great challenge. This article proposes a new compression format TaiChi to further reduce index data copies and improve the performance of SpMV, especially for diagonally dominant binary sparse matrices. Input matrices are first partitioned into relatively dense and ultra-sparse areas. Then the dense areas are encoded inversely by marking ‚Äú0‚Äùs, while the ultra-sparse area is encoded by marking ‚Äú1‚Äùs. We also designed a new SpMV algorithm only using addition and subtraction for binary matrices based on our partition and encoding format. Evaluation results on real-world binary sparse matrices show that our hybrid encoding for binary matrix significantly reduces the data transfer and speeds up the kernel execution. It achieves the highest transfer and kernel execution speedups of 5.63x and 3.84x on GTX 1080 Ti, 3.39x and 3.91x on Tesla V100.},
  archive      = {J_TPDS},
  author       = {Jianhua Gao and Weixing Ji and Zhaonian Tan and Yizhuo Wang and Feng Shi},
  doi          = {10.1109/TPDS.2022.3170501},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3732-3745},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {TaiChi: A hybrid compression format for binary sparse matrix-vector multiplication on GPU},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed evolution strategies for black-box stochastic
optimization. <em>TPDS</em>, <em>33</em>(12), 3718‚Äì3731. (<a
href="https://doi.org/10.1109/TPDS.2022.3168873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work concerns the evolutionary approaches to distributed stochastic black-box optimization, in which each worker can individually solve an approximation of the problem with nature-inspired algorithms. We propose a distributed evolution strategy (DES) algorithm grounded on a proper modification to evolution strategies, a family of classic evolutionary algorithms, as well as a careful combination with existing distributed frameworks. On smooth and nonconvex landscapes, DES has a convergence rate competitive to existing zeroth-order methods, and can exploit the sparsity, if applicable, to match the rate of first-order methods. The DES method uses a Gaussian probability model to guide the search and avoids the numerical issue resulted from finite-difference techniques in existing zeroth-order methods. The DES method is also fully adaptive to the problem landscape, as its convergence is guaranteed with any parameter setting. We further propose two alternative sampling schemes which significantly improve the sampling efficiency while leading to similar performance. Simulation studies on several machine learning problems suggest that the proposed methods show much promise in reducing the convergence time and improving the robustness to parameter settings.},
  archive      = {J_TPDS},
  author       = {Xiaoyu He and Zibin Zheng and Chuan Chen and Yuren Zhou and Chuan Luo and Qingwei Lin},
  doi          = {10.1109/TPDS.2022.3168873},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3718-3731},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Distributed evolution strategies for black-box stochastic optimization},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ZMesh: Theories and methods to exploring application
characteristics to improve lossy compression ratio for adaptive mesh
refinement. <em>TPDS</em>, <em>33</em>(12), 3702‚Äì3717. (<a
href="https://doi.org/10.1109/TPDS.2022.3168386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific simulations on high-performance computing systems produce vast amounts of data that need to be stored and analyzed efficiently. Lossy compression significantly reduces the data volume by trading accuracy for performance. Despite the recent success of lossy compressions, such as ZFP and SZ, the compression performance is still far from being able to keep up with the exponential growth of data. This article aims to further take advantage of application characteristics, an area that is often under-explored, to improve the compression ratios of adaptive mesh refinement (AMR) - a widely used numerical solver that allows for an improved resolution in limited regions. We propose a level reordering technique zMesh to reduce the storage footprint of AMR applications. In particular, we group the data points that are mapped to the same or adjacent geometric coordinates such that the dataset is smoother and more compressible. Unlike the prior work where the compression performance is affected by the overhead of metadata, this work re-generates the restore recipe using a chained tree structure, thus involving no extra storage overhead for compressed data, which substantially improves the compression ratios. We further derive a mathematical proof that lays the foundation for our method. The results demonstrate that zMesh can improve the smoothness of data by 67.9\% and 71.3\% for Z-ordering and Hilbert, respectively. Overall, zMesh improves the compression ratios by up to 16.5\% and 133.7\% for ZFP and SZ, respectively. Despite that zMesh involves additional compute overhead for tree and restore recipe construction, we show that the cost can be amortized as the number of quantities to be compressed increases.},
  archive      = {J_TPDS},
  author       = {Huizhang Luo and Junqi Wang and Qing Liu and Jieyang Chen and Scott Klasky and Norbert Podhorszki},
  doi          = {10.1109/TPDS.2022.3168386},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3702-3717},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ZMesh: Theories and methods to exploring application characteristics to improve lossy compression ratio for adaptive mesh refinement},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Differentially private byzantine-robust federated learning.
<em>TPDS</em>, <em>33</em>(12), 3690‚Äì3701. (<a
href="https://doi.org/10.1109/TPDS.2022.3167434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a collaborative machine learning framework where a global model is trained by different organizations under the privacy restrictions. Promising as it is, privacy and robustness issues emerge when an adversary attempts to infer the private information from the exchanged parameters or compromise the global model. Various protocols have been proposed to counter the security risks, however, it becomes challenging when one wants to make federated learning protocols robust against Byzantine adversaries while preserving the privacy of the individual participant. In this article, we propose a differentially private Byzantine-robust federated learning scheme (DPBFL) with high computation and communication efficiency. The proposed scheme is effective in preventing adversarial attacks launched by the Byzantine participants and achieves differential privacy through a novel aggregation protocol in the shuffle model. The theoretical analysis indicates that the proposed scheme converges to the approximate optimal solution with the learning error dependent on the differential privacy budget and the number of Byzantine participants. Experimental results on MNIST, FashionMNIST and CIFAR10 demonstrate that the proposed scheme is effective and efficient.},
  archive      = {J_TPDS},
  author       = {Xu Ma and Xiaoqian Sun and Yuduo Wu and Zheli Liu and Xiaofeng Chen and Changyu Dong},
  doi          = {10.1109/TPDS.2022.3167434},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3690-3701},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Differentially private byzantine-robust federated learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Communicational and computational efficient federated domain
adaptation. <em>TPDS</em>, <em>33</em>(12), 3678‚Äì3689. (<a
href="https://doi.org/10.1109/TPDS.2022.3167457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging paradigm of Federated Learning enables mobile users to collaboratively train a model without disclosing their privacy-sensitive data. Nevertheless, data collected from different mobile users may not be independent and identically distributed. Thus directly applying the trained model to a new mobile user usually leads to performance degradation due to the so-called domain shift. Unsupervised Domain Adaptation is an effective technique to mitigate domain shift and transfer knowledge from labeled source domains to the unlabeled target domain. In this article, we design a Federated Domain Adaptation framework that extends Domain Adaptation with the constraints of Federated Learning to train a model for the target domain and preserve the data privacy of all the source and target domains. As mobile devices usually have limited computation and communication capabilities, we design a set of optimization methods that significantly enhance our framework‚Äôs computation and communication efficiency, making it more friendly to resource-constrained edge devices. Evaluation results on three datasets show that our framework has comparable performance with the standard centralized training approach, and the optimization methods can reduce the computation and communication overheads by up to two orders of magnitude.},
  archive      = {J_TPDS},
  author       = {Hua Kang and Zhiyang Li and Qian Zhang},
  doi          = {10.1109/TPDS.2022.3167457},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3678-3689},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Communicational and computational efficient federated domain adaptation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient data redistribution algorithms from irregular to
block cyclic data distribution. <em>TPDS</em>, <em>33</em>(12),
3667‚Äì3677. (<a href="https://doi.org/10.1109/TPDS.2022.3166484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose some efficient data redistribution algorithms for redistributing matrices from 1D or 2D irregular format to block cyclic data distribution (BCDD) format, which can be much faster than the BLACS routine PXGEMR2D . These algorithms can be used to combine direct methods with iterative methods. The proposed algorithms divide the communication into two phases: one for processes in the same column and the other for processes in the same row, and the whole data redistribution task is divided into several independent sub-communications. The communication time can be reduced a lot compared with BLACS. Performance results show that our algorithms can be $2\times$ ‚Äì $5\times$ faster than the BLACS routine PXGEMR2D when using 4096 processes and the experiments are performed on Tianhe-2A supercomputer.},
  archive      = {J_TPDS},
  author       = {Shengguo Li and Hao Jiang and Dezun Dong and Chun Huang and Jie Liu and Xia Liao and Xuguang Chen},
  doi          = {10.1109/TPDS.2022.3166484},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3667-3677},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient data redistribution algorithms from irregular to block cyclic data distribution},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High performance evaluation of helmholtz potentials using
the multi-level fast multipole algorithm. <em>TPDS</em>,
<em>33</em>(12), 3651‚Äì3666. (<a
href="https://doi.org/10.1109/TPDS.2022.3165649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluation of pair potentials is critical in a number of areas of physics. The classical $N$ -body problem has its root in evaluating the Laplace potential, and has spawned tree-algorithms, the fast multipole method (FMM), as well as kernel independent approaches. Over the years, FMM for Laplace potential has had a profound impact on a number of disciplines as it has been possible to develop highly scalable parallel versions of these algorithms. This is in stark contrast to parallel algorithms for oscillatory potentials such as the Helmholtz potential. The principal bottlenecks to scalable parallelism are the computation and communication costs of operations necessary to traverse up, across, and down the tree. In this article, we analyze asymptotic costs for both computation and communication in a parallel implementation, and describe techniques to overcome bottlenecks and achieve high performance evaluation of the Helmholtz potential for different distributions of particles. We demonstrate that the resulting implementation has a load balancing effect that significantly reduces the time-to-solution and enhances the scale of problems that can be treated using full wave physics.},
  archive      = {J_TPDS},
  author       = {Michael P. Lingg and Stephen M. Hughey and Balasubramaniam Shanker and Hasan Metin Aktulga},
  doi          = {10.1109/TPDS.2022.3165649},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3651-3666},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {High performance evaluation of helmholtz potentials using the multi-level fast multipole algorithm},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ethernet for high-throughput computing at CERN.
<em>TPDS</em>, <em>33</em>(12), 3640‚Äì3650. (<a
href="https://doi.org/10.1109/TPDS.2022.3163472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When high throughput and utilization of fabric at close-to-the-link capacity are most needed in a cluster, Ethernet is a potential candidate, rivaling traditional HPC interconnects. The distributed real-time data acquisition at particle physics experiments presents an interesting use case. This article evaluates possible Ethernet-based solutions for aggregating data from hundreds of data sources at a throughput of dozens of Tb/s. This leads us to many-to-one data exchanges where we strive for a cost-optimized setup sustaining more than 80\% of the theoretical link-load. We investigate possible Ethernet-based traffic patterns to handle data acquisition on large multi-source apparatuses. Different numbers of producers and receivers and different link speeds are allowed in a large-scale network. Performance tests were conducted using customized benchmarks and evaluation test benches. The article presents tested scenarios and problems encountered in practice. We describe how our findings influenced the design of a large production system at CERN. We also present relevant general conclusions for a broader range of applications of Ethernet in HPC.},
  archive      = {J_TPDS},
  author       = {Rafal Krawczyk and Tommaso Colombo and Niko Neufeld and Flavio Pisani and S√©bastien Valat},
  doi          = {10.1109/TPDS.2022.3163472},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3640-3650},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Ethernet for high-throughput computing at CERN},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Meet: Rack-level pooling based load balancing in datacenter
networks. <em>TPDS</em>, <em>33</em>(12), 3628‚Äì3639. (<a
href="https://doi.org/10.1109/TPDS.2022.3162297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datacenter networks enable multiple paths between hosts to provide large bisection bandwidth. It requires load balancers to cope with network uncertainties such as traffic dynamics and topology asymmetry. Existing edge-based load balancing schemes are usually faced with the problem of limited network visibility. This article proposes Meet , a rack-level pooling based load-balancer deployed at the edge that can handle the aformentioned uncertainties. Meet utilizes both passive information as well as active probing to comprehensively sense the network conditions with relatively low cost. Meet dynamically reroutes flows effectively based on the visibility of the network condition. Meet has been tested with extensive flow-level simulations against state-of-the-art load balancers. It outperforms Hermes by up to 10\% in the experiments, and outperforms others solutions such as DRILL by up to 50\%. Meet requires no modifications to the switches and is feasible to deploy at the edge.},
  archive      = {J_TPDS},
  author       = {Jiaqing Dong and Lijuan Tan and Chen Tian and Yuhang Zhou and Yi Wang and Wanchun Dou and Guihai Chen},
  doi          = {10.1109/TPDS.2022.3162297},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3628-3639},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Meet: Rack-level pooling based load balancing in datacenter networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep neural network training with distributed k-FAC.
<em>TPDS</em>, <em>33</em>(12), 3616‚Äì3627. (<a
href="https://doi.org/10.1109/TPDS.2022.3161187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scaling deep neural network training to more processors and larger batch sizes is key to reducing end-to-end training time; yet, maintaining comparable convergence and hardware utilization at larger scales is challenging. Increases in training scales have enabled natural gradient optimization methods as a reasonable alternative to stochastic gradient descent and variants thereof. Kronecker-factored Approximate Curvature (K-FAC), a natural gradient method, preconditions gradients with an efficient approximation of the Fisher Information Matrix to improve per-iteration progress when optimizing an objective function. Here we propose a scalable K-FAC algorithm and investigate K-FAC‚Äôs applicability in large-scale deep neural network training. Specifically, we explore layer-wise distribution strategies, inverse-free second-order gradient evaluation, and dynamic K-FAC update decoupling, with the goal of preserving convergence while minimizing training time. We evaluate the convergence and scaling properties of our K-FAC gradient preconditioner, for image classification, object detection, and language modeling applications. In all applications, our implementation converges to baseline performance targets in 9‚Äì25\% less time than the standard first-order optimizers on GPU clusters across a variety of scales.},
  archive      = {J_TPDS},
  author       = {J. Gregory Pauloski and Lei Huang and Weijia Xu and Kyle Chard and Ian T. Foster and Zhao Zhang},
  doi          = {10.1109/TPDS.2022.3161187},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3616-3627},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Deep neural network training with distributed K-FAC},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Batch crowdsourcing for complex tasks based on distributed
team formation in e-markets. <em>TPDS</em>, <em>33</em>(12), 3600‚Äì3615.
(<a href="https://doi.org/10.1109/TPDS.2022.3161019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Team formation has been extensively studied for complex task crowdsourcing in E-markets, in which a set of workers are hired to form a team to complete a complex task collaboratively. However, existing studies have two typical drawbacks: 1) each team is created for only one task, which may be costly and cannot accommodate crowdsourcing markets with a large number of tasks; and 2) most existing studies form teams in a centralized manner by the requesters, which may place a heavy burden on requesters. In fact, we observe that many complex tasks at real-world crowdsourcing platforms have similar skill requirements and workers are often connected through social networks. Therefore, this paper explores distributed team formation-based batch crowdsourcing for complex tasks to address the drawbacks in existing studies, in which similar tasks can be addressed in a batch to reduce computational costs and workers can self-organize through their social networks to form teams. To solve such an NP-hard problem, this paper presents two approaches: one is to form a fixed team for all tasks in the batch; the other is to form a basic team that can be dynamically adjusted for each task in the batch. In comparison, the former approach has lower computational complexity but the latter approach performs better in reducing the total payments by requesters. With the experiments on a real-world dataset comparing with previous benchmark approaches, it is shown that the presented approaches have better performance in saving the costs of forming teams, payments by requesters, and communication among team members; moreover, the presented approaches have higher success rate of tasks and much better scalability.},
  archive      = {J_TPDS},
  author       = {Jiuchuan Jiang and Kai Di and Bo An and Yichuan Jiang and Zhan Bu and Jie Cao},
  doi          = {10.1109/TPDS.2022.3161019},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3600-3615},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Batch crowdsourcing for complex tasks based on distributed team formation in E-markets},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Storage-heterogeneity aware task-based programming models to
optimize i/o intensive applications. <em>TPDS</em>, <em>33</em>(12),
3589‚Äì3599. (<a href="https://doi.org/10.1109/TPDS.2022.3161123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task-based programming models have enabled the optimized execution of the computation workloads of applications. These programming models can take advantage of large-scale distributed infrastructures by allowing the parallel and distributed execution of applications in high-level work components called tasks . Nevertheless, in the era of Big Data and Exascale, the amount of data produced by modern scientific applications has already surpassed terabytes and is rapidly increasing. Hence, I/O performance became the bottleneck to overcome in order to achieve more total performance improvement. New storage technologies offer higher bandwidth and faster solutions than traditional Parallel File Systems (PFS). Such storage devices are deployed in modern day infrastructures to boost I/O performance by offering a fast layer that absorbs the generated data. Therefore, it is necessary for any programming model targeting more performance to manage this heterogeneity and take advantage of it to improve the I/O performance of applications. Towards this goal, we propose in this article a set of programming model capabilities that we refer to as Storage-Heterogeneity Awareness . Such capabilities include: (i) abstracting the heterogeneity of storage systems, and (ii) optimizing I/O performance by supporting dedicated I/O schedulers and an automatic data flushing technique. The evaluation section of this article presents the performance results of different applications on the MareNostrum CTE-Power heterogeneous storage cluster. Our experiments demonstrate that a storage-heterogeneity aware programming model can achieve up to almost 5x I/O performance speedup and 48\% total time improvement compared to the reference PFS-based usage of the execution infrastructure.},
  archive      = {J_TPDS},
  author       = {Hatem Elshazly and Jorge Ejarque and Rosa M. Badia},
  doi          = {10.1109/TPDS.2022.3161123},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3589-3599},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Storage-heterogeneity aware task-based programming models to optimize I/O intensive applications},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SciSpot: Scientific computing on temporally constrained
cloud preemptible VMs. <em>TPDS</em>, <em>33</em>(12), 3575‚Äì3588. (<a
href="https://doi.org/10.1109/TPDS.2022.3157272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific computing applications are being increasingly deployed on cloud computing platforms. Transient servers such as EC2 spot instances and Google Preemptible VMs, can be used to lower the costs of running applications on the cloud by up to $10\times$ . However, the frequent preemptions and resource heterogeneity of these transient servers introduces many challenges in their effective and efficient use. In this paper, we develop techniques for modeling and mitigating preemptions of transient servers, and present SciSpot, a software framework that enables low-cost scientific computing on the cloud. SciSpot deploys applications on Google Cloud Preemptible Virtual Machines that exhibit temporally constrained preemptions: VMs are always preempted in a 24 hour interval. Our empirical analysis shows that the preemption rate is generally bathtub shaped, which raises multiple fundamental challenges in performance modeling and policy design. We develop a new reliability model for temporally constrained preemptions, and use statistical mechanics to show why the bathtub shape is generally exhibited. SciSpot‚Äôs design is guided by our observation that many emerging scientific computing applications that integrate machine learning with simulations, can be deployed as ‚Äúbags‚Äù of jobs, which represent multiple instantiations of the same computation with different physical model parameters. For a bag of jobs, SciSpot finds the optimal transient server on-the-fly, by taking into account the price, performance, and preemption rates of different servers. SciSpot reduces costs by $5\times$ compared to conventional cloud deployments, and reduces makespans by up to $10\times$ compared to conventional high performance computing clusters.},
  archive      = {J_TPDS},
  author       = {JCS Kadupitiya and Vikram Jadhao and Prateek Sharma},
  doi          = {10.1109/TPDS.2022.3157272},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3575-3588},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SciSpot: Scientific computing on temporally constrained cloud preemptible VMs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Leveraging code snippets to detect variations in the
performance of HPC systems. <em>TPDS</em>, <em>33</em>(12), 3558‚Äì3574.
(<a href="https://doi.org/10.1109/TPDS.2022.3158742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variations in the performance of parallel and distributed systems are becoming increasingly challenging. The runtimes of different executions can vary greatly even with a fixed number of computing nodes. Many HPC applications on supercomputers exhibit such variance. This not only leads to unpredictable execution times, but also renders the system‚Äôs behavior unintuitive. The efficient online detection of variations in performance is an open problem in HPC research. To solve it, we propose an approach, called vSensor , to detect variations in the performance of systems. The key finding of this study is that the source code of programs can better represent performance at runtime than an external detector. Specifically, many HPC applications contain code snippets that are fixed workload patterns of execution, e.g., the workload of an invariant quantity and a linearly growing workload. This observation allows us to automatically identify these snippets of workload-related code and use them to detect variations in performance. We evaluate vSensor on the Tianhe-2A system with a large number of parallel applications, and the results indicate that it can efficiently identify variations in system performance. The average overhead of 4,096 processes is less than 6\% for fixed-workload v-sensors. We identify a problematic node with slow memory by using vSensor that degrades the performance of the program by 21\%. A serious issue with network performance is also detected that slows down the Tianhe-2A system by 3.37 times for an HPC kernel.},
  archive      = {J_TPDS},
  author       = {Jidong Zhai and Liyan Zheng and Jinghan Sun and Feng Zhang and Xiongchao Tang and Xuehai Qian and Bingsheng He and Wei Xue and Wenguang Chen and Weimin Zheng},
  doi          = {10.1109/TPDS.2022.3158742},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3558-3574},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Leveraging code snippets to detect variations in the performance of HPC systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reliable wide-area data transfers for streaming workflows.
<em>TPDS</em>, <em>33</em>(12), 3546‚Äì3557. (<a
href="https://doi.org/10.1109/TPDS.2022.3158673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many large science projects rely on remote clusters for (near) real-time data processing, thus they demand reliable wide-area data transfer performance for smooth end-to-end workflow executions. However, data transfers are often exposed to performance variations due to the changing network (e.g., background traffic) and dataset (e.g., average file size) conditions, necessitating adaptive solutions to meet stringent performance requirements of delay-sensitive streaming workflows. In this article, we propose FStream++ to provide reliable transfer performance for large streaming science applications by dynamically adjusting transfer settings to adapt to changing transfer conditions. FStream++ combines three optimization methods as dynamic tuning , online profiling , and historical analysis to swiftly and accurately discover optimal transfer settings that can meet workflow requirements. Dynamic tuning uses a heuristic model to predict the values of transfer parameters based on dataset characteristics and network settings. Since heuristic models fall short to incorporate many important factors such as I/O throughput and resource interference, we complement it with online profiling to execute a real-time search for a subset of transfer settings. Finally, historical analysis takes advantage of the long-running nature of streaming workflows by storing and analyzing previous performance observations to shorten the execution time of online profiling. We evaluate the performance of FStream++ by transferring several synthetic and real-world workloads in high-performance production networks and show that it offers up to $3.6x$ performance improvement over legacy transfer applications and up to 24\% over our previous work FStream .},
  archive      = {J_TPDS},
  author       = {Hemanta Sapkota and Engin Arslan},
  doi          = {10.1109/TPDS.2022.3158673},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3546-3557},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Reliable wide-area data transfers for streaming workflows},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal convex hull formation on a grid by asynchronous
robots with lights. <em>TPDS</em>, <em>33</em>(12), 3532‚Äì3545. (<a
href="https://doi.org/10.1109/TPDS.2022.3158202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the distributed setting of $n$ autonomous mobile robots that operate in Look-Compute-Move cycles and communicate with other robots using a constant number of colored lights (the robots with lights model). We assume obstructed visibility where collinear robots do not see each other. In addition, we consider a grid-based terrain embedded in the 2-dimensional euclidean plane. The Convex Hull Formation problem is to relocate the $n$ robots (starting at arbitrary, but distinct, initial positions) so that each robot is positioned on a vertex of a convex hull. In this article, we provide a framework for solving Convex Hull Formation . We then provide four asynchronous algorithms under this framework. Key measures of the algorithms‚Äô performance include the time taken and the space occupied. The presented algorithms are randomized and their time bounds hold with high probability. The first $O(\max \lbrace n^{2},D\rbrace)$ -time, $O({n^{2}})$ -perimeter, and $O({n^{3}})$ -area algorithm serves to introduce key ideas, where $D$ is the diameter of the initial configuration. The subsequent algorithms, differing in computational requirements, run in $O(\max \lbrace n^{\frac{3}{2}},D\rbrace)$ time with a perimeter of $O(n^{\frac{3}{2}})$ and area of $O(n^{3})$ . We also prove lower bounds of $\Omega (n^{\frac{3}{2}})$ for time and perimeter and $\Omega (n^{3})$ for area, for any Convex Hull Formation algorithm; i.e., our $O(\max \lbrace n^{\frac{3}{2}},D\rbrace)-$ time algorithm is optimal in time, perimeter, and area.},
  archive      = {J_TPDS},
  author       = {Rory Hector and Ramachandran Vaidyanathan and Gokarna Sharma and Jerry L. Trahan},
  doi          = {10.1109/TPDS.2022.3158202},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3532-3545},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimal convex hull formation on a grid by asynchronous robots with lights},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Max-tree computation on GPUs. <em>TPDS</em>,
<em>33</em>(12), 3520‚Äì3531. (<a
href="https://doi.org/10.1109/TPDS.2022.3158488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Mathematical Morphology, the max-tree is a region-based representation that encodes the inclusion relationship of the threshold sets of an image. This tree has proved useful in numerous image processing applications. For the last decade, work has led to improving the construction time of this structure; mixing algorithmic optimizations, parallel and distributed computing. Nevertheless, there is still no algorithm that benefits from the computing power of the massively parallel architectures. In this work, we propose the first GPU algorithm to compute the max-tree. The proposed approach leads to significant speed-ups, and is up to one order of magnitude faster than the current State-of-the-Art parallel CPU algorithms. This work paves the way for a max-tree integration in image processing GPU pipelines and real-time image processing based on Mathematical Morphology. It is also a foundation for porting other image representations from Mathematical Morphology on GPUs.},
  archive      = {J_TPDS},
  author       = {Nicolas Blin and Edwin Carlinet and Florian Lemaitre and Lionel Lacassagne and Thierry G√©raud},
  doi          = {10.1109/TPDS.2022.3158488},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3520-3531},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Max-tree computation on GPUs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OptZConfig: Efficient parallel optimization of lossy
compression configuration. <em>TPDS</em>, <em>33</em>(12), 3505‚Äì3519.
(<a href="https://doi.org/10.1109/TPDS.2022.3154096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lossless compressors have very low compression ratios that do not meet the needs of today‚Äôs large-scale scientific applications that produce vast volumes of data. Error-bounded lossy compression (EBLC) is considered a critical technique for the success of scientific research. Although EBLC allows users to set an error bound for the compression, users have been unable to specify the requirements on the compression quality, limiting practical use. Our contributions are: (1) We formulate the problem of configuring EBLC to preserve a user-defined metric as an optimization problem. This allows many classes of new metrics to be preserved, which improves over current practices. (2) We present a framework, OptZConfig, that can adapt to improvements in the search algorithm, compressor, and metrics with minimal changes, enabling future advancements in this area. (3) We demonstrate the advantages of our approach against the leading methods to configure compressors to preserve specific metrics. Our approach improves compression ratios against a specialized compressor by up to $3\times$ , has a 56√ó speedup over FRaZ, 1000√ó speedup over MGARD-QOI post tuning, and 110√ó speedup over systematic approaches which had not been bounded by compressors before.},
  archive      = {J_TPDS},
  author       = {Robert Underwood and Jon C. Calhoun and Sheng Di and Amy Apon and Franck Cappello},
  doi          = {10.1109/TPDS.2022.3154096},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3505-3519},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {OptZConfig: Efficient parallel optimization of lossy compression configuration},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Jdebug: A fast, non-intrusive and scalable fault locating
tool for ten-million-scale parallel applications. <em>TPDS</em>,
<em>33</em>(12), 3491‚Äì3504. (<a
href="https://doi.org/10.1109/TPDS.2022.3157690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents Jdebug, a fast, non-intrusive and scalable fault locating tool for extreme-scale parallel applications. Large-scale debugging has drawn more attention with the increasing scale of supercomputers and applications. To eliminate program intrusion caused by traditional instrumentation or interception during debugging information acquisition, we introduce the out-of-band management into large-scale debugging. We propose a rapid information gathering scheme that separates user and debugging traffic to solve scalability problem and to eliminate program interference during merging data. Observations of Program Counters (PC) and performance characteristics in suspended applications find abnormalities and help locate abnormal threads caused by software errors or hardware failures effectively. Evaluation shows that Jdebug collects PCs of over 20 million cores on the new Sunway supercomputer within 1.97 seconds, and can locate the abnormal threads in 1.4 seconds with an accuracy of 92.5\%. In the running test of three fundamental benchmarks (HPL, HPCG, Graph500) and seventeen real-world applications, Jdebug quickly and accurately locates abnormal threads to help find scalability errors and hardware failures including memory access failures, communication failures, and execution component failures, which validates its effectiveness.},
  archive      = {J_TPDS},
  author       = {Dajia Peng and Yunlong Feng and Yong Liu and Xin Liu and Wei Xue and Dexun Chen and Jiawei Song and Zuoning Chen},
  doi          = {10.1109/TPDS.2022.3157690},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3491-3504},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Jdebug: A fast, non-intrusive and scalable fault locating tool for ten-million-scale parallel applications},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Solving consensus in true partial synchrony. <em>TPDS</em>,
<em>33</em>(12), 3478‚Äì3490. (<a
href="https://doi.org/10.1109/TPDS.2022.3156925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The notion of partial synchrony has been introduced to circumvent the FLP impossibility result for solvability of consensus in fault tolerant distributed systems. This notion helps us to evaluate the efficiency of algorithms that needs to solve consensus in the given time interval $ T[\tau _{1} \leq T_{S} \leq \tau _{2}]$ where $ T_{S}$ is the stable time when the system becomes synchronous after a transient period of asynchrony. It has been shown that consensus can be solved in constant time after system enters $ T_{S}$ with an upper bound of $ T_{S}+17\delta$ , and this was later reduced to $ T_{S}+11\delta$ , where $ \delta$ is the upper bound on message delivery. The above result is not trivial as they allowed processes that failed before $ T_{S}$ to recover after $ T_{S}$ and participate in consensus. But these algorithms assume that the upper bound for message delivery $\delta$ holds even for messages sent before $ T_{S}$ (i.e.,) when the system is in asynchrony. This assumption is limiting as messages can get delayed beyond expected timings in real time distributed systems due to different reasons like network congestion, packet re-transmission, e.t.c. In this work, we overcome this limitation by assuming messages sent before $ T_{S}$ has no upper bound while also allowing process restarts after $ T_{S}$ and show that consensus can be solved in constant time after $ T_{S}$ . Our proposed algorithm solves consensus in $ T_{S}+10\delta$ time, which is more efficient than the current upper bound and without the limiting assumption of bounded message delay before $ T_{S}$ .},
  archive      = {J_TPDS},
  author       = {Sathyanarayanan Srinivasan and Ramesh Kandukoori},
  doi          = {10.1109/TPDS.2022.3156925},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3478-3490},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Solving consensus in true partial synchrony},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OCTOPUS: Overcoming performance and privatization
bottlenecks in distributed learning. <em>TPDS</em>, <em>33</em>(12),
3460‚Äì3477. (<a href="https://doi.org/10.1109/TPDS.2022.3157258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diversity and quantity of data warehouses, gathering data from distributed devices such as mobile devices, can enhance the success and robustness of machine learning algorithms. Federated learning enables distributed participants to collaboratively learn a commonly shared model while holding data locally. However, it is also faced with expensive communication and limitations due to the heterogeneity of distributed data sources and lack of access to global data. In this paper, we investigate a practical distributed learning scenario where multiple downstream tasks (e.g., classifiers) could be efficiently learned from dynamically updated and non-iid distributed data sources while providing local data privatization. We introduce a new distributed/collaborative learning scheme to address communication overhead via latent compression, leveraging global data while providing privatization of local data without additional cost due to encryption or perturbation. This scheme divides learning into (1) informative feature encoding, and transmitting the latent representation of local data to address communication overhead; (2) downstream tasks centralized at the server using the encoded codes gathered from each node to address computing overhead. Besides, a disentanglement strategy is applied to address the privatization of sensitive components of local data. Extensive experiments are conducted on image and speech datasets. The results demonstrate that downstream tasks with the compact latent representations with the privatization of local data can achieve comparable accuracy to centralized learning.},
  archive      = {J_TPDS},
  author       = {Shuo Wang and Surya Nepal and Kristen Moore and Marthie Grobler and Carsten Rudolph and Alsharif Abuadbba},
  doi          = {10.1109/TPDS.2022.3157258},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3460-3477},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {OCTOPUS: Overcoming performance and privatization bottlenecks in distributed learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient flow-based scheduling for geo-distributed
simulation tasks in collaborative edge and cloud environments.
<em>TPDS</em>, <em>33</em>(12), 3442‚Äì3459. (<a
href="https://doi.org/10.1109/TPDS.2022.3155713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing is a good complement to cloud computing for deploying large-scale geo-distributed simulation applications, which are very sensitive to the communication delay among different simulation components (also called tasks in this paper) and users. We mainly focus on the efficient scheduling of simulation components in collaborative edge and cloud environments. As components should be deployed jointly with the consideration of capacity constraints of hosts, it is actually an NP-complete multi-dimensional bin packing problem. Meanwhile, dynamic changes of component and host states require the low deployment latency of scheduling algorithms. Unfortunately, most of the existing schedulers for modern clusters are queue-based, in which tasks are scheduled sequentially, thus lacking the ability to process tightly coupled tasks jointly. Other batching-based placement algorithms are usually time-consuming. This paper describes Pond, a novel flow-based scheduler with the awareness of interactions among tasks and users as well as heterogeneous multi-dimensional resources. First, characteristics of distributed simulation tasks are analysed and the scheduling problem is formulated as a min-cost max-flow (MCMF) problem over the flow network by mapping the communication overhead among tasks and users to the costs of arcs in the network. Considering the inherent defects of existing flow-based schedulers in dealing with multi-dimensional resources, a new method based on dominant resource is proposed and some problem specific heuristics are also designed. Extensive simulation experiments based on Alibaba production trace and some random synthetic parameters are conducted. Results show that Pond can reduce the average communication cost for each task significantly in a quite low deployment latency compared with some baselines.},
  archive      = {J_TPDS},
  author       = {Zhang Miao and Peng Yong and Zhu Jiancheng and Yin Quanjun},
  doi          = {10.1109/TPDS.2022.3155713},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3442-3459},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient flow-based scheduling for geo-distributed simulation tasks in collaborative edge and cloud environments},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient function queryable and privacy preserving data
aggregation scheme in smart grid. <em>TPDS</em>, <em>33</em>(12),
3430‚Äì3441. (<a href="https://doi.org/10.1109/TPDS.2022.3153930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The collection of users‚Äô near-real-time electricity consumption data brings advantages to the operation of smart grids, while raising some security and privacy issues. Multiple privacy preserving data aggregation schemes have been proposed to address these problems. However, most schemes only focus on the aggregation of electricity consumption data without considering the data availability. In addition, although a data aggregation scheme that supports function queries on encrypted data has also been developed, its efficiency is insufficient. In this paper, we first propose an EC-ElGamal encryption algorithm with a double trapdoor decryption mechanism. Through employing the proposed algorithm and the elliptic curve Schnorr signature scheme, an efficient data aggregation scheme supporting privacy protection and function query is proposed for smart grids. This solution allows the control center and users to initiate various function queries on encrypted data. In order to lighten the calculation burden of the control center, we propose another cryptosystem named ElGamal-OU to improve the decryption efficiency, which also supports two independent decryption methods. Finally, the security analysis and performance comparison with related work show that our schemes have advantages in terms of computational, communication and storage overhead.},
  archive      = {J_TPDS},
  author       = {Yu Zhan and Liguo Zhou and Baocang Wang and Pu Duan and Benyu Zhang},
  doi          = {10.1109/TPDS.2022.3153930},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3430-3441},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient function queryable and privacy preserving data aggregation scheme in smart grid},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OSM: Off-chip shared memory for GPUs. <em>TPDS</em>,
<em>33</em>(12), 3415‚Äì3429. (<a
href="https://doi.org/10.1109/TPDS.2022.3154315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphics Processing Units (GPUs) employ a shared memory, a software-managed cache for programmers, in each streaming multiprocessor to accelerate data sharing among the threads in a thread block. Although 60\% of the shared memory space is underutilized, on average, there are some workloads that demand higher shared memory capacities. Therefore, improving shared memory utilization while satisfying the needs of shared memory intensive workloads is challenging. We make a key observation that the lifetime of each shared memory address is significantly shorter than the execution time of a thread block. In this paper, we first propose Off-Chip Shared Memory (OSM) that allocates shared memory space in the off-chip memory, and accelerates accesses to it via a small on-chip cache. Using an 8 KB cache for shared memory addresses, OSM provides almost the same performance as the baseline GPU that uses 96 KB on-chip shared memory. OSM improves GPU performance in two ways. First, it allocates higher shared memory capacities in the off-chip memory, and improves thread-level parallelism (TLP). Second, it designs a unified cache for shared memory and global address spaces, providing more caching space for global memory address space even for the workloads with high shared memory utilization. Our experimental results show an average 21\% and 18\% IPC improvement compared to the baseline and the state-of-the-art architectures.},
  archive      = {J_TPDS},
  author       = {Sina Darabi and Ehsan Yousefzadeh-Asl-Miandoab and Negar Akbarzadeh and Hajar Falahati and Pejman Lotfi-Kamran and Mohammad Sadrosadati and Hamid Sarbazi-Azad},
  doi          = {10.1109/TPDS.2022.3154315},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3415-3429},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {OSM: Off-chip shared memory for GPUs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast proactive repair in erasure-coded storage: Analysis,
design, and implementation. <em>TPDS</em>, <em>33</em>(12), 3400‚Äì3414.
(<a href="https://doi.org/10.1109/TPDS.2022.3152817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Erasure coding offers a storage-efficient redundancy mechanism for maintaining data availability guarantees in large-scale storage clusters, yet it also incurs high performance overhead in failure repair. Recent developments in accurate disk failure prediction allow soon-to-fail (STF) nodes to be repaired in advance, thereby opening new opportunities for accelerating failure repair in erasure-coded storage. To this end, we present a fast proactive repair solution called ${{\sf FastPR}}$ , which carefully couples two repair methods, namely migration (i.e., relocating the chunks of an STF node) and reconstruction (i.e., decoding the chunks of an STF node through erasure coding), so as to fully parallelize the repair operation across the storage cluster. ${{\sf FastPR}}$ solves a bipartite maximum matching problem and schedules both migration and reconstruction in a parallel fashion. We show that ${{\sf FastPR}}$ significantly reduces the repair time over the baseline repair approaches for both Reed-Solomon codes and Azure&#39;s Local Reconstruction Codes via mathematical analysis, large-scale simulation, and Amazon EC2 experiments.},
  archive      = {J_TPDS},
  author       = {Xiaolu Li and Keyun Cheng and Zhirong Shen and Patrick P. C. Lee},
  doi          = {10.1109/TPDS.2022.3152817},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3400-3414},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Fast proactive repair in erasure-coded storage: Analysis, design, and implementation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerating bayesian neural networks via algorithmic and
hardware optimizations. <em>TPDS</em>, <em>33</em>(12), 3387‚Äì3399. (<a
href="https://doi.org/10.1109/TPDS.2022.3153682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian neural networks (BayesNNs) have demonstrated their advantages in various safety-critical applications, such as autonomous driving or healthcare, due to their ability to capture and represent model uncertainty. However, standard BayesNNs require to be repeatedly run because of Monte Carlo sampling to quantify their uncertainty, which puts a burden on their real-world hardware performance. To address this performance issue, this article systematically exploits the extensive structured sparsity and redundant computation in BayesNNs. Different from the unstructured or structured sparsity in standard convolutional NNs, the structured sparsity of BayesNNs is introduced by Monte Carlo Dropout and its associated sampling required during uncertainty estimation and prediction, which can be exploited through both algorithmic and hardware optimizations. We first classify the observed sparsity patterns into three categories: channel sparsity, layer sparsity and sample sparsity. On the algorithmic side, a framework is proposed to automatically explore these three sparsity categories without sacrificing algorithmic performance. We demonstrated that structured sparsity can be exploited to accelerate CPU designs by up to 49 times, and GPU designs by up to 40 times. On the hardware side, a novel hardware architecture is proposed to accelerate BayesNNs, which achieves a high hardware performance using the runtime adaptable hardware engines and the intelligent skipping support. Upon implementing the proposed hardware design on an FPGA, our experiments demonstrated that the algorithm-optimized BayesNNs can achieve up to 56 times speedup when compared with unoptimized Bayesian nets. Comparing with the optimized GPU implementation, our FPGA design achieved up to 7.6 times speedup and up to 39.3 times higher energy efficiency.},
  archive      = {J_TPDS},
  author       = {Hongxiang Fan and Martin Ferianc and Zhiqiang Que and Xinyu Niu and Miguel Rodrigues and Wayne Luk},
  doi          = {10.1109/TPDS.2022.3153682},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3387-3399},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating bayesian neural networks via algorithmic and hardware optimizations},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FenceKV: Enabling efficient range query for key-value
separation. <em>TPDS</em>, <em>33</em>(12), 3375‚Äì3386. (<a
href="https://doi.org/10.1109/TPDS.2022.3149003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LSM-tree is widely used in key-value stores for big data storage, but it suffers from write amplification brought by frequent compaction operations. An effective solution for this problem is key-value separation, which decouples values from the LSM-tree and stores them in a separate value log. However, existing key-value separation schemes achieve poor range query performance, especially for small key-value pairs, because they focus on mitigating write amplification but neglect access characteristics of the SSD. In this article, we propose FenceKV, which aims to achieve better range query performance while maintaining reasonable update performance for update-intensive workloads. FenceKV employs a new partition method to map values to the storage space based on the key-range to achieve efficient update and range query. Moreover, it adopts a key-range garbage collection policy to mitigate the garbage collection overhead and maintain sequential access for range queries. We compare FenceKV with modern key-value stores with various workloads, and results show that FenceKV can improve the range query performance significantly, while maintaining reasonable update performance compared to the existing designs of key-value separation.},
  archive      = {J_TPDS},
  author       = {Chenlei Tang and Jiguang Wan and Changsheng Xie},
  doi          = {10.1109/TPDS.2022.3149003},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3375-3386},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FenceKV: Enabling efficient range query for key-value separation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A native tensor‚Äìvector multiplication algorithm for high
performance computing. <em>TPDS</em>, <em>33</em>(12), 3363‚Äì3374. (<a
href="https://doi.org/10.1109/TPDS.2022.3153113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor computations are important mathematical operations for applications that rely on multidimensional data. The tensor‚Äìvector multiplication (TVM) is the most memory-bound tensor contraction in this class of operations. This article proposes an open-source TVM algorithm which is much simpler and efficient than previous approaches, making it suitable for integration in the most popular BLAS libraries available today. Our algorithm has been written from scratch and features unit-stride memory accesses, cache awareness, mode obliviousness, full vectorization and multi-threading as well as NUMA awareness for non-hierarchically stored dense tensors. Numerical experiments are carried out on tensors up to order 10 and various compilers and hardware architectures equipped with traditional DDR and high bandwidth memory (HBM). For large tensors the average performance of the TVM ranges between 62\% and 76\% of the theoretical bandwidth for NUMA systems with DDR memory and remains independent of the contraction mode. On NUMA systems with HBM the TVM exhibits some mode dependency but manages to reach performance figures close to peak values. Finally, the higher-order power method is benchmarked with the proposed TVM kernel and delivers on average between 58\% and 69\% of the theoretical bandwidth for large tensors.},
  archive      = {J_TPDS},
  author       = {Pedro J. Martinez-Ferrer and A. N. Yzelman and Vicen√ß Beltran},
  doi          = {10.1109/TPDS.2022.3153113},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3363-3374},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A native Tensor‚ÄìVector multiplication algorithm for high performance computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coordinating fast concurrency adapting with autoscaling for
SLO-oriented web applications. <em>TPDS</em>, <em>33</em>(12),
3349‚Äì3362. (<a href="https://doi.org/10.1109/TPDS.2022.3151512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud providers tend to support dynamic computing resources reallocation (e.g., Autoscaling) to handle the bursty workload for web applications (e.g., e-commerce) in the cloud environment. Nevertheless, we demonstrate that directly scaling a bottleneck server without quickly adjusting its soft resources (e.g., server threads and database connections) can cause significant response time fluctuations of the target web application. Since soft resources determine the request processing concurrency of each server in the system, simply scaling out/in the bottleneck service can unintentionally change the concurrency level of related services, inducing either under- or over-utilization of the critical hardware resource. In this paper, we propose the Scatter-Concurrency-Throughput (SCT) model, which can rapidly identify the near-optimal soft resource allocation of each server in the system using the measurement of each server‚Äôs real-time throughput and concurrency. Furthermore, we implement a Concurrency-aware autoScaling (ConScale) framework that integrates the SCT model to quickly reallocate the soft resources of the key servers in the system to best utilize the new hardware resource capacity after the system scaling. Based on extensive experimental comparisons with two widely used hardware-only scaling mechanisms for web applications: EC2-AutoScaling (VM-based autoscaler) and Kubernetes HPA (container-based autoscaler), we show that ConScale can successfully mitigate the response time fluctuations over the system scaling phase in both VM-based and container-based environments.},
  archive      = {J_TPDS},
  author       = {Jianshu Liu and Shungeng Zhang and Qingyang Wang and Jinpeng Wei},
  doi          = {10.1109/TPDS.2022.3151512},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3349-3362},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Coordinating fast concurrency adapting with autoscaling for SLO-oriented web applications},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bandwidth-aware scheduling repair techniques in
erasure-coded clusters: Design and analysis. <em>TPDS</em>,
<em>33</em>(12), 3333‚Äì3348. (<a
href="https://doi.org/10.1109/TPDS.2022.3153061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Erasure codes offer a storage-efficient redundancy mechanism for maintaining data availability guarantees in storage clusters, yet also incur high network traffic consumption and recovery time in failure repair. Extensive research has been carried out to reduce the recovery time. However, previous works either target specific erasure code constructions which are not commonly used in today‚Äôs distributed storage clusters or neglect the heterogeneous bandwidth property in real network environments. Since erasure-coded clusters are typically composed of multi-node with heterogeneous bandwidth and accessed in parallel, the whole recovery time is mainly restricted by the low-bandwidth links. In this article, we propose SMFRepair, a single-node multi-level forwarding repair technique that is designed to improve the performance in heterogeneous networks based on Reed-Solomon codes for general fault tolerance. SMFRepair carefully selects the helper nodes and uses idle nodes to bypass low-bandwidth links. Idle nodes have sufficient and unused network bandwidth. It also pipelines the repair links that are optimized by idle nodes. Furthermore, a multi-node scheduling repair technique, called MSRepair, is proposed. MSRepair carefully schedules the multi-node repair link to saturate the most unoccupied bandwidth and transfers data from as large-bandwidth links as possible, with the primary objective of minimizing the recovery time. Large-scale simulation and Amazon EC2 real experiments show that compared to state-of-the-art repair techniques, SMFRepair can accelerate the single-node recovery by up to 47.69\%, and MSRepair can reduce the multi-node recovery time by 33.78\% $\sim$ 67.53\%.},
  archive      = {J_TPDS},
  author       = {Hai Zhou and Dan Feng and Yuchong Hu},
  doi          = {10.1109/TPDS.2022.3153061},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3333-3348},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Bandwidth-aware scheduling repair techniques in erasure-coded clusters: Design and analysis},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GraphOpt: Constrained-optimization-based parallelization of
irregular graphs. <em>TPDS</em>, <em>33</em>(12), 3321‚Äì3332. (<a
href="https://doi.org/10.1109/TPDS.2022.3151194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse, irregular graphs show up in various applications like linear algebra, machine learning, engineering simulations, robotic control, etc. These graphs have a high degree of parallelism, but their execution on parallel threads of modern platforms remains challenging due to the irregular data dependencies. The execution performance can be improved by efficiently partitioning the graphs such that the communication and thread synchronization overheads are minimized without hurting the utilization of the threads. To achieve this, this article proposes GraphOpt , a tool that models the graph parallelization as a constrained optimization problem and uses the open Google OR-Tools solver to find good partitions. Several scalability techniques are developed to handle large real-world graphs with millions of nodes and edges. Extensive experiments are performed on the graphs of sparse matrix triangular solves (linear algebra) and sum-product networks (machine learning), respectively, showing a mean speedup of 2.0√ó and 1.8√ó over previous state-of-the-art libraries, demonstrating the effectiveness of the constrained-optimization-based graph parallelization.},
  archive      = {J_TPDS},
  author       = {Nimish Shah and Wannes Meert and Marian Verhelst},
  doi          = {10.1109/TPDS.2022.3151194},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3321-3332},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GraphOpt: Constrained-optimization-based parallelization of irregular graphs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bayesian approach to distributed anomaly detection in edge
AI networks. <em>TPDS</em>, <em>33</em>(12), 3306‚Äì3320. (<a
href="https://doi.org/10.1109/TPDS.2022.3151853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of anomaly detection is to obtain an accurate understanding of expected behaviour which is intensified when the data are distributed heterogeneously. Transmitting raw data to a central site incurs high communication overhead and raises privacy issues. The concept of Edge AI allows computation to be performed at the edge site allowing for quick decision making in mission critical scenarios such as self-driving cars. A model is learnt locally and its parameters are transmitted and aggregated. However, existing methods of aggregation do not account for variance and heterogeneous distribution of data. They also do not consider edge constraints such as limited computational, memory and communication capabilities of edge devices. In this work, a fully Bayesian approach is employed by means of a Bayesian Random Vector Functional Link AutoEncoder being incorporated with Expectation Propagation for distributed training. Our anomaly detection system operates without any transmission of raw data, is robust under inhomogeneous network densities and under uneven and biased data distributions. It allows for asynchronous updates to converge in a few iterations and is a relatively simple neural network addressing edge constraints without compromising on performance as compared to existing more complex models.},
  archive      = {J_TPDS},
  author       = {Murugaraj Odiathevar and Winston K.G. Seah and Marcus Frean},
  doi          = {10.1109/TPDS.2022.3151853},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3306-3320},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A bayesian approach to distributed anomaly detection in edge AI networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards efficient and stable k-asynchronous federated
learning with unbounded stale gradients on non-IID data. <em>TPDS</em>,
<em>33</em>(12), 3291‚Äì3305. (<a
href="https://doi.org/10.1109/TPDS.2022.3150579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an emerging privacy-preserving paradigm that enables multiple participants collaboratively to train a global model without uploading raw data. Considering heterogeneous computing and communication capabilities of different participants, asynchronous FL can avoid the stragglers effect in synchronous FL and adapts to scenarios with vast participants. Both staleness and non-IID data in asynchronous FL would reduce the model utility. However, there exists an inherent contradiction between the solutions to the two problems. That is, mitigating the staleness requires to select less but consistent gradients while coping with non-IID data demands more comprehensive gradients. To address the dilemma, this paper proposes a two-stage weighted $K$ asynchronous FL with adaptive learning rate (WKAFL). By selecting consistent gradients and adjusting learning rate adaptively, WKAFL utilizes stale gradients and mitigates the impact of non-IID data, which can achieve multifaceted enhancement in training speed, prediction accuracy and training stability. We also present the convergence analysis for WKAFL under the assumption of unbounded staleness to understand the impact of staleness and non-IID data. Experiments implemented on both benchmark and synthetic FL datasets show that WKAFL has better overall performance compared to existing algorithms.},
  archive      = {J_TPDS},
  author       = {Zihao Zhou and Yanan Li and Xuebin Ren and Shusen Yang},
  doi          = {10.1109/TPDS.2022.3150579},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3291-3305},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards efficient and stable K-asynchronous federated learning with unbounded stale gradients on non-IID data},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DHash: Dynamic hash tables with non-blocking regular
operations. <em>TPDS</em>, <em>33</em>(12), 3274‚Äì3290. (<a
href="https://doi.org/10.1109/TPDS.2022.3151499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Once started, existing hash tables cannot change their pre-defined hash functions, even if the incoming data cannot be evenly distributed to the hash table buckets. In this paper, we present DHash , a type of hash table for shared memory systems, that can change its hash function and rebuild the hash table on the fly, without noticeably degrading its service. The major technical novelty of DHash stems from an efficient distributing mechanism that can atomically distribute every node when rebuilding, without locking the corresponding hash table buckets. This not only enables non-blocking lookup, insert, and delete operations, but more importantly, makes DHash independent of the implementation of hash table buckets, such that DHash allows programmers to select the set algorithms that meet their requirements best from a variety of existing lock-free and wait-free set algorithms. Evaluations show that DHash can efficiently change its hash function on the fly. Moreover, when rebuilding, DHash consistently outperforms the state-of-the-art hash tables in terms of throughput and response time of concurrent operations, at different concurrency levels, and with different operation mixes and average load factors.},
  archive      = {J_TPDS},
  author       = {Junchang Wang and Dunwei Liu and Xiong Fu and Fu Xiao and Chen Tian},
  doi          = {10.1109/TPDS.2022.3151499},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3274-3290},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DHash: Dynamic hash tables with non-blocking regular operations},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decentralized application placement in fog computing.
<em>TPDS</em>, <em>33</em>(12), 3262‚Äì3273. (<a
href="https://doi.org/10.1109/TPDS.2022.3148985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, cloud computing concepts have been extended towards the network edge, leading to paradigms like fog and edge computing. As a result, applications can be placed on a variety of resources, including fog nodes and cloud data centers. Application placement has significant impact on important metrics like latency. Finding an optimal application placement is computationally challenging, particularly because of the potentially huge number of infrastructure nodes and application components. To overcome the limited scalability of application placement algorithms, optimization can be decentralized, i.e., performed separately for different parts of the infrastructure. The infrastructure can be split into fog colonies, where a fog colony consists of the computational resources in a given geographical region. Application placement can then be performed for the individual fog colonies, thus mitigating the scalability problem. However, independent optimization of application placement in different fog colonies may lead to missed synergies and thus to sub-optimal overall results. Hence, some kind of coordination between fog colonies may be beneficial. In this article, we analyze the effects of decentralization and coordination on the optimization results. In particular, we compare empirically four different approaches: (i) centralized decision-making, where decisions are made in one go for the entire infrastructure, (ii) independent fog colonies, where optimization is carried out in each fog colony independently from each other, (iii) fog colonies with communication, where excess application components in one fog colony can be sent to a neighboring fog colony, and (iv) fog colonies with overlaps, where shared resources may be dynamically distributed between neighboring fog colonies. Our experiments show that, for large problem instances, decentralization combined with coordination leads to the best results.},
  archive      = {J_TPDS},
  author       = {Zolt√°n √Åd√°m Mann},
  doi          = {10.1109/TPDS.2022.3148985},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3262-3273},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Decentralized application placement in fog computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NITI: Training integer neural networks using integer-only
arithmetic. <em>TPDS</em>, <em>33</em>(11), 3249‚Äì3261. (<a
href="https://doi.org/10.1109/TPDS.2022.3149787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low bitwidth integer arithmetic has been widely adopted in hardware implementations of deep neural network inference applications. However, despite the promised energy-efficiency improvements demanding edge applications, the use of low bitwidth integer arithmetic for neural network training remains limited. Unlike inference, training demands high dynamic range and numerical accuracy for high quality results, making the use of low-bitwidth integer arithmetic particularly challenging. To address this challenge, we present a novel neural network training framework called NITI that exclusively utilizes low bitwidth integer arithmetic. NITI stores all parameters and accumulates intermediate values as 8-bit integers while using no more than 5 bits for gradients. To provide the necessary dynamic range during the training process, a per-layer block scaling exponentiation scheme is utilized. By deeply integrating with the rounding procedures and integer entropy loss calculation, the proposed scaling scheme incurs only minimal overhead in terms of storage and additional computation. Furthermore, a hardware-efficient pseudo-stochastic rounding scheme that eliminates the need for external random number generation is proposed to facilitate conversion from wider intermediate arithmetic results to lower precision for storage. Since NITI operates only with standard 8-bit integer arithmetic and storage, it is possible to accelerate it using existing low bitwidth operators originally developed for inference in commodity accelerators. To demonstrate this, an open-source software implementation of end-to-end training, using native 8-bit integer operations in modern GPUs is presented. In addition, experiments have been conducted on an FPGA-based training accelerator to evaluate the hardware advantage of NITI. When compared with an equivalent training setup implemented with floating point storage and arithmetic, NITI has no accuracy degradation on the MNIST and CIFAR10 datasets. On ImageNet, NITI achieves similar accuracy as state-of-the-art integer training frameworks without relying on full-precision floating-point first and last layers.},
  archive      = {J_TPDS},
  author       = {Maolin Wang and Seyedramin Rasoulinezhad and Philip H. W. Leong and Hayden K.-H. So},
  doi          = {10.1109/TPDS.2022.3149787},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3249-3261},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {NITI: Training integer neural networks using integer-only arithmetic},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heterogeneous multi-agent system for brain-computer
interaction in routing and forwarding with memristive neuron networks.
<em>TPDS</em>, <em>33</em>(11), 3233‚Äì3248. (<a
href="https://doi.org/10.1109/TPDS.2021.3137837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, we aimed to design a novel brain-computer interaction (BCI) approach in routing and forwarding mechanisms to realize a networking multi-modal and multi-brain linked control. In recent years, the field programmable gate array (FPGA) has become a popular choice to construct a heterogeneous network for routing and forwarding processes due to its ability to complete a high performance computation in single node. Conventional BCI mode focus on capturing the dynamic EEG signals from multiple electrode channels and this concept neglects the existing complexity and diversity of connecting information between two brain regions. To this moment, we build a heterogeneous multi-agent system (HMAS) architecture to simulate the memristive neuron networks (MNN) with a set of logic units for multi-agent in the Internet. Specifically, we first feed the non-linear features into an intelligent router to adjust their routing and forwarding processes. Subsequently, at one-time step, the output of all testing nodes is written into a heterogeneous logic space, which mainly consists of the sub-agent, forward port, and memory memristive neuron. In the FPGA, each programmable cell selectively integrates and stores a huge number of motion information between multiple interacting channels. We can achieve the networking cooperative functions of forward and route through FPGA large-scale computation. According to the extensive experimental results on several data, we validate the effectiveness of the proposed HMAS-BCI architecture to compare the optimization method of the multiple simulations.},
  archive      = {J_TPDS},
  author       = {Yuxuan Zhou and Wanzhong Chen and Linlin Li and Linlin Gong and Tao Zhang},
  doi          = {10.1109/TPDS.2021.3137837},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3233-3248},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Heterogeneous multi-agent system for brain-computer interaction in routing and forwarding with memristive neuron networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NeoFlow: A flexible framework for enabling efficient
compilation for high performance DNN training. <em>TPDS</em>,
<em>33</em>(11), 3220‚Äì3232. (<a
href="https://doi.org/10.1109/TPDS.2021.3138862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are increasingly deployed in various image recognition and natural language processing applications. The continuous demand for accuracy and high performance has led to innovations in DNN design and a proliferation of new operators. However, existing DNN training frameworks such as PyTorch and TensorFlow only support a limited range of operators and rely on hand-optimized libraries to provide efficient implementations for these operators. To evaluate novel neural networks with new operators, the programmers have to either replace the holistic new operators with existing operators or provide low-level implementations manually. Therefore, a critical requirement for DNN training frameworks is to provide high-performance implementations for the neural networks containing new operators automatically in the absence of efficient library support. In this article, we introduce NeoFlow, which is a flexible framework for enabling efficient compilation for high-performance DNN training. NeoFlow allows the programmers to directly write customized expressions as new operators to be mapped to graph representation and low-level implementations automatically, providing both high programming productivity and high performance. First, NeoFlow provides expression-based automatic differentiation to support customized model definitions with new operators. Then, NeoFlow proposes an efficient compilation system that partitions the neural network graph into subgraphs, explores optimized schedules, and generates high-performance libraries for subgraphs automatically. Finally, NeoFlow develops an efficient runtime system to combine the compilation and training as a whole by overlapping their execution. In the experiments, we examine the numerical accuracy and performance of NeoFlow. The results show that NeoFlow can achieve similar or even better performance at the operator and whole graph level for DNNs compared to deep learning frameworks. Especially, for novel networks training, the geometric mean speedups of NeoFlow to PyTorch, TensorFlow, and CuDNN are 3.16X, 2.43X, and 1.92X, respectively.},
  archive      = {J_TPDS},
  author       = {Size Zheng and Renze Chen and Yicheng Jin and Anjiang Wei and Bingyang Wu and Xiuhong Li and Shengen Yan and Yun Liang},
  doi          = {10.1109/TPDS.2021.3138862},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3220-3232},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {NeoFlow: A flexible framework for enabling efficient compilation for high performance DNN training},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). E3NE: An end-to-end framework for accelerating spiking
neural networks with emerging neural encoding on FPGAs. <em>TPDS</em>,
<em>33</em>(11), 3207‚Äì3219. (<a
href="https://doi.org/10.1109/TPDS.2021.3128945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compiler frameworks are crucial for the widespread use of FPGA-based deep learning accelerators. They allow researchers and developers, who are not familiar with hardware engineering, to harness the performance attained by domain-specific logic. There exists a variety of frameworks for conventional artificial neural networks. However, not much research effort has been put into the creation of frameworks optimized for spiking neural networks (SNNs). This new generation of neural networks becomes increasingly interesting for the deployment of AI on edge devices, which have tight power and resource constraints. Our end-to-end framework E3NE automates the generation of efficient SNN inference logic for FPGAs. Based on a PyTorch model and user parameters, it applies various optimizations and assesses trade-offs inherent to spike-based accelerators. Multiple levels of parallelism and the use of an emerging neural encoding scheme result in an efficiency superior to previous SNN hardware implementations. For a similar model, E3NE uses less than 50\% of hardware resources and 20\% less power, while reducing the latency by an order of magnitude. Furthermore, scalability and generality allowed the deployment of the large-scale SNN models AlexNet and VGG.},
  archive      = {J_TPDS},
  author       = {Daniel Gerlinghoff and Zhehui Wang and Xiaozhe Gu and Rick Siow Mong Goh and Tao Luo},
  doi          = {10.1109/TPDS.2021.3128945},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3207-3219},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {E3NE: An end-to-end framework for accelerating spiking neural networks with emerging neural encoding on FPGAs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DLS: A fast and flexible neural network training system with
fine-grained heterogeneous device orchestration. <em>TPDS</em>,
<em>33</em>(11), 3194‚Äì3206. (<a
href="https://doi.org/10.1109/TPDS.2022.3144453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network accelerators (e.g., TPUs) have become mainstream devices in computing systems. Unfortunately, the existing accelerator-based systems for neural networks fail to fully leverage the acceleration opportunities due to the limited flexibility . Specifically, the majority of the accelerators focus on only the compute-intensive operations of neural networks (e.g., convolution and fully-connected layers). However, we identify that sub-optimal handling of auxiliary operations such as embedding and compression can incur non-trivial loss in terms of accuracy, training speed, and adaptability to new domains. The problem persists considering that recent advancements in neural networks often come from auxiliary operations. To effectively handle rapidly evolving auxiliary operations and maximize acceleration opportunities, we propose DLS, a holistic neural network acceleration system using heterogeneous computing devices. The key idea is to distribute compute-intensive operations on highly specialized ASICs for maximum performance, and auxiliary operations on flexible devices (e.g., FPGA, GPU) for better adaptability. We emphasize that a na√Øve integration of different devices fails to deliver high performance due to high communication overheads. To address this communication inefficiency, we propose an efficient FPGA-based device orchestration utilizing direct device-to-device communication and fine-grained operation scheduling. In this way, our system alleviates the communication overhead between heterogeneous devices by removing expensive kernel stack traversal and leveraging computation units and communication links in parallel. The evaluation using popular neural networks with emerging auxiliary operations shows that our system achieves both flexibility and high performance for various cases from single-accelerator training to distributed training (2.6‚Äì8.9√ó speedup).},
  archive      = {J_TPDS},
  author       = {Pyeongsu Park and Jaewon Lee and Heetaek Jeong and Jangwoo Kim},
  doi          = {10.1109/TPDS.2022.3144453},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3194-3206},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DLS: A fast and flexible neural network training system with fine-grained heterogeneous device orchestration},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Elastic significant bit quantization and acceleration for
deep neural networks. <em>TPDS</em>, <em>33</em>(11), 3178‚Äì3193. (<a
href="https://doi.org/10.1109/TPDS.2021.3129615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantization has been proven to be a vital method for improving the inference efficiency of deep neural networks (DNNs). However, it is still challenging to strike a good balance between accuracy and efficiency while quantizing DNN weights or activation values from high-precision formats to their quantized counterparts. We propose a new method called elastic significant bit quantization (ESB) that controls the number of significant bits of quantized values to obtain better inference accuracy with fewer resources. We design a unified mathematical formula to constrain the quantized values of the ESB with a flexible number of significant bits. We also introduce a distribution difference aligner (DDA) to quantitatively align the distributions between the full-precision weight or activation values and quantized values. Consequently, ESB is suitable for various bell-shaped distributions of weights and activation of DNNs, thus maintaining a high inference accuracy. Benefitting from fewer significant bits of quantized values, ESB can reduce the multiplication complexity. We implement ESB as an accelerator and quantitatively evaluate its efficiency on FPGAs. Extensive experimental results illustrate that ESB quantization consistently outperforms state-of-the-art methods and achieves average accuracy improvements of 4.78\% , 1.92\% , and 3.56\% over AlexNet, ResNet18, and MobileNetV2, respectively. Furthermore, ESB as an accelerator can achieve 10.95 GOPS peak performance of 1k LUTs without DSPs on the Xilinx ZCU102 FPGA platform. Compared with CPU, GPU, and state-of-the-art accelerators on FPGAs, the ESB accelerator can improve the energy efficiency by up to 65√ó , 11√ó , and 26√ó , respectively.},
  archive      = {J_TPDS},
  author       = {Cheng Gong and Ye Lu and Kunpeng Xie and Zongming Jin and Tao Li and Yanzhi Wang},
  doi          = {10.1109/TPDS.2021.3129615},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3178-3193},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Elastic significant bit quantization and acceleration for deep neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixing activations and labels in distributed training for
split learning. <em>TPDS</em>, <em>33</em>(11), 3165‚Äì3177. (<a
href="https://doi.org/10.1109/TPDS.2021.3139191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Split Learning (SL) is a distributed machine learning setting that allows several nodes to train neural networks based on model parallelism. Since SL avoids sharing raw data among training nodes, it can protect data privacy by nature. However, recent studies show that, raw data may be reconstructed from activations in training, which may cause data privacy leakage. Besides raw data, label sharing in SL may also cause privacy problems. In order to address these issues, we propose a novel mechanism called multiple activations and labels mix (MALM). By taking advantage of the diversity of sample categories, MALM generates mixed activations that preserve a low distance correlation with the raw data so as to reduce the risk of reconstruction attacks. To protect label information, MALM creates obfuscated labels associated with the raw data so as to prevent adversaries from inferring ground-truth labels. Since clients with few sample categories may not effectively generate mixed activations and obfuscated labels, we propose a bipartite graph based assistant client match technique for MALM, which lets clients with a large number of categories provide mixed activations and obfuscated labels for clients with few categories. Those clients with few categories can mix the obtained mixed activations and obfuscated labels with their own activations and labels. Experimental results show that, compared with baselines, MALM can reduce the risk of raw data and label information leakage with lower cost, while achieving comparable even better model performance.},
  archive      = {J_TPDS},
  author       = {Danyang Xiao and Chengang Yang and Weigang Wu},
  doi          = {10.1109/TPDS.2021.3139191},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3165-3177},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Mixing activations and labels in distributed training for split learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FRuDA: Framework for distributed adversarial domain
adaptation. <em>TPDS</em>, <em>33</em>(11), 3153‚Äì3164. (<a
href="https://doi.org/10.1109/TPDS.2021.3136673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breakthroughs in unsupervised domain adaptation (uDA) can help in adapting models from a label-rich source domain to unlabeled target domains. Despite these advancements, there is a lack of research on how uDA algorithms, particularly those based on adversarial learning, can work in distributed settings. In real-world applications, target domains are often distributed across thousands of devices, and existing adversarial uDA algorithms ‚Äì which are centralized in nature ‚Äì cannot be applied in these settings. To solve this important problem, we introduce FRuDA: an end-to-end framework for distributed adversarial uDA. Through a careful analysis of the uDA literature, we identify the design goals for a distributed uDA system and propose two novel algorithms to increase adaptation accuracy and training efficiency of adversarial uDA in distributed settings. Our evaluation of FRuDA with five image and speech datasets show that it can boost target domain accuracy by up to 50\% and improve the training efficiency of adversarial uDA by at least $11\times$ .},
  archive      = {J_TPDS},
  author       = {Shaoduo Gan and Akhil Mathur and Anton Isopoussu and Fahim Kawsar and Nadia Berthouze and Nicholas D. Lane},
  doi          = {10.1109/TPDS.2021.3136673},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3153-3164},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FRuDA: Framework for distributed adversarial domain adaptation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BLB-gcForest: A high-performance distributed deep forest
with adaptive sub-forest splitting. <em>TPDS</em>, <em>33</em>(11),
3141‚Äì3152. (<a href="https://doi.org/10.1109/TPDS.2021.3133544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an emulous alternative to deep neural networks, Deep Forest emerges with features like low complexity, fewer hyper-parameters, and good robustness, which are predominantly desired in distributed computing applications and ecosystems. Recently, an efficient distributed Deep Forest system, named ForestLayer, was proposed, designing a fine-grained sub-Forest-based task-parallel algorithm to improve the parallel computing efficiency of Deep Forest. However, the sub-Forest splitting of ForestLayer is static and one-off without adaptability to the computing environment, nevertheless, the size of splitting granularity has a significant impact on the system performance. To further improve the computing efficiency and scalability of the distributed Deep Forest, in this paper, we propose a novel distributed Deep Forest algorithm, named BLB-gcForest (Bag of Little Bootstraps-gcForest), which augments the gcForest (multi-Grained Cascade Forest) approach for constructing Deep Forest. BLB-gcForest carries out parallel computation for each tree in sub-Forests at a finer parallel granularity and integrates with the Bag of Little Bootstraps (BLB) mechanism to reduce massive transmitted feature instances for Cascade Forest Layers, utterly improving both computation efficiency and communication efficiency. Moreover, to solve the problem of the forest splitting granularity, we further design an adaptive sub-Forest splitting algorithm to ensure the maximum resource utilization for parallel computation of each sub-Forest. Experimental results on four well-known large-scale datasets, namely YEAST, LETTER, MNIST, CIFAR10, show that the training efficiency of BLB-gcForest achieves up to 20.3x and 1.64x speedups compared with the state-of-the-art gcForest and ForestLayer, respectively while guaranteeing higher accuracy and better robustness},
  archive      = {J_TPDS},
  author       = {Zexi Chen and Ting Wang and Haibin Cai and Subrota Kumar Mondal and Jyoti Prakash Sahoo},
  doi          = {10.1109/TPDS.2021.3133544},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3141-3152},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {BLB-gcForest: A high-performance distributed deep forest with adaptive sub-forest splitting},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An accurate and efficient large-scale regression method
through best friend clustering. <em>TPDS</em>, <em>33</em>(11),
3129‚Äì3140. (<a href="https://doi.org/10.1109/TPDS.2021.3134336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the data size in Machine Learning fields grows exponentially, it is inevitable to accelerate the computation by utilizing the ever-growing large number of available cores provided by high-performance computing hardware. However, existing parallel methods for clustering or regression often suffer from problems of low accuracy, slow convergence, and complex hyperparameter-tuning. Furthermore, the parallel efficiency is usually difficult to improve while striking a balance between preserving model properties and partitioning computing workloads on distributed systems. In this article, we propose a novel and simple data structure capturing the most important information among data samples. It has several advantageous properties supporting a hierarchical clustering strategy that contains well-defined metrics for determining optimal hierarchy, balanced partition for maintaining the clustering property, and efficient parallelization for accelerating computation phases. Then we combine the clustering with regression techniques as a parallel library and utilize a hybrid structure of data and model parallelism to make predictions. Experiments illustrate that our library obtains remarkable performance on convergence, accuracy, and scalability.},
  archive      = {J_TPDS},
  author       = {Kun Li and Liang Yuan and Yunquan Zhang and Gongwei Chen},
  doi          = {10.1109/TPDS.2021.3134336},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3129-3140},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An accurate and efficient large-scale regression method through best friend clustering},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Auto-GNAS: A parallel graph neural architecture search
framework. <em>TPDS</em>, <em>33</em>(11), 3117‚Äì3128. (<a
href="https://doi.org/10.1109/TPDS.2022.3151895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have received much attention as GNNs have recently been successfully applied on non-euclidean data. However, artificially designed graph neural networks often fail to get satisfactory model performance for a given graph data. Graph neural architecture search effectively constructs the GNNs that achieve the expected model performance with the rise of automatic machine learning. The challenge is efficiently and automatically getting the optimal GNN architecture in a vast search space. Existing search methods serially evaluate the GNN architectures, severely limiting system efficiency. To solve these problems, we develop an Auto matic G raph N eural A rchitecture S earch framework (Auto-GNAS) with parallel estimation to implement an automatic graph neural search process that requires almost no manual intervention. In Auto-GNAS, we design the search algorithm with multiple genetic searchers. Each searcher can simultaneously use evaluation feedback information, information entropy, and search results from other searchers based on sharing mechanism to improve the search efficiency. As far as we know, this is the first work using parallel computing to improve the system efficiency of graph neural architecture search. According to the experiment on the real datasets, Auto-GNAS obtain competitive model performance and better search efficiency than other search algorithms. Since the parallel estimation ability of Auto-GNAS is independent of search algorithms, we expand different search algorithms based on Auto-GNAS for scalability experiments. The results show that Auto-GNAS with varying search algorithms can achieve nearly linear acceleration with the increase of computing resources.},
  archive      = {J_TPDS},
  author       = {Jiamin Chen and Jianliang Gao and Yibo Chen and Babatounde Moctard Oloulade and Tengfei Lyu and Zhao Li},
  doi          = {10.1109/TPDS.2022.3151895},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3117-3128},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Auto-GNAS: A parallel graph neural architecture search framework},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lossy compression of communication traces using recurrent
neural networks. <em>TPDS</em>, <em>33</em>(11), 3106‚Äì3116. (<a
href="https://doi.org/10.1109/TPDS.2021.3132417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high performance computing (HPC) systems, collecting and replaying communication traces are fundamental approaches to analyze performance. With increasingly large-scale HPC systems and applications, tracing tools can produce huge trace data that is costly and challenging to store and analyze. Due to the inherent repetition of behaviors of HPC applications, domain-aware data compression methods can effectively reduce the storage cost of trace data. This study proposes LCR (Lossy Compression and Replay), a framework that aggressively compresses and replays MPI communication traces. Differing from existing trace compression methods, which explicitly identify loop and synchronization structures of communication events, LCR models traces as time series and compactly represents them by lightweight recurrent neural networks. Experimental results demonstrate that LCR can further reduce the size of irregular traces by three orders of magnitude at most, compared with existing structural methods. Meanwhile, LCR accurately reproduces performance and communication patterns of original MPI programs.},
  archive      = {J_TPDS},
  author       = {Jingwei Sun and Tao Yan and Hao Sun and Huancheng Lin and Guangzhong Sun},
  doi          = {10.1109/TPDS.2021.3132417},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3106-3116},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Lossy compression of communication traces using recurrent neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Boosting graph embedding on a single GPU. <em>TPDS</em>,
<em>33</em>(11), 3092‚Äì3105. (<a
href="https://doi.org/10.1109/TPDS.2021.3129617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs are ubiquitous, and they can model unique characteristics and complex relations of real-life systems. Although using machine learning (ML) on graphs is promising, their raw representation is not suitable for ML algorithms. Graph embedding represents each node of a graph as a $d$ -dimensional vector which is more suitable for ML tasks. However, the embedding process is expensive, and CPU-based tools do not scale to real-world graphs. In this work, we present GOSH, a GPU-based tool for embedding large-scale graphs with minimum hardware constraints. GOSH employs a novel graph coarsening algorithm to enhance the impact of updates and minimize the work for embedding. It also incorporates a decomposition schema that enables any arbitrarily large graph to be embedded with a single GPU. As a result, GOSH sets a new state-of-the-art in link prediction both in accuracy and speed, and delivers high-quality embeddings for node classification at a fraction of the time compared to the state-of-the-art. For instance, it can embed a graph with over 65 million vertices and 1.8 billion edges in less than 30 minutes on a single GPU.},
  archive      = {J_TPDS},
  author       = {Amro Alabsi Aljundi and Taha Atahan Akyildiz and Kamer Kaya},
  doi          = {10.1109/TPDS.2021.3129617},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3092-3105},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Boosting graph embedding on a single GPU},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Secure deep neural network models publishing against
membership inference attacks via training task parallelism.
<em>TPDS</em>, <em>33</em>(11), 3079‚Äì3091. (<a
href="https://doi.org/10.1109/TPDS.2021.3129612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vast data and computing resources are commonly needed to train deep neural networks, causing an unaffordable price for individual users. Motivated by the increasing demands of deep learning applications, sharing well-trained models becomes popular. The owner of a pre-trained model can share it by publishing the model directly or providing a prediction interface. Either way, individual users can benefit from deep learning without much cost, and computing resources can be saved. However, recent studies of machine learning security have identified severe threats to these model publishing approaches. This article will focus on the privacy leakage issue of publishing well-trained deep neural network models. To tackle this problem, we propose a series of secure model publishing solutions based on training task parallelism. Specifically, we show how to estimate private model parameters through parallel model training and generate new model parameters in a privacy-preserving manner to replace the original ones for publishing. Based on data parallelism and parameter generating techniques, we design another two solutions concentrating on model quality and parameter privacy, respectively. Through privacy leakage analysis and experimental attack evaluation, we conclude that deep neural network models published with our solutions can provide on-demand model quality guarantees and resist membership inference attacks.},
  archive      = {J_TPDS},
  author       = {Yunlong Mao and Wenbo Hong and Boyu Zhu and Zhifei Zhu and Yuan Zhang and Sheng Zhong},
  doi          = {10.1109/TPDS.2021.3129612},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3079-3091},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Secure deep neural network models publishing against membership inference attacks via training task parallelism},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). <span
class="math inline"><em>T</em><em>C</em>‚ÄÖ‚àí‚ÄÖ<em>S</em><em>t</em><em>r</em><em>e</em><em>a</em><em>m</em></span>TC-stream:
Large-scale graph triangle counting on a single machine using GPUs.
<em>TPDS</em>, <em>33</em>(11), 3067‚Äì3078. (<a
href="https://doi.org/10.1109/TPDS.2021.3135329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we build a $TC$ - $Stream$ , a high-performance graph processing system specific for a triangle counting algorithm on graph data with up to tens of billions of edges, which significantly exceeds the device memory capacity of Graphics Processing Units (GPUs). The triangle counting problem is a broad research topic in data mining and social network analysis in the graph processing field. As the scale of the graph data grows, a portion of the graph data must be loaded iteratively. In the existing literature, graphs with billions of edges need to be done distributively, which is cost-intensive. Also, many disk-based triangle counting systems are proposed for CPU architectures, but their tackling performances are inefficient. To solve the above problem, we propose $TC$ - $Stream$ , and it focuses on three issues: 1) For power-law graphs, because the amount of tasks of each vertex or edge is inconsistent, it is bound to cause different demands of computing and memory resources for different task types. We propose a parallel vertex approach and the reordering of vertices for graph data that can be placed in the GPU device memory to ensure the maximum workload balancing; 2) A binary-search-based set intersection method is designed to achieve the maximum parallelism in GPU; 3) For the graph data that exceeds the GPU device memory capacity, we develop a novel vertical partition algorithm to guarantee the independent computing on each partition so that the three computation processes, i.e., the computation on GPU, the data transmission between main memory of CPU and SSD, and the communication between the CPU and the GPU can be perfectly overlapped. Moreover, the $TC$ - $Stream$ optimizes edge-iterator models and benefits from multi-thread parallelism. Extensive experiments conducted on large-scale datasets showed that the $TC$ - $stream$ running on a single Tesla V100 GPU performs $2.4-6\times$ and $1.8-4.4\times$ faster than the state-of-the-art single-machine in-memory triangle counting system and GPU-based triangle counting system, respectively, and achieves $2.4\times$ faster than the state-of-the-art out-of-core distributed system PDTL running on an 8-node cluster when processing the graph data with 42.5 billion edges, which demonstrates the high performance and cost-effectiveness of the $TC$ - $Stream$ .},
  archive      = {J_TPDS},
  author       = {Jianqiang Huang and Haojie Wang and Xiang Fei and Xiaoying Wang and Wenguang Chen},
  doi          = {10.1109/TPDS.2021.3135329},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3067-3078},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {$TC-Stream$TC-stream: Large-scale graph triangle counting on a single machine using GPUs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). MIPD: An adaptive gradient sparsification framework for
distributed DNNs training. <em>TPDS</em>, <em>33</em>(11), 3053‚Äì3066.
(<a href="https://doi.org/10.1109/TPDS.2022.3154387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asynchronous training based on the parameter server architecture is widely used for scaling up the DNN training over large datasets and DNN models. Communication has been identified as the major bottleneck when deploying the DNN training over the large-scale distributed deep learning systems. Recent studies try to reduce the communication traffic through gradient sparsification and quantization approaches. We identify three limitations in previous studies. First, the fundamental guideline for gradient sparsification of their work is the magnitude of the gradient. However, the gradients‚Äô magnitude represents the current optimization direction while it cannot indicate the significance of the parameters, which potentially results in delayed updating for the significant parameters. Second, their gradient quantization methods based on the entire model often lead to error accumulation for gradients aggregation since the gradients from different layers of the DNN model follow different distributions. Third, previous quantization approaches are CPU intensive, which generates strong overhead for the server. We propose MIPD , an adaptive and layer-wise gradient sparsification framework that compresses the gradients based on model interpretability and probability distribution of gradients. MIPD compresses the gradients according to the corresponding significance of its parameters, which is defined by model interpretability. An Exponential Smoothing method is also proposed to compensate for the dropped gradients on the server to reduce the gradients error. MIPD proposes to update half of the parameters for each training step to reduce the CPU overhead of the server. It encodes the gradients based on their probability distribution, thereby minimizing the approximated errors. Extensive experimental results generated on the GPU cluster indicate that the proposed framework effectively improves the training performance of DNNs by up to 36.2\%, which ensures high accuracy as compared to state-of-art solutions. Accordingly, the CPU and network usage of the server dropped by up to 42.0\% and 32.7\% respectively.},
  archive      = {J_TPDS},
  author       = {Zhaorui Zhang and Choli Wang},
  doi          = {10.1109/TPDS.2022.3154387},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3053-3066},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {MIPD: An adaptive gradient sparsification framework for distributed DNNs training},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerating large sparse neural network inference using GPU
task graph parallelism. <em>TPDS</em>, <em>33</em>(11), 3041‚Äì3052. (<a
href="https://doi.org/10.1109/TPDS.2021.3138856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ever-increasing size of modern deep neural network (DNN) architectures has put increasing strain on the hardware needed to implement them. Sparsified DNNs can greatly reduce memory costs and increase throughput over standard DNNs, if the loss of accuracy can be adequately controlled. However, sparse DNNs present unique computational challenges. Efficient model or data parallelism algorithms are extremely hard to design and implement. The recent effort MIT/IEEE/Amazon HPEC Graph Challenge has drawn attention to high-performance inference methods for large sparse DNNs. In this article, we introduce SNIG, an efficient inference engine for large sparse DNNs. SNIG develops highly optimized inference kernels and leverages the power of CUDA Graphs to enable efficient decomposition of model and data parallelisms. Our decomposition strategy is flexible and scalable to different partitions of data volumes, model sizes, and GPU numbers. We have evaluated SNIG on the official benchmarks of HPEC Sparse DNN Challenge and demonstrated its promising performance scalable from a single GPU to multiple GPUs. Compared to the champion of the 2019 HPEC Sparse DNN Challenge, SNIG can finish all inference workloads using only a single GPU. At the largest DNN, which has more than 4 billion parameters across 1920 layers each of 65536 neurons, SNIG is up to 2.3√ó faster than a state-of-the-art baseline under a machine of 4 GPUs. SNIG receives the Champion Award in 2020 HPEC Sparse DNN Challenge.},
  archive      = {J_TPDS},
  author       = {Dian-Lun Lin and Tsung-Wei Huang},
  doi          = {10.1109/TPDS.2021.3138856},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3041-3052},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating large sparse neural network inference using GPU task graph parallelism},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable unsupervised ML: Latency hiding in distributed
sparse tensor decomposition. <em>TPDS</em>, <em>33</em>(11), 3028‚Äì3040.
(<a href="https://doi.org/10.1109/TPDS.2021.3128827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latency overhead in distributed-memory parallel CPD-ALS scales with the number of processors, limiting the scalability of computing CPD of large irregularly sparse tensors. This overhead comes in the form of sparse reduce and expand operations performed on factor-matrix rows via point-to-point messages. We propose to hide the latency overhead through embedding all of the point-to-point messages incurred by the sparse reduce and expand into dense collective operations which already exist in the CPD-ALS. The conventional parallel CPD-ALS algorithm is not amenable for embedding so we propose a computation/communication rearrangement to enable the embedding. We embed the sparse expand and reduce into a hypercube-based ALL-REDUCE operation to limit the latency overhead to $O(\log _2 K)$ for a $K$ -processor system. The embedding comes with the cost of increased bandwidth overhead due to the multi-hop routing of factor-matrix rows during the embedded- ALL-REDUCE . We propose an embedding scheme that takes advantage of the expand/reduce properties to reduce this overhead. Furthermore, we propose a novel recursive bipartitioning framework that enables simultaneous hypergraph partitioning and subhypergraph-to-subhypercube mapping to achieve subtensor-to-processor assignment with the objective of reducing the bandwidth overhead during the embedded- ALL-REDUCE . We also propose a bin-packing-based algorithm for factor-matrix row to processor assignment aiming at reducing processors‚Äô maximum send and receive volumes during the embedded- ALL-REDUCE . Experiments on up to 4096 processors show that the proposed framework scales significantly better than the state-of-the-art point-to-point method.},
  archive      = {J_TPDS},
  author       = {Nabil Abubaker and M. Ozan Karsavuran and Cevdet Aykanat},
  doi          = {10.1109/TPDS.2021.3128827},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3028-3040},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Scalable unsupervised ML: Latency hiding in distributed sparse tensor decomposition},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). G-SLIDE: A GPU-based sub-linear deep learning engine via LSH
sparsification. <em>TPDS</em>, <em>33</em>(11), 3015‚Äì3027. (<a
href="https://doi.org/10.1109/TPDS.2021.3132493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been one of the trendiest research topics. However, as data quantities rise exponentially, training large neural networks can become prohibitively expensive with billions of parameters. Fortunately, recent research has discovered that not all of the computations in traditional network training are necessary. By selectively sparsifying the majority of the neurons during training, we can still obtain acceptable accuracy. SLIDE, a C++ OpenMP-based sub-linear deep learning engine, has been developed in this situation. SLIDE uses the algorithm of locality sensitive hashing (LSH) to query neurons with high activation in sub-linear time. It achieves a remarkable speedup in training large fully-connected networks by making use of the network sparsity as well as multi-core parallelism. However, SLIDE is limited to CPUs, ignoring the popular GPU devices with greater parallel potential and computational capability. In this article, we propose G-SLIDE, a GPU-based sub-linear deep learning engine, which combines the benefits of SLIDE‚Äôs adaptive sparsification algorithms with GPUs‚Äô high performance. The main challenges in developing G-SLIDE are efficiently using LSH to sparsify networks and training the special sparse neural networks on the GPU. To address these challenges, we propose several novel solutions, such as specific data formats and appropriate workload partitioning for threads to fully utilize the GPU resources. We evaluate G-SLIDE on two extremely sparse datasets with a 2080 Ti GPU, and the results demonstrate that for the time of one training epoch, G-SLIDE can achieve more than 16.4√ó speedup over SLIDE on a 32-core/64-thread CPU. Furthermore, on the same platform, G-SLIDE can earn an average of 16.2√ó speedup over TensorFlow-GPU and 30.8√ó speedup over TensorFlow-CPU.},
  archive      = {J_TPDS},
  author       = {Zaifeng Pan and Feng Zhang and Hourun Li and Chenyang Zhang and Xiaoyong Du and Dong Deng},
  doi          = {10.1109/TPDS.2021.3132493},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3015-3027},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {G-SLIDE: A GPU-based sub-linear deep learning engine via LSH sparsification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep reinforcement learning enhanced greedy optimization for
online scheduling of batched tasks in cloud HPC systems. <em>TPDS</em>,
<em>33</em>(11), 3003‚Äì3014. (<a
href="https://doi.org/10.1109/TPDS.2021.3138459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a large cloud data center HPC system, a critical problem is how to allocate the submitted tasks to heterogeneous servers that will achieve the goal of maximizing the system‚Äôs gain defined as the value of completed tasks minus system operation costs. We consider this problem in the online setting that tasks arrive in batches and propose a novel deep reinforcement learning (DRL) enhanced greedy optimization algorithm of two-stage scheduling interacting task sequencing and task allocation. For task sequencing, we deploy a DRL module to predict the best allocation sequence for each arriving batch of tasks based on the knowledge (allocation strategies) learnt from previous batches. For task allocation, we propose a greedy strategy that allocates tasks to servers one by one online following the allocation sequence to maximize the total gain increase. We show that our greedy strategy has a performance guarantee of competitive ratio $\frac{1}{1+\kappa }$ to the optimal offline solution, which improves the existing result for the same problem, where $\kappa$ is upper bounded by the maximum cost-to-gain ratio of each task. While our DRL module enhances the greedy algorithm by providing the likely-optimal allocation sequence for each batch of arriving tasks, our greedy strategy bounds DRL‚Äôs prediction error within a proven worst-case performance guarantee for any allocation sequence. It enables a better solution quality than that obtainable from both DRL and greedy optimization alone. Extensive experiment evaluation results in both simulation and real application environments demonstrate the effectiveness and efficiency of our proposed algorithm. Compared with the state-of-the-art baselines, our algorithm increases the system gain by about 10\% to 30\%. Our algorithm provides an interesting example of combining machine learning (ML) and greedy optimization techniques to improve ML-based solutions with a worst-case performance guarantee for solving hard optimization problems.},
  archive      = {J_TPDS},
  author       = {Yuanhao Yang and Hong Shen},
  doi          = {10.1109/TPDS.2021.3138459},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {3003-3014},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Deep reinforcement learning enhanced greedy optimization for online scheduling of batched tasks in cloud HPC systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive DRL-based virtual machine consolidation in
energy-efficient cloud data center. <em>TPDS</em>, <em>33</em>(11),
2991‚Äì3002. (<a href="https://doi.org/10.1109/TPDS.2022.3147851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dramatic increasing of data and demands for computing capabilities may result in excessive use of resources in cloud data centers, which not only causes the raising of energy consumption, but also leads to the violation of Service Level Agreement (SLA). Dynamic consolidation of virtual machines (VMs) is proven to be an efficient way to tackle this issue. In this paper, we present an Adaptive Deep Reinforcement Learning (DRL)-based Virtual Machine Consolidation (ADVMC) framework for energy-efficient cloud data centers. ADVMC has two phases. In the first phase, Influence Coefficient is introduced to measure the impact of a VM on producing host overload, and a dynamic Influence Coefficient-based VM selection algorithm (ICVMS) is proposed to preferentially choose those VMs with the greatest impact for migration in order to remove the excessive workloads of the overloaded host quickly and accurately. In the second phase, a Prediction Aware DRL-based VM placement method (PADRL) is further proposed to automatically find suitable hosts for VMs to be migrated, in which a state prediction network is designed based on LSTM to provide DRL-based model more reasonable environment states so as to accelerate the convergence of DRL. Simulation experiments on the real-world workload provided by Google Cluster Trace have shown that our ADVMC approach can largely cut down system energy consumption and reduce SLA violation of users as compared to many other VM consolidation policies.},
  archive      = {J_TPDS},
  author       = {Jing Zeng and Ding Ding and Kaixuan Kang and HuaMao Xie and Qian Yin},
  doi          = {10.1109/TPDS.2022.3147851},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2991-3002},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Adaptive DRL-based virtual machine consolidation in energy-efficient cloud data center},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient parallel reinforcement learning approach to
cross-layer defense mechanism in industrial control systems.
<em>TPDS</em>, <em>33</em>(11), 2979‚Äì2990. (<a
href="https://doi.org/10.1109/TPDS.2021.3135412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ongoing digitalization enables stable control processes and smooth operations of Industrial Control Systems (ICSs). A direct consequence of the highly interconnected architecture of ICSs is the introduced cyber vulnerability and increasing cyber security threats to ICSs. Numerous researches pay attention to the security problem of ICSs. However, most current studies face two challenges. First, the interaction problem between the cyber layer and the physical layer of ICSs may result in incorrect attack response strategies. Second, ICSs are real-time systems, but existing defense decision algorithms based on game theory or reinforcement learning techniques have high computational complexity, which prevents them from making decisions quickly. In this paper, we design a new multi-attribute based method for quantifying rewards and propose a multi-attribute based Q-learning algorithm to resolve the interaction problem. In addition, to overcome the limitation of slow convergence, we develop an effective parallel Q-learning (PQL) algorithm to quickly find the optimal strategy. The experimental results show the effectiveness of the PQL algorithm. Compared with the Q-learning algorithm (QL) and the deep Q-network (DQN) algorithm, our proposed solution can reduce the average completion time by 12.5 to 37 percent.},
  archive      = {J_TPDS},
  author       = {Kai Zhong and Zhibang Yang and Guoqing Xiao and Xingpei Li and Wangdong Yang and Kenli Li},
  doi          = {10.1109/TPDS.2021.3135412},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2979-2990},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An efficient parallel reinforcement learning approach to cross-layer defense mechanism in industrial control systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Microservice deployment in edge computing based on deep q
learning. <em>TPDS</em>, <em>33</em>(11), 2968‚Äì2978. (<a
href="https://doi.org/10.1109/TPDS.2022.3150311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The microservice deployment strategy is promising in reducing the overall service response time in the microservice-oriented edge computing platform. However, existing works ignore the effect of different interaction frequencies among microservices and the decrease in service execution performance caused by the increased node loads. In this article, we first model the invocation relationships among microservices as an undirected and weighted interaction graph to characterize the communication overhead. Then, we propose a multi-objective microservice deployment problem (MMDP) in edge computing. MMDP aims to minimize the communication overhead while achieving load balance between edge nodes. Without the requirement for domain experts, we propose Reward Sharing Deep Q Learning (RSDQL), a learning-based algorithm, to solve MMDP and obtain the optimal deployment strategy. In addition, to improve the scalability of the services, we propose an Elastic Scaling algorithm (ES) based on heuristics to deal with the dynamic pressure of requests. Finally, we conduct a series of experiments in Kubernetes to evaluate the performance of our approach. Experimental results indicate that, compared with interaction-aware strategy and Kubernetes default strategy, RSDQL has shorter response times, more balanced resource loads, and makes services scale elastically according to the request pressure.},
  archive      = {J_TPDS},
  author       = {Wenkai Lv and Quan Wang and Pengfei Yang and Yunqing Ding and Bijie Yi and Zhenyi Wang and Chengmin Lin},
  doi          = {10.1109/TPDS.2022.3150311},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2968-2978},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Microservice deployment in edge computing based on deep q learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FarSpot: Optimizing monetary cost for HPC applications in
the cloud spot market. <em>TPDS</em>, <em>33</em>(11), 2955‚Äì2967. (<a
href="https://doi.org/10.1109/TPDS.2021.3134644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, we have witnessed many HPC applications developed and hosted in the cloud, which can benefit from the elastic and diversified resources on the cloud, while on the other hand confronting high costs for executing the long-running HPC applications. Although public clouds such as Amazon EC2 offer spot instances with dynamic and usually low prices compared to on-demand ones, the spot prices can vary significantly and sometimes can even be more expensive than on-demand prices of the same type. Previous work on reducing the monetary cost for HPC applications using spot instances focused on designing fault tolerance techniques or selecting appropriate instance types/bid prices to make good usage of the low spot prices. However, with the recent update of spot pricing model on Amazon EC2, these work may become either inefficient or invalid. In this article, we present FarSpot which is an optimization framework for HPC applications in the latest cloud spot market with the goal of minimizing application cost while ensuring performance constraints. FarSpot provides accurate long-term price prediction for a wide range of spot instance types using ensemble-based learning method. It further incorporates a cost-aware deadline assignment algorithm to distribute application deadline to each task according to spot price changes. With the assigned subdeadline of each task, FarSpot dynamically migrates tasks among spot instances to reduce execution cost. Evaluation results using real HPC benchmark show that 1) the prediction error of FarSpot is very low (below 3\%), 2) FarSpot reduced the monetary cost by 32\% on average compared to state-of-the-art algorithms, and 3) FarSpot satisfies the user-specified deadline constraints at all time.},
  archive      = {J_TPDS},
  author       = {Amelie Chi Zhou and Jianming Lao and Zhoubin Ke and Yi Wang and Rui Mao},
  doi          = {10.1109/TPDS.2021.3134644},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2955-2967},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FarSpot: Optimizing monetary cost for HPC applications in the cloud spot market},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic GPU energy optimization for machine learning
training workloads. <em>TPDS</em>, <em>33</em>(11), 2943‚Äì2954. (<a
href="https://doi.org/10.1109/TPDS.2021.3137867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GPUs are widely used to accelerate the training of machine learning workloads. As modern machine learning models become increasingly larger, they require a longer time to train, leading to higher GPU energy consumption. This paper presents GPOEO, an online GPU energy optimization framework for machine learning training workloads. GPOEO dynamically determines the optimal energy configuration by employing novel techniques for online measurement, multi-objective prediction modeling, and search optimization. To characterize the target workload behavior, GPOEO utilizes GPU performance counters. To reduce the performance counter profiling overhead, it uses an analytical model to detect the training iteration change and only collects performance counter data when an iteration shift is detected. GPOEO employs multi-objective models based on gradient boosting and a local search algorithm to find a trade-off between execution time and energy consumption. We evaluate the GPOEO by applying it to 71 machine learning workloads from two AI benchmark suites running on an NVIDIA RTX3080Ti GPU. Compared with the NVIDIA default scheduling strategy, GPOEO delivers a mean energy saving of 16.2\% with a modest average execution time increase of 5.1\%.},
  archive      = {J_TPDS},
  author       = {Farui Wang and Weizhe Zhang and Shichao Lai and Meng Hao and Zheng Wang},
  doi          = {10.1109/TPDS.2021.3137867},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2943-2954},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Dynamic GPU energy optimization for machine learning training workloads},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight and accurate DNN-based anomaly detection at
edge. <em>TPDS</em>, <em>33</em>(11), 2927‚Äì2942. (<a
href="https://doi.org/10.1109/TPDS.2021.3137631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have been showing significant success in various anomaly detection applications such as smart surveillance and industrial quality control. It is increasingly important to detect anomalies directly on edge devices, because of high responsiveness requirements and tight latency constraints. The accuracy of DNN-based solutions rely on large model capacity and thus long training and inference time, making them inapplicable on resource strenuous edge devices. It is hence imperative to scale DNN model sizes in correspondence to the run-time system requirements, i.e., meeting deadlines with minimal accuracy losses, which are highly dependent on the platforms and real-time system status. Existing scaling techniques either take long training time to pre-generate scaling options or disturb the unsteady training process of anomaly detection DNNs, lacking the adaptability to heterogeneous edge systems and incurring low inference accuracies. In this article, we present LightDNN to scale DNN models for anomaly detection applications at edge, featuring high detection accuracies with lightweight training and inference time. To this end, LightDNN quickly extracts and compresses blocks in a DNN, and provides large scaling space (e.g., 1 million options) by dynamically combining these compressed blocks online. At run-time, LightDNN predicts the DNN‚Äôs inference latency according to the monitored system status, and optimizes the combination of blocks to maximize its accuracy under deadline constraints. We implement and extensively evaluate LightDNN on both CPU and GPU edge platforms using 8 popular anomaly detection workloads. Comparative experiments with state-of-the-art methods show that our approach provides 145.8 to 0.56 trillion times more scaling options without increasing training and inference overheads, thus achieving as much as 15.05\% increase in accuracy under the same deadlines.},
  archive      = {J_TPDS},
  author       = {Qinglong Zhang and Rui Han and Gaofeng Xin and Chi Harold Liu and Guoren Wang and Lydia Y. Chen},
  doi          = {10.1109/TPDS.2021.3137631},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2927-2942},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Lightweight and accurate DNN-based anomaly detection at edge},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Building high-throughput neural architecture search
workflows via a decoupled fitness prediction engine. <em>TPDS</em>,
<em>33</em>(11), 2913‚Äì2926. (<a
href="https://doi.org/10.1109/TPDS.2022.3140681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks (NN) are used in high-performance computing and high-throughput analysis to extract knowledge from datasets. Neural architecture search (NAS) automates NN design by generating, training, and analyzing thousands of NNs. However, NAS requires massive computational power for NN training. To address challenges of efficiency and scalability, we propose PENGUIN , a decoupled fitness prediction engine that informs the search without interfering in it. PENGUIN uses parametric modeling to predict fitness of NNs. Existing NAS methods and parametric modeling functions can be plugged into PENGUIN to build flexible NAS workflows. Through this decoupling and flexible parametric modeling, PENGUIN reduces training costs: it predicts the fitness of NNs, enabling NAS to terminate training NNs early. Early termination increases the number of NNs that fixed compute resources can evaluate, thus giving NAS additional opportunity to find better NNs. We assess the effectiveness of our engine on 6,000 NNs across three diverse benchmark datasets and three state of the art NAS implementations using the Summit supercomputer. Augmenting these NAS implementations with PENGUIN can increase throughput by a factor of 1.6 to 7.1. Furthermore, walltime tests indicate that PENGUIN can reduce training time by a factor of 2.5 to 5.3.},
  archive      = {J_TPDS},
  author       = {Ariel Keller Rorabaugh and Silvina Ca√≠no-Lores and Travis Johnston and Michela Taufer},
  doi          = {10.1109/TPDS.2022.3140681},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2913-2926},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Building high-throughput neural architecture search workflows via a decoupled fitness prediction engine},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting throughput of distributed stochastic gradient
descent. <em>TPDS</em>, <em>33</em>(11), 2900‚Äì2912. (<a
href="https://doi.org/10.1109/TPDS.2022.3151739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training jobs of deep neural networks (DNNs) can be accelerated through distributed variants of stochastic gradient descent (SGD), where multiple nodes process training examples and exchange updates. The total throughput of the nodes depends not only on their computing power, but also on their networking speeds and coordination mechanism (synchronous or asynchronous, centralized or decentralized), since communication bottlenecks and stragglers can result in sublinear scaling when additional nodes are provisioned. In this paper, we propose two classes of performance models to predict throughput of distributed SGD: fine-grained models , representing many elementary computation/communication operations and their dependencies; and coarse-grained models , where SGD steps at each node are represented as a sequence of high-level phases without parallelism between computation and communication. Using a PyTorch implementation, real-world DNN models and different cloud environments, our experimental evaluation illustrates that, while fine-grained models are more accurate and can be easily adapted to new variants of distributed SGD, coarse-grained models can provide similarly accurate predictions when augmented with ad hoc heuristics, and their parameters can be estimated with profiling information that is easier to collect.},
  archive      = {J_TPDS},
  author       = {Zhuojin Li and Marco Paolieri and Leana Golubchik and Sung-Han Lin and Wumo Yan},
  doi          = {10.1109/TPDS.2022.3151739},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2900-2912},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Predicting throughput of distributed stochastic gradient descent},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic generation of high-performance convolution kernels
on ARM CPUs for deep learning. <em>TPDS</em>, <em>33</em>(11),
2885‚Äì2899. (<a href="https://doi.org/10.1109/TPDS.2022.3146257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present FastConv , a template-based code auto-generation open-source library that can automatically generate high-performance deep learning convolution kernels of arbitrary matrices/tensors shapes. FastConv is based on the Winograd algorithm, which is reportedly the highest performing algorithm for the time-consuming layers of convolutional neural networks. ARM CPUs cover a wide range of designs and specifications, from embedded devices to HPC-grade CPUs. The leads to the dilemma of how to consistently optimize Winograd-based convolution solvers for convolution layers of different shapes. FastConv addresses this problem by using templates to auto-generate multiple shapes of tuned kernels variants suitable for skinny tall matrices. As a performance portable library, FastConv transparently searches for the best combination of kernel shapes, cache tiles, scheduling of loop orders, packing strategies, access patterns, and online/offline computations. Auto-tuning is used to search the parameter configuration space for the best performance for a given target architecture and problem size. Results show 1.02x to 1.40x, 1.14x to 2.17x, and 1.22x and 2.48x speedup is achieved over NNPACK, ARM NN, and FeatherCNN on Kunpeng 920. Furthermore, performance portability experiments with various convolution shapes show that FastConv achieves 1.2x to 1.7x speedup and 2x to 22x speedup over NNPACK and ARM NN inference engine using Winograd on Kunpeng 920. CPU performance portability evaluation on VGG‚Äì16 show an average speedup over NNPACK of 1.42x, 1.21x, 1.26x, 1.37x, 2.26x, and 11.02x on Kunpeng 920, Snapdragon 835, 855, 888, Apple M1, and AWS Graviton2, respectively.},
  archive      = {J_TPDS},
  author       = {Jintao Meng and Chen Zhuang and Peng Chen and Mohamed Wahib and Bertil Schmidt and Xiao Wang and Haidong Lan and Dou Wu and Minwen Deng and Yanjie Wei and Shengzhong Feng},
  doi          = {10.1109/TPDS.2022.3146257},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2885-2899},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Automatic generation of high-performance convolution kernels on ARM CPUs for deep learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ReHy: A ReRAM-based digital/analog hybrid PIM architecture
for accelerating CNN training. <em>TPDS</em>, <em>33</em>(11),
2872‚Äì2884. (<a href="https://doi.org/10.1109/TPDS.2021.3138087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing-In-Memory (PIM) has emerged as a high-performance and energy-efficient computing paradigm for accelerating convolutional neural network (CNN) applications. Resistive random access memory (ReRAM) has been widely used in PIM architectures due to its extremely high efficiency for accelerating matrix-vector multiplications through analog computing. However, because CNN training usually requires high-precision computation in the backward propagation (BP) stage, the limited precision of analog PIM accelerators impedes their adoption in CNN training. In this article, we propose ReHy, a hybrid PIM accelerator to support CNN training in ReRAM arrays. It is composed of Analog PIM (APIM) and Digital PIM (DPIM) modules. ReHy uses APIM to accelerate the feed-forward propagation (FP) stage for high performance, and DPIM to process the BP stage for high accuracy. We exploit the capability of ReRAM for Boolean logic operations to design the DPIM architecture. Particularly, we design floating-point multiplication and addition operators to support matrix multiplications in ReRAM arrays. We also propose a performance model to offload high-precision matrix multiplications to DPIM according to the data parallelism. Experimental results show that ReHy can speed up CNN training by 48.8√ó and 2.4√ó, and reduce energy consumption by 35.1√ó and 2.33√ó, compared with CPU/GPU architectures (baseline) and the state-of-the-art FloatPIM, respectively.},
  archive      = {J_TPDS},
  author       = {Hai Jin and Cong Liu and Haikun Liu and Ruikun Luo and Jiahong Xu and Fubing Mao and Xiaofei Liao},
  doi          = {10.1109/TPDS.2021.3138087},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2872-2884},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ReHy: A ReRAM-based Digital/Analog hybrid PIM architecture for accelerating CNN training},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heterogeneous systolic array architecture for compact CNNs
hardware accelerators. <em>TPDS</em>, <em>33</em>(11), 2860‚Äì2871. (<a
href="https://doi.org/10.1109/TPDS.2021.3129647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compact convolutional neural networks have become a hot research topic. However, we find that the systolic array accelerators are extremely inefficient in dealing with compact models, especially when processing depthwise convolutional layers in the neural networks. To make systolic arrays more efficient for compact convolutional neural networks, we propose the heterogeneous systolic array (HeSA) architecture. It introduces heterogeneous processing elements that support multiple dataflows, which can further exploit the reuse data chance of depthwise convolutional layers and without changing the structure of the na√É¬Øve systolic array. By increasing the utilization rate of processing elements in the array, the HeSA improves the performance, throughput, and energy efficiency compared to the standard baseline. In addition, we design the flexible buffer structure for the HeSA. Through configuring it, the HeSA can allocate bandwidth flexibly to maintaining high performance and low communication cost. Based on our evaluation with typical workloads, the HeSA improves the utilization rate of the computing resource in depthwise convolutional layers by 4.5√ó - 11.2√ó and acquires 1.6 - 3.1√ó total performance speedup compared to the standard systolic array architecture. In the large-scale array design, the HeSA can reduce the data traffic by 40\% while maintaining the same performance as the scaling-out method. By improving the on-chip data reuse opportunities and reducing data traffic, the HeSA saves over 20\% in energy consumption. Meanwhile, the area of the HeSA is basically unchanged compared to the baseline due to its simple design.},
  archive      = {J_TPDS},
  author       = {Rui Xu and Sheng Ma and Yaohua Wang and Yang Guo and Dongsheng Li and Yuran Qiao},
  doi          = {10.1109/TPDS.2021.3129647},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2860-2871},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Heterogeneous systolic array architecture for compact CNNs hardware accelerators},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bridging the gap between deep learning and frustrated
quantum spin system for extreme-scale simulations on new generation of
sunway supercomputer. <em>TPDS</em>, <em>33</em>(11), 2846‚Äì2859. (<a
href="https://doi.org/10.1109/TPDS.2022.3145163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient numerical methods are promising tools for delivering unique insights into the fascinating properties of physics, such as the highly frustrated quantum many-body systems. However, the computational complexity of obtaining the wave functions for accurately describing the quantum states increases exponentially with respect to particle number. Here we present a novel convolutional neural network (CNN) for simulating the two-dimensional highly frustrated spin- $1/2$ $J_1-J_2$ Heisenberg model, meanwhile the simulation is performed at an extreme scale system with low cost and high scalability. By ingenious employment of transfer learning and CNN‚Äôs translational invariance, we successfully investigate the quantum system with the lattice size up to $24\times 24$ , within 30 million cores of the new generation of sunway supercomputer. The final achievement demonstrates the effectiveness of CNN-based representation of quantum-state and brings the state-of-the-art record up to a brand-new level from both aspects of remarkable accuracy and unprecedented scales.},
  archive      = {J_TPDS},
  author       = {Mingfan Li and Junshi Chen and Qian Xiao and Fei Wang and Qingcai Jiang and Xuncheng Zhao and Rongfen Lin and Hong An and Xiao Liang and Lixin He},
  doi          = {10.1109/TPDS.2022.3145163},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2846-2859},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Bridging the gap between deep learning and frustrated quantum spin system for extreme-scale simulations on new generation of sunway supercomputer},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SGCNAX: A scalable graph convolutional neural network
accelerator with workload balancing. <em>TPDS</em>, <em>33</em>(11),
2834‚Äì2845. (<a href="https://doi.org/10.1109/TPDS.2021.3133691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (GCNs) have emerged as promising tools for graph-based machine learning applications. Given that GCNs are both compute- and memory-intensive, this constitutes a major challenge for the underlying hardware to efficiently process large-scale GCNs. In this article, we introduce SGCNAX, a scalable GCN accelerator architecture for the high-performance and energy-efficient acceleration of GCNs. Unlike prior GCN accelerators that either employ limited loop optimization techniques, or determine the design variables based on random sampling, we systematically explore the loop optimization techniques for GCN acceleration and propose a flexible GCN dataflow that adapts to different GCN configurations to achieve optimal efficiency. We further propose two hardware-based techniques to address the workload imbalance problem caused by the unbalanced distribution of zeros in GCNs. Specifically, SGCNAX exploits an outer-product-based computation architecture that mitigates the intra-PE (Processing Elements) workload imbalance, and employs a group-and-shuffle approach to mitigate the inter-PE workload imbalance. Simulation results show that SGCNAX performs 9.2√ó, 1.6√ó and 1.2√ó better, and reduces DRAM accesses by a factor of 9.7√ó, 2.9√ó and 1.2√ó compared to HyGCN, AWB-GCN, and GCNAX, respectively.},
  archive      = {J_TPDS},
  author       = {Jiajun Li and Hao Zheng and Ke Wang and Ahmed Louri},
  doi          = {10.1109/TPDS.2021.3133691},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2834-2845},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SGCNAX: A scalable graph convolutional neural network accelerator with workload balancing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). GOSH: Task scheduling using deep surrogate models in fog
computing environments. <em>TPDS</em>, <em>33</em>(11), 2821‚Äì2833. (<a
href="https://doi.org/10.1109/TPDS.2021.3136672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, intelligent scheduling approaches using surrogate models have been proposed to efficiently allocate volatile tasks in heterogeneous fog environments. Advances like deterministic surrogate models, deep neural networks (DNN) and gradient-based optimization allow low energy consumption and response times to be reached. However, deterministic surrogate models, which estimate objective values for optimization, do not consider the uncertainties in the distribution of the Quality of Service (QoS) objective function that can lead to high Service Level Agreement (SLA) violation rates. Moreover, the brittle nature of DNN training and the limited exploration with low agility in gradient-based optimization prevent such models from reaching minimal energy or response times. To overcome these difficulties, we present a novel scheduler that we call GOSH for Gradient Based Optimization using Second Order derivatives and Heteroscedastic Deep Surrogate Models. GOSH uses a second-order gradient based optimization approach to obtain better QoS and reduce the number of iterations to converge to a scheduling decision, subsequently lowering the scheduling time. Instead of a vanilla DNN, GOSH uses a Natural Parameter Network (NPN) to approximate objective scores. Further, a Lower Confidence Bound (LCB) optimization approach allows GOSH to find an optimal trade-off between greedy minimization of the mean latency and uncertainty reduction by employing error-based exploration. Thus, GOSH and its co-simulation based extension GOSH*, can adapt quickly and reach better objective scores than baseline methods. We show that GOSH* reaches better objective scores than GOSH, but it is suitable only for high resource availability settings, whereas GOSH is apt for limited resource settings. Real system experiments for both GOSH and GOSH* show significant improvements against the state-of-the-art in terms of energy consumption, response time and SLA violations by up to 18, 27 and 82 percent, respectively.},
  archive      = {J_TPDS},
  author       = {Shreshth Tuli and Giuliano Casale and Nicholas R. Jennings},
  doi          = {10.1109/TPDS.2021.3136672},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2821-2833},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GOSH: Task scheduling using deep surrogate models in fog computing environments},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Liquid: Intelligent resource estimation and
network-efficient scheduling for deep learning jobs on distributed GPU
clusters. <em>TPDS</em>, <em>33</em>(11), 2808‚Äì2820. (<a
href="https://doi.org/10.1109/TPDS.2021.3138825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) is becoming increasingly popular in many domains, including computer vision, speech recognition, self-driving automobiles, etc. GPU can train DL models efficiently but is expensive, which motivates users to share GPU resource to reduce money costs in practice. To ensure efficient sharing among multiple users, it is necessary to develop efficient GPU resource management and scheduling solutions. However, existing ones have several shortcomings. First, they require the users to specify the job resource requirement which is usually quite inaccurate and leads to cluster resource underutilization. Second, when scheduling DL jobs, they rarely take the cluster network characteristics into consideration, resulting in low job execution performance. To overcome the above issues, we propose Liquid, an efficient GPU resource management platform for DL jobs with intel li gent resource re qui rement estimation and sche d uling. First, we propose a regression model based method for job resource requirement estimation to avoid users over-allocating computing resources. Second, we propose intelligent cluster network-efficient scheduling methods in both immediate and batch modes based on the above resource requirement estimation techniques. Third, we further propose three system-level optimizations, including pre-scheduling data transmission, fine-grained GPU sharing, and event-driven communication. Experimental results show that our Liquid can accelerate the job execution speed by 18\% on average and shorten the average job completion time (JCT) by 21\% compared with cutting-edge solutions. Moreover, the proposed optimization methods are effective in various scenarios.},
  archive      = {J_TPDS},
  author       = {Rong Gu and Yuquan Chen and Shuai Liu and Haipeng Dai and Guihai Chen and Kai Zhang and Yang Che and Yihua Huang},
  doi          = {10.1109/TPDS.2021.3138825},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2808-2820},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Liquid: Intelligent resource estimation and network-efficient scheduling for deep learning jobs on distributed GPU clusters},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). MCDS: AI augmented workflow scheduling in mobile edge cloud
computing systems. <em>TPDS</em>, <em>33</em>(11), 2794‚Äì2807. (<a
href="https://doi.org/10.1109/TPDS.2021.3135907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Workflow scheduling is a long-studied problem in parallel and distributed computing (PDC), aiming to efficiently utilize compute resources to meet user&#39;s service requirements. Recently proposed scheduling methods leverage the low response times of edge computing platforms to optimize application Quality of Service (QoS). However, scheduling workflow applications in mobile edge-cloud systems is challenging due to computational heterogeneity, changing latencies of mobile devices and the volatile nature of workload resource requirements. To overcome these difficulties, it is essential, but at the same time challenging, to develop a long-sighted optimization scheme that efficiently models the QoS objectives. In this work, we propose MCDS: Monte Carlo Learning using Deep Surrogate Models to efficiently schedule workflow applications in mobile edge-cloud computing systems. MCDS is an Artificial Intelligence (AI) based scheduling approach that uses a tree-based search strategy and a deep neural network-based surrogate model to estimate the long-term QoS impact of immediate actions for robust optimization of scheduling decisions. Experiments on physical and simulated edge-cloud testbeds show that MCDS can improve over the state-of-the-art methods in terms of energy consumption, response time, SLA violations and cost by at least 6.13, 4.56, 45.09 and 30.71 percent respectively.},
  archive      = {J_TPDS},
  author       = {Shreshth Tuli and Giuliano Casale and Nicholas R. Jennings},
  doi          = {10.1109/TPDS.2021.3135907},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2794-2807},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {MCDS: AI augmented workflow scheduling in mobile edge cloud computing systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ASTRAEA: A fair deep learning scheduler for multi-tenant GPU
clusters. <em>TPDS</em>, <em>33</em>(11), 2781‚Äì2793. (<a
href="https://doi.org/10.1109/TPDS.2021.3136245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern GPU clusters are designed to support distributed Deep Learning jobs from multiple tenants concurrently. Each tenant may have varied and dynamic resource demands. Unfortunately, existing GPU schedulers fail to thoroughly consider the fairness among the tenants and jobs, which can result in unbalanced resource allocation and unfair user experience. In this article, we present an efficient solution to provide strong fairness while maintaining high scheduling effectiveness in multi-tenant GPU clusters. First, we introduce a novel Long-Term GPU-time Fairness metric, which can comprehensively evaluate the fairness at both the tenant and job levels, based on both the temporal and spatial impacts of resource allocation. Second, we design a new and practical GPU scheduler, Astraea , to enforce the desired fairness among tenants and jobs. Large-scale evaluations show that Astraea can improve tenant fairness by up to 9.42√ó compared to state-of-the-art schedulers, without sacrificing the average job completion time.},
  archive      = {J_TPDS},
  author       = {Zhisheng Ye and Peng Sun and Wei Gao and Tianwei Zhang and Xiaolin Wang and Shengen Yan and Yingwei Luo},
  doi          = {10.1109/TPDS.2021.3136245},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2781-2793},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ASTRAEA: A fair deep learning scheduler for multi-tenant GPU clusters},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). WAMP<span class="math inline"><sup>2</sup></span><!-- -->2S:
Workload-aware GPU performance model based pseudo-preemptive real-time
scheduling for the airborne embedded system. <em>TPDS</em>,
<em>33</em>(11), 2767‚Äì2780. (<a
href="https://doi.org/10.1109/TPDS.2021.3134269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New generation airborne embedded system has deployed Graphical Processing Units (GPUs) to raise processing capability to meet growing computational demands. Comparing with the cloud system, the airborne embedded system usually has a fixed application set, but strict real-time constraints. Unfortunately, the inherent GPU scheduler does not consider the application priority, which cannot provide the sufficient real-time capability to the airborne embedded system. To meet timeliness requirements, it is necessary to predict timing behaviors of those applications and design a real-time scheduling policy based on priority and deadline. We therefore propose WAMP $^2$ S, a workload-aware GPU performance model based pseudo-preemptive real-time scheduling algorithm for the airborne embedded system. The workload-aware GPU performance model can accurately predict the execution time of an application, which is running concurrently with other applications on GPU. The pseudo-preemptive real-time scheduling algorithm can provide the approximate preemption by dynamically adjusting GPU computing resources for active applications. Unlike previous work on GPU performance model and GPU real-time scheduling, WAMP $^2$ S considers the impact of co-executing workload on the execution time estimation and provides a software-only approach for preemption support. In addition, WAMP $^2$ S implements a prototype GPU scheduler without any source code analysis. We evaluate the proposed GPU performance model and real-time scheduling algorithm in both simulated and realistic application sets. Experimental results illustrate that WAMP $^2$ S can achieve low prediction error and high scheduling success ratio.},
  archive      = {J_TPDS},
  author       = {Yuan Yao and Shuangyang Liu and Sikai Wu and Jinyu Wang and Jinting Ni and Gang Yang and Yu Zhang},
  doi          = {10.1109/TPDS.2021.3134269},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2767-2780},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {WAMP$^2$2S: Workload-aware GPU performance model based pseudo-preemptive real-time scheduling for the airborne embedded system},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A global cost-aware container scheduling strategy in cloud
data centers. <em>TPDS</em>, <em>33</em>(11), 2752‚Äì2766. (<a
href="https://doi.org/10.1109/TPDS.2021.3133868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale Internet applications running on data centers are typically instantiated as a set of containers. Assigning a container to its affinity machine can reduce communication and transport costs while assigning it to the anti-affinity machine may affect the proper operation of the container. Existing container scheduling methods cannot accommodate these two types of requirements. In order to reduce the operation and maintenance cost of data centers, this article focuses on the container instance allocation problem in heterogeneous server cluster, and proposes a global cost-aware scheduling algorithm (GCCS) to solve it. The purpose is to minimize the total power consumption of the cluster from a global perspective, while trying to meet the affinity/anti-affinity requirements of applications. We study the number of containers per server selected by the application, model it as an integer linear program (ILP), and then propose a heuristic search algorithm to repair the relaxation solution of the ILP into a suboptimal feasible solution. In particular, we use Bayesian optimizer to perform a number of automated development and exploration processes for the selection of the cost coefficient. The experiments are carried out with the best cost coefficient recommended by Bayesian optimizer. Finally, the results demonstrate that GCCS can significantly reduce the total power consumption of the cluster, while maintaining a high affinity satisfaction ratio.},
  archive      = {J_TPDS},
  author       = {Saiqin Long and Wen Wen and Zhetao Li and Kenli Li and Rong Yu and Jiang Zhu},
  doi          = {10.1109/TPDS.2021.3133868},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2752-2766},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A global cost-aware container scheduling strategy in cloud data centers},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The supermarket model with known and predicted service
times. <em>TPDS</em>, <em>33</em>(11), 2740‚Äì2751. (<a
href="https://doi.org/10.1109/TPDS.2022.3146195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The supermarket model refers to a system with a large number of queues, where new customers choose $d$ queues at random and join the one with the fewest customers. This model demonstrates the power of even small amounts of choice, as compared to simply joining a queue chosen uniformly at random, for load balancing systems. In this work we perform simulation-based studies to consider variations where service times for a customer are predicted , as might be done in modern settings using machine learning techniques or related mechanisms. Our primary takeaway is that using even seemingly weak predictions of service times can yield significant benefits over blind First In First Out queueing in this context. However, some care must be taken when using predicted service time information to both choose a queue and order elements for service within a queue; while in many cases using the information for both choosing and ordering is beneficial, in many of our simulation settings we find that simply using the number of jobs to choose a queue is better when using predicted service times to order jobs in a queue. In our simulations, we evaluate both synthetic and real-world workloads‚Äìin the latter, service times are predicted by machine learning. Our results provide practical guidance for the design of real-world systems; moreover, we leave many natural theoretical open questions for future work, validating their relevance to real-world situations.},
  archive      = {J_TPDS},
  author       = {Michael Mitzenmacher and Matteo Dell&#39;Amico},
  doi          = {10.1109/TPDS.2022.3146195},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2740-2751},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {The supermarket model with known and predicted service times},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the benefits of multiple gossip steps in
communication-constrained decentralized federated learning.
<em>TPDS</em>, <em>33</em>(11), 2727‚Äì2739. (<a
href="https://doi.org/10.1109/TPDS.2021.3138977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an emerging collaborative machine learning (ML) framework that enables training of predictive models in a distributed fashion where the communication among the participating nodes are facilitated by a central server. To deal with the communication bottleneck at the server, decentralized FL (DFL) methods advocate rely on local communication of nodes with their neighbors according to a specific communication network. In DFL, it is common algorithmic practice to have nodes interleave (local) gradient descent iterations with gossip (i.e., averaging over the network) steps. As the size of the ML models grows, the limited communication bandwidth among the nodes does not permit communication of full-precision messages; hence, it is becoming increasingly common to require that messages be lossy, compressed versions of the local parameters. The requirement of communicating compressed messages gives rise to the important question: given a fixed communication budget, what should be our communication strategy to minimize the (training) loss as much as possible? In this article, we explore this direction, and show that in such compressed DFL settings, there are benefits to having multiple gossip steps between subsequent gradient iterations, even when the cost of doing so is appropriately accounted for, e.g., by means of reducing the precision of compressed information. In particular, we show that having ${\mathcal O}(\log \frac{1}{\epsilon })$ gradient iterations with constant step size - and ${\mathcal O}(\log \frac{1}{\epsilon })$ gossip steps between every pair of these iterations - enables convergence to within $\epsilon$ of the optimal value for a class of non-convex problems that arise in the training of deep learning models, namely, smooth non-convex objectives satisfying Polyak-≈Åojasiewicz condition. Empirically, we show that our proposed scheme bridges the gap between centralized gradient descent and DFL on various machine learning tasks across different network topologies and compression operators.},
  archive      = {J_TPDS},
  author       = {Abolfazl Hashemi and Anish Acharya and Rudrajit Das and Haris Vikalo and Sujay Sanghavi and Inderjit Dhillon},
  doi          = {10.1109/TPDS.2021.3138977},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2727-2739},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {On the benefits of multiple gossip steps in communication-constrained decentralized federated learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Differentially private federated temporal difference
learning. <em>TPDS</em>, <em>33</em>(11), 2714‚Äì2726. (<a
href="https://doi.org/10.1109/TPDS.2021.3133898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers a federated temporal difference (TD) learning algorithm and provides both asymptotic and finite-time analyses. To protect each worker agent&#39;s cost information from being acquired by possible attackers, we propose a privacy-preserving variant of the algorithm by adding perturbation to the exchanged information. We show the rigorous differential privacy guarantee by using moments accountant and derive an upper bound of the utility loss for the privacy-preserving algorithm. Evaluations are also provided to corroborate the efficiency of the algorithms.},
  archive      = {J_TPDS},
  author       = {Yiming Zeng and Yixuan Lin and Yuanyuan Yang and Ji Liu},
  doi          = {10.1109/TPDS.2021.3133898},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2714-2726},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Differentially private federated temporal difference learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LightFed: An efficient and secure federated edge learning
system on model splitting. <em>TPDS</em>, <em>33</em>(11), 2701‚Äì2713.
(<a href="https://doi.org/10.1109/TPDS.2021.3127712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the integration of Artificial Intelligence (AI) and Internet of Things (IoT), the Federated Edge Learning (FEL), a promising computing framework is developing. However, there are still unsolved issues on communication efficiency and data security due to the huge models and unreliable transmission links. To address these issues, this paper proposes a novel federated edge learning system, called LightFed, where the edge nodes upload only vital partial local models, and successfully achieve lightweight communication and model aggregation. First, a novel model aggregation method Model Splitting and Splicing (MSS) and a Selective Parameter Transmission (SPT) scheme are proposed. By detecting the updating gradients of local parameters and filtering significant parameters, selective rotated transmission and efficient aggregation of local models are achieved. Second, a Training Filling Model (TFM) is proposed to infer the total data distribution of edge nodes, and train a filling model to mitigate the unbalanced training data without violating the data privacy of individual users. Moreover, a blockchain-powered confusion transmission mechanism is proposed for defending the attacks from external adversaries and protecting the model information. Finally, extensive experimental results demonstrate that our LightFed significantly outperforms the existing FEL systems in terms of communication efficiency and privacy security.},
  archive      = {J_TPDS},
  author       = {Jialin Guo and Jie Wu and Anfeng Liu and Neal N. Xiong},
  doi          = {10.1109/TPDS.2021.3127712},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2701-2713},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {LightFed: An efficient and secure federated edge learning system on model splitting},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Min-max cost optimization for efficient hierarchical
federated learning in wireless edge networks. <em>TPDS</em>,
<em>33</em>(11), 2687‚Äì2700. (<a
href="https://doi.org/10.1109/TPDS.2021.3131654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a distributed machine learning technology that can protect users‚Äô data privacy, so it has attracted more and more attention in the industry and academia. Nonetheless, most of the existing works focused on the cost optimization of the entire process, while the cost of individual participants cannot be considered. In this article, we explore a min-max cost-optimal problem to guarantee the convergence rate of federated learning in terms of cost in wireless edge networks. In particular, we minimize the cost of the worst-case participant subject to the delay, local CPU-cycle frequency, power allocation, local accuracy, and subcarrier assignment constraints. Considering that the formulated problem is a mixed-integer nonlinear programming problem, we decompose it into several sub-problems to derive its solutions, in which the subcarrier assignment and power allocation are obtained by utilizing the Lagrangian dual decomposition method, the CPU-cycle frequency is obtained by a heuristic algorithm, and the local accuracy is obtained by an iteration algorithm. Simulation results show the convergence of the proposed algorithm and reveal that the proposed scheme can accomplish a tradeoff between the cost and fairness by comparing the proposed scheme with the existing schemes.},
  archive      = {J_TPDS},
  author       = {Jie Feng and Lei Liu and Qingqi Pei and Keqin Li},
  doi          = {10.1109/TPDS.2021.3131654},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2687-2700},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Min-max cost optimization for efficient hierarchical federated learning in wireless edge networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reputation-aware hedonic coalition formation for efficient
serverless hierarchical federated learning. <em>TPDS</em>,
<em>33</em>(11), 2675‚Äì2686. (<a
href="https://doi.org/10.1109/TPDS.2021.3139039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amid growing concerns on data privacy, Federated Learning (FL) has emerged as a promising privacy preserving distributed machine learning paradigm. Given that the FL network is expected to be implemented at scale, several studies have proposed system architectures towards improving the network scalability and efficiency. Specifically, the Hierarchical FL (HFL) network utilizes cluster heads, e.g., base stations, for the intermediate aggregation and relay of model parameters. Serverless FL is also proposed recently, in which the data owners, i.e., workers, exchange the local model parameters among a neighborhood of workers. This decentralized approach reduces the risk of a single point of failure but inevitably incurs significant communication overheads. To achieve the best of both worlds, we propose the Serverless Hierarchical Federated Learning (SHFL) framework in this article. The SHFL framework adopts a two-layer system architecture. In the lower layer, the FL workers are grouped into clusters under cluster heads. In the upper layer, the cluster heads exchange the intermediate parameters with their one-hop neighbors without the aid of a central server. To improve the sustainable efficiency of the FL system while taking into account the incentive design for workers‚Äô marginal contributions in the system, we propose the reputation-aware hedonic coalition formation game in this article. Specifically, the workers are rewarded for their marginal contribution to the cluster, whereas the reputation opinions of each cluster head is updated in a decentralized manner, thereby deterring malicious behaviors by the cluster head. This improves the performance of the network since cluster heads with higher reputation scores are more reliable in relaying the intermediate model parameters. The simulation results show that our proposed hedonic coalition formation algorithm converges to a Nash-stable partition and improves the network efficiency.},
  archive      = {J_TPDS},
  author       = {Jer Shyuan Ng and Wei Yang Bryan Lim and Zehui Xiong and Xianbin Cao and Jiangming Jin and Dusit Niyato and Cyril Leung and Chunyan Miao},
  doi          = {10.1109/TPDS.2021.3139039},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2675-2686},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Reputation-aware hedonic coalition formation for efficient serverless hierarchical federated learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Flexible clustered federated learning for client-level data
distribution shift. <em>TPDS</em>, <em>33</em>(11), 2661‚Äì2674. (<a
href="https://doi.org/10.1109/TPDS.2021.3134263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) enables the multiple participating devices to collaboratively contribute to a global neural network model while keeping the training data locally. Unlike the centralized training setting, the non-IID, imbalanced (statistical heterogeneity) and distribution shifted training data of FL is distributed in the federated network, which will increase the divergences between the local models and the global model, further degrading performance. In this paper, we propose a flexible clustered federated learning (CFL) framework named FlexCFL, in which we 1) group the training of clients based on the similarities between the clients‚Äô optimization directions for lower training divergence; 2) implement an efficient newcomer device cold start mechanism for framework scalability and practicality; 3) flexibly migrate clients to meet the challenge of client-level data distribution shift. FlexCFL can achieve improvements by dividing joint optimization into groups of sub-optimization and can strike a balance between accuracy and communication efficiency in the distribution shift environment. The convergence and complexity are analyzed to demonstrate the efficiency of FlexCFL. We also evaluate FlexCFL on several open datasets and made comparisons with related CFL frameworks. The results show that FlexCFL can significantly improve absolute test accuracy by $+10.6\%$ on FEMNIST compared with FedAvg , $+3.5\%$ on FashionMNIST compared with FedProx , $+8.4\%$ on MNIST compared with FeSEM , $+4.7\%$ on Sentiment140 compare with IFCA . The experiment results show that FlexCFL is also communication efficient in the distribution shift environment.},
  archive      = {J_TPDS},
  author       = {Moming Duan and Duo Liu and Xinyuan Ji and Yu Wu and Liang Liang and Xianzhang Chen and Yujuan Tan and Ao Ren},
  doi          = {10.1109/TPDS.2021.3134263},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2661-2674},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Flexible clustered federated learning for client-level data distribution shift},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DONE: Distributed approximate newton-type method for
federated edge learning. <em>TPDS</em>, <em>33</em>(11), 2648‚Äì2660. (<a
href="https://doi.org/10.1109/TPDS.2022.3146253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is growing interest in applying distributed machine learning to edge computing, forming federated edge learning . Federated edge learning faces non-i.i.d. and heterogeneous data, and the communication between edge workers, possibly through distant locations and with unstable wireless networks, is more costly than their local computational overhead. In this work, we propose ${{\sf DONE}}$ , a distributed approximate Newton-type algorithm with fast convergence rate for communication-efficient federated edge learning. First, with strongly convex and smooth loss functions, ${{\sf DONE}}$ approximates the Newton direction in a distributed manner using the classical Richardson iteration on each edge worker. Second, we prove that ${{\sf DONE}}$ has linear-quadratic convergence and analyze its communication complexities. Finally, the experimental results with non-i.i.d. and heterogeneous data show that ${{\sf DONE}}$ attains a comparable performance to Newton&#39;s method. Notably, ${{\sf DONE}}$ requires fewer communication iterations compared to distributed gradient descent and outperforms DANE, FEDL, and GIANT, state-of-the-art approaches, in the case of non-quadratic loss functions.},
  archive      = {J_TPDS},
  author       = {Canh T. Dinh and Nguyen H. Tran and Tuan Dung Nguyen and Wei Bao and Amir Rezaei Balef and Bing B. Zhou and Albert Y. Zomaya},
  doi          = {10.1109/TPDS.2022.3146253},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2648-2660},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DONE: Distributed approximate newton-type method for federated edge learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guest editorial. <em>TPDS</em>, <em>33</em>(11), 2644‚Äì2647.
(<a href="https://doi.org/10.1109/TPDS.2022.3166681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special section focuses on the state-of-the-art technologies on parallel and distributed computing techniques for artificial intelligence (AI), machine learning (ML), and deep learning (DL). AI, ML, and DL can enable computers the ability to learn from a large amount of data and use the learned model to optimize a complex problem or discover rules in a complicated system. AI, ML and DL can be applied to push forward the boundaries for many domains and significantly influence our daily life.},
  archive      = {J_TPDS},
  author       = {Jidong Zhai and Min Si and Antonio J. Pe√±a},
  doi          = {10.1109/TPDS.2022.3166681},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2644-2647},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Guest editorial},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DrTM+b: Replication-driven live reconfiguration for fast and
general distributed transaction processing. <em>TPDS</em>,
<em>33</em>(10), 2628‚Äì2643. (<a
href="https://doi.org/10.1109/TPDS.2022.3148251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent in-memory database systems leverage advanced hardware features like RDMA to provide transaction processing at millions of transactions per second. Distributed transaction processing systems can scale to even higher rates, especially for partitionable workloads. Unfortunately, it is challenging to sustain such high rates during live reconfiguration of partitions. In this article, we observe that state-of-the-art approaches would cause notable performance disruption under fast transaction processing. To this end, this article presents DrTM+B, a live reconfiguration approach that seamlessly repartitions data with little performance disruption to running transactions. DrTM+B uses a pre-copy-based mechanism to avoid excessive data transfer by leveraging common properties in recent transactional systems. DrTM+B&#39;s reconfiguration plans reduce data movement by preferring existing data replicas, while copying data from multiple replicas asynchronously and in parallel. It further reuses the log forwarding mechanism in primary-backup replication to seamlessly track and forward dirty database tuples and avoids iterative copying costs. To commit a reconfiguration plan in a transactional-safe way, DrTM+B designs a cooperative commit protocol for synchronization of data and state among replicas. To boost the performance during data migration, DrTM+B combines the pre-copy and post-copy schemes to propose a hybrid copy scheme. The live reconfiguration approach can also coexist with fault-tolerance mechanisms of primary-backup replication to provide high availability. Evaluation on a working system based on DrTM+R with 3-way replication using typical OLTP workloads like TPC-C and SmallBank shows that DrTM+B incurs only very small performance degradation during live reconfiguration and provides high availability. Both the reconfiguration time and the downtime are also minimal.},
  archive      = {J_TPDS},
  author       = {Sijie Shen and Xingda Wei and Rong Chen and Haibo Chen and Binyu Zang},
  doi          = {10.1109/TPDS.2022.3148251},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2628-2643},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DrTM+B: Replication-driven live reconfiguration for fast and general distributed transaction processing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incentive-aware autonomous client participation in federated
learning. <em>TPDS</em>, <em>33</em>(10), 2612‚Äì2627. (<a
href="https://doi.org/10.1109/TPDS.2022.3148113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) emerges as a promising paradigm to enable a federation of clients to train a machine learning model in a privacy-preserving manner. Most existing works assumed that the central parameter server (PS) determines the participation of clients implying that clients cannot make autonomous participation decisions. The above assumption is unrealistic because the participation in FL training may incur various cost and clients also have strong desire to be rewarded for participation. To address this problem, we design a novel autonomous client participation scheme to incentivize clients. Specifically, the PS provides a certain reward shared among participating clients for each training round. Clients decide whether to participate each FL training round or not based on their own utilities (i.e., reward minus cost). The process can be modeled as a minority game (MG) with incomplete information and clients end up in the minority side win after each training round because the reward of each participating client may not cover its cost if too many clients participate and vice verse. The challenge of autonomous participation schemes lies in lowering the volatility of participating clients in each round due to the lack of coordination among clients. Through solid analysis, we prove that: 1) The volatility of participating clients in each round is very high under the standard MG scheme. 2) The volatility of participating clients can be reduced significantly under the stochastic MG scheme. 3) A coalition based MG is proposed, which can further reduce the volatility in each round. By conducting extensive experiments in real settings, we demonstrate that the stochastic MG-based scheme outperforms other state-of-the-art algorithms in terms of utility and volatility, and the coalition MG-based client participation scheme can further boost the utility by 39\%-48\% and reduce the volatility by 51\%‚Äì100\%. Moreover, our algorithms can achieve almost the same model accuracy as that obtained by centralized client participation algorithms.},
  archive      = {J_TPDS},
  author       = {Miao Hu and Di Wu and Yipeng Zhou and Xu Chen and Min Chen},
  doi          = {10.1109/TPDS.2022.3148113},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2612-2627},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Incentive-aware autonomous client participation in federated learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimizing video caching at the edge: A hybrid multi-point
process approach. <em>TPDS</em>, <em>33</em>(10), 2597‚Äì2611. (<a
href="https://doi.org/10.1109/TPDS.2022.3147240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is always a challenging problem to deliver a huge volume of videos over the Internet. To meet the high bandwidth and stringent playback demand, one feasible solution is to cache video contents on edge servers based on predicted video popularity. Traditional caching algorithms (e.g., LRU, LFU) are too simple to capture the dynamics of video popularity, especially long-tailed videos. Recent learning-driven caching algorithms (e.g., DeepCache) show promising performance, however, such black-box approaches are lack of explainability and interpretability. Moreover, the parameter tuning requires a large number of historical records, which are difficult to obtain for videos with low popularity. In this paper, we optimize video caching at the edge using a white-box approach, which is highly efficient and also completely explainable. To accurately capture the evolution of video popularity, we develop a mathematical model called HRS model, which is the combination of multiple point processes, including Hawkes‚Äô self-exciting, reactive and self-correcting processes. The key advantage of the HRS model is its explainability, and much less number of model parameters. In addition, all its model parameters can be learned automatically through maximizing the Log-likelihood function constructed by past video request events. Next, we further design an online HRS-based video caching algorithm. To verify its effectiveness, we conduct a series of experiments using real video traces collected from Tencent Video, one of the largest online video providers in China. Experiment results demonstrate that our proposed algorithm outperforms the state-of-the-art algorithms, with 15.5\% improvement on average in terms of cache hit rate under realistic settings.},
  archive      = {J_TPDS},
  author       = {Xianzhi Zhang and Yipeng Zhou and Di Wu and Miao Hu and Xi Zheng and Min Chen and Song Guo},
  doi          = {10.1109/TPDS.2022.3147240},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2597-2611},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimizing video caching at the edge: A hybrid multi-point process approach},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal embedding of aggregated service function tree.
<em>TPDS</em>, <em>33</em>(10), 2584‚Äì2596. (<a
href="https://doi.org/10.1109/TPDS.2022.3147870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many hardware-based security middleboxes have been deployed in the networks to defend against different threats. However, these hardware middleboxes are hard to upgrade or migrate. The emergence of network functions virtualization (NFV), which realizes various security functions in the form of virtual network functions (VNFs), brings many benefits to network security. To improve the security level further, several VNFs are coordinated in a pre-defined order to form service function chains (SFCs). It is expected that the SFCs are embedded properly with low cost, including the VNF setup cost and the flow routing cost. In this paper, we find that when an SFC is required by multiple flows for the identical network security threats, the total cost could be reduced by embedding an aggregated service function tree (ASFT) instead of multiple independent SFCs. We formally characterize the integer programming model of this problem and prove that it is NP-hard. Then we propose a performance-guaranteed approximation algorithm and prove that the algorithm could find the optimal solution in a special case. Extensive experiments indicate that our method can reduce the total cost by $22.0\%$ and $24.1\%$ against two compared algorithms, respectively.},
  archive      = {J_TPDS},
  author       = {Deke Guo and Bangbang Ren and Guoming Tang and Lailong Luo and Tao Chen and Xiaoming Fu},
  doi          = {10.1109/TPDS.2022.3147870},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2584-2596},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimal embedding of aggregated service function tree},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NetEC: Accelerating erasure coding reconstruction with
in-network aggregation. <em>TPDS</em>, <em>33</em>(10), 2571‚Äì2583. (<a
href="https://doi.org/10.1109/TPDS.2022.3145836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In distributed storage systems, Erasure Coding (EC) is a crucial technology to enable high data availability. By downloading parity data from survived machines, EC can reconstruct lost data with much lower storage overheads than data replication. However, this reduction in storage cost comes at the expense of extra performance problems: low reconstruction rate , high degraded read latency , and high host CPU utilization . Our analysis shows that these performance problems are deeply rooted in the host-based EC processing. To resolve these problems, we present NetEC, an in-network accelerating framework that fully offloads EC to the new generation programmable switching ASICs. We propose Explicit Buffer Size Notification (EBSN) to constrain decoding buffer usage, and design an on-switch one-to-many TCP proxy to integrate EBSN with TCP. We also design two parallel Galois Field (GF) offloading methods‚Äîtable lookup and bitmatrix methods‚Äîto maximize parsable bytes. We implement NetEC on programmable switches and integrate it with HDFS. Extensive evaluations show that NetEC improves the reconstruction rate by 2.7x-6.8x, reduces the degraded read latency significantly, and removes the host CPU overhead completely. We also emulate multi-rack scenarios and show that NetEC is able to support $\sim$ GB/s reconstruction rate and tens of concurrent tasks.},
  archive      = {J_TPDS},
  author       = {Yi Qiao and Menghao Zhang and Yu Zhou and Xiao Kong and Han Zhang and Mingwei Xu and Jun Bi and Jilong Wang},
  doi          = {10.1109/TPDS.2022.3145836},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2571-2583},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {NetEC: Accelerating erasure coding reconstruction with in-network aggregation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NetSync: A network adaptive and deduplication-inspired delta
synchronization approach for cloud storage services. <em>TPDS</em>,
<em>33</em>(10), 2554‚Äì2570. (<a
href="https://doi.org/10.1109/TPDS.2022.3145025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Delta sync (synchronization) is a key bandwidth-saving technique for cloud storage services. The representative delta sync utility, rsync , matches data chunks by sliding a search window byte-by-byte to maximize the redundancy detection for bandwidth efficiency. However, it is difficult for this process to cater to the forthcoming high-bandwidth cloud storage services which require lightweight delta sync that can well support large files. Moreover, rsync employs invariant chunking and compression methods during the sync process, making it unable to cater to services from various network environments which require the sync approach to perform well under different network conditions. Inspired by the Content-Defined Chunking (CDC) technique used in data deduplication, we propose NetSync, a network adaptive and CDC-based lightweight delta sync approach with less computing and protocol (metadata) overheads than the state-of-the-art delta sync approaches. Besides, NetSync can choose appropriate compressing and chunking strategies for different network conditions. The key idea of NetSync is (1) to simplify the process of chunk matching by proposing a fast weak hash called FastFP that is piggybacked on the rolling hashes from CDC, and redesigning the delta sync protocol by exploiting deduplication locality and weak/strong hash properties; (2) to minimize the sync time by adaptively choosing chunking parameters and compression methods according to the current network conditions. Our evaluation results driven by both benchmark and real-world datasets suggest NetSync performs $2\times$ ‚Äì $10\times$ faster and supports $30\%$ ‚Äì $80\%$ more clients than the state-of-the-art rsync -based WebR2sync+ and deduplication-based approach.},
  archive      = {J_TPDS},
  author       = {Wen Xia and Can Wei and Zhenhua Li and Xuan Wang and Xiangyu Zou},
  doi          = {10.1109/TPDS.2022.3145025},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2554-2570},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {NetSync: A network adaptive and deduplication-inspired delta synchronization approach for cloud storage services},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EdgeTB: A hybrid testbed for distributed machine learning at
the edge with high fidelity. <em>TPDS</em>, <em>33</em>(10), 2540‚Äì2553.
(<a href="https://doi.org/10.1109/TPDS.2022.3144994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed Machine Learning (DML) at the edge has become an essential topic for providing low-latency intelligence near the data sources. However, both the development and testing of DMLs lack sufficient support. Reusable libraries that abstract the general functionalities of DMLs are needed for rapid development. Moreover, existing physical testbeds are usually small and lack network flexibility, while virtual testbeds like simulators and emulators lack fidelity. This paper proposes a novel hybrid testbed EdgeTB, which provides numerous emulated nodes to generate large-scale and network-flexible test environments while incorporating physical nodes to guarantee fidelity. EdgeTB manages physical nodes and emulated nodes uniformly and supports arbitrary network topologies between nodes through dynamic configurations. Importantly, we propose Role-oriented development to support the rapid development of DMLs. Through case studies and experiments, we demonstrate that EdgeTB provides convenience for efficiently developing and testing DMLs in various structures with high fidelity and scalability.},
  archive      = {J_TPDS},
  author       = {Lei Yang and Fulin Wen and Jiannong Cao and Zhenyu Wang},
  doi          = {10.1109/TPDS.2022.3144994},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2540-2553},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {EdgeTB: A hybrid testbed for distributed machine learning at the edge with high fidelity},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed algorithms for multi-resource allocation.
<em>TPDS</em>, <em>33</em>(10), 2524‚Äì2539. (<a
href="https://doi.org/10.1109/TPDS.2022.3144376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novel network infrastructures require the distribution of computing and network resource control to meet stringent requirements in terms of latency, reliability and bitrate. 5G systems bring a key novelty in systems design that it the ‚Äònetwork slice‚Äôas a new resource provisioning entity. A network slice is meant to serve end-to-end services as a composition of different network and system resources as radio, link, storage and computing resources. Conventionally, each resource is managed by a distinct decision-maker, platform, provider, orchestrator or controller. Naturally, centralized slice orchestration approaches are proposed in the literature, where a multi-domain orchestrator allocates the resources, for instance using a multi-resource allocation rule. Nonetheless, while simplifying the algorithmic approach, centralization can come at the expense of scalability and performance. In this article, we propose new ways to distribute the slice multi-resource resource allocation problem, using cascade and parallel resource allocations that are functionally compatible with novel software platforms. We also show how to adapt the proposed algorithms to make them able to guarantee service level agreements on the minimum resource needed, and to take into account deadline priority policy scheduling. We provide an exhaustive analysis of the advantages and disadvantages of the different approaches, including a numerical analysis for a realistic setting.},
  archive      = {J_TPDS},
  author       = {Francesca Fossati and St√©phane Rovedakis and Stefano Secci},
  doi          = {10.1109/TPDS.2022.3144376},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2524-2539},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Distributed algorithms for multi-resource allocation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Communication-efficient <span
class="math inline"><em>k</em></span>k-means for edge-based machine
learning. <em>TPDS</em>, <em>33</em>(10), 2509‚Äì2523. (<a
href="https://doi.org/10.1109/TPDS.2022.3144595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of computing the $k$ -means centers for a large high-dimensional dataset in the context of edge-based machine learning, where data sources offload machine learning computation to nearby edge servers. $k$ -Means computation is fundamental to many data analytics, and the capability of computing provably accurate $k$ -means centers by leveraging the computation power of the edge servers, at a low communication and computation cost to the data sources, will greatly improve the performance of these analytics. We propose to let the data sources send small summaries, generated by joint dimensionality reduction (DR), cardinality reduction (CR), and quantization (QT), to support approximate $k$ -means computation at reduced complexity and communication cost. By analyzing the complexity, the communication cost, and the approximation error of $k$ -means algorithms based on carefully designed composition of DR/CR/QT methods, we show that: (i) it is possible to compute near-optimal $k$ -means centers at a near-linear complexity and a constant or logarithmic communication cost, (ii) the order of applying DR and CR significantly affects the complexity and the communication cost, and (iii) combining DR/CR methods with a properly configured quantizer can further reduce the communication cost without compromising the other performance metrics. Our theoretical analysis has been validated through experiments based on real datasets.},
  archive      = {J_TPDS},
  author       = {Hanlin Lu and Ting He and Shiqiang Wang and Changchang Liu and Mehrdad Mahdavi and Vijaykrishnan Narayanan and Kevin S. Chan and Stephen Pasteris},
  doi          = {10.1109/TPDS.2022.3144595},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2509-2523},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Communication-efficient $k$k-means for edge-based machine learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coordinated batching and DVFS for DNN inference on GPU
accelerators. <em>TPDS</em>, <em>33</em>(10), 2496‚Äì2508. (<a
href="https://doi.org/10.1109/TPDS.2022.3144614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Employing hardware accelerators to improve the performance and energy-efficiency of DNN applications is on the rise. One challenge of using hardware accelerators, including the GPU-based ones, is that their performance is limited by internal and external factors, such as power caps. A common approach to meet the power cap constraint is using the Dynamic Voltage Frequency Scaling (DVFS) technique. However, the functionally of this technique is limited and platform-dependent. To tackle this challenge, we propose a new control knob, which is the size of input batches fed to the GPU accelerator in DNN inference applications. We first evaluate the impact of batch size on power consumption and performance of DNN inference. Then, we introduce the design and implementation of a fast and lightweight runtime system, called BatchDVFS. Dynamic batching is implemented in BatchDVFS to adaptively change the batch size, and hence, trade-off throughput with power consumption. It employs an approach based on binary search to find the proper batch size within a short period of time. Combining dynamic batching with the DVFS technique, BatchDVFS can control the power consumption in wider ranges, and hence, yield higher throughput in the presence of power caps. To find near-optimal solution for long-running jobs that can afford a relatively significant profiling overhead, compared with BatchDVFS overhead, we also design an approach, called BOBD, that employs Bayesian Optimization to wisely explore the vast state space resulted by combination of the batch size and DVFS solutions. Conducting several experiments using a modern GPU and several DNN models and input datasets, we show that our BatchDVFS can significantly surpass the techniques solely based on DVFS or batching, regarding throughput (up to 11.2x and 2.2x, respectively), while successfully meeting the power cap.},
  archive      = {J_TPDS},
  author       = {Seyed Morteza Nabavinejad and Sherief Reda and Masoumeh Ebrahimi},
  doi          = {10.1109/TPDS.2022.3144614},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2496-2508},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Coordinated batching and DVFS for DNN inference on GPU accelerators},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic controller/switch mapping: A service oriented
assignment approach. <em>TPDS</em>, <em>33</em>(10), 2482‚Äì2495. (<a
href="https://doi.org/10.1109/TPDS.2022.3144116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the capability of decoupling the control plane and the data plane of networks, Software-Defined Network (SDN) enables flexible and efficient implementations in networks. In addition, Network Function Virtualization (NFV) with Virtual Network Function (VNF) service chain capabilities provides high-performance networks with greater scalability, elasticity, and adaptability. Such an elastic deployment of service chains results in different Service Level Agreements (SLA) and resource requirements on the control plane. In this work, we illustrate the impact of service chains on the control plane and formulate the dynamic controller/switch mapping (DCSM) problem in NFV networks in order to reduce the operational cost. We address the combinatorial optimization problem, DCSM, by designing a novel mechanism to relax DCSM into a tractable problem based on the Penalty Successive Upper Bound Minimization (PSUM) method. In doing so, we conduct several simulation scenarios to evaluate the performance. The experimental results show that our proposed algorithms can achieve a near-optimal result and reduce the operational cost up to 31.7\% and 28.3\% compared to K-Mean and the matching game-based approaches, respectively.},
  archive      = {J_TPDS},
  author       = {Chuan Pham and Duong Tuan Nguyen and Nguyen H. Tran and Kim Khoa Nguyen and Mohamed Cheriet},
  doi          = {10.1109/TPDS.2022.3144116},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2482-2495},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Dynamic Controller/Switch mapping: A service oriented assignment approach},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Execution time estimation of multithreaded programs with
critical sections. <em>TPDS</em>, <em>33</em>(10), 2470‚Äì2481. (<a
href="https://doi.org/10.1109/TPDS.2022.3143455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ideal benefit of parallelizing/multithreading a program is diminished in practice by several factors such as hardware scaling, memory bandwidth, power constraints, and synchronization due to critical sections. Several models have been proposed in the past to estimate the resulting performance and extend the traditional Amdahl‚Äôs law. In this work, we focus on the effect of synchronization, and develop a model for the execution time estimation of multithreaded programs under the presence of critical sections. The proposed model is applicable to multiple different critical sections and generalizes and improves previously proposed models. Experimental results on simulated, synthetic and benchmark examples show that the proposed model provides accurate approximations.},
  archive      = {J_TPDS},
  author       = {Dimitri Kagaris and Sourav Dutta and Stijn Eyerman},
  doi          = {10.1109/TPDS.2022.3143455},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2470-2481},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Execution time estimation of multithreaded programs with critical sections},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast and accurate statistical simulation of shared-memory
applications on multicore systems. <em>TPDS</em>, <em>33</em>(10),
2455‚Äì2469. (<a href="https://doi.org/10.1109/TPDS.2022.3143535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detailed cycle-accurate simulation of multicore systems is naturally slow. Statistical simulation is one alternative that permits trading off simulation speed for accuracy. However, there is a lack of effective memory locality models for multicore applications. Hence, existing statistical simulators neglect data-sharing between threads. Additionally, the standard method to speed up statistical simulations is to blindly reduce the trace length to be synthesized. While this gives good control over the speedup, it leaves the simulation error unbounded. In this work, we introduce a novel statistical simulation methodology for exploration of shared-memory multicore systems. It includes a new sha ring- lo cality m odel ( Shalom ) that captures and reproduces data-sharing in multithread applications. Furthermore, we propose a method to bound the simulation error for a particular metric while maximizing speedup. The technique works by monitoring the convergence of the statistical synthesis. It is referred to as con vergence- de termi n istic s imulation ( Condens ). The combination of Shalom and Condens is around 130x faster than cycle-accurate simulations with reasonable accuracy loss. Our approach is also 5x faster than state-of-the-art sampling simulation under the same accuracy level. Compared to previous statistical simulators ignoring sharing, our approach is 2x more accurate for performance metrics and 8x more accurate for cache miss estimations.},
  archive      = {J_TPDS},
  author       = {Fan Jiang and Rafael K. V. Maeda and Jun Feng and Shixi Chen and Lin Chen and Xiao Li and Jiang Xu},
  doi          = {10.1109/TPDS.2022.3143535},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2455-2469},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Fast and accurate statistical simulation of shared-memory applications on multicore systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving fairness for SSD devices through DRAM
over-provisioning cache management. <em>TPDS</em>, <em>33</em>(10),
2444‚Äì2454. (<a href="https://doi.org/10.1109/TPDS.2022.3143295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern NVMe SSDs have been widely deployed in multi-tenant cloud computing environments or multi-programming systems. When multiple applications concurrently access one SSD hardware, unfairness within the shared SSD will slow down the application significantly and lead to a violation of service level objectives. However, traditional data cache management within SSDs mainly focuses on improving cache hit ratio, which causes data cache contention and sacrifices fairness among multiple applications. In this paper, we propose a DRAM-based Over-Provisioning (OP) cache management mechanism, named Justitia, to reduce data cache contention and improve fairness for modern SSDs. Justitia consists of two stages including Static-OP stage and Dynamic-OP stage. Through the novel OP mechanism in the two stages, Justitia reduces the max slowdown by $4.5\times$ on average. At the same time, Justitia increases fairness by $20.6\times$ and buffer hit ratio by $19.6\%$ averagely, compared with the traditional shared mechanism.},
  archive      = {J_TPDS},
  author       = {Renping Liu and Zhenhua Tan and Linbo Long and Yu Wu and Yujuan Tan and Duo Liu},
  doi          = {10.1109/TPDS.2022.3143295},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2444-2454},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Improving fairness for SSD devices through DRAM over-provisioning cache management},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mobility-aware offloading and resource allocation for
distributed services collaboration. <em>TPDS</em>, <em>33</em>(10),
2428‚Äì2443. (<a href="https://doi.org/10.1109/TPDS.2022.3142314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mobile edge computing (MEC) systems, mobile users (MUs) are capable of allocating local resources (CPU frequency and transmission power) and offloading tasks to edge servers in the vicinity in order to enhance their computation capabilities and reduce back-and-forth transmission over backhaul link. Nevertheless, mobile environment makes it hard to draw offloading and resource allocation decisions under dynamical wireless channel state and users‚Äô locations. In real life, social relationship is also provably a significant factor affecting integral performance in collaborative work, which results in MUs decisions strongly coupled and renders this problem further intractable. Most of previous works ignore the impact of inter-user dependency (or data dependency among IoT devices). To bridge this gap, we study the service collaboration with master-slave dependency among service chains of MUs and formulate this combinational optimization problem as a mixed integer non-linear programming (MINLP) problem. To this end, we derive the closed-form expression of resource allocation solution by convex optimization and transform it to integer linear programming (ILP) problem. Subsequently, we propose a distributed algorithm based on Markov approximation which has polynomial computation complexity. Experimental result on real-world dataset substantiates the usefulness and superiority of our scheme, in terms of reducing latency and energy consumption.},
  archive      = {J_TPDS},
  author       = {Haowei Chen and Shuiguang Deng and Hongze Zhu and Hailiang Zhao and Rong Jiang and Schahram Dustdar and Albert Y. Zomaya},
  doi          = {10.1109/TPDS.2022.3142314},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2428-2443},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Mobility-aware offloading and resource allocation for distributed services collaboration},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decentralised data quality control in ground truth
production for autonomic decisions. <em>TPDS</em>, <em>33</em>(10),
2416‚Äì2427. (<a href="https://doi.org/10.1109/TPDS.2022.3142967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomic decision-making based on rules and metrics is inevitably on the rise in distributed software systems. Often, the metrics are acquired from system observations such as static checks and runtime traces. To avoid bias propagation and hence reduce wrong decisions in increasingly autonomous systems due to poor observation data quality, multiple independent observers can exchange their findings and produce a majority-accepted, complete and outlier-cleaned ground truth in the form of consensus-supported metrics. In this work, we motivate the growing importance of metrics for informed and autonomic decisions in clouds and other distributed systems, present reasons for diverging observations, and describe a federated approach to produce ground truth with data-centric consensus voting for more reliable decision making processes. We validate the system design with experiments in the area of cloud software artefact observations and highlight benefits for reproducible distributed system behaviour.},
  archive      = {J_TPDS},
  author       = {Panagiotis Gkikopoulos and Valerio Schiavoni and Josef Spillner},
  doi          = {10.1109/TPDS.2022.3142967},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2416-2427},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Decentralised data quality control in ground truth production for autonomic decisions},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blockchain assisted decentralized federated learning
(BLADE-FL): Performance analysis and resource allocation. <em>TPDS</em>,
<em>33</em>(10), 2401‚Äì2415. (<a
href="https://doi.org/10.1109/TPDS.2021.3138848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL), as a distributed machine learning paradigm, promotes personal privacy by local data processing at each client. However, relying on a centralized server for model aggregation, standard FL is vulnerable to server malfunctions, untrustworthy servers, and external attacks. To address these issues, we propose a decentralized FL framework by integrating blockchain into FL, namely, blockchain assisted decentralized federated learning (BLADE-FL). In a round of the proposed BLADE-FL, each client broadcasts its trained model to other clients, aggregates its own model with received ones, and then competes to generate a block before its local training on the next round. We evaluate the learning performance of BLADE-FL, and develop an upper bound on the global loss function. Then we verify that this bound is convex with respect to the number of overall aggregation rounds $K$ , and optimize the computing resource allocation for minimizing the upper bound. We also note that there is a critical problem of training deficiency, caused by lazy clients who plagiarize others‚Äô trained models and add artificial noises to disguise their cheating behaviors. Focusing on this problem, we explore the impact of lazy clients on the learning performance of BLADE-FL, and characterize the relationship among the optimal $K$ , the learning parameters, and the proportion of lazy clients. Based on the MNIST and Fashion-MNIST datasets, we see that the experimental results are consistent with the analytical ones. To be specific, the gap between the developed upper bound and experimental results is lower than $5\%$ , and the optimized $K$ based on the upper bound can effectively minimize the loss function.},
  archive      = {J_TPDS},
  author       = {Jun Li and Yumeng Shao and Kang Wei and Ming Ding and Chuan Ma and Long Shi and Zhu Han and H. Vincent Poor},
  doi          = {10.1109/TPDS.2021.3138848},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2401-2415},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Blockchain assisted decentralized federated learning (BLADE-FL): Performance analysis and resource allocation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SelectiveEC: Towards balanced recovery load on erasure-coded
storage systems. <em>TPDS</em>, <em>33</em>(10), 2386‚Äì2400. (<a
href="https://doi.org/10.1109/TPDS.2021.3129973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Erasure coding (EC) has been commonly used to offer high data reliability with low storage cost. Upon failures, the lost blocks are recovered in batches. Due to the limited number of stripes, the data layout within a batch is non-uniform. Together with the random selection of source and replacement nodes for recovery tasks, the recovery workload among live nodes is skewed within a batch, which severely slows down failure recovery. To solve this problem, We present SelectiveEC, a new recovery task scheduling module that provides provable network traffic and recovery load balancing for large-scale EC-based storage systems. It relies on bipartite graphs to model the recovery traffic among live nodes. Then, it intelligently selects tasks to form batches and carefully determines where to read source blocks or to store recovered ones, using theories such as a perfect or maximum matching and $k$ -regular spanning subgraph. SelectiveEC supports single-node failure and multi-node failure recovery, and can be deployed in both homogeneous and heterogeneous network environments. We implement SelectiveEC in HDFS, and evaluate its recovery performance in a local cluster of 18 nodes and AWS EC2 of 50 virtual machine instances. SelectiveEC increases the recovery throughput by up to $30.68\%$ compared with state-of-the-art baselines in homogeneous network environments. It further achieves $1.32\times$ recovery throughput and $1.23\times$ benchmark throughput of HDFS on average in heterogeneous network environments, due to the straggler avoidance by the balanced scheduling.},
  archive      = {J_TPDS},
  author       = {Liangliang Xu and Min Lyu and Qiliang Li and Lingjiang Xie and Cheng Li and Yinlong Xu},
  doi          = {10.1109/TPDS.2021.3129973},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2386-2400},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SelectiveEC: Towards balanced recovery load on erasure-coded storage systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Elastic resource allocation against imbalanced transaction
assignments in sharding-based permissioned blockchains. <em>TPDS</em>,
<em>33</em>(10), 2372‚Äì2385. (<a
href="https://doi.org/10.1109/TPDS.2022.3141737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the PBFT-based sharded permissioned blockchain, which executes in either a local datacenter or a rented cloud platform. In such permissioned blockchain, the transaction (TX) assignment strategy could be malicious such that the network shards may possibly receive imbalanced transactions or even bursty-TX injection attacks. An imbalanced transaction assignment brings serious threats to the stability of the sharded blockchain. A stable sharded blockchain can ensure that each shard processes the arrived transactions timely. Since the system stability is closely related to the blockchain throughput, how to maintain a stable sharded blockchain becomes a challenge. To depict the transaction processing in each network shard, we adopt the Lyapunov Optimization framework. Exploiting drift-plus-penalty (DPP) technique, we then propose an adaptive resource-allocation algorithm, which can yield the near-optimal solution for each network shard while the shard queues can also be stably maintained. We also rigorously analyze the theoretical boundaries of both the system objective and the queue length of shards. The numerical results show that the proposed algorithm can achieve a better balance between resource consumption and queue stability than other baselines. We particularly evaluate two representative cases of bursty-TX injection attacks, i.e., the continued attacks against all network shards and the drastic attacks against a single network shard. The evaluation results show that the DPP-based algorithm can well alleviate the imbalanced TX assignment, and simultaneously maintain high throughput while consuming fewer resources than other baselines.},
  archive      = {J_TPDS},
  author       = {Huawei Huang and Zhengyu Yue and Xiaowen Peng and Liuding He and Wuhui Chen and Hong-Ning Dai and Zibin Zheng and Song Guo},
  doi          = {10.1109/TPDS.2022.3141737},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2372-2385},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Elastic resource allocation against imbalanced transaction assignments in sharding-based permissioned blockchains},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient index-based approach to distributed set
reachability on small-world graphs. <em>TPDS</em>, <em>33</em>(10),
2358‚Äì2371. (<a href="https://doi.org/10.1109/TPDS.2021.3139111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Set reachability query in directed graphs has a plethora of graph-based applications such as dependency analysis and graph centrality calculation. Given two sets $S$ and $T$ of source and target vertices, set reachability query needs to acquire all pairs $(s,t)$ where $s{\in }S$ , $t{\in }T$ and $s$ can reach $t$ . The state-of-the-art approach distributed set reachability (DSR) investigates the set reachability query in a distributed environment and adopts a static graph-based index to enhance the query efficiency. Nevertheless, DSR needs to store the graph-based index in all partitions, which causes a huge space overhead. Furthermore, it cannot efficiently solve the negative query $(s,t)$ where $s$ cannot reach $t$ , since DSR needs to traverse the whole reachable paths and becomes unable to efficiently reduce the computations. To alleviate these issues, we propose a novel multi-level 2-hop (ML2hop) index for the set reachability query in a distributed environment. Based on ML2hop, we further present a bi-directional query algorithm, called MLQA, to achieve efficient support for both positive and negative queries in Pregel-like systems. Generally, MLQA is equipped with the following three significant properties: (1) Low computation costs. It reduces redundant local computations in each partition by controlling the rounds of path traversals. (2) Low communication costs. It restricts the message exchange among different partitions within one single round with guaranteed accuracy of query results. (3) High parallelism. It adopts a bi-directional query technique for message propagation, achieving the better query efficiency than the forward-traversal query strategy utilized in DSR. Experimental results over several real-world graphs demonstrate that MLQA significantly outperforms the state-of-the-art algorithm by up to two orders of magnitude speedup.},
  archive      = {J_TPDS},
  author       = {Yuanyuan Zeng and Kenli Li and Xu Zhou and Wensheng Luo and Yunjun Gao},
  doi          = {10.1109/TPDS.2021.3139111},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2358-2371},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An efficient index-based approach to distributed set reachability on small-world graphs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dependent function embedding for distributed serverless edge
computing. <em>TPDS</em>, <em>33</em>(10), 2346‚Äì2357. (<a
href="https://doi.org/10.1109/TPDS.2021.3137380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing is booming as a promising paradigm to extend service provisioning from the centralized cloud to the network edge. Benefit from the development of serverless computing, an edge server can be configured as a carrier of limited serverless functions, in the way of deploying Docker runtime and Kubernetes engine. Meanwhile, an application generally takes the form of directed acyclic graphs (DAGs), where vertices represent dependent functions and edges represent data traffic. The status quo of minimizing the completion time (a.k.a. makespan) of the application motivates the study on optimal function placement. However, current approaches lose sight of proactively splitting and mapping the traffic to the logical data paths between the heterogeneous edge servers, which could affect the makespan significantly. To remedy that, we propose an algorithm, termed as Dependent Function Embedding (DPE), to get the optimal edge server for each function to execute and the moment it starts executing. DPE finds the best segmentation of each data traffic by exquisitely solving several infinity norm minimization problems. DPE is theoretically verified to achieve the global optimality. Extensive experiments on Alibaba cluster trace show that DPE significantly outperforms two baseline algorithms in makespan by 43.19\% and 40.71\%, respectively.},
  archive      = {J_TPDS},
  author       = {Shuiguang Deng and Hailiang Zhao and Zhengzhe Xiang and Cheng Zhang and Rong Jiang and Ying Li and Jianwei Yin and Schahram Dustdar and Albert Y. Zomaya},
  doi          = {10.1109/TPDS.2021.3137380},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2346-2357},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Dependent function embedding for distributed serverless edge computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SPRINT: A high-performance, energy-efficient, and scalable
chiplet-based accelerator with photonic interconnects for CNN inference.
<em>TPDS</em>, <em>33</em>(10), 2332‚Äì2345. (<a
href="https://doi.org/10.1109/TPDS.2021.3139015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chiplet-based convolution neural network (CNN) accelerators have emerged as a promising solution to provide substantial processing power and on-chip memory capacity for CNN inference. The performance of these accelerators is often limited by inter-chiplet metallic interconnects. Emerging technologies such as photonic interconnects can overcome the limitations of metallic interconnects due to several superior properties including high bandwidth density and distance-independent latency. However, implementing photonic interconnects in chiplet-based CNN accelerators is challenging and requires combined effort of network architectural optimization and CNN dataflow customization. In this article, we propose SPRINT, a chiplet-based CNN accelerator that consists of a global buffer and several accelerator chiplets. SPRINT introduces two novel designs: (1) a photonic inter-chiplet network that can adapt to specific communication patterns in CNN inference through wavelength allocation and waveguide reconfiguration, and (2) a CNN dataflow that can leverage the broadcasting capability of photonic interconnects while minimizing the costly electrical-to-optical and optical-to-electrical signal conversions. Simulations using multiple CNN models show that SPRINT achieves up to 76\% and 68\% reduction in execution time and energy consumption, respectively, as compared to other state-of-the-art chiplet-based architectures with either metallic or photonic interconnects.},
  archive      = {J_TPDS},
  author       = {Yuan Li and Ahmed Louri and Avinash Karanth},
  doi          = {10.1109/TPDS.2021.3139015},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2332-2345},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SPRINT: A high-performance, energy-efficient, and scalable chiplet-based accelerator with photonic interconnects for CNN inference},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CAMIG: Concurrency-aware live migration management of
multiple virtual machines in SDN-enabled clouds. <em>TPDS</em>,
<em>33</em>(10), 2318‚Äì2331. (<a
href="https://doi.org/10.1109/TPDS.2021.3139014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By integrating Software-Defined Networking and cloud computing, virtualized networking and computing resources can be dynamically reallocated through live migration of Virtual Machines (VMs). Dynamic resource management such as load balancing and energy-saving policies can request multiple migrations when the algorithms are triggered periodically. There exist notable research efforts in dynamic resource management that alleviate single migration overheads, such as single migration time and co-location interference while selecting the potential VMs and migration destinations. However, by neglecting the resource dependency among potential migration requests, the existing solutions of dynamic resource management can result in the Quality of Service (QoS) degradation and Service Level Agreement (SLA) violations during the migration schedule. Therefore, it is essential to integrate both single and multiple migration overheads into VM reallocation planning. In this paper, we propose a concurrency-aware multiple migration selector that operates based on the maximal cliques and independent sets of the resource dependency graph of multiple migration requests. Our proposed method can be integrated with existing dynamic resource management policies. The experimental results demonstrate that our solution efficiently minimizes migration interference and shortens the convergence time of reallocation by maximizing the multiple migration performance while achieving the objective of dynamic resource management.},
  archive      = {J_TPDS},
  author       = {TianZhang He and Adel N. Toosi and Rajkumar Buyya},
  doi          = {10.1109/TPDS.2021.3139014},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2318-2331},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CAMIG: Concurrency-aware live migration management of multiple virtual machines in SDN-enabled clouds},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CoPA: Cold page awakening to overcome retention failures in
STT-MRAM based i/o buffers. <em>TPDS</em>, <em>33</em>(10), 2304‚Äì2317.
(<a href="https://doi.org/10.1109/TPDS.2021.3137315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance and reliability are two prominent factors in the design of data storage systems. To achieve higher performance, recently storage system designers use $Dynamic$ $RAM$ (DRAM)-based buffers. The volatility of DRAM brings up the possibility of data loss and data inconsistency. Thus, a part of the main storage is conventionally used as the journal area to be able of recovering unflushed data pages in the case of power failure. Moreover, periodically flushing buffered data pages to the main storage is a common mechanism to preserve a high level of reliability. This scheme, however, leads to a considerable increase in storage write traffic, which adversely affects the performance. To address this shortcoming, recent studies offer a small $Non-Volatile$ $Memory$ (NVM) as the $Persistent$ $Journal$ $Area$ (PJA) along with DRAM as an efficient approach to overcome DRAM vulnerability against power failure while effectively reducing storage write traffic. This approach, named $NVM-Backed$ $Buffer$ (NVB-Buffer), features from advantages of NVMs and addresses DRAM shortcomings. In this article, we employ the most promising technologies for PJA among the emerging technologies, which is $Spin-Transfer$ $Torque$ $Magnetic$ $Random$ $Access$ $Memory$ (STT-MRAM) to meet the requirements of efficient PJA by providing high endurance, non-volatility, and DRAM-like latency. Despite these advantages, STT-MRAM faces major reliability challenges, i.e., Retention Failure , Read Disturbance , and Write Failure , which have not been addressed in previously suggested NVB-Buffers. In this article, we first demonstrate that the retention failure is the dominant source of errors in NVB-Buffers as it suffers from long and unpredictable page idle intervals (i.e., the time interval between two consecutive accesses to a PJA page). Then, we propose a novel NVB-Buffer management scheme, named, $\underline{Co}ld$ $\underline{P}age$ $\underline{A}wakening$ (CoPA), which predictably reduces the idle time of PJA pages. To this aim, CoPA employs $Distant$ $Refreshing$ to periodically overwrite the vulnerable PJA page contents by opportunistically using their replica in DRAM-based buffer. We compare CoPA with the state-of-the-art schemes over several well-known storage workloads based on physical journaling. Our evaluations show that CoPA significantly reduces the maximum page idle time, which leads to three orders of magnitude lower failure rate with negligible performance degradation (1.1\%) and memory overhead (1.2\%).},
  archive      = {J_TPDS},
  author       = {Mostafa Hadizadeh and Elham Cheshmikhani and Maysam Rahmanpour and Onur Mutlu and Hossein Asadi},
  doi          = {10.1109/TPDS.2021.3137315},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2304-2317},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CoPA: Cold page awakening to overcome retention failures in STT-MRAM based I/O buffers},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Privacy-preserving efficient federated-learning model
debugging. <em>TPDS</em>, <em>33</em>(10), 2291‚Äì2303. (<a
href="https://doi.org/10.1109/TPDS.2021.3137321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning allows large amounts of mobile clients to jointly construct a global model without sending their private data to a central server. A fundamental issue in this framework is the susceptibility to the erroneous training data. This problem is especially challenging due to the invisibility of clients‚Äô local training data and training process, as well as the resource constraints. In this paper, we aim to solve this issue by introducing the first FL debugging framework, FLDebugger , for mitigating test error caused by erroneous training data. The proposed solution traces the global model‚Äôs bugs (test errors), jointly through the training log and the underlying learning algorithm, back to first identify the clients and subsequently their training samples that are most responsible for the errors. In addition, we devise an influence-based participant selection strategy to fix bugs as well as to accelerate the convergence of model retraining. The performance of the identification algorithm is evaluated via extensive experiments on a real AIoT system (50 clients, including 20 edge computers, 20 laptops and 10 desktops) and in larger-scale simulated environments. The evaluation results attest to that our framework achieves accurate, privacy-preserving and efficient identification of negatively influential clients and samples, and significantly improves the model performance by fixing bugs.},
  archive      = {J_TPDS},
  author       = {Anran Li and Lan Zhang and Junhao Wang and Feng Han and Xiang-Yang Li},
  doi          = {10.1109/TPDS.2021.3137321},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2291-2303},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Privacy-preserving efficient federated-learning model debugging},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ComboTree: A persistent indexing structure with universal
operational efficiency and scalability. <em>TPDS</em>, <em>33</em>(10),
2277‚Äì2290. (<a href="https://doi.org/10.1109/TPDS.2021.3137247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To leverage the larger-than-DRAM capacity and close-to-DRAM performance of persistent memory (PM) to build future memory systems, more scalable and efficient indexing structures are of paramount importance. However, as we evaluate existing PM indexing structures, we find that (1) both ordered and unordered indexing structures cannot support efficiently all KV operation types, i.e., Put, Get, Delete and Scan, and (2) the majority of indexes scale poorly as the PM capacity and dataset increases, i.e., the decreasing throughput of ordered indexes with the growth of dataset and the blocking of foreground requests by costly hash resizing of unordered indexes. To provide better operational efficiency on both point accesses and range queries for persistent memory in the ever-increasing volume of data, this article proposes ComboTree, a three-tiered indexing structure with a sorted key space. In ComboTree, we break the global B+Tree into multiple low height B+Trees (Tier C) and arrange them with a sorted array (Tier B). Further, we accelerate the lookup of the sorted array by cumulative distribution function (Tier A). Last but not least, a background resizing policy is proposed to avoid performance degradation when the capacity of the ComboTree grows. We implement and evaluate ComboTree on Intel‚Äôs Optane DCPMM. Test results show that ComboTree delivers $2.1\times -3.6\times$ put throughput and $1.5\times -2.1\times$ get throughput of the state-of-art sorted indexes. Furthermore, ComboTree is $1.27\times$ faster than the efficient B+Tree variant in various scan granularities, and it is open-sourced 1 .},
  archive      = {J_TPDS},
  author       = {Zhonghua Wang and Ting Yao and Jiguang Wan and Hong Jiang and Qiu Cui and Liu Tang and Yiwen Zhang and Qiuyang Zhang},
  doi          = {10.1109/TPDS.2021.3137247},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2277-2290},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ComboTree: A persistent indexing structure with universal operational efficiency and scalability},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LOFS: A lightweight online file storage strategy for
effective data deduplication at network edge. <em>TPDS</em>,
<em>33</em>(10), 2263‚Äì2276. (<a
href="https://doi.org/10.1109/TPDS.2021.3133098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing responds to users‚Äô requests with low latency by storing the relevant files at the network edge. Various data deduplication technologies are currently employed at edge to eliminate redundant data chunks for space saving. However, the lookup for the global huge-volume fingerprint indexes imposed by detecting redundancies can significantly degrade the data processing performance. Besides, we envision a novel file storage strategy that realizes the following rationales simultaneously: 1) space efficiency, 2) access efficiency, and 3) load balance, while the existing methods fail to achieve them at one shot. To this end, we report LOFS, a Lightweight Online File Storage strategy, which aims at eliminating redundancies through maximizing the probability of successful data deduplication, while realizing the three design rationales simultaneously. LOFS leverages a lightweight three-layer hash mapping scheme to solve this problem with constant-time complexity. To be specific, LOFS employs the Bloom filter to generate a sketch for each file, and thereafter feeds the sketches to the Locality Sensitivity hash (LSH) such that similar files are likely to be projected nearby in LSH tablespace. At last, LOFS assigns the files to real-world edge servers with the joint consideration of the LSH load distribution and the edge server capacity. Trace-driven experiments show that LOFS closely tracks the global deduplication ratio and generates a relatively low load std compared with the comparison methods.},
  archive      = {J_TPDS},
  author       = {Geyao Cheng and Deke Guo and Lailong Luo and Junxu Xia and Siyuan Gu},
  doi          = {10.1109/TPDS.2021.3133098},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2263-2276},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {LOFS: A lightweight online file storage strategy for effective data deduplication at network edge},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CoFilter: High-performance switch-accelerated stateful
packet filter for bare-metal servers. <em>TPDS</em>, <em>33</em>(9),
2249‚Äì2262. (<a href="https://doi.org/10.1109/TPDS.2021.3136575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most critical cloud services, Bare-Metal Servers (BMS) introduce stringent performance requirements on data center networks (DCN). Stateful packet filter is an integral DCN component of ensuring connection security for BMS. However, the off-the-shelf stateful packet filters either are costly for cloud DCNs or introduce significant performance bottlenecks. In this article, we present CoFilter , which leverages low-cost programmable switches to accelerate the stateful packet filter for BMS. CoFilter uses (1) stateful process partition to enable complex stateful packet filtering logic on programmability-limited switching ASICs, (2) state compression to track tens of millions of connections with constrained hardware memory, and (3) per-tenant packet rate limit and tenant-aware flow migration to achieve efficient performance isolation among different tenants. Overall, CoFilter implements a high-performance stateful packet filter via the co-design of programmable switching ASIC and CPU. We evaluate CoFilter under various data center traffic traces with real-world flow distributions. The evaluation results show that CoFilter remarkably outperforms NetFilter, i.e., forwarding packets at line rate (13x throughput of NetFilter), keeping packet delay within 1us, and freeing a significant quantity of CPU cores, with rather small memory usage, i.e., accommodating over $10^7$ connections with only 16MB SRAM.},
  archive      = {J_TPDS},
  author       = {Jiamin Cao and Ying Liu and Yu Zhou and Lin He and Chen Sun and Yangyang Wang and Mingwei Xu},
  doi          = {10.1109/TPDS.2021.3136575},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2249-2262},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CoFilter: High-performance switch-accelerated stateful packet filter for bare-metal servers},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Flexible performant GEMM kernels on GPUs. <em>TPDS</em>,
<em>33</em>(9), 2230‚Äì2248. (<a
href="https://doi.org/10.1109/TPDS.2021.3136457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {General Matrix Multiplication or GEMM kernels take centre place in high performance computing and machine learning. Recent NVIDIA GPUs include GEMM accelerators, such as NVIDIA‚Äôs Tensor Cores. Their exploitation is hampered by the two-language problem: it requires either low-level programming which implies low programmer productivity or using libraries that only offer a limited set of components. Because rephrasing algorithms in terms of established components often introduces overhead, the libraries‚Äô lack of flexibility limits the freedom to explore new algorithms. Researchers using GEMMs can hence not enjoy programming productivity, high performance, and research flexibility at once. In this paper we solve this problem. We present three sets of abstractions and interfaces to program GEMMs within the scientific Julia programming language. The interfaces and abstractions are co-designed for researchers‚Äô needs and Julia‚Äôs features to achieve sufficient separation of concerns and flexibility to easily extend basic GEMMs in many different ways without paying a performance price. Comparing our GEMMs to state-of-the-art libraries cuBLAS and CUTLASS, we demonstrate that our performance is in the same ballpark of the libraries, and in some cases even exceeds it, without having to write a single line of code in CUDA C++ or assembly, and without facing flexibility limitations.},
  archive      = {J_TPDS},
  author       = {Thomas Faingnaert and Tim Besard and Bjorn De Sutter},
  doi          = {10.1109/TPDS.2021.3136457},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2230-2248},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Flexible performant GEMM kernels on GPUs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NetSHa: In-network acceleration of LSH-based distributed
search. <em>TPDS</em>, <em>33</em>(9), 2213‚Äì2229. (<a
href="https://doi.org/10.1109/TPDS.2021.3135842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locality Sensitive Hashing (LSH) is widely adopted to index similar data in high-dimensional space for approximate nearest neighbor search. Demanding applications (e.g. web search) mean that LSH must exhibit low response times and high throughput. To achieve this, they tend to load balance between multiple machines. However, as the scale of concurrent queries and the volume of data grow, large numbers of index messages are required. Hence, the network is a key bottleneck. To address this gap, we propose NetSHa, which exploits the computational capacity of programmable switches. Specifically, we introduce a heuristic sort-reduce approach to drop potentially poor candidate answers while preserving search quality. Then, NetSHa aggregates good candidate answers from different index messages when transmitting them. Through this, it reduces the network communication cost. Furthermore, we introduce a best-effort replacement mechanism to improve its concurrency. We implement NetSHa on a Barefoot Tofino programmable switch and evaluate it using 7 real-world datasets. The experimental results show that NetSHa reduces the packet volume by $4\sim 10$ times and improves the search efficiency by least 3√ó in comparison with typical LSH-based distributed search frameworks.},
  archive      = {J_TPDS},
  author       = {Penghao Zhang and Heng Pan and Zhenyu Li and Penglai Cui and Ru Jia and Peng He and Zhibin Zhang and Gareth Tyson and Gaogang Xie},
  doi          = {10.1109/TPDS.2021.3135842},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2213-2229},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {NetSHa: In-network acceleration of LSH-based distributed search},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cost-efficient server configuration and placement for mobile
edge computing. <em>TPDS</em>, <em>33</em>(9), 2198‚Äì2212. (<a
href="https://doi.org/10.1109/TPDS.2021.3135955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing resource configuration and site selection of edge servers (ESs) are two critical steps to build up a mobile edge computing (MEC) platform. In this paper, the joint optimization problem of configuration and placement for ES in the MEC environment is investigated. First, we treat each ES as an M/G/m queueing model, and establish mathematical models to characterize the MEC environment, such that the performance and operational expenditures (OPEX) of the system can be calculated analytically. Then, we design a two-stage method and develop a series of algorithms based on bisection algorithm and genetic algorithm (GA) to obtain the optimal configuration scheme and sub-optimal placement scheme (including the deployment quantity) of ESs, with the goal of minimizing OPEX while maintaining system performance at a predetermined level. Finally, we conduct experiments based on a real base station dataset provided by Shanghai Telecom to show the effectiveness of the proposed algorithms. To the best of our knowledge, this work is the first research of the joint optimization problem of configuration and placement for ES in the MEC environment, where the main objective is to increase the cost efficiency.},
  archive      = {J_TPDS},
  author       = {Zhenli He and Kenli Li and Keqin Li},
  doi          = {10.1109/TPDS.2021.3135955},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2198-2212},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cost-efficient server configuration and placement for mobile edge computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-swarm co-evolution based hybrid intelligent
optimization for bi-objective multi-workflow scheduling in the cloud.
<em>TPDS</em>, <em>33</em>(9), 2183‚Äì2197. (<a
href="https://doi.org/10.1109/TPDS.2021.3122428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many scientific applications can be well modelled as large-scale workflows. Cloud computing has become a suitable platform for hosting and executing them. Workflow scheduling has gained much attention in recent years. However, since cloud service providers must offer services for multiple users with various QoS demands, scheduling multiple applications with different QoS requirements is highly challenging. This work proposes a Multi-swarm Co-evolution-based Hybrid Intelligent Optimization (MCHO) algorithm for multiple-workflow scheduling to minimize total makespan and cost while meeting the deadline constraint of each workflow. First, we design a multi-swarm co-evolutionary mechanism where three swarms are adopted to sufficiently search for various elite solutions. Second, to improve global search and convergence performance, we embed local and global guiding information into the updating process of a Particle Swarm Optimizer, and develop a swarm cooperation technique. Third, we propose a Genetic Algorithm-based elite enhancement strategy to exploit more non-dominated individuals, and apply the Metropolis Acceptance rule of Simulated Annealing to update the local guiding solution for each swarm so as to prevent it from being stuck into a local optimum at an early stage. Extensive experimental results demonstrate that MCHO outperforms the state-of-art scheduling algorithms with better distributed non-dominated solutions.},
  archive      = {J_TPDS},
  author       = {Huifang Li and Danjing Wang and MengChu Zhou and Yushun Fan and Yuanqing Xia},
  doi          = {10.1109/TPDS.2021.3122428},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2183-2197},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Multi-swarm co-evolution based hybrid intelligent optimization for bi-objective multi-workflow scheduling in the cloud},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). UFC2: User-friendly collaborative cloud. <em>TPDS</em>,
<em>33</em>(9), 2163‚Äì2182. (<a
href="https://doi.org/10.1109/TPDS.2021.3132496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies how today&#39;s cloud storage services support collaborative file editing. As a tradeoff for transparency and user-friendliness, they do not ask collaborators to use version control systems but instead implement their own heuristics for handling conflicts, which however often lead to unexpected and undesired experiences. With specialized measurements and reverse engineering, we unravel a number of their design and implementation issues as the root causes of poor experiences. Driven by the findings, we propose to reconsider the collaboration support of cloud storage services from a novel perspective of operations without using any locks. To enable this idea, we design intelligent and efficient approaches to the inference and transformation of users‚Äô editing operations, as well as optimizations to the maintenance of files‚Äô historical versions and the update of individual files. We build an open-source system UFC2 (User-Friendly Collaborative Cloud) to embody our design, which can avoid most (98\%) conflicts with little (2\%) overhead.},
  archive      = {J_TPDS},
  author       = {Minghao Zhao and Zhenhua Li and Wei Liu and Jian Chen and Xingyao Li},
  doi          = {10.1109/TPDS.2021.3132496},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2163-2182},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {UFC2: User-friendly collaborative cloud},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performant, multi-objective scheduling of highly interleaved
task graphs on heterogeneous system on chip devices. <em>TPDS</em>,
<em>33</em>(9), 2148‚Äì2162. (<a
href="https://doi.org/10.1109/TPDS.2021.3135876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance-, power-, and energy-aware scheduling techniques play an essential role in optimally utilizing processing elements (PEs) of heterogeneous systems. List schedulers, a class of low-complexity static schedulers, have commonly been used in static execution scenarios. However, list schedulers are not suitable for runtime decision making, particularly when multiple concurrent applications are interleaved dynamically. For such cases, the static task execution times and expectation of idle PEs assumed by list schedulers lead to inefficient system utilization and poor performance. To address this problem, we present techniques for optimizing execution of list scheduling algorithms in dynamic runtime scenarios via a family of algorithms inspired by the well-known heterogeneous earliest finish time (HEFT) list scheduler. Through dynamically arriving, realistic workload scenarios that are simulated in an open-source discrete event heterogeneous SoC simulator, we exhaustively evaluate each of the proposed algorithms across two SoCs modeled after the Xilinx Zynq Ultrascale+ ZCU102 and O-Droid XU3 development boards. Altogether, depending on the chosen variant in this family of algorithms, we are able to achieve an up to 39\% execution time improvement, up to 7.24x algorithmic speedup, or up to 30\% energy consumption improvement compared to the baseline HEFT implementation.},
  archive      = {J_TPDS},
  author       = {Joshua Mack and Samet E. Arda and Umit Y. Ogras and Ali Akoglu},
  doi          = {10.1109/TPDS.2021.3135876},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2148-2162},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Performant, multi-objective scheduling of highly interleaved task graphs on heterogeneous system on chip devices},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting concurrency in sharded parallel state machine
replication. <em>TPDS</em>, <em>33</em>(9), 2133‚Äì2147. (<a
href="https://doi.org/10.1109/TPDS.2021.3135761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State machine replication (SMR) is a well-known approach to implementing fault-tolerant services, providing high availability and strong consistency. In classic SMR, commands are executed sequentially, in the same order by all replicas. To improve performance, two classes of protocols have been proposed to parallelize the execution of commands. Early scheduling protocols reduce scheduling overhead but introduce costly synchronization of worker threads; late scheduling protocols, instead, reduce the cost of thread synchronization but suffer from scheduling overhead. Depending on the characteristics of the workload, one class can outperform the other. We introduce a hybrid scheduling technique that builds on the existing protocols. An experimental evaluation has revealed that the hybrid approach not only inherits the advantages of each technique but also scales better than either one of them, improving the system performance by up to $3\times$ in a workload with conflicting commands.},
  archive      = {J_TPDS},
  author       = {Aldenio Burgos and Eduardo Alchieri and Fernando Dotti and Fernando Pedone},
  doi          = {10.1109/TPDS.2021.3135761},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2133-2147},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Exploiting concurrency in sharded parallel state machine replication},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DePo: Dynamically offload expensive event processing to the
edge of cyber-physical systems. <em>TPDS</em>, <em>33</em>(9),
2120‚Äì2132. (<a href="https://doi.org/10.1109/TPDS.2021.3135441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event processing is one of the cornerstones to manage massive data streams in Cyber-Physical Systems (CPS). Due to CPS applications&#39; increasing complexity, detecting highly complicated events ( aka . ‚Äúexpensive‚Äù events) leads to significant performance degradation, particularly harmful to mission-critical systems. To tackle this challenge, we define a new task - dynamic event processing offloading to CPS-edges. This paper proves the problem NP-hard and proposes a solution - DePo . DePo splits the expensive events into sub-models and offloads them to CPS edges. We design a long and short-term event memory mechanism in DePo that enables the edges and server to process expensive events collaboratively within their capabilities. Besides, we propose a concept called Edge Utility to measure the optimality of offloading schemes. A heuristic algorithm is presented in this study to guide how to dispatch events to edges, thereby helping DePo generate a sub-optimal solution in polynomial computational complexity. Our extensive experiments show that the performance gap between DePo and the optimal benchmark is less than 5\%. DePo effectively reduces more than 40\% redundant states and provides over 100\% higher throughput than state-of-the-art approaches. Experimental results verified the high stability and scalability of DePo , especially when dealing with a large number of expensive events.},
  archive      = {J_TPDS},
  author       = {Meng Ma and Jingbin Zhang and Ping Wang},
  doi          = {10.1109/TPDS.2021.3135441},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2120-2132},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DePo: Dynamically offload expensive event processing to the edge of cyber-physical systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Addressing the read-performance impact of reconfigurations
in replicated key-value stores. <em>TPDS</em>, <em>33</em>(9),
2106‚Äì2119. (<a href="https://doi.org/10.1109/TPDS.2021.3135137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Raw data are often orders of magnitude larger than main memory for many applications. As the performance of storage devices is still significantly slower than main memory, systems still rely on memory caching to improve performance. Data replication schemes are prevalent in data stores for high availability and reliability. In such schemes, while data updates are propagated to all replicas (either synchronously or in the background), reads are usually served by only a subset of replica group members (e.g., as in primary-backup and quorum systems). As a result, non-serving replicas cannot keep their memory cache state updated; thus, during a reconfiguration or a fail-over action, the system suffers from a high read-performance impact for a significant amount of time due to cold-cache misses. In our study we observed up to 70\% hit after a reconfiguration due to cold cache misses, taking almost 18 minutes in some cases to fully restore to the pre-reconfiguration level of performance. In this article we propose a mechanism to maintain up-to-date read caches across replicas by sending read hints to the non-serving replicas to keep their caches warm. Thus the system is able to seamlessly achieve the same performance level even in the face of a replica group reorganization. This is especially important under the read-intensive workloads that are common today. Our evaluation shows that our mechanism has significant benefits during reconfigurations, with low performance impact under periods of resource strain. Given its advisory nature, the maintenance of read hints can be reduced or held off if needed during such periods.},
  archive      = {J_TPDS},
  author       = {Antonis Papaioannou and Kostas Magoutis},
  doi          = {10.1109/TPDS.2021.3135137},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2106-2119},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Addressing the read-performance impact of reconfigurations in replicated key-value stores},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cooperative edge caching based on temporal convolutional
networks. <em>TPDS</em>, <em>33</em>(9), 2093‚Äì2105. (<a
href="https://doi.org/10.1109/TPDS.2021.3135257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of networked multimedia services in the Internet, wireless network traffic has increased dramatically. However, the current mainstream content caching schemes do not take into account the cooperation of different edge servers, resulting in deteriorated system performance. In this paper, we propose a learning-based edge caching scheme to enable mutual cooperation among different edge servers with limited caching resources, thus effectively reducing the content delivery latency. Specifically, we formulate the cooperative content caching problem as an optimization problem, which is proven to be NP-hard. To solve this problem, we design a new learning-based cooperative caching strategy (LECS) that encompasses three key components. Firstly, a temporal convolutional network driven content popularity prediction model is developed to estimate the content popularity with high accuracy. Secondly, with the predicted content popularity, the concept of content caching value (CCV) is introduced to weigh the value of a content cached on a given edge server. Thirdly, an novel dynamic programming algorithm is developed to maximize the overall CCV. Extensive simulation results have demonstrated the superiority of our approach. Compared with the state-of-the-art caching schemes, LECS can improve the cache hit rate by 8.3\%-10.1\%, and reduce the average content delivery delay by 9.1\%-15.1\%.},
  archive      = {J_TPDS},
  author       = {Xu Zhang and Zhengnan Qi and Geyong Min and Wang Miao and Qilin Fan and Zhan Ma},
  doi          = {10.1109/TPDS.2021.3135257},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2093-2105},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cooperative edge caching based on temporal convolutional networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cost-efficient workflow scheduling algorithm for
applications with deadline constraint on heterogeneous clouds.
<em>TPDS</em>, <em>33</em>(9), 2079‚Äì2092. (<a
href="https://doi.org/10.1109/TPDS.2021.3134247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, more and more large-scale data processing and computing workflow applications run on heterogeneous clouds. Such cloud applications with precedence-constrained tasks are usually deadline-constrained and their scheduling is an essential problem faced by cloud providers. Moreover, minimizing the workflow execution cost based on cloud billing periods is also a complex and challenging problem for clouds. In realizing this, we first model the workflow applications as I/O Data-aware Directed Acyclic Graph (DDAG), according to clouds with global storage systems. Then, we mathematically state this deadline-constrained workflow scheduling problem with the goal of minimum execution financial cost. We also prove that the time complexity of this problem is NP-hard by deducing from a multidimensional multiple-choice knapsack problem. Third, we propose a heuristic cost-efficient task scheduling strategy called CETSS, which includes workflow DDAG model building, task subdeadline initialization, greedy workflow scheduling algorithm, and task adjusting method. The greedy workflow scheduling algorithm mainly consists of dynamical task renting billing period sharing method and unscheduled task subdeadline relax technique. We perform rigorous simulations on some synthetic randomly generated applications and real-world applications, such as Epigenomics, CyberShake, and LIGO. The experimental results clearly demonstrate that our proposed heuristic CETSS outperforms the existing algorithms and can effective save the total workflow execution cost. In particular, CETSS is very suitable for large workflow applications.},
  archive      = {J_TPDS},
  author       = {Xiaoyong Tang and Wenbiao Cao and Huiya Tang and Tan Deng and Jing Mei and Yi Liu and Cheng Shi and Meng Xia and Zeng Zeng},
  doi          = {10.1109/TPDS.2021.3134247},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2079-2092},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cost-efficient workflow scheduling algorithm for applications with deadline constraint on heterogeneous clouds},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PPOAccel: A high-throughput acceleration framework for
proximal policy optimization. <em>TPDS</em>, <em>33</em>(9), 2066‚Äì2078.
(<a href="https://doi.org/10.1109/TPDS.2021.3134709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement Learning (RL) is a major branch of AI that enables agents to learn optimal decision making via interaction with the environment. Proximal Policy Optimization (PPO) is the state-of-the-art policy optimization based RL algorithm which achieves superior overall performance on various benchmarks. A PPO agent iteratively optimizes its policy - a function which chooses optimal actions approximated by a DNN, with each iteration consisting of two computationally intensive phases: Sample Generation - where agents inference on its policy and interact with the environment to collect data, and Model Update - where the policy is trained using the collected data. In this paper, we develop the first high-throughput PPO accelerator on CPU-FPGA heterogeneous platform. Our unified systolic-array based design accelerates both the inference and the training of the deep neural network used in a RL algorithm, and is generalizable to various MLP and CNN models across a wide range of RL applications. We develop novel optimizations to simultaneously reduce data access and computation latencies, specifically: (a) optimal data flow mapping to systolic array, (b) novel memory-blocked data layout to enable streaming stall-free data access in both forward and backward propagations, and, (c) a systolic array compute sharing technique to mitigate load imbalance in the training of two networks. We evaluate our design on widely used robotics and gaming benchmarks, achieving 1.4√ó‚Äì26√ó and 1.3√ó‚Äì2.7√ó improvements in throughput, respectively, when compared with state-of-the-art CPU/CPU-GPU implementations.},
  archive      = {J_TPDS},
  author       = {Yuan Meng and Sanmukh Kuppannagari and Rajgopal Kannan and Viktor Prasanna},
  doi          = {10.1109/TPDS.2021.3134709},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2066-2078},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {PPOAccel: A high-throughput acceleration framework for proximal policy optimization},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Critique of ‚ÄúMemXCT: Memory-centric x-ray CT reconstruction
with massive parallelization‚Äù by SCC team from the university of texas
at austin. <em>TPDS</em>, <em>33</em>(9), 2062‚Äì2065. (<a
href="https://doi.org/10.1109/TPDS.2021.3127826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This report describes The University of Texas Student Cluster Competition team‚Äôs effort to reproduce the results of ‚ÄúMemXCT: memory-centric X-ray CT reconstruction with massive parallelization‚Äù (Hidayetoƒülu et al. , 2019). The article details a new memory-centric approach that reconstructs X-ray computed tomography (XCT) from noisy raw data. In our reproduction experiments, we utilized Microsoft Azure‚Äôs CycleCloud tool to provision, orchestrate, and manage our computing cluster in the cloud. In particular, we scheduled and benchmarked reconstruction workloads using Azure‚Äôs CPU-based HC44rs and GPU-based NC12s v2 virtual machine (VM) types to evaluate the scalability properties of the reconstruction approach and the performance differences between architectures. The HC44rs VMs contained 44 Intel Xeon Platinum cores, while the NC12s v2 VM was equipped with two NVIDIA P100 GPUs. We used a recent version of Intel‚Äôs compiler stack with the MKL library for our CPU code along with CUDA 11.1 on GPUs. Overall, our results confirm the findings of the original article, demonstrating similar acceleration on GPUs and scalability properties on CPUs. Digital artifacts from these experiments are available at: 10.5281/zenodo.5598108},
  archive      = {J_TPDS},
  author       = {Brock Davis and Juan Paez and Jack Gaither and Joe A. Garcia},
  doi          = {10.1109/TPDS.2021.3127826},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2062-2065},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of ‚ÄúMemXCT: Memory-centric X-ray CT reconstruction with massive parallelization‚Äù by SCC team from the university of texas at austin},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Critique of ‚ÄùMemXCT: Memory-centric x-ray CT reconstruction
with massive parallelization‚Äù by SCC team from nanyang technological
university. <em>TPDS</em>, <em>33</em>(9), 2058‚Äì2061. (<a
href="https://doi.org/10.1109/TPDS.2021.3128040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this technical report, we focus on reproducing the results reported in the paper &amp;#x201C;MemXCT: Memory-Centric X-ray CT Reconstruction with Massive Parallelization&amp;#x201D; [1]. MemXCT is a scalable approach to X-ray Computed Tomography reconstruction which removes redundant computation. We reproduced the single CPU/GPU performance as well as strong scaling experiments. We set up our configurations on Microsoft Azure CycleCloud and have two clusters. One cluster has 4 nodes with 60 CPUs on each node and the other cluster has 4 nodes with 4 NVIDIA V100 GPUs on each node. Both clusters come with InfiniBand. The original author conducted his experiments on Theta and Blue Waters supercomputers. We were able to reproduce part of the results in the original paper, however, failed to produce similar performance on other experiments. This report was submitted as part of the reproducibility challenge in SC20 Student Cluster Competition. Digital artifacts from these experiments are available at: 10.5281/zenodo.5598108.},
  archive      = {J_TPDS},
  author       = {Shenggui Li and Bu-Sung Lee},
  doi          = {10.1109/TPDS.2021.3128040},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2058-2061},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of &amp;#x201D;MemXCT: Memory-centric X-ray CT reconstruction with massive Parallelization&amp;#x201D; by SCC team from nanyang technological university},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Critique of ‚ÄúMemXCT: Memory-centric x-ray CT reconstruction
with massive parallelization‚Äù by SCC team from clemson university.
<em>TPDS</em>, <em>33</em>(9), 2054‚Äì2057. (<a
href="https://doi.org/10.1109/TPDS.2021.3108961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article reproduces the results of the Supercomputing 2019 article, MemXCT: Memory-Centric X-ray CT Reconstruction with Massive Parallelization by Hidayetoƒülu et al. as part of the Supercomputing 2020 Student Cluster Competition Reproducibility Challenge. We reproduce the single CPU-GPU performance experiments and the strong scaling experiments and compare the results to the original article. Although we are not able to use the exact HPC system from the original article, we use similar hardware and configurations to recreate the original environment in Microsoft Azure Cloud services. We were not able to demonstrate exact performance characteristics from the original article due to hardware limitations in our environment, but we are able to reproduce similar performance trends for both single-node and scaling experiments.},
  archive      = {J_TPDS},
  author       = {Griffin Dube and Cavender Holt and John Hollowell and Sarah Placke and Sansriti Ranjan and Nikolas Heitzig and Jon Calhoun},
  doi          = {10.1109/TPDS.2021.3108961},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2054-2057},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of ‚ÄúMemXCT: Memory-centric X-ray CT reconstruction with massive parallelization‚Äù by SCC team from clemson university},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Critique of ‚ÄúMemXCT: Memory-centric x-ray CT reconstruction
with massive parallelization‚Äù by SCC team from tsinghua university.
<em>TPDS</em>, <em>33</em>(9), 2050‚Äì2053. (<a
href="https://doi.org/10.1109/TPDS.2021.3108964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidayetoƒülu et al. propose a novel memory-centric algorithm to reconstruct X-ray CT images in the SC19 article entitled ‚ÄúMemXCT: Memory-Centric X-ray CT Reconstruction with Massive Parallelization‚Äù. They formulate the reconstruction with several SpMVs, and propose two memory-centric optimizations to improve cache locality for better memory bandwidth utilization, i.e., a two-level pseudo-Hilbert ordering and a multi-stage input buffering. In this article, we present our results on reproducing that article to show its effectiveness and generality, as part of the SC20 Student Cluster Competition Reproducibility Challenge. We reproduce the execution time and memory bandwidth tests in that article on various architectures, including Intel CPUs, AMD CPUs, and NVIDIA GPUs. We further analyze the bottleneck on different architectures by comparing the achieved memory bandwidth with the peak bandwidth on those architectures. We then reproduce the strong scaling test on CPU and GPU clusters with different scales, and use the proposed algorithm to reconstruct three new X-ray computed tomograms.},
  archive      = {J_TPDS},
  author       = {Runxin Zhong and Jiajie Chen and Chen Zhang and Mingshu Zhai and Zeyu Song and Yutian Wang and Wentao Han and Lin Gan and Jidong Zhai},
  doi          = {10.1109/TPDS.2021.3108964},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2050-2053},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of ‚ÄúMemXCT: Memory-centric X-ray CT reconstruction with massive parallelization‚Äù by SCC team from tsinghua university},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reproducibility: Performance evaluation of MemXCT on azure
CycleCloud platform. <em>TPDS</em>, <em>33</em>(9), 2047‚Äì2049. (<a
href="https://doi.org/10.1109/TPDS.2021.3127450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory-Centric X-ray Computational Tomography(CT) is an iterative reconstruction technique that trades compute simplifications with higher memory accesses. MemXCT implements a sparse matrix-vector multiplication(SpMV) with multi-stage buffering and two-level pseudo-Hilbert ordering for optimization. Motivated by the need to validate conclusions from previous work, we reproduce the numerical results, the algorithm‚Äôs performance, and the scaling behavior of the algorithms as the number of MPI processes increases on Azure. Digital artifacts from these experiments are available at: 10.5281/zenodo.5598108},
  archive      = {J_TPDS},
  author       = {Yuchen Liu and Yixuan Meng and Kaiyuan Xu and Zijun Xu and Tianyuan Wu and Yiwei Yang and Shu Yin},
  doi          = {10.1109/TPDS.2021.3127450},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2047-2049},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Reproducibility: Performance evaluation of MemXCT on azure CycleCloud platform},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Critique of ‚ÄúMemXCT: Memory-centric x-ray CT reconstruction
with massive parallelization‚Äù by SCC team from university of california
san diego. <em>TPDS</em>, <em>33</em>(9), 2043‚Äì2046. (<a
href="https://doi.org/10.1109/TPDS.2021.3128840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we describe our efforts to reproduce results reported in the SC19 article by Hidayetoƒülu et al. , titled ‚ÄúMemXCT: Memory-Centric X-ray CT Reconstruction with Massive Parallelization‚Äù . MemXCT &#39;s single-device performance, parallelized via OpenMP and MPI, was characterized using AMD Zen2 CPU cores and NVIDIA V100 GPU devices running on the Microsoft Azure cloud. We were able to reproduce most of the results, and exceed the performance of larger inputs, on an AMD EPYC HBv2 cluster. We were also able to reproduce the strong scaling trends for optimized CPU and GPU versions. Slight variations in performance of the CPU version were observed due to differences in the underlying hardware, input size, and number of available nodes. Digital artifacts from these experiments are available at: 10.5281/zenodo.5598108},
  archive      = {J_TPDS},
  author       = {Xiaochen Li and Maximilian Apodaca and Arunav Gupta and Zihao Kong and Hongyi Pan and Hongyu Zhou and Mary Thomas and Martin Kandes and Zhaoyi Li and Mahidhar Tatineni and Lewis Carroll},
  doi          = {10.1109/TPDS.2021.3128840},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2043-2046},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of ‚ÄúMemXCT: Memory-centric X-ray CT reconstruction with massive parallelization‚Äù by SCC team from university of california san diego},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Critique of ‚ÄúMemXCT: Memory-centric x-ray CT reconstruction
with massive parallelization‚Äù by SCC team from ETH z√ºrich.
<em>TPDS</em>, <em>33</em>(9), 2039‚Äì2042. (<a
href="https://doi.org/10.1109/TPDS.2021.3127711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This report analyzes the reproducibility of the paper ‚ÄúMemXCT: Memory-Centric X-ray CT Reconstruction with Massive Parallelization‚Äù by Hidayetoƒülu et al. in the cloud as part of the SC20 Virtual Student Cluster Competition (VSCC). To reproduce the results from the original work, the ETH Z√ºrich SC20 VSCC team performed a series of CT reconstructions and performed a scaling study using three provided sinograms. All experimental runs were performed during the SC20 VSCC on an HPC cluster hosted in the Microsoft Azure CycleCloud. In this paper, we describe discrepancies in results as a factor of the differences in experiment environments and insufficient parameter tuning. We successfully reproduce the single device performance and partially reproduce the strong scaling behavior. Digital artifacts from these experiments are available at: 10.5281/zenodo.5598108.},
  archive      = {J_TPDS},
  author       = {Jan Kleine and Rahul Steiger and Simon Wachter and Emir ƒ∞≈üman and Simon Jacob and Dario Romaniello},
  doi          = {10.1109/TPDS.2021.3127711},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2039-2042},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of ‚ÄúMemXCT: Memory-centric X-ray CT reconstruction with massive parallelization‚Äù by SCC team from ETH z√ºrich},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Critique of ‚ÄúMemXCT: Memory-centric x-ray CT reconstruction
with massive parallelization‚Äù by SCC team from georgia tech.
<em>TPDS</em>, <em>33</em>(9), 2035‚Äì2038. (<a
href="https://doi.org/10.1109/TPDS.2021.3108956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the 2020 Student Cluster Competition, we reproduced results from ‚ÄúMemXCT: Memory-Centric X-ray CT Reconstruction with Massive Parallelization‚Äù (Hidayetoƒülu et al. ). Reproducibility is of critical importance to the scientific community, not just to verify correctness of results but also to see how easily others can understand and work with the given methods. MemXCT is an approach for image reconstruction in X-ray ptychography, which has a broad range of applications in materials science. MemXCT is not the only X-ray tomography algorithm, though; as opposed to compute-centric algorithms, it is designed to scale better by optimizing for memory bandwidth and memory latency. MemXCT also applies several key optimizations in order to ease memory pressure. In this article, we test the performance and strong scaling of MemXCT on 1 to 256 AMD CPU cores (1-4 nodes) and 1-16 Nvidia V100 GPUs (1-4 nodes). We confirm the impact of MemXCT‚Äôs optimizations. Still, we find that the performance of some important loops in the MemXCT kernel is much lower on the AMD processors (with AVX2) of our CPU nodes compared to the Intel CPUs (with AVX-512) used in the original article. We also confirm MemXCT performance on Tesla V100 GPUs, as reported in the article.},
  archive      = {J_TPDS},
  author       = {Nicole Prindle and Ali Kazmi and Aman Jain and Albert Chen and Marissa Sorkin and Sudhanshu Agarwal and Richard Vuduc and Vijay Thakkar},
  doi          = {10.1109/TPDS.2021.3108956},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2035-2038},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of ‚ÄúMemXCT: Memory-centric X-ray CT reconstruction with massive parallelization‚Äù by SCC team from georgia tech},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Critique of ‚ÄúMemXCT: Memory-centric x-ray CT reconstruction
with massive parallelization‚Äù by SCC team from peking university.
<em>TPDS</em>, <em>33</em>(9), 2032‚Äì2034. (<a
href="https://doi.org/10.1109/TPDS.2021.3092273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidayetolu et al. (2019) proposed a novel memory-centric computation system, MemXCT. As a challenge at SC20, we reproduce the computational efficiency of MemXCT on our Azure cloud cluster. Our experiments evaluate the overall performance and the strong scalability with real datasets and verify part of the conclusions in the original article.},
  archive      = {J_TPDS},
  author       = {Zejia Fan and Yuchen Gu and Zhewen Hao and Yueyang Pan and Pengcheng Xu and Yuxuan Yan and Fangyuan Yang and Zhenxin Fu and Yun Liang},
  doi          = {10.1109/TPDS.2021.3092273},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2032-2034},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of ‚ÄúMemXCT: Memory-centric X-ray CT reconstruction with massive parallelization‚Äù by SCC team from peking university},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MemXCT: Design, optimization, scaling, and reproducibility
of x-ray tomography imaging. <em>TPDS</em>, <em>33</em>(9), 2014‚Äì2031.
(<a href="https://doi.org/10.1109/TPDS.2021.3128032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work extends our previous research entitled ‚ÄúMemXCT: Memory-centric X-ray CT Reconstruction with Massive Parallelization‚Äù that was originally published at SC19 conference (Hidayetoƒülu et al. , 2019) with reproducibility of the computational imaging performance. X-ray computed tomography (XCT) is regularly used at synchrotron light sources to study the internal morphology of materials at high resolution. However, experimental constraints, such as radiation sensitivity, can result in noisy or undersampled measurements. Further, depending on the resolution, sample size and data acquisition rates, the resulting noisy dataset can be in the order of terabytes. Advanced iterative reconstruction techniques can produce high-quality images from noisy measurements, but their computational requirements have made their use an exception rather than the rule. We propose a novel memory-centric approach that avoids redundant computations at the expense of additional memory complexity. We develop a memory-centric iterative reconstruction system, MemXCT, that uses an optimized SpMV implementation with two-level pseudo-Hilbert ordering and multi-stage input buffering. We evaluate MemXCT on various supercomputer architectures involving KNL and GPU. MemXCT can reconstruct a large (11K√ó11K) mouse brain tomogram in 10 seconds using 4096 KNL nodes (256K cores). The results presented in our original article at the SC19 were based on large-scale supercomputing resources. The MemXCT application was selected for the Student Cluster Competition (SCC) Reproducibility Challenge and evaluated on a variety of cloud computing resources by universities around the world in the SC20 conference. We summarize the results of the top-ranked SCC Reproducibility Challenge teams and identify the most pertinent measures for ensuring the reproducibility of our experiments in this article.},
  archive      = {J_TPDS},
  author       = {Mert Hidayetoƒülu and Tekin Bi√ßer and Simon Garcia de Gonzalo and Bin Ren and Doƒüa G√ºrsoy and Rajkumar Kettimuthu and Ian T. Foster and Wen-Mei W. Hwu},
  doi          = {10.1109/TPDS.2021.3128032},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2014-2031},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {MemXCT: Design, optimization, scaling, and reproducibility of X-ray tomography imaging},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Advancing adoption of reproducibility in HPC: A preface to
the special section. <em>TPDS</em>, <em>33</em>(9), 2011‚Äì2013. (<a
href="https://doi.org/10.1109/TPDS.2021.3128796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this special section we bring you a practice and experience effort in reproducibility for large-scale computational science at SC20. This section includes nine critiques, each by a student team that reproduced results from a paper published at SC19, during the following year‚Äôs Student Cluster Competition. The paper is also included in this section and has been expanded upon, now including an analysis of the outcomes of the students‚Äô reproducibility experiments. Lastly, this special section encapsulates a variety of advances in reproducibility in the SC conference series technical program.},
  archive      = {J_TPDS},
  author       = {Stephen Lien Harrell and Scott Michael and Carlos Maltzahn},
  doi          = {10.1109/TPDS.2021.3128796},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2011-2013},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Advancing adoption of reproducibility in HPC: A preface to the special section},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EiC editorial ‚Äì advancing reproducibility in parallel and
distributed systems research. <em>TPDS</em>, <em>33</em>(9), 2010. (<a
href="https://doi.org/10.1109/TPDS.2021.3137871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {, the TPDS Reproducibility Initiative [2][3] has being exploring postpublication peer review of code associated with published articles for a few years. Authors who have published in TPDS can make their published article more reproducible and earn a reproducibility badge by submitting their associated code for post-publication peer review. To date, this pilot has largely focused on two badges: 1. Code Available: The code, including any associated data and documentation, provided by the authors is reasonable and complete and can potentially be used to support reproducibility of the published results. 2. Code Reviewed: The code, including any associated data and documentation, provided by the authors is reasonable and complete, runs to produce the outputs described, and can support reproducibility of the published results. While TPDS‚Äô goal has always been to include badges for reproducing research results using the code and/or data provided, the nature of research in parallel and distributed systems covered by TPDS makes it challenging to evaluate code and data challenging for reproducibility. This is because such an evaluation may require access to specific hardware, system architectures and scales, OS configurations, and so on, which may not be feasible or practical. vel system/middleware services, is typically infeasible. Consequently, TPDS has piloted an alternate approach where members of the community can submit short, supplemental ‚Äòcritique‚Äô papers that present their experiences in reproducing published results using the artifacts, and/or evaluations or experiences with published artifacts. These supplemental paper submissions are reviewed and, if accepted, are linked to the original publication and are citable, serving to help validate the reproducibility of the original publication. This approach was first implemented in a special section guest edited by Stephen Lien Harrell and Beth Plale and consisting of a primary paper and 6 critique papers that reproduce the results of the primary paper. This special section continues this effort, building on the SC20 Student Cluster Competition, which was part of the SC20 conference. It consists of 9 critique papers that reproduce the results of the primary paper.},
  archive      = {J_TPDS},
  author       = {Manish Parashar},
  doi          = {10.1109/TPDS.2021.3137871},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2010},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {EiC editorial ‚Äì advancing reproducibility in parallel and distributed systems research},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AUCTION: Automated and quality-aware client selection
framework for efficient federated learning. <em>TPDS</em>,
<em>33</em>(8), 1996‚Äì2009. (<a
href="https://doi.org/10.1109/TPDS.2021.3134647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergency of federated learning (FL) enables distributed data owners to collaboratively build a global model without sharing their raw data, which creates a new business chance for building data market. However, in practical FL scenarios, the hardware conditions and data resources of the participant clients can vary significantly, leading to different positive/negative effects on the FL performance, where the client selection problem becomes crucial. To this end, we propose AUCTION , an A utomated and q U ality-aware C lient selec TION framework for efficient FL, which can evaluate the learning quality of clients and select them automatically with quality-awareness for a given FL task within a limited budget. To design AUCTION , multiple factors such as data size, data quality, and learning budget that can affect the learning performance should be properly balanced. It is nontrivial since their impacts on the FL model are intricate and unquantifiable. Therefore, AUCTION is designed to encode the client selection policy into a neural network and employ reinforcement learning to automatically learn client selection policies based on the observed client status and feedback rewards quantified by the federated learning performance. In particular, the policy network is built upon an encoder-decoder deep neural network with an attention mechanism, which can adapt to dynamic changes of the number of candidate clients and make sequential client selection actions to reduce the learning space significantly. Extensive experiments are carried out based on real-world datasets and well-known learning models to demonstrate the efficiency, robustness, and scalability of AUCTION .},
  archive      = {J_TPDS},
  author       = {Yongheng Deng and Feng Lyu and Ju Ren and Huaqing Wu and Yuezhi Zhou and Yaoxue Zhang and Xuemin Shen},
  doi          = {10.1109/TPDS.2021.3134647},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1996-2009},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {AUCTION: Automated and quality-aware client selection framework for efficient federated learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cost-effective web application replication and deployment in
multi-cloud environment. <em>TPDS</em>, <em>33</em>(8), 1982‚Äì1995. (<a
href="https://doi.org/10.1109/TPDS.2021.3133884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-cloud is becoming a popular cloud ecosystem because it allows enterprise users to share the workload across multiple cloud service providers to achieve high-quality services with lower operation cost and higher application resilience. In multi-cloud, cloud services are widely distributed at different locations with differentiated prices. Therefore, Web application providers face the challenge to select proper cloud services for application replication and deployment with the aim of minimizing the deployment cost. Meanwhile, the deployed application replicas must satisfy the constraint on request response time to maintain the quality of user experience. To meet the two major requirements, this article studies a new problem of Web application replication and deployment in multi-cloud (WARDMC) that jointly considers both the cost minimization and constraints on average response time, including particularly request processing time and network latency. To address the problem, we develop a new approach named MCApp. MCApp combines iterative mixed integer linear programming with domain-tailored large neighborhood search to optimize both application replicas deployment and user requests dispatching. Extensive experiments using the real-world datasets demonstrate that MCApp significantly outperforms several recently proposed approaches.},
  archive      = {J_TPDS},
  author       = {Tao Shi and Hui Ma and Gang Chen and Sven Hartmann},
  doi          = {10.1109/TPDS.2021.3133884},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1982-1995},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cost-effective web application replication and deployment in multi-cloud environment},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TensorOpt: Exploring the tradeoffs in distributed DNN
training with auto-parallelism. <em>TPDS</em>, <em>33</em>(8),
1967‚Äì1981. (<a href="https://doi.org/10.1109/TPDS.2021.3132413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective parallelization strategies are crucial for the performance of distributed deep neural network (DNN) training. Recently, several methods have been proposed to search parallelization strategies but they all optimize a single objective (e.g., execution time, memory consumption) and produce only one strategy. We propose Frontier Tracking (FT), an efficient algorithm that finds a set of Pareto-optimal parallelization strategies to explore the best trade-off among different objectives. FT can minimize the memory consumption when the number of devices is limited and fully utilize additional resources to reduce the execution time. Based on FT , we develop a user-friendly system, called TensorOpt , which allows users to run their distributed DNN training jobs without caring the details about searching and coding parallelization strategies. Experimental results show that TensorOpt is more flexible in adapting to resource availability compared with existing frameworks.},
  archive      = {J_TPDS},
  author       = {Zhenkun Cai and Xiao Yan and Kaihao Ma and Yidi Wu and Yuzhen Huang and James Cheng and Teng Su and Fan Yu},
  doi          = {10.1109/TPDS.2021.3132413},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1967-1981},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {TensorOpt: Exploring the tradeoffs in distributed DNN training with auto-parallelism},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TridentKV: A read-optimized LSM-tree based KV store via
adaptive indexing and space-efficient partitioning. <em>TPDS</em>,
<em>33</em>(8), 1953‚Äì1966. (<a
href="https://doi.org/10.1109/TPDS.2021.3118599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LSM-tree based key-value (KV) stores suffer severe read performance loss due to the leveled structure of the LSM-tree. Especially, when modern storage devices with high bandwidth and low latency are used, the read performance of KV store is seriously affected by inefficient file indexing. Besides, due to the deletion pattern of inserting tombstones, the KV stores based on LSM-tree are faced with the problem of read performance fluctuations that are caused by large-scale data deletion (also referred to as the Read-After-Delete problem). In this article, TridentKV is proposed to improve the read performance of KV stores. An adaptive learned index structure is first designed to speed up file indexing. Also, a space-efficient partition strategy is proposed to solve the Read-After-Delete problem. Besides, asynchronous reading design is adopted, and SPDK is supported for high concurrency and low latency. TridentKV is implemented on RocksDB and the evaluation results indicate that compared with RocksDB, the read performance of TridentKV is improved by 7√ó to 12√ó without loss of write performance and TridentKV provides stable read performance even if a large number of deletions or migrations occur. Instead of RocksDB, TridentKV is exploited to store metadata in Ceph, which improves the read performance of Ceph by 20\% $\sim$ 60\%.},
  archive      = {J_TPDS},
  author       = {Kai Lu and Nannan Zhao and Jiguang Wan and Changhong Fei and Wei Zhao and Tongliang Deng},
  doi          = {10.1109/TPDS.2021.3118599},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1953-1966},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {TridentKV: A read-optimized LSM-tree based KV store via adaptive indexing and space-efficient partitioning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Completely independent spanning trees on BCCC data center
networks with an application to fault-tolerant routing. <em>TPDS</em>,
<em>33</em>(8), 1939‚Äì1952. (<a
href="https://doi.org/10.1109/TPDS.2021.3133595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A set of $k$ spanning trees in a graph $G$ are called completely independent spanning trees (CISTs for short) if the paths joining every pair of vertices $x$ and $y$ in any two trees have neither vertex nor edge in common, except for $x$ and $y$ . The existence of multiple CISTs in the underlying graph of a network has applications in fault-tolerant broadcasting and secure message distribution. In this paper, we investigate the construction of CISTs in a server-centric data center network called BCube connected crossbars (BCCC), which can provide good network performance using inexpensive commodity off-the-shelf switches and commodity servers with only two network interface card (NIC) ports. The significant advantages of BCCC are its good expandability, lower communication latency, and higher robustness in component failure. Based on the structure of compound graphs of BCCC, we provide efficient algorithms to construct $\lceil \frac{n}{4}\rceil$ CISTs in the logical graph of BCCC, denoted by $L$ - $BCCC(n,k)$ , for $n\geqslant 5$ . As a by-product, we obtain a fault-tolerant routing that takes the constructed CISTs as its routing table. We then evaluate the performance of the fault-tolerant routing through simulation results.},
  archive      = {J_TPDS},
  author       = {Xiao-Yan Li and Wanling Lin and Ximeng Liu and Cheng-Kuan Lin and Kung-Jui Pai and Jou-Ming Chang},
  doi          = {10.1109/TPDS.2021.3133595},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1939-1952},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Completely independent spanning trees on BCCC data center networks with an application to fault-tolerant routing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A GPU-oriented application programming interface for digital
audio workstations. <em>TPDS</em>, <em>33</em>(8), 1924‚Äì1938. (<a
href="https://doi.org/10.1109/TPDS.2021.3131659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Digital Audio Workstation (DAW) is a hardware and/or software device aiming to ease those operations required for music production, such as arranging, recording, editing, mixing, and, more in general, modifying sounds creatively. A peculiarity of a DAW environment is that most of the work is highly parallelizable, since the basic architecture of a DAW consists in the simultaneous processing of different audio tracks, mainly independent from each other. In order to exploit such a feature, this paper proposes an interface that lets the DAW interact with the Graphics Processing Unit (GPU) in a standardized way. Despite some academic research and experimentation, the professional audio software industry almost never exploited GPUs when implementing entire DAWs, but only when realising very specific tools or third party extensions (plugins). This work also presents and discusses the outcomes of a number of tests conducted in order to choose the optimal architecture. As a result, a GPU-based approach turned to be a valid alternative to the use of CPUs in the computation of audio effects, such as the rendering of audio tracks after mixing and mastering operations, both in real time and offline.},
  archive      = {J_TPDS},
  author       = {Daniele Bianchi and Federico Avanzini and Adriano Barat√® and Luca A. Ludovico and Giorgio Presti},
  doi          = {10.1109/TPDS.2021.3131659},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1924-1938},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A GPU-oriented application programming interface for digital audio workstations},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive and efficient resource allocation in cloud
datacenters using actor-critic deep reinforcement learning.
<em>TPDS</em>, <em>33</em>(8), 1911‚Äì1923. (<a
href="https://doi.org/10.1109/TPDS.2021.3132422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ever-expanding scale of cloud datacenters necessitates automated resource provisioning to best meet the requirements of low latency and high energy-efficiency. However, due to the dynamic system states and various user demands, efficient resource allocation in cloud faces huge challenges. Most of the existing solutions for cloud resource allocation cannot effectively handle the dynamic cloud environments because they depend on the prior knowledge of a cloud system, which may lead to excessive energy consumption and degraded Quality-of-Service (QoS). To address this problem, we propose an adaptive and efficient cloud resource allocation scheme based on Actor-Critic Deep Reinforcement Learning (DRL). First, the actor parameterizes the policy (allocating resources) and chooses actions (scheduling jobs) based on the scores assessed by the critic (evaluating actions). Next, the resource allocation policy is updated by using gradient ascent while the variance of policy gradient is reduced with an advantage function, which improves the training efficiency of the proposed method. We conduct extensive simulation experiments using real-world data from Google cloud datacenters. The results show that our method can obtain the superior QoS in terms of latency and job dismissing rate with enhanced energy-efficiency, compared to two advanced DRL-based and five classic cloud resource allocation methods.},
  archive      = {J_TPDS},
  author       = {Zheyi Chen and Jia Hu and Geyong Min and Chunbo Luo and Tarek El-Ghazawi},
  doi          = {10.1109/TPDS.2021.3132422},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1911-1923},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Adaptive and efficient resource allocation in cloud datacenters using actor-critic deep reinforcement learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Construction of dual-CISTs on an infinite class of networks.
<em>TPDS</em>, <em>33</em>(8), 1902‚Äì1910. (<a
href="https://doi.org/10.1109/TPDS.2021.3132412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main method to achieve fault-tolerant network systems is by exploiting and effectively utilizing the edge-disjoint and/or inner-vertex-disjoint paths between pairs of source and destination vertices. Completely independent spanning trees (CISTs for short) are powerful tools for reliable broadcasting/unicasting and secure message distribution. Particularly, it has been shown that two CISTs have an application on configuring a protection routing in IP networks, such as mobile ad hoc networks and relatively large (static) network topologies with scalability in [IEEE/ACM Trans. Netw., 27 (2019) 1112-1123]. Many results focus on CISTs in specific networks in the literature, however, few results are given on an infinite class of networks having common properties. In this article, we prove the existence of dual-CISTs in an infinite number of networks satisfying some Hamilton sufficient conditions. A unique algorithm to construct a CIST-partition is proposed, which can be applied to not only many kinds of networks, but our algorithm can also be implemented very easily in parallel or distributed systems satisfying the conditions. In addition, we make a comparative analysis between the proposed conditions and several known results on an infinite number of networks, the advantage of our result is significant. In particular, the bound in our conditions is sharp. The results will provide a powerful framework for the design of fault-tolerant network topologies and routing protocols for future networks.},
  archive      = {J_TPDS},
  author       = {Xiao-Wen Qin and Rong-Xia Hao and Jie Wu},
  doi          = {10.1109/TPDS.2021.3132412},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1902-1910},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Construction of dual-CISTs on an infinite class of networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scaling poisson solvers on many cores via MMEwald.
<em>TPDS</em>, <em>33</em>(8), 1888‚Äì1901. (<a
href="https://doi.org/10.1109/TPDS.2021.3127138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Poisson solver for the calculation of the electrostatic potential is an essential primitive in quantum mechanics calculations. In this article, we adopt the Ewald method and propose a highly-optimized and scalable framework for Poisson solver, MMEwald, on the new generation Sunway supercomputer, capable of utilizing the collection of 390-core accelerators it uses. The MMEwald is based on a grid adapted cut-plane approach to partition the points into batches and distribute the batch to the processors. Furthermore, we propose a set of architecture-specific optimizations to efficiently utilize the memory bandwidth and computation capacity of the supercomputer. Experimental results demonstrate the efficiency of the MMEwald in providing strong and weak scaling performance.},
  archive      = {J_TPDS},
  author       = {Mingchuan Wu and Yangjun Wu and Honghui Shang and Ying Liu and Huimin Cui and Fang Li and Xiaohui Duan and Yunquan Zhang and Xiaobing Feng},
  doi          = {10.1109/TPDS.2021.3127138},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1888-1901},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Scaling poisson solvers on many cores via MMEwald},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CSEdge: Enabling collaborative edge storage for multi-access
edge computing based on blockchain. <em>TPDS</em>, <em>33</em>(8),
1873‚Äì1887. (<a href="https://doi.org/10.1109/TPDS.2021.3131680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access Edge Computing (MEC), as an extension of cloud computing, provides storage resources at the network edge to enable low-latency data retrieval for users. Due to limited physical sizes and constrained storage resources, individual edge servers cannot store a large amount of data when operating independently. They often need to offload data to other edge servers to serve users collaboratively. Operated by different edge infrastructure providers, edge servers usually work in a distrusted environment. Incentive and trust are the two main challenges in facilitating collaborative edge storage. This article proposes CSEdge, a novel decentralized system that tackles these challenges to enable collaborative edge storage based on blockchain. On CSEdge, edge servers can submit data offloading requests for others to contend for. Winners are selected based on their reputations. They will store the offloaded data and receive rewards for successfully finishing data offloading tasks. Via a distributed consensus, their performance will be recorded on blockchain for future reputation evaluation. A prototype of CSEdge is built on Hyperledger Sawtooth and experimentally evaluated against a baseline system and two start-of-the-art systems in a simulated MEC environment. The results demonstrate that CSEdge can effectively and efficiently facilitate collaborative edge storage among edge servers.},
  archive      = {J_TPDS},
  author       = {Liang Yuan and Qiang He and Feifei Chen and Jun Zhang and Lianyong Qi and Xiaolong Xu and Yang Xiang and Yun Yang},
  doi          = {10.1109/TPDS.2021.3131680},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1873-1887},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CSEdge: Enabling collaborative edge storage for multi-access edge computing based on blockchain},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluating data redistribution in PaRSEC. <em>TPDS</em>,
<em>33</em>(8), 1856‚Äì1872. (<a
href="https://doi.org/10.1109/TPDS.2021.3131657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data redistribution aims to reshuffle data to optimize some objective for an algorithm. The objective can be multi-dimensional, such as improving computational load balance or decreasing communication volume or cost, with the ultimate goal of increasing the efficiency and therefore reducing the time-to-solution for the algorithm. The classic redistribution problem focuses on optimally scheduling communications when reshuffling data between two regular, usually block-cyclic, data distributions. Besides distribution, data size is also a performance-critical parameter because it affects the reshuffling algorithm in terms of cache, communication efficiency, and potential parallelism. In addition, task-based runtime systems have gained popularity recently as a potential candidate to address the programming complexity on the way to exascale. In this scenario, it becomes paramount to develop a flexible redistribution algorithm for task-based runtime systems, which could support all types of regular and irregular data distributions and take data size into account. In this article, we detail a flexible redistribution algorithm and implement an efficient approach in a task-based runtime system, PaRSEC . Performance results show great capability compared to the theoretical bound and ScaLAPACK , and applications highlight an increased efficiency with little overhead in terms of data distribution, data size, and data format.},
  archive      = {J_TPDS},
  author       = {Qinglei Cao and George Bosilca and Nuria Losada and Wei Wu and Dong Zhong and Jack Dongarra},
  doi          = {10.1109/TPDS.2021.3131657},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1856-1872},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Evaluating data redistribution in PaRSEC},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online learning for distributed computation offloading in
wireless powered mobile edge computing networks. <em>TPDS</em>,
<em>33</em>(8), 1841‚Äì1855. (<a
href="https://doi.org/10.1109/TPDS.2021.3129618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel paradigm named Wireless Powered Mobile Edge Computing (WP-MEC) emerges recently, which integrates Mobile Edge Computing (MEC) and Wireless Power Transfer (WPT) technologies. It enables mobile clients to both extend their computing capacities by task offloading, and charge from edge servers via energy transmission. Existing studies generally focus on the centralized design of task scheduling and energy charging in WP-MEC networks. To meet the decentralization requirement of the near-coming 6G network, we propose an online learning algorithm for computation offloading in WP-MEC networks with a distributed execution manner. Specifically, we first define the delay minimization problem by considering task deadline and energy constraints. Then, we transform it into a primal-dual optimization problem based on the Bellman equation. After that, we design a novel neural model that learns both offloading and time division decisions in each time slot to solve the formulated optimization problem. To train and execute the designed algorithm distributivity, we form multiple learning models decentralized on edge servers and they work coordinately to achieve parameter synchronization. At last, both theoretical and performance analyses show that the designed algorithm has significant advantages in comparison with other representative schemes.},
  archive      = {J_TPDS},
  author       = {Xiaojie Wang and Zhaolong Ning and Lei Guo and Song Guo and Xinbo Gao and Guoyin Wang},
  doi          = {10.1109/TPDS.2021.3129618},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1841-1855},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Online learning for distributed computation offloading in wireless powered mobile edge computing networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive resource efficient microservice deployment in
cloud-edge continuum. <em>TPDS</em>, <em>33</em>(8), 1825‚Äì1840. (<a
href="https://doi.org/10.1109/TPDS.2021.3128037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User-facing services are now evolving towards the microservice architecture where a service is built by connecting multiple microservice stages. Since the entire service is heavy, the microservice architecture shows the opportunity to only offload some microservice stages to the edge devices that are close to the end users. However, emerging techniques often result in the violation of Quality-of-Service (QoS) of microservice-based services in cloud-edge continuum, as they do not consider the communication overhead or the resource contention between microservices and external co-located tasks. We propose Nautilus, a runtime system that effectively deploys microservice-based user-facing services in cloud-edge continuum. Nautilus ensures the QoS of microservice-based user-facing services while minimizing the required computational resources, which is comprised of a communication-aware microservice mapper, a contention-aware resource manager and an IO-sensitive and load-aware microservice migration scheduler. The mapper divides the microservice graph into multiple partitions based on the communication overhead and maps the partitions to appropriate nodes. On each node, the resource manager determines the optimal resource allocation for its microservices based on reinforcement learning that may capture the complex contention behaviors. Once the microservices are suffered from external IO pressure, the IO-sensitive microservice scheduler migrates the critical one to idle nodes. Furthermore, when the load of microservices changes dynamically, the load-aware microservice scheduler migrates microservices from busy nodes to idle ones to ensure the QoS goal of the entire service. Our experimental results show that Nautilus can guarantee the required QoS target under external shared resources contention while the state-of-the-art suffers from QoS violations. Meanwhile, Nautilus reduces the computational resource usage by 23.9\% and the network bandwidth usage by 53.4\%, while achieving the required 99\%-ile latency.},
  archive      = {J_TPDS},
  author       = {Kaihua Fu and Wei Zhang and Quan Chen and Deze Zeng and Minyi Guo},
  doi          = {10.1109/TPDS.2021.3128037},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1825-1840},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Adaptive resource efficient microservice deployment in cloud-edge continuum},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient and automated deployment architecture for
OpenStack in TianHe SuperComputing environment. <em>TPDS</em>,
<em>33</em>(8), 1811‚Äì1824. (<a
href="https://doi.org/10.1109/TPDS.2021.3127128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, with the large-scale outbreak of the global financial crisis and public safety incidents (such as COVID-19), high-performance computing has been widely applied to risk prediction, vaccine development, and other fields. In scenarios where high-performance computing infrastructure responds to the instantaneous explosion of computing demands, a crucial issue is to provide large-scale flexible allocation and adjustment of computing capability by rapidly constructing computing clusters. Existing large-scale computing cluster deployment solutions usually utilize source code deployment or other deployment tools. The great challenge of existing deployment methods is to reduce excessive image distribution time and refrain from configuration defects. In this article, we design an intelligent distributed registry deployment (IDRD) architecture based on the OpenStack cloud platform, which adaptively places distributed image repositories using the containerized deployment of multiple registries. We propose a server load priority algorithm to solve multiple registries placement problems in IDRD. Furthermore, we devise a clustering algorithm based on demand density that can optimize the global performance of IDRD and improve large-scale cluster load balancing capabilities, which has been implemented in the TianHe Supercomputing environment. Extensive experimental results demonstrate that IDRD can effectively reduce $30\%$ - $50\%$ of the distribution time of component images and significantly improve the efficiency of large-scale cluster deployment.},
  archive      = {J_TPDS},
  author       = {Bingting Jiang and Zhuo Tang and Xiong Xiao and Jing Yao and Ronghui Cao and Kenli Li},
  doi          = {10.1109/TPDS.2021.3127128},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1811-1824},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient and automated deployment architecture for OpenStack in TianHe SuperComputing environment},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LoomIO: Object-level coordination in distributed file
systems. <em>TPDS</em>, <em>33</em>(8), 1799‚Äì1810. (<a
href="https://doi.org/10.1109/TPDS.2021.3126260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Device-level interference is recognized as a major cause of the performance degradation in distributed file systems. Although the approaches of mitigating interference through coordination at application-level, middleware-level, and server-level have shown beneficial results in previous studies, we find their effectiveness is largely reduced since I/O requests are re-arranged by underlying object file systems. In this research study, we prove that object-level coordination is critical and often the key to address the interference issue, as the scheduling of object requests determines the device-level accesses and thus determines the actual I/O bandwidth and latency. This article proposes an object-level coordination system, LoomIO, which uses an OBOP (One-Broadcast-One-Propagate) method and a time-limited coordination process to deliver highly efficient coordination service. Specifically, LoomIO enables object requests to achieve an optimized scheduling decision within a few milliseconds and largely mitigates the device-level interference. We have implemented a LoomIO prototye and integrated it into Ceph file system. The evaluation results show that LoomIO achieved the considerable improvements in resource utilization (by up to 35\%), in I/O throughput (by up to 31\%), and in 99th percentile latency (by up to 54\%) compared to the K-optimal method which uses the same scheduling algorithm as LoomIO but does not have the coordination support.},
  archive      = {J_TPDS},
  author       = {Yusheng Hua and Xuanhua Shi and Kang He and Hai Jin and Wei Xie and Ligang He and Yong Chen},
  doi          = {10.1109/TPDS.2021.3126260},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1799-1810},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {LoomIO: Object-level coordination in distributed file systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bifactor approximation algorithm for cloudlet placement in
edge computing. <em>TPDS</em>, <em>33</em>(8), 1787‚Äì1798. (<a
href="https://doi.org/10.1109/TPDS.2021.3126256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging applications with low-latency requirements such as real-time analytics, immersive media applications, and intelligent virtual assistants have rendered Edge Computing as a critical computing infrastructure. Existing studies have explored the cloudlet placement problem in a homogeneous scenario with different goals such as latency minimization, load balancing, energy efficiency, and placement cost minimization. However, placing cloudlets in a highly heterogeneous deployment scenario considering the next-generation 5G networks and IoT applications is still an open challenge. The novel requirements of these applications indicate that there is still a gap in ensuring low-latency service guarantees when deploying cloudlets. Furthermore, deploying cloudlets in a cost-effective manner and ensuring full coverage for all users in edge computing are other critical conflicting issues. In this article, we address these issues by designing a bifactor approximation algorithm to solve the heterogeneous cloudlet placement problem to guarantee a bounded latency and placement cost, while fully mapping user applications to appropriate cloudlets. We first formulate the problem as a multi-objective integer programming model and show that it is a computationally NP-hard problem. We then propose a bifactor approximation algorithm, ACP, to tackle its intractability. We investigate the effectiveness of ACP by performing extensive theoretical analysis and experiments on multiple deployment scenarios based on New York City OpenData. We prove that ACP provides a (2,4)-approximation ratio for the latency and the placement cost. The experimental results show that ACP obtains near-optimal results in a polynomial running time making it suitable for both short-term and long-term cloudlet placement in heterogeneous deployment scenarios.},
  archive      = {J_TPDS},
  author       = {Dixit Bhatta and Lena Mashayekhy},
  doi          = {10.1109/TPDS.2021.3126256},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1787-1798},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A bifactor approximation algorithm for cloudlet placement in edge computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FedGraph: Federated graph learning with intelligent
sampling. <em>TPDS</em>, <em>33</em>(8), 1775‚Äì1786. (<a
href="https://doi.org/10.1109/TPDS.2021.3125565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning has attracted much research attention due to its privacy protection in distributed machine learning. However, existing work of federated learning mainly focuses on Convolutional Neural Network (CNN), which cannot efficiently handle graph data that are popular in many applications. Graph Convolutional Network (GCN) has been proposed as one of the most promising techniques for graph learning, but its federated setting has been seldom explored. In this article, we propose FedGraph for federated graph learning among multiple computing clients, each of which holds a subgraph. FedGraph provides strong graph learning capability across clients by addressing two unique challenges. First, traditional GCN training needs feature data sharing among clients, leading to risk of privacy leakage. FedGraph solves this issue using a novel cross-client convolution operation. The second challenge is high GCN training overhead incurred by large graph size. We propose an intelligent graph sampling algorithm based on deep reinforcement learning, which can automatically converge to the optimal sampling policies that balance training speed and accuracy. We implement FedGraph based on PyTorch and deploy it on a testbed for performance evaluation. The experimental results of four popular datasets demonstrate that FedGraph significantly outperforms existing work by enabling faster convergence to higher accuracy.},
  archive      = {J_TPDS},
  author       = {Fahao Chen and Peng Li and Toshiaki Miyazaki and Celimuge Wu},
  doi          = {10.1109/TPDS.2021.3125565},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1775-1786},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FedGraph: Federated graph learning with intelligent sampling},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IBalancer: Load-aware in-server flow scheduling for
sub-millisecond tail latency. <em>TPDS</em>, <em>33</em>(8), 1761‚Äì1774.
(<a href="https://doi.org/10.1109/TPDS.2021.3120021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving microsecond-scale tail latency poses an extreme challenge to the conventional architecture of ‚ÄúNIC-OS-Application‚Äù in the face of high concurrent requests. Existing kernel-bypass network systems improve this situation significantly. Still, they cannot achieve load-aware in-server requests distribution, which in turn not only harms resource efficiency but, more importantly, beats the goal of squeezing tail latency. This paper proposes iBalancer, an in-server proactive load balancer for the kernel-bypass system, which aggressively handles NIC-side flow scheduling according to the load of threads on the processor-side. Furthermore, we propose a novel metric, ‚Äúpolling time interval (PTI),‚Äù to quantify the load of worker threads, which not only indicates utilization of the core bound to the worker thread but also reflects the differences in the processing time of different flows. By scheduling flows according to the metric PTI, iBalancer tends to average the queueing latencies of different flows, such as Set &amp; Get operations for an in-memory key-value store. In addition, by decoupling flow scheduling from packet steering, iBalancer achieves a tail latency aware flow-to-core binding and preserves hardware-based request distribution among cores. The proposed system is evaluated and compared to mTCP and Shenango using two representative microsecond-scale network applications: Memcached KVS and a real-time deep-learning-based financial fraud identification application. Experimental results show that iBalancer can process up to 4.75 $ \times $ and 1.55 $ \times \ $ higher load over mTCP and Shenango under 500Œºs 99 th percentile tail latency limit on Memcached. For the financial fraud identification application, iBalancer is able to process 4.56 $ \times $ and 1.16 $ \times $ higher load than mTCP and Shenango considering 900Œºs tail latency.},
  archive      = {J_TPDS},
  author       = {Qi Zhang and Yi Liu and Tao Liu},
  doi          = {10.1109/TPDS.2021.3120021},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1761-1774},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {IBalancer: Load-aware in-server flow scheduling for sub-millisecond tail latency},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hamiltonian paths of <span
class="math inline"><em>k</em></span>k-ary <span
class="math inline"><em>n</em></span>n-cubes avoiding faulty links and
passing through prescribed linear forests. <em>TPDS</em>,
<em>33</em>(7), 1752‚Äì1760. (<a
href="https://doi.org/10.1109/TPDS.2021.3126254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The $k$ -ary $n$ -cube $Q_n^k$ is one of the most attractive interconnection networks for parallel and distributed systems. Let $F$ be a set of faulty links in $Q_n^k$ and let $L$ be a linear forest in $Q_n^k-F$ such that $|E(L)|+|F|\leq 2n-3$ . For any two distinct nodes $u$ and $v$ of $Q_n^k$ with $n\geq 2$ and odd $k\geq 3$ , we prove that $Q_n^k-F$ admits a Hamiltonian path between $u$ and $v$ passing through $L$ if and only if none of the paths in $L$ has $u$ or $v$ as internal nodes or both of them as end-nodes. The upper bound $2n-3$ on $|E(L)|+|F|$ is optimal in the worst case. The main results in this paper generalized some known results.},
  archive      = {J_TPDS},
  author       = {Yuxing Yang and Lingling Zhang},
  doi          = {10.1109/TPDS.2021.3126254},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1752-1760},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Hamiltonian paths of $k$k-ary $n$n-cubes avoiding faulty links and passing through prescribed linear forests},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FFNLFD: Fault diagnosis of multiprocessor systems at local
node with fault-free neighbors under PMC model and MM* model.
<em>TPDS</em>, <em>33</em>(7), 1739‚Äì1751. (<a
href="https://doi.org/10.1109/TPDS.2021.3126257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault diagnosability is utilized as a significant measure that reflects the reliability of a multiprocessor system. However, people frequently pay close attention to the entire system‚Äôs diagnosability while ignoring the system‚Äôs important local information. The $m$ -fault-free-neighbor local fault diagnosability (for short, $m$ -FFNLFD) is a novel indicator, which describes the diagnosability of a system at a local node with $m$ fault-free neighbors. In this paper, we propose the $m$ -FFNLFD of general networks at local node under the Preparata Metze Chien model. Moreover, we also characterize some important properties of $m$ -FFNLFD of a multiprocessor system under the comparison model. Furthermore, we apply our proposed conclusions to directly obtain the $m$ -FFNLFD of 11 well-known networks under PMC-M and MM*-M, including hypercubes, locally twisted cubes, $k$ -ary $n$ -cubes, crossed cubes, twisted hypercubes, exchanged hypercubes, star graphs, $(n,k)$ -star graphs, $(n,k)$ -arrangement graphs, data center network DCells and BCDCs. Finally, we compare the $m$ -FFNLFD with both diagnosability and conditional diagnosability, and it is shown that the $m$ -FFNLFD is greater than all the other fault diagnosabilities.},
  archive      = {J_TPDS},
  author       = {Limei Lin and Yanze Huang and Yuhang Lin and Sun-Yuan Hsieh and Li Xu},
  doi          = {10.1109/TPDS.2021.3126257},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1739-1751},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FFNLFD: Fault diagnosis of multiprocessor systems at local node with fault-free neighbors under PMC model and MM* model},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FARNN: FPGA-GPU hybrid acceleration platform for recurrent
neural networks. <em>TPDS</em>, <em>33</em>(7), 1725‚Äì1738. (<a
href="https://doi.org/10.1109/TPDS.2021.3124125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GPU-based platforms provide high computation throughput for large mini-batch deep neural network computations. However, a large batch size may not be ideal for some situations, such as aiming at low latency, training on edge/mobile devices, partial retraining for personalization, and having irregular input sequence lengths. GPU performance suffers from low utilization especially for small-batch recurrent neural network (RNN) applications where sequential computations are required. In this article, we propose a hybrid architecture, called FARNN, which combines a GPU and an FPGA to accelerate RNN computation for small batch sizes. After separating RNN computations into GPU-efficient and GPU-inefficient tasks, we design special FPGA computation units that accelerate the GPU-inefficient RNN tasks. FARNN off-loads the GPU-inefficient tasks to the FPGA. We evaluate FARNN with synthetic RNN layers of various configurations on the Xilinx UltraScale+ FPGA and the NVIDIA P100 GPU in addition to evaluating it with real RNN applications. The evaluation result indicates that FARNN outperforms the P100 GPU platform for RNN training by up to 4.2 $\times {}$ with small batch sizes, long input sequences, and many RNN cells per layer.},
  archive      = {J_TPDS},
  author       = {Hyungmin Cho and Jeesoo Lee and Jaejin Lee},
  doi          = {10.1109/TPDS.2021.3124125},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1725-1738},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FARNN: FPGA-GPU hybrid acceleration platform for recurrent neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A highly-available move operation for replicated trees.
<em>TPDS</em>, <em>33</em>(7), 1711‚Äì1724. (<a
href="https://doi.org/10.1109/TPDS.2021.3118603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Replicated tree data structures are a fundamental building block of distributed filesystems, such as Google Drive and Dropbox, and collaborative applications with a JSON or XML data model. These systems need to support a move operation that allows a subtree to be moved to a new location within the tree. However, such a move operation is difficult to implement correctly if different replicas can concurrently perform arbitrary move operations, and we demonstrate bugs in Google Drive and Dropbox that arise with concurrent moves. In this article we present a CRDT algorithm that handles arbitrary concurrent modifications on trees, while ensuring that the tree structure remains valid (in particular, no cycles are introduced), and guaranteeing that all replicas converge towards the same consistent state. Our algorithm requires no synchronous coordination between replicas, making it highly available in the face of network partitions. We formally prove the correctness of our algorithm using the Isabelle/HOL proof assistant, and evaluate the performance of our formally verified implementation in a geo-replicated setting.},
  archive      = {J_TPDS},
  author       = {Martin Kleppmann and Dominic P. Mulligan and Victor B. F. Gomes and Alastair R. Beresford},
  doi          = {10.1109/TPDS.2021.3118603},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1711-1724},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A highly-available move operation for replicated trees},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performance and cost-efficient spark job scheduling based on
deep reinforcement learning in cloud computing environments.
<em>TPDS</em>, <em>33</em>(7), 1695‚Äì1710. (<a
href="https://doi.org/10.1109/TPDS.2021.3124670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data frameworks such as Spark and Hadoop are widely adopted to run analytics jobs in both research and industry. Cloud offers affordable compute resources which are easier to manage. Hence, many organizations are shifting towards a cloud deployment of their big data computing clusters. However, job scheduling is a complex problem in the presence of various Service Level Agreement (SLA) objectives such as monetary cost reduction, and job performance improvement. Most of the existing research does not address multiple objectives together and fail to capture the inherent cluster and workload characteristics. In this article, we formulate the job scheduling problem of a cloud-deployed Spark cluster and propose a novel Reinforcement Learning (RL) model to accommodate the SLA objectives. We develop the RL cluster environment and implement two Deep Reinforce Learning (DRL) based schedulers in TF-Agents framework. The proposed DRL-based scheduling agents work at a fine-grained level to place the executors of jobs while leveraging the pricing model of cloud VM instances. In addition, the DRL-based agents can also learn the inherent characteristics of different types of jobs to find a proper placement to reduce both the total cluster VM usage cost and the average job duration. The results show that the proposed DRL-based algorithms can reduce the VM usage cost up to 30\%.},
  archive      = {J_TPDS},
  author       = {Muhammed Tawfiqul Islam and Shanika Karunasekera and Rajkumar Buyya},
  doi          = {10.1109/TPDS.2021.3124670},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1695-1710},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Performance and cost-efficient spark job scheduling based on deep reinforcement learning in cloud computing environments},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TherMa-MiCs: Thermal-aware scheduling for fault-tolerant
mixed-criticality systems. <em>TPDS</em>, <em>33</em>(7), 1678‚Äì1694. (<a
href="https://doi.org/10.1109/TPDS.2021.3123544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multicore platforms are becoming the dominant trend in designing Mixed-Criticality Systems (MCSs), which integrate applications of different levels of criticality into the same platform. A well-known MCS is the dual-criticality system that is composed of low-criticality and high-criticality tasks. The availability of multiple cores on a single chip provides opportunities to employ fault-tolerant techniques, such as N-Modular Redundancy (NMR), to ensure the reliability of MCSs. However, applying fault-tolerant techniques will increase the power consumption on the chip, and thereby on-chip temperatures might increase beyond safe limits. To prevent thermal emergencies, urgent countermeasures, like Dynamic Voltage and Frequency Scaling (DVFS) or Dynamic Power Management (DPM) will be triggered to cool down the chip. Such countermeasures, however, might not only lead to suspending low-criticality tasks, but also it might lead to violating timing constraints of high-criticality tasks. In order to prevent such severe scenarios, it is indispensable to consider a temperature constraint within the scheduling process of fault-tolerant MCSs. Therefore, this paper presents, for the first time, a thermal-aware scheduling scheme for fault-tolerant MCSs, named TherMa-MiCs. In particular, TherMa-MiCs, satisfies the temperature constraint jointly with the timing constraints of the high-criticality tasks, while attempting to maximize the QoS of low-criticality tasks under the predefined constraints. At the same time, a reliability target is satisfied by employing the well-known N-Modular Redundancy (NMR) fault-tolerant technique. Experimental results show that our proposed scheme meets the temperature and timing constraints, while at the same time, improving the QoS of low-criticality tasks, with an average of 44\%.},
  archive      = {J_TPDS},
  author       = {Sepideh Safari and Heba Khdr and Pourya Gohari-Nazari and Mohsen Ansari and Shaahin Hessabi and J√∂rg Henkel},
  doi          = {10.1109/TPDS.2021.3123544},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1678-1694},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {TherMa-MiCs: Thermal-aware scheduling for fault-tolerant mixed-criticality systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and simulation of content-aware hybrid DRAM-PCM
memory system. <em>TPDS</em>, <em>33</em>(7), 1666‚Äì1677. (<a
href="https://doi.org/10.1109/TPDS.2021.3123539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase Change Memory (PCM) can directly connect persistent memory to main memory bus, while it achieves high read throughput and low standby power, the critical concerns are its poor write performance and limited durability. A naturally in-spired design is the hybrid memory architecture that fuses DRAM and PCM, so as to exploit the positive aspects of both types of memory. Unfortunately, existing solutions are seriously challenged by the limited main memory size, which is the primary bottleneck of in-memory computing. In this paper, we introduce a novel Content Aware Hybrid DRAM-PCM memory system framework‚ÄîCAHRAM, which exploits deduplication to improve line sharing with high memory efficiency. It reduces write traffic to hybrid memory by removing unnecessary duplicate line writes, thereby further enhancing the write endurance of PCM. And it also substantially extends available free memory space by coalescing redundant lines in hybrid memory. We also design a reference-based page migration technique to minimize the access overheads caused by the performance gap between DRAM and PCM. Compared with the state-of-the-art in a hybrid memory simulator, our experiment results show that CAHRAM can achieve the highest I/O performance and the longest PCM lifetime with the competitive efficiencies in space and energy.},
  archive      = {J_TPDS},
  author       = {Yinjin Fu and Yutong Lu and Zhiguang Chen and Yang Wu and Nong Xiao},
  doi          = {10.1109/TPDS.2021.3123539},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1666-1677},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Design and simulation of content-aware hybrid DRAM-PCM memory system},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TODG: Distributed task offloading with delay guarantees for
edge computing. <em>TPDS</em>, <em>33</em>(7), 1650‚Äì1665. (<a
href="https://doi.org/10.1109/TPDS.2021.3123535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing has been an efficient way to provide prompt and near-data computing services for resource-and-delay sensitive IoT applications via computation offloading. Effective computation offloading strategies need to comprehensively cope with several major issues, including 1) the allocation of dynamic communication and computational resources, 2) delay constraints of heterogeneous tasks, and 3) requirements for computationally inexpensive and distributed algorithms. However, most of the existing works mainly focus on part of these issues, which would not suffice to achieve expected performance in complex and practical scenarios. To tackle this challenge, in this paper, we systematically study a distributed computation offloading problem with delay constraints, where heterogeneous computational tasks require continually offloading to a set of edge servers via a limiting number of stochastic communication channels. The task offloading problem is formulated as a delay-constrained long-term stochastic optimization problem under unknown prior statistical knowledge. To solve this problem, we first provide a technical path to transform and decompose it into several slot-level sub-problems. Then, we devise a distributed online algorithm, namely TODG, to efficiently allocate resources and schedule offloading tasks. Further, we present a comprehensive analysis for TODG in terms of the optimality gap, the worst-case delay, and the impact of system parameters. Extensive simulation results demonstrate the effectiveness and efficiency of TODG.},
  archive      = {J_TPDS},
  author       = {Sheng Yue and Ju Ren and Nan Qiao and Yongmin Zhang and Hongbo Jiang and Yaoxue Zhang and Yuanyuan Yang},
  doi          = {10.1109/TPDS.2021.3123535},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1650-1665},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {TODG: Distributed task offloading with delay guarantees for edge computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Pistis: Issuing trusted and authorized certificates with
distributed ledger and TEE. <em>TPDS</em>, <em>33</em>(7), 1636‚Äì1649.
(<a href="https://doi.org/10.1109/TPDS.2021.3121562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The security of HTTPS fundamentally relies on SSL/TLS certificates issued by Certificate Authorities (CAs), which, however, are vulnerable to be compromised to issue unauthorized certificates (i.e., certificates issued without domains‚Äô permission). Current countermeasures such as Certificate Transparency (CT) can only detect unauthorized certificates rather than preventing them. In this article, we present Pistis , a framework for issuing authorized and trusted certificates with the distributed ledger and Trusted Execution Environment (TEE) technology. In Pistis , TEE nodes validate whether the domain in a requested certificate passes the domain ownership validation (i.e., under corresponding applicants‚Äô control) and submit attested results to a smart contract in the distributed ledger. The smart contract issues a certificate to the applicant when an attested result shows a pass. Therefore, Pistis can ensure its issued certificates are authorized due to the domain ownership validation mechanism in the TEE. Furthermore, as the issued certificates are stored in a Merkle Patricia Tree (MPT) in Pistis , they are trusted and can be verified by a normal user easily. The security of Pistis is formally proved in the Universally Composable (UC) framework. Compared with state-of-the-art, Pistis avoids potential damages by preventing unauthorized certificates from issuing.},
  archive      = {J_TPDS},
  author       = {Zecheng Li and Haotian Wu and Lap Hou Lao and Songtao Guo and Yuanyuan Yang and Bin Xiao},
  doi          = {10.1109/TPDS.2021.3121562},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1636-1649},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Pistis: Issuing trusted and authorized certificates with distributed ledger and TEE},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wukong+g: Fast and concurrent RDF query processing using
RDMA-assisted GPU graph exploration. <em>TPDS</em>, <em>33</em>(7),
1619‚Äì1635. (<a href="https://doi.org/10.1109/TPDS.2021.3121568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RDF graph has been increasingly used to store and represent information shared over the Web, including social graphs and knowledge bases. With the increasing scale of RDF graphs and the concurrency level of SPARQL queries, current RDF systems are confronted with inefficient concurrent query processing on massive data parallelism. The situation becomes more severe in the face of data-intensive queries (aka heavy query), which usually lead to suboptimal response time (latency) as well as throughput collapse. In this article, we present Wukong+G, the first graph-based distributed RDF query processing system that efficiently exploits the hybrid parallelism of CPU and GPU. Wukong+G is made fast and concurrent with four key designs. First, Wukong+G tames massive random memory accesses in graph exploration by efficiently mapping data between CPU and GPU for latency hiding, including a set of techniques like query-aware prefetching, pattern-aware pipelining and fine-grained swapping. Second, Wukong+G scales up by introducing a GPU-friendly RDF store to support RDF graphs exceeding GPU memory size, by using techniques like predicate-based grouping, pairwise caching and look-ahead replacing to narrow the gap between host and device memory scale. Third, Wukong+G scales out through a communication layer that decouples the transferring process for query metadata and intermediate results, and further leverages both native and GPUDirect RDMA to enable efficient communication on a CPU/GPU cluster. Finally, Wukong+G simultaneously runs multiple queries on a single GPU to improve overall throughput and fully exploits hardware heterogeneity (CPU/GPU) by scheduling a single query on CPU and GPU adaptively. We have implemented Wukong+G by extending a state-of-the-art distributed RDF store (i.e., Wukong) with distributed GPU support. Evaluation on a heterogeneous CPU/GPU cluster with RDMA-capable network shows that Wukong+G outperforms Wukong by up to 9.0√ó (from 2.3√ó) and scales well on 10 GPU cards for heavy queries. Wukong+G can also improve both latency and throughput by more than one order of magnitude when facing hybrid workloads.},
  archive      = {J_TPDS},
  author       = {Zihang Yao and Rong Chen and Binyu Zang and Haibo Chen},
  doi          = {10.1109/TPDS.2021.3121568},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1619-1635},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Wukong+G: Fast and concurrent RDF query processing using RDMA-assisted GPU graph exploration},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cooperative scheduling schemes for explainable DNN
acceleration in satellite image analysis and retraining. <em>TPDS</em>,
<em>33</em>(7), 1605‚Äì1618. (<a
href="https://doi.org/10.1109/TPDS.2021.3122454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep learning-based satellite image analysis and retraining systems are getting emerging technologies to enhance the capability of the sophisticated analysis of terrestrial objects. In principle, to apply the explainable DNN model for the process of satellite image analysis and retraining, we consider a new acceleration scheduling mechanism. Especially, the conventional DNN acceleration schemes cause serious performance degradation due to computational complexity and costs in satellite image analysis and retraining. In this article, to overcome the performance degradation, we propose cooperative scheduling schemes for explainable DNN acceleration in analysis and retraining process. For the purpose of it, we define the latency and energy cost modeling to derive the optimized processing time and cost required for explainable DNN acceleration. Especially, we show a minimum processing cost considered in the proposed scheduling via layer-level management of the explainable DNN on FPGA-GPU acceleration system. In addition, we evaluate the performance using an adaptive unlabeled data selection scheme with confidence threshold and a semi-supervised learning driven data parallelism scheme in accelerating retraining process. The experimental results demonstrate that the proposed schemes reduce the energy cost of the conventional DNN acceleration systems by up to about 40\% while guaranteeing the latency constraints.},
  archive      = {J_TPDS},
  author       = {Woo-Joong Kim and Chan-Hyun Youn},
  doi          = {10.1109/TPDS.2021.3122454},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1605-1618},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cooperative scheduling schemes for explainable DNN acceleration in satellite image analysis and retraining},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fast <span
class="math inline"><em>f</em>(<em>r</em>,‚ÄÜ<em>k</em>‚ÄÖ+‚ÄÖ1)/<em>k</em></span>f(r,k+1)/k-diagnosis
for interconnection networks under MM* model. <em>TPDS</em>,
<em>33</em>(7), 1593‚Äì1604. (<a
href="https://doi.org/10.1109/TPDS.2021.3122440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyberspace is not a ‚Äúvacuum space‚Äù, and it is normal that there are inevitable viruses and worms in cyberspace. Cyberspace security threats stem from the problem of endogenous security, which is caused by the incompleteness of theoretical system and technology of the information field itself. Thus it is impossible and unnecessary for us to build an ‚Äúaseptic‚Äù cyberspace. On the contrast, we must focus on improving the ‚Äúself-immunity‚Äù of network. Literally, endogenous security is an endogenous effect from its own structural factors rather than external ones. The $t/k$ -diagnosis strategy plays a very important role in measuring endogenous network security without prior knowledge, which can significantly enhance the self-diagnosing capability of network. As far as we know, few research involves $t/k$ -diagnosis algorithm and $t/k$ -diagnosability of interconnection networks under MM* model. In this article, we propose a fast $f(r,k+1)/k$ -diagnosis algorithm of complexity $O(Nr^2)$ , say $G$ MIS $k$ DIAGMM*, for a general $r$ -regular network $G$ under MM* model by designing a 0-comparison subgraph $M_0(G)$ , where $N$ is the size of $G$ . We determine that the $t/k$ -diagnosability $(t(G)/k)^M$ of $G$ under MM* model is $f(r,k+1)$ by $G$ MIS $k$ DIAGMM* algorithm. Moreover, we establish the $(t(G)/k)^M$ of some interconnection networks under MM* model, including BC networks, $(n,l)$ -star graph networks, and data center network DCells. Finally, we compare $(t(G)/k)^M$ with diagnosability, conditional diagnosability, pessimistic diagnosability, extra diagnosability, and good-neighbor diagnosability under MM* model. It can be seen that $(t(G)/k)^M$ is greater than other fault diagnosabilities in most cases.},
  archive      = {J_TPDS},
  author       = {Yanze Huang and Limei Lin and Sun-Yuan Hsieh},
  doi          = {10.1109/TPDS.2021.3122440},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1593-1604},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A fast $f(r,k+1)/k$f(r,k+1)/k-diagnosis for interconnection networks under MM* model},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LOCUS: User-perceived delay-aware service placement and user
allocation in MEC environment. <em>TPDS</em>, <em>33</em>(7), 1581‚Äì1592.
(<a href="https://doi.org/10.1109/TPDS.2021.3119948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the multi-access edge computing environment, app vendors deploy their services and applications at the network edges, and edge users offload their computation tasks to edge servers. We study the user-perceived delay-aware service placement and user-allocation problem in edge environment. We model the MEC-enabled network, where the user-perceived delay consists of computing delay and transmission delay. The total cost in the offloading system is defined as the sum of service placement, edge server usage and energy consumption cost, and we need to minimize the total cost by determining the overall service-placing decision and user-allocation decision, while guaranteeing that the user-perceived delay requirement of each user is fulfilled. Our considered problem is formulated as a Mixed Integer Linear Programming problem, and we prove its NP-hardness. Due to the intractability of the considered problem, we propose a LOCal-search based algorithm for USer-perceived delay-aware service placement and user-allocation in edge environment, named LOCUS, which starts with a feasible solution and then repeatedly reduces the total cost by performing local-search steps. After that, we analyze the time complexity of LOCUS and prove that it achieves provable guaranteed performance. Finally, we compare LOCUS with other existing methods and show its good performance through experiments.},
  archive      = {J_TPDS},
  author       = {Yu Chen and Sheng Zhang and Yibo Jin and Zhuzhong Qian and Mingjun Xiao and Jidong Ge and Sanglu Lu},
  doi          = {10.1109/TPDS.2021.3119948},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1581-1592},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {LOCUS: User-perceived delay-aware service placement and user allocation in MEC environment},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). SaPus: Self-adaptive parameter update strategy for DNN
training on multi-GPU clusters. <em>TPDS</em>, <em>33</em>(7),
1569‚Äì1580. (<a href="https://doi.org/10.1109/TPDS.2021.3118609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter server architecture has been identified as an efficient framework for scaling DNNs training on clusters. For large-scale deployment, communication becomes the bottleneck, and the parameter updating strategy strongly impacts the training performance and accuracy. Recent state-of-art solutions have adopted the local SGD approach, which enables workers to update their local version of models and only aggregate them to update the global parameters after finishing a number of iterations, to alleviate heavy communication pressure on the parameter server and improving the training performance. We identify three limitations of these works. First, these works do not provide an approach for determining when the worker is to update the parameter with the server under asynchronous communication strategies that can guarantee the training performance. Second, local SGD suffers from the problem of unbounded gradient delay. Previous work works well for a short delay while can not guarantee the performance with an increase of gradient delay. Third, they do not consider the system performance when determining the update interval of the local SGD, including the CPU, memory, and network, which affects the training performance extremely. We provide a self-adaptive parameter updating strategy called SaPus, which allows each worker to detect their training results through quantification of the accumulated gradient updates and determine when to update the parameter with the server adaptively and individually. Theoretical lower and upper bound of the update interval is also provided. We also propose a weighted aggregation algorithm based on a global-loss window, which is used to collect the most recent loss value of other workers to calculate a weight for the accumulated gradients of each worker to solve the unbounded delay problem in asynchronous local SGD. To increase the robustness of our parameter updating strategy, a performance model is built to provide a resource-aware lower bound for the update interval. Extensive experimental results generated on GPU cluster indicate that our model improves the training performance of DNNs, achieving up to $66.67\%$ speedup as compared with state-of-art solutions. Further, results show the CPU utilization of server dropped by up to $81.1\%$ and network bandwidth usage reduced to less than $1~Gbps$ on an average during the training.},
  archive      = {J_TPDS},
  author       = {Zhaorui Zhang and Choli Wang},
  doi          = {10.1109/TPDS.2021.3118609},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1569-1580},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SaPus: Self-adaptive parameter update strategy for DNN training on multi-GPU clusters},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring data analytics without decompression on embedded
GPU systems. <em>TPDS</em>, <em>33</em>(7), 1553‚Äì1568. (<a
href="https://doi.org/10.1109/TPDS.2021.3119402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of computer architecture, even for embedded systems, GPU devices can be integrated, providing outstanding performance and energy efficiency to meet the requirements of different industries, applications, and deployment environments. Data analytics is an important application scenario for embedded systems. Unfortunately, due to the limitation of the capacity of the embedded device, the scale of problems handled by the embedded system is limited. In this paper, we propose a novel data analytics method, called G-TADOC, for efficient text analytics directly on compression on embedded GPU systems. A large amount of data can be compressed and stored in embedded systems, and can be processed directly in the compressed state, which greatly enhances the processing capabilities of the systems. Particularly, G-TADOC has three innovations. First, a novel fine-grained thread-level workload scheduling strategy for GPU threads has been developed, which partitions heavily-dependent loads adaptively in a fine-grained manner. Second, a GPU thread-safe memory pool has been developed to handle inconsistency with low synchronization overheads. Third, a sequence-support strategy is provided to maintain high GPU parallelism while ensuring sequence information for lossless compression. Moreover, G-TADOC involves special optimizations for embedded GPUs, such as utilizing the CPU-GPU shared unified memory. Experiments show that G-TADOC provides 13.2√ó average speedup compared to the state-of-the-art TADOC. G-TADOC also improves performance-per-cost by 2.6√ó and energy efficiency by 32.5√ó over TADOC.},
  archive      = {J_TPDS},
  author       = {Zaifeng Pan and Feng Zhang and Yanliang Zhou and Jidong Zhai and Xipeng Shen and Onur Mutlu and Xiaoyong Du},
  doi          = {10.1109/TPDS.2021.3119402},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1553-1568},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Exploring data analytics without decompression on embedded GPU systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Topology-aware neural model for highly accurate QoS
prediction. <em>TPDS</em>, <em>33</em>(7), 1538‚Äì1552. (<a
href="https://doi.org/10.1109/TPDS.2021.3116865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread deployment of various cloud computing and service-oriented systems, there is a rapidly increasing demand for collaborative quality-of-service (QoS) prediction. Existing QoS prediction methods have made great progress in modeling users and services as well as exploiting contexts of service invocations. However, they ignore the completion of service requests/responses relies on the underlying network topology and the complex interactions between Autonomous Systems. To tackle this challenge, we propose a topology-aware neural (TAN) model for collaborative QoS prediction. In the TAN model, the features of users, services, and intermediate nodes on the communication path are projected to a shared latent space as input features. To jointly characterize the invocation process, the path features and end-cross features are captured respectively through an explicit path modeling layer and an implicit cross-modeling layer. After that, a gating layer fuses and transmits these features to the prediction layer for estimating unknown QoS values. In this way, TAN provides a flexible framework that can comprehensively capture the invocation context for making accurate QoS prediction. Experimental results on two real-world datasets demonstrate that TAN significantly outperforms state-of-the-art methods on the tasks of response time, throughput, and reliability prediction. Also, TAN shows better extensibility of using auxiliary information.},
  archive      = {J_TPDS},
  author       = {Jiahui Li and Hao Wu and Jiapei Chen and Qiang He and Ching-Hsien Hsu},
  doi          = {10.1109/TPDS.2021.3116865},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1538-1552},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Topology-aware neural model for highly accurate QoS prediction},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Necessary feasibility analysis for mixed-criticality
real-time embedded systems. <em>TPDS</em>, <em>33</em>(7), 1520‚Äì1537.
(<a href="https://doi.org/10.1109/TPDS.2021.3118610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As multiple software components with different safety-criticality levels are integrated on a shared computing platform, a real-time embedded system becomes a mixed-criticality (MC) system, which should provide timing guarantees at all different levels of assurance to software components with different criticality levels. In the real-time systems community, the concept of an MC system is regarded as a promising, emerging solution to solve an inherent challenge of real-time systems: pessimistic reservation of computing resources, which yields a low resource-utilization for the sake of guaranteeing timing requirements. Since a timing guarantee should be provided before a real-time system starts to operate, its feasibility has been extensively studied for single-criticality systems; however, the same cannot be said for MC systems. In this article, we develop necessary feasibility tests for MC real-time embedded systems, which is the first study that yields non-trivial results for MC necessary feasibility on both uniprocessor and multiprocessor platforms. To this end, we investigate characteristics of MC necessary feasibility conditions, and identify new challenges posed by the characteristics. By addressing those challenges, we develop two collective necessary feasibility tests and their simplified versions, which are able to exploit a tradeoff between capability in finding infeasible task sets and time-complexity. The simulation results demonstrate that the proposed tests find a number of additional infeasible task sets for both uniprocessor and multiprocessor platforms, which have been proven neither feasible nor infeasible by any existing studies.},
  archive      = {J_TPDS},
  author       = {Hoon Sung Chwa and Hyeongboo Baek and Jinkyu Lee},
  doi          = {10.1109/TPDS.2021.3118610},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1520-1537},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Necessary feasibility analysis for mixed-criticality real-time embedded systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A potential game theoretic approach to computation
offloading strategy optimization in end-edge-cloud computing.
<em>TPDS</em>, <em>33</em>(6), 1503‚Äì1519. (<a
href="https://doi.org/10.1109/TPDS.2021.3112604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating user ends (UEs), edge servers (ESs), and the cloud into end-edge-cloud computing (EECC) can enhance the utilization of resources and improve quality of experience (QoE). However, the performance of EECC is significantly affected by its architecture. In this article, we classify EECC into two computing architectures types according to the visibility and accessibility of the cloud to UEs, i.e., hierarchical end-edge-cloud computing (Hi-EECC) and horizontal end-edge-cloud computing (Ho-EECC). In Hi-EECC, UEs can offload their tasks only to ESs. When the resources of ESs are exhausted, the ESs request the cloud to provide resources to UEs. In Ho-EECC, UEs can offload their tasks directly to ESs and the cloud. In this article, we construct a potential game for the EECC environment, in which each UE selfishly minimizes its payoff, study the computation offloading strategy optimization problems, and develop two potential game-based algorithms in Hi-EECC and Ho-EECC. Extensive experiments with real-world data are conducted to demonstrate the performance of the proposed algorithms. Moreover, the scalability and applicability of the two computing architectures are comprehensively analyzed. The conclusions of our work can provide useful suggestions for choosing specific computing architectures under different application environments to improve the performance of EECC and QoE.},
  archive      = {J_TPDS},
  author       = {Yan Ding and Kenli Li and Chubo Liu and Keqin Li},
  doi          = {10.1109/TPDS.2021.3112604},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1503-1519},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A potential game theoretic approach to computation offloading strategy optimization in end-edge-cloud computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep reinforcement learning for load-balancing aware network
control in IoT edge systems. <em>TPDS</em>, <em>33</em>(6), 1491‚Äì1502.
(<a href="https://doi.org/10.1109/TPDS.2021.3116863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Load balancing is directly associated with the overall performance of a parallel and distributed computing system. Although the relevant problems in communication and computation have been well studied in data center environments, few works have considered the issues in an Internet of Things (IoT) edge scenario. In fact, processing data in a load balancing way for the latter case is more challenging. The main reason is that, unlike a data center, both the data sources and the network infrastructure in an IoT edge system can be dynamic. Moreover, with different performance requirements from IoT networks and edge servers, it will be hard to characterize the performance model and to perform runtime optimization for the whole system. To tackle this problem, in this work, we propose a load-balancing aware networking approach for efficient data processing in IoT edge systems. Specifically, we introduce an IoT network dynamic clustering solution using the emerging deep reinforcement learning (DRL), which can both fulfill the communication balancing requirements from IoT networks and the computation balancing requirements from edge servers. Moreover, we implement our system with a long short term memory (LSTM) based Dueling Double Deep Q-Learning Network (D3QN) model, and our experiments with real-world datasets collected from an autopilot vehicle demonstrate that our proposed method can achieve significant performance improvement compared to benchmark solutions.},
  archive      = {J_TPDS},
  author       = {Qingzhi Liu and Tiancong Xia and Long Cheng and Merijn van Eijk and Tanir Ozcelebi and Ying Mao},
  doi          = {10.1109/TPDS.2021.3116863},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1491-1502},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Deep reinforcement learning for load-balancing aware network control in IoT edge systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coarse grained FPGA overlay for rapid just-in-time
accelerator compilation. <em>TPDS</em>, <em>33</em>(6), 1478‚Äì1490. (<a
href="https://doi.org/10.1109/TPDS.2021.3116859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coarse-grained FPGA overlays built around the runtime programmable DSP blocks in modern FPGAs can achieve high throughput and improved scalability compared to traditional overlays built without detailed consideration of FPGA architecture. These overlays can be mapped to using higher level compilers, achieving fast compilation, software-like programmability and run-time management, and high-level design abstraction. OpenCL allows programs running on a host computer to launch accelerator kernels which can be compiled at run-time for a specific architecture, thus enabling portability. However, prohibitive hardware compilation times in traditional design flows mean that the tools cannot effectively use just-in-time (JIT) compilation or runtime performance scaling on FPGAs. We present a methodology for runtime compilation of dataflow graphs expressed as OpenCL kernels onto coarse-grained overlays. The methodology benefits from the high level of abstraction afforded by using the OpenCL programming model, while the mapping to the overlay significantly reduces compilation and load times. Key characteristics of this work include highly performant DSP-optimized functional units that scale to large overlays on modern devices and the ability to perform automatic resource-aware kernel replication up to the size of the overlay. We demonstrate place and route times orders of magnitude better than traditional HLS flows, even when running on an embedded processor in the Xilinx Zynq.},
  archive      = {J_TPDS},
  author       = {Abhishek Kumar Jain and Douglas L. Maskell and Suhaib A. Fahmy},
  doi          = {10.1109/TPDS.2021.3116859},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1478-1490},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Coarse grained FPGA overlay for rapid just-in-time accelerator compilation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EnosLib: A library for experiment-driven research in
distributed computing. <em>TPDS</em>, <em>33</em>(6), 1464‚Äì1477. (<a
href="https://doi.org/10.1109/TPDS.2021.3111159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the importance of experiment-driven research in the distributed computing community, there has been little progress in helping researchers conduct their experiments. In most cases, they have to achieve tedious and time-consuming development and instrumentation activities to deal with the specifics of testbeds and the system under study. In order to relieve researchers of the burden of those efforts, we have developed EnosLib : a Python library that takes into account best experimentation practices and leverages modern toolkits on automatic deployment and configuration systems. EnosLib helps researchers not only in the process of developing their experimental artifacts, but also in running them over different infrastructures. To demonstrate the relevance of our library, we discuss three experimental engines built on top of EnosLib , and used to conduct empirical studies on complex software stacks between 2016 and 2019 (database systems, communication buses and OpenStack). By introducing EnosLib , our goal is to gather academic and industrial actors of our community around a library that aggregates everyday experiment-driven research operations. A library that has been already adopted by open-source projects and members of the scientific community thanks to its ease of use and extension.},
  archive      = {J_TPDS},
  author       = {Ronan-Alexandre Cherrueau and Marie Delavergne and Alexandre van Kempen and Adrien Lebre and Dimitri Pertin and Javier Rojas Balderrama and Anthony Simonet and Matthieu Simonin},
  doi          = {10.1109/TPDS.2021.3111159},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1464-1477},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {EnosLib: A library for experiment-driven research in distributed computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey of GPU multitasking methods supported by hardware
architecture. <em>TPDS</em>, <em>33</em>(6), 1451‚Äì1463. (<a
href="https://doi.org/10.1109/TPDS.2021.3115630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to support multitasking becomes more and more important in the development of graphic processing unit (GPU). GPU multitasking methods are classified into three types: temporal multitasking, spatial multitasking, and simultaneous multitasking (SMK). This article first introduces the features of some commercial GPU architectures to support multitasking and the common metrics used for evaluating the performance of GPU multitasking methods, and then reviews the GPU multitasking methods supported by hardware architecture (i.e., hardware GPU multitasking methods). The main problems of each type of hardware GPU multitasking methods to be solved are illustrated. Meanwhile, the key idea of each previous hardware GPU multitasking method is introduced. In addition, the characteristics of hardware GPU multitasking methods belonging to the same type are compared. This article also gives some valuable suggestions for the future research. An enhanced GPU simulator is needed to bridge the gap between academia and industry. In addition, it is promising to expand the research space with machine learning technologies, advanced GPU architectural innovations, 3D stacked memory, etc. Because most previous GPU multitasking methods are based on NVIDIA GPUs, this article focuses on NVIDIA GPU architecture, and uses NVIDIA&#39;s terminology. To our knowledge, this article is the first survey about hardware GPU multitasking methods. We believe that our survey can help the readers gain insights into the research field of hardware GPU multitasking methods.},
  archive      = {J_TPDS},
  author       = {Chen Zhao and Wu Gao and Feiping Nie and Huiyang Zhou},
  doi          = {10.1109/TPDS.2021.3115630},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1451-1463},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A survey of GPU multitasking methods supported by hardware architecture},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling speedup in multi-OS environments. <em>TPDS</em>,
<em>33</em>(6), 1436‚Äì1450. (<a
href="https://doi.org/10.1109/TPDS.2021.3114984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For workloads that place strenuous demands on system software, novel operating system designs like unikernels, library OSes, and hybrid runtimes offer a promising path forward. However, while these systems can outperform general-purpose OSes, they have limited ability to support legacy applications. Multi-OS environments, where the application‚Äôs execution is split between a control plane and a data plane operating system, can address this challenge, but reasoning about the performance of applications that run in such a split execution environment is currently guided only by expert intuition and empirical analysis. As the level of specialization in system software and hardware continues to increase, there is both a pressing need and ripe opportunity for investigating analytical models that can predict application performance and guide programmers‚Äô intuition when considering multi-OS environments. In this paper we present such a model to place bounds on application speedup, beginning with a simple, intuitive formulation, and progressing to a more refined model. We present an analysis of the model for a diverse set of benchmarks, as well as a prototype tool to project multi-OS speedups for applications on existing systems. Finally, we validate our model on state-of-the-art multi-OS systems, demonstrating that it reliably predicts speedup with 96\% average accuracy.},
  archive      = {J_TPDS},
  author       = {Brian R. Tauro and Conghao Liu and Kyle C. Hale},
  doi          = {10.1109/TPDS.2021.3114984},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1436-1450},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Modeling speedup in multi-OS environments},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). HSA-net: Hidden-state-aware networks for high-precision QoS
prediction. <em>TPDS</em>, <em>33</em>(6), 1421‚Äì1435. (<a
href="https://doi.org/10.1109/TPDS.2021.3111810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high-precision QoS (quality of service) prediction is based on the comprehensive perception of state information of users and services. However, the current QoS prediction approaches have limited accuracy, for most state information of users and services (i.e., network speed, latency, network type, and more) are hidden due to privacy protection. Therefore, this article proposes a hidden-state-aware network (HSA-Net) that includes three steps called hidden state initialization, hidden state perception, and QoS prediction. A hidden state initialization approach is developed first based on the latent dirichlet allocation (LDA). After that, a hidden-state perception approach is proposed to abstract the initialized hidden state by fusing the known information (e.g., service ID and user location). The perception approach consists of four hidden-state perception (HSP) modes (i.e., known mode, object mode, hybrid mode and overall mode) implemented to generate explainable and fused features through four adaptive convolutional kernels. Finally, the relationship between the fused features and the QoS is discovered through a fully connected network to complete the high-precision QoS prediction process. The proposed HSA-Net is evaluated on two real-world datasets. According to the results, the HSA-Net&#39;s mean absolute error (MAE) index reduced by 3.67\% and 28.84\%, whereas the root mean squared error (RMSE) index decreased by 3.07\% and 7.14\% compared with ten baselines on average in the two datasets.},
  archive      = {J_TPDS},
  author       = {Ziliang Wang and Xiaohong Zhang and Meng Yan and Ling Xu and Dan Yang},
  doi          = {10.1109/TPDS.2021.3111810},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1421-1435},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HSA-net: Hidden-state-aware networks for high-precision QoS prediction},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Network cost-aware geo-distributed data analytics system.
<em>TPDS</em>, <em>33</em>(6), 1407‚Äì1420. (<a
href="https://doi.org/10.1109/TPDS.2021.3108893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many geo-distributed data analytics (GDA) systems have focused on the network performance-bottleneck: inter-data center network bandwidth to improve performance. Unfortunately, these systems may encounter a cost-bottleneck ( ${\$}$ ) because they have not considered data transfer cost ( ${\$}$ ), one of the most expensive and heterogeneous resources in a multi-cloud environment. In this article, we present Kimchi , a network cost-aware GDA system to meet the cost-performance tradeoff by exploiting data transfer cost heterogeneity to avoid the cost-bottleneck. Kimchi determines cost-aware task placement decisions for scheduling tasks given inputs including data transfer cost, network bandwidth, input data size and locations, and desired cost-performance tradeoff preference. In addition, Kimchi is also mindful of data transfer cost in the presence of dynamics. Kimchi has been applied to two common GDA MapReduce models: synchronous barrier and asynchronous push-based shuffle. A Kimchi prototype has been implemented on Spark, and experiments show that it reduces cost by 5\% $\scriptstyle \sim$ 24\% without impacting performance and reduces query execution time by 45\% $\scriptstyle \sim$ 70\% without impacting cost compared to other baseline approaches centralized, vanilla Spark, and bandwidth-aware (e.g., Iridium). More importantly, Kimchi allows applications to explore a much richer cost-performance tradeoff space in a multi-cloud environment.},
  archive      = {J_TPDS},
  author       = {Kwangsung Oh and Minmin Zhang and Abhishek Chandra and Jon Weissman},
  doi          = {10.1109/TPDS.2021.3108893},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1407-1420},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Network cost-aware geo-distributed data analytics system},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VQL: Efficient and verifiable cloud query services for
blockchain systems. <em>TPDS</em>, <em>33</em>(6), 1393‚Äì1406. (<a
href="https://doi.org/10.1109/TPDS.2021.3113873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite increasingly emerging applications, a primary concern for blockchain to be fully practical is the inefficiency of data query. Direct queries on the blockchain take much time by searching every block, while indirect queries on a blockchain database greatly degrade the authenticity of query results. To conquer the authenticity problem, we propose a Verifiable Query Layer (VQL) that can be deployed in the cloud to provide both efficient and verifiable data query services for blockchain systems. The middleware layer extracts data from the underlying blockchain system and efficiently reorganizes them in databases. To prevent falsified data from being stored in the middleware, a cryptographic fingerprint is calculated based on each constructed database. The database fingerprint will be first verified by miners and then written into the blockchain. Moreover, public users can verify the entire databases or several databases that interest them in the middleware layer. We implement VQL together with the verification schemes and conduct extensive experiments based on a practical blockchain system. The evaluation results demonstrate that VQL can efficiently support various data query services and guarantee the authenticity of query results for blockchain systems.},
  archive      = {J_TPDS},
  author       = {Haotian Wu and Zhe Peng and Songtao Guo and Yuanyuan Yang and Bin Xiao},
  doi          = {10.1109/TPDS.2021.3113873},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1393-1406},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {VQL: Efficient and verifiable cloud query services for blockchain systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Customer adaptive resource provisioning for long-term cloud
profit maximization under constrained budget. <em>TPDS</em>,
<em>33</em>(6), 1373‚Äì1392. (<a
href="https://doi.org/10.1109/TPDS.2021.3112562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an efficient commercial information technology, cloud computing has attracted more and more users and enterprises to use it. Faced with such a large number and variety of customers, it is necessary for cloud providers (CPs) with limited budget to provide satisfactory customized pricing services, profitable customer and system investments, and flexible system resource provisioning strategies to improve both customer experience and long-term profit. Existing profit optimization research rarely considers customer diversity and dynamics, which may have a negative impact on long-term profit growth due to poor management of customer relations. In this article, we implement customer relationship management by considering both customer diversity and dynamics, and propose a customer adaptive resource provisioning scheme to maximize long-term profit under constrained budget. We consider four customer types (i.e., loyal, old, new, and lost) that can transition to each other during the customer&#39;s lifetime of interaction with the CP. The CP builds multiple cloud service sub-platforms, each of which contains multiple multiserver systems and serves the same type of customers. For the cloud service platform, we first analyze single multiserver system using an analytical method to obtain its optimal profit, invested funding, and system configuration. In particular, for systems serving new and lost customers, we develop a novel customer lifetime value (CLV)-based customer investment scheme that selects valuable customers for investment under limited marketing budget. Based on the above analysis, we then present a customer retention rate (CRR)-driven three-stage heuristic scheme that prioritizes investment in multiserver systems with endangered customers under limited infrastructure budget for reducing customer churn and promoting long-term profit growth. We conduct extensive simulation experiments to validate the effectiveness of our method. Simulation results show that compared with the benchmark algorithms, our method can improve the long-term profit and CRR by up to 3.4x and 7.8x, respectively.},
  archive      = {J_TPDS},
  author       = {Peijin Cong and Zhixing Zhang and Junlong Zhou and Xin Liu and Yao Liu and Tongquan Wei},
  doi          = {10.1109/TPDS.2021.3112562},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1373-1392},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Customer adaptive resource provisioning for long-term cloud profit maximization under constrained budget},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Benchmarking 50-photon gaussian boson sampling on the sunway
TaihuLight. <em>TPDS</em>, <em>33</em>(6), 1357‚Äì1372. (<a
href="https://doi.org/10.1109/TPDS.2021.3111185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boson sampling is expected to be an important milestone that will demonstrate quantum computational advantage (or quantum supremacy). This work establishes the benchmarking of Gaussian boson sampling (GBS) with threshold detection based on the Sunway TaihuLight supercomputer. To achieve the best performance and provide a competitive scenario for future quantum computing studies, the selected simulation algorithm is fully optimized based on a set of innovative approaches, including a parallel framework with almost perfect load balance and an instruction-level optimizing scheme based on a shortest-path-based instruction scheduling. In addition, data precision is carefully processed by an integer-instruction-based and multiple-precision fixed-point implementation, including 128- and 256-bit precison mode, which can be appropriately selected based on an adaptive precision optimizing scheme. Based on these methods, a highly efficient parallel quantum sampling algorithm is designed. The largest run enables us to obtain one Torontonian function of a $100\times 100$ submatrix from 50-photon GBS within 20 hours in 128-bit precision and 2 days in 256-bit precision. To our knowledge, this was the largest quantum computing simulation based on Boson Sampling by using modern supercomputers.},
  archive      = {J_TPDS},
  author       = {Yuxuan Li and Lin Gan and Mingcheng Chen and Yaojian Chen and Haitian Lu and Chaoyang Lu and Jianwei Pan and Haohuan Fu and Guangwen Yang},
  doi          = {10.1109/TPDS.2021.3111185},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1357-1372},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Benchmarking 50-photon gaussian boson sampling on the sunway TaihuLight},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On mixing eventual and strong consistency: Acute cloud
types. <em>TPDS</em>, <em>33</em>(6), 1338‚Äì1356. (<a
href="https://doi.org/10.1109/TPDS.2021.3090318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we study the properties of distributed systems that mix eventual and strong consistency. We formalize such systems through acute cloud types (ACTs), abstractions similar to conflict-free replicated data types (CRDTs), which by default work in a highly available, eventually consistent fashion, but which also feature strongly consistent operations for tasks which require global agreement. Unlike other mixed-consistency solutions, ACTs can rely on efficient quorum-based protocols, such as Paxos. Hence, ACTs gracefully tolerate machine and network failures also for the strongly consistent operations. We formally study ACTs and demonstrate phenomena which are neither present in purely eventually consistent nor strongly consistent systems. In particular, we identify temporary operation reordering , which implies interim disagreement between replicas on the relative order in which the client requests were executed. When not handled carefully, this phenomenon may lead to undesired anomalies, including circular causality. We prove an impossibility result which states that temporary operation reordering is unavoidable in mixed-consistency systems with sufficiently complex semantics. Our result is startling, because it shows that apparent strengthening of the semantics of a system (by introducing strongly consistent operations to an eventually consistent system) results in the weakening of the guarantees on the eventually consistent operations.},
  archive      = {J_TPDS},
  author       = {Maciej Kokoci≈Ñski and Tadeusz Kobus and Pawe≈Ç T. Wojciechowski},
  doi          = {10.1109/TPDS.2021.3090318},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1338-1356},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {On mixing eventual and strong consistency: Acute cloud types},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed graph realizations. <em>TPDS</em>,
<em>33</em>(6), 1321‚Äì1337. (<a
href="https://doi.org/10.1109/TPDS.2021.3104239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study graph realization problems for the first time from a distributed perspective. Graph realization problems are encountered in distributed construction of overlay networks that must satisfy certain degree or connectivity properties. We study them in the node capacitated clique (NCC) model of distributed computing, recently introduced for representing peer-to-peer overlay networks. We focus on two central variants, degree-sequence realization and minimum threshold-connectivity realization. In the degree sequence problem, each node $v$ is associated with a degree $d(v)$ , and the resulting degree sequence is realizable if it is possible to construct an overlay network in which the degree of each node $v$ is $d(v)$ . The minimum threshold-connectivity problem requires us to construct an overlay network that satisfies connectivity constraints specified between every pair of nodes. Overlay network realizations can be either explicit or implicit. Explicit realizations require both endpoints of any edge in the realized graph to be aware of the edge. In implicit realizations, on the other hand, at least one endpoint of each edge of the realized graph needs to be aware of the edge. The main realization algorithms we present are the following. (Note that all our algorithms are randomized Las Vegas algorithms unless specified otherwise. The stated running times hold with high probability.) 1) An $\tilde{O}(\min \lbrace \sqrt{m},\Delta \rbrace)$ time algorithm for implicit realization of a degree sequence. Here, $\Delta = \max _v d(v)$ is the maximum degree and $m = (1/2) \sum _v d(v)$ is the number of edges in the final realization. 2) $\tilde{O}(\Delta)$ time algorithm for an explicit realization of a degree sequence. We first compute an implicit realization and then transform it into an explicit one in $\tilde{O}(\Delta)$ additional rounds. 3) An $\tilde{O}(\Delta)$ time algorithm for the threshold connectivity problem that obtains an explicit solution and an improved $\tilde{O}(1)$ algorithm for implicit realization when all nodes know each other‚Äôs IDs. These algorithms yield 2-approximations w.r.t. the number of edges. We complement our upper bounds with lower bounds to show that the above algorithms are tight up to factors of $\log n$ . Additionally, we provide algorithms for realizing trees (including a procedure for obtaining a tree with a minimal diameter), an $\tilde{O}(1)$ round algorithm for approximate degree sequence realization and finally an $O(\log ^2 n)$ algorithm for degree sequence realization in the non-preassigned case namely, where the input degree sequence may be permuted among the nodes.},
  archive      = {J_TPDS},
  author       = {John Augustine and Keerti Choudhary and Avi Cohen and David Peleg and Sumathi Sivasubramaniam and Suman Sourav},
  doi          = {10.1109/TPDS.2021.3104239},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1321-1337},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Distributed graph realizations},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Taskflow: A lightweight parallel and heterogeneous task
graph computing system. <em>TPDS</em>, <em>33</em>(6), 1303‚Äì1320. (<a
href="https://doi.org/10.1109/TPDS.2021.3104255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taskflow aims to streamline the building of parallel and heterogeneous applications using a lightweight task graph-based approach. Taskflow introduces an expressive task graph programming model to assist developers in the implementation of parallel and heterogeneous decomposition strategies on a heterogeneous computing platform. Our programming model distinguishes itself as a very general class of task graph parallelism with in-graph control flow to enable end-to-end parallel optimization. To support our model with high performance, we design an efficient system runtime that solves many of the new scheduling challenges arising out of our models and optimizes the performance across latency, energy efficiency, and throughput. We have demonstrated the promising performance of Taskflow in real-world applications. As an example, Taskflow solves a large-scale machine learning workload up to 29\% faster, 1.5√ó less memory, and 1.9√ó higher throughput than the industrial system, oneTBB, on a machine of 40 CPUs and 4 GPUs. We have opened the source of Taskflow and deployed it to large numbers of users in the open-source community.},
  archive      = {J_TPDS},
  author       = {Tsung-Wei Huang and Dian-Lun Lin and Chun-Xun Lin and Yibo Lin},
  doi          = {10.1109/TPDS.2021.3104255},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1303-1320},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Taskflow: A lightweight parallel and heterogeneous task graph computing system},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DS-ADMM++: A novel distributed quantized ADMM to speed up
differentially private matrix factorization. <em>TPDS</em>,
<em>33</em>(6), 1289‚Äì1302. (<a
href="https://doi.org/10.1109/TPDS.2021.3110104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix factorization is a powerful method to implement collaborative filtering recommender systems. This article addresses two major challenges, privacy and efficiency, which matrix factorization is facing. We based our work on DS-ADMM, a distributed matrix factorization algorithm with decent efficiency, to achieve the following two pieces of work: (1) Integrated local differential privacy paradigm into DS-ADMM to provide the privacy-preserving property; (2) Introduced a stochastic quantized function to reduce transmission overheads in ADMM to further improve efficiency. We named our work DS-ADMM++, in which one ‚Äô+‚Äô refers to differential privacy, and the other ‚Äô+‚Äô refers to quantized techniques. DS-ADMM++ is the first to perform efficient and private matrix factorization under the scenarios of differential privacy and DS-ADMM. We conducted experiments with benchmark data sets to demonstrate that our approach provides differential privacy and excellent scalability with a decent loss of accuracy.},
  archive      = {J_TPDS},
  author       = {Feng Zhang and Erkang Xue and Ruixin Guo and Guangzhi Qu and Gansen Zhao and Albert Y. Zomaya},
  doi          = {10.1109/TPDS.2021.3110104},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1289-1302},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DS-ADMM++: A novel distributed quantized ADMM to speed up differentially private matrix factorization},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Power log‚Äôn‚Äôroll: Power-efficient localized rollback for MPI
applications using message logging protocols. <em>TPDS</em>,
<em>33</em>(6), 1276‚Äì1288. (<a
href="https://doi.org/10.1109/TPDS.2021.3107745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In fault tolerance for parallel and distributed systems, message logging protocols have played a prominent role in the last three decades. Such protocols enable local rollback to provide recovery from fail-stop errors. Global rollback techniques can be straightforward to implement but at times lead to slower recovery than local rollback. Local rollback is more complicated but can offer faster recovery times. In this work, we study the power and energy efficiency implications of global and local rollback. We propose a power-efficient version of local rollback to reduce power consumption for non-critical, blocked processes, using Dynamic Voltage and Frequency Scaling (DVFS) and clock modulation (CM). Our results for 3 different MPI codes on 2 parallel systems show that power-efficient local rollback reduces CPU energy waste up to 50\% during the recovery phase, compared to existing global and local rollback techniques, without introducing significant overheads. Furthermore, we show that savings manifest for all blocked processes, which grow linearly with the process count. We estimate that for settings with high recovery overheads the total energy waste of parallel codes is reduced with the proposed local rollback.},
  archive      = {J_TPDS},
  author       = {Kiril Dichev and Daniele De Sensi and Dimitrios S. Nikolopoulos and Kirk W. Cameron and Ivor Spence},
  doi          = {10.1109/TPDS.2021.3107745},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1276-1288},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Power Log‚Äôn‚ÄôRoll: Power-efficient localized rollback for MPI applications using message logging protocols},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DH-SVRF: A reconfigurable unicast/multicast forwarding for
high-performance packet forwarding engines. <em>TPDS</em>,
<em>33</em>(5), 1262‚Äì1275. (<a
href="https://doi.org/10.1109/TPDS.2021.3108899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-performance multicast-enabled packet forwarding engines (PFEs), as an essential component of high-end switches, use a polynomial-time membership query algorithm to determine which port(s) the data packet should be forwarded. The currently widely used query algorithm is Bloom Filter (BF), which has been proven to have many fatal flaws. Another error-free membership query algorithm includes Scalar-pair Vectors Routing Forwarding (SVRF), Fractional- N Scalar-pair Vectors Routing Forwarding (Frac- N SVRF), and the Per-Port Prime Filter Array (P 3 FA) also have some shortcomings in space and time efficiencies. In this paper, we proposed a hybrid strategy: Divaricate Heterogeneous SVRF (DH-SVRF) scheme, which based on the P 3 FA and Frac- N SVRF, which randomly divides all member ships into N groups, and each group has the same structure and is independent of each other to obtain higher time efficiency and space utilization. Finally, we also discussed the selection of the optimal egress-diversity threshold. Through mathematical modeling and simulation, we validate that the proposed DH-SVRF scheme is superior to the SVRF/Frac -N SVRF and traditional BF in terms of scalability, space utilization, and time efficiency in specific conditions such as appropriate egress-diversity thresholds.},
  archive      = {J_TPDS},
  author       = {Zhu Jin and Wen-Kang Jia},
  doi          = {10.1109/TPDS.2021.3108899},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1262-1275},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DH-SVRF: A reconfigurable Unicast/Multicast forwarding for high-performance packet forwarding engines},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A practical framework for secure document retrieval in
encrypted cloud file systems. <em>TPDS</em>, <em>33</em>(5), 1246‚Äì1261.
(<a href="https://doi.org/10.1109/TPDS.2021.3107752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of cloud computing, more and more data owners are motivated to outsource their documents to the cloud and share them with the authorized data users securely and flexibly. To protect data privacy, the documents are generally encrypted before being outsourced to the cloud and hence their searchability decreases. Though many privacy-preserving document search schemes have been proposed, they cannot reach a proper balance among functionality, flexibility, security and efficiency. In this paper, a new encrypted document retrieval system is designed and a proxy server is integrated into the system to alleviate data owner&#39;s workload and improve the whole system&#39;s security level. In this process, we consider a more practical and stronger threat model in which the cloud server can collude with a small number of data users. To support multiple document search patterns, we construct two AVL trees for the filenames and authors, and a Hierarchical Retrieval Features tree (HRF tree) for the document vectors. A depth-first search algorithm is designed for the HRF tree and the Enhanced Asymmetric Scalar-Product-Preserving Encryption (Enhanced ASPE) algorithm is utilized to encrypt the HRF tree. All the three index trees are linked with each other to efficiently support the search requests with multiple parameters. Theoretical analysis and simulation results illustrate the security and efficiency of the proposed framework.},
  archive      = {J_TPDS},
  author       = {Junsong Fu and Na Wang and Baojiang Cui and Bharat K. Bhargava},
  doi          = {10.1109/TPDS.2021.3107752},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1246-1261},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A practical framework for secure document retrieval in encrypted cloud file systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Workload balancing via graph reordering on multicore
systems. <em>TPDS</em>, <em>33</em>(5), 1231‚Äì1245. (<a
href="https://doi.org/10.1109/TPDS.2021.3105323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a shared-memory multicore system, the intrinsic irregular data structure of graphs leads to poor cache utilization, and therefore deteriorates the performance of graph analytics. To address the problem, prior works have proposed a variety of lightweight reordering methods with focus on the optimization of cache locality. However, there is a compromise between cache locality and workload balance. Little insight has been devoted into the issue of workload imbalance for the underlying multicore system, which degrades the effectiveness of parallel graph processing. In this work, a measurement approach is proposed to quantify the imbalance incurred by the concentration of vertices. Inspired by it, we present Cache-aware Reorder (Corder) , a lightweight reordering method exploiting the cache hierarchy of multicore systems. At the shared-memory level, Corder promotes even distribution of computation loads amongst multicores. At the private-cache level, Corder facilitates cache efficiency by applying further refinement to local vertex order. Comprehensive performance evaluation of Corder is conducted on various graph applications and datasets. Experimental results show that Corder yields speedup of up to $2.59\times$ and on average $1.45\times$ , which significantly outperforms existing lightweight reordering methods. To identify the root causes of performance boost delivered by Corder, multicore activities are investigated in terms of thread behavior, cache efficiency, and memory utilization. Statistical analysis demonstrates that the issue of imbalanced thread execution time dominates other factors in determining the overall graph processing time. Moreover, Corder achieves remarkable advantages in cross-platform scalability and reordering overhead.},
  archive      = {J_TPDS},
  author       = {YuAng Chen and Yeh-Ching Chung},
  doi          = {10.1109/TPDS.2021.3105323},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1231-1245},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Workload balancing via graph reordering on multicore systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mapping-aware kernel partitioning method for CGRAs assisted
by deep learning. <em>TPDS</em>, <em>33</em>(5), 1213‚Äì1230. (<a
href="https://doi.org/10.1109/TPDS.2021.3107746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coarse-grained reconfigurable architectures (CGRAs) provide high energy efficiency with word-level programmability rather than bit-level ones such as FPGAs. The coarser reconfigurability brings about higher energy efficiency and reduces the complexity of compiler tasks compared to the FPGAs. However, application mapping process for CGRAs is still time-consuming. When the compiler tries to map a large and complicated application data-flow-graph(DFG) onto the reconfigurable fabric, it tends to result in inefficient resource use or to fail in mapping. In case of failure, the compiler must divide it into several sub-DFGs and goes back to the same flow. In this work, we propose a novel partitioning method based on a genetic algorithm to eliminate the unmappable DFGs and improve the mapping quality. In order not to generate unmappable sub-DFGs, we also propose an estimation model which predicts the mappability and resource requirements using a DGCNN (Deep Graph Convolutional Neural Network). The genetic algorithm with this model can seek the most resource-efficient mapping without the back-end mapping process. Our model can predict the mappability with more than 98\% accuracy and resource usage with a negligible error for two studied CGRAs. Besides, the proposed partitioning method demonstrates 53-75\% of memory saving, 1.28-1.39x higher throughput, and better mapping quality over three comparative approaches.},
  archive      = {J_TPDS},
  author       = {Takuya Kojima and Ayaka Ohwada and Hideharu Amano},
  doi          = {10.1109/TPDS.2021.3107746},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1213-1230},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Mapping-aware kernel partitioning method for CGRAs assisted by deep learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Maximizing user service satisfaction for delay-sensitive IoT
applications in edge computing. <em>TPDS</em>, <em>33</em>(5),
1199‚Äì1212. (<a href="https://doi.org/10.1109/TPDS.2021.3107137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) technology provisions unprecedented opportunities to evolve the interconnection among human beings. However, the latency brought by unstable wireless networks and computation failures caused by limited resources on IoT devices prevents users from experiencing high efficiency and seamless user experience. To address these shortcomings, the integrated Mobile Edge Computing (MEC) with remote clouds is a promising platform to enable delay-sensitive service provisioning for IoT applications, where edge-clouds (cloudlets) are co-located with wireless access points in the proximity of IoT devices. Thus, computation-intensive and sensing data from IoT devices can be offloaded to the MEC network immediately for processing, and the service response latency can be significantly reduced. In this paper, we first formulate two novel optimization problems for delay-sensitive IoT applications, i.e., the total utility maximization problems under both static and dynamic offloading task request settings, with the aim to maximize the accumulative user satisfaction on the use of the services provided by the MEC, and show the NP-hardness of the defined problems. We then devise efficient approximation and online algorithms with provable performance guarantees for the problems in a special case where the bandwidth capacity constraint is negligible. We also develop efficient heuristic algorithms for the problems with the bandwidth capacity constraint. We finally evaluate the performance of the proposed algorithms through experimental simulations. Experimental results demonstrate that the proposed algorithms are promising in reducing service delays and enhancing user satisfaction, and the proposed algorithms outperform their counterparts by at least 10.8 percent.},
  archive      = {J_TPDS},
  author       = {Jing Li and Weifa Liang and Wenzheng Xu and Zichuan Xu and Xiaohua Jia and Wanlei Zhou and Jin Zhao},
  doi          = {10.1109/TPDS.2021.3107137},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1199-1212},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Maximizing user service satisfaction for delay-sensitive IoT applications in edge computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards revenue-driven multi-user online task offloading in
edge computing. <em>TPDS</em>, <em>33</em>(5), 1185‚Äì1198. (<a
href="https://doi.org/10.1109/TPDS.2021.3105325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) has become an attractive solution to enhance the computing and storage capacity of mobile devices by leveraging available resources on edge nodes. In MEC, the arrivals of tasks are highly dynamic and are hard to predict precisely. It is of great importance yet very challenging to assign the tasks to edge nodes with guaranteed system performance. In this article, we aim to optimize the revenue earned by each edge node by optimally offloading tasks to the edge nodes. We formulate the revenue-driven online task offloading (ROTO) problem, which is proved to be NP-hard. We first relax ROTO to a linear fractional programming problem, for which we propose the Level Balanced Allocation (LBA) algorithm. We then show the performance guarantee of LBA through rigorous theoretical analysis, and present the LB-Rounding algorithm for ROTO using the primal-dual technique. The algorithm achieves an approximation ratio of $2(1+\xi)\ln (d+1)$ with a considerable probability, where $d$ is the maximum number of process slots of an edge node and $\xi$ is a small constant. The performance of the proposed algorithm is validated through both trace-driven simulations and testbed experiments. Results show that our proposed scheme is more efficient compared to baseline algorithms.},
  archive      = {J_TPDS},
  author       = {Zhi Ma and Sheng Zhang and Zhiqi Chen and Tao Han and Zhuzhong Qian and Mingjun Xiao and Ning Chen and Jie Wu and Sanglu Lu},
  doi          = {10.1109/TPDS.2021.3105325},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1185-1198},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards revenue-driven multi-user online task offloading in edge computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DIESEL+: Accelerating distributed deep learning tasks on
image datasets. <em>TPDS</em>, <em>33</em>(5), 1173‚Äì1184. (<a
href="https://doi.org/10.1109/TPDS.2021.3104252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We observe that data access and processing takes a significant amount of time in large-scale deep learning training tasks (DLTs) on image datasets. Three factors contribute to this problem: (1) the massive and recurrent accesses to large numbers of small files; (2) the repeated, expensive decoding computation on each image, and (3) the frequent communication between computation nodes and storage nodes. Existing work has addressed some aspects of these problems; however, no end-to-end solutions have been proposed. In this article, we propose DIESEL+, an all-in-one system which accelerates the entire I/O pipeline of deep learning training tasks. DIESEL+ contains several components: (1) local metadata snapshot; (2) per-task distributed caching; (3) chunk-wise shuffling; (4) GPU-assisted image decoding and (5) online region-of-interest (ROI) decoding. The metadata snapshot removes the bottleneck on metadata access in frequent reading of large numbers of files. The per-task distributed cache across the worker nodes of a DLT task to reduce the I/O pressure on the underlying storage. The chunk-based shuffle method converts small file reads into large chunk reads, so that the performance is improved without sacrificing the training accuracy. The GPU-assisted image decoding and the online ROI method minimize the image decoding workloads and reduce the cost of data movement between nodes. These techniques are seamlessly integrated into the system. In our experiments, DIESEL+ outperforms existing systems by a factor of two to three times on the overall training time.},
  archive      = {J_TPDS},
  author       = {Lipeng Wang and Qiong Luo and Shengen Yan},
  doi          = {10.1109/TPDS.2021.3104252},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1173-1184},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DIESEL+: Accelerating distributed deep learning tasks on image datasets},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online reconfiguration of IoT applications in the fog: The
information-coordination trade-off. <em>TPDS</em>, <em>33</em>(5),
1156‚Äì1172. (<a href="https://doi.org/10.1109/TPDS.2021.3097281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of the Internet of Things (IoT) is driving an extraordinary growth of traffic and processing demands, persuading 5G players to change their infrastructures. In this context, Fog computing emerges as a potential solution, providing nearby resources to run IoT applications. However, the Fog raises several challenges which hinders its adoption. In this article, we consider the reconfiguration problem , i.e., how to dynamically adapt the placement of IoT applications running on the Fog, depending on application needs and evolution of resource usage. We propose and evaluate a series of reconfiguration algorithms, based on both online scheduling and online learning approaches. Through an extensive set of experiments in a realistic testbed, we demonstrate that the performance strongly depends on the quality and availability of information from both Fog infrastructure and IoT applications. This information mainly concerns the application‚Äôs resource usage (estimated by the user during the design of the application) and the availability of resources in the infrastructure (collected by commercial off-the-shelf monitoring tools). Finally, we show that a reactive and greedy strategy, which relies on this additional information, can overcome the performance of state-of-the-art online learning algorithms, even in a scenario with inaccurate information.},
  archive      = {J_TPDS},
  author       = {Bruno Donassolo and Arnaud Legrand and Panayotis Mertikopoulos and Ilhem Fajjari},
  doi          = {10.1109/TPDS.2021.3097281},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1156-1172},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Online reconfiguration of IoT applications in the fog: The information-coordination trade-off},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data, user and power allocations for caching in multi-access
edge computing. <em>TPDS</em>, <em>33</em>(5), 1144‚Äì1155. (<a
href="https://doi.org/10.1109/TPDS.2021.3104241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the multi-access edge computing (MEC) environment, app vendors‚Äô data can be cached on edge servers to ensure low-latency data retrieval. Massive users can simultaneously access edge servers with high data rates through flexible allocations of transmit power. The ability to manage networking resources offers unique opportunities to app vendors but also raises unprecedented challenges. To ensure fast data retrieval for users in the MEC environment, edge data caching must take into account the allocations of data, users, and transmit power jointly. We make the first attempt to study the Data, User, and Power Allocation (DUPA $^3$ ) problem, aiming to serve the most users and maximize their overall data rate. First, we formulate the DUPA $^3$ problem and prove its $\mathcal {NP}$ -completeness. Then, we model the DUPA $^3$ problem as a potential DUPA $^3$ game admitting at least one Nash equilibrium and propose a two-phase game-theoretic decentralized algorithm named DUPA $^3$ Game to achieve the Nash equilibrium as the solution to the DUPA $^3$ problem. To evaluate DUPA $^3$ Game, we analyze its theoretical performance and conduct extensive experiments to demonstrate its effectiveness and efficiency.},
  archive      = {J_TPDS},
  author       = {Xiaoyu Xia and Feifei Chen and Qiang He and Guangming Cui and John C. Grundy and Mohamed Abdelrazek and Xiaolong Xu and Hai Jin},
  doi          = {10.1109/TPDS.2021.3104241},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1144-1155},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Data, user and power allocations for caching in multi-access edge computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Elastic parameter server: Accelerating ML training with
scalable resource scheduling. <em>TPDS</em>, <em>33</em>(5), 1128‚Äì1143.
(<a href="https://doi.org/10.1109/TPDS.2021.3104242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter server (PS) based on worker-server communication is designed for distributed machine learning (ML) training in clusters. In feedback-driven exploration of ML model training, users exploit early feedback from each job to decide whether to kill the job or keep it running so as to find the optimal model configuration. However, PS does not support adjusting the number of workers and servers of a job at runtime. It becomes the bottleneck of scalable distributed ML training because the cluster resources cannot be dynamically allocated or deallocated to jobs, resulting in significant early feedback latency and resource under-utilization. This article rethinks the principle of PS architecture. We present Elastic Parameter Server (EPS), a lightweight and user-transparent PS that accelerates feedback-driven exploration for distributed ML training. EPS allows to remove a subset of workers and servers from running jobs and allocate the released resources to an incoming job at runtime so as to reduce its early feedback latency. It can also use the released resources from a killed job to add workers and servers to running jobs to improve resource utilization and the training speed. We develop a heuristic scheduler that leverages EPS and offers scalable resource scheduling for multiple ML jobs. We implement EPS in Tencent Angel and the scheduler in Apache Yarn, and conduct evaluations with various ML models. Experimental results show that EPS achieves up to 1.5x improvement on the ML training speed compared to PS.},
  archive      = {J_TPDS},
  author       = {Shaoqi Wang and Aidi Pi and Xiaobo Zhou},
  doi          = {10.1109/TPDS.2021.3104242},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1128-1143},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Elastic parameter server: Accelerating ML training with scalable resource scheduling},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Addictive incentive mechanism in crowdsensing from the
perspective of behavioral economics. <em>TPDS</em>, <em>33</em>(5),
1109‚Äì1127. (<a href="https://doi.org/10.1109/TPDS.2021.3104247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mobile crowdsensing, many mobile devices are collectively used to complete complex sensing tasks. Most tasks require users to consume resources to ensure continuous performance over multiple periods of time. Therefore, it is important to incentivize enough users to continuously participate in the tasks. However, there are two issues with current incentive mechanisms. First, most studies are designed for maximizing the revenue of a single round of tasks rather than long-term incentives. Second, although some studies use historical data to design mechanisms for long-term operation, the law of diminishing marginal utility is not considered; thus, the actual performance is lower than expected. In this study, the concepts of capital deposit and intertemporal choice from behavioral economics are introduced to explain the principle of addiction, which is a representative long-term incentive. Consequently, an Addiction Incentive Mechanism (AIM) is proposed. It influences the utility and demand functions of users by accelerating the accumulation of capital deposits and promoting users to become addicted to cooperative behavior. It also mitigates the effect of diminishing marginal utility through intertemporal choice theory to maintain user engagement. Simulations demonstrate that AIM improves participation and repetition rates compared with the state-of-the-art mechanisms.},
  archive      = {J_TPDS},
  author       = {Jiaqi Liu and Shiyue Huang and Deng Li and Sheng Wen and Hui Liu},
  doi          = {10.1109/TPDS.2021.3104247},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1109-1127},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Addictive incentive mechanism in crowdsensing from the perspective of behavioral economics},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parallel and asynchronous smart contract execution.
<em>TPDS</em>, <em>33</em>(5), 1097‚Äì1108. (<a
href="https://doi.org/10.1109/TPDS.2021.3095234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today&#39;s blockchains suffer from low throughput and high latency, which impedes their widespread adoption of more complex applications like smart contracts. In this article, we propose a novel paradigm for smart contract execution. It distinguishes between consensus nodes and execution nodes: different groups of execution nodes can execute transactions in parallel; meanwhile, consensus nodes can asynchronously order transactions and process execution results. Moreover, it requires no coordination among execution nodes and can effectively prevent livelocks. We show two ways of applying this paradigm to blockchains. First, we show how we can make Ethereum support parallel and asynchronous contract execution without hard-forks . Then, we propose a new public, permissionless blockchain. Our benchmark shows that, with a fast consensus layer, it can provide a high throughput even for complex transactions like Cryptokitties gene mixing. It can also protect simple transactions from being starved by complex transactions.},
  archive      = {J_TPDS},
  author       = {Jian Liu and Peilun Li and Raymond Cheng and N. Asokan and Dawn Song},
  doi          = {10.1109/TPDS.2021.3095234},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1097-1108},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Parallel and asynchronous smart contract execution},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parallel and distributed structured SVM training.
<em>TPDS</em>, <em>33</em>(5), 1084‚Äì1096. (<a
href="https://doi.org/10.1109/TPDS.2021.3101155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structured Support Vector Machines (structured SVMs) are a fundamental machine learning algorithm, and have solid theoretical foundation and high effectiveness in applications such as natural language parsing and computer vision. However, training structured SVMs is very time-consuming, due to the large number of constraints and inferior convergence rates, especially for large training data sets. The high cost of training structured SVMs has hindered its adoption to new applications. In this article, we aim to improve the efficiency of structured SVMs by proposing a parallel and distributed solution (namely FastSSVM ) for training structured SVMs building on top of MPI and OpenMP. FastSSVM exploits a series of optimizations (e.g., optimizations on data storage and synchronization) to efficiently use the resources of the nodes in a cluster and the cores of the nodes. Moreover, FastSSVM tackles the large constraint set problem by batch processing and addresses the slow convergence challenge by adapting stop conditions based on the improvement of each iteration. We theoretically prove that our solution is guaranteed to converge to a global optimum. A comprehensive experimental study shows that FastSSVM can achieve at least four times speedup over the existing solutions, and in some cases can achieve two to three orders of magnitude speedup.},
  archive      = {J_TPDS},
  author       = {Jiantong Jiang and Zeyi Wen and Zeke Wang and Bingsheng He and Jian Chen},
  doi          = {10.1109/TPDS.2021.3101155},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1084-1096},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Parallel and distributed structured SVM training},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient and accurate flow record collection with HashFlow.
<em>TPDS</em>, <em>33</em>(5), 1069‚Äì1083. (<a
href="https://doi.org/10.1109/TPDS.2021.3099442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional tools like NetFlow face great challenges as both the speed and the complexity of the network traffic increase. To keep the pace up, we propose HashFlow for more efficient and accurate collection of flow records. HashFlow keeps large flows in its main flow table and uses an ancillary table to summarize the other flows when the main table is full. With our flow collision resolution and flow record promotion schemes, a flow in the ancillary table is promoted back to the main flow table with a guaranteed probability when it becomes large enough. These operations can be performed highly efficiently, so HashFlow can keep up with ultra-high traffic speed. We implement HashFlow in a Tofino switch, and using traces from different operational networks, we compare its performance against some state-of-the-art flow measurement algorithms. Our experiments show that, for various types of traffic analysis applications, HashFlow consistently demonstrates clearly better performance than its competitors. For example, the performance of HashFlow in flow size estimation, flow size distribution estimation and heavy hitter detection is up to 21, 60 and 35 percent better than those of the best competitors respectively, and these merits of HashFlow come with almost no degradation of throughput.},
  archive      = {J_TPDS},
  author       = {Zongyi Zhao and Xingang Shi and Zhiliang Wang and Qing Li and Han Zhang and Xia Yin},
  doi          = {10.1109/TPDS.2021.3099442},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1069-1083},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient and accurate flow record collection with HashFlow},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring the galaxyfly family to build flexible-scale
interconnection networks. <em>TPDS</em>, <em>33</em>(5), 1054‚Äì1068. (<a
href="https://doi.org/10.1109/TPDS.2021.3100783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interconnection networks play an essential role in the architecture of high-performance computing (HPC) systems. In this article, we explore the Galaxyfly family to build flexible-scale interconnection networks. Galaxyfly is guaranteed to retain a small constant diameter while achieving a flexible tradeoff between network scale and bisection bandwidth. Galaxyfly not only supports small-scale interconnection networks with smaller diameter but also lowers the demands for high-radix routers and is able to utilize routers with moderate radix to build exascale interconnection networks. We analyze the constructible configuration of Galaxyfly and evaluate the properties of Galaxyfly. We conduct extensive simulations and analysis to evaluate the performance, cost, and power consumption of Galaxyfly on physical layout against state-of-the-art topologies. The results show that our design achieves better performance than most existing topologies under typical HPC workloads, and is cost-effective to deploy for exascale HPC systems.},
  archive      = {J_TPDS},
  author       = {Fei Lei and Dezun Dong and Xiangke Liao},
  doi          = {10.1109/TPDS.2021.3100783},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1054-1068},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Exploring the galaxyfly family to build flexible-scale interconnection networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A low-power transprecision floating-point cluster for
efficient near-sensor data analytics. <em>TPDS</em>, <em>33</em>(5),
1038‚Äì1053. (<a href="https://doi.org/10.1109/TPDS.2021.3101764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent applications in low-power (1-20 mW) near-sensor computing require the adoption of floating-point arithmetic to reconcile high precision results with a wide dynamic range. In this article, we propose a low-power multi-core computing cluster that leverages the fined-grained tunable principles of transprecision computing to provide support to near-sensor applications at a minimum power budget. Our solution ‚Äì based on the open-source RISC-V architecture ‚Äì combines parallelization and sub-word vectorization with a dedicated interconnect design capable of sharing floating-point units (FPUs) among the cores. On top of this architecture, we provide a full-fledged software stack support, including a parallel low-level runtime, a compilation toolchain, and a high-level programming model, with the aim to support the development of end-to-end applications. We performed an exhaustive exploration of the design space of the transprecision cluster on a cycle-accurate FPGA emulator, varying the number of cores and FPUs to maximize performance. Orthogonally, we performed a vertical exploration to identify the most efficient solutions in terms of non-functional requirements (operating frequency, power, and area). We conducted an experimental assessment on a set of benchmarks representative of the near-sensor processing domain, complementing the timing results with a post place-&amp;-route analysis of the power consumption. A comparison with the state-of-the-art shows that our solution outperforms the competitors in energy efficiency, reaching a peak of 97 Gflop/s/W on single-precision scalars and 162 Gflop/s/W on half-precision vectors. Finally, a real-life use case demonstrates the effectiveness of our approach in fulfilling accuracy constraints.},
  archive      = {J_TPDS},
  author       = {Fabio Montagna and Stefan Mach and Simone Benatti and Angelo Garofalo and Gianmarco Ottavi and Luca Benini and Davide Rossi and Giuseppe Tagliavini},
  doi          = {10.1109/TPDS.2021.3101764},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1038-1053},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A low-power transprecision floating-point cluster for efficient near-sensor data analytics},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerating restarted GMRES with mixed precision
arithmetic. <em>TPDS</em>, <em>33</em>(4), 1027‚Äì1037. (<a
href="https://doi.org/10.1109/TPDS.2021.3090757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generalized minimum residual method (GMRES) is a commonly used iterative Krylov solver for sparse, non-symmetric systems of linear equations. Like other iterative solvers, data movement dominates its run time. To improve this performance, we propose running GMRES in reduced precision with key operations remaining in full precision. Additionally, we provide theoretical results linking the convergence of finite precision GMRES with classical Gram-Schmidt with reorthogonalization (CGSR) and its infinite precision counterpart which helps justify the convergence of this method to double-precision accuracy. We tested the mixed-precision approach with a variety of matrices and preconditioners on a GPU-accelerated node. Excluding the incomplete LU factorization without fill in (ILU(0)) preconditioner, we achieved average speedups ranging from 8 to 61 percent relative to comparable double-precision implementations, with the simpler preconditioners achieving the higher speedups.},
  archive      = {J_TPDS},
  author       = {Neil Lindquist and Piotr Luszczek and Jack Dongarra},
  doi          = {10.1109/TPDS.2021.3090757},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1027-1037},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating restarted GMRES with mixed precision arithmetic},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GSoFa: Scalable sparse symbolic LU factorization on GPUs.
<em>TPDS</em>, <em>33</em>(4), 1015‚Äì1026. (<a
href="https://doi.org/10.1109/TPDS.2021.3090316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decomposing a matrix $\mathbf {A}$ into a lower matrix $\mathbf {L}$ and an upper matrix $\mathbf {U}$ , which is also known as LU decomposition, is an essential operation in numerical linear algebra. For a sparse matrix, LU decomposition often introduces more nonzero entries in the $\mathbf {L}$ and $\mathbf {U}$ factors than in the original matrix. A symbolic factorization step is needed to identify the nonzero structures of $\mathbf {L}$ and $\mathbf {U}$ matrices. Attracted by the enormous potentials of the Graphics Processing Units (GPUs), an array of efforts have surged to deploy various LU factorization steps except for the symbolic factorization, to the best of our knowledge, on GPUs. This article introduces gSoFa , the first G PU-based s ymb o lic fa ctorization design with the following three optimizations to enable scalable LU symbolic factorization for nonsymmetric pattern sparse matrices on GPUs. First, we introduce a novel fine-grained parallel symbolic factorization algorithm that is well suited for the Single Instruction Multiple Thread (SIMT) architecture of GPUs. Second, we tailor supernode detection into a SIMT friendly process and strive to balance the workload, minimize the communication and saturate the GPU computing resources during supernode detection. Third, we introduce a three-pronged optimization to reduce the excessive space consumption problem faced by multi-source concurrent symbolic factorization. Taken together, gSoFa achieves up to 31√ó speedup from 1 to 44 Summit nodes (6 to 264 GPUs) and outperforms the state-of-the-art CPU project, on average, by 5√ó. Notably, gSoFa also achieves up to 47 percent of the peak memory throughput of a V100 GPU in the Summit Supercomputer.},
  archive      = {J_TPDS},
  author       = {Anil Gaihre and Xiaoye Sherry Li and Hang Liu},
  doi          = {10.1109/TPDS.2021.3090316},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1015-1026},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GSoFa: Scalable sparse symbolic LU factorization on GPUs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluating spatial accelerator architectures with tiled
matrix-matrix multiplication. <em>TPDS</em>, <em>33</em>(4), 1002‚Äì1014.
(<a href="https://doi.org/10.1109/TPDS.2021.3104240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing interest in custom spatial accelerators for machine learning applications. These accelerators employ a spatial array of processing elements (PEs) interacting via custom buffer hierarchies and networks-on-chip. The efficiency of these accelerators comes from employing optimized dataflow (i.e., spatial/temporal partitioning of data across the PEs and fine-grained scheduling) strategies to optimize data reuse. The focus of this work is to evaluate these accelerator architectures using a tiled general matrix-matrix multiplication (GEMM) kernel. To do so, we develop a framework that finds optimized mappings (dataflow and tile sizes) for a tiled GEMM for a given spatial accelerator and workload combination, leveraging an analytical cost model for runtime and energy. Our evaluations over five spatial accelerators demonstrate that the tiled GEMM mappings systematically generated by our framework achieve high performance on various GEMM workloads and accelerators.},
  archive      = {J_TPDS},
  author       = {Gordon Euhyun Moon and Hyoukjun Kwon and Geonhwa Jeong and Prasanth Chatarasi and Sivasankaran Rajamanickam and Tushar Krishna},
  doi          = {10.1109/TPDS.2021.3104240},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1002-1014},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Evaluating spatial accelerator architectures with tiled matrix-matrix multiplication},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Combinatorial BLAS 2.0: Scaling combinatorial algorithms on
distributed-memory systems. <em>TPDS</em>, <em>33</em>(4), 989‚Äì1001. (<a
href="https://doi.org/10.1109/TPDS.2021.3094091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combinatorial algorithms such as those that arise in graph analysis, modeling of discrete systems, bioinformatics, and chemistry, are often hard to parallelize. The Combinatorial BLAS library implements key computational primitives for rapid development of combinatorial algorithms in distributed-memory systems. During the decade since its first introduction, the Combinatorial BLAS library has evolved and expanded significantly. This article details many of the key technical features of Combinatorial BLAS version 2.0, such as communication avoidance, hierarchical parallelism via in-node multithreading, accelerator support via GPU kernels, generalized semiring support, implementations of key data structures and functions, and scalable distributed I/O operations for human-readable files. Our article also presents several rules of thumb for choosing the right data structures and functions in Combinatorial BLAS 2.0, under various common application scenarios.},
  archive      = {J_TPDS},
  author       = {Ariful Azad and Oguz Selvitopi and Md Taufique Hussain and John R. Gilbert and Aydƒ±n Bulu√ß},
  doi          = {10.1109/TPDS.2021.3094091},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {989-1001},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Combinatorial BLAS 2.0: Scaling combinatorial algorithms on distributed-memory systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LibEnsemble: A library to coordinate the concurrent
evaluation of dynamic ensembles of calculations. <em>TPDS</em>,
<em>33</em>(4), 977‚Äì988. (<a
href="https://doi.org/10.1109/TPDS.2021.3082815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Almost all applications stop scaling at some point; those that don&#39;t are seldom performant when considering time to solution on anything but aspirational/unicorn resources. Recognizing these tradeoffs as well as greater user functionality in a near-term exascale computing era, we present libEnsemble, a library aimed at particular scalability- and capability-stretching uses. libEnsemble enables running concurrent instances of an application in dynamically allocated ensembles through an extensible Python library. We highlight the structure, execution, and capabilities of the library on leading pre-exascale environments as well as advanced capabilities for exascale environments and beyond.},
  archive      = {J_TPDS},
  author       = {Stephen Hudson and Jeffrey Larson and John-Luke Navarro and Stefan M. Wild},
  doi          = {10.1109/TPDS.2021.3082815},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {977-988},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {LibEnsemble: A library to coordinate the concurrent evaluation of dynamic ensembles of calculations},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerating geostatistical modeling and prediction with
mixed-precision computations: A high-productivity approach with PaRSEC.
<em>TPDS</em>, <em>33</em>(4), 964‚Äì976. (<a
href="https://doi.org/10.1109/TPDS.2021.3084071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geostatistical modeling, one of the prime motivating applications for exascale computing, is a technique for predicting desired quantities from geographically distributed data, based on statistical models and optimization of parameters. Spatial data are assumed to possess properties of stationarity or non-stationarity via a kernel fitted to a covariance matrix. A primary workhorse of stationary spatial statistics is Gaussian maximum log-likelihood estimation (MLE), whose central data structure is a dense, symmetric positive definite covariance matrix of the dimension of the number of correlated observations. Two essential operations in MLE are the application of the inverse and evaluation of the determinant of the covariance matrix. These can be rendered through the Cholesky decomposition and triangular solution. In this contribution, we reduce the precision of weakly correlated locations to single- or half- precision based on distance. We thus exploit mathematical structure to migrate MLE to a three-precision approximation that takes advantage of contemporary architectures offering BLAS3-like operations in a single instruction that are extremely fast for reduced precision. We illustrate application-expected accuracy worthy of double-precision from a majority half-precision computation, in a context where uniform single-precision is by itself insufficient. In tackling the complexity and imbalance caused by the mixing of three precisions, we deploy the PaRSEC runtime system. PaRSEC delivers on-demand casting of precisions while orchestrating tasks and data movement in a multi-GPU distributed-memory environment within a tile-based Cholesky factorization. Application-expected accuracy is maintained while achieving up to $1.59X$ by mixing FP64/FP32 operations on 1536 nodes of HAWK or 4096 nodes of Shaheen II , and up to $2.64X$ by mixing FP64/FP32/FP16 operations on 128 nodes of Summit , relative to FP64-only operations. This translates into up to 4.5, 4.7, and 9.1 (mixed) PFlop/s sustained performance, respectively, demonstrating a synergistic combination of exascale architecture, dynamic runtime software, and algorithmic adaptation applied to challenging environmental problems.},
  archive      = {J_TPDS},
  author       = {Sameh Abdulah and Qinglei Cao and Yu Pei and George Bosilca and Jack Dongarra and Marc G. Genton and David E. Keyes and Hatem Ltaief and Ying Sun},
  doi          = {10.1109/TPDS.2021.3084071},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {964-976},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating geostatistical modeling and prediction with mixed-precision computations: A high-productivity approach with PaRSEC},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VPIC 2.0: Next generation particle-in-cell simulations.
<em>TPDS</em>, <em>33</em>(4), 952‚Äì963. (<a
href="https://doi.org/10.1109/TPDS.2021.3084795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {VPIC is a general purpose particle-in-cell simulation code for modeling plasma phenomena such as magnetic reconnection, fusion, solar weather, and laser-plasma interaction in three dimensions using large numbers of particles. VPIC&#39;s capacity in both fidelity and scale makes it particularly well-suited for plasma research on pre-exascale and exascale platforms. In this article, we demonstrate the unique challenges involved in preparing the VPIC code for operation at exascale, outlining important optimizations to make VPIC efficient on accelerators. Specifically, we show the work undertaken in adapting VPIC to exploit the portability-enabling framework Kokkos and highlight the enhancements to VPIC&#39;s modeling capabilities to achieve performance at exascale. We assess the achieved performance-portability trade-off through a suite of studies on nine different varieties of modern pre-exascale hardware. Our performance-portability study includes weak-scaling runs on three of the top ten TOP500 supercomputers, as well as a comparison of low-level system performance of hardware from four different vendors.},
  archive      = {J_TPDS},
  author       = {Robert Bird and Nigel Tan and Scott V. Luedtke and Stephen Lien Harrell and Michela Taufer and Brian Albright},
  doi          = {10.1109/TPDS.2021.3084795},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {952-963},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {VPIC 2.0: Next generation particle-in-cell simulations},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TianheGraph: Customizing graph search for graph500 on tianhe
supercomputer. <em>TPDS</em>, <em>33</em>(4), 941‚Äì951. (<a
href="https://doi.org/10.1109/TPDS.2021.3100785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the era of exascale supercomputing is coming, it is vital for next-generation supercomputers to find appropriate applications with high social and economic benefit. In recent years, it has been widely accepted that extremely-large graph computation is a promising killer application for supercomputing. Although Tianhe series supercomputers are leading in the world-wide competition of supercomputing (ranked No. 1 in the Top500 list for six times), previously they had been inefficient in graph computation according to the Graph500 list. This is mainly because the previous graph processing system cannot leverage the advanced hardware features of Tianhe supercomputers. To address the problem, in this paper we present our integrated optimizations for improving the graph computation performance on our next-generation Tianhe supercomputing system, mainly including sorting with buffering for heavy vertices, vectorized searching with SVE (Scalable Vector Extension) on matrix2000+ CPUs, and group communication on the proprietary interconnection network. Performance evaluation on a subset of the Tianhe supercomputer (with 512 nodes and 196,608 cores) shows that our customized graph processing system effectively improves the graph search performance and achieves the BFS performance of 2131.98 GTEPS.},
  archive      = {J_TPDS},
  author       = {Xinbiao Gan and Yiming Zhang and Ruibo Wang and Tiejun Li and Tiaojie Xiao and Ruigeng Zeng and Jie Liu and Kai Lu},
  doi          = {10.1109/TPDS.2021.3100785},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {941-951},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {TianheGraph: Customizing graph search for graph500 on tianhe supercomputer},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A parallel algorithm template for updating single-source
shortest paths in large-scale dynamic networks. <em>TPDS</em>,
<em>33</em>(4), 929‚Äì940. (<a
href="https://doi.org/10.1109/TPDS.2021.3084096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Single Source Shortest Path (SSSP) problem is a classic graph theory problem that arises frequently in various practical scenarios; hence, many parallel algorithms have been developed to solve it. However, these algorithms operate on static graphs, whereas many real-world problems are best modeled as dynamic networks, where the structure of the network changes with time. This gap between the dynamic graph modeling and the assumed static graph model in the conventional SSSP algorithms motivates this work. We present a novel parallel algorithmic framework for updating the SSSP in large-scale dynamic networks and implement it on the shared-memory and GPU platforms. The basic idea is to identify the portion of the network affected by the changes and update the information in a rooted tree data structure that stores the edges of the network that are most relevant to the analysis. Extensive experimental evaluations on real-world and synthetic networks demonstrate that our proposed parallel updating algorithm is scalable and, in most cases, requires significantly less execution time than the state-of-the-art recomputing-from-scratch algorithms.},
  archive      = {J_TPDS},
  author       = {Arindam Khanda and Sriram Srinivasan and Sanjukta Bhowmick and Boyana Norris and Sajal K. Das},
  doi          = {10.1109/TPDS.2021.3084096},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {929-940},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A parallel algorithm template for updating single-source shortest paths in large-scale dynamic networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Characterizing performance of graph neighborhood
communication patterns. <em>TPDS</em>, <em>33</em>(4), 915‚Äì928. (<a
href="https://doi.org/10.1109/TPDS.2021.3101425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed-memory graph algorithms are fundamental enablers in scientific computing and analytics workflows. A majority of graph algorithms rely on the graph neighborhood communication pattern, i.e., repeated asynchronous communication between a vertex and its neighbors in the graph. The pattern is adversarial for communication software and hardware due to high message injection rates and input-dependent, many-to-one traffic with variable destinations and volumes. We present benchmarks and performance analysis of graph neighborhood communication on modern large-scale network interconnects from four supercomputers: ALCF Theta, NERSC Cori, OLCF Summit and R-CCS Fugaku. Our benchmarks characterize communication from the perspectives of latency and throughput. Benchmark parameters make it possible to mimic the behaviors of complex applications on real world or synthetic graphs by varying work distribution, remote edges, message volume, and per-vertex work. We find that minor changes in the input graph can substantially increase latencies; and contention can develop in memory caches and network stacks before contention in the network itself. Further, latencies and contention vary significantly for different graph neighborhoods, motivating the need for exploring asynchronous algorithms in greater detail. When adding work, load imbalance on real-world graphs can be pronounced: latencies for the 99th percentile were 8‚Äì128√ó than the corresponding average latencies. Our results help analysts and developers understand the performance implications of this important pattern, especially for the impending exascale platforms.},
  archive      = {J_TPDS},
  author       = {Sayan Ghosh and Nathan R. Tallent and Mahantesh Halappanavar},
  doi          = {10.1109/TPDS.2021.3101425},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {915-928},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Characterizing performance of graph neighborhood communication patterns},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerating HDF5 i/o for exascale using DAOS.
<em>TPDS</em>, <em>33</em>(4), 903‚Äì914. (<a
href="https://doi.org/10.1109/TPDS.2021.3097884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Hierarchical Data Format 5 (HDF5) has long been defined as one of the most prominent data models, binary file formats and I/O libraries for storing and managing scientific data. Introduced in the late 90s when POSIX I/O was the standard, the library has since then been continuously improved to respond and adapt to the ever-growing demands of high-performance computing (HPC) software and hardware. Given the limitations of POSIX I/O and with the emergence of new technologies such as object stores, non-volatile memory, and SSDs, the need for an interface that can efficiently store and access data at scale through new paradigms has become more and more pressing. The Distributed Asynchronous Object Storage (DAOS) file system is an emerging file system that aims at responding to those demands by taking disk-based storage out of the loop. We present in this article the research efforts that have been taking place to prepare the HDF5 library for Exascale using DAOS. By enabling and defining a new storage file format, we focus on the benefits that it delivers to the applications in terms of features and performance.},
  archive      = {J_TPDS},
  author       = {Jerome Soumagne and Jordan Henderson and Mohamad Chaarawi and Neil Fortner and Scot Breitenfeld and Songyu Lu and Dana Robinson and Elena Pourmal and Johann Lombardi},
  doi          = {10.1109/TPDS.2021.3097884},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {903-914},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating HDF5 I/O for exascale using DAOS},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transparent asynchronous parallel i/o using background
threads. <em>TPDS</em>, <em>33</em>(4), 891‚Äì902. (<a
href="https://doi.org/10.1109/TPDS.2021.3090322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moving toward exascale computing, the size of data stored and accessed by applications is ever increasing. However, traditional disk-based storage has not seen improvements that keep up with the explosion of data volume or the speed of processors. Multiple levels of non-volatile storage devices are being added to handle bursty I/O, however, moving data across the storage hierarchy can take longer than the data generation or analysis. Asynchronous I/O can reduce the impact of I/O latency as it allows applications to schedule I/O early and to check their status later. I/O is thus overlapped with application communication or computation or both, effectively hiding some or all of the I/O latency. POSIX and MPI-I/O provide asynchronous read and write operations, but lack the support for non-data operations such as file open and close. Users also have to manually manage data dependencies and use low-level byte offsets, which requires significant effort and expertise to adopt. In this article, we present an asynchronous I/O framework that supports all types of I/O operations, manages data dependencies transparently and automatically, provides implicit and explicit modes for application flexibility, and error information retrieval. We implemented these techniques in HDF5. Our evaluation of several benchmarks and application workloads demonstrates it effectiveness on hiding the I/O cost from the application.},
  archive      = {J_TPDS},
  author       = {Houjun Tang and Quincey Koziol and John Ravi and Suren Byna},
  doi          = {10.1109/TPDS.2021.3090322},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {891-902},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Transparent asynchronous parallel I/O using background threads},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving i/o performance for exascale applications through
online data layout reorganization. <em>TPDS</em>, <em>33</em>(4),
878‚Äì890. (<a href="https://doi.org/10.1109/TPDS.2021.3100784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The applications being developed within the U.S. Exascale Computing Project (ECP) to run on imminent Exascale computers will generate scientific results with unprecedented fidelity and record turn-around time. Many of these codes are based on particle-mesh methods and use advanced algorithms, especially dynamic load-balancing and mesh-refinement, to achieve high performance on Exascale machines. Yet, as such algorithms improve parallel application efficiency, they raise new challenges for I/O logic due to their irregular and dynamic data distributions. Thus, while the enormous data rates of Exascale simulations already challenge existing file system write strategies, the need for efficient read and processing of generated data introduces additional constraints on the data layout strategies that can be used when writing data to secondary storage. We review these I/O challenges and introduce two online data layout reorganization approaches for achieving good tradeoffs between read and write performance. We demonstrate the benefits of using these two approaches for the ECP particle-in-cell simulation WarpX, which serves as a motif for a large class of important Exascale applications. We show that by understanding application I/O patterns and carefully designing data layouts we can increase read performance by more than 80 percent.},
  archive      = {J_TPDS},
  author       = {Lipeng Wan and Axel Huebl and Junmin Gu and Franz Poeschel and Ana Gainaru and Ruonan Wang and Jieyang Chen and Xin Liang and Dmitry Ganyushin and Todd Munson and Ian Foster and Jean-Luc Vay and Norbert Podhorszki and Kesheng Wu and Scott Klasky},
  doi          = {10.1109/TPDS.2021.3100784},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {878-890},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Improving I/O performance for exascale applications through online data layout reorganization},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enabling scalable and extensible memory-mapped datastores in
userspace. <em>TPDS</em>, <em>33</em>(4), 866‚Äì877. (<a
href="https://doi.org/10.1109/TPDS.2021.3086302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exascale workloads are expected to incorporate data-intensive processing in close coordination with traditional physics simulations. These emerging scientific, data-analytics and machine learning applications need to access a wide variety of datastores in flat files and structured databases. Programmer productivity is greatly enhanced by mapping datastores into the application process&#39;s virtual memory space to provide a unified ‚Äúin-memory‚Äù interface. Currently, memory mapping is provided by system software primarily designed for generality and reliability. However, scalability at high concurrency is a formidable challenge on exascale systems. Also, there is a need for extensibility to support new datastores potentially requiring HPC data transfer services. In this article, we present UMap , a scalable and extensible userspace service for memory-mapping datastores. Through decoupled queue management, concurrency aware adaptation, and dynamic load balancing, UMap enables application performance to scale even at high concurrency. We evaluate UMap in data-intensive applications, including sorting, graph traversal, database operations, and metagenomic analytics. Our results show that UMap as a userspace service outperforms an optimized kernel-based service across a wide range of intra-node concurrency by 1.22-1.9 ${\times}$ . We performed two case studies to demonstrate UMap &#39;s extensibility. First, a new datastore residing in remote memory is incorporated into UMap as an application-specific plugin. Second, we present a persistent memory allocator Metall built atop UMap for unified storage/memory.},
  archive      = {J_TPDS},
  author       = {Ivy B. Peng and Maya B. Gokhale and Karim Youssef and Keita Iwabuchi and Roger Pearce},
  doi          = {10.1109/TPDS.2021.3086302},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {866-877},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Enabling scalable and extensible memory-mapped datastores in userspace},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An automated tool for analysis and tuning of GPU-accelerated
code in HPC applications. <em>TPDS</em>, <em>33</em>(4), 854‚Äì865. (<a
href="https://doi.org/10.1109/TPDS.2021.3094169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The US Department of Energy‚Äôs fastest supercomputers and forthcoming exascale systems employ Graphics Processing Units (GPUs) to increase the computational performance of compute nodes. However, the complexity of GPU architectures makes tailoring sophisticated applications to achieve high performance on GPU-accelerated systems a major challenge. At best, prior performance tools for GPU code only provide coarse-grained tuning advice at the kernel level. In this article, we describe GPA, a performance advisor that suggests potential code optimizations at a hierarchy of levels, including individual lines, loops, and functions. To gather the fine-grained measurements needed to produce such insights, GPA uses instruction sampling and binary instrumentation to monitor execution of GPU code. At the time of this writing, GPU instruction sampling is only available on NVIDIA GPUs. To understand performance losses, GPA uses data flow analysis to approximately attribute measured instruction stalls back to their causes. GPA then analyzes patterns of stalls using information about a program‚Äôs structure and the GPU architecture to identify optimization strategies that address inefficiencies observed. GPA then employs detailed performance models to estimate the potential speedup that each optimization might provide. Experiments with benchmarks and applications show that GPA provides useful advice for tuning GPU code. We applied GPA to analyze and tune a collection of codes on NVIDIA V100 and A100 GPUs. GPA suggested optimizations that it estimates will accelerate performance across the set of codes by a geometric mean of 1.21√ó. Applying these optimizations suggested by GPA accelerated these codes by a geometric mean of 1.19√ó.},
  archive      = {J_TPDS},
  author       = {Keren Zhou and Xiaozhu Meng and Ryuichi Sai and Dejan Grubisic and John Mellor-Crummey},
  doi          = {10.1109/TPDS.2021.3094169},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {854-865},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An automated tool for analysis and tuning of GPU-accelerated code in HPC applications},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The PetscSF scalable communication layer. <em>TPDS</em>,
<em>33</em>(4), 842‚Äì853. (<a
href="https://doi.org/10.1109/TPDS.2021.3084070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PetscSF, the communication component of the Portable, Extensible Toolkit for Scientific Computation (PETSc), is designed to provide PETSc&#39;s communication infrastructure suitable for exascale computers that utilize GPUs and other accelerators. PetscSF provides a simple application programming interface (API) for managing common communication patterns in scientific computations by using a star-forest graph representation. PetscSF supports several implementations based on MPI and NVSHMEM, whose selection is based on the characteristics of the application or the target architecture. An efficient and portable model for network and intra-node communication is essential for implementing large-scale applications. The Message Passing Interface, which has been the de facto standard for distributed memory systems, has developed into a large complex API that does not yet provide high performance on the emerging heterogeneous CPU-GPU-based exascale systems. In this article, we discuss the design of PetscSF, how it can overcome some difficulties of working directly with MPI on GPUs, and we demonstrate its performance, scalability, and novel features.},
  archive      = {J_TPDS},
  author       = {Junchao Zhang and Jed Brown and Satish Balay and Jacob Faibussowitsch and Matthew Knepley and Oana Marin and Richard Tran Mills and Todd Munson and Barry F. Smith and Stefano Zampini},
  doi          = {10.1109/TPDS.2021.3084070},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {842-853},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {The PetscSF scalable communication layer},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LB4OMP: A dynamic load balancing library for multithreaded
applications. <em>TPDS</em>, <em>33</em>(4), 830‚Äì841. (<a
href="https://doi.org/10.1109/TPDS.2021.3107775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exascale computing systems will exhibit high degrees of hierarchical parallelism, with thousands of computing nodes and hundreds of cores per node. Efficiently exploiting hierarchical parallelism is challenging due to load imbalance that arises at multiple levels. OpenMP is the most widely-used standard for expressing and exploiting the ever-increasing node-level parallelism. The scheduling options in OpenMP are insufficient to address the load imbalance that arises during the execution of multithreaded applications. The limited scheduling options in OpenMP hinder research on novel scheduling techniques which require comparison with others from the literature. This work introduces LB4OMP, an open-source dynamic load balancing library that implements successful scheduling algorithms from the literature. LB4OMP is a research infrastructure designed to spur and support present and future scheduling research, for the benefit of multithreaded applications performance. Through an extensive performance analysis campaign, we assess the effectiveness and demystify the performance of all loop scheduling techniques in the library. We show that, for numerous applications-systems pairs, the scheduling techniques in LB4OMP outperform the scheduling options in OpenMP. Node-level load balancing using LB4OMP leads to reduced cross-node load imbalance and to improved MPI+OpenMP applications performance, which is critical for Exascale computing.},
  archive      = {J_TPDS},
  author       = {Jonas H. M√ºller Kornd√∂rfer and Ahmed Eleliemy and Ali Mohammed and Florina M. Ciorba},
  doi          = {10.1109/TPDS.2021.3107775},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {830-841},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {LB4OMP: A dynamic load balancing library for multithreaded applications},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and performance characterization of RADICAL-pilot on
leadership-class platforms. <em>TPDS</em>, <em>33</em>(4), 818‚Äì829. (<a
href="https://doi.org/10.1109/TPDS.2021.3105994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many extreme scale scientific applications have workloads comprised of a large number of individual high-performance tasks. The Pilot abstraction decouples workload specification, resource management, and task execution via job placeholders and late-binding. As such, suitable implementations of the Pilot abstraction can support the collective execution of large number of tasks on supercomputers. We introduce RADICAL-Pilot (RP) as a portable, modular and extensible pilot-enabled runtime system. We describe RP&#39;s design, architecture and implementation. We characterize its performance and show its ability to scalably execute workloads comprised of tens of thousands heterogeneous tasks on DOE and NSF leadership-class HPC platforms. Specifically, we investigate RP&#39;s weak/strong scaling with CPU/GPU, single/multi core, (non)MPI tasks and Python functions when using most of ORNL Summit and TACC Frontera. RADICAL-Pilot can be used stand-alone, as well as the runtime for third-party workflow systems.},
  archive      = {J_TPDS},
  author       = {Andre Merzky and Matteo Turilli and Mikhail Titov and Aymen Al-Saadi and Shantenu Jha},
  doi          = {10.1109/TPDS.2021.3105994},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {818-829},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Design and performance characterization of RADICAL-pilot on leadership-class platforms},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kokkos 3: Programming model extensions for the exascale era.
<em>TPDS</em>, <em>33</em>(4), 805‚Äì817. (<a
href="https://doi.org/10.1109/TPDS.2021.3097283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the push towards exascale hardware has increased the diversity of system architectures, performance portability has become a critical aspect for scientific software. We describe the Kokkos Performance Portable Programming Model that allows developers to write single source applications for diverse high-performance computing architectures. Kokkos provides key abstractions for both the compute and memory hierarchy of modern hardware. We describe the novel abstractions that have been added to Kokkos version 3 such as hierarchical parallelism, containers, task graphs, and arbitrary-sized atomic operations to prepare for exascale era architectures. We demonstrate the performance of these new features with reproducible benchmarks on CPUs and GPUs.},
  archive      = {J_TPDS},
  author       = {Christian R. Trott and Damien Lebrun-Grandi√© and Daniel Arndt and Jan Ciesko and Vinh Dang and Nathan Ellingwood and Rahulkumar Gayatri and Evan Harvey and Daisy S. Hollman and Dan Ibanez and Nevin Liber and Jonathan Madsen and Jeff Miles and David Poliakoff and Amy Powell and Sivasankaran Rajamanickam and Mikael Simberg and Dan Sunderland and Bruno Turcksin and Jeremiah Wilke},
  doi          = {10.1109/TPDS.2021.3097283},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {805-817},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Kokkos 3: Programming model extensions for the exascale era},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EXA2PRO: A framework for high development productivity on
heterogeneous computing systems. <em>TPDS</em>, <em>33</em>(4), 792‚Äì804.
(<a href="https://doi.org/10.1109/TPDS.2021.3104257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Programming upcoming exascale computing systems is expected to be a major challenge. New programming models are required to improve programmability, by hiding the complexity of these systems from application developers. The EXA2PRO programming framework aims at improving developers‚Äô productivity for applications that target heterogeneous computing systems. It is based on advanced programming models and abstractions that encapsulate low-level platform-specific optimizations and it is supported by a runtime that handles application deployment on heterogeneous nodes. It supports a wide variety of platforms and accelerators (CPU, GPU, FPGA-based Data-Flow Engines), allowing developers to efficiently exploit heterogeneous computing systems, thus enabling more HPC applications to reach exascale computing. The EXA2PRO framework was evaluated using four HPC applications from different domains. By applying the EXA2PRO framework, the applications were automatically deployed and evaluated on a variety of computing architectures, enabling developers to obtain performance results on accelerators, test scalability on MPI clusters and productively investigate the degree by which each application can efficiently use different types of hardware resources.},
  archive      = {J_TPDS},
  author       = {Lazaros Papadopoulos and Dimitrios Soudris and Christoph Kessler and August Ernstsson and Johan Ahlqvist and Nikos Vasilas and Athanasios I. Papadopoulos and Panos Seferlis and Charles Prouveur and Matthieu Haefele and Samuel Thibault and Athanasios Salamanis and Theodoros Ioakimidis and Dionysios Kehagias},
  doi          = {10.1109/TPDS.2021.3104257},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {792-804},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {EXA2PRO: A framework for high development productivity on heterogeneous computing systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Compiler-assisted compaction/restoration of SIMD
instructions. <em>TPDS</em>, <em>33</em>(4), 779‚Äì791. (<a
href="https://doi.org/10.1109/TPDS.2021.3091015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vector processors (e.g., SIMD or GPUs) are ubiquitous in high performance systems. All the supercomputers in the world exploit data-level parallelism (DLP), for example by using single instructions to operate over several data elements. Improving vector processing is therefore key for exascale computing. However, despite its potential, vector code generation and execution have significant challenges. Among these challenges, control flow divergence is one of the main performance limiting factors. Most modern vector instruction sets, including SIMD, rely on predication to support divergence control. Nevertheless, the performance and energy consumption in predicated codes is usually insensitive to the number of active elements in a predicated mask. Since the trend is that vector register size increases, the energy efficiency of exascale computing systems will become sub-optimal. This article proposes a novel approach to improve execution efficiency in predicated vector codes, the Compiler-Assisted Compaction/Restoration (CACR) technique. Baseline CR delays predicated SIMD instructions with inactive elements, compacting active elements from instances of the same instruction of consecutive loop iterations. Compacted elements form an equivalent dense vector instruction. After executing the dense instructions, their results are restored to the original instructions. However, CR has a significant performance and energy penalty when it fails to find active elements, either due to lack of resources when unrolling or because of inter-loop dependencies. In CACR, the compiler analyzes the code looking for key information required to configure CR. Then, it passes this information to the processor via new instructions inserted in the code. This prevents CR from waiting for active elements on scenarios when it would fail to form dense instructions. Simulated results (gem5) show that CACR improves performance by up to 29 percent and reduces dynamic energy by up to 24.2 percent on average, for a a set of applications with predicated execution. The baseline CR only achieves 18.6 percent performance and 14 percent energy improvements for the same configuration and applications.},
  archive      = {J_TPDS},
  author       = {Juan M. Cebrian and Thibaud Balem and Adri√°n Barredo and Marc Casas and Miquel Moret√≥ and Alberto Ros and Alexandra Jimborean},
  doi          = {10.1109/TPDS.2021.3091015},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {779-791},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Compiler-assisted Compaction/Restoration of SIMD instructions},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Near-zero downtime recovery from transient-error-induced
crashes. <em>TPDS</em>, <em>33</em>(4), 765‚Äì778. (<a
href="https://doi.org/10.1109/TPDS.2021.3096055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the system scaling, transient errors caused by external noise, e.g., heat fluxes and particle strikes, have become a growing concern for the current and upcoming exa-scale high-performance-computing (HPC) systems. Applications running on these systems are expected to experience transient errors more frequently than ever before, which will either lead them to generate incorrect outputs or cause them to crash. However, since such errors are still quite rare as compared to no-fault cases, desirable solutions call for low/no-overhead systems that do not compromise the performance under no-fault conditions and also allow very fast fault recovery to minimize downtime. In this article, we present IterPro , a light-weight compiler-assisted resilience technique to quickly and accurately recover processes from transient-error-induced crashes. During the compilation of applications, IterPro constructs a set of recovery kernels for crash-prone instructions. These recovery kernels are executed to repair the corrupted process states on-the-fly upon occurrences of errors, enabling applications to continue their executions instead of being terminated. When constructing recovery kernels, IterPro exploits side effects introduced by induction variable based code optimization techniques based on loop unrolling and strength reduction to improve its recovery capability. To this end, two new code transformation passes are introduced to expose the side effects for resilience purposes. We evaluated IterPro with 4 scientific workloads as well as the NPB benchmarks suite. During their normal execution, IterPro incurs almost zero runtime overhead and a small, fixed 27MB memory overhead. Meanwhile, IterPro can recover on an average 83.55 percent of crash-causing errors within dozens of milliseconds with negligible downtime. We also evaluated IterPro with parallel jobs running on 3072 cores and showed that IterPro can successfully mask the impact of crash-causing errors by providing almost uninterrupted execution. Finally, we present our preliminary evaluation result for BLAS, which shows that IterPro is capable of recovering failures in libraries with a very high coverage rate of 83 percent and negligible overheads. With such an effective recovery mechanism, IterPro could tremendously mitigate the overheads and resource requirements of the resilience subsystem in future exa-scale systems.},
  archive      = {J_TPDS},
  author       = {Chao Chen and Greg Eisenhauer and Santosh Pande},
  doi          = {10.1109/TPDS.2021.3096055},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {765-778},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Near-zero downtime recovery from transient-error-induced crashes},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Online power management for multi-cores: A reinforcement
learning based approach. <em>TPDS</em>, <em>33</em>(4), 751‚Äì764. (<a
href="https://doi.org/10.1109/TPDS.2021.3092270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power and energy is the first-class design constraint for multi-core processors and is a limiting factor for future-generation supercomputers. While modern processor design provides a wide range of mechanisms for power and energy optimization, it remains unclear how software can make the best use of them. This article presents a novel approach for runtime power optimization on modern multi-core systems. Our policy combines power capping and uncore frequency scaling to match the hardware power profile to the dynamically changing program behavior at runtime. We achieve this by employing reinforcement learning (RL) to automatically explore the energy-performance optimization space from training programs, learning the subtle relationships between the hardware power profile, the program characteristics, power consumption and program running times. Our RL framework then uses the learned knowledge to adapt the chip&#39;s power budget and uncore frequency to match the changing program phases for any new, previously unseen program. We evaluate our approach on two computing clusters by applying our techniques to 11 parallel programs that were not seen by our RL framework at the training stage. Experimental results show that our approach can reduce the system-level energy consumption by 12 percent, on average, with less than 3 percent of slowdown on the application performance. By lowering the uncore frequency to leave more energy budget to allow the processor cores to run at a higher frequency, our approach can reduce the energy consumption by up to 17 percent while improving the application performance by 5 percent for specific workloads.},
  archive      = {J_TPDS},
  author       = {Yiming Wang and Weizhe Zhang and Meng Hao and Zheng Wang},
  doi          = {10.1109/TPDS.2021.3092270},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {751-764},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Online power management for multi-cores: A reinforcement learning based approach},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Anomaly detection and anticipation in high performance
computing systems. <em>TPDS</em>, <em>33</em>(4), 739‚Äì750. (<a
href="https://doi.org/10.1109/TPDS.2021.3082802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In their quest toward Exascale, High Performance Computing (HPC) systems are rapidly becoming larger and more complex, together with the issues concerning their maintenance. Luckily, many current HPC systems are endowed with data monitoring infrastructures that characterize the system state, and whose data can be used to train Deep Learning (DL) anomaly detection models, a very popular research area. However, the lack of labels describing the state of the system is a wide-spread issue, as annotating data is a costly task, generally falling on human system administrators and thus does not scale toward exascale. In this article we investigate the possibility to extract labels from a service monitoring tool (Nagios) currently used by HPC system administrators to flag the nodes which undergo maintenance operations. This allows to automatically annotate data collected by a fine-grained monitoring infrastructure; this labelled data is then used to train and validate a DL model for anomaly detection. We conduct the experimental evaluation on a tier-0 production supercomputer hosted at CINECA, Bologna, Italy. The results reveal that the DL model can accurately detect the real failures, and, moreover, it can predict the insurgency of anomalies, by systematically anticipating the actual labels (i.e., the moment when system administrators realize when an anomalous event happened); the average advance time computed on historical traces is around 45 minutes. The proposed technology can be easily scaled toward exascale systems to easy their maintenance.},
  archive      = {J_TPDS},
  author       = {Andrea Borghesi and Martin Molan and Michela Milano and Andrea Bartolini},
  doi          = {10.1109/TPDS.2021.3082802},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {739-750},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Anomaly detection and anticipation in high performance computing systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IEEE special issue on innovative r&amp;d toward the exascale
era. <em>TPDS</em>, <em>33</em>(4), 736‚Äì738. (<a
href="https://doi.org/10.1109/TPDS.2021.3109651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special issue on Innovative research and development toward the exascale era explores new foundational and translational research toward enabling exascale computing for emerging scientific and societal challenges. Exascale computing is defined as the capability to perform 1018 operations per second. Productively harnessing such a scale of processing, storage, and networking capabilities for diverse domains‚Äî including high-performance computing (HPC) simulations, artificial intelligence (AI), and extreme data-driven computing‚Äî relies on not only revitalizing existing parallel and distributed computing technologies but also innovating new solutions. Papers in this special issue explore diverse topics in research encompassing parallel, distributed, and heterogeneous systems for exascale, including advances in applications, programming environments, runtimes, libraries, innovative algorithms, domain-specific frameworks, systems architecture, performance analysis, data processing, and networking technologies.},
  archive      = {J_TPDS},
  author       = {Sadaf R. Alam and Lois Curfman McInnes and Kengo Nakajima},
  doi          = {10.1109/TPDS.2021.3109651},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {736-738},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {IEEE special issue on innovative R&amp;amp;D toward the exascale era},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ViTrack: Efficient tracking on the edge for commodity video
surveillance systems. <em>TPDS</em>, <em>33</em>(3), 723‚Äì735. (<a
href="https://doi.org/10.1109/TPDS.2021.3081254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, video surveillance systems are widely deployed in various places, e.g., schools, parks, airports, roads, etc. However, existing video surveillance systems are far from full utilization due to high computation overhead in video processing. In this work, we present ViTrack, a framework for efficient multi-video tracking using computation resource on the edge for commodity video surveillance systems. In the heart of ViTrack lies a two layer spatial/temporal compressed target detection method to significantly reduce the computation overhead by combining videos from multiple cameras. Further, ViTrack derives the video relationship and camera information even in absence of camera location, direction, etc. To alleviate the impact of variant video quality and missing targets, ViTrack leverages a Markov Model based approach to efficiently recover missing information and finally derive the complete trajectory. We implement ViTrack on a real deployed video surveillance system with 110 cameras. The experiment results demonstrate that ViTrack can provide efficient trajectory tracking with processing time 45x less than the existing approach. For 110 video cameras, ViTrack can run on a Dell OptiPlex 390 computer to track given targets in almost real time. We believe ViTrack can enable practical video analysis for widely deployed commodity video surveillance systems.},
  archive      = {J_TPDS},
  author       = {Linsong Cheng and Jiliang Wang and Yinghui Li},
  doi          = {10.1109/TPDS.2021.3081254},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {723-735},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ViTrack: Efficient tracking on the edge for commodity video surveillance systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient, dynamic multi-task execution on FPGA-based
computing systems. <em>TPDS</em>, <em>33</em>(3), 710‚Äì722. (<a
href="https://doi.org/10.1109/TPDS.2021.3101153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With growing Field Programmable Gate Array (FPGA) device sizes and their integration in environments enabling sharing of computing resources such as cloud and edge computing, there is a requirement to share the FPGA area between multiple tasks. The resource sharing typically involves partitioning the FPGA space into fix-sized slots. This results in suboptimal resource utilisation and relatively poor performance, particularly as the number of tasks increase. Using OpenCL‚Äôs exploration capabilities, we employ clever clustering and custom, task-specific partitioning and mapping to create a novel, area sharing methodology where task resource requirements are more effectively managed. Using models with varying resource/throughput profiles, we select the most appropriate distribution based on the runtime, workload needs to enhance temporal compute density. The approach is enabled in the system stack by a corresponding task-based virtualisation model. Using 11 high performance tasks from graph analysis, linear algebra and media streaming, we demonstrate an average $2.8\times$ higher system throughput at $2.3\times$ better energy efficiency over existing approaches.},
  archive      = {J_TPDS},
  author       = {Umar Ibrahim Minhas and Roger Woods and Dimitrios S. Nikolopoulos and Georgios Karakonstantis},
  doi          = {10.1109/TPDS.2021.3101153},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {710-722},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient, dynamic multi-task execution on FPGA-based computing systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Timed loops for distributed storage in wireless networks.
<em>TPDS</em>, <em>33</em>(3), 698‚Äì709. (<a
href="https://doi.org/10.1109/TPDS.2021.3100780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IoT deployments that have limited memories lack sustained computation power and have limited connectivity to the Internet due to intermittent last-mile connectivity, particularly in rural and remote locations. For maintaining congestion-free operations, most of the collected data from these networks are discarded, instead of being transmitted remotely for further processing. In this article, we propose the paradigm Timed Loop Storage to distribute the data and use the underutilized bandwidth of local network links for sequentially queuing packets of computational data that are being operated on in parts in one of the IoT nodes. While the sequenced packets are executed sequentially on the target IoT device, the remaining packets, which are currently not being operated on, distribute and keep looping over the network links until they are required for processing. A time-synchronized packet deflection mechanism on each node handles data transfer and looping of individual packets. In our implementation, although we observe that the proposed approach requires data rates of 6 Mbps, it incurs only 45 Kb usage of primary storage systems even for sizeable data, ensuring scalability of the connected IoT devices&#39; temporary storage capabilities, thereby making it useful for real-life applications.},
  archive      = {J_TPDS},
  author       = {Anandarup Mukherjee and Pallav Kumar Deb and Sudip Misra},
  doi          = {10.1109/TPDS.2021.3100780},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {698-709},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Timed loops for distributed storage in wireless networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy-efficient offloading for DNN-based smart IoT systems
in cloud-edge environments. <em>TPDS</em>, <em>33</em>(3), 683‚Äì697. (<a
href="https://doi.org/10.1109/TPDS.2021.3100298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have become an essential and important supporting technology for smart Internet-of-Things (IoT) systems. Due to the high computational costs of large-scale DNNs, it might be infeasible to directly deploy them in energy-constrained IoT devices. Through offloading computation-intensive tasks to the cloud or edges, the computation offloading technology offers a feasible solution to execute DNNs. However, energy-efficient offloading for DNN based smart IoT systems with deadline constraints in the cloud-edge environments is still an open challenge. To address this challenge, we first design a new system energy consumption model, which takes into account the runtime, switching, and computing energy consumption of all participating servers (from both the cloud and edge) and IoT devices. Next, a novel energy-efficient offloading strategy based on a Self-adaptive Particle Swarm Optimization algorithm using the Genetic Algorithm operators (SPSO-GA) is proposed. This new strategy can efficiently make offloading decisions for DNN layers with layer partition operations, which can lessen the encoding dimension and improve the execution time of SPSO-GA. Simulation results demonstrate that the proposed strategy can significantly reduce energy consumption compared to other classic methods.},
  archive      = {J_TPDS},
  author       = {Xing Chen and Jianshan Zhang and Bing Lin and Zheyi Chen and Katinka Wolter and Geyong Min},
  doi          = {10.1109/TPDS.2021.3100298},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {683-697},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Energy-efficient offloading for DNN-based smart IoT systems in cloud-edge environments},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mechanisms for resource allocation and pricing in mobile
edge computing systems. <em>TPDS</em>, <em>33</em>(3), 667‚Äì682. (<a
href="https://doi.org/10.1109/TPDS.2021.3099731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we address the resource allocation and monetization challenges in Mobile Edge Computing (MEC) systems, where users have heterogeneous demands and compete for high quality services. We formulate the Edge Resource Allocation Problem (ERAP) as a Mixed-Integer Linear Program (MILP) and prove that ERAP is NP-hard. To solve the problem efficiently, we propose two resource allocation mechanisms. First, we develop an auction-based mechanism and prove that the proposed mechanism is individually-rational and produces envy-free allocations. We also propose an LP-based approximation mechanism that does not guarantee envy-freeness, but it provides solutions that are guaranteed to be within a given distance from the optimal solution. We evaluate the performance of the proposed mechanisms by conducting an extensive experimental analysis on ERAP instances of various sizes. We use the optimal solutions obtained by solving the MILP model using a commercial solver as benchmarks to evaluate thequality of solutions. Our analysis shows that the proposed mechanisms obtain near optimal solutions for fairly large size instances of the problem in a reasonable amount of time.},
  archive      = {J_TPDS},
  author       = {Tayebeh Bahreini and Hossein Badri and Daniel Grosu},
  doi          = {10.1109/TPDS.2021.3099731},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {667-682},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Mechanisms for resource allocation and pricing in mobile edge computing systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the analysis of cache invalidation with LRU replacement.
<em>TPDS</em>, <em>33</em>(3), 654‚Äì666. (<a
href="https://doi.org/10.1109/TPDS.2021.3098459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Caching contents close to end-users can improve the network performance, while causing the problem of guaranteeing consistency. Specifically, solutions are classified into validation and invalidation, the latter of which can provide strong cache consistency strictly required in some scenarios. To date, little work on the analysis of cache invalidation has been covered. In this work, by using conditional probability to characterize the interactive relationship between existence and validity, we develop an analytical model that evaluates the performance (hit probability and server load) of four different invalidation schemes with LRU replacement under arbitrary invalidation frequency distribution. The model allows us to theoretically identify some key parameters that affect our metrics of interest and gain some common insights on parameter settings to balance the performance of cache invalidation. Compared with other cache invalidation models, our model can achieve higher accuracy in predicting the cache hit probability. We also conduct extensive simulations that demonstrate the achievable performance of our model.},
  archive      = {J_TPDS},
  author       = {Quan Zheng and Tao Yang and Yuanzhi Kan and Xiaobin Tan and Jian Yang and Xiaofeng Jiang},
  doi          = {10.1109/TPDS.2021.3098459},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {654-666},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {On the analysis of cache invalidation with LRU replacement},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Propagation pattern for moment representation of the lattice
boltzmann method. <em>TPDS</em>, <em>33</em>(3), 642‚Äì653. (<a
href="https://doi.org/10.1109/TPDS.2021.3098456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A propagation pattern for the moment representation of the regularized lattice Boltzmann method (LBM) in three dimensions is presented. Using effectively lossless compression, the simulation state is stored as a set of moments of the lattice Boltzmann distribution function, instead of the distribution function itself. An efficient cache-aware propagation pattern for this moment representation has the effect of substantially reducing both the storage and memory bandwidth required for LBM simulations. This article extends recent work with the moment representation by expanding the performance analysis on central processing unit (CPU) architectures, considering how boundary conditions are implemented, and demonstrating the effectiveness of the moment representation on a graphics processing unit (GPU) architecture.},
  archive      = {J_TPDS},
  author       = {John Gounley and Madhurima Vardhan and Erik W. Draeger and Pedro Valero-Lara and Shirley V. Moore and Amanda Randles},
  doi          = {10.1109/TPDS.2021.3098456},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {642-653},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Propagation pattern for moment representation of the lattice boltzmann method},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-task federated learning for personalised deep neural
networks in edge computing. <em>TPDS</em>, <em>33</em>(3), 630‚Äì641. (<a
href="https://doi.org/10.1109/TPDS.2021.3098467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is an emerging approach for collaboratively training Deep Neural Networks (DNNs) on mobile devices, without private user data leaving the devices. Previous works have shown that non-Independent and Identically Distributed (non-IID) user data harms the convergence speed of the FL algorithms. Furthermore, most existing work on FL measures global-model accuracy, but in many cases, such as user content-recommendation, improving individual User model Accuracy (UA) is the real objective. To address these issues, we propose a Multi-Task FL (MTFL) algorithm that introduces non-federated Batch-Normalization (BN) layers into the federated DNN. MTFL benefits UA and convergence speed by allowing users to train models personalised to their own data. MTFL is compatible with popular iterative FL optimisation algorithms such as Federated Averaging (FedAvg), and we show empirically that a distributed form of Adam optimisation (FedAvg-Adam) benefits convergence speed even further when used as the optimisation strategy within MTFL. Experiments using MNIST and CIFAR10 demonstrate that MTFL is able to significantly reduce the number of rounds required to reach a target UA, by up to $5\times$ when using existing FL optimisation strategies, and with a further $3\times$ improvement when using FedAvg-Adam. We compare MTFL to competing personalised FL algorithms, showing that it is able to achieve the best UA for MNIST and CIFAR10 in all considered scenarios. Finally, we evaluate MTFL with FedAvg-Adam on an edge-computing testbed, showing that its convergence and UA benefits outweigh its overhead.},
  archive      = {J_TPDS},
  author       = {Jed Mills and Jia Hu and Geyong Min},
  doi          = {10.1109/TPDS.2021.3098467},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {630-641},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Multi-task federated learning for personalised deep neural networks in edge computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Harnessing the potential of function-reuse in multimedia
cloud systems. <em>TPDS</em>, <em>33</em>(3), 617‚Äì629. (<a
href="https://doi.org/10.1109/TPDS.2021.3097911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-based computing systems can get oversubscribed due to the budget constraints of their users or limitations in certain resource types. The oversubscription can, in turn, degrade the users perceived Quality of Service (QoS). The approach we investigate to mitigate both the oversubscription and the incurred cost is based on smart reusing of the computation needed to process the service requests (i.e., tasks). We propose a reusing paradigm for the tasks that are waiting for execution. This paradigm can be particularly impactful in serverless platforms where multiple users can request similar services simultaneously. Our motivation is a multimedia streaming engine that processes the media segments in an on-demand manner. We propose a mechanism to identify various types of ‚Äúmergeable‚Äù tasks and aggregate them to improve the QoS and mitigate the incurred cost. We develop novel approaches to determine when and how to perform task aggregation such that the QoS of other tasks is not affected. Evaluation results show that the proposed mechanism can improve the QoS by significantly reducing the percentage of tasks missing their deadlines and reduce the overall time (and subsequently the incurred cost) of utilizing cloud services by more than 9 percent.},
  archive      = {J_TPDS},
  author       = {Chavit Denninnart and Mohsen Amini Salehi},
  doi          = {10.1109/TPDS.2021.3097911},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {617-629},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Harnessing the potential of function-reuse in multimedia cloud systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast and portable concurrent FIFO queues with deterministic
memory reclamation. <em>TPDS</em>, <em>33</em>(3), 604‚Äì616. (<a
href="https://doi.org/10.1109/TPDS.2021.3097901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we present an algorithm for a high performance, unbounded, portable, multi-producer/multi-consumer, lock-free FIFO (first-in first-out) queue. Aside from its competitive performance on current hardware, it is further characterized by its integrated memory reclamation mechanism, which is able to reliably and deterministically de-allocate nodes as soon as the final operation with a reference has concluded, similar to reference counting. This differentiates our approach from most other lock-free data structures, which usually require external (generic) memory reclamation or garbage collection mechanisms such as hazard pointers. Our deterministic memory reclamation mechanism completely prevents the build up of memory awaiting reclamation and is hence very memory efficient, yet it does not introduce any substantial performance overhead. By utilizing concrete knowledge about the internal structure and access patterns of our queue, we are able to construct and constrain the reclamation mechanism in such a way that keeps the overhead for memory management almost entirely out of the common fast path. The presented algorithm is portable to all modern 64-bit processor architectures, as it only relies on the commonly available and lock-free atomic synchronization primitives compare-and-swap and fetch-and-add.},
  archive      = {J_TPDS},
  author       = {Oliver Giersch and J√∂rg Nolte},
  doi          = {10.1109/TPDS.2021.3097901},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {604-616},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Fast and portable concurrent FIFO queues with deterministic memory reclamation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Resilient real-valued consensus in spite of mobile malicious
agents on directed graphs. <em>TPDS</em>, <em>33</em>(3), 586‚Äì603. (<a
href="https://doi.org/10.1109/TPDS.2021.3096074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses novel real-valued consensus problems in the presence of malicious adversaries that can move within the network and induce faulty behaviors in the attacked agents. By adopting several mobile adversary models from the computer science literature, we develop protocols which can mitigate the influence of such malicious agents. The algorithms follow the class of mean subsequence reduced (MSR) algorithms, under which agents ignore the suspicious values received from neighbors during their state updates. Different from the static adversary models, even after the adversaries move away, the infected agents may remain faulty in their values, whose effects must be taken into account. We develop conditions on the network structures for both the complete and non-complete directed graph cases, under which the proposed algorithms are guaranteed to attain resilient consensus. The tolerance bound for network conditions becomes more strict as the adversaries are allowed to have more power. Extensive simulations are carried out over random graphs to verify the effectiveness of our approach when the information of the adversarial agents in terms of their models and numbers is unknown to the agents.},
  archive      = {J_TPDS},
  author       = {Yuan Wang and Hideaki Ishii and Fran√ßois Bonnet and Xavier D√©fago},
  doi          = {10.1109/TPDS.2021.3096074},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {586-603},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Resilient real-valued consensus in spite of mobile malicious agents on directed graphs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online pricing and trading of private data in correlated
queries. <em>TPDS</em>, <em>33</em>(3), 569‚Äì585. (<a
href="https://doi.org/10.1109/TPDS.2021.3095238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the commoditization of private data, data trading in consideration of user privacy protection has become a fascinating research topic. The trading for private web browsing histories brings huge economic value to data consumers when leveraged by targeted advertising. And the online pricing of these private data further helps achieve more realistic data trading. In this paper, we study the trading and pricing of multiple correlated queries on private web browsing history data at the same time. We propose CTRADE, which is a novel online data CommodiTization fRamework for trAding multiple correlateD queriEs over private data. CTRADE first devises a modified matrix mechanism to perturb query answers. It especially quantifies privacy loss under the relaxation of classical differential privacy and a newly devised mechanism with relaxed matrix sensitivity, and further compensates data owners for their diverse privacy losses in a satisfying manner. CTRADE then proposes an ellipsoid-based query pricing mechanism according to a given linear market value model, which exploits the features of the ellipsoid to explore and exploit the close-optimal dynamic price at each round. In particular, the proposed mechanism produces a low cumulative regret, which is quadratic in the dimension of the feature vector and logarithmic in the number of total rounds. Through real-data based experiments, our analysis and evaluation results demonstrate that CTRADE balances total error and privacy preferences well within acceptable running time, indeed produces a convergent cumulative regret with more rounds, and also achieves all desired economic properties of budget balance, individual rationality, and truthfulness.},
  archive      = {J_TPDS},
  author       = {Hui Cai and Fan Ye and Yuanyuan Yang and Yanmin Zhu and Jie Li and Fu Xiao},
  doi          = {10.1109/TPDS.2021.3095238},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {569-585},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Online pricing and trading of private data in correlated queries},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CuNH: Efficient GPU implementations of post-quantum KEM
NewHope. <em>TPDS</em>, <em>33</em>(3), 551‚Äì568. (<a
href="https://doi.org/10.1109/TPDS.2021.3097277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-quantum cryptography was proposed in the past years due to the foreseeable emergence of quantum computers that are able to break the conventional public key cryptosystems at acceptable costs. However, post-quantum schemes are usually less efficient than conventional ones, which makes them less practical in scenarios with limited resources or high concurrency. Server-side applications always feature multiple users, therefore requiring efficient execution of batch tasks. GPU is intrinsically well-suited to batch tasks owing to its SIMD/SIMT execution fashion, so it naturally helps to achieve high performance. However, a naive GPU-based implementation cannot make the best use of hardware resources of the GPU regardless of task loads. In this article, we propose SIMD parallelization paradigms for fine-grained GPU implementations and then apply them to a post-quantum key encapsulation algorithm called NewHope, where we carefully design every module, especially NTT and inverse NTT, to fit into the SIMD parallelization paradigms. In addition, we employ multi-streaming to improve performance in user&#39;s perspective. Finally, our evaluations are made on two testbeds with GPU accelerators NVIDIA GeForce MX150 and GeForce GTX 1650, respectively. The experimental results show that the fine-grained implementations save up to 98 percent latency at low task loads, and their throughputs increase by up to 86 percent at high task loads, when compared with the naive ones in kernel&#39;s perspective, and the multi-streaming implementations greatly reduce the latency overhead percentage at high task loads by up to 86 percent, when compared with the fine-grained implementation in user&#39;s perspective. Moreover, our fine-grained implementation and multi-streaming implementation are respectively 51.5 and 45.5 percent faster than Gupta et al.&#39;s implementations when compared with it under reasonable assumptions. Furthermore, as lattice-based post-quantum schemes have similar operations, our proposal also easily applies to other lattice-based post-quantum schemes.},
  archive      = {J_TPDS},
  author       = {Yiwen Gao and Jia Xu and Hongbing Wang},
  doi          = {10.1109/TPDS.2021.3097277},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {551-568},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CuNH: Efficient GPU implementations of post-quantum KEM NewHope},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decentralized edge intelligence: A dynamic resource
allocation framework for hierarchical federated learning. <em>TPDS</em>,
<em>33</em>(3), 536‚Äì550. (<a
href="https://doi.org/10.1109/TPDS.2021.3096076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enable the large scale and efficient deployment of Artificial Intelligence (AI), the confluence of AI and Edge Computing has given rise to Edge Intelligence, which leverages on the computation and communication capabilities of end devices and edge servers to process data closer to where it is produced. One of the enabling technologies of Edge Intelligence is the privacy preserving machine learning paradigm known as Federated Learning (FL), which enables data owners to conduct model training without having to transmit their raw data to third-party servers. However, the FL network is envisioned to involve thousands of heterogeneous distributed devices. As a result, communication inefficiency remains a key bottleneck. To reduce node failures and device dropouts, the Hierarchical Federated Learning (HFL) framework has been proposed whereby cluster heads are designated to support the data owners through intermediate model aggregation. This decentralized learning approach reduces the reliance on a central controller, e.g., the model owner. However, the issues of resource allocation and incentive design are not well-studied in the HFL framework. In this article, we consider a two-level resource allocation and incentive mechanism design problem. In the lower level, the cluster heads offer rewards in exchange for the data owners&#39; participation, and the data owners are free to choose which cluster to join. Specifically, we apply the evolutionary game theory to model the dynamics of the cluster selection process. In the upper level, each cluster head can choose to serve a model owner, whereas the model owners have to compete amongst each other for the services of the cluster heads. As such, we propose a deep learning based auction mechanism to derive the valuation of each cluster head&#39;s services. The performance evaluation shows the uniqueness and stability of our proposed evolutionary game, as well as the revenue maximizing properties of the deep learning based auction.},
  archive      = {J_TPDS},
  author       = {Wei Yang Bryan Lim and Jer Shyuan Ng and Zehui Xiong and Jiangming Jin and Yang Zhang and Dusit Niyato and Cyril Leung and Chunyan Miao},
  doi          = {10.1109/TPDS.2021.3096076},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {536-550},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Decentralized edge intelligence: A dynamic resource allocation framework for hierarchical federated learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Work-stealing prefix scan: Addressing load imbalance in
large-scale image registration. <em>TPDS</em>, <em>33</em>(3), 523‚Äì535.
(<a href="https://doi.org/10.1109/TPDS.2021.3095230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallelism patterns (e.g., map or reduce) have proven to be effective tools for parallelizing high-performance applications. In this article, we study the recursive registration of a series of electron microscopy images - a time consuming and imbalanced computation necessary for nano-scale microscopy analysis. We show that by translating the image registration into a specific instance of the prefix scan, we can convert this seemingly sequential problem into a parallel computation that scales to over thousand of cores. We analyze a variety of scan algorithms that behave similarly for common low-compute operators and propose a novel work-stealing procedure for a hierarchical prefix scan. Our evaluation shows that by identifying a suitable and well-optimized prefix scan algorithm, we reduce time-to-solution on a series of 4,096 images spanning ten seconds of microscopy acquisition from over 10 hours to less than 3 minutes (using 1024 Intel Haswell cores), enabling derivation of material properties at nanoscale for long microscopy image series.},
  archive      = {J_TPDS},
  author       = {Marcin Copik and Tobias Grosser and Torsten Hoefler and Paolo Bientinesi and Benjamin Berkels},
  doi          = {10.1109/TPDS.2021.3095230},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {523-535},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Work-stealing prefix scan: Addressing load imbalance in large-scale image registration},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal checkpointing strategies for iterative applications.
<em>TPDS</em>, <em>33</em>(3), 507‚Äì522. (<a
href="https://doi.org/10.1109/TPDS.2021.3099440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work provides an optimal checkpointing strategy to protect iterative applications from fail-stop errors. We consider a general framework, where the application repeats the same execution pattern by executing consecutive iterations, and where each iteration is composed of several tasks. These tasks have different execution lengths and different checkpoint costs. Assume that there are n tasks and that task a i , where 0 ‚â§ i n , has execution time t i and checkpoint cost c i . A naive strategy would checkpoint after each task. Another naive strategy would checkpoint at the end of each iteration. A strategy inspired by the Young/Daly formula would work for ‚àö{2 Œºc ave } seconds, where Œº is the application MTBF and c ave is the average checkpoint time, and checkpoint at the end of the current task (and repeat). Another strategy, also inspired by the Young/Daly formula, would select the task a min with smallest checkpoint cost c min and would checkpoint after every p th instance of that task, leading to a checkpointing period p T, where T = Œ£ i=0 n-1 a i is the time per iteration. One would choose the period so that p T ‚âà ‚àö{2 Œºc min } to obey the Young/Daly formula. All these naive and Young/Daly strategies are suboptimal. Our main contribution is to show that the optimal checkpoint strategy is globally periodic, and to design a dynamic programming algorithm that computes the optimal checkpointing pattern. This pattern may well checkpoint many different tasks, and this across many different iterations. We show through simulations, both from synthetic and real-life application scenarios, that the optimal strategy outperforms the naive and Young/Daly strategies.},
  archive      = {J_TPDS},
  author       = {Yishu Du and Loris Marchal and Guillaume Pallez and Yves Robert},
  doi          = {10.1109/TPDS.2021.3099440},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {507-522},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimal checkpointing strategies for iterative applications},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VPipe: A virtualized acceleration system for achieving
efficient and scalable pipeline parallel DNN training. <em>TPDS</em>,
<em>33</em>(3), 489‚Äì506. (<a
href="https://doi.org/10.1109/TPDS.2021.3094364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing computational complexity of DNNs achieved unprecedented successes in various areas such as machine vision and natural language processing (NLP), e.g., the recent advanced Transformer has billions of parameters. However, as large-scale DNNs significantly exceed GPU‚Äôs physical memory limit, they cannot be trained by conventional methods such as data parallelism. Pipeline parallelism that partitions a large DNN into small subnets and trains them on different GPUs is a plausible solution. Unfortunately, the layer partitioning and memory management in existing pipeline parallel systems are fixed during training, making them easily impeded by out-of-memory errors and the GPU under-utilization. These drawbacks amplify when performing neural architecture search (NAS) such as the evolved Transformer, where different network architectures of Transformer needed to be trained repeatedly. vPipe is the first system that transparently provides dynamic layer partitioning and memory management for pipeline parallelism. vPipe has two unique contributions, including (1) an online algorithm for searching a near-optimal layer partitioning and memory management plan, and (2) a live layer migration protocol for re-balancing the layer distribution across a training pipeline. vPipe improved the training throughput of two notable baselines (Pipedream and GPipe) by 61.4-463.4 percent and 24.8-291.3 percent on various large DNNs and training settings.},
  archive      = {J_TPDS},
  author       = {Shixiong Zhao and Fanxin Li and Xusheng Chen and Xiuxian Guan and Jianyu Jiang and Dong Huang and Yuhao Qing and Sen Wang and Peng Wang and Gong Zhang and Cheng Li and Ping Luo and Heming Cui},
  doi          = {10.1109/TPDS.2021.3094364},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {489-506},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {VPipe: A virtualized acceleration system for achieving efficient and scalable pipeline parallel DNN training},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A practical and efficient bidirectional access control
scheme for cloud-edge data sharing. <em>TPDS</em>, <em>33</em>(2),
476‚Äì488. (<a href="https://doi.org/10.1109/TPDS.2021.3094126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cloud computing paradigm provides numerous tempting advantages, enabling users to store and share their data conveniently. However, users are naturally resistant to directly outsourcing their data to the cloud since the data often contain sensitive information. Although several fine-grained access control schemes for cloud-data sharing have been proposed, most of them focus on the access control of the encrypted data (e.g., restricting the decryption capabilities of the receivers). Distinct from the existing work, this article aims to address this challenging problem by developing a more practical bidirectional fine-grained access control scheme that can restrict the capabilities of both senders and receivers. To this end, we systematically investigate the access control for cloud data sharing. Inspired by the access control encryption (ACE), we propose a novel data sharing framework that combines the cloud side and the edge side. The edge server is located in the middle of all the communications, checking and preventing illegal communications according to the predefined access policy. Next, we develop an efficient access control algorithm by exploiting the attribute-based encryption and proxy re-encryption for the proposed framework. The experimental results show that our scheme exhibits superior performance in the encryption and decryption compared to the prior work.},
  archive      = {J_TPDS},
  author       = {Jie Cui and Bei Li and Hong Zhong and Geyong Min and Yan Xu and Lu Liu},
  doi          = {10.1109/TPDS.2021.3094126},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {476-488},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A practical and efficient bidirectional access control scheme for cloud-edge data sharing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). POCLib: A high-performance framework for enabling near
orthogonal processing on compression. <em>TPDS</em>, <em>33</em>(2),
459‚Äì475. (<a href="https://doi.org/10.1109/TPDS.2021.3093234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel technology boosts data processing in recent years, and parallel direct data processing on hierarchically compressed documents exhibits great promise. The high-performance direct data processing technique brings large savings in both time and space by removing the need for decompressing data. However, its benefits have been limited to data traversal operations; for random accesses, direct data processing is several times slower than the state-of-the-art baselines. This article proposes a novel concept, orthogonal processing on compression (orthogonal POC), which means that text analytics can be efficiently supported directly on compressed data, regardless of the type of the data processing ‚Äì that is, the type of data processing is orthogonal to its capability of conducting POC. Previous proposals, such as TADOC, are not orthogonal POC. This article presents a set of techniques that successfully eliminate the limitation, and for the first time, establishes the near orthogonal POC feasibility of effectively handling both data traversal operations and random data accesses on hierarchically-compressed data. The work focuses on text data and yields a unified high-performance library, called POCLib. In a ten-node distributed Spark cluster on Amazon EC2, POCLib achieves 3.1√ó speedup over the state-of-the-art on random data accesses to compressed data, while preserving the capability of supporting traversal operations efficiently and providing large (3.9√ó) space savings.},
  archive      = {J_TPDS},
  author       = {Feng Zhang and Jidong Zhai and Xipeng Shen and Onur Mutlu and Xiaoyong Du},
  doi          = {10.1109/TPDS.2021.3093234},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {459-475},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {POCLib: A high-performance framework for enabling near orthogonal processing on compression},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A block-based triangle counting algorithm on heterogeneous
environments. <em>TPDS</em>, <em>33</em>(2), 444‚Äì458. (<a
href="https://doi.org/10.1109/TPDS.2021.3093240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triangle counting is a fundamental building block in graph algorithms. In this article, we propose a block-based triangle counting algorithm to reduce data movement during both sequential and parallel execution. Our block-based formulation makes the algorithm naturally suitable for heterogeneous architectures. The problem of partitioning the adjacency matrix of a graph is well-studied. Our task decomposition goes one step further: it partitions the set of triangles in the graph. By streaming these small tasks to compute resources, we can solve problems that do not fit on a device. We demonstrate the effectiveness of our approach by providing an implementation on a compute node with multiple sockets, cores and GPUs. The current state-of-the-art in triangle enumeration processes the Friendster graph in 2.1 seconds, not including data copy time between CPU and GPU. Using that metric, our approach is 20 percent faster. When copy times are included, our algorithm takes 3.2 seconds. This is 5.6 times faster than the fastest published CPU-only time.},
  archive      = {J_TPDS},
  author       = {Abdurrahman Ya≈üar and Sivasankaran Rajamanickam and Jonathan W. Berry and √úmit V. √áataly√ºrek},
  doi          = {10.1109/TPDS.2021.3093240},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {444-458},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A block-based triangle counting algorithm on heterogeneous environments},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tensorox: Accelerating GPU applications via neural
approximation on unused tensor cores. <em>TPDS</em>, <em>33</em>(2),
429‚Äì443. (<a href="https://doi.org/10.1109/TPDS.2021.3093239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by the demands of deep learning, many hardware accelerators, including GPUs, have begun to include specialized tensor processing units to accelerate matrix operations. However, general-purpose GPU applications that have little or no large dense matrix operations cannot benefit from these tensor units. This article proposes Tensorox, a framework that exploits the half-precision tensor cores available on recent GPUs for approximable, non deep learning applications. In essence, a shallow neural network is trained based on the input-output mapping of the function to be approximated. The key innovation in our implementation is the use of the small and dimension-restricted tensor operations in Nvidia GPUs to run multiple instances of the approximation neural network in parallel. With the proper scaling and training methods, our approximation yielded an overall accuracy that is higher than na√Øvely running the original programs with half-precision. Furthermore, Tensorox allows for the runtime adjustment of the degree of approximation. For the 10 benchmarks we tested, we achieved speedups from 2√ó to 112√ó compared to the original in single precision floating point, while maintaining the error caused by the approximation to below 10 percent in most applications.},
  archive      = {J_TPDS},
  author       = {Nhut-Minh Ho and Weng-Fai Wong},
  doi          = {10.1109/TPDS.2021.3093239},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {429-443},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Tensorox: Accelerating GPU applications via neural approximation on unused tensor cores},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A pessimistic fault diagnosability of large-scale connected
networks via extra connectivity. <em>TPDS</em>, <em>33</em>(2), 415‚Äì428.
(<a href="https://doi.org/10.1109/TPDS.2021.3093243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The t/kt/k-diagnosability and hh-extra connectivity are regarded as two important indicators to improve the network reliability. The t/k-diagnosis strategy can significantly improve the self-diagnosing capability of a network at the expense of no more than k fault-free nodes being mistakenly diagnosed as faulty. The h -extra connectivity can tremendously improve the real fault tolerability of a network by insuring that each remaining component has no fewer than h+1 nodes. However, there is few result on the inherent relationship between these two indicators. In this article, we investigate the reason that caused the serious flawed results in (Liu, 2020), and we propose a diagnosis algorithm to establish the t/k-diagnosability for a large-scale connected network G under the PMC model by considering its h-extra connectivity. Let Œ∫ h (G) be the h-extra connectivity of G. Then, we can deduce that G is Œ∫ h (G)/h-diagnosable under the PMC model with some basic conditions. All Œ∫ h (G)faulty nodes can be correctly diagnosed in the large-scale connected network G and at most h fault-free nodes would be misdiagnosed as faulty. The complete fault tolerant method adopts combinatorial properties and linearly many fault analysis to conquer the core of our proofs. We will apply the newly found relationship to directly obtain the Œ∫ h (G)/h-diagnosability of a series of well known networks, including hypercubes, folded hypercubes, balanced hypercubes, dual-cubes, BC graphs, star graphs, Cayley graphs generated by transposition trees, bubble-sort star graphs, alternating group graphs, split-star networks, k-ary n-cubes and (n,k)-star graphs.},
  archive      = {J_TPDS},
  author       = {Limei Lin and Yanze Huang and Li Xu and Sun-Yuan Hsieh},
  doi          = {10.1109/TPDS.2021.3093243},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {415-428},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A pessimistic fault diagnosability of large-scale connected networks via extra connectivity},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimizing network transfers for data analytic jobs across
geo-distributed datacenters. <em>TPDS</em>, <em>33</em>(2), 403‚Äì414. (<a
href="https://doi.org/10.1109/TPDS.2021.3093232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has become a recent trend that large volumes of data are generated, stored, and processed across geographically distributed datacenters. When popular data parallel frameworks, such as MapReduce and Spark, are employed to process such geo-distributed data, optimizing the network transfer in communication stages becomes increasingly crucial to application performance, as the inter-datacenter links have much lower bandwidth than intra-datacenter links. In this article, we focus on exploiting the flexibility of multi-path routing for inter-datacenter flows of data analytic jobs, with the hope of better utilizing inter-datacenter links and thus improve job performance. We design an optimal multi-path routing and scheduling strategy to achieve the best possible network performance for all concurrent jobs, based on our formulation of an optimization problem that can be transformed into an equivalent linear programming (LP) problem to be efficiently solved. As a highlight of this article, we have implemented our proposed algorithm in the controller of an application-layer software-defined inter-datacenter overlay testbed, designed to provide transfer optimization service for Spark jobs. With extensive evaluations of our real-world implementation on Google Cloud, we have shown convincing evidence that our optimal multi-path routing and scheduling strategies have achieved significant improvements in terms of job performance.},
  archive      = {J_TPDS},
  author       = {Li Chen and Shuhao Liu and Baochun Li},
  doi          = {10.1109/TPDS.2021.3093232},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {403-414},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimizing network transfers for data analytic jobs across geo-distributed datacenters},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Repurposing GPU microarchitectures with light-weight
out-of-order execution. <em>TPDS</em>, <em>33</em>(2), 388‚Äì402. (<a
href="https://doi.org/10.1109/TPDS.2021.3093231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GPU is the dominant platform for accelerating general-purpose workloads due to its computing capacity and cost-efficiency. GPU applications cover an ever-growing range of domains. To achieve high throughput, GPUs rely on massive multi-threading and fast context switching to overlap computations with memory operations. We observe that among the diverse GPU workloads, there exists a significant class of kernels that fail to maintain a sufficient number of active warps to hide the latency of memory operations, and thus suffer from frequent stalling. We argue that the dominant Thread-Level Parallelism model is not enough to efficiently accommodate the variability of modern GPU applications. To address this inherent inefficiency, we propose a novel micro-architecture with lightweight Out-Of-Order execution capability enabling Instruction-Level Parallelism to complement the conventional Thread-Level Parallelism model. To minimize the hardware overhead, we carefully design our extension to highly re-use the existing micro-architectural structures and study various design trade-offs to contain the overall area and power overhead, while providing improved performance. We show that the proposed architecture outperforms traditional platforms by 23 percent on average for low-occupancy kernels, with an area and power overhead of 1.29 and 10.05 percent, respectively. Finally, we establish the potential of our proposal as a micro-architecture alternative by providing 16 percent speedup over a wide collection of 60 general-purpose kernels.},
  archive      = {J_TPDS},
  author       = {Konstantinos Iliakis and Sotirios Xydis and Dimitrios Soudris},
  doi          = {10.1109/TPDS.2021.3093231},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {388-402},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Repurposing GPU microarchitectures with light-weight out-of-order execution},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PostMan: Rapidly mitigating bursty traffic via on-demand
offloading of packet processing. <em>TPDS</em>, <em>33</em>(2), 374‚Äì387.
(<a href="https://doi.org/10.1109/TPDS.2021.3092266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unexpected bursty traffic brought by certain sudden events, such as news in the spotlight on a social network or discounted items on sale, can cause severe load imbalance in backend services. Migrating hot data - the standard approach to achieve load balance - meets a challenge when handling such unexpected load imbalance, because migrating data will slow down the server that is already under heavy pressure. This article proposes PostMan, an alternative approach to rapidly mitigate load imbalance for services processing small requests. Motivated by the observation that processing large packets incurs far less CPU overhead than processing small ones, PostMan deploys a number of middleboxes called helpers to assemble small packets into large ones for the heavily-loaded server. This approach essentially offloads the overhead of packet processing from the heavily-loaded server to helpers. To minimize the overhead, PostMan activates helpers on demand, only when bursty traffic is detected. The heavily-loaded server determines when clients connect/disconnect to/from helpers based on the real-time load statistics. To tolerate helper failures, PostMan can migrate connections across helpers and can ensure packet ordering despite such migration. Driven by real-world workloads, our evaluation shows that, with the help of PostMan, a Memcached server can mitigate bursty traffic within hundreds of milliseconds, while migrating data takes tens of seconds and increases the latency during migration.},
  archive      = {J_TPDS},
  author       = {Yipei Niu and Panpan Jin and Jian Guo and Yikai Xiao and Rong Shi and Fangming Liu and Chen Qian and Yang Wang},
  doi          = {10.1109/TPDS.2021.3092266},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {374-387},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {PostMan: Rapidly mitigating bursty traffic via on-demand offloading of packet processing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimization of reactive force field simulation: Refactor,
parallelization, and vectorization for interactions. <em>TPDS</em>,
<em>33</em>(2), 359‚Äì373. (<a
href="https://doi.org/10.1109/TPDS.2021.3091408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular dynamics (MD) simulations are playing an increasingly important role in many areas ranging from chemical materials to biological molecules. With the continuing development of MD models, the potentials are getting larger and more complex. In this article, we focus on the reactive force field (ReaxFF) potential from LAMMPS to optimize the computation of interactions. We present our efforts on refactoring for neighbor list building, bond order computation, as well as valence angles and torsion angles computation. After redesigning these kernels, we develop a vectorized implementation for non-bonded interactions, which is nearly 100 √ó faster than the management processing element (MPE) on the Sunway TaihuLight supercomputer. Furthermore, we have implemented the three-body-list free torsion angles computation, and propose a line-locked software cache method to eliminate write conflicts in the torsion angle and valence angle interactions resulting in an order-of-magnitude speedup on a single Sunway TaihuLight node. In addition, we achieve a speedup of up to 3.5 compared to the KOKKOS package on an Intel Xeon Gold 6148 core. When executed on 1,024 processes, our implementation enables the simulation of 21,233,664 atoms on 66,560 cores with a performance of 0.032 ns/day and a weak scaling efficiency of 95.71 percent.},
  archive      = {J_TPDS},
  author       = {Ping Gao and Xiaohui Duan and Bertil Schmidt and Wusheng Zhang and Lin Gan and Haohuan Fu and Wei Xue and Weiguo Liu and Guangwen Yang},
  doi          = {10.1109/TPDS.2021.3091408},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {359-373},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimization of reactive force field simulation: Refactor, parallelization, and vectorization for interactions},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EdgeDR: An online mechanism design for demand response in
edge clouds. <em>TPDS</em>, <em>33</em>(2), 343‚Äì358. (<a
href="https://doi.org/10.1109/TPDS.2021.3087360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computing frontier is moving from centralized mega datacenters towards distributed cloudlets at the network edge. We argue that cloudlets are well-suited for handling power demand response to help the grid maintain stability due to more flexible workload management attributed to their distributed nature. However, they also require computing demand response to avoid overload and maintain reliability. To this end, we propose a novel online market mechanism, EdgeDR, to achieve cost efficiency in edge demand response programs. At a high level, we observe that the cloudlet operator can dynamically switch on/off entire cloudlets to compensate for the energy reduction required by the power grid or provide enough computing resources to the edge service. We formulate a long-term social cost minimization problem and decompose it into a series of one-round procurement auctions. In each auction instance, we propose to let the cloudlet tenants bid with cost functions of their two-dimension service quality degradation tolerance, and let the cloudlet operator choose the service quality, manage the workload, and schedule the cloudlet activation status. In addition, we present a dynamic payment mechanism for the operator to balance the tradeoff between short-term profit and long-term benefit in more practical scenarios. Via rigorous analysis, we exhibit that our bidding policy is individually rational and truthful; our workload management algorithm has near-optimal performance in each auction; and our overall online algorithm achieves a provable competitive ratio. We further confirm the performance of our mechanism through extensive trace-driven simulations.},
  archive      = {J_TPDS},
  author       = {Shutong Chen and Lei Jiao and Fangming Liu and Lin Wang},
  doi          = {10.1109/TPDS.2021.3087360},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {343-358},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {EdgeDR: An online mechanism design for demand response in edge clouds},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Monodirectional evolutional symport tissue p systems with
promoters and cell division. <em>TPDS</em>, <em>33</em>(2), 332‚Äì342. (<a
href="https://doi.org/10.1109/TPDS.2021.3065397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monodirectional tissue P systems with promoters are natural inspired parallel computing paradigms, where only symport rules are permitted, and with the restriction of ‚Äúmonodirectionality‚Äù, objects for two given regions are transferred in one direction. In this article, a novel kind of P systems, monodirectional evolutional symport tissue P systems with promoters (MESTP P systems) is raised, where objects may be revised during the movement between two regions. The computational theory of MESTP P systems that rules are employed in a flat maximally parallel pattern is investigated. We prove that finite natural number sets are created by MESTP P systems applying one cell, at most 1 promoter and all evolutional symport rules having a maximal length 2 or with arbitrary number of cells, promoters and all evolutional symport rules having a maximal length 2. MESTP P systems are Turing universal when two cells, at most 1 promoter and all evolutional symport rules having a maximal length 2 are employed. In addition, with the help of cell division mechanism, monodirectional evolutional symport tissue P systems with promoters and cell division (MESTPD P systems) are employed to solve NP-complete (the SAT) problem, where system uses at most 1 promoter and all evolutional symport rules having a maximal length 3. These results show that MESTP(D) P systems are still computationally powerful even if monodirectionality control mechanism is imposed, thereby developing membrane algorithms for MESTP(D) P systems is theoretically possible as well as potentially exploitable.},
  archive      = {J_TPDS},
  author       = {Bosheng Song and Kenli Li and Xiangxiang Zeng},
  doi          = {10.1109/TPDS.2021.3065397},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {332-342},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Monodirectional evolutional symport tissue p systems with promoters and cell division},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Protein structured reservoir computing for spike-based
pattern recognition. <em>TPDS</em>, <em>33</em>(2), 322‚Äì331. (<a
href="https://doi.org/10.1109/TPDS.2021.3068826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays we witness a miniaturisation trend in the semiconductor industry backed up by groundbreaking discoveries and designs in nanoscale characterisation and fabrication. To facilitate the trend and produce ever smaller, faster and cheaper computing devices, the size of nanoelectronic devices is now reaching the scale of atoms or molecules - a technical goal undoubtedly demanding for novel devices. Following the trend, we explore an unconventional route of implementing reservoir computing on a single protein molecule and introduce neuromorphic connectivity with a small-world networking property. We have chosen Izhikevich spiking neurons as elementary processors, corresponding to the atoms of verotoxin protein, and its molecule as a `hardware&#39; architecture of the communication networks connecting the processors. We apply on a single readout layer, various training methods in a supervised fashion to investigate whether the molecular structured Reservoir Computing (RC) system is capable to deal with machine learning benchmarks. We start with the Remote Supervised Method, based on Spike-Timing-Dependent-Plasticity, and carry on with linear regression and scaled conjugate gradient back-propagation training methods. The RC network is evaluated as a proof-of-concept on the handwritten digit images from the standard MNIST and the extended MNIST datasets and demonstrates acceptable classification accuracies in comparison with other similar approaches.},
  archive      = {J_TPDS},
  author       = {Karolos-Alexandros Tsakalos and Georgios Ch. Sirakoulis and Andrew Adamatzky and Jim Smith},
  doi          = {10.1109/TPDS.2021.3068826},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {322-331},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Protein structured reservoir computing for spike-based pattern recognition},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inferring the dynamics of the state evolution during quantum
annealing. <em>TPDS</em>, <em>33</em>(2), 310‚Äì321. (<a
href="https://doi.org/10.1109/TPDS.2020.3044846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve an optimization problem using a commercial quantum annealer, one has to represent the problem of interest as an Ising or a quadratic unconstrained binary optimization (QUBO) problem and submit its coefficients to the annealer, which then returns a user-specified number of low-energy solutions. It would be useful to know what happens in the quantum processor during the anneal process so that one could design better algorithms or suggest improvements to the hardware. However, existing quantum annealers are not able to directly extract such information from the processor. Hence, in this article we propose to use advanced features of D-Wave 2000Q to indirectly infer information about the dynamics of the state evolution during the anneal process. Specifically, D-Wave 2000Q allows the user to customize the anneal schedule, that is, the schedule with which the anneal fraction is changed from the start to the end of the anneal. Using this feature, we design a set of modified anneal schedules whose outputs can be used to generate information about the states of the system at user-defined time points during a standard anneal. With this process, called slicing, we obtain approximate distributions of lowest-energy anneal solutions as the anneal time evolves. We use our technique to obtain a variety of insights into the annealer, such as the state evolution during annealing, when individual bits in an evolving solution flip during the anneal process and when they stabilize, and we introduce a technique to estimate the freeze-out point of both the system as well as of individual qubits.},
  archive      = {J_TPDS},
  author       = {Elijah Pelofske and Georg Hahn and Hristo Djidjev},
  doi          = {10.1109/TPDS.2020.3044846},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {310-321},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Inferring the dynamics of the state evolution during quantum annealing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast post-hoc normalization for brain inspired sparse coding
on a neuromorphic device. <em>TPDS</em>, <em>33</em>(2), 302‚Äì309. (<a
href="https://doi.org/10.1109/TPDS.2021.3068777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploration of novel computational platforms is critical for the advancement of artificial intelligence as we approach the physical limitations of traditional hardware. Biologically accurate, energy efficient neuromorphic systems are particularly promising for enabling future breakthroughs because of their ability to process information in parallel and to scale using extremely low power. Sparse coding is a signal processing technique which has been known to model the information encoding in the primary visual cortex. When sparse solutions are solved using local neuron competition along with the unsupervised dictionary learning that mimics cortical development, we can build an end to end, hardware to software, brain inspired solution to a machine learning problem. In this article, we perform a detailed comparison of sparse coding solutions generated classically by orthogonal matching pursuit (OMP) implemented on a conventional digital processor with spike-based solutions obtained using the Intel Loihi neuromorphic processor. A novel ‚Äúpost-hoc‚Äù normalization technique to shorten simulation time for Loihi is presented along with analysis of optimal parameter selection, reconstruction errors, and unsupervised dictionary learning for Loihi approaches and their classical counterparts. Preliminary results show that both the Loihi full simulation approach and the post-hoc normalization approach are well suited to neuromorphic processors and operate in a size, weight and power regime that is not accessible by classical approaches. Ultimately, the use of this normalization technique allows for faster and, often, better solutions than demonstrated previously.},
  archive      = {J_TPDS},
  author       = {Kyle Henke and Garrett T. Kenyon and Ben Migliori},
  doi          = {10.1109/TPDS.2021.3068777},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {302-309},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Fast post-hoc normalization for brain inspired sparse coding on a neuromorphic device},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Endurance-aware mapping of spiking neural networks to
neuromorphic hardware. <em>TPDS</em>, <em>33</em>(2), 288‚Äì301. (<a
href="https://doi.org/10.1109/TPDS.2021.3065591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromorphic computing systems are embracing memristors to implement high density and low power synaptic storage as crossbar arrays in hardware. These systems are energy efficient in executing Spiking Neural Networks (SNNs). We observe that long bitlines and wordlines in a memristive crossbar are a major source of parasitic voltage drops, which create current asymmetry. Through circuit simulations, we show the significant endurance variation that results from this asymmetry. Therefore, if the critical memristors (ones with lower endurance) are overutilized, they may lead to a reduction of the crossbar&#39;s lifetime. We propose eSpine, a novel technique to improve lifetime by incorporating the endurance variation within each crossbar in mapping machine learning workloads, ensuring that synapses with higher activation are always implemented on memristors with higher endurance, and vice versa. eSpine works in two steps. First, it uses the Kernighan-Lin Graph Partitioning algorithm to partition a workload into clusters of neurons and synapses, where each cluster can fit in a crossbar. Second, it uses an instance of Particle Swarm Optimization (PSO) to map clusters to tiles, where the placement of synapses of a cluster to memristors of a crossbar is performed by analyzing their activation within the workload. We evaluate eSpine for a state-of-the-art neuromorphic hardware model with phase-change memory (PCM)-based memristors. Using 10 SNN workloads, we demonstrate a significant improvement in the effective lifetime.},
  archive      = {J_TPDS},
  author       = {Twisha Titirsha and Shihao Song and Anup Das and Jeffrey Krichmar and Nikil Dutt and Nagarajan Kandasamy and Francky Catthoor},
  doi          = {10.1109/TPDS.2021.3065591},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {288-301},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Endurance-aware mapping of spiking neural networks to neuromorphic hardware},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GIRAF: General purpose in-storage resistive associative
framework. <em>TPDS</em>, <em>33</em>(2), 276‚Äì287. (<a
href="https://doi.org/10.1109/TPDS.2021.3065448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GIRAF is a General purpose In-storage Resistive Associative Framework based on resistive content addressable memory (RCAM), which functions simultaneously as a storage and a massively parallel associative processor. GIRAF alleviates the bandwidth wall by connecting every memory bit to processing transistors and keeping computing inside the storage arrays, thus implementing deep in-data, rather than near-data, processing. We show that GIRAF outperformed a reference computer architecture with a bandwidth-limited external storage access on a variety of data-intensive workloads. The performance of GIRAF Dot Product and Sparse Matrix-Vector multiplication exceeds the attainable performance of a reference architecture by 1200 √ó and 130 √ó, respectively.},
  archive      = {J_TPDS},
  author       = {Leonid Yavits and Roman Kaplan and Ran Ginosar},
  doi          = {10.1109/TPDS.2021.3065448},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {276-287},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GIRAF: General purpose in-storage resistive associative framework},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Look-up-table based processing-in-memory architecture with
programmable precision-scaling for deep learning applications.
<em>TPDS</em>, <em>33</em>(2), 263‚Äì275. (<a
href="https://doi.org/10.1109/TPDS.2021.3066909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing in memory (PIM) architecture, with its ability to perform ultra-low-latency parallel processing, is regarded as a more suitable alternative to von Neumann computing architectures for implementing data-intensive applications such as Deep Neural Networks (DNN) and Convolutional Neural Networks (CNN). In this article, we present a Look-up Table (LUT) based PIM architecture aimed at CNN/DNN acceleration that replaces logic-based processing with pre-calculated results stored inside the LUTs in order to perform complex computations on the DRAM memory platform. Our LUT-based DRAM-PIM architecture offers superior performance at a significantly higher energy-efficiency compared to the more conventional bit-wise parallel PIM architectures, while at the same time avoids fabrication challenges associated with the in-memory implementation of logic circuits. Alongside, the processing elements can be programmed and re-programmed to perform virtually any operation, including operations of Convolutional, Fully Connected, Pooling, and Activating Layers of CNN/DNN. Furthermore, it is capable of operating on several combinations of bit-widths of the operand data and thereby offers a wider range of flexibility across performance, precision, and efficiency. Transmission Gate (TG) realization of the circuitry ensures minimal footprint from the PIM architecture. Our simulations demonstrate that the proposed architecture can perform AlexNet inference at a nearly 13√ó faster rate and 125√ó more efficiency compared to state-of-the-art GPU and also provides 1.35√ó higher throughput at 2.5√ó higher energy-efficiency than another recent DRAM-implemented LUT-based PIM architecture in its baseline operation mode. Moreover, it offers 12√ó higher frame-rate at 9√ó more efficiency per frame for the lowest operand precision setting, with respect to its own baseline operation mode.},
  archive      = {J_TPDS},
  author       = {Purab Ranjan Sutradhar and Sathwika Bavikadi and Mark Connolly and Savankumar Prajapati and Mark A. Indovina and Sai Manoj Pudukotai Dinakarrao and Amlan Ganguly},
  doi          = {10.1109/TPDS.2021.3066909},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {263-275},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Look-up-table based processing-in-memory architecture with programmable precision-scaling for deep learning applications},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Silent-PIM: Realizing the processing-in-memory computing
with standard memory requests. <em>TPDS</em>, <em>33</em>(2), 251‚Äì262.
(<a href="https://doi.org/10.1109/TPDS.2021.3065365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Deep Neural Network (DNN), Recurrent Neural Network (RNN) applications, rapidly becoming attractive to the market, process a large amount of low-locality data; thus, the memory bandwidth limits their peak performance. Therefore, many data centers actively adapt high-bandwidth memory like HBM2/HBM2E to resolve the problem. However, this approach would not provide a complete solution since it still transfers the data from the memory to the computing unit. Thus, processing-in-memory (PIM), which performs the computation inside memory, has attracted attention. However, most previous methods require the modification or the extension of core pipelines and memory system components like memory controllers, making the practical implementation of PIM very challenging and expensive in development. In this article, we propose a Silent-PIM that performs the PIM computation with standard DRAM memory requests; thus, requiring no hardware modifications and allowing the PIM memory device to perform the computation while servicing non-PIM applications‚Äô memory requests. We can achieve our design goal by preserving the standard memory request behaviors and satisfying the DRAM standard timing requirements. In addition, using standard memory requests makes it possible to use DMA as a PIM‚Äôs offloading engine, resulting in processing the PIM memory requests fast and making a core perform other tasks. We compared the performance of three Long Short-Term Memory models (LSTM) kernels on real platforms, such as the Silent-PIM modeled on the FPGA, GPU, and CPU. For $(p \times 512) \times (512 \times 2048)$ matrix multiplication with a batch size $p$ varying from 1 to 128, the Silent-PIM performed up to 16.9x and 24.6x faster than GPU and CPU, respectively, $p=1$ , which was the case without having any data reuse. At $p=128$ , the highest data reuse case, the GPU performance was the highest, but the PIM performance was still higher than the CPU execution. Similarly, at $(p \times 2048)$ element-wise multiplication and addition, where there was no data reuse, the Silent-PIM always achieved higher than both CPU and GPU. It also showed that when the PIM‚Äôs EDP performance was superior to the others in all the cases having no data reuse.},
  archive      = {J_TPDS},
  author       = {Chang Hyun Kim and Won Jun Lee and Yoonah Paik and Kiyong Kwon and Seok Young Kim and Il Park and Seon Wook Kim},
  doi          = {10.1109/TPDS.2021.3065365},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {251-262},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Silent-PIM: Realizing the processing-in-memory computing with standard memory requests},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guest editorial: Special section on parallel and distributed
computing techniques for non-von neumann technologies. <em>TPDS</em>,
<em>33</em>(2), 249‚Äì250. (<a
href="https://doi.org/10.1109/TPDS.2021.3093148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The title of this special section, Parallel and Distributed Computing Techniques for Non-Von Neumann Technologies, is a bit misleading. Technically, parallel and distributed computers already diverge from the basic architecture that John von Neumann proposed in 1945, although they are still based on processors that execute a sequence of instructions, each of which performs a simple action such as computing an arithmetic result, reading or writing memory, or branching to a new location in the instruction sequence. But what would a computer that is not based on this model of execution look like? The technologies discussed in the following articles are more exotic, more innovative, and more intriguing than what you are likely to encounter in a typical collection of peer reviewed computer-science articles. We hope that these articles will help you view computing in a new light and give you a sense of what the future of computing may look like.},
  archive      = {J_TPDS},
  author       = {Scott Pakin and Christof Teuscher and Catherine Schuman},
  doi          = {10.1109/TPDS.2021.3093148},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {249-250},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Guest editorial: Special section on parallel and distributed computing techniques for non-von neumann technologies},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Taming system dynamics on resource optimization for data
processing workflows: A probabilistic approach. <em>TPDS</em>,
<em>33</em>(1), 231‚Äì248. (<a
href="https://doi.org/10.1109/TPDS.2021.3091400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many data-intensive applications, workflow is often used as an important model for organizing data processing tasks and resource provisioning is an important and challenging problem for improving the performance of workflows. Recently, system variations in the cloud and large-scale clusters, such as those in I/O and network performances and failure events, have been observed to greatly affect the performance of workflows. Traditional resource provisioning methods, which overlook these variations, can lead to suboptimal resource provisioning results. In this article, we provide a general solution for workflow performance optimizations considering system variations. Specifically, we model system dynamics as time-dependent random variables and take their probability distributions as optimization input. Despite its effectiveness, this solution involves heavy computation overhead. Thus, we propose three pruning techniques to simplify workflow structure and reduce the probability evaluation overhead. We implement our techniques in a runtime library, which allows users to incorporate efficient probabilistic optimization into existing resource provisioning methods. Experiments show that probabilistic solutions can improve the performance by up to 65 percent compared to state-of-the-art static solutions, and our pruning techniques can greatly reduce the overhead of our probabilistic approach.},
  archive      = {J_TPDS},
  author       = {Amelie Chi Zhou and Weilin Xue and Yao Xiao and Bingsheng He and Shadi Ibrahim and Reynold Cheng},
  doi          = {10.1109/TPDS.2021.3091400},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {231-248},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Taming system dynamics on resource optimization for data processing workflows: A probabilistic approach},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PLVER: Joint stable allocation and content replication for
edge-assisted live video delivery. <em>TPDS</em>, <em>33</em>(1),
218‚Äì230. (<a href="https://doi.org/10.1109/TPDS.2021.3090784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Live streaming services have gained extreme popularity in recent years. Due to the spiky traffic patterns of live videos, utilizing distributed edge servers to improve viewers&#39; quality of experience (QoE) has become a common practice nowadays. Nevertheless, the current client-driven content caching mechanism does not support pre-caching from the cloud to the edge, resulting in a considerable amount of cache misses in live video delivery. By jointly considering the features of live videos and edge servers, we propose PLVER, a proactive live video push scheme to address the cache miss problem in live video delivery. Specifically, PLVER first conducts a one-to-multiple stable allocation between edge clusters and user groups to balance the load of live traffic over the edge servers. It then adopts proactive video replication algorithms to speed up video replication among the edge servers. We conduct extensive trace-driven evaluation, covering 0.3 million Twitch viewers and more than 300 Twitch channels. The results demonstrate that with PLVER, edge servers can carry 28 and 82 percent more traffic than the auction-based replication (ABR) method and the caching on requested time (CORT) method, respectively.},
  archive      = {J_TPDS},
  author       = {Huan Wang and Guoming Tang and Kui Wu and Jianping Wang},
  doi          = {10.1109/TPDS.2021.3090784},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {218-230},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {PLVER: Joint stable allocation and content replication for edge-assisted live video delivery},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy-efficient cache-aware scheduling on heterogeneous
multicore systems. <em>TPDS</em>, <em>33</em>(1), 206‚Äì217. (<a
href="https://doi.org/10.1109/TPDS.2021.3090587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of heterogeneous multicore architectures into deadline-constrained embedded systems has various benefits in terms of schedulability and energy-efficiency. Existing energy-efficient algorithms, in this domain, allocate tasks to their energy-favorable core-types while using dynamic voltage and frequency scaling to reduce energy consumption. However, the practicality of such algorithms is limited due to the underlying assumptions made to simplify the analysis. This article paves the way for more practical approaches to minimize the energy consumption on heterogeneous multicores. Specifically, we investigate the nonlinear impacts that core-frequency and cache-partitioning have on task-executions in a heterogeneous multicore environment. In doing so, we propose an algorithm that exploits this relationship to effectively allocate tasks to specific cores and core-types, and determine the number of cache-partitions for each core. Extensive simulations using real-world benchmarks show the proficiency of our approach by achieving an average and maximum energy savings of 14.9 and 20.4 percent, respectively for core-level energy consumption, and 20.2 and 60.4 percent, respectively for system-level energy consumption.},
  archive      = {J_TPDS},
  author       = {Saad Zia Sheikh and Muhammad Adeel Pasha},
  doi          = {10.1109/TPDS.2021.3090587},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {206-217},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Energy-efficient cache-aware scheduling on heterogeneous multicore systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Communication-efficient federated learning with compensated
overlap-FedAvg. <em>TPDS</em>, <em>33</em>(1), 192‚Äì205. (<a
href="https://doi.org/10.1109/TPDS.2021.3090331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While petabytes of data are generated each day by a number of independent computing devices, only a few of them can be finally collected and used for deep learning (DL) due to the apprehension of data security and privacy leakage, thus seriously retarding the extension of DL. In such a circumstance, federated learning (FL) was proposed to perform model training by multiple clients&#39; combined data without the dataset sharing within the cluster. Nevertheless, federated learning with periodic model averaging (FedAvg) introduced massive communication overhead as the synchronized data in each iteration is about the same size as the model, and thereby leading to a low communication efficiency. Consequently, variant proposals focusing on the communication rounds reduction and data compression were proposed to decrease the communication overhead of FL. In this article, we propose Overlap-FedAvg, an innovative framework that loosed the chain-like constraint of federated learning and paralleled the model training phase with the model communication phase (i.e., uploading local models and downloading the global model), so that the latter phase could be totally covered by the former phase. Compared to vanilla FedAvg, Overlap-FedAvg was further developed with a hierarchical computing strategy, a data compensation mechanism, and a nesterov accelerated gradients (NAG) algorithm. In Particular, Overlap-FedAvg is orthogonal to many other compression methods so that they could be applied together to maximize the utilization of the cluster. Besides, the theoretical analysis is provided to prove the convergence of the proposed framework. Extensive experiments conducting on both image classification and natural language processing tasks with multiple models and datasets also demonstrate that the proposed framework substantially reduced the communication overhead and boosted the federated learning process.},
  archive      = {J_TPDS},
  author       = {Yuhao Zhou and Qing Ye and Jiancheng Lv},
  doi          = {10.1109/TPDS.2021.3090331},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {192-205},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Communication-efficient federated learning with compensated overlap-FedAvg},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable, confidential and survivable software updates.
<em>TPDS</em>, <em>33</em>(1), 176‚Äì191. (<a
href="https://doi.org/10.1109/TPDS.2021.3090330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software update systems must guarantee high availability, integrity and security even in presence of cyber attacks. We propose the first survivable software update framework for the secure distribution of confidential updates that is based on a distributed infrastructure with no single points of failure. Previous works guarantee either survivability or confidentiality of software updates but do not ensure both properties. Our proposal is based on an original application of a multi-authority attribute-based encryption scheme in the context of decentralized access control management that avoids single-point-of-vulnerability. We describe the original framework, propose the protocols to implement it, and demonstrate its feasibility through a security and performance evaluation.},
  archive      = {J_TPDS},
  author       = {Federico Magnanini and Luca Ferretti and Michele Colajanni},
  doi          = {10.1109/TPDS.2021.3090330},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {176-191},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Scalable, confidential and survivable software updates},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A pattern-based SpGEMM library for multi-core and many-core
architectures. <em>TPDS</em>, <em>33</em>(1), 159‚Äì175. (<a
href="https://doi.org/10.1109/TPDS.2021.3090328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {General sparse matrix-matrix multiplication (SpGEMM) is one of the most important mathematical library routines in a number of applications. In recent years, several efficient SpGEMM algorithms have been proposed, however, most of them are based on the compressed sparse row (CSR) format, and the possible performance gain from exploiting other formats has not been well studied. And some specific algorithms are restricted to parameter tuning that has a significant impact on performance. So the particular format, algorithm, and parameter that yield the best performance for SpGEMM remain undetermined. In this article, we conduct a prospective study on format-specific parallel SpGEMM algorithms and analyze their pros and cons. We then propose a pattern-based SpGEMM library, that provides a unified programming interface in the CSR format, analyses the pattern of two input matrices, and automatically determines the best format, algorithm, and parameter for arbitrary matrix pairs. For this purpose, we build an algorithm set that integrates three new designed algorithms with existing popular libraries, and design a hybrid deep learning model called MatNet to quickly identify patterns of input matrices and accurately predict the best solution by using sparse features and density representations. The evaluation shows that this library consistently outperforms the state-of-the-art library. We also demonstrate its adaptability in an AMG solver and a BFS algorithm with 30 percent performance improvement.},
  archive      = {J_TPDS},
  author       = {Zhen Xie and Guangming Tan and Weifeng Liu and Ninghui Sun},
  doi          = {10.1109/TPDS.2021.3090328},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {159-175},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A pattern-based SpGEMM library for multi-core and many-core architectures},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Elastic deep learning in multi-tenant GPU clusters.
<em>TPDS</em>, <em>33</em>(1), 144‚Äì158. (<a
href="https://doi.org/10.1109/TPDS.2021.3064966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study how to support elasticity, that is, the ability to dynamically adjust the parallelism (i.e., the number of GPUs), for deep neural network (DNN) training in a GPU cluster. Elasticity can benefit multi-tenant GPU cluster management in many ways, for example, achieving various scheduling objectives (e.g., job throughput, job completion time, GPU efficiency) according to cluster load variations, utilizing transient idle resources, and supporting performance profiling, job migration, and straggler mitigation. We propose EDL, which enables elastic deep learning with a simple API and can be easily integrated with existing deep learning frameworks such as TensorFlow and PyTorch. EDL also incorporates techniques that are necessary to reduce the overhead of parallelism adjustments, such as stop-free scaling and dynamic data pipeline. We demonstrate with experiments that EDL can indeed bring significant benefits to the above-listed applications in GPU cluster management.},
  archive      = {J_TPDS},
  author       = {Yidi Wu and Kaihao Ma and Xiao Yan and Zhi Liu and Zhenkun Cai and Yuzhen Huang and James Cheng and Han Yuan and Fan Yu},
  doi          = {10.1109/TPDS.2021.3064966},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {144-158},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Elastic deep learning in multi-tenant GPU clusters},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient distributed approaches to core maintenance on
large dynamic graphs. <em>TPDS</em>, <em>33</em>(1), 129‚Äì143. (<a
href="https://doi.org/10.1109/TPDS.2021.3090759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a fundamental problem in graph analysis, core decomposition aims to compute the core numbers of vertices in a given graph. It is a powerful tool for mining important graph structures. For dynamic graphs with real-time updates of vertices/edges, core maintenance has been utilized to update the core numbers of vertices. The previous approaches to core maintenance face challenges in terms of storage and efficiency. In this article, we investigate distributed approaches to core maintenance on a pregel-like system, which is a famous graph computing system. We first design a core decomposition algorithm to obtain core numbers of vertices in a given graph. Based on it, a distributed batch-stream combined algorithm (DBCA) is devised to efficiently maintain the core numbers when vertex/edge updates happen. In particular, we introduce a new task assignment strategy to DBCA based on diversity of the edge-cores of updated edges. To ensure that DBCA can accurately process core maintenance, we develop a message interaction protocol to resolve the problem of crosstalk among different tasks. Comprehensive experiments have been conducted on real/synthetic graphs, more specifically, in two typical distributed environments built on Supercomputing Center and Alibaba Cloud. The experiment results demonstrate that our proposed algorithms are efficient and scalable.},
  archive      = {J_TPDS},
  author       = {Tongfeng Weng and Xu Zhou and Kenli Li and Peng Peng and Keqin Li},
  doi          = {10.1109/TPDS.2021.3090759},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {129-143},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient distributed approaches to core maintenance on large dynamic graphs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FlitZip: Effective packet compression for NoC in
MultiProcessor system-on-chip. <em>TPDS</em>, <em>33</em>(1), 117‚Äì128.
(<a href="https://doi.org/10.1109/TPDS.2021.3090315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applications running on Network on Chip (NoC) based multicore systems demand increased on-chip network bandwidth that can cater to the need for intensive communication among the cores and caches. Due to strict area and power budget, the bandwidth offered by NoC is very limited. Data-intensive and communication-centric applications on encountering a cache miss lead to a considerable burden on the underlying network for transferring blocks from multiple cache hierarchies to the requesting core as packets. This increases the packet transmission latency, thereby slowing down the system performance. Also, NoC being the highest component of power consumption after the cores, an increase in packets increases the dynamic power consumption of NoC. The article proposes FlitZip that addresses the problem by reducing on-chip traffic through compressing network packets. Hence, the compressed packet requires less bandwidth during its transfer, reducing the network&#39;s power consumption. Experimental analysis shows that FlitZip achieves a better compression ratio of 52 percent, reduces packet latency and bandwidth utilization by 19.28 and 27 percent, respectively. It also reduces the area and power consumption of the de/compression units by 53.33 and 62.3 percent, respectively, compared to the state-of-the-art packet compression technique, NoŒî.},
  archive      = {J_TPDS},
  author       = {Dipika Deb and Rohith M.K. and John Jose},
  doi          = {10.1109/TPDS.2021.3090315},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {117-128},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FlitZip: Effective packet compression for NoC in MultiProcessor system-on-chip},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). COSCO: Container orchestration using co-simulation and
gradient based optimization for fog computing environments.
<em>TPDS</em>, <em>33</em>(1), 101‚Äì116. (<a
href="https://doi.org/10.1109/TPDS.2021.3087349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent task placement and management of tasks in large-scale fog platforms is challenging due to the highly volatile nature of modern workload applications and sensitive user requirements of low energy consumption and response time. Container orchestration platforms have emerged to alleviate this problem with prior art either using heuristics to quickly reach scheduling decisions or AI driven methods like reinforcement learning and evolutionary approaches to adapt to dynamic scenarios. The former often fail to quickly adapt in highly dynamic environments, whereas the latter have run-times that are slow enough to negatively impact response time. Therefore, there is a need for scheduling policies that are both reactive to work efficiently in volatile environments and have low scheduling overheads. To achieve this, we propose a Gradient Based Optimization Strategy using Back-propagation of gradients with respect to Input (GOBI). Further, we leverage the accuracy of predictive digital-twin models and simulation capabilities by developing a Coupled Simulation and Container Orchestration Framework (COSCO). Using this, we create a hybrid simulation driven decision approach, GOBI*, to optimize Quality of Service (QoS) parameters. Co-simulation and the back-propagation approaches allow these methods to adapt quickly in volatile environments. Experiments conducted using real-world data on fog applications using the GOBI and GOBI* methods, show a significant improvement in terms of energy consumption, response time, Service Level Objective and scheduling time by up to 15, 40, 4, and 82 percent respectively when compared to the state-of-the-art algorithms.},
  archive      = {J_TPDS},
  author       = {Shreshth Tuli and Shivananda R. Poojara and Satish N. Srirama and Giuliano Casale and Nicholas R. Jennings},
  doi          = {10.1109/TPDS.2021.3087349},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {101-116},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {COSCO: Container orchestration using co-simulation and gradient based optimization for fog computing environments},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Horus: Interference-aware and prediction-based scheduling in
deep learning systems. <em>TPDS</em>, <em>33</em>(1), 88‚Äì100. (<a
href="https://doi.org/10.1109/TPDS.2021.3079202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To accelerate the training of Deep Learning (DL) models, clusters of machines equipped with hardware accelerators such as GPUs are leveraged to reduce execution time. State-of-the-art resource managers are needed to increase GPU utilization and maximize throughput. While co-locating DL jobs on the same GPU has been shown to be effective, this can incur interference causing slowdown. In this article we propose Horus: an interference-aware and prediction-based resource manager for DL systems. Horus proactively predicts GPU utilization of heterogeneous DL jobs extrapolated from the DL model&#39;s computation graph features, removing the need for online profiling and isolated reserved GPUs. Through micro-benchmarks and job co-location combinations across heterogeneous GPU hardware, we identify GPU utilization as a general proxy metric to determine good placement decisions, in contrast to current approaches which reserve isolated GPUs to perform online profiling and directly measure GPU utilization for each unique submitted job. Our approach promotes high resource utilization and makespan reduction; via real-world experimentation and large-scale trace driven simulation, we demonstrate that Horus outperforms other DL resource managers by up to 61.5 percent for GPU resource utilization, 23.7-30.7 percent for makespan reduction and 68.3 percent in job wait time reduction.},
  archive      = {J_TPDS},
  author       = {Gingfung Yeung and Damian Borowiec and Renyu Yang and Adrian Friday and Richard Harper and Peter Garraghan},
  doi          = {10.1109/TPDS.2021.3079202},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {88-100},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Horus: Interference-aware and prediction-based scheduling in deep learning systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimizing depthwise separable convolution operations on
GPUs. <em>TPDS</em>, <em>33</em>(1), 70‚Äì87. (<a
href="https://doi.org/10.1109/TPDS.2021.3084813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The depthwise separable convolution is commonly seen in convolutional neural networks (CNNs), and is widely used to reduce the computation overhead of a standard multi-channel 2D convolution. Existing implementations of depthwise separable convolutions target accelerating model training with large batch sizes with a large number of samples to be processed at once. Such approaches are inadequate for small-batch-sized model training and the typical scenario of model inference where the model takes in a few samples at once. This article aims to bridge the gap of optimizing depthwise separable convolutions by targeting the GPU architecture. We achieve this by designing two novel algorithms to improve the column and row reuse of the convolution operation to reduce the number of memory operations performed on the width and the height dimensions of the 2D convolution. Our approach employs a dynamic tile size scheme to adaptively distribute the computational data across GPU threads to improve GPU utilization and to hide the memory access latency. We apply our approach on two GPU platforms: an NVIDIA RTX 2080Ti GPU and an embedded NVIDIA Jetson AGX Xavier GPU, and two data types: 32-bit floating point (FP32) and 8-bit integer (INT8). We compared our approach against cuDNN that is heavily tuned for the NVIDIA GPU architecture. Experimental results show that our approach delivers over 2√ó (up to 3√ó) performance improvement over cuDNN. We show that, when using a moderate batch size, our approach averagely reduces the end-to-end training time of MobileNet and EfficientNet by 9.7 and 7.3 percent respectively, and reduces the end-to-end inference time of MobileNet and EfficientNet by 12.2 and 11.6 percent respectively.},
  archive      = {J_TPDS},
  author       = {Gangzhao Lu and Weizhe Zhang and Zheng Wang},
  doi          = {10.1109/TPDS.2021.3084813},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {70-87},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimizing depthwise separable convolution operations on GPUs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal repair-scaling trade-off in locally repairable
codes: Analysis and evaluation. <em>TPDS</em>, <em>33</em>(1), 56‚Äì69.
(<a href="https://doi.org/10.1109/TPDS.2021.3087352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to improve the repair performance of erasure-coded storage is a critical issue for maintaining high reliability of modern large-scale storage systems. Locally repairable codes (LRC) are one popular family of repair-efficient erasure codes that mitigate the repair bandwidth and are deployed in practice. To adapt to the changing demands of access efficiency and fault tolerance, modern storage systems also conduct frequent scaling operations on erasure-coded data. In this article, we analyze the optimal trade-off between the repair and scaling performance of LRC in clustered storage systems. Specifically, we focus on two optimal repair-scaling trade-offs, and design placement strategies that operate along the two optimal repair-scaling trade-off curves subject to the fault tolerance constraints. We prototype and evaluate our placement strategies on a LAN testbed, and show that they outperform the conventional placement schemes in repair and scaling operations.},
  archive      = {J_TPDS},
  author       = {Si Wu and Zhirong Shen and Patrick P. C. Lee and Yinlong Xu},
  doi          = {10.1109/TPDS.2021.3087352},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {56-69},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimal repair-scaling trade-off in locally repairable codes: Analysis and evaluation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). <span
class="math inline"><em>r</em><em>u</em><em>n</em></span> runData:
Re-distributing data via piggybacking for geo-distributed data analytics
over edges. <em>TPDS</em>, <em>33</em>(1), 40‚Äì55. (<a
href="https://doi.org/10.1109/TPDS.2021.3086274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficiently analyzing geo-distributed datasets is emerging as a major demand in a cloud-edge system. Since the datasets are often generated in closer proximity to end users, traditional works mainly focus on offloading proper tasks from those hotspot edges to the datacenter to decrease the overall completion time of submitted jobs in a one-shot manner. However, optimizing the completion time of current job alone is insufficient in a long-term scope since some datasets would be used multiple times. Instead, optimizing the data distribution is much more efficient and could directly benefit forthcoming jobs, although it may postpone the execution of current one. Unfortunately, due to the throwaway feature of data fetcher, existing data analytics systems fail to re-distribute corresponding data out of hotspot edges after the execution of data analytics. In order to minimize the overall completion time for a sequence of jobs as well as to guarantee the performance of current one, we propose to re-distribute the data along with task offloading, and formulate corresponding Œµ-bounded data-driven task scheduling problem over wide area network under the consideration of edge heterogeneity. We design an online schema run Data, which offloads proper tasks and related data via piggybacking to the datacenter based on delicately calculated probabilities. Through rigorous theoretical analysis, run Data is proved concentrated on its optimum with high probability. We implement run Data based on Spark and HDFS. Both testbed results and trace-driven simulations show that run Data re-distributes proper data via piggybacking and achieves up to 37 percent reduction on average response time compared with state-of-the-art schemas.},
  archive      = {J_TPDS},
  author       = {Yibo Jin and Zhuzhong Qian and Song Guo and Sheng Zhang and Lei Jiao and Sanglu Lu},
  doi          = {10.1109/TPDS.2021.3086274},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {40-55},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {$run$ runData: Re-distributing data via piggybacking for geo-distributed data analytics over edges},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Capelin: Data-driven compute capacity procurement for cloud
datacenters using portfolios of scenarios. <em>TPDS</em>,
<em>33</em>(1), 26‚Äì39. (<a
href="https://doi.org/10.1109/TPDS.2021.3084816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud datacenters provide a backbone to our digital society. Inaccurate capacity procurement for cloud datacenters can lead to significant performance degradation, denser targets for failure, and unsustainable energy consumption. Although this activity is core to improving cloud infrastructure, relatively few comprehensive approaches and support tools exist for mid-tier operators, leaving many planners with merely rule-of-thumb judgement. We derive requirements from a unique survey of experts in charge of diverse datacenters in several countries. We propose Capelin, a data-driven, scenario-based capacity planning system for mid-tier cloud datacenters. Capelin introduces the notion of portfolios of scenarios, which it leverages in its probing for alternative capacity-plans. At the core of the system, a trace-based, discrete-event simulator enables the exploration of different possible topologies, with support for scaling the volume, variety, and velocity of resources, and for horizontal (scale-out) and vertical (scale-up) scaling. Capelin compares alternative topologies and for each gives detailed quantitative operational information, which could facilitate human decisions of capacity planning. We implement and open-source Capelin, and show through comprehensive trace-based experiments it can aid practitioners. The results give evidence that reasonable choices can be worse by a factor of 1.5-2.0 than the best, in terms of performance degradation or energy consumption.},
  archive      = {J_TPDS},
  author       = {Georgios Andreadis and Fabian Mastenbroek and Vincent van Beek and Alexandru Iosup},
  doi          = {10.1109/TPDS.2021.3084816},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {26-39},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Capelin: Data-driven compute capacity procurement for cloud datacenters using portfolios of scenarios},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Error-compensated sparsification for communication-efficient
decentralized training in edge environment. <em>TPDS</em>,
<em>33</em>(1), 14‚Äì25. (<a
href="https://doi.org/10.1109/TPDS.2021.3084104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication has been considered as a major bottleneck in large-scale decentralized training systems since participating nodes iteratively exchange large amounts of intermediate data with their neighbors. Although compression techniques like sparsification can significantly reduce the communication overhead in each iteration, errors caused by compression will be accumulated, resulting in a severely degraded convergence rate. Recently, the error compensation method for sparsification has been proposed in centralized training to tolerate the accumulated compression errors. However, the analog technique and the corresponding theory about its convergence in decentralized training are still unknown. To fill in the gap, we design a method named ECSD-SGD that significantly accelerates decentralized training via error-compensated sparsification. The novelty lies in that we identify the component of the exchanging information in each iteration (i.e., the sparsified model update) and make targeted error compensation over the component. Our thorough theoretical analysis shows that ECSD-SGD supports arbitrary sparsification ratio and achieves the same convergence rate as the non-sparsified decentralized training methods. We also conduct extensive experiments on multiple deep learning models to validate our theoretical findings. Results show that ECSD-SGD outperforms all the start-of-the-art sparsified methods in terms of both the convergence speed and the final generalization accuracy.},
  archive      = {J_TPDS},
  author       = {Haozhao Wang and Song Guo and Zhihao Qu and Ruixuan Li and Ziming Liu},
  doi          = {10.1109/TPDS.2021.3084104},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {14-25},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Error-compensated sparsification for communication-efficient decentralized training in edge environment},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DeTraS: Delaying stores for friendly-fire mitigation in
hardware transactional memory. <em>TPDS</em>, <em>33</em>(1), 1‚Äì13. (<a
href="https://doi.org/10.1109/TPDS.2021.3085210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commercial Hardware Transactional Memory (HTM) systems are best-effort designs that leverage the coherence substrate to detect conflicts eagerly. Resolving conflicts in favor of the requesting core is the simplest option for ensuring deadlock freedom, yet it is prone to livelocks. In this work, we propose and evaluate DeTraS (Delayed Transactional Stores), an HTM-aware store buffer design aimed at mitigating such livelocks. DeTraS takes advantage of the fact that modern commercial processors implement a large store buffer, and uses it to prevent transactional stores predicted to conflict from performing early in the transaction. By leveraging existing processor structures, we propose a simple design that improves the ability of requester-wins HTM systems to achieve forward progress in spite of high contention while side-stepping the performance penalty of falling back to mutual exclusion. With just over 50 extra bytes, DeTraS captures the advantages of lazy conflict management without the complexity brought into the coherence fabric by commit arbitration schemes nor the relaxation of the single-writer invariant of prior works. Through detailed simulations of a 16-core tiled CMP using gem5, we demonstrate that DeTraS brings reductions in average execution time of 25 percent when compared to an Intel RTM-like design.},
  archive      = {J_TPDS},
  author       = {Rub√©n Titos-Gil and Ricardo Fern√°ndez-Pascual and Alberto Ros and Manuel E. Acacio},
  doi          = {10.1109/TPDS.2021.3085210},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DeTraS: Delaying stores for friendly-fire mitigation in hardware transactional memory},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
