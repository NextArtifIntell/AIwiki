<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TCDS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tcds---152">TCDS - 152</h2>
<ul>
<li><details>
<summary>
(2022). Accelerating evolutionary neural architecture search via
multifidelity evaluation. <em>TCDS</em>, <em>14</em>(4), 1778–1792. (<a
href="https://doi.org/10.1109/TCDS.2022.3179482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary neural architecture search (ENAS) has recently received increasing attention by effectively finding high-quality neural architectures, which however consumes high computational cost by training the architecture encoded by each individual for complete epochs in individual evaluation. Numerous ENAS approaches have been developed to reduce the evaluation cost, but it is often difficult for most of these approaches to achieve high evaluation accuracy. To address this issue, in this article, we propose an accelerated ENAS via multifidelity evaluation termed MFENAS, where the individual evaluation cost is significantly reduced by training the architecture encoded by each individual for only a small number of epochs. The balance between evaluation cost and evaluation accuracy is well maintained by suggesting a multifidelity evaluation, which identifies the potentially good individuals that cannot survive from previous generations by integrating multiple evaluations under different numbers of training epochs. Besides, a population initialization strategy is devised to produce diverse neural architectures varying from ResNet-like architectures to Inception-like ones. As shown by experiments, the proposed MFENAS takes only 0.6 GPU days to find the best architecture holding a 2.39% test error rate, which is superior to most state-of-the-art neural architecture search approaches. And the architectures transferred to CIFAR-100 and ImageNet also exhibit competitive performance.},
  archive      = {J_TCDS},
  author       = {Shangshang Yang and Ye Tian and Xiaoshu Xiang and Shichen Peng and Xingyi Zhang},
  doi          = {10.1109/TCDS.2022.3179482},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1778-1792},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Accelerating evolutionary neural architecture search via multifidelity evaluation},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-evolutionary neuron model for fast-response spiking
neural networks. <em>TCDS</em>, <em>14</em>(4), 1766–1777. (<a
href="https://doi.org/10.1109/TCDS.2021.3139444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose two simple and effective spiking neuron models to improve the response time of the conventional spiking neural network. The proposed neuron models adaptively tune the presynaptic input current depending on the input received from its presynapses and subsequent neuron firing events. We analyze and derive the firing activity homeostatic convergence of the proposed models. We experimentally verify and compare the models on MNIST handwritten digits and FashionMNIST classification tasks. We show that the proposed neuron models significantly increase the response speed to the input signal. Experiment codes are available at https://github.com/anvien/Evol-SNN .},
  archive      = {J_TCDS},
  author       = {Anguo Zhang and Ying Han and Yuzhen Niu and Yueming Gao and Zhizhang Chen and Kai Zhao},
  doi          = {10.1109/TCDS.2021.3139444},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1766-1777},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Self-evolutionary neuron model for fast-response spiking neural networks},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to integrate an artificial sensory device: How
bayesian integration may lead to nonoptimal perception. <em>TCDS</em>,
<em>14</em>(4), 1755–1765. (<a
href="https://doi.org/10.1109/TCDS.2022.3144092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines how artificial tactile stimulation from a novel noninvasive sensory device is learned and integrated with information from the visual sensory system. In our experiment, visual direction information was paired with reliable symbolic tactile information. Over several training blocks, discrimination performance in unimodal tactile test trials and participants’ confidence in their decision improved, indicating that participants could associate the visual and tactile information consciously and thus, learned the meaning of the symbolic tactile cues. Our results showed that information from both modalities is integrated during the early learning phase. Even though this integration is consistent with a Bayesian integration model, under certain conditions, it may even lead to nonoptimal perception such that participants’ performance is worse than if they were using only a single cue. Furthermore, we showed that a confidence-based Bayesian integration explains the observed behavioral data better than the classical variance-based Bayesian integration. The present study demonstrates that humans can consciously learn and integrate an artificial sensory device providing symbolic tactile information. We also shed light on how Bayesian integration can lead to nonoptimal perception by providing additional models and simulations. Our finding connects the field of multisensory integration to the development of sensory substitution systems.},
  archive      = {J_TCDS},
  author       = {Mohammad-Ali Nikouei Mahani and Karin Maria Bausenhart and Rolf Ulrich and Majid Nili Ahmadabadi},
  doi          = {10.1109/TCDS.2022.3144092},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1755-1765},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Learning to integrate an artificial sensory device: How bayesian integration may lead to nonoptimal perception},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A framework of improving human demonstration efficiency for
goal-directed robot skill learning. <em>TCDS</em>, <em>14</em>(4),
1743–1754. (<a href="https://doi.org/10.1109/TCDS.2021.3137262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robot learning from humans allows robots to automatically adjust to stochastic and dynamic environments by learning from nontechnical end user’s demonstrations, which is best known as robot programming by demonstration, robot learning from demonstration, apprenticeship learning, and imitation learning. Although most of those methods are probabilistic, and their performances intensively depend on the demonstrated data, measuring and evaluating human demonstrations are rarely investigated. A poorly demonstrated data set with useless prior knowledge or redundant demonstrations increases the complexity and time cost of robot learning. To solve these problems, a goal-directed robot skill learning framework named GPm-MOGP is presented. It 1) decides when and where to add a new demonstration by calculating the trajectory uncertainty; 2) determines which demonstration is useless or redundant by Kullback–Leibler (KL) divergence; 3) implements robot skill learning with a minimum number of demonstrations using a multioutput Gaussian process; and 4) learns orientation uncertainty and representation by combining logarithmic and exponential maps. The proposed framework significantly reduces the demonstrated effort of nontechnical end users who lack an understanding of how and what the robot learns during the demonstrating process. To evaluate the proposed framework, a pick-and-place experiment was designed with five unseen goals to verify the effectiveness of our methods. This experiment is well illustrated with two phases: 1) demonstration efficiency and 2) skill representation and reproduction. The results indicate an improvement of 60% in human demonstration efficiency, compared to common learning from demonstrations (LfD) applications that require at least ten demonstrations, and the robot average success rate of pick-and-place task reaches 85%.},
  archive      = {J_TCDS},
  author       = {Hongmin Wu and Wu Yan and Zhihao Xu and Xuefeng Zhou},
  doi          = {10.1109/TCDS.2021.3137262},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1743-1754},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A framework of improving human demonstration efficiency for goal-directed robot skill learning},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical deep multitask learning with the attention
mechanism for similarity learning. <em>TCDS</em>, <em>14</em>(4),
1729–1742. (<a href="https://doi.org/10.1109/TCDS.2021.3137316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similarity learning is often adopted as an auxiliary task of deep multitask learning methods to learn discriminant features. Most existing approaches only use the single-layer features extracted by the last fully connected layer, which ignores the abundant information of feature channels in lower layers. Besides, small cliques are the most commonly used methods in similarity learning tasks to model the correlation of data, which can lead to the limited relation learning. In this article, we present an end-to-end hierarchical deep multitask learning framework for similarity learning which can learn more discriminant features by sharing information from different layers of network and dealing with complex correlation. Its main task is graph similarity inference. We build focus graphs for each sample. Then, an attention mechanism and a node feature enhancing model are introduced into backbone network to extract the abundant and important channel information from multiple layers of network. In the similarity inference task, a relation enhancing mechanism is applied to graph convolutional network to leverage the crucial relation in channels, which can effectively facilitate the learning ability of the whole framework. The extensive experiments have been conducted to demonstrate the effectiveness of the proposed method on person reidentification and face clustering applications.},
  archive      = {J_TCDS},
  author       = {Yan Huang and Qicong Wang and Wenming Yang and Qingmin Liao and Hongying Meng},
  doi          = {10.1109/TCDS.2021.3137316},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1729-1742},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Hierarchical deep multitask learning with the attention mechanism for similarity learning},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep multiinstance mammogram classification with region
label assignment strategy and metric-based optimization. <em>TCDS</em>,
<em>14</em>(4), 1717–1728. (<a
href="https://doi.org/10.1109/TCDS.2021.3135947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mammography is one of the most widely used and effective ways to screen early breast cancer. Convolutional-neural-network-based methods have obtained promising results for automatic mammography diagnosis. However, most of those approaches ignore the relationship between global and local characteristics of mammograms and lose sight of the relationship between different views of a patient. This study designs a novel region label assignment strategy, which takes advantage of all regions in each mammogram from a patient by assigning different labels to different regions and calculating the loss for each region separately. This approach enables the classifier to distinguish variable and tiny lesions in complex global conditions better. Only one case-level classification label is needed for diagnosing one patient (case). Moreover, the categories of mammogram data sets are always imbalanced. To address this problem, this study designs an area under the receiver operating characteristic curve (AUC)-based optimization method on minibatch strategy. Experimental results on a constructed data set and two publicly available data sets demonstrate that the proposed method performs satisfactorily compared with state-of-the-art mammogram classifiers. Visualization results show the proposed method can find out mammograms containing malignant lesions and illustrate the rough location of lesions.},
  archive      = {J_TCDS},
  author       = {Dong Li and Lituan Wang and Ting Hu and Lei Zhang and Qing Lv},
  doi          = {10.1109/TCDS.2021.3135947},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1717-1728},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep multiinstance mammogram classification with region label assignment strategy and metric-based optimization},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic distribution alignment with dual-subspace mapping
for cross-subject driver mental state detection. <em>TCDS</em>,
<em>14</em>(4), 1705–1716. (<a
href="https://doi.org/10.1109/TCDS.2021.3137530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the detection of electroencephalogram-based driving mental states, it is important to utilize transfer learning to overcome individual and period differences. However, there are two challenges in existing unsupervised domain adaptation methods: 1) they ignore the geometric divergence of the source and target domains with single subspace mapping and 2) they usually employ a fixed weight distribution to align the marginal and conditional probability distributions. In this article, we propose a dynamic distribution alignment with dual-subspace mapping (DDADSM) method for cross-subject driver mental state detection. Initially, DDADSM explores two optimally aligned subspaces for the source and target domains, which can significantly reduce the geometric shifting. Subsequently, the dynamic probability distribution alignment method is introduced to acquire the adaptive weight between the marginal and conditional distributions, which adapts to domains with wide variations. Through our experiments based on the driving mental state detection task, DDADSM demonstrated superior performance compared with state-of-the-art models.},
  archive      = {J_TCDS},
  author       = {Jin Cui and Xuanyu Jin and Hua Hu and Li Zhu and Kenji Ozawa and Gang Pan and Wanzeng Kong},
  doi          = {10.1109/TCDS.2021.3137530},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1705-1716},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Dynamic distribution alignment with dual-subspace mapping for cross-subject driver mental state detection},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Motion learning and rapid generalization for musculoskeletal
systems based on recurrent neural network modulated by initial states.
<em>TCDS</em>, <em>14</em>(4), 1691–1704. (<a
href="https://doi.org/10.1109/TCDS.2021.3136854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Musculoskeletal robot with high precision and robustness is a promising direction for the next generation of robots. However, motion learning and rapid generalization of complex musculoskeletal systems are still challenging. Therefore, inspired by the movement preparation mechanism of the motor cortex, this article proposes a motion learning framework based on the recurrent neural network (RNN) modulated by initial states. First, two RNNs are introduced as a preparation network and an execution network to generate initial states of the execution network and time-varying motor commands of movement, respectively. The preparation network is trained by a reward-modulated learning rule, and the execution network is fixed. With the modulation of initial states, initial states can be explicitly expressed as knowledge of movements. By dividing the preparation and execution of movements into two RNNs, the motion learning is accelerated to converge under the application of the node-perturbation method. Second, with the utilization of learned initial states, a rapid generalization method for new movement targets is proposed. Initial states of unlearned movements can be computed by searching for low-dimensional ones in latent space constructed by learned initial states and then transforming them into the whole neural space. The proposed framework is verified in simulation with a musculoskeletal model. The results indicate that the proposed motion learning framework can realize goal-oriented movements of the musculoskeletal system with high precision and significantly improve the generalization efficiency for new movements.},
  archive      = {J_TCDS},
  author       = {Xiaona Wang and Jiahao Chen and Hong Qiao},
  doi          = {10.1109/TCDS.2021.3136854},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1691-1704},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Motion learning and rapid generalization for musculoskeletal systems based on recurrent neural network modulated by initial states},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual sentiment classification via low-rank regularization
and label relaxation. <em>TCDS</em>, <em>14</em>(4), 1678–1690. (<a
href="https://doi.org/10.1109/TCDS.2021.3135948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the human cognitive system, the emotional feeling is a complicated process. Visual sentiment classification aims to predict the human emotions evoked by different images. In this article, we proposed a novel visual sentiment classification algorithm by modeling this task as a low-rank subspace learning problem. To reduce the discrepancy between global and local features, image features of relevant regions are selected from the whole image by sparse encoding. The label relaxation item is employed for alleviating the label ambiguity caused by subjective evaluation. We develop an alternative iterative method to optimize the proposed objective function. This model can be naturally extended for online learning, which improves efficiency. We conduct extensive experiments on three publicly available data sets. Compared with several state-of-the-art methods, we achieve better performance.},
  archive      = {J_TCDS},
  author       = {Xiao Jin and Peiguang Jing and Jiesheng Wu and Jing Xu and Yuting Su},
  doi          = {10.1109/TCDS.2021.3135948},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1678-1690},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Visual sentiment classification via low-rank regularization and label relaxation},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recognizing tonal and nontonal mandarin sentences for
EEG-based brain–computer interface. <em>TCDS</em>, <em>14</em>(4),
1666–1677. (<a href="https://doi.org/10.1109/TCDS.2021.3137251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most current research has focused on nontonal languages such as English. However, more than 60% of the world’s population speaks tonal languages. Mandarin is the most spoken tonal languages in the world. Interestingly, the use of tone in tonal languages may represent different meanings of words and reflect feelings, which is very different from nontonal languages. The objective of this study is to determine whether a spoken Mandarin sentence with or without tone can be distinguished by analyzing electroencephalographic (EEG) signals. We first constructed a new Brain Research Center Speech (BRCSpeech) database to recognize Mandarin. The EEG data of 14 participants were recorded, while they articulated preselected sentences. To the best of our knowledge, this is the first study to apply the method of asymmetric feature extraction method for speech recognition using EEG signals. This study shows that the feature extraction method of rational asymmetry (RASM) can achieve the best accuracy in the classification of cross-subjects. In addition, our proposed binomial variable algorithm methodology can achieve 98.82% accuracy in cross-subject classification. Furthermore, we demonstrate that the use of eight channels [(F7, F8), (C5, C6), (P5, P6), and (O1, O2)] can achieve an accurate of 94.44%. This study explores the neurophysiological correlation of Mandarin pronunciation, which can help develop a tonal language synthesis system based on BCI in the future.},
  archive      = {J_TCDS},
  author       = {Shiau-Ru Yang and Tzyy-Ping Jung and Chin-Teng Lin and Kuan-Chih Huang and Chun-Shu Wei and Herming Chiueh and Yue-Loong Hsin and Guan-Ting Liou and Li-Chun Wang},
  doi          = {10.1109/TCDS.2021.3137251},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1666-1677},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Recognizing tonal and nontonal mandarin sentences for EEG-based Brain–Computer interface},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive spatiotemporal representation learning for
skeleton-based human action recognition. <em>TCDS</em>, <em>14</em>(4),
1654–1665. (<a href="https://doi.org/10.1109/TCDS.2021.3131253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How do humans recognize an action or an interaction in the real world? Due to the diversity of viewing perspectives, it is a challenge for humans to identify a regular activity when they observe it from an uncommon perspective. We argue that discriminative spatiotemporal information remains an essential cue for human action recognition. Most existing skeleton-based methods learn optimal representation based on the human-crafted criterion that requires many labeled data and much human effort. This article introduces adaptive skeleton-based neural networks to learn optimal spatiotemporal representation automatically through a data-driven manner. First, an adaptive skeleton representation transformation method (ASRT) is proposed to model view-variation data without hand-crafted criteria. Next, powered by a novel attentional LSTM (C3D-LSTM) encapsulated with 3-D-convolution, the proposed model could effectively enable memory blocks to learn short-term frame dependency and long-term relations. Hence, the proposed model can more accurately understand long-term or complex actions. Furthermore, a data enhancement-driven end-to-end training scheme is proposed to train key parameters under fewer training samples. Enhanced by learned high-performance spatiotemporal representation, the proposed model achieves state-of-the-art performance on five challenging benchmarks.},
  archive      = {J_TCDS},
  author       = {Jiahui Yu and Hongwei Gao and Yongquan Chen and Dalin Zhou and Jinguo Liu and Zhaojie Ju},
  doi          = {10.1109/TCDS.2021.3131253},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1654-1665},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Adaptive spatiotemporal representation learning for skeleton-based human action recognition},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recurrent network knowledge distillation for image rain
removal. <em>TCDS</em>, <em>14</em>(4), 1642–1653. (<a
href="https://doi.org/10.1109/TCDS.2021.3131045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-image rain removal (SIRR) based on deep learning has long been a problem of great interest in low-level vision systems. However, traditional convolutional neural network (CNN)-based approaches fail to capture long-range location dependencies effectively and may cause the image background blurred. In this article, we propose a knowledge distilling deraining network (KDRN) to address the SIRR problem. In the proposed network, the teacher regards rain streaks as a linear combination of many residual networks. It is used for image reconstruction at different resolutions. With the aid of a teacher network, the proposed deraining network performs better. A spatial channel aggregation residual attention block (SCARAB) is designed to remove the rain streaks. The block not only concentrates on the rain streak features but also captures the spatial-channel information of the image. For the network structure, we used an end-to-end approach to design the teacher and student networks separately. The proposed KDRN obtains the predicted residual image by a combination of the stage-wise results and the original input image. Extensive experiments show that the proposed KDRN obtains better subjective quality than most of the compared methods, on both heavy and light rain data sets.},
  archive      = {J_TCDS},
  author       = {Zhipeng Su and Yixiong Zhang and Jianghong Shi and Xiao-Ping Zhang},
  doi          = {10.1109/TCDS.2021.3131045},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1642-1653},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Recurrent network knowledge distillation for image rain removal},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RPPG-based heart rate estimation using spatial-temporal
attention network. <em>TCDS</em>, <em>14</em>(4), 1630–1641. (<a
href="https://doi.org/10.1109/TCDS.2021.3131197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote photoplethysmography (rPPG) based on the computer vision technology is widely used to calculate the heart rate (HR) from facial videos. Existing rPPG techniques have been subject to some limitations [e.g., highly redundant spatial information, head movement noise, and region of interest (ROI) selection]. To address these limitations, this article introduces an effective spatial-temporal attention network. A temporal fusion module is first proposed to fully exploit the time-domain information, aiming to reduce the redundant video information and strengthen the association relationships of long-range videos. Furthermore, a spatial attention mechanism is designed in the backbone net to precisely target the skin ROIs. Finally, to assist the network in learning the weights between channels, we project the RGB images using the plane orthogonal to skin (POS) algorithm and add motion representation to complement physiological signals’ extraction. The extensive experiments on the public PURE, MMSE-HR, and UBFC-rPPG datasets demonstrate that our model achieves competitive results compared with other methods.},
  archive      = {J_TCDS},
  author       = {Min Hu and Dong Guo and Mingxing Jiang and Fei Qian and Xiaohua Wang and Fuji Ren},
  doi          = {10.1109/TCDS.2021.3131197},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1630-1641},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {RPPG-based heart rate estimation using spatial-temporal attention network},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A computational developmental model of perceptual learning
for mobile robot. <em>TCDS</em>, <em>14</em>(4), 1615–1629. (<a
href="https://doi.org/10.1109/TCDS.2021.3128179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the perceptual learning (PL) of a mobile robot, how to improve the efficiency of its behavior decision is a great challenge. The existing methods suffer from the less flexibility and low efficiency. This article proposes a novel methodology to address these problems. It mimics the principle that the human’s brain can still think, even when he/she neither receives any sensory stimulus nor performs any action, during the off-task process, a gated self-organization mechanism is used to trigger the “brain” of the mobile robot to work, analyze the scenarios that it encountered in the former tasks to decide the best moving direction, then store the sensed scenario information by means of the lateral excitation of the internal neurons in a developmental network (DN), and set up the weight connections from the internal area to high-level decision-making area of the DN. In the following PL, if the robot encounters a similar scenario, it can make a better and faster behavior decision. Therefore, this methodology can greatly enhance the efficiency of behavior decision with the limited training samples, without retraining in facing a new scenario. Most importantly, this methodology makes the robot continuously improve its intelligence through the autonomous learning, even in offline state. Extensive simulation and experiment results of the mobile robot PL demonstrate its potential. Since it is task nonspecific, the same learning principles are potentially suitable for other fields. To the best of our knowledge, it is the first trial to apply the transfer learning through the lateral excitation mechanism of internal neurons to the robot field with the emergent representation.},
  archive      = {J_TCDS},
  author       = {Dongshu Wang and Kai Yang and Jianbin Xin},
  doi          = {10.1109/TCDS.2021.3128179},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1615-1629},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A computational developmental model of perceptual learning for mobile robot},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preference learning to multifocus image fusion via
generative adversarial network. <em>TCDS</em>, <em>14</em>(4),
1604–1614. (<a href="https://doi.org/10.1109/TCDS.2021.3126330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multifocus image fusion (MFIF) is to produce an all-in-focus fused image by integrating a pair of multifocus images with the same scene. In this article, an effective focus detection method is proposed for MFIF, by a generative adversarial network (GAN) with preference learning (PL). Benefitting from more obvious focus characteristics from the luminance channel in HSV, we take this luminance as the input of our GAN to carry out the easier focus detection, instead of the grayscale images in existing methods. On the other hand, to train our GAN more effectively, we utilize the ${\ell }_{{2},{1}}$ norm to construct a focus fidelity loss with structural group sparseness, to regularize the generator loss, pledging a more accurate focus confidence map. More importantly, a novel learning strategy, termed PL, is further developed to enhance model training. Functionally, it assigns a larger learning weight to a sample more difficult to be learned. Extensive experiments demonstrate that our proposed method is superior to other state-of-the-art methods.},
  archive      = {J_TCDS},
  author       = {Min He and Shishuang Yu and Rencan Nie and Chengchao Wang},
  doi          = {10.1109/TCDS.2021.3126330},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1604-1614},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Preference learning to multifocus image fusion via generative adversarial network},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A two-stream neural network for pose-based hand gesture
recognition. <em>TCDS</em>, <em>14</em>(4), 1594–1603. (<a
href="https://doi.org/10.1109/TCDS.2021.3126637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose-based hand gesture recognition has been widely studied in the recent years. Compared with full body action recognition, hand gesture involves joints that are more spatially closely distributed with stronger collaboration. This nature requires a different approach from action recognition to capturing the complex spatial features. Many gesture categories, such as “Grab” and “Pinch,” have very similar motion or temporal patterns posing a challenge on temporal processing. To address these challenges, this article proposes a two-stream neural network with one stream being a self-attention-based graph convolutional network (SAGCN) extracting the short-term temporal information and hierarchical spatial information, and the other being a residual-connection-enhanced bidirectional independently recurrent neural network (IndRNN) for extracting long-term temporal information. The SAGCN has a dynamic self-attention mechanism to adaptively exploit the relationships of all hand joints in addition to the fixed topology and local feature extraction in the GCNs. The proposed method effectively takes advantage of the GCN and IndRNN to capture the temporal-spatial information. The widely used Dynamic Hand Gesture dataset (two evaluation protocols) and First-Person Hand Action dataset are used to validate its effectiveness and our method achieves state-of-the-art performance with 96.31%, 94.05%, and 90.26%, respectively, in terms of recognition accuracy.},
  archive      = {J_TCDS},
  author       = {Chuankun Li and Shuai Li and Yanbo Gao and Xiang Zhang and Wanqing Li},
  doi          = {10.1109/TCDS.2021.3126637},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1594-1603},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A two-stream neural network for pose-based hand gesture recognition},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predefined-time synchronization of stochastic
memristor-based bidirectional associative memory neural networks with
time-varying delays. <em>TCDS</em>, <em>14</em>(4), 1584–1593. (<a
href="https://doi.org/10.1109/TCDS.2021.3126759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider the predefined-time synchronization problem of stochastic memristor-based bidirectional associative memory neural networks (MBAMNNs). First, considering the influence of stochastic disturbance, we propose two new predefined-time theorems. Second, combined with the predefined-time theorem proposed in this article, we design a feedback controller to realize the predefined-time synchronization of MBAMNNs. Then, the sufficient conditions for the predefined-time synchronization of MBAMNNs are obtained based on Ito’s formula and Lyapunov theorem. Finally, the effectiveness of the theorems is verified by numerical simulations.},
  archive      = {J_TCDS},
  author       = {Qingjie Wang and Hui Zhao and Aidi Liu and Lixiang Li and Sijie Niu and Chuan Chen},
  doi          = {10.1109/TCDS.2021.3126759},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1584-1593},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Predefined-time synchronization of stochastic memristor-based bidirectional associative memory neural networks with time-varying delays},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial cognition-driven deep learning for car detection in
unmanned aerial vehicle imagery. <em>TCDS</em>, <em>14</em>(4),
1574–1583. (<a href="https://doi.org/10.1109/TCDS.2021.3124764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object detection is the main challenge for image detection of unmanned aerial vehicles (UAVs), especially with small pixel ratios and blurred boundaries. In this article, a one-stage detector (SF-SSD) is proposed with a new spatial cognition algorithm. The deconvolution operation is introduced to a feature fusion module, which enhances the representation of shallow features. These more representative features prove effective for small-scale object detection. Empowered by a spatial cognition method, the deep model can redetect objects with less-reliable confidence scores. This enables the detector to improve detection accuracy significantly. Both between-class similarity and within-class similarity are fully exploited to suppress useless background information. This motivates the proposed model to take full use of semantic features in the detection process of multiclass small objects. A simplified network structure can improve the speed of object detection. The experiments are conducted on a newly collected dataset (SY-UAV) and the benchmark datasets (CARPK and PUCPR+). To further demonstrate the effectiveness of the spatial cognition module, a multiclass object detection experiment is conducted on the Stanford Drone dataset (SDD). The results show that the proposed model achieves high frame rates and better detection accuracies than the state-of-the-art methods, which are 90.1% (CAPPK), 90.8% (PUCPR+), and 91.2% (SDD).},
  archive      = {J_TCDS},
  author       = {Jiahui Yu and Hongwei Gao and Jian Sun and Dalin Zhou and Zhaojie Ju},
  doi          = {10.1109/TCDS.2021.3124764},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1574-1583},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Spatial cognition-driven deep learning for car detection in unmanned aerial vehicle imagery},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convolutional-recurrent neural networks with multiple
attention mechanisms for speech emotion recognition. <em>TCDS</em>,
<em>14</em>(4), 1564–1573. (<a
href="https://doi.org/10.1109/TCDS.2021.3123979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech emotion recognition (SER) aims to endow machines with the intelligence in perceiving latent affective components from speech. However, the existing works on deep-learning-based SER make it difficult to jointly consider time–frequency and sequential information in speech due to their structures, which may lead to deficiencies in exploring reasonable local emotional representations. In this regard, we propose a convolutional-recurrent neural network with multiple attention mechanisms (CRNN-MAs) for SER in this article, including the paralleled convolutional neural network (CNN) and long short-term memory (LSTM) modules, using extracted Mel-spectrums and frame-level features, respectively, in order to acquire time–frequency and sequential information simultaneously. Furthermore, we set three strategies for the proposed CRNN-MA: 1) a multiple self-attention layer in the CNN module on frame-level weights; 2) a multidimensional attention layer as the input features of the LSTM; and 3) a fusion layer summarizing the features of the two modules. Experimental results on three conventional SER corpora demonstrate the effectiveness of the proposed approach through using the convolutional-recurrent and multiple-attention modules, compared with other related models and existing state-of-the-art approaches.},
  archive      = {J_TCDS},
  author       = {Pengxu Jiang and Xinzhou Xu and Huawei Tao and Li Zhao and Cairong Zou},
  doi          = {10.1109/TCDS.2021.3123979},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1564-1573},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Convolutional-recurrent neural networks with multiple attention mechanisms for speech emotion recognition},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hierarchical architecture for multisymptom assessment of
early parkinson’s disease via wearable sensors. <em>TCDS</em>,
<em>14</em>(4), 1553–1563. (<a
href="https://doi.org/10.1109/TCDS.2021.3123157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parkinson’s disease (PD) is the second most common neurodegenerative disorder and the heterogeneity of early PD leads to interrater and intrarater variability in observation-based clinical assessment. Thus, objective monitoring of PD-induced motor abnormalities has attracted significant attention to manage disease progression. Here, we proposed a hierarchical architecture to reliably detect abnormal characteristics and comprehensively quantify the multisymptom severity in patients with PD. A novel wearable device was designed to measure motor features in 15 PD patients and 15 age-matched healthy subjects, while performing five types of motor tasks. The abnormality classes of multimodal measurements were recognized by hidden Markov models (HMMs) in the first layer of the proposed architecture, aiming at motivating the evaluation of specific motor manifestations. Subsequently, in the second layer, three single-symptom models differentiated PD motor characteristics from normal motion patterns and quantified the severity of cardinal PD symptoms in parallel. In order to further analyze the disease status, the multilevel severity quantification was fused in the third layer, where machine learning algorithms were adopted to develop a multisymptom severity score. The experimental results demonstrated that the quantification of three cardinal symptoms was highly accurate to distinguish PD patients from healthy controls. Furthermore, strong correlations were observed between the Unified PD Rating Scale (UPDRS) scores and the predicted subscores for tremor ${(R = 0.75,\;P = 1.40e - 3)}$ , bradykinesia ${(R = 0.71,\;P = 2.80e - 3)}$ , and coordination impairments ${(R = 0.69,\;P = 4.20e - 3)}$ , and the correlation coefficient can be enhanced to ${0.88}\,\,{(P = 1.26e - 5)}$ based on the fusion schemes. In conclusion, the proposed assessment architecture holds great promise to push forward the in-home monitoring of clinical manifestations, thus enabling the self-assessment of disease progression.},
  archive      = {J_TCDS},
  author       = {Chen Wang and Liang Peng and Zeng-Guang Hou and Yanfeng Li and Ying Tan and Honglin Hao},
  doi          = {10.1109/TCDS.2021.3123157},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1553-1563},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A hierarchical architecture for multisymptom assessment of early parkinson’s disease via wearable sensors},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient motion symbol detection and multikernel learning
for AER object recognition. <em>TCDS</em>, <em>14</em>(4), 1544–1552.
(<a href="https://doi.org/10.1109/TCDS.2021.3122131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Address event representation (AER) vision sensors process visual information by monitoring changes in light intensity and generating event streams. In this article, a threshold mechanism based on the statistical results of the peak membrane potentials is proposed to improve the accuracy and efficiency of the motion symbol detection (MSD) method based on the leaky integrate-and-fire (LIF) neuron model and a peak spiking monitoring unit (PSMU). A multikernel learning algorithm based on the tempotron rule, namely, MK-tempotron, is proposed to improve the antinoise performance of the classifier. In MK-tempotron, different kernels are applied to calculate the post-synaptic membrane potentials (PSPs) according to different input spiking patterns, to counteract the impact of noise on the spiking activities, so that the synaptic weights can be changed in the direction of correct firing. We verified the effectiveness of the threshold mechanism and multikernel learning on a variety of data sets. The experimental results show that among the several recent algorithms based on the temporal encoding and spiking neural network classifier, which have advantages in power consumption and network latency, our method achieved the best accuracy on MNIST-DVS (100 ms) and N-CARS, and it also achieved competitive results on N-MNIST, CIFAR10-DVS, Posture-DVS, Poker-DVS, and MNIST-DVS (200 ms/500 ms/2000 ms).},
  archive      = {J_TCDS},
  author       = {Yunhua Chen and Weijie Bai and Qingkun Huang and Jinsheng Xiao},
  doi          = {10.1109/TCDS.2021.3122131},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1544-1552},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Efficient motion symbol detection and multikernel learning for AER object recognition},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid loop closure detection method based on
brain-inspired models. <em>TCDS</em>, <em>14</em>(4), 1532–1543. (<a
href="https://doi.org/10.1109/TCDS.2022.3152910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various hippocampal-entorhinal-based models are used to construct brain-inspired simultaneous localization and mapping (SLAM) systems. Loop closure detection (LCD) is a critical process of SLAM systems for robots to relocalize themselves and correct accumulative errors. The existing LCD methods of brain-inspired SLAM systems cannot solve well with challenging or large-scale environments by hand-crafted features and brute force search strategy. In this article, we propose a hybrid LCD method, which is based on the convolutional neural network (CNN) features and the fly’s locality-sensitive hashing algorithm (Fly-LSH). CNN features can improve the reliability of image matching results, and the Fly-LSH, a nearest neighbor search method derived from the fruit fly olfactory circuit, is used to accelerate the image processing. We use multiple hash tables to make a balance between the image matching time and the accuracy of loop closures. The proposed method provides a general approach for SLAM systems to process visual cues, and has application in a hippocampal-entorhinal-based SLAM system. The experimental results verify that the proposed method enables the system to build cognitive maps with better robustness and efficiency.},
  archive      = {J_TCDS},
  author       = {Jiaxin Li and Huajin Tang and Rui Yan},
  doi          = {10.1109/TCDS.2022.3152910},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1532-1543},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A hybrid loop closure detection method based on brain-inspired models},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel simulation-reality closed-loop learning framework
for autonomous robot skill learning. <em>TCDS</em>, <em>14</em>(4),
1520–1531. (<a href="https://doi.org/10.1109/TCDS.2021.3118294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, data-driven learning methods have been widely studied for autonomous robot skill learning. However, these methods rely on large amounts of robot–environment interaction data for training, which largely prevents them from being applied to real-world robots. To address this problem, this article proposes a novel simulation-reality closed-loop learning framework for autonomous robot skill learning that can improve data efficiency, enhance policy stability, and achieve effective policy simulation-to-reality (sim2real) transfer. First, a hybrid control model combining the asymmetric deep deterministic policy gradients (Asym-DDPGs) model and the forward prediction control (FPC) model is proposed to learn vision-based manipulation policies in simulations, which can decompose complex tasks to improve learning efficiency. Second, a novel pixel-level domain adaptation method named Position-CycleGAN is designed to translate real images to simulated images while also preserving the task-related information. The policy trained in simulations can be directly migrated into real robots in a reverse reality-to-simulation manner using the Position-CycleGAN model. The experimental results validate the effectiveness of the proposed framework. This work provides an efficient and feasible path for achieving autonomous skill learning.},
  archive      = {J_TCDS},
  author       = {Rong Jiang and Bin He and Zhipeng Wang and Yanmin Zhou and Shoulin Xu and Xin Li},
  doi          = {10.1109/TCDS.2021.3118294},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1520-1531},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A novel simulation-reality closed-loop learning framework for autonomous robot skill learning},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EEG-based cognitive state classification and analysis of
brain dynamics using deep ensemble model and graphical brain network.
<em>TCDS</em>, <em>14</em>(4), 1507–1519. (<a
href="https://doi.org/10.1109/TCDS.2021.3116079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the cognitive state of an operator has drawn increasing attention in recent years. Cognitive behavior has been effectively studied by analyzing electroencephalogram (EEG) signals. EEG has an excellent temporal resolution but poor spatial resolution; therefore, it cannot be efficiently used for the cognitive state assessment. To obtain the spatial as well as the temporal resolution of the EEG signal, we propose to combine the filter bank common spatial pattern (FBCSP) method and long short-term memory (LSTM)-based deep ensemble model for classifying the cognitive state of a user during the mental arithmetic experiment. The proposed deep ensemble model produces 87.04% classification accuracy, and it achieves an improvement of 2.04% over the state-of-the-art method. In addition, we propose an algorithm that enables us to identify the brain dynamics for each cognitive state, which is equally important to the estimation of the cognitive state in cognitive neuroscience. For each cognitive state, the proposed information flow algorithm constructs a graphical brain network using functional and effective brain connectivity patterns, which indicates the connection strength as well as the causal effects between two anatomically separated brain regions. The proposed information flow algorithm provides an effective way of communication between different brain regions.},
  archive      = {J_TCDS},
  author       = {Debashis Das Chakladar and Partha Pratim Roy and Masakazu Iwamura},
  doi          = {10.1109/TCDS.2021.3116079},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1507-1519},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {EEG-based cognitive state classification and analysis of brain dynamics using deep ensemble model and graphical brain network},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving synthetic to realistic semantic segmentation with
parallel generative ensembles for autonomous urban driving.
<em>TCDS</em>, <em>14</em>(4), 1496–1506. (<a
href="https://doi.org/10.1109/TCDS.2021.3117925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is paramount for autonomous vehicles to have a deeper understanding of the surrounding traffic environment and enhance safety. Deep neural networks (DNNs) have achieved remarkable performances in semantic segmentation. However, training such a DNN requires a large amount of labeled data at the pixel level. In practice, it is a labor-intensive task to manually annotate dense pixel-level labels. To tackle the problem associated with a small amount of labeled data, deep domain adaptation (DDA) methods have recently been developed to examine the use of synthetic driving scenes so as to significantly reduce the manual annotation cost. Despite remarkable advances, these methods, unfortunately, suffer from the generalizability problem that fails to provide a holistic representation of the mapping from the source image domain to the target image domain. In this article, we, therefore, develop a novel ensembled DDA to train models with different upsampling strategies, discrepancy, and segmentation loss functions. The models are, therefore, complementary with each other to achieve better generalization in the target image domain. Such a design does not only improves the adapted semantic segmentation performance but also strengthens the model reliability and robustness. Extensive experimental results demonstrate the superiorities of our approach over several state-of-the-art methods.},
  archive      = {J_TCDS},
  author       = {Dewei Yi and Hui Fang and Yining Hua and Jinya Su and Mohammed Quddus and Jungong Han},
  doi          = {10.1109/TCDS.2021.3117925},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1496-1506},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Improving synthetic to realistic semantic segmentation with parallel generative ensembles for autonomous urban driving},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A dynamically adaptive approach to reducing strategic
interference for multiagent systems. <em>TCDS</em>, <em>14</em>(4),
1486–1495. (<a href="https://doi.org/10.1109/TCDS.2021.3110959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiagent reinforcement learning (RL) is widely used and can successfully solve many problems in the real world. In the multiagent RL system, a global critic network is used to guide each agent’s strategy to update to learn the most beneficial strategy for the collective. However, the global critic network also makes the current agent’s learning be affected by other agents’ strategies, which leads to unstable learning. To solve this problem, we propose dynamic decomposed multiagent deep deterministic policy gradient (DD-MADDPG): a new network that considers both global and local evaluations and adaptively adjusts the agent’s attention to the two evaluations. Besides, the use of the experience replay buffer by multiagent deep deterministic policy gradient (MADDPG) produces outdated experience, and the outdated strategies of other agents further affect the learning of the current agent. To reduce the influence of other agents’ outdated experience, we propose TD-Error and Time-based experience sampling (T2-PER) based on DD-MADDPG. We evaluate the proposed algorithm’s performance according to the learning stability and the average return obtained by the agents. We have conducted experiments in the MPE environment. The results show that the proposed method has better stability and higher learning efficiency than MADDPG and has a certain generalization ability.},
  archive      = {J_TCDS},
  author       = {Wei Pan and Nanding Wang and Chenxi Xu and Kao-Shing Hwang},
  doi          = {10.1109/TCDS.2021.3110959},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1486-1495},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A dynamically adaptive approach to reducing strategic interference for multiagent systems},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting secondary task performance: A directly actionable
metric for cognitive overload detection. <em>TCDS</em>, <em>14</em>(4),
1474–1485. (<a href="https://doi.org/10.1109/TCDS.2021.3114162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we address cognitive overload detection from unobtrusive physiological signals for users in dual-tasking scenarios. Anticipating cognitive overload is a pivotal challenge in interactive cognitive systems and could lead to safer shared-control between users and assistance systems. Our framework builds on the assumption that decision mistakes on the cognitive secondary task of dual-tasking users correspond to cognitive overload events, wherein the cognitive resources required to perform the task exceed the ones available to the users. We propose DecNet, an end-to-end sequence-to-sequence deep learning model that infers in real time the likelihood of user mistakes on the secondary task, i.e., the practical impact of cognitive overload, from eye-gaze and head-pose data. We train and test DecNet on a data set collected in a simulated driving setup from a cohort of 20 users on two dual-tasking decision-making scenarios, with either visual or auditory decision stimuli. DecNet anticipates cognitive overload events in both scenarios and can perform in time-constrained scenarios, anticipating cognitive overload events up to 2 s before they occur. We show that DecNet’s performance gap between audio and visual scenarios is consistent with user-perceived difficulty. This suggests that single modality stimulation induces higher cognitive load on users, hindering their decision-making abilities.},
  archive      = {J_TCDS},
  author       = {Pierluigi Vito Amadori and Tobias Fischer and Ruohan Wang and Yiannis Demiris},
  doi          = {10.1109/TCDS.2021.3114162},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1474-1485},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Predicting secondary task performance: A directly actionable metric for cognitive overload detection},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Partially recurrent network with highway connections.
<em>TCDS</em>, <em>14</em>(4), 1465–1473. (<a
href="https://doi.org/10.1109/TCDS.2021.3109577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is difficult to train deep recurrent neural networks (RNNs) to learn the complex dependencies in sequential data, because of gradient problems and learning conflicts. To address these learning problems, this article proposes a partially recurrent network (PR-Net), which consists of the partially recurrent (PR) layers with highway connections. The proposed PR layer explicitly arranges the neurons into a memory module and an output module, which are implemented by highway networks. The memory module places memory on the information highway, to maintain long short-term memory (LSTM). The output module places external input on the information highway, to approximate the complex transition function from one step to the next. Because of the highway connections in the output module, the PR layer can pass gradient information across layers easily, to allow the training of the PR-Net with multiple stacked PR layers. Furthermore, the explicit separation of the memory module and output module can reduce the learning burden of RNNs that which neurons are responsible for memory and which are for output. With a comparable number of parameters, sublayers in the recurrent layer, and stacked recurrent layers, the proposed PR-Net outperformed the recurrent highway network and LSTM on three sequence learning benchmarks, significantly.},
  archive      = {J_TCDS},
  author       = {Jianyong Wang and Haixian Zhang and Zhang Yi},
  doi          = {10.1109/TCDS.2021.3109577},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1465-1473},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Partially recurrent network with highway connections},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). First-person hand action recognition using multimodal data.
<em>TCDS</em>, <em>14</em>(4), 1449–1464. (<a
href="https://doi.org/10.1109/TCDS.2021.3108136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extensive studies have been conducted on human action recognition, whereas relatively few methods have been proposed for hand action recognition. Although it is very natural and straightforward to apply a human action recognition method to hand action recognition, this approach cannot always lead to state-of-the-art performance. One of the important reasons is that both the between-class difference and the within-class difference in hand actions are much smaller than those in human actions. In this article, we study first-person hand action recognition from RGB-D sequences. To explore whether pretrained networks substantially influence accuracy, eight classic pretrained networks and one pretrained network designed by us are introduced for extracting RGB-D features. A Lie group is introduced for hand pose representation. Ablation studies are conducted to compare the discriminative power of the RGB modality, depth modality, pose modality, and their combinations. In our method, a fixed number of frames are randomly sampled to represent an action. This temporal modeling strategy is simple but is proven more effective than both the graph convolutional network (GCN) and the recurrent neural network (RNN), which are widely adopted by conventional methods. Evaluation experiments on two public data sets demonstrate that our method markedly outperforms recent baselines.},
  archive      = {J_TCDS},
  author       = {Rui Li and Hongyu Wang and Zhenyu Liu and Na Cheng and Hongye Xie},
  doi          = {10.1109/TCDS.2021.3108136},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1449-1464},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {First-person hand action recognition using multimodal data},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Minimum adversarial distribution discrepancy for domain
adaptation. <em>TCDS</em>, <em>14</em>(4), 1440–1448. (<a
href="https://doi.org/10.1109/TCDS.2021.3104231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation (DA) refers to generalize a learning technique across the source domain and target domain under different distributions. Therefore, the essential problem in DA is how to reduce the distribution discrepancy between the source and target domains. Typical methods are to embed the adversarial learning technique into deep networks to learn transferable feature representations. However, existing adversarial related DA methods may not sufficiently minimize the distribution discrepancy. In this article, a DA method minimum adversarial distribution discrepancy (MADD) is proposed by combining feature distribution with adversarial learning. Specifically, we design a novel divergence metric loss, named maximum mean discrepancy based on conditional entropy (MMD-CE), and embed it in the adversarial DA network. The proposed MMD-CE loss can address two problems: 1) the misalignment from different class distributions between domains and 2) the equilibrium challenge issue in adversarial DA. Comparative experiments on Office-31, ImageCLEF-DA, and Office-Home data sets with state-of-the-art methods show that our method has some advantageous performances.},
  archive      = {J_TCDS},
  author       = {Xiaohan Huang and Xuesong Wang and Qiang Yu and Yuhu Cheng},
  doi          = {10.1109/TCDS.2021.3104231},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1440-1448},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Minimum adversarial distribution discrepancy for domain adaptation},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cognitive hexagon-controlled intelligent speech interaction
system. <em>TCDS</em>, <em>14</em>(4), 1413–1439. (<a
href="https://doi.org/10.1109/TCDS.2022.3168807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several intelligent speech interaction (ISI) systems have emerged over the past four decades that have served the human community. The research papers show that these systems are very well connected to the cognitive hexagon and the six hybrid approaches. Where this hexagon reveals six distinct cognitive areas, one of the six hybrid perspectives gives rise to the dimensions of speech quality. This survey has been undertaken to reveal the dimensions of speech quality and to discuss the role of cognitive hexagonal regions on these dimensions with hybrid approaches. Here, ISI systems support this discussion and follow them as cognitive machines. An overview of the state of the art related to ISI systems is also described here. Techniques, such as processing [natural language (NL)], speech synthesis [speech-to-text (STT) or text-to-speech (TTS)], computing (voice/mobile), and audio mining are presented in this overview. These are contributing well with technologies, such as the Internet of Things (IoT), Voice over Internet Protocol (VoIP), and cloud-based systems (CBSs). In addition, stochastic components, such as reliability, availability, and failure rate were discussed to analyze whether the Quality of Service (QoS) of these ISI systems is described. Additionally, after the discussion, some aspects of the applications are also discussed along with the essential advantages and significant drawbacks.},
  archive      = {J_TCDS},
  author       = {Himanshu Chaurasiya},
  doi          = {10.1109/TCDS.2022.3168807},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1413-1439},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Cognitive hexagon-controlled intelligent speech interaction system},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The challenges and opportunities of human-centered AI for
trustworthy robots and autonomous systems. <em>TCDS</em>,
<em>14</em>(4), 1398–1412. (<a
href="https://doi.org/10.1109/TCDS.2021.3132282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The trustworthiness of robots and autonomous systems (RASs) has taken a prominent position on the way toward full autonomy. This work is the first to systematically explore the key facets of human-centered AI (HAI) for trustworthy RAS (TRAS). We identified five key properties of a TRAS, i.e., RAS must be: 1) safe in any uncertain and dynamic environment; 2) secure, i.e., protect itself from cyber threats; 3) healthy and fault tolerant; 4) trusted and easy to use to enable effective human–machine interaction (HMI); and 5) compliant with the law and ethical expectations. While the applications of RAS have mainly focused on performance and productivity, not enough scientific attention has been paid to the risks posed by advanced artificial intelligence (AI) in RAS. We analytically examine the challenges of implementing TRAS with respect to the five key properties and explore the role and roadmap of AI technologies in ensuring the trustworthiness of RAS in respect of safety, security, health, HMI, and ethics. A new acceptance model of RAS is provided as a framework for HAI requirements and for implementing TRAS by design. This approach promotes human-level intelligence to augment human capabilities and focuses on contribution to humanity.},
  archive      = {J_TCDS},
  author       = {Hongmei He and John Gray and Angelo Cangelosi and Qinggang Meng and T. Martin McGinnity and Jörn Mehnen},
  doi          = {10.1109/TCDS.2021.3132282},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1398-1412},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {The challenges and opportunities of human-centered AI for trustworthy robots and autonomous systems},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conversational affective social robots for ageing and
dementia support. <em>TCDS</em>, <em>14</em>(4), 1378–1397. (<a
href="https://doi.org/10.1109/TCDS.2021.3115228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Socially assistive robots (SAR) hold significant potential to assist older adults and people with dementia in human engagement and clinical contexts by supporting mental health and independence at home. While SAR research has recently experienced prolific growth, long-term trust, clinical translation, and patient benefit remain immature. Affective human–robot interactions are unresolved and the deployment of robots with conversational abilities is fundamental for robustness and human–robot engagement. In this article, we review the state of the art within the past two decades, design trends, and current applications of conversational affective SAR for ageing and dementia support. A horizon scanning of AI voice technology for healthcare, including ubiquitous smart speakers, is further introduced to address current gaps inhibiting home use. We discuss the role of user-centered approaches in the design of voice systems, including the capacity to handle communication breakdowns for effective use by target populations. We summarize the state of development in interactions using speech and natural language processing, which forms a baseline for longitudinal health monitoring and cognitive assessment. Drawing from this foundation, we identify open challenges and propose future directions to advance conversational affective social robots for: 1) user engagement; 2) deployment in real-world settings; and 3) clinical translation.},
  archive      = {J_TCDS},
  author       = {Maria R. Lima and Maitreyee Wairagkar and Manish Gupta and Ferdinando Rodriguez y Baena and Payam Barnaghi and David J. Sharp and Ravi Vaidyanathan},
  doi          = {10.1109/TCDS.2021.3115228},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1378-1397},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Conversational affective social robots for ageing and dementia support},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient online interest-driven exploration for
developmental robots. <em>TCDS</em>, <em>14</em>(4), 1367–1377. (<a
href="https://doi.org/10.1109/TCDS.2020.3001633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A major challenge for online and data-driven model learning in robotics is the high sample complexity. This hinders its efficiency and practical feasibility for lifelong learning, in particular, for developmental robots that autonomously bootstrap their sensorimotor skills in an open-ended environment. In this work, we propose new methods to mediate this problem in order to permit the learning of robot models online, from scratch, and in learning while behaving fashion. Exploration is utilized and autonomously driven by a novel intrinsic motivation signal which combines knowledge-based and competence-based elements and surpasses other state-of-the-art methods. In addition, we propose an episodic online mental replay to accelerate online learning, to ensure sample efficiency, and to update the model online rapidly. The efficiency as well as the applicability of our methods are demonstrated with a physical 7-DoF Baxter manipulator. We show that our learning schemes are able to drastically reduce the sample complexity and learn the data-driven model online, even within a limited time frame.},
  archive      = {J_TCDS},
  author       = {Rania Rayyes and Heiko Donat and Jochen Steil},
  doi          = {10.1109/TCDS.2020.3001633},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1367-1377},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Efficient online interest-driven exploration for developmental robots},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robot depth estimation inspired by fixational movements.
<em>TCDS</em>, <em>14</em>(4), 1356–1366. (<a
href="https://doi.org/10.1109/TCDS.2020.3025057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance estimation is a challenge for robots, human beings, and other animals in their adaptation to changing environments. Different approaches have been proposed to tackle this problem based on classical vision algorithms or, more recently, deep learning. We present a novel approach inspired by mechanisms involved in fixational movements to estimate a depth image with a monocular camera. An algorithm based on microsaccades and head movements during visual fixation is presented. It combines the images generated by these micro-movements with the ego-motion signal, to compute the depth map. Systematic experiments using the Baxter robot in the Gazebo/ROS simulator are described to test the approach in two different scenarios and evaluate the influence of its parameters and its robustness in the presence of noise.},
  archive      = {J_TCDS},
  author       = {Angel J. Duran and Angel P. del Pobil},
  doi          = {10.1109/TCDS.2020.3025057},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1356-1366},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Robot depth estimation inspired by fixational movements},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Behavioral repertoire via generative adversarial policy
networks. <em>TCDS</em>, <em>14</em>(4), 1344–1355. (<a
href="https://doi.org/10.1109/TCDS.2020.3008574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning algorithms are enabling robots to solve increasingly challenging real-world tasks. These approaches often rely on demonstrations and reproduce the behavior shown. Unexpected changes in the environment or in robot morphology may require using different behaviors to achieve the same effect, for instance, to reach and grasp an object in changing clutter. An emerging paradigm addressing this robustness issue is to learn a diverse set of successful behaviors for a given task, from which a robot can select the most suitable policy when faced with a new environment. In this article, we explore a novel realization of this vision by learning a generative model over policies. Rather than learning a single policy, or a small fixed repertoire, our generative model for policies compactly encodes an unbounded number of policies and allows novel controller variants to be sampled. Leveraging our generative policy network, a robot can sample novel behaviors until it finds one that works for a new scenario. We demonstrate this idea with an application of robust ball throwing in the presence of obstacles, as well as joint-damage-robust throwing. We show that this approach achieves a greater diversity of behaviors than an existing evolutionary approach, while maintaining good efficacy of sampled behaviors, allowing a Baxter robot to hit targets more often when ball throwing in the presence of varying obstacles or joint impediments.},
  archive      = {J_TCDS},
  author       = {Marija Jegorova and Stéphane Doncieux and Timothy M. Hospedales},
  doi          = {10.1109/TCDS.2020.3008574},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1344-1355},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Behavioral repertoire via generative adversarial policy networks},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhancing a neurocognitive shared visuomotor model for
object identification, localization, and grasping with learning from
auxiliary tasks. <em>TCDS</em>, <em>14</em>(4), 1331–1343. (<a
href="https://doi.org/10.1109/TCDS.2020.3028460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a follow-up study on our unified visuomotor neural model for the robotic tasks of identifying, localizing, and grasping a target object in a scene with multiple objects. Our Retinanet-based model enables end-to-end training of visuomotor abilities in a biologically inspired developmental approach. In our initial implementation, a neural model was able to grasp selected objects from a planar surface. We embodied the model on the NICO humanoid robot. In this follow-up study, we expand the task and the model to reaching for objects in a 3-D space with a novel data set based on augmented reality and a simulation environment. We evaluate the influence of training with auxiliary tasks, i.e., if learning of the primary visuomotor task is supported by learning to classify and locate different objects. We show that the proposed visuomotor model can learn to reach for objects in a 3-D space. We analyze the results for biologically plausible biases based on object locations or properties. We show that the primary visuomotor task can be successfully trained simultaneously with one of the two auxiliary tasks. This is enabled by a complex neurocognitive model with shared and task-specific components, similar to models found in biological systems.},
  archive      = {J_TCDS},
  author       = {Matthias Kerzel and Fares Abawi and Manfred Eppe and Stefan Wermter},
  doi          = {10.1109/TCDS.2020.3028460},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1331-1343},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Enhancing a neurocognitive shared visuomotor model for object identification, localization, and grasping with learning from auxiliary tasks},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving object detection performance using scene
contextual constraints. <em>TCDS</em>, <em>14</em>(4), 1320–1330. (<a
href="https://doi.org/10.1109/TCDS.2020.3008213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contextual information, such as the co-occurrence of objects and the spatial and relative size among objects, provides rich and complex information about digital scenes. It also plays an important role in improving object detection and determining out-of-context objects. In this work, we present contextual models that leverage contextual information (16 contextual relationships are applied in this article) to enhance the performance of two of the state-of-the-art object detectors (i.e., faster RCNN and you look only once), which are applied as a postprocessing process for most of the existing detectors, especially for refining the confidences and associated categorical labels, without refining bounding boxes. We experimentally demonstrate that our models lead to enhancement in detection performance using the most common data set used in this field (MSCOCO), where in some experiments, PASCAL2012 is also used. We also show that iterating the process of applying our contextual models also enhances the detection performance further.},
  archive      = {J_TCDS},
  author       = {Faisal Alamri and Nicolas Pugeault},
  doi          = {10.1109/TCDS.2020.3008213},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1320-1330},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Improving object detection performance using scene contextual constraints},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A predictive coding account for cognition in human children
and chimpanzees: A case study of drawing. <em>TCDS</em>, <em>14</em>(4),
1306–1319. (<a href="https://doi.org/10.1109/TCDS.2020.3006497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans and chimpanzees differ in their cognitive abilities, in particular, in social-cognitive processing; however, the underlying neural mechanisms are still unknown. Based on the theory of predictive coding, we hypothesize that crucial differences in cognitive processing might arise from aberrant reliance on predictions. We test this hypothesis using a recurrent neural network that integrates sensory information with predictions based on the rules of Bayesian inference. Altering a network parameter, we vary how strongly the network relies on its predictions during development. Our model qualitatively replicates findings from a behavioral study on the drawing ability of human children and chimpanzees. Moderate parameter values replicate the ability of human children to complete drawings by adding missing elements. With weak reliance on predictions, the model’s behavior is similar to chimpanzees’ behaviors: trained networks can follow existing lines but fail to complete drawings. Furthermore, with a strong reliance on predictions, networks learn more abstract representations of drawings and confuse different trained patterns. An analysis of the internal network representations reveals that an aberrant reliance on predictions affects the formation of attractors in the network. Thus, appropriate reliance on their own predictions in humans may be crucial for developing abstract representations and acquiring cognitive skills.},
  archive      = {J_TCDS},
  author       = {Anja Philippsen and Yukie Nagai},
  doi          = {10.1109/TCDS.2020.3006497},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1306-1319},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A predictive coding account for cognition in human children and chimpanzees: A case study of drawing},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guest editorial special issue on development and learning
and on epigenetic robotics. <em>TCDS</em>, <em>14</em>(4), 1302–1305.
(<a href="https://doi.org/10.1109/TCDS.2022.3210665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IEEE ICDL (the Joint IEEE International Conference on Development and Learning—formerly named ICDL-EpiRob) is a unique conference gathering researchers from computer science, robotics, psychology, and developmental studies to share knowledge and research on how humans and animals develop sensing, reasoning, and actions. This includes taking advantage of interaction with social and physical environments and how cognitive and developmental capabilities can be transferred to computing systems and robotics. This approach goes hand in hand with the goals of both understanding human and animal development and applying this knowledge to improve future intelligent technology, including robots that will be in close interaction with humans.},
  archive      = {J_TCDS},
  author       = {Kai Olav Ellefsen and Kerstin Dautenhahn and Jim Torresen},
  doi          = {10.1109/TCDS.2022.3210665},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1302-1305},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Guest editorial special issue on development and learning and on epigenetic robotics},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Construction of diverse DropBlock branches for person
reidentification. <em>TCDS</em>, <em>14</em>(3), 1296–1300. (<a
href="https://doi.org/10.1109/TCDS.2021.3096546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose to use the data augmentation of batch drop-block with varying dropping ratios for constructing diversity-achieving branches in person reidentification (Re-ID). Since a considerable portion of input images may be dropped, this reinforces feature learning of the un-dropped region but makes the training process hard to converge. Hence, we propose a novel double-batch-split co-training approach for remedying this problem. In particular, we show that the feature diversity can be well achieved with the use of multiple dropping branches by setting individual dropping ratio for each branch. Empirical evidence demonstrates that the proposed method performs competitively on popular person Re-ID data sets, including Market-1501, DukeMTMC-reID, and CUHK03, and the use of more dropping branches can further boost the performance. Source code is available at https://github.com/AI-NERC-NUPT/DDB .},
  archive      = {J_TCDS},
  author       = {Xiaofu Wu and Ben Xie and Yuxin Zhang and Shiliang Zhao and Suofei Zhang},
  doi          = {10.1109/TCDS.2021.3096546},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1296-1300},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Construction of diverse DropBlock branches for person reidentification},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning curve of a short-time neurofeedback training:
Reflection of brain network dynamics based on phase-locking value.
<em>TCDS</em>, <em>14</em>(3), 1282–1295. (<a
href="https://doi.org/10.1109/TCDS.2021.3125948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurofeedback (NF) training is a type of online biofeedback in which neural activity is measured and provided to the participant in real time to facilitate the top-down control of specific activation patterns. To improve the training efficiency, an investigation on the learning of EEG regulation and effect on neural activity during NF is critical. This article attempts to analyze the learning curve and the dynamics of the phase-locking value (PLV)-based brain network for a short-time EEG-based NF, in which 28 participants carried out alpha downregulating NF training in two consecutive days. The results reveal that participants could successfully construct the related learning network to achieve the training goals in the first day training and the beginning of the second day training. Moreover, the learning plateaus were discovered from the results of the relative amplitude and the functional brain network in the middle of the second day training. These findings could be helpful for better understanding of the learning process in NF from the functional connectivity viewpoint and would contribute to building a more efficient learning protocol for NF training.},
  archive      = {J_TCDS},
  author       = {Ze Wang and Chi Man Wong and Wenya Nan and Qi Tang and Agostinho C. Rosa and Peng Xu and Feng Wan},
  doi          = {10.1109/TCDS.2021.3125948},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1282-1295},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Learning curve of a short-time neurofeedback training: Reflection of brain network dynamics based on phase-locking value},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep-learning-based signal enhancement of low-resolution
accelerometer for fall detection systems. <em>TCDS</em>, <em>14</em>(3),
1270–1281. (<a href="https://doi.org/10.1109/TCDS.2021.3116228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last two decades, fall detection (FD) systems have been developed as a popular assistive technology. To support long-term FD services, various power-saving strategies have been implemented. Among them, a reduced sampling rate is a common approach for an energy-efficient system in the real world. However, the performance of FD systems is diminished owing to low-resolution (LR) accelerometer signals. To improve the detection accuracy with LR accelerometer signals, several technical challenges must be considered, including mismatch of effective features and the degradation effects. In this work, a deep-learning-based accelerometer signal enhancement (ASE) model is proposed as a front-end processor to help typical LR-FD systems achieve better detection performance. The proposed ASE model based on a deep denoising convolutional autoencoder architecture reconstructs high-resolution (HR) signals from the LR signals by learning the relationship between the LR and HR signals. The results show that the FD system using support vector machine (SVM) and the proposed ASE model at an extremely low sampling rate (sampling rate &lt; 2 Hz) achieved 97.34% and 90.52% accuracies in the SisFall and FallAllD data sets, respectively, while those without ASE models only achieved 95.92% and 87.47% accuracies in the SisFall and FallAllD data sets, respectively. The results also demonstrate that the proposed ASE mode can be suitably combined with deep-learning-based FD systems.},
  archive      = {J_TCDS},
  author       = {Kai-Chun Liu and Kuo-Hsuan Hung and Chia-Yeh Hsieh and Hsiang-Yun Huang and Chia-Tai Chan and Yu Tsao},
  doi          = {10.1109/TCDS.2021.3116228},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1270-1281},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep-learning-based signal enhancement of low-resolution accelerometer for fall detection systems},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive graph convolutional network with adversarial
learning for skeleton-based action prediction. <em>TCDS</em>,
<em>14</em>(3), 1258–1269. (<a
href="https://doi.org/10.1109/TCDS.2021.3103960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of action prediction is to recognize an action before it is completed to reduce recognition latency. Because action prediction has lower latency than action recognition, it can be applied to a variety of surveillance scenarios and responds faster. However, action prediction is more difficult because it cannot obtain the complete action execution. In this article, we study the action prediction which is based on skeleton data and propose a new network called adaptive graph convolutional network with adversarial learning (AGCN-AL) for it. The AGCN-AL uses adversarial learning to make the features of the partial sequences as similar as possible to the features of the full sequences to learn the potential global information in the partial sequences. Besides, partial sequences with different numbers of frames contain different amounts of information. We introduce temporal-dependent loss functions to prevent the network from paying too much attention to partial sequences whose observation ratios are small, and ignoring partial sequences whose observation ratios are large. Moreover, the AGCN-AL is combined with the local AGCN into a two-stream network to enhance the prediction, proving that the local information and the potential global information in partial sequences are complementary. We evaluate the proposed approach on two data sets and show excellent performance.},
  archive      = {J_TCDS},
  author       = {Guangxin Li and Nanjun Li and Faliang Chang and Chunsheng Liu},
  doi          = {10.1109/TCDS.2021.3103960},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1258-1269},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Adaptive graph convolutional network with adversarial learning for skeleton-based action prediction},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Buddy system: An adaptive mental state support system based
on active inference and free-energy principles. <em>TCDS</em>,
<em>14</em>(3), 1245–1257. (<a
href="https://doi.org/10.1109/TCDS.2021.3102993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To support a healthy human mental state, controlling the environment is one of the best-known solutions. However, it is difficult to design an environmental control system because of the uniqueness of individual preferences and mental state fluctuations. Here, we propose “Buddy system” as an adaptive mental state support solution that controls devices in the environment, depending on the recognized user’s mental state at the time and how it could be improved, serving a role similar to a “buddy” to individuals. The recognition of mental states and the locus of actions to control one’s surrounding environment are implemented on the basis of a brain-derived theory of computation known as active inference and free-energy principles, which provide biologically plausible computations for perceptions and behavior in a changing world. For the generation of actions, we modify the general calculations of active inference to adjust to individual environmental preferences. In the experiments, the Buddy system sought to maintain the participants’ concentration while a calculation task was conducted. As a result, the task performance for most of the participants was improved through the aid of the Buddy system. The results indicated that the Buddy system can adaptively support to improve the mental states of individual users.},
  archive      = {J_TCDS},
  author       = {Motoko Iwashita and Makiko Ishikawa},
  doi          = {10.1109/TCDS.2021.3102993},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1245-1257},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Buddy system: An adaptive mental state support system based on active inference and free-energy principles},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A trend on autism spectrum disorder research: Eye
tracking-EEG correlative analytics. <em>TCDS</em>, <em>14</em>(3),
1232–1244. (<a href="https://doi.org/10.1109/TCDS.2021.3102646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The greatest challenge in autism spectrum disorder (ASD) detection and treatment arises from the elusive etiological origins and heterogeneity of ASD. ET-EEG correlative analytics refers to correlating simultaneously recorded eye tracking (ET) and electroencephalography (EEG) data or combining them to yield diagnostic biomarkers. This approach was recently applied in ASD research as EEG and video-based ET are suitable for children and do not interfere with each other. It allows researchers to associate neural correlates with gaze patterns in the same cognitive task. Besides, correlative analytics has shed light on the inconsistent findings derived from the unimodal approach, reveals the developmental trajectory of ASD, and provides biomarkers to assist ASD diagnosis or intervention. This review focuses on eight articles applying ET-EEG correlative analytics and synthesized the reported correlation between ET and EEG patterns. Studies concerning such patterns that acquired either ET or EEG data were also reviewed to provide comparisons. In recent years, correlative analytics has also been developed in other research fields, such as visual information processing and EEG artifact correction. Incorporating methodologies used in these research works into ASD studies would help identify cognitive alteration related to a specific visual pattern in naturalistic settings. The opportunities and limitations of this approach in ASD research are discussed at last.},
  archive      = {J_TCDS},
  author       = {Gansheng Tan and Kai Xu and Jinbiao Liu and Honghai Liu},
  doi          = {10.1109/TCDS.2021.3102646},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1232-1244},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A trend on autism spectrum disorder research: Eye tracking-EEG correlative analytics},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hemodynamic analysis for olfactory perceptual degradation
assessment using generalized type-2 fuzzy regression. <em>TCDS</em>,
<em>14</em>(3), 1217–1231. (<a
href="https://doi.org/10.1109/TCDS.2021.3101897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Olfactory perceptual degradation (OPD) refers to the inability of people to recognize the variation in concentration levels of olfactory stimuli. This article attempts to assess the degree of OPD of subjects from their hemodynamic response to olfactory stimuli. This is done in two phases. In the first (training) phase, a regression model is developed to assess the degree of concentration levels of an olfactory stimulus by a subject from her hemodynamic response to the stimulus. In the second (test) phase, the model is employed to predict the possible concentration level experienced by the subject in [0, 100] scale. The difference between the model-predicted response and the oral response (the center value of the qualitative grades) of the subject about her perceived concentration level is regarded as the quantitative measure of the degree of subject’s olfactory degradation. The novelty of the present research lies in the design of a general type-2 fuzzy regression model, which is capable of handling uncertainty due to the presence of intrasession and intersession variations in the brain responses to olfactory stimuli. The attractive feature of the article lies in adaptive tuning of secondary membership functions to reduce model prediction error in an evolutionary optimization setting. The effect of such adaptation in secondary measures is utilized to adjust the corresponding primary memberships in order to reduce the uncertainty involved in the regression process. The proposed regression model has good prediction accuracy and high time efficiency as evident from average percentage success rate (PSR) and runtime complexity analysis, respectively. The Friedman test undertaken also confirms the superior performance of the proposed technique with other competitive techniques at 95% confidence level.},
  archive      = {J_TCDS},
  author       = {Mousumi Laha and Amit Konar and Pratyusha Rakshit and Atulya K. Nagar},
  doi          = {10.1109/TCDS.2021.3101897},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1217-1231},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Hemodynamic analysis for olfactory perceptual degradation assessment using generalized type-2 fuzzy regression},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning robust features from nonstationary brain signals by
multiscale domain adaptation networks for seizure prediction.
<em>TCDS</em>, <em>14</em>(3), 1208–1216. (<a
href="https://doi.org/10.1109/TCDS.2021.3100270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seizure prediction from intracranial electroencephalogram (iEEG) has great potentials to improve the life quality of epileptic patients, but faces big challenges. One major difficulty lies in which the brain signal changes occasionally during long-term monitoring, due to electrode movements or the nonstationary brain dynamics. This leads to a serious situation that a predictor learned from historical data usually only works well in a short time period as long as the data do not change much. While in a long time span, the performance of the learned features decreases or even becomes totally invalid. To deal with the problem, we propose a domain adaptation convolutional neural network to learn robust preictal features that is invariant across different time periods. Specifically, the preictal feature is learned and enhanced by multiscale temporal convolutions in the neural network. Based on this, a domain adaptation method is adopted to constrain that the learned features should be invariant across different time periods. Experimental results demonstrate that our approach can effectively improve seizure prediction performance against signal changes.},
  archive      = {J_TCDS},
  author       = {Yu Qi and Ling Ding and Yueming Wang and Gang Pan},
  doi          = {10.1109/TCDS.2021.3100270},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1208-1216},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Learning robust features from nonstationary brain signals by multiscale domain adaptation networks for seizure prediction},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical graph pooling with self-adaptive cluster
aggregation. <em>TCDS</em>, <em>14</em>(3), 1198–1207. (<a
href="https://doi.org/10.1109/TCDS.2021.3100883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural network (GNN) introduces deep neural networks into graph structure data. It has achieved advanced performance in many fields, such as traffic prediction, recommendation systems, and computer vision, which has received extensive attention from the academic community. Most of the existing research on GNNs focuses on graph convolution, while graph pooling is usually ignored. Although there are also some graph pooling methods, most of the current pooling methods are based on top-k node selection. In the top-k-based pooling method, unselected nodes will be directly discarded, which will cause the loss of feature information during the pooling process. In this article, we propose a novel graph pooling operator, called hierarchical graph pooling with self-adaptive cluster aggregation (HGP-SACA), which uses a sparse and differentiable method to capture the graph structure. Before using top-k for cluster selection, the unselected clusters and the selected clusters in the neighbor perform an n-hop feature information aggregation. The merged clusters which contain neighborhood clusters are used for top-k selection, which can enhance the function of the unselected clusters. Through extensive theoretical analysis and experimental verification on multiple data sets, our experimental results show that combining the existing GNN architecture with HGP-SACA can achieve state-of-the-art results on multiple graph classification benchmarks, which proves the effectiveness of our proposed model.},
  archive      = {J_TCDS},
  author       = {Zhi-Peng Li and Hai-Long Su and Xiao-Bo Zhu and Xiu-Mei Wei and Xue-Song Jiang and Valeriya Gribova and Vladimir Fedorovich Filaretov and De-Shuang Huang},
  doi          = {10.1109/TCDS.2021.3100883},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1198-1207},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Hierarchical graph pooling with self-adaptive cluster aggregation},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Logistic regression with tangent space-based cross-subject
learning for enhancing motor imagery classification. <em>TCDS</em>,
<em>14</em>(3), 1188–1197. (<a
href="https://doi.org/10.1109/TCDS.2021.3099988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-computer interface (BCI) performance is often impacted due to the inherent nonstationarity in the recorded EEG signals coupled with a high variability across subjects. This study proposes a novel method using logistic regression with tangent space-based transfer learning (LR-TSTL) for motor imagery (MI)-based BCI classification problems. The single-trial covariance matrix (CM) features computed from the EEG signals are transformed into a Riemannian geometry frame and tangent space features are computed by considering the lower triangular matrix. These are then further classified using the logistic regression model to improve classification accuracy. The performance of LR-TSTL is tested on healthy subjects’ data set as well as on stroke patients’ data set. As compared to existing within-subject learning approaches the proposed method gave an equivalent or better performance in terms of average classification accuracy (78.95 ± 11.68%), while applied as leave-one-out cross-subject learning for healthy subjects. Interestingly, for the patient data set LR-TSTL significantly ( $p&amp;lt; 0.05$ ) outperformed the current benchmark performance by achieving an average classification accuracy of 81.75 ± 6.88%. The results show that the proposed method for cross-subject learning has the potential to realize the next generation of calibration-free BCI technologies with enhanced practical usability especially in the case of neurorehabilitative BCI designs for stroke patients.},
  archive      = {J_TCDS},
  author       = {Pramod Gaur and Anirban Chowdhury and Karl McCreadie and Ram Bilas Pachori and Hui Wang},
  doi          = {10.1109/TCDS.2021.3099988},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1188-1197},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Logistic regression with tangent space-based cross-subject learning for enhancing motor imagery classification},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Application of dispersion entropy for the detection of
emotions with electroencephalographic signals. <em>TCDS</em>,
<em>14</em>(3), 1179–1187. (<a
href="https://doi.org/10.1109/TCDS.2021.3099344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interest in the application of nonlinear methodologies for the analysis of electroencephalography signals for automatic recognition of emotions has increased notably. A vast number of studies in the research field of emotions detection have focused on the identification of the four quadrants of the valence-arousal emotional model. In this work, the recently introduced dispersion entropy (DispEn) has been applied for the first time to discern between the four groups of emotions corresponding to the four quadrants. This entropy-based index has demonstrated a considerable performance when dealing with this problem. Concretely, frontal and parieto-occipital brain regions reported the most relevant results when identifying the four emotional groups. Furthermore, the implementation of a classification model based on a sequential forward selection (SFS) approach and a support vector machine (SVM) classifier provided an average accuracy of 89.54%. This result is comparable to similar works in the scientific literature, with the advantage that the controlled selection of a reduced number of input features by means of the SFS scheme allowed to give a clinical interpretation of the outcomes. Therefore, it has been possible to reveal new insights about the performance of the most relevant brain regions involved in the processing of emotional information.},
  archive      = {J_TCDS},
  author       = {Beatriz García-Martínez and Antonio Fernández-Caballero and Raúl Alcaraz and Arturo Martínez-Rodrigo},
  doi          = {10.1109/TCDS.2021.3099344},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1179-1187},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Application of dispersion entropy for the detection of emotions with electroencephalographic signals},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhancing integrity modeling for emotional conversation
generation. <em>TCDS</em>, <em>14</em>(3), 1170–1178. (<a
href="https://doi.org/10.1109/TCDS.2021.3098444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Establishing an open-domain emotional conversation generation with content and emotional dependence is a key step toward intelligent interactions. However, the researchers only dedicated to adding the thematic content or emotional words and handling both factors are not yet properly solved. In this article, we aim to address the issue of integrity in emotional content which indicates the response has a dependency relationship between theme and emotion. We propose a thematic–emotional conversation model, which consists of parallel channels and joint encoding, to maximize integrity in conversation generation. We first use the encoding of the parallel channel to strengthen the decoding weight of the theme and emotion. The joint encoding of the dual attention mechanism and the thematic emotional tradeoff strategy are involved in the encoding to ensure that the theme complements the emotional content during the response generation. After that, the bidirectional training interactions between post and response are calculated to derive the new response inference vectors. Experiment results suggest that the proposed model is effective not only in content integrity but also in emotion.},
  archive      = {J_TCDS},
  author       = {Rui Yang and Zhiqiang Ma and Chunyu Wang and Baoxiang Du},
  doi          = {10.1109/TCDS.2021.3098444},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1170-1178},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Enhancing integrity modeling for emotional conversation generation},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A developmental evolutionary learning framework for robotic
chinese stroke writing. <em>TCDS</em>, <em>14</em>(3), 1155–1169. (<a
href="https://doi.org/10.1109/TCDS.2021.3098229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability of robots to write Chinese strokes, which is recognized as a sophisticated task, involves complicated kinematic control algorithms. The conventional approaches for robotic writing of Chinese strokes often suffer from limited font generation methods, which limits the ability of robots to perform high-quality writing. This article instead proposes a developmental evolutionary learning framework that enables a robot to learn to write fundamental Chinese strokes. The framework first considers the learning process of robotic writing as an evolutionary easy-to-difficult procedure. Then, a developmental learning mechanism called “Lift-constraint, act and saturate” that stems from developmental robotics is used to determine how the robot learns tasks ranging from simple to difficult by building on the learning results from the easy tasks. The developmental constraints, which include altitude adjustments, number of mutation points, and stroke trajectory points, determine the learning complexity of robot writing. The developmental algorithm divides the evolutionary procedure into three developmental learning stages. In each stage, the stroke trajectory points gradually increase, while the number of mutation points and adjustment altitudes gradually decrease, allowing the learning difficulties involved in these three stages to be categorized as easy, medium, and difficult. Our robot starts with an easy learning task and then gradually progresses to the medium and difficult tasks. Under various developmental constraint setups in each stage, the robot applies an evolutionary algorithm to handle the basic shapes of the Chinese strokes and eventually acquires the ability to write with good quality. The experimental results demonstrate that the proposed framework allows a calligraphic robot to gradually learn to write five fundamental Chinese strokes and also reveals a developmental pattern similar to that of humans. Compared to an evolutionary algorithm without the developmental mechanism, the proposed framework achieves good writing quality more rapidly.},
  archive      = {J_TCDS},
  author       = {Ruiqi Wu and Fei Chao and Changle Zhou and Yuxuan Huang and Longzhi Yang and Chih-Min Lin and Xiang Chang and Qiang Shen and Changjing Shang},
  doi          = {10.1109/TCDS.2021.3098229},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1155-1169},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A developmental evolutionary learning framework for robotic chinese stroke writing},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relation-aware facial expression recognition. <em>TCDS</em>,
<em>14</em>(3), 1143–1154. (<a
href="https://doi.org/10.1109/TCDS.2021.3100131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on facial expression recognition has been moving from the constrained lab scenarios to the in-the-wild situations and has made progress in recent years. However, it is still very challenging to deal with facial expression in the wild due to large poses and occlusion as well as illumination and intensity variations. Generally, existing methods mainly take the whole face as a uniform source of features for facial expression analysis. Actually, physiology and psychology research shows that some crucial regions, such as the eye and mouth, reflect the differences of different facial expressions, which have close relationships with emotion expression. Inspired by this observation, a novel relation-aware facial expression recognition method called relation convolutional neural network (ReCNN) is proposed in this article, which can adaptively capture the relationship between crucial regions and facial expressions leading to the focus on the most discriminative regions for recognition. We have evaluated the proposed ReCNN on two large in-the-wild databases: 1) AffectNet and 2) RAF-DB. Extensive experiments on these databases show that our method has superior recognition accuracy compared with state-of-the-art methods and the relationship between crucial regions and facial expressions is beneficial to improve the performance of facial expression recognition.},
  archive      = {J_TCDS},
  author       = {Yifan Xia and Hui Yu and Xiao Wang and Muwei Jian and Fei-Yue Wang},
  doi          = {10.1109/TCDS.2021.3100131},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1143-1154},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Relation-aware facial expression recognition},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Action command encoding for surrogate-assisted neural
architecture search. <em>TCDS</em>, <em>14</em>(3), 1129–1142. (<a
href="https://doi.org/10.1109/TCDS.2021.3107555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of neural architecture search, the performance of deep neural networks has been considerably enhanced with less human expertise. While the existing work mainly focuses on the development of optimizers, the design of encoding scheme is still in its infancy. This article thus proposes a novel encoding scheme for neural architecture search, termed action command encoding (ACEncoding). Inspired by the gene expression process, ACEncoding defines several action commands to indicate the addition and clone of layers, connections, and local modules, where an architecture grows from empty according to multiple action commands. ACEncoding provides a compact and rich search space that can be explored by various optimizers efficiently. Furthermore, a surrogate-assisted performance evaluator is tailored for ACEncoding, termed sequence-to-rank (Seq2Rank). By integrating the Seq2Seq model with RankNet, Seq2Rank embeds the variable-length encoding of ACEncoding into a continuous space, and then predicts the rankings of architectures based on the continuous representation. In the experiments, ACEncoding brings improvement to neural architecture search with existing encoding schemes and Seq2Rank shows better accuracy than existing performance evaluators. The neural architectures obtained by ACEncoding and Seq2Rank have competitive test errors and complexities on image classification tasks, and also show high transferability between different data sets.},
  archive      = {J_TCDS},
  author       = {Ye Tian and Shichen Peng and Shangshang Yang and Xingyi Zhang and Kay Chen Tan and Yaochu Jin},
  doi          = {10.1109/TCDS.2021.3107555},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1129-1142},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Action command encoding for surrogate-assisted neural architecture search},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient spatial filters enhance SSVEP target recognition
based on task-related component analysis. <em>TCDS</em>, <em>14</em>(3),
1119–1128. (<a href="https://doi.org/10.1109/TCDS.2021.3096812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task-related component analysis (TRCA) has been applied successfully in the recently popular steady-state visual evoked potential (SSVEP) target recognition methods. However, a spatial filter is trained for each class in TRCA, and the training of each filter uses only the training data of the corresponding class. Therefore, the information between classes is ignored in the training process, which leads to classification inefficiency. Aiming at solving this defect in TRCA, we proposed a 2-D locality preserving projections (2DLPP) method and a 2-D linear discriminant analysis (2DLDA) method based on the 2-Norm form of Pearson’s correlation coefficient. The 2DLPP and 2DLDA methods can simultaneously use the samples of all categories to train the spatial filters so that these two methods can make use of the information between classes to some extent. We also showed that the 2DLPP method and the 2DLDA method performed significantly better than the multiset canonical correlation analysis (MsetCCA), extended CCA (eCCA), and TRCA methods with two public data sets. Therefore, the proposed methods based on 2DLPP or 2DLDA can make more efficient use of sample information and have a great potential for SSVEP target recognition.},
  archive      = {J_TCDS},
  author       = {Zhiqiang Wang and Jing Jin and Ren Xu and Chang Liu and Xingyu Wang and Andrzej Cichocki},
  doi          = {10.1109/TCDS.2021.3096812},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1119-1128},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Efficient spatial filters enhance SSVEP target recognition based on task-related component analysis},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Temporal information-guided generative adversarial networks
for stimuli image reconstruction from human brain activities.
<em>TCDS</em>, <em>14</em>(3), 1104–1118. (<a
href="https://doi.org/10.1109/TCDS.2021.3098743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding how the human brain works has attracted increasing attention in both fields of neuroscience and machine learning. Previous studies use autoencoder and generative adversarial networks (GANs) to improve the quality of stimuli image reconstruction from functional magnetic resonance imaging (fMRI) data. However, these methods mainly focus on acquiring relevant features between two different modalities of data, i.e., stimuli images and fMRI, while ignoring the temporal information of fMRI data, thus leading to suboptimal performance. To address this issue, in this article, we propose a temporal information-guided GAN (TIGAN) to reconstruct visual stimuli from human brain activities. Specifically, the proposed method consists of three key components, including: 1) an image encoder for mapping the stimuli images into latent space; 2) a long short-term memory (LSTM) generator for fMRI feature mapping, which is used to capture temporal information in fMRI data; and 3) a discriminator for image reconstruction, which is used to make the reconstructed image more similar to the original image. In addition, to better measure the relationship of two different modalities of data (i.e., fMRI and natural images), we leverage a pairwise ranking loss to rank the stimuli images and fMRI to ensure strongly associated pairs at the top and weakly related ones at the bottom. The experimental results on real-world data sets suggest that the proposed TIGAN achieves better performance in comparison with several state-of-the-art image reconstruction approaches.},
  archive      = {J_TCDS},
  author       = {Shuo Huang and Liang Sun and Muhammad Yousefnezhad and Meiling Wang and Daoqiang Zhang},
  doi          = {10.1109/TCDS.2021.3098743},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1104-1118},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Temporal information-guided generative adversarial networks for stimuli image reconstruction from human brain activities},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An end-to-end spiking neural network platform for edge
robotics: From event-cameras to central pattern generation.
<em>TCDS</em>, <em>14</em>(3), 1092–1103. (<a
href="https://doi.org/10.1109/TCDS.2021.3097675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to adapt one’s gait with environmental changes plays an essential role in the locomotion of legged robots which remains challenging for constrained computing resources and energy budget, as in the case of edge-robots. Recent advances in bio-inspired vision with dynamic vision sensors (DVSs) and associated neuromorphic processing can provide promising solutions for end-to-end sensing, cognition, and control tasks. However, such bio-mimetic closed-loop robotic systems based on event-based visual sensing and actuation in the form of spiking neural networks (SNNs) have not been well explored. In this work, we program the weights of a bio-mimetic multigait central pattern generator (CPG) and couple it with DVS-based visual data processing to show a spike-only closed-loop robotic system for a prey-tracking scenario. We first propose a supervised learning rule based on stochastic weight updates to produce a multigait producing spiking-CPG (SCPG) for hexapod robot locomotion. We then actuate the SCPG to seamlessly transition between the gaits for a nearest prey tracking task by incorporating SNN-based visual processing for input event-data generated by the DVS. This for the first time, demonstrates the natural coupling of event data flow from event-camera through SNN and neuromorphic locomotion. Thus, we exploit bio-mimetic dynamics and energy advantages of spike-based processing for autonomous edge-robotics.},
  archive      = {J_TCDS},
  author       = {Ashwin Lele and Yan Fang and Justin Ting and Arijit Raychowdhury},
  doi          = {10.1109/TCDS.2021.3097675},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1092-1103},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An end-to-end spiking neural network platform for edge robotics: From event-cameras to central pattern generation},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reactive task adaptation of a dynamic system with external
disturbances based on invariance control and movement primitives.
<em>TCDS</em>, <em>14</em>(3), 1082–1091. (<a
href="https://doi.org/10.1109/TCDS.2021.3094982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a general control framework for robot physical contact tasks. To satisfy the tradeoff between safety and performance in the interaction between robots and the environment, multipriority control is regarded as a preemptive strategy for constraint management when unknown disturbances exist. The equality and inequality constraints related to robot safety are enforced by invariance control, while reference profile tracking is accommodated by dynamic movement primitives without violating the higher priority constraints. With this framework, we complete the unification of force control and motion control in robot task execution. The robot thus acquires the capability for strong disturbance rejection and transferrable intelligence between similar tasks. At the same time, a variant linear–quadratic regulator (LQR) is integrated into the framework, which enables the robot to achieve exponential convergence of the tracking error. The proposed approach is tested and evaluated with two types of physical contact tasks, showing a superior control effect and faster convergence than the existing methods.},
  archive      = {J_TCDS},
  author       = {Caiwei Song and Gangfeng Liu and Changle Li and Jie Zhao},
  doi          = {10.1109/TCDS.2021.3094982},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1082-1091},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Reactive task adaptation of a dynamic system with external disturbances based on invariance control and movement primitives},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vector field streamline clustering framework for brain fiber
tract segmentation. <em>TCDS</em>, <em>14</em>(3), 1066–1081. (<a
href="https://doi.org/10.1109/TCDS.2021.3094555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain fiber tracts are widely used in studying brain diseases, which may lead to a better understanding of how disease affects the brain. The segmentation of brain fiber tracts assumed enormous importance in disease analysis. In this article, we propose a novel vector field streamline clustering framework for brain fiber tract segmentations. Brain fiber tracts are first expressed in a vector field and compressed using the streamline simplification algorithm. After streamline normalization and regular-polyhedron projection, high-dimensional features of each fiber tract are computed and fed to the improved deep embedded clustering (IDEC) algorithm. We also provide qualitative and quantitative evaluations of the IDEC clustering method and QB clustering method. Our clustering results of the brain fiber tracts help researchers gain perception of the brain structure. This work has the potential to automatically create a robust fiber bundle template that can effectively segment brain fiber tracts while enabling consistent anatomical tract identification.},
  archive      = {J_TCDS},
  author       = {Chaoqing Xu and Guodao Sun and Ronghua Liang and Xiufang Xu},
  doi          = {10.1109/TCDS.2021.3094555},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1066-1081},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Vector field streamline clustering framework for brain fiber tract segmentation},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robotic manipulation skill acquisition via demonstration
policy learning. <em>TCDS</em>, <em>14</em>(3), 1054–1065. (<a
href="https://doi.org/10.1109/TCDS.2021.3094269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current robots can perform repetitive tasks well, but are constrained to environment and task variations. Teaching a robot by demonstration is a powerful approach to solve the problem. The learning methods using large sensory and joint state information are extremely difficult to efficiently learn the demonstration policy. This article proposes a learning-by-imitation approach that learns demonstration policy for robotic manipulation skill acquisition from what-where-how interaction data. The method can improve the robotic adaptability to the environment and tasks with fewer training inputs. RGB-D image interaction demonstration is used. At each time step, we interact with an object and select a high-level action. The demonstration is formed through multistep interactions. An imitation learning architecture (OPLN) consisting of the objects list network (OLN) and policy learning network (PLN) is proposed. OLN and PLN are constructed, respectively, with long short-term memory (LSTM) neural networks. OLN learns objects sequence feature extracted from demonstration data while PLN learns policy. An action and a target object are obtained as outputs to control robot’s manipulation. The experiments show that the Block Stacking skill and Pick and Place skill can be successfully acquired, and the method can adapt to environment variations and generalize to similar tasks.},
  archive      = {J_TCDS},
  author       = {Dong Liu and Binpeng Lu and Ming Cong and Honghua Yu and Qiang Zou and Yu Du},
  doi          = {10.1109/TCDS.2021.3094269},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1054-1065},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Robotic manipulation skill acquisition via demonstration policy learning},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effect of rhythmic and nonrhythmic brain activity on power
spectral analysis in children with attention deficit hyperactivity
disorder. <em>TCDS</em>, <em>14</em>(3), 1046–1053. (<a
href="https://doi.org/10.1109/TCDS.2021.3094516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power spectral analysis is one of the most important analysis tools for resting-state electroencephalography (EEG) in attention deficit hyperactivity disorder (ADHD) children. However, some conclusions of power spectral analysis are different or even contradictory in ADHD studies. Little is known about the reasons. We suspect that the conventional methods do not clearly differentiate rhythmic activity from nonrhythmic activity. We propose a modified better oscillation detection method (MBOSC) to identify the rhythmic brain activity. The performance of the MBOSC method is estimated using the simulated data. Then, the relative power and the MBOSC method are, respectively, used to analyze the EEG signals recorded in 30 ADHD children and 28 typically developing children at the resting state. The results show that the frequency range detected by MBOSC is more concentrated than two other counterparts, and indicate that the change of relative power in the alpha band is caused by rhythmic brain activity, while the change of relative power in the delta band is caused by nonrhythmic brain activity. These findings provide a new perspective to investigate the inconsistency in power spectral analysis of ADHD children.},
  archive      = {J_TCDS},
  author       = {Yue Gu and Xue Li and Shengyong Chen and Xiaoli Li},
  doi          = {10.1109/TCDS.2021.3094516},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1046-1053},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Effect of rhythmic and nonrhythmic brain activity on power spectral analysis in children with attention deficit hyperactivity disorder},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparing word diversity versus amount of speech in parents’
responses to infants’ prelinguistic vocalizations. <em>TCDS</em>,
<em>14</em>(3), 1036–1045. (<a
href="https://doi.org/10.1109/TCDS.2021.3095766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our prior research posits that the prelinguistic vocalizations of infants may elicit caregiver speech which is simplified in its linguistic structure. Caregivers’ speech clearly contributes to infants’ development; infants’ communicative and cognitive development are predicted by their ambient language environment. There are at least two sources of variation in infants’ language environment: 1) the number and 2) the diversity of words infants hear. We compare the change in the total number of words (tokens) to the diversity of words against one another. Distributions of words of differing sizes are difficult to compare to one another because the size of the distribution largely determines the word diversity of the distribution. A novel approach to minimizing the challenges of comparing distributions of words is applied to data which were previously reported. We also conducted a new simulation study to estimate the probability that these results are expected by chance. We found that the linguistic structure of caregivers’ responses to infants’ prelinguistic vocalizations has fewer word types as compared to infant-directed but noncontingent speech. Our new method shows that contingent word distributions remain simplified as the number of total words sampled increases. By vocalizing, infants elicit caregiver speech, which is simpler in structure and may be easier to learn.},
  archive      = {J_TCDS},
  author       = {Steven L. Elmlinger and Deokgun Park and Jennifer A. Schwade and Michael H. Goldstein},
  doi          = {10.1109/TCDS.2021.3095766},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1036-1045},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Comparing word diversity versus amount of speech in parents’ responses to infants’ prelinguistic vocalizations},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discovering schema-based action sequences through play in
situated humanoid robots. <em>TCDS</em>, <em>14</em>(3), 1021–1035. (<a
href="https://doi.org/10.1109/TCDS.2021.3094513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exercising sensorimotor and cognitive functions allows humans, including infants, to interact with the environment and objects within it. In particular, during everyday activities, infants continuously enrich their repertoire of actions, and by playing, they experimentally plan such actions in sequences to achieve desired goals. The latter, reflected as perceptual target states, are built on previously acquired experiences shaped by infants to predict their actions. Imitating this, in developmental robotics, we seek methods that allow autonomous embodied agents with no prior knowledge to acquire information about the environment. Like infants, robots that actively explore the surroundings and manipulate proximate objects are capable of learning. Their understanding of the environment develops through the discovery of actions and their association with the resulting perceptions in the world. We extend the development of Dev-PSchema, a schema-based, open-ended learning system and examine the infant-like discovery process of new generalized skills while engaging with objects in free-play using an iCub robot. Our experiments demonstrate the capability of Dev-PSchema to utilize the newly discovered skills to solve user-defined goals beyond its past experiences. The robot can generate and evaluate sequences of interdependent high-level actions to form potential solutions and ultimately solve complex problems toward tool-use.},
  archive      = {J_TCDS},
  author       = {Suresh Kumar and Alexandros Giagkos and Patricia Shaw and Raphaël Braud and Mark Lee and Qiang Shen},
  doi          = {10.1109/TCDS.2021.3094513},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1021-1035},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Discovering schema-based action sequences through play in situated humanoid robots},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A path integration approach based on multiscale grid cells
for large-scale navigation. <em>TCDS</em>, <em>14</em>(3), 1009–1020.
(<a href="https://doi.org/10.1109/TCDS.2021.3092609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mammals have the ability to perform accurate and robust path integration-based metric navigation even in the absence of visual or other environmental cues, which provides a new idea to find brain-inspired solutions to tackle the problems of serious drift of the inertial measurement unit (IMU)-based inertial navigation of unmanned aerial vehicle (UAV) when the external sensory cues are not available. Multiscale grid cells in the medial entorhinal cortex are thought to be a fundamental portion of mammals’ ability to perform 3-D path integration-based metric navigation. This article studies and presents, for the first time, a neural system to implement path integration-based metric navigation in 3-D environments integrating networks of encoding and decoding multiscale grid cells using neural dynamic models, i.e., 3-D continuous attractor network and neural cliques, respectively. Experimental results show that the neural system can successfully path integrate self-motion information for large-scale 3-D navigation and provides robust and error-correcting position information, displaying possible neural solution to overcome serious drift of IMU-based inertial navigation of UAV in the absence of external sensory cues.},
  archive      = {J_TCDS},
  author       = {Chuang Yang and Zhi Xiong and Jianye Liu and Lijun Chao and Yudi Chen},
  doi          = {10.1109/TCDS.2021.3092609},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1009-1020},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A path integration approach based on multiscale grid cells for large-scale navigation},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A gated fusion network for dynamic saliency prediction.
<em>TCDS</em>, <em>14</em>(3), 995–1008. (<a
href="https://doi.org/10.1109/TCDS.2021.3094974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting saliency in videos is a challenging problem due to complex modeling of interactions between spatial and temporal information, especially when ever-changing, dynamic nature of videos is considered. Recently, researchers have proposed large-scale data sets and models that take advantage of deep learning as a way to understand what is important for video saliency. These approaches, however, learn to combine spatial and temporal features in a static manner and do not adapt themselves much to the changes in the video content. In this article, we introduce the gated fusion network for dynamic saliency (GFSalNet), the first deep saliency model capable of making predictions in a dynamic way via the gated fusion mechanism. Moreover, our model also exploits spatial and channelwise attention within a multiscale architecture that further allows for highly accurate predictions. We evaluate the proposed approach on a number of data sets, and our experimental analysis demonstrates that it outperforms or is highly competitive with the state of the art. Importantly, we show that it has a good generalization ability, and moreover, exploits temporal information more effectively via its adaptive fusion scheme.},
  archive      = {J_TCDS},
  author       = {Aysun Kocak and Erkut Erdem and Aykut Erdem},
  doi          = {10.1109/TCDS.2021.3094974},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {995-1008},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A gated fusion network for dynamic saliency prediction},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The study of cortical lateralization and motor performance
evoked by external visual stimulus during continuous training.
<em>TCDS</em>, <em>14</em>(3), 985–994. (<a
href="https://doi.org/10.1109/TCDS.2021.3089735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motor imagery (MI) as an important way of motor learning has been studied widely. The visual task stimulus is an efficient way to improve MI abilities; however, its impact mechanism on motor training is still obscure. This study aimed to explore the influence of visual task stimulus on MI during continuous training. Three types of kinesthetic motor imageries, including a visual task stimulus, a visual stimulus, or no stimulus, were assessed. Brain lateralization and the variations of the event-related desynchronization were investigated. The results demonstrated a greater improvement motor performance after a visual task stimulus. The MI evoked by the visual task shows the most complex event-related lateralization (ERL) among the three contexts, with the presence of many ERL components. These results reveal a complex process of planning, attention focusing, and information processing on the brain, and the complexity of the information process increases with the one of external visual stimulus. This work provides a promising foundation for the incorporation of visual-task stimuli as a strategy to improve motor ability during motor training and motor rehabilitation.},
  archive      = {J_TCDS},
  author       = {Zhongliang Yu and Lili Li and Zhizhong Wang and Hangyuan Lv and Jinchun Song},
  doi          = {10.1109/TCDS.2021.3089735},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {985-994},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {The study of cortical lateralization and motor performance evoked by external visual stimulus during continuous training},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LieNet: A deep convolution neural network framework for
detecting deception. <em>TCDS</em>, <em>14</em>(3), 971–984. (<a
href="https://doi.org/10.1109/TCDS.2021.3086011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, automatic deception detection has received considerable attention in the machine learning community owing to this research interest to its vast applications in the fields of social media, interviews, law enforcement, and the military. In this study, a novel deep convolution neural network (DCNN) named LieNet is proposed to precisely detect the multiscale variations of deception automatically. Our approach is a combination of contact and noncontact-based approaches. First, 20 frames from each video are fetched and concatenated to form a single image. Moreover, an audio signal is extracted from video and treated as image input by plotting the signal into 2-D plane. Furthermore, 13 channels of electroencephalogram signals are plotted into 2-D plane and concatenated to generate an image. Second, the LieNet model extracts features from each modality separately. Third, scores are estimated using a softmax classifier for all the modalities. Finally, three scores are combined using score level fusion to obtain a score, which gives support in favor of either deception or truth. The LieNet is validated on the “Bag-of-Lies (BoL),” “ real-life (RL) trail,” and “Miami University Deception Detection (MU3D)” databases by considering four evaluation indexes, viz., accuracy, precision, recall, and F1-score. Experimental outcomes depict that the LieNet defeats an initial work on Set-A and Set-B of the BoL database with average accuracies of 95.91% and 96.04%. respectively. The accuracies obtained by the LieNet are 97% and 98% on RL trail and MU3D databases respectively.},
  archive      = {J_TCDS},
  author       = {Mohan Karnati and Ayan Seal and Anis Yazidi and Ondrej Krejcar},
  doi          = {10.1109/TCDS.2021.3086011},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {971-984},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {LieNet: A deep convolution neural network framework for detecting deception},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geometry-aware graph embedding projection metric learning
for image set classification. <em>TCDS</em>, <em>14</em>(3), 957–970.
(<a href="https://doi.org/10.1109/TCDS.2021.3086814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By describing image sets as linear subspaces on the Grassmann manifold, image set classification has received persistent attention. Despite the success made so far, the unhelpfully intraclass diversity and interclass similarity remain two key challenges in finding an effective lower dimensional feature space for similarity measurement. To explore a feasible solution to these issues, we propose a geometry-aware graph embedding projection metric learning (GEPML) algorithm. The proposed approach first constructs the interclass and the intraclass similarity graphs on the Grassmann manifold, aiming to exploit the local structural information of the data manifold. Besides, we generalize the Euclidean collaborative representation mechanism to the Grassmann manifold to adaptively perform graph learning. Then, to learn the embedding mapping and the similarity metric jointly, we formulate the Grassmannian dimensionality reduction (GDR) problem into an elaborately designed metric learning regularization term. The proposed algorithm is appraised on five benchmarking data sets and the competitive experimental results demonstrate its feasibility and effectiveness.},
  archive      = {J_TCDS},
  author       = {Rui Wang and Xiao-Jun Wu and Zhen Liu and Josef Kittler},
  doi          = {10.1109/TCDS.2021.3086814},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {957-970},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Geometry-aware graph embedding projection metric learning for image set classification},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel teacher-assistance-based method to detect and handle
bad training demonstrations in learning from demonstration.
<em>TCDS</em>, <em>14</em>(3), 948–956. (<a
href="https://doi.org/10.1109/TCDS.2021.3097251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from demonstration (LfD) assists robots to derive a policy from a training set to execute a task. The training set consists of training demonstrations collected from a human who executes the same task. However, due to the human’s varied skill level, the quality of the training set may be bad, which will affect the accuracy of the derived policy. To solve this problem, this article proposes a novel method to improve the quality of the training set. This method includes two steps, namely detecting and handling bad training demonstrations in the training set. In the detecting step, a reference set containing reference demonstrations is provided by a human teacher. Based on the reference set, we calculate the influence of each training demonstration on the policy derivation. If the influence is negative, the corresponding training demonstration is bad. Afterward, in the handling step, we calculate the proportion of the negative influence with respect to the overall influence and reduce the proportion by iteratively removing bad training demonstrations until it is less than a threshold. The results show that the accuracy of a policy derived from the improved training set increases with up to 19.30%, which verifies the effectiveness of our method.},
  archive      = {J_TCDS},
  author       = {Qin Li and Yong Wang},
  doi          = {10.1109/TCDS.2021.3097251},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {948-956},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A novel teacher-assistance-based method to detect and handle bad training demonstrations in learning from demonstration},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GFIL: A unified framework for the importance analysis of
features, frequency bands, and channels in EEG-based emotion
recognition. <em>TCDS</em>, <em>14</em>(3), 935–947. (<a
href="https://doi.org/10.1109/TCDS.2021.3082803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately and automatically recognizing the emotional states of human beings is the central task in affective computing. The electroencephalography (EEG) data, generated from the neural activities in brain cortex, provide us with a reliable data source to perform emotion recognition. Besides the recognition accuracy, it is also necessary to explore the importance of different EEG features, frequency bands, and channels in emotion expression. In this article, we propose a unified framework termed graph-regularized least square regression with feature importance learning (GFIL) to simultaneously achieve these goals by incorporating an autoweighting variable into the least square regression. Unlike the widely used trial-and-error manner, GFIL automatically completes the identification once it is trained. Specifically, GFIL can: 1) adaptively discriminate the contributions of different feature dimensions; 2) automatically identify the critical frequency bands and channels; and 3) quantitatively rank and select the features by the learned autoweighting variable. From the experimental results on the SEED_IV data set, we find GFIL obtained improved accuracies based on the feature autoweighting strategy, which are 75.33%, 75.03%, and 79.17% corresponding to the three cross-session recognition tasks (session1-&gt;session2, session1-&gt;session3, session2-&gt;session3), respectively. Additionally, the Gamma band is identified as the most important one and the channels locating in the prefrontal and left/right central regions are more important.},
  archive      = {J_TCDS},
  author       = {Yong Peng and Feiwei Qin and Wanzeng Kong and Yuan Ge and Feiping Nie and Andrzej Cichocki},
  doi          = {10.1109/TCDS.2021.3082803},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {935-947},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {GFIL: A unified framework for the importance analysis of features, frequency bands, and channels in EEG-based emotion recognition},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Diagnosis of multiple sclerosis using graph-theoretic
measures of cognitive-task-based functional connectivity networks.
<em>TCDS</em>, <em>14</em>(3), 926–934. (<a
href="https://doi.org/10.1109/TCDS.2021.3081605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph theory allows us to gain better insight concerning changes in brain functional architecture associated with cognitive impairments in the early stages of multiple sclerosis (MS). In the present study, we employed a machine-learning system based on graph measures from functional networks constructed by cognitive-task-related functional magnetic resonance imaging (fMRI) data. We used a predefined atlas to define the brain regions and Pearson’s correlation to describe the connectivity strength between the regions. Then, several graph metrics were extracted for each subject. After that, the most efficient subsets of features were selected through the Wilcoxon rank-sum test, and the linear support vector machine (SVM) classifier was employed to distinguish between MS and healthy subjects. The node degree, subgraph centrality, K-coreness, and PageRank centralities measured in the left fusiform, hippocampus, and parahippocampal gyri regions demonstrated an accuracy of 85% through the combination of all local measures. Two optimal global measures, modularity and small-worldness index, and individual betweenness centrality feature improved the identification of MS patients with a sensitivity of 81.25%. Our results indicated the potential of the proposed system to identify cognitive changes in early MS for diagnostic purposes.},
  archive      = {J_TCDS},
  author       = {Seyedeh Naghmeh Miri Ashtiani and Hamid Behnam and Mohammad Reza Daliri},
  doi          = {10.1109/TCDS.2021.3081605},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {926-934},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Diagnosis of multiple sclerosis using graph-theoretic measures of cognitive-task-based functional connectivity networks},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A general method of realistic avatar modeling and driving
for head-mounted display users. <em>TCDS</em>, <em>14</em>(3), 916–925.
(<a href="https://doi.org/10.1109/TCDS.2021.3080588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The head-mounted displays (HMDs) provide immersive experiences in the virtual reality (VR). However, the face interactions are limited due to the serious occlusion of the user’s face. Existed approaches try to recover the user’s facial expression by adding additional sensors to HMD. In this article, we develop a novel framework to reconstruct the user’s 3-D face in VR only using an RGB camera. Given a reference face, a realistic full-textured avatar is created by fitting a 3-D Morphable Model (3DMM). A self-supervised UV map generative adversarial network (GAN) is proposed to make the facial texture look more realistic. Next, we propose a novel landmark detection method to locate the landmark positions under HMD occlusion since facial landmarks are commonly used for driving the avatar. To this end, we synthesize a face data set with HMD. Our method is easy to build and popularize with low cost. The experiments on the synthetic and real HMD data demonstrate that the proposed method can detect landmark accurately and restore facial expressions faithfully despite the large occlusion of HMD.},
  archive      = {J_TCDS},
  author       = {Ting Lu and Zhengfu Peng and Xiaofen Xing and Xiangmin Xu and Jianxin Pang},
  doi          = {10.1109/TCDS.2021.3080588},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {916-925},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A general method of realistic avatar modeling and driving for head-mounted display users},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). STEG-net: Spatiotemporal edge guidance network for video
salient object detection. <em>TCDS</em>, <em>14</em>(3), 902–915. (<a
href="https://doi.org/10.1109/TCDS.2021.3078824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decades, video salient object detection (VSOD) has achieved rapid development. However, the effect of edge information on spatial and temporal features extraction has not been explored. The method STEG-Net is proposed to solve the problem, we use the extracted edge information to guide the extraction of spatial and temporal features simultaneously. We combine deep texture information with shallow edge information, which can not only retain the edge of the object but also enhance the global information, leading to the more accurate location of the object. At the same time, the shallow edge information and the deep texture information are complementary. As a result, edge information and salient features could advance each other while extracting features. Finally, we evaluate the 16 state-of-the-art VSOD methods on ViSal and FBMS data sets. At the same time we also evaluate the 13 state-of-the-art VSOD methods on UVSD and MCL data sets. The experiment results indicate that the proposed method outperforms the state-of-the-art approaches.},
  archive      = {J_TCDS},
  author       = {Hongbo Bi and Lina Yang and Huihui Zhu and Di Lu and Jianguo Jiang},
  doi          = {10.1109/TCDS.2021.3078824},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {902-915},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {STEG-net: Spatiotemporal edge guidance network for video salient object detection},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixture distribution graph network for few shot learning.
<em>TCDS</em>, <em>14</em>(3), 892–901. (<a
href="https://doi.org/10.1109/TCDS.2021.3075280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning aims at heuristically resolving new tasks with limited labeled data; most of the existing approaches are affected by knowledge learned from similar experiences. However, interclass barriers and new samples insufficiency limit the transfer of knowledge. In this article, we propose a novel mixture distribution graph network, in which the interclass relation is explicitly modeled and propagated via graph generation. Owing to the weighted distribution features based on the Gaussian mixture model, we take class diversity into consideration, thereby utilizing information precisely and efficiently. Equipped with minimal gated units, the “memory” of similar tasks can be preserved and reused through episode training, which fills a gap in temporal characteristics and softens the impact of data insufficiency. Extensive trials are carried out based on the MiniImageNet and CIFAR-FS data sets. The results turn out that our method exceeds most state-of-the-art approaches, which shows the validity and universality of our method in few-shot learning.},
  archive      = {J_TCDS},
  author       = {Baiyan Zhang and Hefei Ling and Jialie Shen and Qian Wang and Jie Lei and Yuxuan Shi and Lei Wu and Ping Li},
  doi          = {10.1109/TCDS.2021.3075280},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {892-901},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Mixture distribution graph network for few shot learning},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Electroencephalogram emotion recognition based on dispersion
entropy feature extraction using random oversampling imbalanced data
processing. <em>TCDS</em>, <em>14</em>(3), 882–891. (<a
href="https://doi.org/10.1109/TCDS.2021.3074811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram (EEG) is the brain’s electrical activity measure, which can reflect people’s inner emotional states objectively. In this article, a dispersion entropy (DispEn) feature extraction-based EEG emotion recognition method is proposed. In feature extraction, the DispEn is computed for the four typical frequency bands, i.e., $\theta $ , $\alpha $ , $\beta $ , and $\gamma $ of EEG signals which are filtered from 32 channels. Furthermore, a random oversampling algorithm is employed to balance the data for the emotional labels to avoid majority biases. The proposed method not only has a better ability to characterize EEG signals but also has a faster recognition speed. In the experiments, the DEAP dataset is used to validate the effectiveness of the proposal, in which the DispEn is extracted from the undecomposed signal and four typical EEG rhythms are compared for emotion recognition by using a support vector machine (SVM). Besides, comparison experiments using DispEn, sample entropy (SampEn), permutation entropy (PerEn), and three other commonly used statistical features are performed. The experimental results show that the proposed method achieves recognition accuracy in high valence (HV)/low valence (LV) and high arousal (HA)/low arousal (LA) is 72.95% and 76.67%, respectively. The computation cost of DispEn feature extraction is ${O}(N)$ that better than some state-of-the-art methods.},
  archive      = {J_TCDS},
  author       = {Xue-Wen Ding and Zhen-Tao Liu and Dan-Yun Li and Yong He and Min Wu},
  doi          = {10.1109/TCDS.2021.3074811},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {882-891},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Electroencephalogram emotion recognition based on dispersion entropy feature extraction using random oversampling imbalanced data processing},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robot navigation based on situational awareness.
<em>TCDS</em>, <em>14</em>(3), 869–881. (<a
href="https://doi.org/10.1109/TCDS.2021.3075862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous navigation for mobile robots is a prerequisite to complete the tasks. In this article, we propose a novel navigation method based on situational awareness. The scene predictor, scene interpreter and the topological map-based planner are the main components, where the scene interpreter is used to realize the mapping of perception information, and the scene predictor predicts the information in the neighborhood of current position. The basic idea of this research is the feeling of path. Based on the definitions of path and the amount of perception, the situational awareness fitting network is designed as the scene interpreter, and the position of the robot in a path can be implicitly described by a situational awareness value. With the global guidance of the planner, the robot achieves navigation. The proposed navigation method does not rely on Cartesian-based global position and its effectiveness is verified by simulations and experiments.},
  archive      = {J_TCDS},
  author       = {Xilong Liu and Zhiqiang Cao and Yingying Yu and Guangli Ren and Junzhi Yu and Min Tan},
  doi          = {10.1109/TCDS.2021.3075862},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {869-881},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Robot navigation based on situational awareness},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Designing an artificial agent for cognitive apprenticeship
learning of elevator pitch in virtual reality. <em>TCDS</em>,
<em>14</em>(3), 857–868. (<a
href="https://doi.org/10.1109/TCDS.2021.3073814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing social skills like elevator pitch requires being situated within authentic activities and contexts, which is difficult to achieve on a daily basis. In this work, we explore whether an artificial agent with embodied feedback in virtual reality (VR) can foster a situated learning experience. Previous works on computer-mediated feedback have shown that VR can foster oral presentation competence for preuniversity and junior undergraduates through delivering feedback. However, it is unclear how well the learning experiences are and how well students perceive an artificial coach in VR, especially for senior undergraduates and postgraduates seeking job and research opportunities. To inform the design of such a system, we conducted interviews with experts and observed real elevator pitches. We then designed a proof-of-concept VR coaching system with three embodied feedback strategies: immediate, after-action, and the combination of both. Through a between-subject experiment with 40 participants, we studied learners’ perceptions under the embodied feedback. We found that receiving embodied feedback can create a stronger sense of cognitive apprenticeship, i.e., coaching and helping from experts, and help improve the perception of the artificial agent and the effect of learning. We further investigated the pros and cons of different strategies and discussed room for improvement.},
  archive      = {J_TCDS},
  author       = {Zhenjie Zhao and Xiaojuan Ma},
  doi          = {10.1109/TCDS.2021.3073814},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {857-868},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Designing an artificial agent for cognitive apprenticeship learning of elevator pitch in virtual reality},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient learning algorithm for direct training deep
spiking neural networks. <em>TCDS</em>, <em>14</em>(3), 847–856. (<a
href="https://doi.org/10.1109/TCDS.2021.3073846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is challenging to train deep spiking neural networks (SNNs) directly due to the difficulties associated with the nondifferentiable neuron model. In this work, an end-to-end learning algorithm based on discrete current-based leaky integrate-and-fire (C-LIF) neuron model and surrogate gradient is proposed to leverage the encoder–decoder network architecture to train deep SNNs directly. The proposed algorithm is capable of learning deep spatiotemporal features relying on current time step only, and several acceleration techniques including backward phase skipping and layerwise FreezeOut are proposed to accelerate the training. Experimental results show that the proposed learning algorithm achieved the classification accuracies of 98.40% and 95.83% on the dynamic neuromorphic data sets MNIST-DVS and DVS-Gestures, respectively, and of 99.58% and 95.97% on static vision data sets MNIST and SVHN, respectively, which are comparable to the existing state-of-the-art results. The training speed was accelerated by up to 49.5% on MNIST and 36.6% on DVS-Gestures with the proposed acceleration techniques while maintaining the same level of accuracy.},
  archive      = {J_TCDS},
  author       = {Xiaolei Zhu and Baixin Zhao and De Ma and Huajin Tang},
  doi          = {10.1109/TCDS.2021.3073846},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {847-856},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An efficient learning algorithm for direct training deep spiking neural networks},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Can emotion be transferred?—a review on transfer learning
for EEG-based emotion recognition. <em>TCDS</em>, <em>14</em>(3),
833–846. (<a href="https://doi.org/10.1109/TCDS.2021.3098842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The issue of electroencephalogram (EEG)-based emotion recognition has great academic and practical significance. Currently, there are numerous research trying to address this issue in the literature. Particularly, transfer learning has gradually become a new methodological trend for the issue in company with the popularity of deep learning. Motivated by capturing the research panorama, summarizing the technological essence, and forecasting the advancement tendency of transfer learning for EEG-based emotion recognition, this article contributes a review work. This work mainly includes five aspects: 1) introducing the issue of EEG-based emotion recognition and expounding the importance of transfer learning for it; 2) analyzing the transfer learning framework and comparing it with the traditional ones; 3) elucidating the issue difficulties and explaining the suitability and capability of transfer learning for this issue; 4) summarizing, categorizing, and exemplifying the typical transfer learning methods for this issue; and 5) clarifying the methodological merits, discussing the challenging problems, and predicting the prospective development of transfer learning for the issue. We expect these contributions can inspire innovation and reformation of the transfer learning methodology for EEG-based emotion recognition as well as other relevant topics in the not-so-far future.},
  archive      = {J_TCDS},
  author       = {Wei Li and Wei Huan and Bowen Hou and Ye Tian and Zhen Zhang and Aiguo Song},
  doi          = {10.1109/TCDS.2021.3098842},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {833-846},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Can emotion be Transferred?—A review on transfer learning for EEG-based emotion recognition},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Diagnosis and intervention for children with autism spectrum
disorder: A survey. <em>TCDS</em>, <em>14</em>(3), 819–832. (<a
href="https://doi.org/10.1109/TCDS.2021.3093040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the prevalence of autism spectrum disorder (ASD) has been proliferating rapidly around the world. This article aims to provide a comprehensive survey of the diagnosis and intervention for autistic children. Two assessment frameworks are introduced first to provide theoretical support for autistic engineering technologies: 1) international classification of diagnosis (ICD) and 2) international classification of functionalities (ICF). Then, autistic diagnosis and intervention techniques are presented respectively with different modalities. Multimodal sensing strategies for autism assessment are summarized subsequently. Finally, this article is concluded with future directions and challenges. This article overviews state-of-the-art research in the field of autistic diagnosis and intervention theory and applications giving priority to ICF-based solutions.},
  archive      = {J_TCDS},
  author       = {Zhiyong Wang and Jingjing Liu and Wanqi Zhang and Wei Nie and Honghai Liu},
  doi          = {10.1109/TCDS.2021.3093040},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {819-832},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Diagnosis and intervention for children with autism spectrum disorder: A survey},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cognitive workload recognition using EEG signals and machine
learning: A review. <em>TCDS</em>, <em>14</em>(3), 799–818. (<a
href="https://doi.org/10.1109/TCDS.2021.3090217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning and its subfield deep learning techniques provide opportunities for the development of operator mental state monitoring, especially for cognitive workload recognition using electroencephalogram (EEG) signals. Although a variety of machine learning methods have been proposed for recognizing cognitive workload via EEG recently, there does not yet exist a review that covers in-depth the application of machine learning methods. To alleviate this gap, in this article, we survey cognitive workload and machine learning literature to identify the approaches and highlight the primary advances. To be specific, we first introduce the concepts of cognitive workload and machine learning. Then, we discuss the steps of classical machine learning for cognitive workload recognition from the following aspects, i.e., EEG data preprocessing, feature extraction and selection, classification method, and evaluation methods. Further, we review the commonly used deep learning methods for this domain. Finally, we expound on the open problem and future outlooks.},
  archive      = {J_TCDS},
  author       = {Yueying Zhou and Shuo Huang and Ziming Xu and Pengpai Wang and Xia Wu and Daoqiang Zhang},
  doi          = {10.1109/TCDS.2021.3090217},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {799-818},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Cognitive workload recognition using EEG signals and machine learning: A review},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dialogue management in conversational systems: A review of
approaches, challenges, and opportunities. <em>TCDS</em>,
<em>14</em>(3), 783–798. (<a
href="https://doi.org/10.1109/TCDS.2021.3086565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attracted by their easy-to-use interfaces and captivating benefits, conversational systems have been widely embraced by many individuals and organizations as side-by-side digital co-workers. They enable the understanding of user needs, expressed in natural language, and on fulfilling such needs by invoking the appropriate backend services (e.g., APIs). Controlling the conversation flow, known as Dialogue Management , is one of the essential tasks in conversational systems and the key to its success and adoption as well. Nevertheless, designing scalable and robust dialogue management techniques to effectively support intelligent conversations remains a deeply challenging problem. This article studies dialogue management from an in-depth design perspective. We discuss the state-of-the-art approaches, identify their recent advances and challenges, and provide an outlook on future research directions. Thus, we contribute to guiding researchers and practitioners in selecting the appropriate dialogue management approach aligned with their objectives, among the variety of approaches proposed so far.},
  archive      = {J_TCDS},
  author       = {Hayet Brabra and Marcos Báez and Boualem Benatallah and Walid Gaaloul and Sara Bouguelia and Shayan Zamanirad},
  doi          = {10.1109/TCDS.2021.3086565},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {783-798},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Dialogue management in conversational systems: A review of approaches, challenges, and opportunities},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). IEEE computational intelligence society. <em>TCDS</em>,
<em>14</em>(2), C3. (<a
href="https://doi.org/10.1109/TCDS.2022.3178531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TCDS},
  doi          = {10.1109/TCDS.2022.3178531},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {C3},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {IEEE computational intelligence society},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EMRES: A new EMotional RESpondent robot. <em>TCDS</em>,
<em>14</em>(2), 772–780. (<a
href="https://doi.org/10.1109/TCDS.2021.3120562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this work is to design an artificial empathetic system and to implement it into an EMotional RESpondent (EMRES) robot, called EMRES. Rather than mimic the expression detected in the human partner, the proposed system achieves a coherent and consistent emotional trajectory resulting in a more credible human-agent interaction. Inspired by developmental robotics theory, EMRES has an internal state and a mood, which contribute in the evolution of the flow of emotions; at every episode, the next emotional state of the agent is affected by its internal state, mood, current emotion, and the expression read in the human partner. As a result, EMRES does not imitate, but it synchronizes to the emotion expressed by the human companion. The agent has been trained to recognize expressive faces of the FER2013 database and it is capable of achieving 78.3% performance with wild images. Our first prototype has been implemented into a robot, which has been created for this purpose. An empirical study run with university students judged in a positive way the newly proposed artificial empathetic system.},
  archive      = {J_TCDS},
  author       = {Elena Battini Sönmez and Hasan Han and Oǧuzcan Karadeniz and Tuǧba Dalyan and Baykal Sarıoǧlu},
  doi          = {10.1109/TCDS.2021.3120562},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {772-780},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {EMRES: A new EMotional RESpondent robot},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Investigating neural substrates of individual independence
and interdependence orientations via efficiency-based dynamic functional
connectivity: A machine learning approach. <em>TCDS</em>,
<em>14</em>(2), 761–771. (<a
href="https://doi.org/10.1109/TCDS.2021.3101643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The self-construal is one of the most significant cultural markers in humans. Accordingly, mapping the relationship between brain activity and self-construal contributes to understanding the nature of such psychological traits. Existing studies have mainly focused on static functional brain activities in specific brain regions. However, evidence has suggested that the functional connectivity (FC) of the brain network is dynamic over time and the high-level psychological processes might require collaboration among multiple regions. In the present study, we explored the dynamic connection patterns of the two most representative types of self-construal traits, namely, independence and interdependence, using machine learning-based models. We performed resting-state functional MRI (rs-fMRI) on a sample of young adults ( $n=359$ ) who completed Singelis’ Self-Construal Scale, and constructed the efficiency-based dynamic FC networks. XGBoost Regression was subsequently applied to learn the relationship between the dynamic FC and the two self-construals without any priori bias or hypothesis. The performance of the regression model was validated by the nested tenfold cross-validation. The results showed that the efficiency-based dynamic FC could identify the orientations of independence and interdependence. The comparison analyses revealed that prediction accuracy using this dynamic FC method was significantly improved compared to the conventional static FC method. By exploring key connectivities selected by the regression model, we observed that the independence orientation was mainly characterized by the right-hemisphere FC, while the interdependence orientation by the left-hemisphere FC. The results suggest that the self-construals are associated with distributed neural networks the entire brain. These findings provide the pivotal ingredients toward the biological essence of culturally related variables in the brain by taking advances in cultural psychology, neuroscience, together with machine-learning analytic technologies.},
  archive      = {J_TCDS},
  author       = {Yifan Zhu and Xuesong Li and Yang Sun and Haixu Wang and Hua Guo and Jie Sui},
  doi          = {10.1109/TCDS.2021.3101643},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {761-771},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Investigating neural substrates of individual independence and interdependence orientations via efficiency-based dynamic functional connectivity: A machine learning approach},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiscale conditional relationship graph network for
referring relationships in images. <em>TCDS</em>, <em>14</em>(2),
752–760. (<a href="https://doi.org/10.1109/TCDS.2021.3079278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images contain not only individual entities but also abundant visual relationships between entities. Therefore, conditioned on visual relationship triples ${&amp;lt; }subject-relationship-object{&amp;gt;}$ that can be viewed as structured texts, entities (subjects or objects) can be localized in images without ambiguity. However, it is challenging to efficiently model visual relationships since a specific relationship usually has dramatic intraclass visual differences when involved with different entities, quite a number of which are in a small scale. In addition, the subject and the object in a relationship triple may have different best scales, and matching the subject and the object with different appropriate scales may improve prediction. To address these issues, a multiscale conditional relationship graph network (CRGN) is proposed in this article to localize entities based on visual relationships. Specifically, an attention pyramid network is first introduced to generate multiscale attention maps to capture entities with various sizes for entity matching. Then, a CRGN is further designed to aggregate and refine multiscale attention features to localize entities via passing relationship contexts between entity attention maps, which sufficiently utilizes the entity attention maps with the best scales. In order to mitigate the negative effects of intraclass visual differences of relationships, vision-agnostic relationship features are utilized in the proposed CRGN to indirectly model relationship contexts. The experiments demonstrate the superiority of the proposed method compared with the previous powerful frameworks on three challenging benchmark data sets, including CLEVR, Visual Genome, and VRD. The project page can be found in https://mic.tongji.edu.cn/d9/5c/c9778a186716/page.htm .},
  archive      = {J_TCDS},
  author       = {Jian Zhu and Hanli Wang},
  doi          = {10.1109/TCDS.2021.3079278},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {752-760},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multiscale conditional relationship graph network for referring relationships in images},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time-spatial multiscale net for vehicle counting and traffic
volume estimation. <em>TCDS</em>, <em>14</em>(2), 740–751. (<a
href="https://doi.org/10.1109/TCDS.2021.3073694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle counting and traffic volume estimation on traffic videos are challenging tasks for traffic monitoring and management. Most previous methods are based on a process of vehicle detection and tracking or a process of time-spatial image (TSI)-based background subtraction, which suffer from occlusion and small size of vehicles. To overcome these difficulties, a time-spatial-based structure is proposed for vehicle counting and traffic volume estimation, without relying on tracking and TSI-based background subtraction. This structure has three main parts. First, a TSI-based density map generation model is proposed for generating density maps for TSIs, which makes it possible to automatically generate TSI training samples. Second, the time-spatial multiscale net (TM-Net) is proposed for estimating density map of TSI; with the stacked multiscale modules and the spatial attention module, the proposed TM-Net can partly overcome the difficulties brought from occlusion, small size, and deformation. Finally, a vehicle counting and traffic volume estimation model is designed for counting and volume estimation on the results of TM-Net. Experiments performed on the public UA-DETRAC data set show that the proposed TM-Net-based vehicle counting method outperforms the tested representative counting methods, and the proposed framework also can estimate traffic volume efficiently.},
  archive      = {J_TCDS},
  author       = {Shuang Li and Chunsheng Liu and Faliang Chang},
  doi          = {10.1109/TCDS.2021.3073694},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {740-751},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Time-spatial multiscale net for vehicle counting and traffic volume estimation},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncertainty modeling for multicenter autism spectrum
disorder classification using takagi–sugeno–kang fuzzy systems.
<em>TCDS</em>, <em>14</em>(2), 730–739. (<a
href="https://doi.org/10.1109/TCDS.2021.3073368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The resting-state functional magnetic resonance imaging (rs-fMRI) is a pivotal tool that can reveal brain dysfunction in the computer-aided diagnosis of the autism spectrum disorder (ASD). However, the instability of data collection devices, complexity of pathogenesis, and ambiguity in the causes of the disease always introduce considerable uncertainty in identifying ASD using rs-fMRI. Due to the strong ability of Takagi–Sugeno–Kang fuzzy inference systems (TSK FISs) in handling the uncertainty of knowledge and expression, we build an ASD classification model based on TSK FISs and further propose a novel multicenter ASD classification method FCG-MTGS-TSK. Specifically, the correlation information of multiple imaging centers is considered by introducing multitask group sparse learning, and the features across multiple imaging centers are thus jointly selected. An augmented lagrange multiplier (ALM) method is further developed to find the optimal solution of the model. Compared with the other existing methods, the proposed method has the advantages of strong interpretability and high classification accuracy. The experimental results also identify the most discriminative functional connectivity in multicenter ASD classification.},
  archive      = {J_TCDS},
  author       = {Zhongyi Hu and Jun Wang and Chunxiang Zhang and Zhenzhen Luo and Xiaoqing Luo and Lei Xiao and Jun Shi},
  doi          = {10.1109/TCDS.2021.3073368},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {730-739},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Uncertainty modeling for multicenter autism spectrum disorder classification using Takagi–Sugeno–Kang fuzzy systems},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparing recognition performance and robustness of
multimodal deep learning models for multimodal emotion recognition.
<em>TCDS</em>, <em>14</em>(2), 715–729. (<a
href="https://doi.org/10.1109/TCDS.2021.3071170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal signals are powerful for emotion recognition since they can represent emotions comprehensively. In this article, we compare the recognition performance and robustness of two multimodal emotion recognition models: 1) deep canonical correlation analysis (DCCA) and 2) bimodal deep autoencoder (BDAE). The contributions of this article are threefold: 1) we propose two methods for extending the original DCCA model for multimodal fusion: a) weighted sum fusion and b) attention-based fusion; 2) we systemically compare the performance of DCCA, BDAE, and traditional approaches on five multimodal data sets; and 3) we investigate the robustness of DCCA, BDAE, and traditional approaches on SEED-V and DREAMER data sets under two conditions: 1) adding noises to multimodal features and 2) replacing electroencephalography features with noises. Our experimental results demonstrate that DCCA achieves state-of-the-art recognition results on all five data sets: 1) 94.6% on the SEED data set; 2) 87.5% on the SEED-IV data set; 3) 84.3% and 85.6% on the DEAP data set; 4) 85.3% on the SEED-V data set; and 5) 89.0%, 90.6%, and 90.7% on the DREAMER data set. Meanwhile, DCCA has greater robustness when adding various amounts of noises to the SEED-V and DREAMER data sets. By visualizing features before and after DCCA transformation on the SEED-V data set, we find that the transformed features are more homogeneous and discriminative across emotions.},
  archive      = {J_TCDS},
  author       = {Wei Liu and Jie-Lin Qiu and Wei-Long Zheng and Bao-Liang Lu},
  doi          = {10.1109/TCDS.2021.3071170},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {715-729},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effect of physical and virtual feedback on reach-to-grasp
movements in virtual environments. <em>TCDS</em>, <em>14</em>(2),
708–714. (<a href="https://doi.org/10.1109/TCDS.2021.3066618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object grabbing is a common interaction task in virtual environments, where the virtual and physical feedback of both the hand and the targeted object may impact its performance. To investigate these effects, we first designed a comparative study where subjects were required to reach and grasp a physical cylinder under four conditions: 1) no virtual hand and no virtual cylinder; 2) virtual cylinder without virtual hand; 3) virtual hand and cylinder; and 4) real environment. The results showed that the subjects performed with different kinematic patterns in all three virtual conditions compared to the real environment. Additionally, we found that this difference is not due to the environment or depth illusion. We performed another experiment to examine the effects of physical cylinder feedback on reach-to-grasp movement. We found that the subjects performed similarly to the physical environment only in the condition with a virtual hand and a virtual cylinder. These results suggest that feedback with virtual hand and virtual targeted object would result in similar kinematic patterns in virtual environments as in physical environments, which implies design guidelines in virtual environments for rehabilitation in patients with motor deficits.},
  archive      = {J_TCDS},
  author       = {Qiuwen Cai and Junhua Li and Jinyi Long},
  doi          = {10.1109/TCDS.2021.3066618},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {708-714},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Effect of physical and virtual feedback on reach-to-grasp movements in virtual environments},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention-mechanism-based real-time gaze tracking in natural
scenes with residual blocks. <em>TCDS</em>, <em>14</em>(2), 696–707. (<a
href="https://doi.org/10.1109/TCDS.2021.3064280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaze tracking is widely used in fatigue driving detection, eye disease diagnosis, mental illness diagnosis, website or advertising design, virtual reality, gaze-control devices, and human–computer interaction. However, the influence of light, specular reflection, and occlusion, and the change of head pose, especially the ever-changing human pose in natural scenes, have brought great challenges to the accurate gaze tracking. In this article, gaze tracking in natural scenes is studied, and a method based on the convolutional neural network with residual blocks is proposed, in which the attention mechanism is integrated into the network to improve the accuracy of gaze tracking. Furthermore, it is tested on the GazeFollow database, which contains six kinds of databases. The results show that the performance of the proposed method outperforms that of the other state-of-the-art methods in natural scenes. Moreover, the proposed method has better real-time performance and is more suitable for practical applications.},
  archive      = {J_TCDS},
  author       = {Lihong Dai and Jinguo Liu and Zhaojie Ju and Yang Gao},
  doi          = {10.1109/TCDS.2021.3064280},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {696-707},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Attention-mechanism-based real-time gaze tracking in natural scenes with residual blocks},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Epileptic classification with deep-transfer-learning-based
feature fusion algorithm. <em>TCDS</em>, <em>14</em>(2), 684–695. (<a
href="https://doi.org/10.1109/TCDS.2021.3064228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epilepsy ictal detection based on scalp electroencephalograms (EEGs) has been comprehensively studied in the past decades. But few attentions have been paid to the preictal classification. In this article, a comprehensive study on epileptic state classification based on deep transfer learning (TL) is presented. The main contributions include: 1) the subband mean amplitude spectrum (MAS) map that characterizes the typical rhythms of brain activities is extracted for EEG representation; 2) five representative deep neural networks (DNNs) pretrained on ImageNet are applied for EEG feature TL; and 3) a 7-layer hierarchical neural network (HNN) that consists of three fully connected (Fc) and three dropout layers followed by a Softmax layer is developed to perform the epileptic state probability learning and classification. Experiments on the benchmark CHB-MIT and iNeuro EEG databases that contain several different types of seizures show that the proposed algorithm achieves the highest overall accuracies of 96.97% and 87.87% on the 5-state epileptic classification, respectively, that outperforms many existing state-of-the-art methods presented in this article.},
  archive      = {J_TCDS},
  author       = {Jiuwen Cao and Dinghan Hu and Yaomin Wang and Jianzhong Wang and Baiying Lei},
  doi          = {10.1109/TCDS.2021.3064228},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {684-695},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Epileptic classification with deep-transfer-learning-based feature fusion algorithm},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-fusion-based two-stage cascade framework for
multimodality face anti-spoofing. <em>TCDS</em>, <em>14</em>(2),
672–683. (<a href="https://doi.org/10.1109/TCDS.2021.3064679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing face anti-spoofing models using deep learning for multimodality data suffer from low generalization in the case of using variety of presentation attacks, such as 2-D printing and high-precision 3-D face masks. One of the main reasons is that the nonlinearity of multispectral information used to preserve the intrinsic attributes between a real and a fake face is not well extracted. To address this issue, we propose a multimodility data-based two-stage cascade framework for face anti-spoofing. The proposed framework has two advantages. First, we design a two-stage cascade architecture that can selectively fuse low-level and high-level features from different modalities to improve feature representation. Second, we use multimodality data to construct a distance-free spectral on RGB and infrared to augment the nonlinearity of data. The presented data fusion strategy is different from popular fusion approaches, since it can strengthen discrimination ability of network models on physical attribute features than identity structure features under certain constraints. In addition, a multiscale patch-based weighted fine-tuning strategy is designed to learn each specific local face region. The experimental results show that the proposed framework achieves better performance than other state-of-the-art methods on both benchmark data sets and self-established data sets, especially on multimaterial masks spoofing.},
  archive      = {J_TCDS},
  author       = {Weihua Liu and Xiaokang Wei and Tao Lei and Xingwu Wang and Hongying Meng and Asoke K. Nandi},
  doi          = {10.1109/TCDS.2021.3064679},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {672-683},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Data-fusion-based two-stage cascade framework for multimodality face anti-spoofing},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional generative adversarial networks for optimal path
planning. <em>TCDS</em>, <em>14</em>(2), 662–671. (<a
href="https://doi.org/10.1109/TCDS.2021.3063273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path planning plays an important role in autonomous robot systems. Effective understanding of the surrounding environment and efficient generation of an optimal collision-free path are both critical parts for solving path-planning problems. Although conventional sampling-based algorithms, such as the rapidly exploring random tree (RRT) and its improved optimal version (RRT*), have been widely used in path-planning problems because of their ability to find a feasible path in even complex environments, they fail to find an optimal path efficiently. To solve this problem and satisfy the two aforementioned requirements, we propose a novel learning-based path-planning algorithm which consists of a novel generative model based on the conditional generative adversarial networks (CGANs) and a modified RRT* algorithm (denoted by CGAN-RRT*). Given the map information, our CGAN model can generate an efficient probability distribution of feasible paths, which can be utilized by the CGAN-RRT* algorithm to find an optimal path with a nonuniform sampling strategy. The CGAN model is trained by learning from ground-truth maps, each of which is generated by putting all the results of executing the RRT algorithm 50 times on one raw map. We demonstrate the efficient performance of this CGAN model by testing it on two groups of maps and comparing the CGAN-RRT* algorithm with the Informed-RRT* algorithm and conventional RRT* algorithm.},
  archive      = {J_TCDS},
  author       = {Nachuan Ma and Jiankun Wang and Jianbang Liu and Max Q.-H. Meng},
  doi          = {10.1109/TCDS.2021.3063273},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {662-671},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Conditional generative adversarial networks for optimal path planning},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MFFFLD: A multimodal-feature-fusion-based fingerprint
liveness detection. <em>TCDS</em>, <em>14</em>(2), 648–661. (<a
href="https://doi.org/10.1109/TCDS.2021.3062624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometrics spoofing attack (BsSA) frequently occurs when an adversary impersonates a lawful user to access to the biometric system by means of some forged or synthetic samples, especially in fingerprint or face authentication. In allusion to the problem above, the mainstream countermeasure, called biometrics liveness detection (BLD), is raised. In this article, we propose a more robust and accurate BLD strategy by taking advantage of weighted multimodal convolutional neural networks (MCNNs) to extract diverse deep features. Before detection, the ROI operation first is performed to remove those invalid backgrounds of fingerprints. Then, a multimodal feature fusion strategy is proposed to make full use of the learning capacity of convolutional neural networks (CNNs) without human interactions. It is well known that characteristics of the different direct splicing together and for the subsequent classification is unreasonable, thus, a weighted summation strategy is explored. More specifically, we assign each type of feature weight contribution rate, sum them, and then learn the optimal combination of different model features. In the final detection phase, to verify our proposed algorithm with higher accuracy, detailed analyses of the fingerprint evaluations on intradatabase, cross-material, and cross-database, respectively, which also include assessment under the fusion of different modal features, and face evaluations on the same-database, are evaluated. Experimental results on several benchmark data sets LivDet 2011, 2013, 2015, and NUAA demonstrate that our approach achieves outstanding results in fingerprint intradatabase and cross-material evaluations as well as face anti-spoofing evaluations comparing with previous methods. Most importantly, our method is more accurate and robust than other existing fingerprint anti-spoofing methods when evaluating cross-database.},
  archive      = {J_TCDS},
  author       = {Chengsheng Yuan and Shengming Jiao and Xingming Sun and Q. M. Jonathan Wu},
  doi          = {10.1109/TCDS.2021.3062624},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {648-661},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {MFFFLD: A multimodal-feature-fusion-based fingerprint liveness detection},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust heart rate estimation with spatial–temporal attention
network from facial videos. <em>TCDS</em>, <em>14</em>(2), 639–647. (<a
href="https://doi.org/10.1109/TCDS.2021.3062370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve the problems of highly redundant spatial information and motion noise in the heart rate (HR) estimation from facial videos based on remote photoplethysmography (rPPG), this article proposes a novel HR estimation method based on spatial–temporal attention model. First, to reduce the redundant information and strengthen the association relationships of long-range videos, the spatial–temporal facial features are extracted by the 2-D convolutional neural network (2DCNN) and 3-D convolutional neural network (3DCNN), respectively. The aggregation function is adopted to incorporate feature maps into short segment spatial–temporal feature maps. Second, the spatial–temporal strip pooling is designed in the spatial–temporal attention module to reduce head movement noises. Then, via the two-part loss function, the model can focus more on the rPPG signal rather than the interference. We conduct extensive experiments on two public data sets to verify the effectiveness of our model. The experimental results show that the proposed method achieves significantly better performances than the state-of-the-art baselines: The mean absolute error could be reduced by 11% on the PURE data set, and by 25% on the COHFACE data set.},
  archive      = {J_TCDS},
  author       = {Min Hu and Fei Qian and Xiaohua Wang and Lei He and Dong Guo and Fuji Ren},
  doi          = {10.1109/TCDS.2021.3062370},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {639-647},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Robust heart rate estimation with Spatial–Temporal attention network from facial videos},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature-specific denoising of neural activity for natural
image identification. <em>TCDS</em>, <em>14</em>(2), 629–638. (<a
href="https://doi.org/10.1109/TCDS.2021.3062067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decoding the content in neural activity through voxelwise encoding plays an important role in investigating cognitive functions of the human brain. However, unlike multivoxel pattern analysis (MVPA), voxelwise encoding builds a model for each individual voxel; therefore, ignores the interactions between voxels and is sensitive to noise. In this work, we propose the feature-specific denoise (FSdenoise), a noise reduction method for encoding-based models to improve their decoding performance. FSdenoise considers the response of a voxel to a stimulus as a combination of two components: 1) feature-relevant component, which can be predicted from stimulus features and 2) feature-irrelevant component, which shows no direct relation to the concerned features. Exploiting the correlations between voxels, FSdenoise reduces the feature-irrelevant component in voxels that exhibit more feature-relevant component, enhancing their predictive power from stimulus features. Decoding performance with the denoised voxels would be improved in consequence. We validate the FSdenoise on two functional magnetic resonance imaging data sets and the results demonstrate that FSdenoise can efficiently improve the decoding accuracy for encoding-based approaches. Moreover, the encoding-based approaches combined with FSdenoise can even outperform the MVPA-based approach in brain decoding.},
  archive      = {J_TCDS},
  author       = {Hao Wu and Nanning Zheng and Badong Chen},
  doi          = {10.1109/TCDS.2021.3062067},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {629-638},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Feature-specific denoising of neural activity for natural image identification},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A brain-inspired self-organizing episodic memory model for a
memory assistance robot. <em>TCDS</em>, <em>14</em>(2), 617–628. (<a
href="https://doi.org/10.1109/TCDS.2021.3061659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article discusses the implementation of a brain-inspired episodic memory model, which provides memory assistance and tackles the modern public issue of memory impairment embedded as an end-to-end system on the robot companion, Pepper. Based on the fusion adaptive resonance theory, the proposed model can observe and memorize the content of daily events in five aspects: 1) people; 2) activities; 3) times; 4) places; and 5) objects. The model is based on the human memory pipeline, containing a working memory and a two-layer long-term memory model, which can effectively merge, cluster, and summarize past memories based on their context and relevance in a self-organizing manner. When providing memory assistance, the robot can analyze a user query and find the best matching memory cluster to generate verbal cues to stimulate recalling of the target event. Moreover, using reinforcement learning, the robot eventually learns the most effective mapping of cue types to event type through social interaction. Experiments show the feasibility of the proposed model, which can handle episodic events with elasticity and stability. Moreover, there is evidence that the robot is able to provide robust memory assistance from knowledge obtained through previous observations, with 99% confidence, intervals in the participants’ mean recall percentage of the events increases 19.63% after receiving memory assistance from the robot.},
  archive      = {J_TCDS},
  author       = {Chiao-Yu Yang and Edwinn Gamborino and Li-Chen Fu and Yu-Ling Chang},
  doi          = {10.1109/TCDS.2021.3061659},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {617-628},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A brain-inspired self-organizing episodic memory model for a memory assistance robot},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluation of mental workload associated with time pressure
in rapid serial visual presentation tasks. <em>TCDS</em>,
<em>14</em>(2), 608–616. (<a
href="https://doi.org/10.1109/TCDS.2021.3061564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing studies related to brain–computer interface (BCI) based on rapid serial visual presentation (RSVP) mainly focused on performance improvement or paradigm expanding. However, less work was reported about evaluating mental workload in RSVP tasks applied in the surveillance field. This article aimed to validate the separability of multilevels of the mental workload associated with time pressure in RSVP tasks, and additionally to explore the influence of mental workload on target detection performance and neural patterns. Time pressure was simulated by setting three levels of presentation rate. Both behavioral and single-trial target detection performance presented a descending trend as an increase of presentation rate. The changes of mental workload mainly affected occipital N2 and P3 components, whose amplitudes presented a significantly negative correlation with $z$ -scored rating scale mental effort (RSME) value and significantly positive correlation with $z$ -scored single-trial hit rate. The average classification accuracy on three levels of mental workload reached to 71.0%. This work implies that the parameter associated with time pressure could affect the mental workload in RSVP tasks, and further influence the neurophysiological system and task performance. Mental workload induced by different presentation rates could be effectively estimated, which provides a potential approach to monitor mental workload in RSVP-based BCI.},
  archive      = {J_TCDS},
  author       = {Weibo Yi and Shuang Qiu and Xinan Fan and Lijian Zhang and Dong Ming},
  doi          = {10.1109/TCDS.2021.3061564},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {608-616},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Evaluation of mental workload associated with time pressure in rapid serial visual presentation tasks},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A portable device for hand rehabilitation with force
cognition: Design, interaction, and experiment. <em>TCDS</em>,
<em>14</em>(2), 599–607. (<a
href="https://doi.org/10.1109/TCDS.2021.3055626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Introducing interactive system into portable robots for hand rehabilitation has always been a crucial topic. Moreover, hand rehabilitation with force cognition can make patients participate actively and improve the rehabilitation effect. In this article, we design a portable robotic device with an interactive system for patients to rehabilitate with force cognition. First, an exoskeleton glove is designed with a compact mechanical structure which is controlled by a real-time feedback system. The portable device allows patients to rehabilitate not only in the hospital. Next, an interactive system, including virtual environment and force cognition, is introduced to detect the hand motion and collision. Finally, clinical tests of our portable device are carried out with nine subjects after tendon injury to show the effective assistance with our device.},
  archive      = {J_TCDS},
  author       = {Lei Yang and Fuhai Zhang and Jingbin Zhu and Yili Fu},
  doi          = {10.1109/TCDS.2021.3055626},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {599-607},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A portable device for hand rehabilitation with force cognition: Design, interaction, and experiment},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-corpus speech emotion recognition based on joint
transfer subspace learning and regression. <em>TCDS</em>,
<em>14</em>(2), 588–598. (<a
href="https://doi.org/10.1109/TCDS.2021.3055524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech emotion recognition has become an attractive research topic due to various emotional states of speech signals in real-life scenarios. Most current speech emotion recognition methods are carried out on a single corpus. However, in practice, the training and testing data often come from different domains, e.g., different corpora. In this case, the model generalizability and recognition performance would decrease greatly due to the domain mismatch. To address this challenging problem, we present a transfer learning method, called joint transfer subspace learning and regression (JTSLR), for cross-corpus speech emotion recognition. Specifically, JTSLR performs transfer subspace learning and regression in a joint framework. First, we learn a latent subspace by introducing a discriminative maximum mean discrepancy (MMD) as the discrepancy metric. Then, we put forward a regression function in this latent subspace to describe the relationships between features and corresponding labels. Moreover, we present a label graph to help transfer knowledge from relevant source data to target data. Finally, we conduct extensive experiments on three popular emotional data sets. The results show that our method can outperform traditional methods and some state-of-the-art transfer learning algorithms for cross-corpus speech emotion recognition tasks.},
  archive      = {J_TCDS},
  author       = {Weijian Zhang and Peng Song and Dongliang Chen and Chao Sheng and Wenjing Zhang},
  doi          = {10.1109/TCDS.2021.3055524},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {588-598},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Cross-corpus speech emotion recognition based on joint transfer subspace learning and regression},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bicriteria velocity minimization approach of self-motion for
redundant robot manipulators with varying-gain recurrent neural network.
<em>TCDS</em>, <em>14</em>(2), 578–587. (<a
href="https://doi.org/10.1109/TCDS.2021.3054999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a varying-gain neural bicriterion velocity minimization self-motion (VGN-BCVM-SM) approach is proposed to solve the self-motion problem for a redundant robot manipulator. First, based on quadratic programming (QP) method and neural dynamic method, the proposed approach is derived in detail. For comparisons, a traditional fixed-parameter neural bicriterion velocity minimization self-motion (FPN-BCVM-SM) approach is also presented. Then, the convergence and robustness of the proposed method is analyzed theoretically. Theoretical analysis shows that the proposed approach has global convergence and can overcome the errors of kinematics measurements. Computer simulations based on a six degrees-of-freedom manipulator demonstrate that the proposed approach can effectively avoid the robot manipulator exceeding the physical limits of joints. Meanwhile, the proposed VGN-BCVM-SM has higher efficiency and accuracy than fixed-parameter approach for solving self-motion problem of a redundant robot manipulator.},
  archive      = {J_TCDS},
  author       = {Xiaohui Ren and Pengchao Zhang and Zhijun Zhang},
  doi          = {10.1109/TCDS.2021.3054999},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {578-587},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Bicriteria velocity minimization approach of self-motion for redundant robot manipulators with varying-gain recurrent neural network},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward emerging cubic-spline patterns with a mobile robotics
swarm system. <em>TCDS</em>, <em>14</em>(2), 565–577. (<a
href="https://doi.org/10.1109/TCDS.2021.3054997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An innovative and flexible approach is introduced to address the challenge of self-organizing a group of mobile robots into cubic-spline-based patterns without any requirement of control points. Besides the self-organization of mobile robots, the approach incorporates a potential field-based control for obstacle/collision avoidance. This will offer more flexibility to swarm robots to efficiently deal with many practical situations, including smoothly avoiding obstacles during movement or exploring and covering areas with complex curved patterns. Essentially, this challenge is approached by proposing a formation control model based on a smoothed particle hydrodynamic estimation technique, which uses special cubic-spline kernel functions applied here to interpolate the density of each robot in the swarm. The moving information is used to weigh the distances to the robot’s neighbors available in its field of view. Then, an artificial physics mesh is finally built among each robot and its three available neighbors having the smallest weighted distances. Significant results toward emerging cubic-spline patterns are shown with a swarm of foot-bot mobile robots simulated in the ARGoS platform. Analysis results with different metrics are also conducted to assess the performance of the model with different swarm sizes and in the presence of sensory noise as well in the presence of partially faulty robots.},
  archive      = {J_TCDS},
  author       = {Belkacem Khaldi and Fouzi Harrou and Foudil Cherif and Ying Sun},
  doi          = {10.1109/TCDS.2021.3054997},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {565-577},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Toward emerging cubic-spline patterns with a mobile robotics swarm system},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Resting-state brain connectivity via multivariate EMD in
mild cognitive impairment. <em>TCDS</em>, <em>14</em>(2), 552–564. (<a
href="https://doi.org/10.1109/TCDS.2021.3054504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mild cognitive impairment (MCI) is regarded as a brain disconnection syndrome. The functional connectivity from resting-state functional magnetic resonance imaging (rs-fMRI) shows that the number and the strength of the connectivity in MCI will decrease. However, many studies have confirmed that the decrease and increase in MCI actually coexist. This article proposes a novel intrinsic-frequency connectivity to study the statistical significance between MCI and normal control (NC), especially stronger positive connectivity on an MCI group mean level. We use a multivariate empirical model decomposition to get intrinsic modal functions (IMFs) and calculate the correlations of the IMFs. In experiments, we use public data to test the proposed coefficient. When some dimension-reduction criteria are adopted, we can find some stronger intrinsic frequency correlation (IFC) in MCI, which cannot be observed in traditional correlations. After further searched, the stronger connectivity is mainly distributed in the regions of MCI functional compensation and abnormal neuron recruitment hypothesis. From an intrinsic frequency spectrum, the reason for MCI’s stronger positive IFC is that NC’s correlation becomes smaller. One of the explanations is that the common respiration and cardiac noises in the IFC are removed so that the positive correlation is weakened.},
  archive      = {J_TCDS},
  author       = {Haifeng Wu and Lingxu Kong and Yu Zeng and Han Bao},
  doi          = {10.1109/TCDS.2021.3054504},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {552-564},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Resting-state brain connectivity via multivariate EMD in mild cognitive impairment},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RCIT: An RSVP-based concealed information test framework
using EEG signals. <em>TCDS</em>, <em>14</em>(2), 541–551. (<a
href="https://doi.org/10.1109/TCDS.2021.3053455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concealed information detection has a strong connection with human cognition. Existing deception detection approaches, such as polygraph testing usually exploit psychophysiological changes. However, they often suffer from low accuracy and attacking by training. Compared with conventional physiological responses, electroencephalogram (EEG) directly reflects activity responses in the brain, thus can be potentially more accurate in deception detection. In this article, we propose an EEG-based concealed information detection framework. In this framework, different test trials are designed to evoke effective brain responses corresponding to concealed information while keeping the subject focused. Meanwhile, we employ rapid serial visual presentation (RSVP) for deception detection. Our framework presents the stimuli on the fringe of awareness, making it not easy to take the countermeasures. Furthermore, we develop a method based on neural network to detect brain responses. Experimental results with 10 subjects show that, our approach can effectively detect concealed information with an accuracy of 87.13%.},
  archive      = {J_TCDS},
  author       = {Hanwen Wang and Yu Qi and Hang Yu and Yueming Wang and Cong Liu and Guoping Hu and Gang Pan},
  doi          = {10.1109/TCDS.2021.3053455},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {541-551},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {RCIT: An RSVP-based concealed information test framework using EEG signals},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural correlates of single-task versus cognitive-motor
dual-task training. <em>TCDS</em>, <em>14</em>(2), 532–540. (<a
href="https://doi.org/10.1109/TCDS.2021.3053050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-task and cognitive-motor dual-task training (CMDT) has been extensively studied for the post-stroke function rehabilitation. However, little is known about the difference between the brain activities when these two types of tasks are being implemented. Therefore, neural correlates of single-cognitive/motor-task training (SCT/SMT) versus CMDT are investigated in this study. Specifically, a pure mathematical problem-solving task and a pure cycling task were designed for the SCT and SMT paradigms, respectively. For the CMDT, subjects should perform these two tasks simultaneously. Electroencephalography signals acquired during performing these two types of training are used to analyze the brain dynamic properties by event-related potential and event-related (de-)synchronization analyses. It can be seen from the results that compared with the SCT, theta event-related synchronization can be increased significantly by the CMDT (frontal theta: $p$ -value $= 5.44e-05 &amp;lt; 0.0001$ and temporal theta: $p$ -value $= 9.18e-06 &amp;lt; 0.0001$ ) and, meanwhile, the mu and central beta event-related desynchronization induced by the CMDT is significantly lower than that induced by the SMT (mu: p-value $= 2.28e-06 &amp;lt; 0.0001$ and central beta: p-value $= 4.37e-06 &amp;lt; 0.0001$ ). Therefore, it is proved in this study that the SMT can induce higher motor neural engagement than the CMDT and, meanwhile, compared with the SCT, higher cognitive neural activity can be induced by the CMDT, which can be explained by that the multitask coordination induces more neural activities. In addition, it is found that more attention has been paid on the relatively difficult task during the dual-task training. Therefore, the difficulty level of each task in the CMDT paradigm can be designed subject specific to implement the personalized training.},
  archive      = {J_TCDS},
  author       = {Jiaxing Wang and Weiqun Wang and Shixin Ren and Weiguo Shi and Zeng-Guang Hou},
  doi          = {10.1109/TCDS.2021.3053050},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {532-540},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Neural correlates of single-task versus cognitive-motor dual-task training},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A dissemination model based on psychological theories in
complex social networks. <em>TCDS</em>, <em>14</em>(2), 519–531. (<a
href="https://doi.org/10.1109/TCDS.2021.3052824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information spread on social media has been extensively studied through both model-driven theoretical research and data-driven case studies. Recent empirical studies have analyzed the differences and complexity of information dissemination, but theoretical explanations of its characteristics from a modeling perspective are underresearched. To capture the complex patterns of the information dissemination mechanism, we propose a resistant linear threshold (RLT) dissemination model based on psychological theories and empirical findings. In this article, we validate the RLT model on three types of networks and then quantify and compare the dissemination characteristics of the simulation results with those from the empirical results. In addition, we examine the factors affecting dissemination. Finally, we perform two case studies of the 2019 novel Corona Virus Disease (COVID-19)-related information dissemination. The dissemination characteristics derived by the simulations are consistent with the empirical research. These results demonstrate that the RLT model is able to capture the patterns of information dissemination on social media and thus provide model-driven insights into the interpretation of public opinion, rumor control, and marketing strategies on social media.},
  archive      = {J_TCDS},
  author       = {Tianyi Luo and Zhidong Cao and Daniel Zeng and Qingpeng Zhang},
  doi          = {10.1109/TCDS.2021.3052824},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {519-531},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A dissemination model based on psychological theories in complex social networks},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiscale brain-like neural network for saliency prediction
on omnidirectional images. <em>TCDS</em>, <em>14</em>(2), 507–518. (<a
href="https://doi.org/10.1109/TCDS.2021.3052526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current top-performing saliency prediction methods of omnidirectional images (ODIs) depend on deep feedforward convolutional neural networks (CNNs), benefiting from their powerful multiscale representation ability. Although these methods adopt deep feedforward CNNs to achieve superb performance in saliency prediction task, they have the following limitations: 1) these deep feedforward CNNs are difficult to map to ventral stream structure of the brain visual system due to their vast number of layers and missing biologically important connections, such as recurrence and 2) most deep feedforward CNNs represent the multiscale features in a layerwise manner. To tackle these issues, models that could learn multiscale features yet share the similarities with human brain are needed. In this article, we propose a novel multiscale brain-like network (MBN) model to predict saliency of head fixations on ODIs. Specifically, our proposed model consists of two major modules: 1) a brain-like CORnet-S module and 2) a multiscale feature extraction module. The CORnet-S module is a lightweight backbone network with four anatomically mapped areas (V1, V2, V4, and IT) and it can simulate the visual processing mechanism of ventral visual stream in the human brain. The multiscale feature extraction module is inspired by the multiscale brain structure, which represents multiscale features at a granular level and increases the range of receptive fields for each network layer. Extensive experiments and ablation studies conducted on two major benchmarks demonstrate the superiority of the proposed MBN model over the state-of-the-art methods.},
  archive      = {J_TCDS},
  author       = {Dandan Zhu and Yongqing Chen and Defang Zhao and Yucheng Zhu and Qiangqiang Zhou and Guangtao Zhai and Xiaokang Yang},
  doi          = {10.1109/TCDS.2021.3052526},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {507-518},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multiscale brain-like neural network for saliency prediction on omnidirectional images},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cogno-vest: A torso-worn, force display to experimentally
induce specific hallucinations and related bodily sensations.
<em>TCDS</em>, <em>14</em>(2), 497–506. (<a
href="https://doi.org/10.1109/TCDS.2021.3051395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in virtual reality and robotic technologies have allowed researchers to explore the mechanisms underlying bodily aspects of self-consciousness which are largely attributed to the multisensory and sensorimotor processing of bodily signals (bodily self-consciousness, BSC). One key contribution to BSC, that is, currently poorly addressed due to the lack of a wearable solution, concerns realistic collision sensations on the torso. Here, we introduce and validate a proof-of-concept prototype of the torso-worn force display, the Cogno-vest, that provides mechanical touch on the user’s back in a sensorimotor perception experiment. In a first empirical study, we characterized human finger poking ( $N=28$ ). In order to match these poking characteristics and meet the wearability criteria, we used bidirectional, push–pull solenoids as a force actuator in the Cogno-vest. Subsequently, and based on an iterative design procedure, a body-conforming, unisex, torso-worn force display was prototyped. Finally, we conducted a behavioral study that investigated BSC in 25 healthy participants by introducing conflicting sensorimotor signals between their hands and torso (back). Using the final reiteration of the Cogno-vest, we successfully replicated previous findings on illusory states of BSC, characterized by presence hallucinations (PH) and passivity symptoms, and achieved higher illusion ratings compared to static conditions used in prior studies.},
  archive      = {J_TCDS},
  author       = {Atena Fadaei Jouybari and Kenny Jeanmonod and Olivier A. Kannape and Jevita Potheegadoo and Hannes Bleuler and Masayuki Hara and Olaf Blanke},
  doi          = {10.1109/TCDS.2021.3051395},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {497-506},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Cogno-vest: A torso-worn, force display to experimentally induce specific hallucinations and related bodily sensations},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Augmented memory replay in reinforcement learning with
continuous control. <em>TCDS</em>, <em>14</em>(2), 485–496. (<a
href="https://doi.org/10.1109/TCDS.2021.3050723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online reinforcement learning agents are currently able to process an increasing amount of data by converting it into a higher order value functions. This expansion of the information collected from the environment increases the agent’s state space enabling it to scale up to more complex problems but also increases the risk of forgetting by learning on redundant or conflicting data. To improve the approximation of a large amount of data, a random mini-batch of the past experiences that are stored in the replay memory buffer is often replayed at each learning step. The proposed work takes inspiration from a biological mechanism which acts as a protective layer of higher cognitive functions found in mammalian brain: active memory consolidation mitigates the effect of forgetting previous memories by dynamically processing the new ones. Similar dynamics are implemented by the proposed augmented memory replay or AMR algorithm. The architecture of AMR , based on a simple artificial neural network is able to provide an augmentation policy which modifies each of the agents experiences by augmenting their relevance prior to storing them in the replay memory. The function approximator of AMR is evolved using genetic algorithm in order to obtain the specific augmentation policy function that yields the best performance of a learning agent in a specific environment given by its received cumulative reward. Experimental results show that an evolved AMR augmentation function capable of increasing the significance of the specific memories is able to further increase the stability and convergence speed of the learning algorithms dealing with the complexity of continuous action domains.},
  archive      = {J_TCDS},
  author       = {Mirza Ramicic and Andrea Bonarini},
  doi          = {10.1109/TCDS.2021.3050723},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {485-496},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Augmented memory replay in reinforcement learning with continuous control},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spontaneous temporal grouping neural network for long-term
memory modeling. <em>TCDS</em>, <em>14</em>(2), 472–484. (<a
href="https://doi.org/10.1109/TCDS.2021.3050759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capacity of long-term memory is an important issue in sequence learning, but it remains challenging for the problems of vanishing gradients or out-of-order dependencies. Inspired by human memory, in which long-term memory is broken into fragments and then can be recalled at appropriate times, we propose a neural network via spontaneous temporal grouping in this article. In the architecture, the segmented layer is used for spontaneous sequence segmentation under guidance of the reset gates which are driven to be sparse in the training process; the cascading layer is used to collect information from the temporal groups, where a filtered long short-term memory with chrono-initialization is proposed to alleviate the gradient vanishing phenomenon, and random skip connections are adopted to capture complex dependencies among the groups. Furthermore, the advantage of our neural architecture in long-term memory is demonstrated via a new measurement method. In experiments, we compare the performance with multiple models on several algorithmic or classification tasks, and both of the sequences with fixed lengths like the MNISTs and with varying lengths like the speech utterances are adopted. The results in different criteria have demonstrated the superiority of our proposed neural network.},
  archive      = {J_TCDS},
  author       = {Dongjing Shan and Xiongwei Zhang and Chao Zhang},
  doi          = {10.1109/TCDS.2021.3050759},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {472-484},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Spontaneous temporal grouping neural network for long-term memory modeling},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An empirical study of active inference on a humanoid robot.
<em>TCDS</em>, <em>14</em>(2), 462–471. (<a
href="https://doi.org/10.1109/TCDS.2021.3049907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the biggest challenges in robotics is interacting under uncertainty. Unlike robots, humans learn, adapt, and perceive their body as a unity when interacting with the world. Here, we investigate the suitability of active inference , a computational model proposed for the brain and governed by the free-energy principle, for robotic body perception and action in a nonsimulated environment. We designed and deployed the algorithm on the humanoid iCub showing how our proposed model enabled the robot to have adaptive body perception and to perform robust upper body reaching and head object tracking behaviors even under high levels of sensor noise and discrepancies between the model and the real robot. Estimation and control are formalized as an inference problem where the body posterior state distribution is approximated by means of the variational free-energy bound, yielding to a minimization of the prediction error. Besides, our study forecasts reactive actions in the presence of sensorimotor conflicts, a mechanism that may be relevant in human body adaptation to uncertain situations.},
  archive      = {J_TCDS},
  author       = {Guillermo Oliver and Pablo Lanillos and Gordon Cheng},
  doi          = {10.1109/TCDS.2021.3049907},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {462-471},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An empirical study of active inference on a humanoid robot},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). In situ learning in hardware compatible multilayer
memristive spiking neural network. <em>TCDS</em>, <em>14</em>(2),
448–461. (<a href="https://doi.org/10.1109/TCDS.2021.3049487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most promising methods in the next generation of neuromorphic systems, memristor-based spiking neural networks (SNNs) show great advantages in terms of power efficiency, integration density, and biological plausibility. However, because of the nondifferentiability of discrete spikes, it is difficult to train SNNs with gradient descent and error backpropagation online. In this article, we propose an improved training algorithm for multilayer memristive SNN (MSNN) with three methods spontaneously, supporting in situ learning in hardware: 1) temporal order encoding is applied to generate different pulse trains in neurons; 2) a simplified homeostasis is realized by the activation state and refractory period to regulate hidden neurons spontaneously; and 3) spiking-timing-dependent plasticity (STDP) in memristive synapses is adopted to update weights in situ . Correspondingly, we provide a circuitry example and verify it in LTSPICE. Then the MSNN is benchmarked with the MNIST data set and analyzed with visualization methods, showing better recognition accuracy (95.15%) than existing SNNs with comparable scales and bio-inspired learning rules. We also consider some nonideal effects in memristor crossbar array and peripheral circuits. Evaluation results show that the proposed MSNN is robust to finite resolution, circuit noise and writing noise; and larger network scale will help the MSNN alleviate the negative impacts of other nonideal factors, including yield and device-to-device variation. Moreover, the energy efficiency of a MSNN system is estimated to achieve 7.6TOPS/W, showing great potential in low-power applications.},
  archive      = {J_TCDS},
  author       = {Jiwei Li and Hui Xu and Sheng-Yang Sun and Nan Li and Qingjiang Li and Zhiwei Li and Haijun Liu},
  doi          = {10.1109/TCDS.2021.3049487},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {448-461},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {In situ learning in hardware compatible multilayer memristive spiking neural network},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Holistically associated transductive zero-shot learning.
<em>TCDS</em>, <em>14</em>(2), 437–447. (<a
href="https://doi.org/10.1109/TCDS.2021.3049274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of visual media categories, zero-shot learning (ZSL) aims to transfer the knowledge obtained from the seen classes to the unseen classes for recognizing novel instances. However, there is a domain gap between the seen and the unseen classes, and simply matching the unseen instances using nearest neighbor searching in the embedding space cannot bridge this gap effectively. In this article, we propose a holistically associated model to overcome this obstacle. In particular, the proposed model is designed to combat two fundamental problems of ZSL: 1) the representation learning and 2) label assignment of the unseen classes. The first problem is addressed by proposing an affinity propagation network, which considers holistic pairwise connections of all classes for producing exemplar features of the unseen samples. We cope with the second issue by proposing a progressive clustering module. It iteratively refines unseen clusters so that holistic unseen instance features can be used for a reliable classwise label assignment. Thanks to the precise exemplar features and classwise label assignment, our model eliminates the domain gap effectively. We extensively evaluate the proposed model on five human action and image data sets, i.e., Olympics Sports, HMDB51, UCF101, Animals with Attributes 2, and SUN. The experimental results show that the proposed model outperforms state-of-the-art methods on these substantially different data sets.},
  archive      = {J_TCDS},
  author       = {Yangyang Xu and Xuemiao Xu and Guoqiang Han and Shengfeng He},
  doi          = {10.1109/TCDS.2021.3049274},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {437-447},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Holistically associated transductive zero-shot learning},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Motor-cortex-like recurrent neural network and multitask
learning for the control of musculoskeletal systems. <em>TCDS</em>,
<em>14</em>(2), 424–436. (<a
href="https://doi.org/10.1109/TCDS.2020.3045574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The musculoskeletal robot is a promising direction of the next-generation robots. However, current control methods of musculoskeletal robots lack multitask learning ability, great generalization, and biological plausibility. In this article, a motor-cortex-like recurrent neural network (RNN) and a reward-modulated multitask learning method are proposed. First, inspired by the dynamic system hypothesis of motor cortex, the RNN is introduced to transform movement targets into muscle excitations. The condition that makes an RNN generate motor-cortex-like consistent population response is investigated. Second, a reward-modulated multitask learning method of such an RNN is proposed. In the experiments, the control of a musculoskeletal system is realized with multitask learning ability, great generalization, and robustness for noises. Furthermore, the RNN and muscle excitations demonstrate motor-cortex-like consistent population response and human-like muscle synergies, respectively. Therefore, the proposed method has better performance and biological plausibility, and verifies the neural mechanisms in the robotic research.},
  archive      = {J_TCDS},
  author       = {Jiahao Chen and Hong Qiao},
  doi          = {10.1109/TCDS.2020.3045574},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {424-436},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Motor-cortex-like recurrent neural network and multitask learning for the control of musculoskeletal systems},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning multipart attention neural network for zero-shot
classification. <em>TCDS</em>, <em>14</em>(2), 414–423. (<a
href="https://doi.org/10.1109/TCDS.2020.3044313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) models typically learn a cross-modal mapping between the visual feature space and the semantic embedding space. Despite promising performance achieved by existing methods, they usually take visual features from the whole image as the main proposed inputs, while pay little attention to image regions which are relevant to human’s visual response to the whole image. In this article, we propose a neural network-based ZSL model which incorporates an attention mechanism to discover the discriminative parts for each image. The proposed model allows us to automatically generate attention maps for visual parts, which provides a flexible way of encoding the salient visual aspects to distinguish the categories. Moreover, we introduce a simple yet effective objective function to exploit the pairwise label information between images and classes, resulting in substantial performance improvement. When multiple semantic spaces are available, a multiple-attention scheme is provided to fuse different semantic spaces, which helps to achieve further improvement in performance. On the widely used CUB-2010-2011 data set for fine-grained image classification, we demonstrate the advantages of using attention mechanism and semantic parts in our model for ZSL. Comprehensive experimental results show that our proposed approach achieves superior performance than the state-of-the-art methods.},
  archive      = {J_TCDS},
  author       = {Min Meng and Jie Wei and Jigang Wu},
  doi          = {10.1109/TCDS.2020.3044313},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {414-423},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Learning multipart attention neural network for zero-shot classification},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Imitation learning-based algorithm for drone cinematography
system. <em>TCDS</em>, <em>14</em>(2), 403–413. (<a
href="https://doi.org/10.1109/TCDS.2020.3043441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Viewpoint selection for capturing human motion is an important task in autonomous aerial videography, animation, and virtual 3-D environments. Existing methods rely on heuristics for selecting the “best” viewpoint, which requires human effort to summarize and integrate viewpoint selection rules into a visual servo system to control a camera. In this work, we propose an integrated aerial filming system for autonomously capturing cinematic shots of action scenes on the basis of a set of demonstrations given for imitation. Our model, which is built on the basis of the deep deterministic policy gradient, takes a sequence of a subject’s skeleton and the camera pose as input and outputs the camera motion with an optimal viewpoint related to the subject. In addition, we design a spatial attention network to selectively focus on the discriminative joints of the skeleton within each frame. Given the demonstrations with human motions, our framework learns to predict the next best viewpoint by imitating the demonstrations for viewing the motion of the subject. Extensive experimental results in simulated and real outdoor environments demonstrate that our method can successfully mimic the viewpoint selection strategy and capture a more accurate viewpoint than state-of-the-art autonomous cinematography methods.},
  archive      = {J_TCDS},
  author       = {Yuanjie Dang and Chong Huang and Peng Chen and Ronghua Liang and Xin Yang and Kwang-Ting Cheng},
  doi          = {10.1109/TCDS.2020.3043441},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {403-413},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Imitation learning-based algorithm for drone cinematography system},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A compressed sensing network for acquiring human pressure
information. <em>TCDS</em>, <em>14</em>(2), 388–402. (<a
href="https://doi.org/10.1109/TCDS.2020.3041422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, sparse autoencoder in deep learning is integrated into the compressed sensing (CS) theory, and a reconstruction algorithm is designed based on the biological mechanism of human brain synaptic connections. The compressive sampling process is modeled as a neural network model. Then a biological mechanism-inspired stacked long short-term memory (LSTM) network model is proposed as a reconstruction algorithm of CS theory. Consequently, a CS network (ComsensNet) model is introduced, by integrating the compressive sampling process and reconstruction algorithm. ComsensNet can provide a bridge between sparse autoencoder in deep learning, synapses in human brain neurons and the CS theory. A deep neural network is designed based on the synaptic biological mechanism of human brain neurons, and then combine with the theory of CS. The effectiveness of ComsensNet is investigated by using acquired pressure data from the human body model. The experimental results demonstrate that the biological mechanism-inspired stacked LSTM network in ComsensNet can improve the reconstruction accuracy compared to other reconstruction algorithms.},
  archive      = {J_TCDS},
  author       = {Tao Han and Kuangrong Hao and Xue-Song Tang and Xin Cai and Tong Wang and Xiaoyan Liu},
  doi          = {10.1109/TCDS.2020.3041422},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {388-402},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A compressed sensing network for acquiring human pressure information},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A matrix determinant feature extraction approach for
decoding motor and mental imagery EEG in subject-specific tasks.
<em>TCDS</em>, <em>14</em>(2), 375–387. (<a
href="https://doi.org/10.1109/TCDS.2020.3040438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel matrix determinant feature extraction approach for efficient classification of motor and mental imagery activities from electroencephalography (EEG) signals. First, the multiscale principal component analysis was utilized to obtain clean EEG signals. Second, denoised data were sequentially arranged to form a square matrix of different orders (e.g.,10, 13, 16, and 20) and determinant was computed for each order matrix. Finally, the extracted matrix determinant features were provided to several machine learning and neural network classification models for classification. All experiments were carried out using a 10-fold cross-validation approach on three publicly accessible data sets: 1) data set IV-a; 2) data set IV-b; and 3) data set V of BCI competition III. Also, this study designs a computerized automatic detection of motor and mental imagery graphical user interface that can assist physicians/experts to efficiently analyses motor and mental imagery data. The experimental results reveal that the highest average classification accuracy of 99.55% (for data set IV-a), 99.52% (for data set IV-b), and 91.80% (for data set V) was obtained for motor and mental imagery, respectively, with 20-order matrix determinant using a feedforward neural network classifier. The experimental results suggest that the proposed framework provides a robust biomarker with the least computational complexity for the development of automated brain–computer interfaces.},
  archive      = {J_TCDS},
  author       = {Muhammad Tariq Sadiq and Xiaojun Yu and Zhaohui Yuan and Muhammad Zulkifal Aziz and Siuly Siuly and Weiping Ding},
  doi          = {10.1109/TCDS.2020.3040438},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {375-387},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A matrix determinant feature extraction approach for decoding motor and mental imagery EEG in subject-specific tasks},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). To move or not to move: Development of fine-tuning of object
motion in haptic exploration. <em>TCDS</em>, <em>14</em>(2), 366–374.
(<a href="https://doi.org/10.1109/TCDS.2020.3034014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing objects through touch is a complex task based on integrating information coming from multiple senses and motor commands guiding the exploratory motions. To gain insight into the development of exploratory strategies in children, in this study, we addressed the question: how does exploration change when the stimulus becomes freely movable rather than fixed? We tested whether the possibility to move the object ushers in a strategic advantage, reducing the time and the number of touches necessary. We analyzed how school-aged children explore iCube, which is a sensorized cube measuring its orientation in space and contacts location. We tasked participants with finding specific cube faces; they could only touch the static cube, move and touch it, or move, touch, and look at it. Visuo-haptic performances were adult-like at seven years of age, whereas haptic exploration was less effective until nine years. The fine-tuning of object movements as a function of task constraints, e.g., the availability of the vision or blind haptic task, increased significantly with age. Shedding light on how different factors shape haptic exploration could help researchers in the pursuit of detecting the occurrence of abnormal exploratory behaviors early on during the development, providing a novel approach to detecting perceptual problems.},
  archive      = {J_TCDS},
  author       = {Alessandra Sciutti and Giulio Sandini},
  doi          = {10.1109/TCDS.2020.3034014},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {366-374},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {To move or not to move: Development of fine-tuning of object motion in haptic exploration},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning in EEG: Advance of the last ten-year critical
period. <em>TCDS</em>, <em>14</em>(2), 348–365. (<a
href="https://doi.org/10.1109/TCDS.2021.3079712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has achieved excellent performance in a wide range of domains, especially in speech recognition and computer vision. Relatively less work has been done for electroencephalogram (EEG), but there is still significant progress attained in the last decade. Due to the lack of a comprehensive and topic widely covered survey for deep learning in EEG, we attempt to summarize recent progress to provide an overview, as well as perspectives for future developments. We first briefly mention the artifacts removal for EEG signal and then introduce deep learning models that have been utilized in EEG processing and classification. Subsequently, the applications of deep learning in EEG are reviewed by categorizing them into groups, such as brain–computer interface, disease detection, and emotion recognition. They are followed by the discussion, in which the pros and cons of deep learning are presented and future directions and challenges for deep learning in EEG are proposed. We hope that this article could serve as a summary of past work for deep learning in EEG and the beginning of further developments and achievements of EEG studies based on deep learning.},
  archive      = {J_TCDS},
  author       = {Shu Gong and Kaibo Xing and Andrzej Cichocki and Junhua Li},
  doi          = {10.1109/TCDS.2021.3079712},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {348-365},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep learning in EEG: Advance of the last ten-year critical period},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The why, what, and how of artificial general intelligence
chip development. <em>TCDS</em>, <em>14</em>(2), 333–347. (<a
href="https://doi.org/10.1109/TCDS.2021.3069871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The AI chips increasingly focus on implementing neural computing at low power and cost. The intelligent sensing, automation, and edge computing applications have been the market drivers for AI chips. Increasingly, the generalisation, performance, robustness, and scalability of the AI chip solutions are compared with human-like intelligence abilities. Such a requirement to transit from application-specific to general intelligence AI chip must consider several factors. This article provides an overview of this cross-disciplinary field of study, elaborating on the generalisation of intelligence as understood in building artificial general intelligence (AGI) systems. This work presents a listing of emerging AI chip technologies, classification of edge AI implementations, and the funnel design flow for AGI chip development. Finally, the design consideration required for building an AGI chip is listed along with the methods for testing and validating it.},
  archive      = {J_TCDS},
  author       = {Alex P. James},
  doi          = {10.1109/TCDS.2021.3069871},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {333-347},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {The why, what, and how of artificial general intelligence chip development},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Vision-based gaze estimation: A review. <em>TCDS</em>,
<em>14</em>(2), 316–332. (<a
href="https://doi.org/10.1109/TCDS.2021.3066465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye gaze is an important natural behavior in social interaction as it delivers complex exchanges between observer and observed, by building up the geometric constraints and relation of the exchanges. These interperson exchanges can be modeled based on gaze direction estimated using computer vision. Despite significant progresses in vision-based gaze estimation in last 10 years, it is still nontrivial since the accuracy of gaze estimation is significantly affected by such intrinsic factors as head pose variance, individual bias between optical axis and visual axis, eye blink, occlusion and image blur, degrade gaze features, lead to inaccurate gaze-involved human social interaction analysis. This article aims to review and discuss existing methods addressing above-mentioned problems, gaze involved applications and data sets against the state of the arts in vision-based gaze estimation. It also points out future research directions and challenges of gaze estimation in terms of metalearning, causal inference, disentangled representation, and social gaze behavior for unconstrained gaze estimation.},
  archive      = {J_TCDS},
  author       = {Xinming Wang and Jianhua Zhang and Hanlin Zhang and Shuwen Zhao and Honghai Liu},
  doi          = {10.1109/TCDS.2021.3066465},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {316-332},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Vision-based gaze estimation: A review},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time perception: A review on psychological, computational,
and robotic models. <em>TCDS</em>, <em>14</em>(2), 301–315. (<a
href="https://doi.org/10.1109/TCDS.2021.3059045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Animals exploit time to survive in the world. Temporal information is required for higher level cognitive abilities, such as planning, decision making, communication, and effective cooperation. Since time is an inseparable part of cognition, there is a growing interest in the artificial intelligence approach to subjective time, which has a possibility of advancing the field. The current survey study aims to provide researchers with an interdisciplinary perspective on time perception. First, we introduce a brief background from the psychology and neuroscience literature, covering the characteristics and models of time perception and related abilities. Second, we summarize the emergent computational and robotic models of time perception. A general overview to the literature reveals that a substantial amount of timing models are based on a dedicated time processing like the emergence of a clock-like mechanism from the neural network dynamics and reveals a relationship between the embodiment and time perception. We also notice that most models of timing are developed for either sensory timing (i.e., ability to assess an interval) or motor timing (i.e., ability to reproduce an interval). The number of timing models capable of retrospective timing, which is the ability to track time without paying attention, is insufficient. In this light, we discuss the possible research directions to promote the interdisciplinary collaboration in the field of time perception.},
  archive      = {J_TCDS},
  author       = {Hamit Basgol and Inci Ayhan and Emre Ugur},
  doi          = {10.1109/TCDS.2021.3059045},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {301-315},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Time perception: A review on psychological, computational, and robotic models},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Philosophical specification of empathetic ethical artificial
intelligence. <em>TCDS</em>, <em>14</em>(2), 292–300. (<a
href="https://doi.org/10.1109/TCDS.2021.3099945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to construct an ethical artificial intelligence (AI) two complex problems must be overcome. First, humans do not consistently agree on what is or is not ethical. Second, contemporary AI and machine learning methods tend to be blunt instruments which either search for solutions within the bounds of predefined rules or mimic behavior. An ethical AI must be capable of inferring unspoken rules, interpreting nuance and context, possess and be able to infer intent, and explain not just its actions but its intent. Using enactivism, semiotics, perceptual symbol systems, and symbol emergence, we specify an agent that learns not just arbitrary relations between signs but their meaning in terms of the perceptual states of its sensorimotor system. Subsequently it can learn what is meant by a sentence and infer the intent of others in terms of its own experiences. It has malleable intent because the meaning of symbols changes as it learns, and its intent is represented symbolically as a goal. As such it may learn a concept of what is most likely to be considered ethical by the majority within a population of humans, which may then be used as a goal. The meaning of abstract symbols is expressed using perceptual symbols of raw, multimodal sensorimotor stimuli as the weakest (consistent with Ockham’s Razor) necessary and sufficient concept, an intensional definition learned from an ostensive definition, from which the extensional definition or category of all ethical decisions may be obtained. Because these abstract symbols are the same for both situation and response, the same symbol is used when either performing or observing an action. This is akin to mirror neurons in the human brain. Mirror symbols may allow the agent to empathize, because its own experiences are associated with the symbol, which is also associated with the observation of another agent experiencing something that symbol represents.},
  archive      = {J_TCDS},
  author       = {Michael Timothy Bennett and Yoshihiro Maruyama},
  doi          = {10.1109/TCDS.2021.3099945},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {292-300},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Philosophical specification of empathetic ethical artificial intelligence},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). In search of a neural model for serial order: A brain theory
for memory development and higher level cognition. <em>TCDS</em>,
<em>14</em>(2), 279–291. (<a
href="https://doi.org/10.1109/TCDS.2022.3168046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to keep trace of information and grow up, the infant brain has to resolve the problem about where old information is located and how to index new ones. We propose that the immature prefrontal cortex (PFC) uses its primary functionality of detecting hierarchical patterns in temporal signals as a second feature to organize the spatial ordering of the cortical networks in the developing brain itself. Our hypothesis is that the PFC detects the hierarchical structure in temporal sequences in the shape of ordinal patterns and uses them to index information hierarchically in different parts of the brain. Henceforth, we propose that this mechanism for detecting ordinal patterns participates also in the hierarchical organization of the brain during development; i.e., the bootstrapping of the connectome. By doing so, it gives the tools to the language-ready brain for manipulating abstract knowledge and for planning temporally ordered information; i.e., the emergence of causality and symbolic thinking. In this position paper, we will review several neural models from the literature that support serial ordering and propose an original one. We will confront then our ideas with evidence from developmental, behavioral, and brain results.},
  archive      = {J_TCDS},
  author       = {Alexandre Pitti and Mathias Quoy and Catherine Lavandier and Sofiane Boucenna and Wassim Swaileh and Claudio Weidmann},
  doi          = {10.1109/TCDS.2022.3168046},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {279-291},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {In search of a neural model for serial order: A brain theory for memory development and higher level cognition},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A sensorimotor perspective on contrastive multiview visual
representation learning. <em>TCDS</em>, <em>14</em>(2), 269–278. (<a
href="https://doi.org/10.1109/TCDS.2021.3086267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The contrastive multiview visual representation learning (CMVRL) framework has recently gained a lot of traction in the unsupervised representation learning literature. Combining a simple data augmentation strategy and a contrastive learning objective, it has been able to generate representations that compare favorably to their supervised counterparts on common downstream visual tasks. The theoretical understanding of this empirical success is currently an active area of research. In this article, we propose a sensorimotor perspective on the various components of the framework. We show how it can be interpreted as building representations that geometrically embed the stable semantic content that a situated agent experiences on short spatiotemporal scales when actively exploring its environment. We also discuss the relevance of the approach in light of contemporary active, dynamical, and hierarchical theories of perception. Finally, we extrapolate this sensorimotor perspective to outline promising future research directions that could push the state of the art further and help better understand how an autonomous agent could develop useful visual representations in an unsupervised fashion.},
  archive      = {J_TCDS},
  author       = {Alban Laflaquière},
  doi          = {10.1109/TCDS.2021.3086267},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {269-278},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A sensorimotor perspective on contrastive multiview visual representation learning},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A biologically inspired computational model of time
perception. <em>TCDS</em>, <em>14</em>(2), 258–268. (<a
href="https://doi.org/10.1109/TCDS.2021.3120301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time perception—how humans and animals perceive the passage of time—forms the basis for important cognitive skills, such as decision making, planning, and communication. In this work, we propose a framework for examining the mechanisms responsible for time perception. We first model neural time perception as a combination of two known timing sources: internal neuronal mechanisms and external (environmental) stimuli, and design a decision-making framework to replicate them. We then implement this framework in a simulated robot. We measure the robot’s success on a temporal discrimination task originally performed by mice to evaluate their capacity to exploit temporal knowledge. We conclude that the robot is able to perceive time similarly to animals when it comes to their intrinsic mechanisms of interpreting time and performing time-aware actions. Next, by analyzing the behavior of agents equipped with the framework, we propose an estimator to infer characteristics of the timing mechanisms intrinsic to the agents. In particular, we show that from their empirical action probability distribution, we are able to estimate parameters used for perceiving time. Overall, our work shows promising results when it comes to drawing conclusions regarding some of the characteristics present in biological timing mechanisms.},
  archive      = {J_TCDS},
  author       = {Inês Lourenço and Robert Mattila and Rodrigo Ventura and Bo Wahlberg},
  doi          = {10.1109/TCDS.2021.3120301},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {258-268},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A biologically inspired computational model of time perception},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on emerging topics on development and
learning. <em>TCDS</em>, <em>14</em>(2), 255–257. (<a
href="https://doi.org/10.1109/TCDS.2022.3165964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special issue includes state-of-the-art research on emerging topics on development and learning in natural and artificial systems. In addition to new submissions, the special issue includes extensions of the paper awarded the Best Paper Award at ICDL-EpiRob 2020—the 10th Joint IEEE Conference on Development and Learning and Epigenetic Robotics 2020 ( https://cdstc.gitlab.io/icdl-2020/ ).},
  archive      = {J_TCDS},
  author       = {María-José Escobar and Nicolás Navarro-Guerrero and Javier Ruiz-Del-Solar and Giulio Sandini},
  doi          = {10.1109/TCDS.2022.3165964},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {255-257},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Special issue on emerging topics on development and learning},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). IEEE computational intelligence society. <em>TCDS</em>,
<em>14</em>(1), C3. (<a
href="https://doi.org/10.1109/TCDS.2022.3154571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {These instructions give guidelines for preparing papers for this publication. Presents information for authors publishing in this journal.},
  archive      = {J_TCDS},
  doi          = {10.1109/TCDS.2022.3154571},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {C3},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {IEEE computational intelligence society},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trear: Transformer-based RGB-d egocentric action
recognition. <em>TCDS</em>, <em>14</em>(1), 246–252. (<a
href="https://doi.org/10.1109/TCDS.2020.3048883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a transformer-based RGB-D egocentric action recognition framework, called Trear. It consists of two modules: 1) interframe attention encoder and 2) mutual-attentional fusion block. Instead of using optical flow or recurrent units, we adopt a self-attention mechanism to model the temporal structure of the data from different modalities. Input frames are cropped randomly to mitigate the effect of the data redundancy. Features from each modality are interacted through the proposed fusion block and combined through a simple yet effective fusion operation to produce a joint RGB-D representation. Empirical experiments on two large egocentric RGB-D data sets: 1) THU-READ and 2) first-person hand action, and one small data set, wearable computer vision systems, have shown that the proposed method outperforms the state-of-the-art results by a large margin.},
  archive      = {J_TCDS},
  author       = {Xiangyu Li and Yonghong Hou and Pichao Wang and Zhimin Gao and Mingliang Xu and Wanqing Li},
  doi          = {10.1109/TCDS.2020.3048883},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {246-252},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Trear: Transformer-based RGB-D egocentric action recognition},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EfficientAutoGAN: Predicting the rewards in
reinforcement-based neural architecture search for generative
adversarial networks. <em>TCDS</em>, <em>14</em>(1), 234–245. (<a
href="https://doi.org/10.1109/TCDS.2020.3040796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is inspired by human’s memory and recognition process to improve neural architecture search (NAS), which has shown novelty and significance in the design of generative adversarial networks (GANs), but the extremely enormous time consumption for searching GAN architectures based on reinforcement learning (RL) limits its applicability to a great extent. The main reason behind the challenge is that, the performance evaluation of subnetworks during the search process takes too much time. To solve this problem, we propose a new algorithm, EfficientAutoGAN, in which a graph convolution network (GCN) predictor is introduced to predict the performance of subnetworks instead of formally assessing or evaluating them. Experiments show that EfficientAutoGAN saves nearly half of the search time and at the same time, demonstrates comparable overall network performance to the state-of-the-art algorithm, AutoGAN, in the field of RL-based NAS for GAN.},
  archive      = {J_TCDS},
  author       = {Yi Fan and Xiulian Tang and Guoqiang Zhou and Jun Shen},
  doi          = {10.1109/TCDS.2020.3040796},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {234-245},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {EfficientAutoGAN: Predicting the rewards in reinforcement-based neural architecture search for generative adversarial networks},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Behavior decision of mobile robot with a
neurophysiologically motivated reinforcement learning model.
<em>TCDS</em>, <em>14</em>(1), 219–233. (<a
href="https://doi.org/10.1109/TCDS.2020.3035778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online model-free reinforcement learning (RL) approaches play a crucial role in coping with the real-world applications, such as the behavioral decision making in robotics. How to balance the exploration and exploitation processes is a central problem in RL. A balanced ratio of exploration/exploitation has a great influence on the total learning time and the quality of the learned strategy. Therefore, various action selection policies have been presented to obtain a balance between the exploration and exploitation procedures. However, these approaches are rarely, automatically, and dynamically regulated to the environment variations. One of the most amazing self-adaptation mechanisms in animals is their capacity to dynamically switch between exploration and exploitation strategies. This article proposes a novel neurophysiologically motivated model which simulates the role of medial prefrontal cortex (MPFC) and lateral prefrontal cortex (LPFC) in behavior decision. The sensory input is transmitted to the MPFC, then the ventral tegmental area (VTA) receives a reward and calculates a dopaminergic reinforcement signal, and the feedback categorization neurons in anterior cingulate cortex (ACC) calculate the vigilance according to the dopaminergic reinforcement signal. Then the vigilance is transformed to LPFC to regulate the exploration rate, finally the exploration rate is transmitted to thalamus to calculate the corresponding action probability. This action selection mechanism is introduced to the actor–critic model of the basal ganglia, combining with the cerebellum model based on the developmental network to construct a new hybrid neuromodulatory model to select the action of the agent. Both the simulation comparison with other four traditional action selection policies and the physical experiment results demonstrate the potential of the proposed neuromodulatory model in action selection.},
  archive      = {J_TCDS},
  author       = {Dongshu Wang and Shuli Chen and Yuhang Hu and Lei Liu and Heshan Wang},
  doi          = {10.1109/TCDS.2020.3035778},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {219-233},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Behavior decision of mobile robot with a neurophysiologically motivated reinforcement learning model},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic segmentation of pneumothorax in chest radiographs
based on a two-stage deep learning method. <em>TCDS</em>,
<em>14</em>(1), 205–218. (<a
href="https://doi.org/10.1109/TCDS.2020.3035572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pneumothorax is common but a life-threatening thoracic disease, which is difficult to diagnose based on chest X-ray images due to its subtle characteristics and low contrast of the disease regions. We aim to develop a fully automatic pneumothorax segmentation method to assist radiologists for timely and accurate diagnosis of pneumothorax. We propose a two-stage deep learning method. In the first stage, the chest X-ray image is classified as having pneumothorax or not using an ensemble of multiple modified U-Net models. Each model is trained using a multitask learning strategy with two highly correlated tasks: 1) pneumothorax classification and 2) segmentation. The segmentation task helps the model focus on more relevant regions to improve classification accuracy. The second stage performs precise segmentation of pneumothorax regions, which applies another ensemble model that consists of four U-Net-like models and one Deeplabv3+ model. We validated our method by participating in the 2019 SIIM-ACR pneumothorax segmentation challenge. Our method produced a classification area under the receiver operating characteristic curve (AUC) of 0.9795 and a Dice score of 0.8883, which secured the second place among 1475 teams. Fine tuning the models using all available data leads to more accurate results that surpassed all other competing methods. The code is made publicly available online at https://github.com/yelanlan/Pneumothorax-Segmentation-2nd-place-solution . Validation on extra polyp and lung segmentation data further proved the general applicability of the proposed method.},
  archive      = {J_TCDS},
  author       = {Xiyue Wang and Sen Yang and Jun Lan and Yuqi Fang and Jianhui He and Minghui Wang and Jing Zhang and Xiao Han},
  doi          = {10.1109/TCDS.2020.3035572},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {205-218},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Automatic segmentation of pneumothorax in chest radiographs based on a two-stage deep learning method},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A partially observable markov-decision-process-based
blackboard architecture for cognitive agents in partially observable
environments. <em>TCDS</em>, <em>14</em>(1), 189–204. (<a
href="https://doi.org/10.1109/TCDS.2020.3034428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial observability, or the inability of an agent to fully observe the state of its environment, exists in many real-world problem domains. However, most cognitive architectures do not have a theoretical foundation that allows for a systematic approach to handling partial observability. To address this issue, in this article, we propose a novel cognitive architecture based on the theory of partially observable Markov decision processes (POMDPs). We focus on one of the most fundamental cognitive architectures, the blackboard architecture, and reformulate it using the POMDP framework so that the agent can automatically decide how to use its information processing modules in partially observable environments. We propose a two-layer POMDP model for the blackboard architecture in which one layer models the dynamics of the environment and the other layer models the dynamics of the blackboard and knowledge sources within the agent. We also provide a planning method to find the optimal actions for the two-layer POMDP model based on the partially observable upper confidence bounds applied to trees (PO-UCT) algorithm. We validate the proposed architecture in both numerical and real-robot experiments.},
  archive      = {J_TCDS},
  author       = {Hideaki Itoh and Hidehiko Nakano and Ryota Tokushima and Hisao Fukumoto and Hiroshi Wakuya},
  doi          = {10.1109/TCDS.2020.3034428},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {189-204},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A partially observable markov-decision-process-based blackboard architecture for cognitive agents in partially observable environments},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Prediction of cognitive task activations via resting-state
functional connectivity networks: An EEG study. <em>TCDS</em>,
<em>14</em>(1), 181–188. (<a
href="https://doi.org/10.1109/TCDS.2020.3031604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective: The resting state is an internal state that is closely related to neural activation and the performance of tasks. Studying the relationship between the resting and task states is helpful for understanding the organization of information processing. It remains unclear how information is translated between these two states. Methods: In this study, we focused on electroencephalography (EEG) data because its high time resolution allowed us to study processing both overall and in detail. Resting-state functional connectivity (FC) networks were constructed in the time and frequency domains. Results: FC constructed by synchronization of signals in the time domain was suitable for predicting event-related potential activation. In addition, FC measured by phase distributions had superior prediction accuracy for predicting spectral power. Conclusion: Our findings suggest that there is intrinsic organization across the two states. Furthermore, the activity flow modeled in different domains could reflect different levels of neuronal activation. Significance: Changes in neural activity across resting and task states on a subsecond time scale can be detected by EEG, which is helpful for understanding the underlying mechanisms of illness and therapeutic outcomes.},
  archive      = {J_TCDS},
  author       = {Luyao Wang and Jian Zhang and Tiantian Liu and Duanduan Chen and Dikun Yang and Ritsu Go and Jinglong Wu and Tianyi Yan},
  doi          = {10.1109/TCDS.2020.3031604},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {181-188},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Prediction of cognitive task activations via resting-state functional connectivity networks: An EEG study},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Measuring the cognitive complexity in the comprehension of
modular process models. <em>TCDS</em>, <em>14</em>(1), 164–180. (<a
href="https://doi.org/10.1109/TCDS.2020.3032730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modularization in process models is a method to cope with the inherent complexity in such models (e.g., model size reduction). Modularization is capable to increase the quality, the ease of reuse, and the scalability of process models. Prior conducted research studied the effects of modular process models to enhance their comprehension. However, the effects of modularization on cognitive factors during process model comprehension are less understood so far. Therefore, this article presents the results of two exploratory studies (i.e., a survey research study with $N = 95$ participants; a follow-up eye tracking study with $N = 19$ participants), in which three types of modularization (i.e., horizontal, vertical, and orthogonal) were applied to process models expressed in terms of the business process model and notation (BPMN) 2.0. Furthermore, the effects of modularization on the cognitive load, the level of acceptability, and the performance in process model comprehension were investigated. In general, the results revealed that participants were confronted with challenges during the comprehension of modularized process models. Furthermore, performance in the comprehension of modularized process models showed only a few significant differences, however, the results obtained regarding the cognitive load revealed that the complexity and concept of modularization in process models were misjudged initially. The insights unraveled that the attitude towards the application and the behavioral intention to apply modularization in process models is still not clear. In this context, horizontal modularization appeared to be the best comprehensible modularization approach leading to a more fine-grained comprehension of the respective process models. The findings indicate that alterations in modular process models (e.g., change in the representation) are important to foster and enable their comprehension. Finally, based on our results, implications for research and practice as well as directions for future work are discussed in this article.},
  archive      = {J_TCDS},
  author       = {Michael Winter and Rüdiger Pryss and Thomas Probst and Julia Baß and Manfred Reichert},
  doi          = {10.1109/TCDS.2020.3032730},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {164-180},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Measuring the cognitive complexity in the comprehension of modular process models},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Functional brain networks underlying auditory saliency
during naturalistic listening experience. <em>TCDS</em>, <em>14</em>(1),
156–163. (<a href="https://doi.org/10.1109/TCDS.2020.3025947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With selective auditory attention (SAA), the human brain can cope with dynamic natural environments efficiently by prioritizing relevant auditory information while filtering out the rest. Auditory saliency describes how bottom-up auditory attention is attracted by local acoustic properties and plays an important role in SAA. In existing neuroimaging studies, the neural processing of auditory saliency has usually been explored via strictly controlled laboratory paradigms with abstract stimuli, overlooking the intrinsic functional interactions among spatially distributed brain networks underlying auditory saliency. In addition, it is unclear how auditory saliency-related neural responses vary in auditory information in different semantic categories. In this study, we explored functional brain networks underlying auditory saliency via a multivariate brain decoding approach, in which naturalistic auditory excerpts in three semantic categories (pop music, classic music, and speech) were used as stimuli in functional magnetic resonance imaging (fMRI). Our experimental results demonstrate that: 1) dynamic auditory saliency can be decoded with relatively high accuracy from fMRI brain activities and 2) the neural processing of auditory saliency in different types of auditory information on the one hand shares some common brain regions (e.g., the primary auditory cortex), on the other hand recruits distinct brain regions/networks, e.g., the sensorimotor network and working memory network for classic music and the auditory language network for speech. Our study provides complementary evidence to the neural processing of auditory saliency under naturalistic experience.},
  archive      = {J_TCDS},
  author       = {Liting Wang and Xintao Hu and Huan Liu and Shijie Zhao and Lei Guo and Junwei Han and Tianming Liu},
  doi          = {10.1109/TCDS.2020.3025947},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {156-163},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Functional brain networks underlying auditory saliency during naturalistic listening experience},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Alpha correlates of practice during mental preparation for
motor imagery. <em>TCDS</em>, <em>14</em>(1), 146–155. (<a
href="https://doi.org/10.1109/TCDS.2020.3026530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we quantified performance variations of motor imagery (MI)-based brain–computer interface (BCI) systems induced by practice. Two experimental sessions were recorded from ten healthy subjects while playing a BCI-oriented video game for 2 weeks. The analysis focused on the exploration of electroencephalographic (EEG) changes during mental preparation between novice and practiced subjects. EEG changes were quantified using global field power (GFP), dynamic time warping (TW), and mutual information (MutInf): GFP represents the strength of the electric field, TW measures signal similarities, and MutInf signals interdependency. Each metric was selected to relate insights extracted from mental preparation to the three experimental hypotheses associating practice with BCI performance. Significant results were identified in lower alpha for GFP and upper alpha for TW and MutInf. GFP in lower alpha during mental preparation assessed not only novice versus practiced variations but also “intrasession” differences. Findings suggest that EEG changes during mental preparation provide a quantitative measure of practice level. These metrics extracted before motor intention could be applied to BCI models targeting MI to monitor a user’s degree of training.},
  archive      = {J_TCDS},
  author       = {Mauro Nascimben and Yu-Kai Wang and Jung-Tai King and Tzyy-Ping Jung and Jonathan Touryan and Brent J. Lance and Chin-Teng Lin},
  doi          = {10.1109/TCDS.2020.3026530},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {146-155},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Alpha correlates of practice during mental preparation for motor imagery},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention-net: An ensemble sketch recognition approach using
vector images. <em>TCDS</em>, <em>14</em>(1), 136–145. (<a
href="https://doi.org/10.1109/TCDS.2020.3023055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the past few decades, machines have replaced humans in several disciplines. However, machine cognition still lags behind the human capabilities. We address the machines’ ability to recognize human drawn sketches in this work. Visual representations, such as sketches have long been a medium of communication for humans. For artificially intelligent systems to effectively immerse in interactive environments, it is required that machines understand such notations. The abstract nature and varied artistic styling of these sketches make automatic recognition of drawings more challenging than other areas of image classification. In this article, we use sketches represented as a sequence of strokes, i.e., as vector images, to effectively capture the long-term temporal dependencies in hand-drawn sketches. The proposed approach combines the self-attention capabilities of Transformers while effectively utilizing the long-term temporal dependencies through temporal convolution networks (TCNs) for sketch recognition. The confidence scores obtained from the two techniques are combined using triangular-norm (T-norm). Attention heat maps are plotted to isolate the discriminating parts of a sketch that contribute to sketch classification. The extensive quantitative and qualitative evaluation confirms that the proposed network performs favorably against state-of-the-art techniques.},
  archive      = {J_TCDS},
  author       = {Gaurav Jain and Shivang Chopra and Suransh Chopra and Anil Singh Parihar},
  doi          = {10.1109/TCDS.2020.3023055},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {136-145},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Attention-net: An ensemble sketch recognition approach using vector images},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Occlusion-based coordination protocol design for autonomous
robotic shepherding tasks. <em>TCDS</em>, <em>14</em>(1), 126–135. (<a
href="https://doi.org/10.1109/TCDS.2020.3018549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robotic shepherding problem has earned significant research interest over the last few decades due to its potential application in precision agriculture. In this article, we first modeled the sheep flocking behavior using adaptive protocols and artificial potential field methods. Then, we designed a coordination algorithm for robotic dogs. An occlusion-based motion control strategy was proposed to herd the sheep to the desired location. Compared to formation-based techniques, the proposed control strategy provides more flexibility and efficiency when herding a large number of sheep. Simulation and lab-based experiments, using real robots and a global vision-based tracking system, were carried out to validate the effectiveness of the proposed approach.},
  archive      = {J_TCDS},
  author       = {Junyan Hu and Ali Emre Turgut and Tomáš Krajník and Barry Lennox and Farshad Arvin},
  doi          = {10.1109/TCDS.2020.3018549},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {126-135},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Occlusion-based coordination protocol design for autonomous robotic shepherding tasks},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting alzheimer’s dementia degree. <em>TCDS</em>,
<em>14</em>(1), 116–125. (<a
href="https://doi.org/10.1109/TCDS.2020.3015131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diagnosis of Alzheimer’s disease (AD) faces two important issues. They are how to extract the features of the rhythms of patients with AD and how to label them and reveal the degree of dementia in patients. This study defines 14 instantaneous power indicators of dementia judgment through Hilbert marginal spectrum (HMS) from rhythm waves. A warped infinite Gaussian mixture model (WiGMM) is proposed to learn the latent variables of these indicators to detect the degree of dementia. The experimental results show that HMS-based indicators are able to reflect the cognitive function of AD patients. This proposed method has the ability to detect brain cognitive status through a warped transform and Dirichlet process parameter prior to inference.},
  archive      = {J_TCDS},
  author       = {Edmond Q. Wu and Xian-Yong Peng and Sheng-Di Chen and Xiao-Yan Zhao and Zhi-Ri Tang},
  doi          = {10.1109/TCDS.2020.3015131},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {116-125},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Detecting alzheimer’s dementia degree},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accurate and fast deep evolutionary networks structured
representation through activating and freezing dense networks.
<em>TCDS</em>, <em>14</em>(1), 102–115. (<a
href="https://doi.org/10.1109/TCDS.2020.3017100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have been scaled up to thousands of layers with the intent to improve their accuracy. Unfortunately, after some point, doubling the number of layers leads to only minor improvements, while the training difficulties increase substantially. In this article, we present an approach for constructing high-accuracy deep evolutionary networks and train them by activating and freezing dense networks (AFNets). The activating and freezing strategy enables us to reduce the classification error of test and reduce the training time required for deeper dense networks. We activate the layers that are being trained and construct a freezing box to freeze the idle and pretrained network layers in order to minimize memory consumption. The training speed in the early stage is not fast enough because many layers are activated for training. As the epochs gradually increase, the training speed becomes faster and faster since fewer and fewer layers are activated. Our method improves the convergence to the optimal performance within a limited number of epochs. Comprehensive experiments on a variety of data sets show that the proposed model achieves better performance when compared to the other state-of-the-art network models.},
  archive      = {J_TCDS},
  author       = {Dayu Tan and Weimin Zhong and Xin Peng and Qiang Wang and Vladimir Mahalec},
  doi          = {10.1109/TCDS.2020.3017100},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {102-115},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Accurate and fast deep evolutionary networks structured representation through activating and freezing dense networks},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A neural dynamic network drives an intentional agent that
autonomously learns beliefs in continuous time. <em>TCDS</em>,
<em>14</em>(1), 90–101. (<a
href="https://doi.org/10.1109/TCDS.2020.3013768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous learning is the ability to form knowledge representations solely through one’s own experience. To autonomously learn, an agent must be able to perceive, act, memorize, plan, and desire; it must be able to form intentional states. We build on a neural process account of intentionality, in which intentional states are stabilized by interactions within populations of neurons that represent perceptual features and movement parameters. Instabilities in such neural dynamics induce sequences of intentional behavior. In this article, we examine the neural process organization required to decide and control when learning takes place, to build the representations that can hold learning data, and to organize the selection of neural substrate to learn the novel patterns. We demonstrate how a neural dynamic network may learn new beliefs about the world from single experiences, may activate and use beliefs to satisfy desires, and may deactivate beliefs when their predictions do not match experience. We illustrate the ideas in a simple toy scenario in which a simulated agent autonomously explores an environment, directs action at objects, and forms beliefs about simple contingencies in this environment. The agent utilizes learned beliefs to satisfy its own fixed desires.},
  archive      = {J_TCDS},
  author       = {Jan Tekülve and Gregor Schöner},
  doi          = {10.1109/TCDS.2020.3013768},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {90-101},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A neural dynamic network drives an intentional agent that autonomously learns beliefs in continuous time},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intrinsic motivations and planning to explain tool-use
development: A study with a simulated robot model. <em>TCDS</em>,
<em>14</em>(1), 75–89. (<a
href="https://doi.org/10.1109/TCDS.2020.2986411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developmental psychology experiments on tool use show that infants’ capacity to use a rake-like tool to retrieve a toy arises quite suddenly around 18 months. We use a developmental-robotics model to propose and test two alternative hypotheses to explain this conundrum. Both hypotheses rely on the assumptions that tool use involves goal-directed behavior processes guided by the goal of retrieving the toy, and that “understanding how to use a tool” means acquiring the capacity to assemble a sequence of actions to accomplish the goal (e.g., to “hook” and then “retrieve” the toy). The first hypothesis is that the tool-use ability emerges when the infant develops enough planning capabilities. The second hypothesis is that the ability emerges when the infant’s intrinsic motivation system develops and makes playing with a couple of objects interesting enough so that the infant plays with objects similar to the tool at home and thus acquires the actions needed to retrieve the toy in the lab. These hypotheses are tested through a neural-network architecture controlling a simulated humanoid robot tested with the tool-rake task. Given the assumptions made in the model, the results show that both hypotheses can reproduce the average behavior of infants but only the intrinsic-motivation hypothesis can reproduce the sudden tool-use improvement.},
  archive      = {J_TCDS},
  author       = {Kristsana Seepanomwan and Daniele Caligiore and Kevin J. O’Regan and Gianluca Baldassarre},
  doi          = {10.1109/TCDS.2020.2986411},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {75-89},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Intrinsic motivations and planning to explain tool-use development: A study with a simulated robot model},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incremental learning framework for autonomous robots based
on q-learning and the adaptive kernel linear model. <em>TCDS</em>,
<em>14</em>(1), 64–74. (<a
href="https://doi.org/10.1109/TCDS.2019.2962228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of autonomous robots in varying environments needs to be improved. For such incremental improvement, here we propose an incremental learning framework based on $Q$ -learning and the adaptive kernel linear (AKL) model. The AKL model is used for storing behavioral policies that are learned by $Q$ -learning. Both the structure and parameters of the AKL model can be trained using a novel L2-norm kernel recursive least squares (L2-KRLS) algorithm. The AKL model initially without nodes and gradually accumulates content. The proposed framework allows to learn new behaviors without forgetting the previous ones. A novel local $\varepsilon $ -greedy policy is proposed to speed the convergence rate of $Q$ -learning. It calculates the exploration probability of each state for generating and selecting more important training samples. The performance of our incremental learning framework was validated in two experiments. A curve-fitting example shows that the L2-KRLS-based AKL model is suitable for incremental learning. The second experiment is based on robot learning tasks. The results show that our framework can incrementally learn behaviors in varying environments. Local $\varepsilon $ -greedy policy-based $Q$ -learning is faster than the existing $Q$ -learning algorithms.},
  archive      = {J_TCDS},
  author       = {Yanming Hu and Decai Li and Yuqing He and Jianda Han},
  doi          = {10.1109/TCDS.2019.2962228},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {64-74},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Incremental learning framework for autonomous robots based on Q-learning and the adaptive kernel linear model},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effective brain connectivity for fNIRS with fuzzy cognitive
maps in neuroergonomics. <em>TCDS</em>, <em>14</em>(1), 50–63. (<a
href="https://doi.org/10.1109/TCDS.2019.2958423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective connectivity (EC) among functional near-infrared spectroscopy (fNIRS) signals is a quantitative measure of the strength of influence between brain activity associated with different regions of the brain. Evidently, accurate deciphering of EC gives further insight into the understanding of the intricately complex nature of neuronal interactions in the human brain. This article presents a novel approach to estimate EC in the human brain signals using enhanced fuzzy cognitive maps (FCMs). The proposed method presents a regularized methodology of FCMs, called effective FCMs (E-FCMs), with improved accuracy for predicting EC between real and synthetic fNIRS signals. Essentially, the revisions made in the FCM methodology include a more powerful prediction formula for FCM combined with independent tuning of the transformation function parameter. A comparison of EC in fNIRS signals obtained from E-FCM with that obtained from standard FCM, general linear model (GLM) parameters that power dynamic causal modeling (DCM), and Granger causality (GC) manifests the greater prowess of the proposed E-FCM over the aforementioned methods. For real fNIRS data, an empirical investigation is also made to gain an insight into the role of oxyhemoglobin and deoxy-hemoglobin (oxy-Hb, deoxy-Hb) in representing the cognitive activity. We believe this article has profound implications for neuroergonomics research communities.},
  archive      = {J_TCDS},
  author       = {Mehrin Kiani and Javier Andreu-Perez and Hani Hagras and Elpiniki I. Papageorgiou and Mukesh Prasad and Chin-Teng Lin},
  doi          = {10.1109/TCDS.2019.2958423},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {50-63},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Effective brain connectivity for fNIRS with fuzzy cognitive maps in neuroergonomics},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approaches, challenges, and applications for deep visual
odometry: Toward complicated and emerging areas. <em>TCDS</em>,
<em>14</em>(1), 35–49. (<a
href="https://doi.org/10.1109/TCDS.2020.3038898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual odometry (VO) is a prevalent way to deal with the relative localization problem, which is becoming increasingly mature and accurate, but it tends to be fragile under challenging environments. Comparing with classical geometry-based methods, deep-learning-based methods can automatically learn effective and robust representations, such as depth, optical flow, feature, ego-motion, etc., from data without explicit computation. Nevertheless, there still lacks a thorough review of the recent advances of deep-learning-based VO (Deep VO). Therefore, this article aims to gain a deep insight on how deep learning can profit and optimize the VO systems. We first screen out a number of qualifications, including accuracy, efficiency, scalability, dynamicity, practicability, and extensibility, and employ them as the criteria. Then, using the offered criteria as the uniform measurements, we detailedly evaluate and discuss how deep learning improves the performance of VO from the aspects of depth estimation, feature extraction and matching, and pose estimation. We also summarize the complicated and emerging areas of Deep VO, such as mobile robots, medical robots, augmented and virtual reality, etc. Through the literature decomposition, analysis, and comparison, we finally put forward a number of open issues and raise some future research directions in this field.},
  archive      = {J_TCDS},
  author       = {Ke Wang and Sai Ma and Junlan Chen and Fan Ren and Jianbo Lu},
  doi          = {10.1109/TCDS.2020.3038898},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {35-49},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Approaches, challenges, and applications for deep visual odometry: Toward complicated and emerging areas},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Agent-based intelligent decision support systems: A
systematic review. <em>TCDS</em>, <em>14</em>(1), 20–34. (<a
href="https://doi.org/10.1109/TCDS.2020.3030571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making complexity, in a distributed environment, is due to hard tasks that a system must resolve. This complexity makes researchers focus on looking for solutions to cope with distributed environment problems. Multiagent system (MAS) technology is one of several proposed solutions. This technology rose the challenge in decision-making applications. To date, no systematic review has been conducting, to the best of the authors’ knowledge, to give an overview of a multiagent-based decision-making system in various areas of science or technology. In this study, we review of 58 studies published from 2007 to 2019. The aim of this article is a critical analysis of recent approaches. We try to survey their impact on practice and research. The analysis of the extracted studies is based on three selection criteria that are defined in the paper. All included studies have analyzed from four different points of view: 1) theoretical view; 2) technical view; 3) agent view; and 4) application view. Moreover, we adopt the SWOT analysis to evaluate studied approaches. Multiagent technology is actually in the process of evolution and enhancement with the appearance of new trends in artificial intelligence, such as neural network and deep learning. The results of this review show suggestions for further research and practice.},
  archive      = {J_TCDS},
  author       = {Faten Khemakhem and Hamdi Ellouzi and Hela Ltifi and Mounir Ben Ayed},
  doi          = {10.1109/TCDS.2020.3030571},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {20-34},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Agent-based intelligent decision support systems: A systematic review},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transfer learning for EEG-based brain–computer interfaces: A
review of progress made since 2016. <em>TCDS</em>, <em>14</em>(1), 4–19.
(<a href="https://doi.org/10.1109/TCDS.2020.3007453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A brain–computer interface (BCI) enables a user to communicate with a computer directly using brain signals. The most common noninvasive BCI modality, electroencephalogram (EEG), is sensitive to noise/artifact and suffers between-subject/within-subject nonstationarity. Therefore, it is difficult to build a generic pattern recognition model in an EEG-based BCI system that is optimal for different subjects, during different sessions, for different devices and tasks. Usually, a calibration session is needed to collect some training data for a new subject, which is time consuming and user unfriendly. Transfer learning (TL), which utilizes data or knowledge from similar or relevant subjects/sessions/devices/tasks to facilitate learning for a new subject/session/device/task, is frequently used to reduce the amount of calibration effort. This article reviews journal publications on TL approaches in EEG-based BCIs in the last few years, i.e., since 2016. Six paradigms and applications—motor imagery, event-related potentials, steady-state visual evoked potentials, affective BCIs, regression problems, and adversarial attacks—are considered. For each paradigm/application, we group the TL approaches into cross-subject/session, cross-device, and cross-task settings and review them separately. Observations and conclusions are made at the end of the article, which may point to future research directions.},
  archive      = {J_TCDS},
  author       = {Dongrui Wu and Yifan Xu and Bao-Liang Lu},
  doi          = {10.1109/TCDS.2020.3007453},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {4-19},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Transfer learning for EEG-based Brain–Computer interfaces: A review of progress made since 2016},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial IEEE transactions on cognitive and developmental
systems. <em>TCDS</em>, <em>14</em>(1), 2–3. (<a
href="https://doi.org/10.1109/TCDS.2022.3151285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Happy “New Year of Tiger!” At celebrating the beginning of 2022 spring, I would like to take this opportunity to wish everyone an energetic, healthy, and prosperous new year! It is my great honor to serve as the Editor-in-Chief of the IEEE Transactions on Cognitive and Developmental Systems (TCDS).},
  archive      = {J_TCDS},
  author       = {Huajin Tang},
  doi          = {10.1109/TCDS.2022.3151285},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {2-3},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Editorial IEEE transactions on cognitive and developmental systems},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
