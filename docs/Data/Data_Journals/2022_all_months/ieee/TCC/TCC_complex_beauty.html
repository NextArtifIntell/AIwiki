<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TCC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tcc---214">TCC - 214</h2>
<ul>
<li><details>
<summary>
(2022). On the security of verifiable searchable encryption schemes.
<em>TCC</em>, <em>10</em>(4), 2977–2978. (<a
href="https://doi.org/10.1109/TCC.2021.3071779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With cloud services, data users can retrieve encrypted data while preserving data confidentiality. However, this new paradigm suffers from many security concerns. A major concern is how to avoid insider Keyword-Guessing Attacks (KGA), which implies that the internal attackers can guess the candidate keywords successfully in an off-line manner. To address this issue, recently, two verifiable searchable encryption schemes (published in IEEE Transactions on Cloud Computing, doi: 10.1109/TCC.2020.2989296) in cloud storage were proposed which enjoys many desirable features. In this letter, we demonstrate that the schemes are insecure against insider keyword-guessing attack. Specifically, we show that the adversary can derive the keywords in an off-line manner.},
  archive      = {J_TCC},
  author       = {Chuang Li and Chunxiang Xu and Shanshan Li and Kefei Chen and Yinbin Miao},
  doi          = {10.1109/TCC.2021.3071779},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2977-2978},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {On the security of verifiable searchable encryption schemes},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VPSL: Verifiable privacy-preserving data search for
cloud-assisted internet of things. <em>TCC</em>, <em>10</em>(4),
2964–2976. (<a href="https://doi.org/10.1109/TCC.2020.3031209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-assisted Internet of Things (IoT) is increasingly prevalent used in various fields, such as the healthcare system. While in such a scenario, sensitive data (e.g., personal electronic medical records) can be easily revealed, which incurs potential security challenges. Thus, Symmetric Searchable Encryption (SSE) has been extensively studied due to its capability of supporting efficient search on encrypted data. However, most SSE schemes require the data owner to share the complete key with query users and take malicious cloud servers out of consideration. Seeking to address these limitations, in this article we propose a Verifiable Privacy-preserving data Search scheme with Limited key-disclosure (VPSL) for cloud-assisted Internet of Things. VPSL first designs a trapdoor generation protocol for obtaining a trapdoor with disclosing limited key information and without revealing plaintext query points to others. Then, VPSL provides an efficient result verification and search processing by employing the Merkle hash tree structure and k-means clustering technique, respectively. VPSL is secure against the level-2 attack. Finally, an enhanced VPSL (called VPSL+) resisting the level-3 attack is constructed by introducing the random splitting technique. Empirical experiments demonstrate the accuracy and efficiency of VPSL or VPSL+ using real-world datasets.},
  archive      = {J_TCC},
  author       = {Qiuyun Tong and Yinbin Miao and Ximeng Liu and Kim-Kwang Raymond Choo and Robert H. Deng and Hongwei Li},
  doi          = {10.1109/TCC.2020.3031209},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2964-2976},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {VPSL: Verifiable privacy-preserving data search for cloud-assisted internet of things},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Virtual network function service provisioning in MEC via
trading off the usages between computing and communication resources.
<em>TCC</em>, <em>10</em>(4), 2949–2963. (<a
href="https://doi.org/10.1109/TCC.2020.3043313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) has emerged as a promising technology that offers resource-intensive yet delay-sensitive applications from the edge of mobile networks. With the emergence of complicated and resource-hungry mobile applications, offloading user tasks to cloudlets of nearby mobile edge-cloud networks is becoming an important approach to leverage the processing capability of mobile devices, reduce mobile device energy consumptions, and improve experiences of mobile users. In this article we first study the provisioning of virtualized network function (VNF) services for user requests in an MEC network, where each user request has a demanded data packet rate with a specified network function service requirement, and different user requests need different services that are represented by virtualized network functions instantiated in cloudlets. We aim to maximize the number of user request admissions while minimizing their admission cost, where the request admission cost consists of the computing cost on instantiations of requested VNF instances and the data packet traffic processing of requests in their VNF instances, and the communication cost of routing data packet traffic of requests between users and the cloudlets hosting their requested VNF instances. We study the joint VNF instance deployment and user requests assignment in MEC, by explicitly exploring a non-trivial usage tradeoff between different types of resources. To this end, we first formulate the cost minimization problem that admits all requests by assuming that there is sufficient computing resource in MEC to accommodate the requested VNF instances of all requests, for which we formulate an Integer Linear Programming solution and two efficient heuristic algorithms. We then deal with the problem under the computing resource constraint. We term this problem as the throughput maximization problem by admitting as many as requests, subject to computing resource capacity on each cloudlet, for which we formulate an ILP solution when the problem size is small; otherwise, we devise efficient algorithms for it. We finally evaluate the performance of the proposed algorithms through experimental simulations. Experimental results demonstrate that the proposed algorithms are promising. To the best of our knowledge, we are the first to explicitly explore the usage tradeoff between computing and communication resources in the admissions of user requests in MEC through introducing a novel load factor concept to minimize the request admission cost and maximize the network throughput.},
  archive      = {J_TCC},
  author       = {Yu Ma and Weifa Liang and Meitian Huang and Wenzheng Xu and Song Guo},
  doi          = {10.1109/TCC.2020.3043313},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2949-2963},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Virtual network function service provisioning in MEC via trading off the usages between computing and communication resources},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Traceable and controllable encrypted cloud image search in
multi-user settings. <em>TCC</em>, <em>10</em>(4), 2936–2948. (<a
href="https://doi.org/10.1109/TCC.2020.3034232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of cloud computing, explosively increasing images are gradually outsourced to the cloud server for costs saving and feasibility. For security and privacy concerns, images (e.g., medical diagnosis, personal photos) should be encrypted before being outsourced. However, traditional encrypted image retrieval techniques still suffer from costly access control and low search accuracy. To solve these challenging issues, in this article, we first propose a Controllable encrypted cloud image Search scheme in Multi-user settings (namely CSM) by using the polynomial-based access strategy and proxy re-encryption technique. CSM achieves efficient access control and avoids heavy communication overhead caused by key transmission. Then, we improve the basic CSM to achieve malicious search user Tracing (namely TCSM) by utilizing the watermark technique, which can further prevent search users from illegally redistributing retrieved images to unauthorized search users. Our formal security analysis proves that our CSM (or TCSM) can guarantee the privacy of images, indexes, and search queries. Our empirical experiments using real-world datasets demonstrate the efficiency and high accuracy of our CSM (or TCSM) in practice.},
  archive      = {J_TCC},
  author       = {Yingying Li and Jianfeng Ma and Yinbin Miao and Yue Wang and Tengfei Yang and Ximeng Liu and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TCC.2020.3034232},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2936-2948},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Traceable and controllable encrypted cloud image search in multi-user settings},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust optimization model for primary and backup resource
allocation in cloud providers. <em>TCC</em>, <em>10</em>(4), 2920–2935.
(<a href="https://doi.org/10.1109/TCC.2021.3051018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a primary and backup resource allocation model that provides a probabilistic protection guarantee for virtual machines against multiple failures of physical machines in a cloud provider to minimize the required total capacity. A physical machine allocates both primary and backup computing resources for virtual machines. When any failure occurs, the survived physical machines with preplanned backup resources recover the virtual machines on the failed physical machines and take over the workloads. The probability that the protection provided by a physical machine does not succeed is guaranteed within a given number. Providing the probabilistic protection can reduce the required backup capacity by allowing backup resource sharing, but it leads to a nonlinear programing problem in a general-capacity case against multiple failures. We apply robust optimization with extensive mathematical operations to formulate the primary and backup resource allocation problem as a mixed integer linear programming problem, where capacity fragmentation is suppressed. We prove the NP-hardness of considered problem. A heuristic is introduced to solve the optimization problem. The results reveal that the proposed model saves about one-third of the total capacity in our examined cases; it outperforms the conventional models in terms of both blocking probability and resource utilization.},
  archive      = {J_TCC},
  author       = {Fujun He and Takehiro Sato and Bijoy Chand Chatterjee and Takashi Kurimoto and Shigeo Urushidani and Eiji Oki},
  doi          = {10.1109/TCC.2021.3051018},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2920-2935},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Robust optimization model for primary and backup resource allocation in cloud providers},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reliability-aware cost-efficient scientific workflows
scheduling strategy on multi-cloud systems. <em>TCC</em>,
<em>10</em>(4), 2909–2919. (<a
href="https://doi.org/10.1109/TCC.2021.3057422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, more and more computation-intensive scientific applications with diverse needs are migrating to cloud computing systems. However, the cloud systems alone cannot meet applications’ requirements at all times with the increasing demands from users. Therefore, the multi-cloud systems that can provide scalable storage and computing resources become a good solution. The main challenges for such systems are multiple billing mechanisms, virtual resources heterogeneity, and systems reliability. In response to these challenges, we first build a multi-cloud systems fault-tolerant workflow scheduling framework, which tries to improve the scientific applications execution reliability and reduce their execution cost. Then, we use Weibull distribution to analyze task execution reliability and hazard rate, which is used to duplicate task with high execution hazard rate. Third, we integrate different multi-cloud providers’ billing mechanism into the proposed scheduling framework, and this workflow scheduling problem is mathematically formulated as an optimization problem. Fourth, we define the DAG tasks cost-efficient bottom level, and propose a fault-tolerant cost-efficient workflow scheduling algorithm (FCWS) that minimizes application execution cost, time while ensuring their reliability. Simulation experiments for performance evaluation were conducted based on two real-world applications: Epigenomics and LIGO. The results clearly demonstrate that our proposed FCWS algorithm outperforms existing FR-MOS, CWS in terms of cost and reliability, and FCWS is also better than CWS and inferior to FR-MOS in term of makespan.},
  archive      = {J_TCC},
  author       = {Xiaoyong Tang},
  doi          = {10.1109/TCC.2021.3057422},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2909-2919},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Reliability-aware cost-efficient scientific workflows scheduling strategy on multi-cloud systems},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Range-price trade-off in sensor-cloud for provisioning
sensors-as-a-service. <em>TCC</em>, <em>10</em>(4), 2897–2908. (<a
href="https://doi.org/10.1109/TCC.2020.3030851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an optimal pricing scheme for provisioning sensors-as-a-service (Se-aaS) for catering to applications with multi-tenancy requirements in a sensor-cloud platform. The scheme orchestrates a trade-off analysis between communication range and price in a sensor-cloud platform with range-reconfigurable nodes. The proposed scheme consists of two phases – (a) selection of a neighbor node of a source node, and determination of optimal price for the selected neighbor node. In the first phase, a source node adjusts its communication range and selects its best possible neighbor node using selectivity factor of all the neighbor nodes. The selectivity factor considers the determinants such as effective residual energy, effective power consumption, and the number of applications to which the neighbor nodes are associated in the neighbor selection process. In the second phase, we design a utility function to determine the optimal price of the selected neighbor node. We use the Lagrangian function to model the proposed problem as a mixed-integer linear program (MILP) and obtain the optimal solution using the Karush-Kuhn-Tucker (KKT) conditions. The existing works on pricing in sensor-cloud are deficient in considering the presence of the reconfigurable communication range of sensor nodes. Moreover, based on the value of the communication range, the charged price of the sensor nodes varies. Thus, in this article, we propose a pricing scheme with a trade-off of the reconfigurable communication range of sensor nodes and the charged price incurred in adjusting the communication range. Extensive experimental results show that the proposed scheme performs better compared to the existing pricing schemes for sensor-cloud. In precise, the proposed scheme is capable of increasing the average number of neighbor nodes by at least 1.38 percent. Further, the proposed scheme is capable of reducing the charged price by 10.55 percent, as compared to the existing pricing scheme, DOPH.},
  archive      = {J_TCC},
  author       = {Arijit Roy and Sudip Misra and Farid Nait-Abdesselam},
  doi          = {10.1109/TCC.2020.3030851},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2897-2908},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Range-price trade-off in sensor-cloud for provisioning sensors-as-a-service},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). QoS-aware data center operations based on chance- and
risk-constrained optimization. <em>TCC</em>, <em>10</em>(4), 2887–2896.
(<a href="https://doi.org/10.1109/TCC.2020.3031612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because data centers are energy-intensive industries, data centers have made significant efforts to improve energy efficiency to save energy cost and utilize renewable energy to contribute to environmental sustainability. In addition, data centers have been considered as a potential application of demand response because they can possibly adjust energy consumption to process workload in response to time-of-use prices and intermittent renewable generation. Quality of service (QoS) should be guaranteed at the desired level to respond to users’ requests with their satisfaction in an effort to achieve energy-efficient and sustainable data center operations. Given this context, this study intends to develop a two-stage stochastic program that can be used to investigate optimal data center operations based on demand response that results in minimum energy cost with QoS guarantee against the stochastic workloads. Then, an additional chance constraint and risk constraint are considered as means of managing the level of QoS guarantee, and comprehensive numerical experiments are conducted with various parameter settings to evaluates the impact of the QoS guarantee with different levels on overall performance. The results show that the risk associated with the QoS guarantee for data centers needs to be properly managed to improve energy efficiency.},
  archive      = {J_TCC},
  author       = {Soongeol Kwon},
  doi          = {10.1109/TCC.2020.3031612},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2887-2896},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {QoS-aware data center operations based on chance- and risk-constrained optimization},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Power control framework for green data centers.
<em>TCC</em>, <em>10</em>(4), 2876–2886. (<a
href="https://doi.org/10.1109/TCC.2020.3022789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, renewable energy, such as wind and photovoltaic electric power has been increasingly integrated into data center power provisioning systems to address high energy consumption of data centers. However, in reality, the intermittency and randomness of renewable energy (power supply fluctuation) is detrimental to the reliable operation of sophisticated IT equipment in those so-called green data centers. In this article, we address the problem of data center power regulation explicitly taking into account the unreliability and instability of renewable energy sources. To this extent, we design a novel data center power control framework that smoothens the power fluctuation and instability of renewable energy sources. The core of our framework is two power regulation optimization algorithms. In particular, a server workload scheduling algorithm deals with high frequency fluctuations while an Uninterruptable Power Supply (UPS) power regulation algorithm handles low frequency and large extent power fluctuations. These algorithms are also designed to satisfy service level agreement (SLA) and standby power supply capacity. We have conducted an extensive evaluation study using trace data of a real data center of 30000-node cluster with 50 x 250 UPS battery groups and 24-hour power generation data from real wind farm and photovoltaic power station. The experimental results show our framework effectively smoothens fluctuations of data center power supply, more effective use of renewable energy, and extend the UPS batteries’ lives to reduce the skyrocketed data center operating expenses.},
  archive      = {J_TCC},
  author       = {Ting Yang and Yucheng Hou and Young Choon Lee and Hao Ji and Albert Y. Zomaya},
  doi          = {10.1109/TCC.2020.3022789},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2876-2886},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Power control framework for green data centers},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). POTUS: Predictive online tuple scheduling for data stream
processing systems. <em>TCC</em>, <em>10</em>(4), 2863–2875. (<a
href="https://doi.org/10.1109/TCC.2020.3032577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most online service providers deploy their own data stream processing systems in the cloud to conduct large-scale and real-time data analytics. However, such systems, e.g., Apache Heron, often adopt naive scheduling schemes to distribute data streams (in the units of tuples) among processing instances, which may result in workload imbalance and system disruption. Hence, there still exists a mismatch between the temporal variations of data streams and such inflexible scheduling scheme designs. Besides, the fundamental limits of benefits of predictive scheduling to data stream processing systems remain unexplored. In this article, we focus on the problem of tuple scheduling with predictive service in Apache Heron. With a careful choice in the granularity of system modeling and decision making, we formulate the problem as a stochastic network optimization problem and propose POTUS , an online predictive scheduling scheme that aims to minimize the response time of data stream processing by steering data streams in a distributed fashion. Theoretical analysis and simulation results show that POTUS achieves an ultra-low response time with a stability guarantee. Moreover, POTUS only requires mild-value of future information to effectively reduce the response time, even with mis-prediction.},
  archive      = {J_TCC},
  author       = {Xi Huang and Ziyu Shao and Yang Yang},
  doi          = {10.1109/TCC.2020.3032577},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2863-2875},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {POTUS: Predictive online tuple scheduling for data stream processing systems},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performance modeling of microservice platforms.
<em>TCC</em>, <em>10</em>(4), 2848–2862. (<a
href="https://doi.org/10.1109/TCC.2020.3029092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microservice architecture has transformed the way developers are building and deploying applications in the nowadays cloud computing centers. This new approach provides increased scalability, flexibility, manageability, and performance while reducing the complexity of the whole software development life cycle. The increase in cloud resource utilization also benefits microservice providers. Various microservice platforms have emerged to facilitate the DevOps of containerized services by enabling continuous integration and delivery. Microservice platforms deploy application containers on virtual or physical machines provided by public/private cloud infrastructures in a seamless manner. In this article, we study and evaluate the provisioning performance of microservice platforms by incorporating the details of all layers (i.e., both micro and macro layers) in the modeling process. To this end, we first build a microservice platform on top of Amazon EC2 cloud and then leverage it to develop a comprehensive performance model to perform what-if analysis and capacity planning for microservice platforms at scale. In other words, the proposed performance model provides a systematic approach to measure the elasticity of the microservice platform by analyzing the provisioning performance at both the microservice platform and the back-end macroservice infrastructures.},
  archive      = {J_TCC},
  author       = {Hamzeh Khazaei and Nima Mahmoudi and Cornel Barna and Marin Litoiu},
  doi          = {10.1109/TCC.2020.3029092},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2848-2862},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Performance modeling of microservice platforms},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performance modeling of serverless computing platforms.
<em>TCC</em>, <em>10</em>(4), 2834–2847. (<a
href="https://doi.org/10.1109/TCC.2020.3033373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analytical performance models have been leveraged extensively to analyze and improve the performance and cost of various cloud computing services. However, in the case of serverless computing, which is projected to be the dominant form of cloud computing in the future, we have not seen analytical performance models to help with the analysis and optimization of such platforms. In this work, we propose an analytical performance model that captures the unique details of serverless computing platforms. The model can be leveraged to improve the quality of service and resource utilization and reduce the operational cost of serverless platforms. Also, the proposed performance model provides a framework that enables serverless platforms to become workload-aware and operate differently for different workloads to provide a better trade-off between the cost and performance depending on the user&#39;s preferences. The current serverless offerings require the user to have extensive knowledge of the internals of the platform to perform efficient deployments. Using the proposed analytical model, the provider can simplify the deployment process by calculating the performance metrics for users even before physical deployments. We validate the applicability and accuracy of the proposed model by extensive experimentation on AWS Lambda. We show that the proposed model can calculate essential performance metrics such as average response time, probability of cold start, and the average number of function instances in the steady-state. Also, we show how the performance model can be used to tune the serverless platform for each workload, which will result in better performance or lower cost without scarifying the other. The presented model assumes no non-realistic restrictions, so that it offers a high degree of fidelity while maintaining tractability at large scale.},
  archive      = {J_TCC},
  author       = {Nima Mahmoudi and Hamzeh Khazaei},
  doi          = {10.1109/TCC.2020.3033373},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2834-2847},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Performance modeling of serverless computing platforms},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal task allocation and coding design for secure edge
computing with heterogeneous edge devices. <em>TCC</em>, <em>10</em>(4),
2817–2833. (<a href="https://doi.org/10.1109/TCC.2021.3050012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, edge computing has attracted significant attention because it can effectively support many delay-sensitive applications. Despite such a salient feature, edge computing also faces many challenges, especially for efficiency and security, because edge devices are usually heterogeneous and may be untrustworthy. To address these challenges, we propose a unified framework to provide efficiency and confidentiality by coded distributed computing. Within the proposed framework, we use matrix multiplication, a fundamental building block of many distributed machine learning algorithms, as the representative computation task. To minimize resource consumption while achieving information-theoretic security, we investigate two highly-coupled problems, (1) task allocation that assigns data blocks in a computing task to edge devices and (2) linear code design that generates data blocks by encoding the original data with random information. Specifically, we first theoretically analyze the necessary conditions for the optimal solution. Based on the theoretical analysis, we develop an efficient task allocation algorithm to obtain a set of selected edge devices and the number of coded vectors allocated to them. Using the task allocation results, we then design secure coded computing schemes, for two cases, (1) with redundant computation and (2) without redundant computation, all of which satisfy the availability and security conditions. Moreover, we also theoretically analyze the optimization of the proposed scheme. Finally, we conduct extensive simulation experiments to demonstrate the effectiveness of the proposed schemes.},
  archive      = {J_TCC},
  author       = {Jin Wang and Chunming Cao and Jianping Wang and Kejie Lu and Admela Jukan and Wei Zhao},
  doi          = {10.1109/TCC.2021.3050012},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2817-2833},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Optimal task allocation and coding design for secure edge computing with heterogeneous edge devices},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OP-MLB: An online VM prediction-based multi-objective load
balancing framework for resource management at cloud data center.
<em>TCC</em>, <em>10</em>(4), 2804–2816. (<a
href="https://doi.org/10.1109/TCC.2021.3059096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The elasticity of cloud resources allows cloud clients to expand and shrink their demand for resources dynamically over time. However, fluctuations in the resource demands and pre-defined size of virtual machines (VMs) lead to lack of resource utilization, load imbalance, and excessive power consumption. To address these issues and to improve the performance of data center, an efficient resource management framework is proposed, which anticipates resource utilization of the servers and balances the load accordingly. It facilitates power saving, by minimizing the number of active servers, VM migrations, and maximizing the resource utilization. An online resource prediction system, is developed and deployed at each VM to minimize the risk of Service Level Agreement (SLA) violations and performance degradation due to under/overloaded servers. In addition, multi-objective VM placement and migration algorithms are proposed to reduce the network traffic and power consumption within data center. The proposed framework is evaluated by executing experiments on three real world workload datasets namely, Google Cluster dataset, Planet Lab, and Bitbrains VM traces. The comparison of proposed framework with the state-of-the-art approaches reveals its superiority in terms of different performance metrics. The improvement in power saving achieved by OP-MLB framework is upto 85.3 percent over the Best-Fit approach.},
  archive      = {J_TCC},
  author       = {Deepika Saxena and Ashutosh Kumar Singh and Rajkumar Buyya},
  doi          = {10.1109/TCC.2021.3059096},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2804-2816},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {OP-MLB: An online VM prediction-based multi-objective load balancing framework for resource management at cloud data center},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On using physical programming for multi-domain SFC placement
with limited visibility. <em>TCC</em>, <em>10</em>(4), 2787–2803. (<a
href="https://doi.org/10.1109/TCC.2020.3046997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service Function Chaining (SFC) is a networking concept by which traffic is steered through a set of ordered functions composing an end-to-end service. It represents one of the facilitating technologies for 5G, and is enabled by the Network Function Virtualization (NFV) and Software Defined Networks (SDN) paradigms. In the multi-domain context, SFC placement faces new challenges related to the lack of visibility on the local domain&#39;s networks. Indeed, the domain operators are often reluctant to unveil details on their topology to external parties. Furthermore, the new 5G use cases introduce new requirements for services such as end-to-end latency, and a minimal guaranteed bandwidth that the placement process needs to optimize simultaneously. In this article, we propose a centralized framework that allows SFC partitioning and embedding over multiple domains with a limited visibility over the global infrastructure. We model the multi-objective SFC placement problem using the Physical Programming method, which allows the expression of the Decision Maker&#39;s preferences through meaningful parameters, and propose an exact algorithm as well as a scalable heuristic solution. We then perform an extensive evaluation of the framework as well as the proposed algorithms. The results demonstrate our solution&#39;s effectiveness with a limited visibility on the network.},
  archive      = {J_TCC},
  author       = {Nassima Toumi and Olivier Bernier and Djamal-Eddine Meddour and Adlen Ksentini},
  doi          = {10.1109/TCC.2020.3046997},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2787-2803},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {On using physical programming for multi-domain SFC placement with limited visibility},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MultiScaler: A multi-loop auto-scaling approach for
cloud-based applications. <em>TCC</em>, <em>10</em>(4), 2769–2786. (<a
href="https://doi.org/10.1109/TCC.2020.3031676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing offers a wide range of services through a pool of heterogeneous Physical Machines (PMs) hosted on cloud data centers, where each PM can host several Virtual Machines (VMs). Resource sharing among VMs comes with major benefits, but it can create technical challenges that have a detrimental effect on the performance. To ensure a specific service level requested by the cloud-based applications, there is a need for an approach to assign adequate resources to each VM. To this end, we present our novel Multi-Loop Control approach, called MultiScaler , to allocate resources to VMs based on the Service Level Agreement (SLA) requirements and the run-time conditions. MultiScaler is mainly composed of three different levels working closely with each other to achieve an optimal resource allocation. We propose a set of tailor-made controllers to monitor VMs and take actions accordingly to regulate contention among collocated VMs, to reallocate resources if required, and to migrate VMs from one PM to another. The evaluation in a VMware cluster have shown that the MultiScaler approach can meet applications performance goals and guarantee the SLA by assigning the exact resources that the applications require. Compared with sophisticated baselines, MultiScaler produces significantly better reaction to changes in workloads even under the presence of noisy neighbors.},
  archive      = {J_TCC},
  author       = {Auday Al-Dulaimy and Javid Taheri and Andreas Kassler and M. Reza HoseinyFarahabady and Shuiguang Deng and Albert Zomaya},
  doi          = {10.1109/TCC.2020.3031676},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2769-2786},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {MultiScaler: A multi-loop auto-scaling approach for cloud-based applications},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-framework reliability approach. <em>TCC</em>,
<em>10</em>(4), 2750–2768. (<a
href="https://doi.org/10.1109/TCC.2021.3065906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite advances in making datacenters dependable, failures still happen. This is particularly onerous for long-running “big data” applications, where partial failures can lead to significant losses and lengthy recomputations. Big data processing frameworks like Hadoop MapReduce include fault tolerance (FT) mechanisms, but these are commonly targeted at specific system/failure models, and are often redundant between frameworks. This article proposes the paradigm of dependable resources : big data processing frameworks are typically built on top of resource management systems (RMSs), and proposing FT support at the level of such an RMS yields generic FT mechanisms, which can be provided with low overhead by leveraging constraints on resources. We demonstrate our concepts through Guardian, a robust RMS based on Mesos and YARN. Guardian allows frameworks to run their applications with individually configurable FT granularity and degree, with only minor changes to their implementation. We demonstrate the benefits of our approach by evaluating Hadoop, Tez, Spark and Pig on a prototype of Guardian running on Amazon-EC2, improving completion time by around 68 percent in the presence of failures, while maintaining around 6 percent overhead.},
  archive      = {J_TCC},
  author       = {Bara Abusalah and Derek Schatzlein and Julian James Stephen and Masoud Saeida Ardekani and Patrick Eugster},
  doi          = {10.1109/TCC.2021.3065906},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2750-2768},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Multi-framework reliability approach},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Maximum profit of real-time IoT content retrieval by joint
content placement and storage allocation in c-RANs. <em>TCC</em>,
<em>10</em>(4), 2739–2749. (<a
href="https://doi.org/10.1109/TCC.2020.3047017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Cloud Radio Access Networks (C-RANs), the performance of real-time Internet of Things (IoT) content retrieval is improved by placing/requesting the contents into/from remote radio heads (RRHs) and baseband units (BBUs). In the previous studies for the problem in jointly placing user contents and allocating storage allocation of RRHs and BBUs, a strategy of minimizing system resource consumption or transmission delay was used. In this article, we adopt a distinct strategy to maximize the profit of content retrieval services under the constraints of meeting the real-time requirements of users and the limited system resources in C-RANs. The problem is formulated as integer linear programming (ILP). Then, an algorithm for solving the ILP is proposed, and it can provide an approximate solution close to the optimal one with a bounded factor. In the simulation conducted, the results verified the above claims. Further, another algorithm is proposed to effectively expand the storage budgets in C-RANs. By controlling a bounded factor, not only the system performance is guaranteed, but also the upper limit of the expanded storage budget is provided. The smaller the bounding factor, the stricter the performance guarantee, but the expected storage budget will increase.},
  archive      = {J_TCC},
  author       = {Chia-Cheng Hu and Jeng-Shyang Pan},
  doi          = {10.1109/TCC.2020.3047017},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2739-2749},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Maximum profit of real-time IoT content retrieval by joint content placement and storage allocation in C-RANs},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LPGA: Location privacy-guaranteed offloading algorithm in
cache-enabled edge clouds. <em>TCC</em>, <em>10</em>(4), 2729–2738. (<a
href="https://doi.org/10.1109/TCC.2020.3030817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computation offloading, where Internet of Things (IoT) devices transfers their task to an external cloud, has several advantages such as low energy consumption of IoT devices and fast response time. To maximize these advantages, IoT devices can exploit the nearest edge cloud. However, frequent offloadings to the nearest edge cloud can cause a location privacy vulnerability due to the proximity of the edge cloud from IoT devices, which is a critical issue in smart city IoT applications. To address this problem, we propose a location privacy-guaranteed offloading algorithm (LPGA) in cache-enabled edge cloud environments. In LPGA, an IoT device decides where to offload the task (i.e., edge cloud or central cloud) with the consideration of the privacy level on its location and the cache hit probability. To minimize the generated traffic volume while maintaining low energy outage probability and providing a sufficient level of location privacy, a constrained Markov decision process (CMDP) problem is developed and it is converted into an equivalent linear programming (LP) model to achieve the optimal policy for offloading. Evaluation results demonstrate LPGA can reduce the traffic volume up to 39 percent compared to a central cloud-based offloading scheme while maintaining the energy outage probability below a certain level and providing required location privacy level.},
  archive      = {J_TCC},
  author       = {Haneul Ko and Hochan Lee and Taeyun Kim and Sangheon Pack},
  doi          = {10.1109/TCC.2020.3030817},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2729-2738},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {LPGA: Location privacy-guaranteed offloading algorithm in cache-enabled edge clouds},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LossPass: Absorbing microbursts by packet eviction for data
center networks. <em>TCC</em>, <em>10</em>(4), 2717–2728. (<a
href="https://doi.org/10.1109/TCC.2021.3054664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A bursty traffic pattern, called the microburst, is a key hurdle to achieve low latency for user-facing applications because it causes excessive packet losses in shallow buffered switches. Explicit Congestion Notification (ECN) can absorb microbursts by reserving buffer headroom, but the existence of headroom results in a fundamental trade-off between latency and throughput. To this end, we present LossPass, a buffer sharing mechanism that absorbs microbursts as many as possible while maintaining line-rate throughput. Specifically, LossPass evicts buffered large flow packets to make free buffer space on demand for arriving small flow packets. Our solution is inexpensive to implement on hardware. We implement a LossPass prototype and evaluate its performance through extensive testbed experiments and large-scale simulations. Our evaluation results show that LossPass reduces the FCT of small flows while maintaining line-rate throughput. For example, in testbed experiments, LossPass outperforms ECN by up to $3.20\times$ in the 99th percentile FCT of small flows.},
  archive      = {J_TCC},
  author       = {Gyuyeong Kim and Wonjun Lee},
  doi          = {10.1109/TCC.2021.3054664},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2717-2728},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {LossPass: Absorbing microbursts by packet eviction for data center networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight public/private auditing scheme for
resource-constrained end devices in cloud storage. <em>TCC</em>,
<em>10</em>(4), 2704–2716. (<a
href="https://doi.org/10.1109/TCC.2020.3045806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data integrity protection, an important feature in cloud storage services, can be achieved using auditing schemes. However, existing public and private auditing schemes are somewhat inefficient in practice. For example, in private auditing schemes, the trusted third-party is generally not able to settle disputes between users and the cloud storage server. In most existing public auditing schemes, it takes a user tens of seconds to generate data tags for every MB of file outsourced, which is clearly impractical particularly on resource-constrained end devices. Since the user is likely to have information than the auditor, we divide the verification phase into private verification and public verification phases. Then, we propose a public/private auditing model and a security model for public/private auditing. In our public/private auditing model, the user uses private verification phase to audit outsourced data promptly in most cases, and the auditor uses public verification phase to audit outsourced data only when dispute occurs or the user is not available to audit. Then, we propose a public/private auditing scheme, and present its security proof in the random oracle model under the discrete logarithm assumption. Experimental findings demonstrate that our scheme only need tens of microseconds to generate data tags for every MB file outsourced. In other words, the efficiency of our scheme is almost as high as existing high-performing private auditing schemes, and our scheme is more effective in comparison to existing efficient public auditing schemes.},
  archive      = {J_TCC},
  author       = {Feng Wang and Li Xu and Jiguo Li and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TCC.2020.3045806},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2704-2716},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Lightweight Public/Private auditing scheme for resource-constrained end devices in cloud storage},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inspecting edge data integrity with aggregate signature in
distributed edge computing environment. <em>TCC</em>, <em>10</em>(4),
2691–2703. (<a href="https://doi.org/10.1109/TCC.2021.3059448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the edge computing environment, app vendors can cache their data on a large number of geographically distributed edge servers to serve their users. However, those cached data are particularly vulnerable to both intentional and accidental corruption, which makes data security a major concern in the EC environment. Given limited computing resources of edge servers, how to effectively and efficiently inspect those data over tremendous edge servers is a critical and open problem. To tackle this edge data integrity (EDI) problem, we first study the entities, threats, system objectives, and the inspection mechanism, then propose a novel approach named EDI-S for inspecting the integrity of edge data and localizing the corrupted ones. Based on the elliptic curve cryptography, EDI-S generates one digital signature as the integrity proof for each replica. Then, multiple integrity proofs can be inspected altogether via an aggregate verification. This allows the integrity of tremendous cache data on multiple edge servers can be inspected more efficiently. EDI-S also provides two methods for localizing the corrupted data on edge servers, one for small-scale scenarios and the other for large-scale scenarios. Both theoretical analysis and experimentally evaluation demonstrate that EDI-S can solve the EDI problem effectively and efficiently.},
  archive      = {J_TCC},
  author       = {Bo Li and Qiang He and Feifei Chen and Hai Jin and Yang Xiang and Yun Yang},
  doi          = {10.1109/TCC.2021.3059448},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2691-2703},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Inspecting edge data integrity with aggregate signature in distributed edge computing environment},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hy-FiX: Fast in-place upgrades of KVM hypervisors.
<em>TCC</em>, <em>10</em>(4), 2679–2690. (<a
href="https://doi.org/10.1109/TCC.2021.3056590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining up-to-date KVM hypervisors requires regular upgrades to the host kernel, hence rebooting the physical host with the consequent termination of running Virtual Machines (VMs). Cloud platforms capable of massive large-scale live migrations evacuate VMs from the hosts before rebooting, minimizing the impact over VM up-time. However, scenarios exist where resource constraints make live migration undesirable, or the presence of fault-tolerant instances (e.g., replicated services) favors the adoption of VM termination, a simpler but more disruptive strategy. In this article, we present Hy-FiX, a fast in-place upgrade solution for KVM hypervisors. Hy-FiX preserves VM memory across host reboots, protecting the execution state of running guests while hypervisor upgrades are applied. Hy-FiX memory preservation across reboot, combined with a mixed suspend-to-disk/suspend-to-RAM technique, achieves a 2.31-second checkpoint/restore time for a 256 GB VM, and Hy-FiX lazy memory initialization reboots an enterprise-class host in constant time (7.6 seconds) regardless of its equipped memory. Hy-FiX is, therefore, a better alternative to classical VM termination and restart.},
  archive      = {J_TCC},
  author       = {Andrea Segalini and Dino Lopez Pacheco and Guillaume Urvoy-Keller and Fabien Hermenier and Quentin Jacquemart},
  doi          = {10.1109/TCC.2021.3056590},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2679-2690},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Hy-FiX: Fast in-place upgrades of KVM hypervisors},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HYDRA: Decentralized location-aware orchestration of
containerized applications. <em>TCC</em>, <em>10</em>(4), 2664–2678. (<a
href="https://doi.org/10.1109/TCC.2020.3041465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The edge computing paradigm, spurred by the Internet-of-Things, poses new requirements and challenges for distributed application deployment. There is a need for an orchestrator design that leverages characteristics that enable this new paradigm. We present HYDRA, a decentralized and distributed orchestrator for containerized microservice applications. This orchestrator focuses on scalability and resiliency to enable the global manageability of cloud and edge environments. It can manage heterogeneous resources across geographical locations and provide robust application control. Further, HYDRA enables the location-aware deployment of microservice applications via containerization. Thus, an application&#39;s services may be deployed to separate locations according to expected needs. In this article, the experiments show the orchestrator scaling to 20000 nodes and simultaneously deploying 30000 applications. Further, empirical results show that location-aware application deployment does not hinder HYDRA&#39;s performance, and the random resource search algorithm currently being employed may be used as a baseline to find resources in this decentralized orchestrator. Therefore, we conclude that HYDRA is a viable orchestrator design for the new computing paradigm.},
  archive      = {J_TCC},
  author       = {Lara Lorna Jiménez and Olov Schelén},
  doi          = {10.1109/TCC.2020.3041465},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2664-2678},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {HYDRA: Decentralized location-aware orchestration of containerized applications},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HPSTOS: High-performance and scalable traffic optimization
strategy for mixed flows in data center networks. <em>TCC</em>,
<em>10</em>(4), 2649–2663. (<a
href="https://doi.org/10.1109/TCC.2021.3063469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data center networks, traffic needs to be distributed among different paths using traffic optimization strategies for mixed flows. Most of the existing strategies consider either distributed or centralized mechanisms to optimize the latency of mice flows or the throughput of elephant flows. However, low network performance and scalability issues are intrinsic limitations of both strategies. In addition, the current elephant flow detection methods are inefficient. In this article, we propose a high-performance and scalable traffic optimization strategy (HPSTOS) based on a hybrid approach that leverages the advantages of both centralized and distributed mechanisms. HPSTOS improves the efficiency of elephant flow detection through sampling and flow-table identification. HPSTOS guarantees preferential transmission of mice flows using priority scheduling and adjusts their transmission rate by coding-based congestion control on the end-host, reducing their latency. Additionally, HPSTOS schedules elephant flows by cost-aware dynamic flow scheduling on a centralized controller to improve their throughput. The controller handles only elephant flows, which constitutes the minority of the flows, allowing effective scalability. Evaluations show that HPSTOS outperforms existing schemes by realizing efficient elephant flow detection and improving network performance and scalability.},
  archive      = {J_TCC},
  author       = {Yong Liu and Huaxi Gu and Ning Wang},
  doi          = {10.1109/TCC.2021.3063469},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2649-2663},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {HPSTOS: High-performance and scalable traffic optimization strategy for mixed flows in data center networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GSSP: Eliminating stragglers through grouping synchronous
for distributed deep learning in heterogeneous cluster. <em>TCC</em>,
<em>10</em>(4), 2637–2648. (<a
href="https://doi.org/10.1109/TCC.2021.3062398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed deep learning has been widely used in training deep neural networks, especially for big models on massive datasets. Parameter Server (PS) architecture is the most popular distributed training framework, which can flexibly design the global parameter update manner. However, when scaling to complex heterogeneous clusters, stragglers make it difficult for existing distributed paradigms on PS framework to balance between synchronous waiting and staleness, which slows down the model training sharply. In this article, we propose Grouping Stale Synchronous Parallel (GSSP) scheme, which groups workers with similar performance together. Group servers coordinate intra-group workers using Stale Synchronous Parallel while they communicate with each other asynchronously to eliminate stragglers and refine the model weights. We further propose Grouping Dynamic Tok-K Sparsification (GDTopK), which dynamically adjusts the upload ratio for each group so as to make communication volume differentiated and mitigate inter-group iteration speed gap. We have conducted experiments on LeNet-5 on MNIST, ResNet-18, VGG-19 on Cifar-10, and Seq2Seq on Multi30k. Results show that GSSP accelerates the training by 46\% $\sim$ 120\%, with less than 1 percent accuracy drop. And GDTopK can make up for part of the lost accuracy.},
  archive      = {J_TCC},
  author       = {Haifeng Sun and Zhiyi Gui and Song Guo and Qi Qi and Jingyu Wang and Jianxin Liao},
  doi          = {10.1109/TCC.2021.3062398},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2637-2648},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {GSSP: Eliminating stragglers through grouping synchronous for distributed deep learning in heterogeneous cluster},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extending kubernetes clusters to low-resource edge devices
using virtual kubelets. <em>TCC</em>, <em>10</em>(4), 2623–2636. (<a
href="https://doi.org/10.1109/TCC.2020.3033807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, containers have gained popularity as a lightweight virtualization technology. This rise in popularity has gone hand in hand with the adoption of microservice architectures, mostly thanks to the scalable, ethereal, and isolated nature of containers. More recently, edge devices have become powerful enough to be able to run containerized microservices, while remaining flexible enough in terms of size and power to be deployed almost anywhere. This has triggered research into several container placement strategies involving edge networks, leading to concepts such as osmotic computing. While these container placement strategies are optimal in terms of workload placement, current container orchestrators are often not suitable for running on edge devices due to their high resource requirements. In this article, FLEDGE is presented as a Kubernetes-compatible container orchestrator based on Virtual Kubelets, aimed primarily at container orchestration on low-resource edge devices. Several aspects of low-resource container orchestration are examined, such as the choice of container runtime and how to realize container networking. A number of evaluations are performed to determine how FLEDGE compares to Kubernetes and K3S in terms of resource requirements, showing that it needs around 60MiB memory and 78MiB storage to run on a Raspberry Pi 3, including all dependencies, which is significantly less than both studied alternatives.},
  archive      = {J_TCC},
  author       = {Tom Goethals and Filip De Turck and Bruno Volckaert},
  doi          = {10.1109/TCC.2020.3033807},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2623-2636},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Extending kubernetes clusters to low-resource edge devices using virtual kubelets},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluation of the performance of tightly coupled parallel
solvers and MPI communications in IaaS from the public cloud.
<em>TCC</em>, <em>10</em>(4), 2613–2622. (<a
href="https://doi.org/10.1109/TCC.2021.3052844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IaaS from the public cloud is becoming a new option for organizations in need of HPC capabilities. IaaS offers greater flexibility in hardware choices and operational costs. Furthermore, IaaS enables small organizations to access resources previously reserved for organizations with larger capitals. This article uses the HPCG benchmark to assess the performance of parallel solvers, which are critical in computational engineering, and microbenchmarks to measure collective MPI operations. The benchmarks encompass IaaS from five cloud vendors (AWS, Google Cloud Platform, Azure, Oracle CIoud Infrastructure, and Packet), and two architectures, ARM and x86_64. The benchmarks cover clusters with up to 4500 cores and illustrate the benefits of higher network bandwidth when scaling up clusters. The results for some of the clusters are particularly promising as they exhibit good scalability and compare well to on-premises supercomputers. Additionally, the study includes a preliminary cost estimate based on on-demand prices for IaaS computational power and memory.},
  archive      = {J_TCC},
  author       = {Arturo Fernandez},
  doi          = {10.1109/TCC.2021.3052844},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2613-2622},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Evaluation of the performance of tightly coupled parallel solvers and MPI communications in IaaS from the public cloud},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhanced semantic-aware multi-keyword ranked search scheme
over encrypted cloud data. <em>TCC</em>, <em>10</em>(4), 2595–2612. (<a
href="https://doi.org/10.1109/TCC.2020.3047921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional searchable encryption schemes based on the Term Frequency-Inverse Document Frequency (TF-IDF) model adopt the presence of keywords to measure the relevance of documents to queries, which ignores the latent semantic meanings that are concealed in the context. Latent Dirichlet Allocation (LDA) topic model can be utilized for modeling the semantics among texts to achieve semantic-aware multi-keyword search. However, the LDA topic model treats queries and documents from the perspective of topics, and the keywords information is ignored. In this article, we propose a privacy-preserving searchable encryption scheme based on the LDA topic model and the query likelihood model. We extract the feature keywords from the document using the LDA-based Information Gain (IG) and Topic Frequency-Inverse Topic Frequency (TF-ITF) model. With feature keyword extraction and the query likelihood model, our scheme can achieve a more accurate semantic-aware keyword search. A special index tree is used to enhance search efficiency. The secure inner product operation is utilized to implement the privacy-preserving ranked search. The experiments on real-world datasets demonstrate the effectiveness of our scheme.},
  archive      = {J_TCC},
  author       = {Xuelong Dai and Hua Dai and Chunming Rong and Geng Yang and Fu Xiao and Bin Xiao},
  doi          = {10.1109/TCC.2020.3047921},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2595-2612},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Enhanced semantic-aware multi-keyword ranked search scheme over encrypted cloud data},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enabling secure and versatile packet inspection with
probable cause privacy for outsourced middlebox. <em>TCC</em>,
<em>10</em>(4), 2580–2594. (<a
href="https://doi.org/10.1109/TCC.2021.3059026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Middlebox is an intermediary network equipment which can be outsourced to remote cloud servers for low-cost and customizable network services, such as load balancer and intrusion detection. A fundamental function of the middlebox is packet inspection, where both the packet header and payload are extracted and analyzed based on inspection rules. However, as the packet may contain sensitive individual or organizational information, it may raise severe privacy concerns without proper countermeasures. In this article, we propose a secure and versatile packet inspection scheme for outsourced middlebox. The proposed scheme builds upon two non-collusion cloud servers, where the first server conducts the inspection task over the ciphertext domain and the second reveal the inspection results. By doing so, the proposed scheme achieves versatile inspection functionalities: range-query-based header inspection and token-based payload inspection, while preserving the privacy of packet header, payload, and inspection rules. Moreover, we identify and address two challenging issues in the state-of-the-art literatures. First, we tailor the design of mis-operation resistant searchable homomorphic encryption (MR-SHE) and somewhat homomorphic encryption in the two-server model, to resist offline dictionary attack on payload headers . Second, we propose a key management mechanism with compelled access for the middlebox, to achieve fine-grained probable cause privacy . We also conduct extensive experiments and compare the results with existing schemes to demonstrate the feasibility of the proposed scheme.},
  archive      = {J_TCC},
  author       = {Hao Ren and Hongwei Li and Dongxiao Liu and Guowen Xu and Xuemin Sherman Shen},
  doi          = {10.1109/TCC.2021.3059026},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2580-2594},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Enabling secure and versatile packet inspection with probable cause privacy for outsourced middlebox},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient online scheduling for coflow-aware machine
learning clusters. <em>TCC</em>, <em>10</em>(4), 2564–2579. (<a
href="https://doi.org/10.1109/TCC.2020.3040312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed machine learning (DML) is an increasingly important workload. In a DML job, each communication phase can comprise a coflow , and there are dependencies among its coflows. Thus, efficient coflow scheduling becomes critical for DML jobs. However, the majority of existing solutions focus on scheduling single-stage coflows with no dependencies. While there are a few studies schedule dependent coflows of multi-stage jobs, they suffer from either practical or theoretical issues. Motivated by this situation, we study how to schedule dependent coflows of multiple DML jobs to minimize the total JCT in a shared cluster. We present a formal mathematical formulation for this problem and prove its NP-hardness. To solve this problem without job size information, we present an online coflow-aware optimization framework called Parrot . The core idea in Parrot is to infer the job with the shortest remaining processing time (SRPT) each time and dynamically control the inferred job&#39;s bandwidth based on how confident it is an SRPT job while being mindful of not starving any other job. Specifically, in the design of Parrot , we present a least per-coflow attained service (LPCAS) policy to infer the SRPT job. We further propose a dynamic job weight assignment mechanism and a linear program (LP) based weighted bandwidth scaling strategy for sharing bandwidth among DML jobs. We have proved that Parrot algorithm has a non-trivial competitive ratio. The results from large-scale trace-driven simulations further demonstrate that our Parrot can reduce the total JCT by up to 58.4 percent, compared to the state-of-the-art Aalo solution.},
  archive      = {J_TCC},
  author       = {Wenxin Li and Sheng Chen and Keqiu Li and Heng Qi and Renhai Xu and Song Zhang},
  doi          = {10.1109/TCC.2020.3040312},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2564-2579},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficient online scheduling for coflow-aware machine learning clusters},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient algorithms for multi-component application
placement in mobile edge computing. <em>TCC</em>, <em>10</em>(4),
2550–2563. (<a href="https://doi.org/10.1109/TCC.2020.3038626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we address the Multi-Component Application Placement Problem ( ${\sf MCAPP}$ ) in Mobile Edge Computing (MEC) systems. We formulate this problem as a Mixed Integer Non-Linear Program (MINLP) with the objective of minimizing the total cost of running the applications. In our formulation, we take into account two important and challenging characteristics of MEC systems, the mobility of users and the network capabilities. We analyze the complexity of ${\sf MCAPP}$ and prove that it is $NP$ -hard, that is, finding the optimal solution in reasonable amount of time is infeasible. We design two algorithms, one based on matching and local search and one based on a greedy approach, and evaluate their performance by conducting an extensive experimental analysis driven by two types of user mobility models, real-life mobility traces and random-walk. The results show that the proposed algorithms obtain near-optimal solutions and require small execution times for reasonably large problem instances.},
  archive      = {J_TCC},
  author       = {Tayebeh Bahreini and Daniel Grosu},
  doi          = {10.1109/TCC.2020.3038626},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2550-2563},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficient algorithms for multi-component application placement in mobile edge computing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficiency and fairness in resource exchange. <em>TCC</em>,
<em>10</em>(4), 2538–2549. (<a
href="https://doi.org/10.1109/TCC.2021.3066291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of Internet, plenty of applications has been implemented for users to exchange resources with each other over networks. As a result, it has been a critical challenge to efficiently and fairly allocate resource among participating agents. Motivated by the famous BitTorrent system, a BD Mechanism for resource exchange allocation is proposed, which is based on a combinatorial structure called bottleneck decomposition . The mechanism has been shown to lead to market equilibrium [1], a state optimizing each individual&#39;s utility, and to be truthful [2], [3], that is robust against agents’ strategic behaviors. However, the crux on how to compute a bottleneck decomposition of any graph remains untouched. In this article, we focus on the computation of bottleneck decomposition to fill the blanks and prove that the bottleneck decomposition of a network can be computed in $O(n^6\,\log (nU))$ time, where $n$ and $U$ are the number of agents and the maximal amount of resource any agent has in the network. Based on the bottleneck decomposition, a fair allocation in a resource exchange network can be obtained in polynomial time. We further carefully discuss the fairness in resource exchange scenario, showing the derived allocation satisfies several common fairness notions simultaneously.},
  archive      = {J_TCC},
  author       = {Xiang Yan and Wei Zhu},
  doi          = {10.1109/TCC.2021.3066291},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2538-2549},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficiency and fairness in resource exchange},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Edge AR x5: An edge-assisted multi-user collaborative
framework for mobile web augmented reality in 5G and beyond.
<em>TCC</em>, <em>10</em>(4), 2521–2537. (<a
href="https://doi.org/10.1109/TCC.2020.3046128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-user mobile Augmented Reality (AR) has been successfully used in various fields as a novel visual interaction technology. But current mainstream wearable device-based and app-based solutions are still facing cross-platform, real-time communication, and intensive computing requirements. Mobile Web technology is envisioned to be a promising supporting technology for cross-platform application of mobile AR especially in 5G networks, which provide pervasive communication and computing resources thereby forming a formidable framework for the practical application of multi-user mobile Web AR. However, the problem of how to use these new techniques properly to achieve efficient communication and computing collaboration is obviously paramount in order for multi-user mobile Web AR to be realized in 5G networks. In this article, we propose the first edge-assisted multi-user collaborative framework for mobile Web AR in the 5G era. First, we propose a heuristic mechanism BA-CPP for efficient communication planning, which allows multi-user interaction synchronization to be achieved. Second, we introduce a motion-aware key frame selection mechanism called Mo-KFP to optimize the computational efficiency of the edge system, and simultaneously alleviate the initialization problem by collaborating with nearby mobile devices using the Device-to-Device (D2D) communication technique. Experiments are conducted in a real-world 5G network, and the results demonstrate the superiority of our proposed collaborative framework.},
  archive      = {J_TCC},
  author       = {Pei Ren and Xiuquan Qiao and Yakun Huang and Ling Liu and Calton Pu and Schahram Dustdar and Junliang Chen},
  doi          = {10.1109/TCC.2020.3046128},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2521-2537},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Edge AR x5: An edge-assisted multi-user collaborative framework for mobile web augmented reality in 5G and beyond},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Duplicacy: A new generation of cloud backup tool based on
lock-free deduplication. <em>TCC</em>, <em>10</em>(4), 2508–2520. (<a
href="https://doi.org/10.1109/TCC.2020.3047403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pervasive deployment of cloud services poses an ever-increasing demand for cross-client deduplication solutions to save network bandwidth, lower storage costs, and improve backup speeds. However, existing solutions typically depend on lock-based approaches relying on a centralized chunk database, which tends to hinder performance scalability. In this article, we present a new cross-client cloud backup solution, named Duplicacy, based on a Lock-Free Deduplication approach. Lock-Free Deduplication stores chunks to network or cloud storage using content hashes as file names. It then adopts a two-step fossil deletion algorithm to solve the hard problem of deleting unreferenced chunks in the presence of concurrent backups, without the need for any locks. Experiments demonstrate that Duplicacy enables significant performance improvement for backups over previous well-known backup tools. In addition, Duplicacy can work with many general-purpose network or cloud storage services which only support a basic set of file operations, and turn them into sophisticated deduplication-aware storage servers without server-side changes.},
  archive      = {J_TCC},
  author       = {Zonghui Li and Gilbert Chen and Yangdong Deng},
  doi          = {10.1109/TCC.2020.3047403},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2508-2520},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Duplicacy: A new generation of cloud backup tool based on lock-free deduplication},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DMORA: Decentralized multi-SP online resource allocation
scheme for mobile edge computing. <em>TCC</em>, <em>10</em>(4),
2497–2507. (<a href="https://doi.org/10.1109/TCC.2020.3044852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) can significantly reduce latency by pushing resources away from remote clouds to distributed base stations (BSs) equipped with MEC servers, which are closer to users and deployed by service providers (SPs) at the edge of cellular networks. To improve user experience and increase their own revenue, SPs tend to use resources in their deployed BSs to provide services instead of using resources in BSs deployed by other SPs. We envision a densely-deployed multi-SP MEC network where a user equipment (UE) is covered by multiple BSs from different SPs. As the resource in BSs and MEC servers is limited, it is a challenging problem for SPs to reasonably allocate resources in the edge computing (EC) layer to improve the quality of service. In this article, we propose a novel resource allocation scheme, Decentralized Multi-SP Resource Allocation (DMRA), which aims to maximize the total profit of all SPs at the EC layer and provide high-quality services. Then, we extend our design to the online scheme. The algorithm Decentralized Multi-SP Online Resource Allocation (DMORA) is proposed to fit the dynamic network environment. Simulation results indicate that our proposed schemes can effectively maximize the total profit of all SPs at the EC layer while improving user experience.},
  archive      = {J_TCC},
  author       = {Chen Zhang and Hongwei Du},
  doi          = {10.1109/TCC.2020.3044852},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2497-2507},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {DMORA: Decentralized multi-SP online resource allocation scheme for mobile edge computing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed traffic engineering for multi-domain SDN without
trust. <em>TCC</em>, <em>10</em>(4), 2481–2496. (<a
href="https://doi.org/10.1109/TCC.2021.3067456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In software defined networking, the flat design of distributed control plane enables the management of multi-domain networks that are incapable of deploying a root controller. However, it is very difficult to avoid policy conflicts between independent local controllers due to the lack of centralized arbitration. Moreover, domains without trust may not be always cooperative and could even cheat to maximize their own interests. In this article, we first consider the cooperative scenario and address the problem of traffic engineering in a flat distributed control plane. We propose a fully distributed algorithm, called DisTE , which can provide max-min fair bandwidth allocation for flows and maximize resource utilization. DisTE also preserves the local topology of each domain and achieves policy consistency by multiple rounds of synchronization. We then consider the non-cooperative scenario, where selfish domains may discriminate bandwidth requests from other domains or overstate theirs owns to squeeze more bandwidths.},
  archive      = {J_TCC},
  author       = {Yangyang Liu and Laiping Zhao and Jingyu Hua and Wenyu Qu and Suohao Zhang and Sheng Zhong},
  doi          = {10.1109/TCC.2021.3067456},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2481-2496},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Distributed traffic engineering for multi-domain SDN without trust},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed energy management for multiple data centers with
renewable resources and energy storages. <em>TCC</em>, <em>10</em>(4),
2469–2480. (<a href="https://doi.org/10.1109/TCC.2020.3031881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For Internet and cloud computing service providers, running massive geo-distributed data centers incurs prodigious electricity cost and water consumption as well as carbon emission rooted in electricity generation. Thus, it is critical significant for providers to lower down the operation cost of data centers. In this article, we investigate the problem of energy management for geo-distributed data centers with renewable resources and energy storages. We aim to minimize the long-term operation cost including electricity cost, water consumption, and carbon emission by leveraging the spatiotemporal diversity of these system states. To this end, we first formulate the cost minimization problem as a stochastic optimization problem, then we adopt the Lyapunov optimization technique to design a close-to-optimal online algorithm which only needs the current system information and achieves a delicate tradeoff between system cost and performance of delay tolerant workloads. To reduce the computational complexity and unnecessary communication, we further propose a distributed algorithm based on the distributed computing framework alternating direction method of multipliers (ADMM), which enables each data center to make their own control decisions. Based on the real-world traces and extensive simulations, we demonstrate the effectiveness of our proposed algorithms.},
  archive      = {J_TCC},
  author       = {Guanglin Zhang and Shun Zhang and Wenqian Zhang and Zhirong Shen and Lin Wang},
  doi          = {10.1109/TCC.2020.3031881},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2469-2480},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Distributed energy management for multiple data centers with renewable resources and energy storages},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dependency-aware computation offloading for mobile edge
computing with edge-cloud cooperation. <em>TCC</em>, <em>10</em>(4),
2451–2468. (<a href="https://doi.org/10.1109/TCC.2020.3037306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of existing Multi-access edge computing (MEC) studies consider the remote cloud server as a special edge server, the opportunity of edge-cloud collaboration has not been well exploited. We propose a dependency-aware offloading scheme in MEC with edge-cloud cooperation under task dependency constraints. Each mobile device has a limited budget and has to determine which sub-task should be computed locally or should be sent to the edge or remote cloud. To address this issue, we divide the offloading problem into two application finishing time minimization sub-problems with two different cooperation modes, both of which are proved to be NP-hard. We then devise one greedy algorithm with approximation ratio of $1+\epsilon$ for the first mode with edge-cloud cooperation but no edge-edge cooperation. Then we design an efficient greedy algorithm for the second mode, considering both edge-cloud and edge-edge co-operations. Extensive simulation results show that for the first mode, the proposed greedy algorithm achieves near optimal performance for typical task topologies. On average, it outperforms the modified Hermes benchmark algorithm by about $23\%\sim 43.6\%$ in terms of application finishing time with given budgets. By further exploiting collaborations among edge servers in the second cooperation mode, the proposed algorithm helps to achieve over 20.3 percent average performance gain on the application finishing time over the first mode under various scenarios. Real-world experiments comply with simulation results.},
  archive      = {J_TCC},
  author       = {Long Chen and Jigang Wu and Jun Zhang and Hong-Ning Dai and Xin Long and Mianyang Yao},
  doi          = {10.1109/TCC.2020.3037306},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2451-2468},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Dependency-aware computation offloading for mobile edge computing with edge-cloud cooperation},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CrypSH: A novel IoT data protection scheme based on BGN
cryptosystem. <em>TCC</em>, <em>10</em>(4), 2437–2450. (<a
href="https://doi.org/10.1109/TCC.2021.3050953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) is an emerging paradigm and has penetrated deeply into our daily life. Due to the seamless connections of the IoT devices with the physical world through the Internet, the IoT applications use the cloud to store and provide ubiquitous access to collected data. Sharing of data with third party services and other users incurs potential risks and leads to unique security and privacy concerns, e.g., data breaches. Existing cryptographic solutions are inapt for resource-constrained IoT devices, because of their significant computational overhead. To address these concerns, we propose a data protection scheme to store the encrypted IoT data in a cloud, while still allowing query processing over the encrypted data. Our proposed scheme features a novel encrypted data sharing scheme based on Boneh-Goh-Nissim (BGN) cryptosystem, with revocation capabilities and in-situ key updates. We perform exhaustive experiments on real datasets, to assess the feasibility of the proposed scheme on the resource constrained IoT devices. The results show the feasibility of our scheme, together with the ability to provide a high level of security. The results also show that our scheme significantly reduces the computation, storage and energy overheads than the best performed scheme in the state-of-the-art.},
  archive      = {J_TCC},
  author       = {Subir Halder and Mauro Conti},
  doi          = {10.1109/TCC.2021.3050953},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2437-2450},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {CrypSH: A novel IoT data protection scheme based on BGN cryptosystem},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CPU: Cross-rack-aware pipelining update for erasure-coded
storage. <em>TCC</em>, <em>10</em>(4), 2424–2436. (<a
href="https://doi.org/10.1109/TCC.2020.3035526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Erasure coding is widely used in distributed storage systems (DSSs) to efficiently achieve fault tolerance. However, when the original data need to be updated, erasure coding must update every encoded block, resulting in long update time and high bandwidth consumption. Exiting solutions are mainly focused on coding schemes to minimize the size of transmitted update information, while ignoring more efficient utilization of bandwidth among update racks. In this article, we propose a parallel Cross-rack Pipelining Update scheme ( CPU ), which divides the update information into small-size units and transmits these units in parallel along with an update pipeline path among multiple racks. The performance of CPU is mainly determined by slice size and update path. More slices bring finer-grained parallel transmissions over cross-rack links, but also introduces more overheads. An update path that traverses all racks with large-bandwidth links provide short update time. We formulate the proposed pipelining update scheme as an optimization problem, based on a new theoretical pipelining update model. We prove the optimization problem is NP-hard and develop a heuristic algorithm to solve it based on the features of practical DSSs and our implementations, including Big chunk and Small overhead . Specifically, we determine the best update path first by solving a max-min problem and then decide the slice size. We further simplify the slice size selection by offline learning a range of interesting ( RoI ), in which all slice sizes provide similar performance. We implement CPU and conduct experiments on Amazon EC2 under a variety of scenarios. The results show that CPU can reduce the average update time by 48.2 percent, compared with the state-of-the-art update schemes.},
  archive      = {J_TCC},
  author       = {Haiqiao Wu and Wan Du and Peng Gong and Dapeng Oliver Wu},
  doi          = {10.1109/TCC.2020.3035526},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2424-2436},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {CPU: Cross-rack-aware pipelining update for erasure-coded storage},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computational cost analysis and data-driven predictive
modeling of cloud-based online-NILM algorithm. <em>TCC</em>,
<em>10</em>(4), 2409–2423. (<a
href="https://doi.org/10.1109/TCC.2021.3051766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online non-intrusive load monitoring algorithms have captivated academia and industries as parsimonious solutions for household energy efficiency monitoring as well as a safety control, anomaly detection, and demand-side management. However, the computational energy cost for executing such algorithms should not overcome the promised energy efficiency from the disaggregated appliance specific consumption information feed-backs. Moreover, the energy efficiency of cloud computing systems is also becoming a concern for the environment due to carbon emission. This study analyzes the energy spent to execute NILM algorithms via computation cost estimation and prediction using computing system-level power monitoring and data-driven approaches. A generic framework for an automated algorithm cost monitoring and modeling methodologies is devised for large load scale deployment of Cloud-based Online-NILM algorithms. The efficacy of the proposed approach was examined and validated on two computing system use-cases, i.e., Dedicated Server and Cloud Virtual Server. The prediction models, developed using statistical and machine learning tools, demonstrate the promising applicability of the data-driven approach with a very high prediction accuracy without detailed knowledge of the computing systems and the algorithm.},
  archive      = {J_TCC},
  author       = {Mulugeta Weldezgina Asres and Luca Ardito and Edoardo Patti},
  doi          = {10.1109/TCC.2021.3051766},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2409-2423},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Computational cost analysis and data-driven predictive modeling of cloud-based online-NILM algorithm},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Chosen-ciphertext secure homomorphic proxy re-encryption.
<em>TCC</em>, <em>10</em>(4), 2398–2408. (<a
href="https://doi.org/10.1109/TCC.2020.3042432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Homomorphic Proxy Re-Encryption (HPRE) is an extension of Proxy Re-Encryption (PRE) which combines the advantages of both Homomorphic Encryption (HE) and PRE. A HPRE scheme allows arbitrary evaluations to be performed on ciphertexts under one (the delegator&#39;s) public key and, using a re-encryption key, it transforms the resulting ciphertext to a new ciphertext under another (the delegatee&#39;s) public key. Prior HPRE schemes are either CPA-secure or CCA-secure but only support partial homomorphic operations. We propose a generic construction of single-hop HPRE scheme which supports fully homomorphic operations. The proposed scheme is proven secure in our new index-based CCA-HPRE model. Our technique is to give a generic transformation that turns any multi-identity identity-based FHE (IBFHE) scheme with key switching into Fully Homomorphic Encryption (FHE) with key switching from which we can obtain the proposed single-hop HPRE scheme. We also present a concrete instantiation of multi-identity IBFHE with key switching from learning with errors (LWE) in the standard model.},
  archive      = {J_TCC},
  author       = {Fucai Luo and Saif Al-Kuwari and Willy Susilo and Dung Hoang Duong},
  doi          = {10.1109/TCC.2020.3042432},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2398-2408},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Chosen-ciphertext secure homomorphic proxy re-encryption},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Characterizing co-located workloads in alibaba cloud
datacenters. <em>TCC</em>, <em>10</em>(4), 2381–2397. (<a
href="https://doi.org/10.1109/TCC.2020.3034500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Workload characteristics are vital for both data center operation and job scheduling in co-located data centers, where online services and batch jobs are deployed on the same production cluster. In this article, a comprehensive analysis is conducted on Alibaba&#39;s cluster-trace-v2018 of a production cluster of 4034 machines. The findings and insights are the following: (1) The workload on the production cluster poses a daily cyclical fluctuation, in terms of CPU and disk I/O utilization, and the memory system has become the performance bottleneck of a co-located cluster. (2) Batch jobs including their tasks and derived instances can be approximated as Zipf distribution. However, for all batch jobs with directed acyclic graph dependency, they suffer from co-location with online services since the online services are highly prioritized. (3) The resource usages of containers have similar cyclical fluctuation consistent with the whole cluster, while their memory usages remain approximately constant. (4) The number of batch jobs co-located with online services is dependent on the mispredictions per kilo instructions of online services. In order to guarantee the QoS of online services, when the MPKI of online services rises, the number of batch jobs to be co-located on the same machine should decrease.},
  archive      = {J_TCC},
  author       = {Congfeng Jiang and Yitao Qiu and Weisong Shi and Zhefeng Ge and Jiwei Wang and Shenglei Chen and Christophe Cérin and Zujie Ren and Guoyao Xu and Jiangbin Lin},
  doi          = {10.1109/TCC.2020.3034500},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2381-2397},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Characterizing co-located workloads in alibaba cloud datacenters},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blockchain-based decentralized public auditing for cloud
storage. <em>TCC</em>, <em>10</em>(4), 2366–2380. (<a
href="https://doi.org/10.1109/TCC.2021.3051622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public auditing schemes for cloud storage systems have been extensively explored with the increasing importance of data integrity. A third-party auditor (TPA) is introduced in public auditing schemes to verify the integrity of outsourced data on behalf of users. To resist malicious TPAs, many blockchain-based public verification schemes have been proposed. However, existing auditing schemes rely on a centralized TPA, and they are vulnerable to tempting auditors who may collude with malicious blockchain miners to produce biased auditing results. In this article, we propose a blockchain-based decentralized public auditing (BDPA) scheme by utilizing a decentralized blockchain network to undertake the responsibility of a centralized TPA, and also mitigate the influence of tempting auditors and malicious blockchain miners by taking the concept of decentralized autonomous organization (DAO). A detailed security analysis shows that BDPA can preserve data integrity against tempting auditors and malicious blockchain miners. A comprehensive performance evaluation demonstrates that BDPA is feasible and scalable.},
  archive      = {J_TCC},
  author       = {Jiangang Shu and Xing Zou and Xiaohua Jia and Weizhe Zhang and Ruitao Xie},
  doi          = {10.1109/TCC.2021.3051622},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2366-2380},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Blockchain-based decentralized public auditing for cloud storage},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bidirectional and malleable proof-of-ownership for large
file in cloud storage. <em>TCC</em>, <em>10</em>(4), 2351–2365. (<a
href="https://doi.org/10.1109/TCC.2021.3054751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud storage is a cost-effective platform to accommodate massive data at low cost. However, advances of cloud services propel data generation, which pushes storage servers to its limit. Deduplication is a popular technique enjoyed by most current cloud servers, which detects and deletes redundant data to save storage and bandwidth. For security concerns, proof-of-ownership (PoW) can be used to guarantee ownership of data such that no malicious user could pass deduplication easily or utilize such mechanism for malicious purposes. Generally, PoW is implemented in static data archive where the data file is supposed to be read-only. However, to satisfy users’ needs for dynamical manipulation on data and support real-time data services, it is required to devise efficient PoW for dynamic archive. Inspired by malleable signature, which offers authentication even after its committed message changes, we propose the notion of bidirectional and malleable proof-of-ownership ( $\sf {BM\mbox{-}PoW}$ ) for the above challenge. Our proposed $\sf {BM\mbox{-}PoW}$ consists of bidirectional PoW ( ${\mbox{B-PoW}}$ ), malleable PoW ( ${\mbox{M-PoW}}$ ) and dispute arbitration protocol $\sf {DAP}$ . While our ${\mbox{B-PoW}}$ is proposed for a static setting, the ${\mbox{M-PoW}}$ caters specifically for dynamic manipulation of data. In addition, our proposed arbitration protocol $\sf {DAP}$ achieves accountable redaction which can arbitrate the originality of file ownership. We provide the security analysis of our proposal, and performance evaluation that suggests our proposed ${\mbox{B-PoW}}$ is secure and efficient for large file in static data archive. In addition, our proposed ${\mbox{M-PoW}}$ achieves acceptable performance under dynamic setting where data is supposed to be outsourced first and updated later in dynamic data archive.},
  archive      = {J_TCC},
  author       = {Ke Huang and Xiaosong Zhang and Yi Mu and Fatemeh Rezaeibagha and Xiaojiang Du},
  doi          = {10.1109/TCC.2021.3054751},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2351-2365},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Bidirectional and malleable proof-of-ownership for large file in cloud storage},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomic resource management for fog computing.
<em>TCC</em>, <em>10</em>(4), 2334–2350. (<a
href="https://doi.org/10.1109/TCC.2021.3064629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog computing is a distributed computing paradigm that extends cloud computing capabilities to the edge of the network and aims at reducing high latency and network congestion, which are characteristics of cloud computing. This recent paradigm enables portions of a transaction to be executed at a fog server and other portions at the cloud. Fog servers are generally not as robust as cloud servers; at peak loads, the data that cannot be processed by fog servers is processed by cloud servers. The data that need to be processed by the cloud is sent over a Wide Area Network (WAN). Therefore, only a fraction of the total data needs to travel through the WAN, as compared with a pure cloud computing paradigm. Additionally, the fog/cloud computing paradigm reduces the cloud processing load when compared with the pure cloud computing model. This article presents a multiclass closed-form analytic queuing network model that is used by an autonomic controller to dynamically change the fraction of processing between edge and cloud servers in order to maximize a utility function of response time and cost. The model was validated using both synthetic and real IoT traces. A detailed design of the autonomic controller is presented and a series of experiments compare the efficacy and efficiency of the controller versus a brute force optimal controller and versus an uncontrolled system using synthetic and real traces. The results show that the controller is able to maintain a high utility in the presence of wide variations of request arrival rates.},
  archive      = {J_TCC},
  author       = {Uma Tadakamalla and Daniel A. Menascé},
  doi          = {10.1109/TCC.2021.3064629},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2334-2350},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Autonomic resource management for fog computing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Application-aware migration algorithm with prefetching in
heterogeneous cloud environments. <em>TCC</em>, <em>10</em>(4),
2324–2333. (<a href="https://doi.org/10.1109/TCC.2021.3064292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inappropriate service migrations can lead to undesirable situations, such as high traffic overhead, long service latency, and service disruption. In this article, we propose an application-aware migration algorithm (AMA) with prefetching. In AMA, a mobile device sends a service offloading request to the controller. After receiving this request, the controller determines the initial service cloud where virtual machine (VM) of the service initially operates by considering the application type. In addition, it periodically decides where to migrate VM and prefetch its core part considering the mobility of the mobile device and application type. To minimize the generated traffic volume while satisfying the requirements of the application, a constrained Markov decision process (CMDP) is formulated and its optimal policy is obtained via linear programming. Evaluation results demonstrate that AMA with the optimal policy can reduce the generated traffic volume while satisfying the requirements of the application (i.e., service latency and probability of service disruption).},
  archive      = {J_TCC},
  author       = {Haneul Ko and Minho Jo and Victor C. M. Leung},
  doi          = {10.1109/TCC.2021.3064292},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2324-2333},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Application-aware migration algorithm with prefetching in heterogeneous cloud environments},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient privacy-aware authentication scheme with
hierarchical access control for mobile cloud computing services.
<em>TCC</em>, <em>10</em>(4), 2309–2323. (<a
href="https://doi.org/10.1109/TCC.2020.3029878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few years, mobile cloud computing (MCC) gains a huge development because of the popularity of mobile applications and cloud computing. User authentication and access control are two indispensable security components in the MCC environment. To the best of our knowledge, they are generally designed in different procedures. Access control can be executed after the authentication completes successfully. In order to improve efficiency, this article constructs an integrated scheme of authentication and hierarchical access control using self-certified public key cryptography (SCPKC) and the Chinese remainder theorem (CRT) for MCC environment. The proposed scheme can achieve mutual authentication while determining the access rights of mobile users without storing any access control list in the MCC service provider side. Besides, we also give a dynamic adding or deletion of MCC service provider to efficiently address potential changes in the hierarchy. The security of our proposed scheme is proved by the random oracle model. Compared with recently related multi-server authentication schemes for the MCC environment, the proposed scheme not only adds a new function of hierarchical access control but also has better computation and communication efficiencies. Therefore, the proposed scheme is more suitable for real-life MCC applications.},
  archive      = {J_TCC},
  author       = {Ling Xiong and Fagen Li and Mingxing He and Zhicai Liu and Tu Peng},
  doi          = {10.1109/TCC.2020.3029878},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2309-2323},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {An efficient privacy-aware authentication scheme with hierarchical access control for mobile cloud computing services},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An automated task scheduling model using non-dominated
sorting genetic algorithm II for fog-cloud systems. <em>TCC</em>,
<em>10</em>(4), 2294–2308. (<a
href="https://doi.org/10.1109/TCC.2020.3032386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing data from Internet of Things (IoT) applications at the cloud centers has known limitations relating to latency, task scheduling, and load balancing. Hence, there have been a shift towards adopting fog computing as a complementary paradigm to cloud systems. In this article, we first propose a multi-objective task-scheduling optimization problem that minimizes both the makespans and total costs in a fog-cloud environment. Then, we suggest an optimization model based on a Discrete Non-dominated Sorting Genetic Algorithm II (DNSGA-II) to deal with the discrete multi-objective task-scheduling problem and to automatically allocate tasks that should be executed either on fog or cloud nodes. The NSGA-II algorithm is adapted to discretize crossover and mutation evolutionary operators, rather than using continuous operators that require high computational resources and not able to allocate proper computing nodes. In our model, the communications between the fog and cloud tiers are formulated as a multi-objective function to optimize the execution of tasks. The proposed model allocates computing resources that would effectively run on either the fog or cloud nodes. Moreover, it efficiently organizes the distribution of workloads through various computing resources at the fog. Several experiments are conducted to determine the performance of the proposed model compared with a continuous NSGA-II (CNSGA-II) algorithm and four peer mechanisms. The outcomes demonstrate that the model is capable of achieving dynamic task scheduling with minimizing the total execution times (i.e., makespans) and costs in fog-cloud environments.},
  archive      = {J_TCC},
  author       = {Ismail M. Ali and Karam M. Sallam and Nour Moustafa and Ripon Chakraborty and Michael Ryan and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TCC.2020.3032386},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2294-2308},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {An automated task scheduling model using non-dominated sorting genetic algorithm II for fog-cloud systems},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A low-load distributed stream processing system for
continuous conjunctive normal form queries. <em>TCC</em>,
<em>10</em>(4), 2281–2293. (<a
href="https://doi.org/10.1109/TCC.2020.3034340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the research issues for stream data is fast notification of continuous query results. Recent researches assume that query processing servers receive data from remote data sources continuously. Therefore, when a large number of processing servers are distributed in networks, large amounts of communication traffic for those are produced. This is the main cause of delaying query result notifications. To tackle this fundamental problem, we focus on the conjunction of operations and conditions in continuous queries for distributed processing systems. When queries consist of some series of operations and conditions, processing servers can stop query executions if the remaining operations or condition checking are dispensable. Thus, communications less arise and communication traffic is further reduced. Our proposed system represents continuous queries by conjunctive normal forms (CNF queries). The proposed system constructs trigger trees for evaluating the sum terms of CNF queries and determines the timing for their evaluations, in order to reduce communication traffic. We confirmed that one of the proposed methods can reduce the average amount of communication traffic by 23 percent compared with a sensor data serial aggregation method in an evaluation situation.},
  archive      = {J_TCC},
  author       = {Tomoki Yoshihisa and Takahiro Hara},
  doi          = {10.1109/TCC.2020.3034340},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2281-2293},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A low-load distributed stream processing system for continuous conjunctive normal form queries},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fast graph neural network-based method for winner
determination in multi-unit combinatorial auctions. <em>TCC</em>,
<em>10</em>(4), 2264–2280. (<a
href="https://doi.org/10.1109/TCC.2020.3046883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combinatorial auction (CA) is an efficient mechanism for resource allocation in different fields, including cloud computing. It can obtain high economic efficiency and user flexibility by allowing bidders to submit bids for combinations of different items instead of only for individual items. However, the problem of allocating items among the bidders to maximize the auctioneers’ revenue, i.e., the winner determination problem (WDP), is NP-complete to solve and inapproximable. Existing works for WDPs are generally based on mathematical optimization techniques and most of them focus on the single-unit WDP, where each item only has one unit. On the contrary, few works consider the multi-unit WDP in which each item may have multiple units. Given that the multi-unit WDP is more complicated but prevalent in cloud computing, we propose leveraging machine learning (ML) techniques to develop a novel low-complexity algorithm for solving this problem with negligible revenue loss. Specifically, we model the multi-unit WDP as an augmented bipartite bid-item graph and use a graph neural network (GNN) with half-convolution operations to learn the probability of each bid belonging to the optimal allocation. To improve the sample generation efficiency and decrease the number of needed labeled instances, we propose two different sample generation processes. We also develop two novel graph-based post-processing algorithms to transform the outputs of the GNN into feasible solutions. Through simulations on both synthetic instances and a specific virtual machine (VM) allocation problem in a cloud computing platform, we validate that our proposed method can approach optimal performance with low complexity and has good generalization ability in terms of problem size and user-type distribution.},
  archive      = {J_TCC},
  author       = {Mengyuan Lee and Seyyedali Hosseinalipour and Christopher G. Brinton and Guanding Yu and Huaiyu Dai},
  doi          = {10.1109/TCC.2020.3046883},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2264-2280},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A fast graph neural network-based method for winner determination in multi-unit combinatorial auctions},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A blockchain-based multi-cloud storage data auditing scheme
to locate faults. <em>TCC</em>, <em>10</em>(4), 2252–2263. (<a
href="https://doi.org/10.1109/TCC.2021.3057771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network storage services have benefited countless users worldwide due to the notable features of convenience, economy and high availability. Since a single service provider is not always reliable enough, more complex multi-cloud storage systems are developed for mitigating the data corruption risk. While a data auditing scheme is still needed in multi-cloud storage to help users confirm the integrity of their outsourced data. Unfortunately, most of the corresponding schemes rely on trusted institutions such as the centralized third-party auditor (TPA) and the cloud service organizer, and it is difficult to identify malicious service providers after service disputes. Therefore, we present a blockchain-based multi-cloud storage data auditing scheme to protect data integrity and accurately arbitrate service disputes. We not only introduce the blockchain to record the interactions among users, service providers, and organizers in data auditing process as evidence, but also employ the smart contract to detect service dispute, so as to enforce the untrusted organizer to honestly identify malicious service providers. We also use the blockchain network and homomorphic verifiable tags to achieve the low-cost batch verification without TPA. Theoretical analyses and experiments reveal that the scheme is effective in multi-cloud environments and the cost is acceptable.},
  archive      = {J_TCC},
  author       = {Cheng Zhang and Yang Xu and Yupeng Hu and Jiajing Wu and Ju Ren and Yaoxue Zhang},
  doi          = {10.1109/TCC.2021.3057771},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2252-2263},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A blockchain-based multi-cloud storage data auditing scheme to locate faults},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3D analytical modeling and iterative solution for high
performance computing clusters. <em>TCC</em>, <em>10</em>(4), 2238–2251.
(<a href="https://doi.org/10.1109/TCC.2021.3055119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Cloud Computing enables the migration of services to the edge of Internet. Therefore, high performance computing clusters are widely deployed to improve computational capabilities of such environments. However, they are prone to failures and need analytical models to predict their behavior in order to deliver desired quality-of-service and quality-of-experience to mobile users. This article proposes a 3D analytical model and a problem-solving approach for sustainability evaluation of high-performance computing clusters. The proposed solution uses an iterative approach to obtain performance measurements to overcome the state space explosion problem. The availability modeling and evaluation of master and computing nodes are performed using a multi-repairman approach. The optimum number of repairmen is also obtained to get realistic results and reduce the overall cost. The proposed model is validated using discrete event simulation. The analytical approach is much faster and in good agreement with the simulations. The analysis focuses on mean queue length, throughput and mean response time outputs. The maximum differences between analytical and simulation results in the considered scenarios of up to a billion states are less than 1.149, 3.82, and 3.76 percent, respectively. These differences are well within the 5 percent of confidence interval of the simulation and the proposed model.},
  archive      = {J_TCC},
  author       = {Yonal Kirsal and Yoney Kirsal Ever and Glenford Mapp and Mohsin Raza},
  doi          = {10.1109/TCC.2021.3055119},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {2238-2251},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {3D analytical modeling and iterative solution for high performance computing clusters},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VM scaling and load balancing via cost optimal MDP solution.
<em>TCC</em>, <em>10</em>(3), 2219–2237. (<a
href="https://doi.org/10.1109/TCC.2020.3000956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic resource allocation mechanism is an essential building block in contemporary cloud computing environment, enabling the support of the large variability of incoming requests from an enormous number of applications utilizing such cloud infrastructure. In this article, we devise a dynamic resource allocation mechanism that optimizes the application’s profit under the set of costs and revenues while maintaining performance constraints. Specifically, we devise a decision-maker (DM) agent which formulates the joint admission control, scaling and load balancing problem as a stochastic process solvable by Markov decision process (MDP) which provides the optimal policy. Accordingly, at each time instance, the DM can determine based on the system’s current state, on the set of requirements and on the set of costs, whether to add or release a VM (scale-out or scale-in, respectively), whether to admit or reject an upcoming task and if admitting it, which VM to allocate it to. We explore the value function structure and provide insights with respect to the optimal policies produced from it. To address scalability issues of the detailed MDP solution we provide an alternative solution by abstract MDP which consolidates multiple system states into a single abstract state, hence can cope with much larger systems at the expense of slight performance degradation. To demonstrate the feasibility of the suggested scheme, we designed and implemented it, alongside with two traditional auto-scalers, on the Amazon Web Services (AWS) infrastructure. We ran numerous MATLAB simulations and AWS-based experiments which provided insights and demonstrated superiority against the traditional policies we compared with.},
  archive      = {J_TCC},
  author       = {Mark Shifrin and Roy Mitrany and Erez Biton and Omer Gurewitz},
  doi          = {10.1109/TCC.2020.3000956},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2219-2237},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {VM scaling and load balancing via cost optimal MDP solution},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Virtual machine placement optimization in mobile cloud
gaming through QoE-oriented resource competition. <em>TCC</em>,
<em>10</em>(3), 2204–2218. (<a
href="https://doi.org/10.1109/TCC.2020.3002023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud gaming is a novel service provisioning paradigm, which hosts video games in the cloud and transmits interactive game streams to game players via the Internet. In such cloud gaming scenarios, the cloud is required to consume tremendous resources for video rendering and streaming, especially when the number of concurrent players reaches a certain level. On the other hand, different game players may have distinct requirements on Quality-of-Experience, such as high video quality, low delay, etc. Under this circumstance, how to satisfy players of different interests by efficiently leveraging cloud resources becomes a major challenge to existing cloud gaming services. In order to meet the overall requirements of players in a cost-effective manner, this article applies game theory to cloud gaming scenarios. It proposes a distributed algorithm to optimize virtual machine (VM) placement in mobile cloud gaming through resource competition. Further, by constructing a potential function, we prove that the resource competition game is a potential game, and the proposed algorithm scales well as the player population increases. We prove theoretically and verify experimentally that, with the proposed distributed VM placement algorithm, players can achieve a mutually satisfying state within a finite number of iterations.},
  archive      = {J_TCC},
  author       = {Yiwen Han and Dongyu Guo and Wei Cai and Xiaofei Wang and Victor C. M. Leung},
  doi          = {10.1109/TCC.2020.3002023},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2204-2218},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Virtual machine placement optimization in mobile cloud gaming through QoE-oriented resource competition},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transferable knowledge for low-cost decision making in cloud
environments. <em>TCC</em>, <em>10</em>(3), 2190–2203. (<a
href="https://doi.org/10.1109/TCC.2020.2989381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users of Infrastructure as a Service (IaaS) are increasingly overwhelmed with the wide range of providers and services offered by each provider. As such, many users select services based on description alone. An emerging alternative is to use a decision support system (DSS), which typically relies on gaining insights from observational data in order to assist a customer in making decisions regarding optimal deployment of cloud applications. The primary activity of such systems is the generation of a prediction model (e.g. using machine learning), which requires a significantly large amount of training data. However, considering the varying architectures of applications, cloud providers, and cloud offerings, this activity is not sustainable as it incurs additional time and cost to collect data to train the models. We overcome this through developing a Transfer Learning (TL) approach where knowledge (in the form of a prediction model and associated data set) gained from running an application on a particular IaaS is transferred in order to substantially reduce the overhead of building new models for the performance of new applications and/or cloud infrastructures. In this article, we present our approach and evaluate it through extensive experimentation involving three real world applications over two major public cloud providers, namely Amazon and Google. Our evaluation shows that our novel two-mode TL scheme increases overall efficiency with a factor of 60 percent reduction in the time and cost of generating a new prediction model. We test this under a number of cross-application and cross-cloud scenarios.},
  archive      = {J_TCC},
  author       = {Faiza Samreen and Gordon S Blair and Yehia Elkhatib},
  doi          = {10.1109/TCC.2020.2989381},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2190-2203},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Transferable knowledge for low-cost decision making in cloud environments},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trading off between user coverage and network robustness for
edge server placement. <em>TCC</em>, <em>10</em>(3), 2178–2189. (<a
href="https://doi.org/10.1109/TCC.2020.3008440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge Cloud Computing (ECC) provides a new paradigm for app vendors to serve their users with low latency by deploying their services on edge servers attached to base stations or access points in close proximity to mobile users. From the edge infrastructure provider’s perspective, a cost-effective $k$ edge server placement ( $k$ ESP) aims to place $k$ edge servers within a particular geographic area to maximize the number of covered mobile users, i.e., to maximize the user coverage . However, in the distributed and volatile ECC environment, edge servers are subject to failures due to various reasons, e.g., software exceptions, hardware faults, cyberattacks, etc. Mobile users connected to a failed edge server have to access services in the remote cloud if they are not covered by any other edge servers. This significantly impacts mobile users quality of experience. Thus, the robustness of the edge server network (referred to as network robustness hereafter) in a specific area must be considered in edge server placement. In this article, we formally model this joint user coverage and network robustness oriented $k$ edge server placement ( $k$ ESP-CR) problem, and prove that finding the optimal solution to this problem is $\mathcal {NP}$ -hard. To tackle this ESP-CR, we first propose an integer programming based optimal approach (namely ESP-O) for finding optimal solutions to small-scale $k$ ESP-CR problems. Then, we propose an approximation approach, namely ESP-A, for solving large-scale $k$ ESP-CR problems efficiently and theoretically prove its approximation ratio. Finally, the performance of these two approaches are experimentally evaluated against three representative approaches on a widely-used real-world dataset.},
  archive      = {J_TCC},
  author       = {Guangming Cui and Qiang He and Feifei Chen and Hai Jin and Yun Yang},
  doi          = {10.1109/TCC.2020.3008440},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2178-2189},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Trading off between user coverage and network robustness for edge server placement},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trading cost and throughput in geo-distributed analytics
with a two time scale approach. <em>TCC</em>, <em>10</em>(3), 2163–2177.
(<a href="https://doi.org/10.1109/TCC.2020.2994195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of global-scale services, analytical queries are performed on datasets that span multiple data centers (DCs). Such geo-distributed queries generate a large amount of inter-DC data transfers at run time. Due to the expensive inter-DC bandwidth, various methods have been proposed to reduce the traffic cost in geo-distributed data analytics. However, current methods do not attempt to address the throughput issue in geo-distributed analytics. In this article, we target at characterizing and optimizing a cost-throughput tradeoff problem in geo-distributed data analytics. Our objectives are two-fold: (1) we minimize the inter-DC traffic cost when serving geo-distributed analytics with uncertain query demand, and (2) we maximize the system throughput, in terms of the number of query requests that can be successfully served with guaranteed queuing delay. Specifically, we formulate a stochastic optimization problem that seamlessly combines these two objectives. To solve this problem, we take advantage of Lyapunov optimization techniques to design and analyze a two-timescale online control framework. Without prior knowledge of future query requests, this framework makes online decisions on input data placement and admission control of query requests. Rigorous theoretical analyses show that our framework can achieve a near-optimal solution and maintain system stability and robustness as well. Extensive trace-driven simulation results further demonstrate that our framework is capable of reducing inter-DC traffic cost, improving system throughput, and guaranteeing a maximum delay for each query request.},
  archive      = {J_TCC},
  author       = {Xinping Xu and Wenxin Li and Renhai Xu and Heng Qi and Keqiu Li and Xiaobo Zhou and Sheng Chen},
  doi          = {10.1109/TCC.2020.2994195},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2163-2177},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Trading cost and throughput in geo-distributed analytics with a two time scale approach},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Threshold multi-keyword search for cloud-based group data
sharing. <em>TCC</em>, <em>10</em>(3), 2146–2162. (<a
href="https://doi.org/10.1109/TCC.2020.2999775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searchable Encryption (SE) is a popular cryptographic primitive for building ciphertexts retrieval systems with far-reaching applications. However, existing SE schemes generally do not support threshold access control (i.e., data users must collaboratively issue search and decryption operations over encrypted cloud data) in a group-oriented cloud data sharing setting, which is increasingly receiving much attention in the research community. Thus, in this article, we first propose a Threshold Multi-keyword Search (TMS) scheme for cloud-based group data sharing (referred to as basic TMS scheme) by utilizing Shamir’s secret sharing technique, to achieve threshold multi-keyword search, threshold decryption, and short record ciphertext size. Then, we extend this basic TMS to realize threshold result verification and threshold traceability (referred to as enhanced TMS). Furthermore, the enhanced TMS is extended to support public result verification and dynamic operations with the public verifier and improved hash tables, respectively. Our formal security analysis proves that both basic TMS and enhanced TMS are semi-adaptively secure and can resist Chosen-Keyword Attack (CKA). Our theoretical evaluation and empirical experiments demonstrate the potential utility of both schemes.},
  archive      = {J_TCC},
  author       = {Yinbin Miao and Robert H. Deng and Kim-Kwang Raymond Choo and Ximeng Liu and Hongwei Li},
  doi          = {10.1109/TCC.2020.2999775},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2146-2162},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Threshold multi-keyword search for cloud-based group data sharing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spotlight: Scalable transport layer load balancing for data
center networks. <em>TCC</em>, <em>10</em>(3), 2131–2145. (<a
href="https://doi.org/10.1109/TCC.2020.3024834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Load Balancing plays a vital role in cloud data centers to distribute traffic among instances of network functions or services. State-of-the-art load balancers dispatch traffic obliviously without considering the real-time utilization of service instances and therefore can lead to uneven load distribution and sub-optimal performance. In this article, we design and implement Spotlight, a scalable and distributed load balancing architecture that maintains connection-to-instance mapping consistency at the edge of data center networks. Spotlight uses a new stateful flow dispatcher which periodically polls instances’ load and dispatches incoming connections to instances in proportion to their available capacity. Our design utilizes a distributed control plane and in-band flow dispatching; thus, it scales horizontally in data center networks. Through extensive flow-level simulation and packet-level experiments on a testbed with HTTP traffic on unmodified Linux kernel, we demonstrate that compared to existing methods Spotlight distributes traffic more efficiently and has near-optimum performance in terms of overall service utilization. Compared to existing solutions, Spotlight improves aggregated throughput and average flow completion time by at least 20 percent with infrequent control plane updates. Moreover, we show that Spotlight scales horizontally as it updates the switches at O(100ms) and is resilient to lack of control plane convergence.},
  archive      = {J_TCC},
  author       = {Ashkan Aghdai and Cing-Yu Chu and Yang Xu and David H. Dai and Jun Xu and H. Jonathan Chao},
  doi          = {10.1109/TCC.2020.3024834},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2131-2145},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Spotlight: Scalable transport layer load balancing for data center networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Service availability analysis in a virtualized system: A
markov regenerative model approach. <em>TCC</em>, <em>10</em>(3),
2118–2130. (<a href="https://doi.org/10.1109/TCC.2020.3028648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid and wide development and deployment of system virtualization, service availability analysis has become increasingly important in a virtualized system (VS) which suffers from software aging. Software rejuvenation techniques can be applied to improve service availability but its effectiveness depends on the rejuvenation policy, which defines when and where to rejuvenate, and which rejuvenation technique to be triggered. This article aims to analyze the optimal inspection time interval for maximizing application service (AS) availability under a three-level rejuvenation policy, in which rejuvenation techniques are deployed at each level, namely, AS, virtual machine (VM), and virtual machine monitor (VMM) levels. We first apply Markov regenerative process to construct an analytical model for the VS. Experiments of injecting memory leaks are conducted to measure aging-related parameters. Furthermore, numerical analysis is carried out to study the quantitative relationship between AS availability and inspection time interval, and determine the approximate optimal inspection time interval.},
  archive      = {J_TCC},
  author       = {Jing Bai and Xiaolin Chang and Gaorong Ning and Zhenjiang Zhang and Kishor S. Trivedi},
  doi          = {10.1109/TCC.2020.3028648},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2118-2130},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Service availability analysis in a virtualized system: A markov regenerative model approach},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Secure similarity search over encrypted non-uniform
datasets. <em>TCC</em>, <em>10</em>(3), 2102–2117. (<a
href="https://doi.org/10.1109/TCC.2020.3000233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searchable symmetric encryption (SSE) enables a user to outsource a private dataset to a cloud server in encrypted form while retaining the ability to search over the encrypted outsourced data. The existing SSE schemes improve the search and safety performances from different perspectives. However, almost none of the existing SSE schemes considers the data distribution issues. We find that when the dataset is not distributed uniformly, the search quality based on the conventional methods decreases. Therefore, the existing SSE schemes cannot guarantee high search quality when faced with non-uniform datasets. In addition, most existing SSE solutions cannot hide the distribution of the query set. In this article, we design a S\ecure similarity search over Encrypted Non-uniform and high-dimensional Datasets (SEND) with a novel way to enhance security. The basic idea is to combine SSE with locality-sensitive hashing (LSH). Unlike earlier schemes, SEND uses selective hashing, which has better performance for non-uniform datasets. Also, we present a novel approach to hide the distribution of the query set, which makes SEND more secure. Our experimental results indicate SEND achieves a high search quality of recall and precision, and it is proven secure against adaptively chosen query attacks in the standard model.},
  archive      = {J_TCC},
  author       = {Cheng Guo and Wanping Liu and Ximeng Liu and Yinghui Zhang},
  doi          = {10.1109/TCC.2020.3000233},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2102-2117},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Secure similarity search over encrypted non-uniform datasets},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Secure cloud storage with data dynamics using secure network
coding techniques. <em>TCC</em>, <em>10</em>(3), 2090–2101. (<a
href="https://doi.org/10.1109/TCC.2020.3000342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the age of cloud computing, cloud users with limited storage can outsource their data to remote servers. These servers, in lieu of monetary benefits, offer retrievability of their clients’ data at any point of time. Secure cloud storage protocols enable a client to check integrity of outsourced data. In this article, we explore the possibility of constructing a secure cloud storage for dynamic data by leveraging the algorithms involved in secure network coding. We show that some of the secure network coding schemes can be used to construct efficient secure cloud storage protocols for dynamic data, and we construct such a protocol (DSCS I) based on a secure network coding protocol. To the best of our knowledge, DSCS I is the first secure cloud storage protocol for dynamic data constructed using secure network coding techniques which is secure in the standard model. Although generic dynamic data support arbitrary insertions, deletions and modifications, append-only data find numerous applications in the real world. We construct another secure cloud storage protocol (DSCS II) specific to append-only data — that overcomes some limitations of DSCS I. Finally, we provide prototype implementations for DSCS I and DSCS II in order to evaluate their performance.},
  archive      = {J_TCC},
  author       = {Binanda Sengupta and Akanksha Dixit and Sushmita Ruj},
  doi          = {10.1109/TCC.2020.3000342},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2090-2101},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Secure cloud storage with data dynamics using secure network coding techniques},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Resource usage cost optimization in cloud computing using
machine learning. <em>TCC</em>, <em>10</em>(3), 2079–2089. (<a
href="https://doi.org/10.1109/TCC.2020.3015769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing is gaining popularity among small and medium-sized enterprises. The cost of cloud resources plays a significant role for these companies and this is why cloud resource optimization has become a very important issue. Numerous methods have been proposed to optimize cloud computing resources according to actual demand and to reduce the cost of cloud services. Such approaches mostly focus on a single factor (i.e., compute power) optimization, but this can yield unsatisfactory results in real-world cloud workloads which are multi-factor, dynamic and irregular. This article presents a novel approach which uses anomaly detection, machine learning and particle swarm optimization to achieve a cost-optimal cloud resource configuration. It is a complete solution which works in a closed loop without the need for external supervision or initialization, builds knowledge about the usage patterns of the system being optimized and filters out anomalous situations on the fly. Our solution can adapt to changes in both system load and the cloud provider’s pricing plan. It was tested in Microsoft’s cloud environment Azure using data collected from a real-life system. Experiments demonstrate that over a period of 10 months, a cost reduction of 85 percent was achieved.},
  archive      = {J_TCC},
  author       = {Patryk Osypanka and Piotr Nawrocki},
  doi          = {10.1109/TCC.2020.3015769},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2079-2089},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Resource usage cost optimization in cloud computing using machine learning},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). QuickN: Practical and secure nearest neighbor search on
encrypted large-scale data. <em>TCC</em>, <em>10</em>(3), 2066–2078. (<a
href="https://doi.org/10.1109/TCC.2020.3009961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a scheme, named QuickN, which can efficiently and securely enable nearest neighbor search over encrypted data on untrusted clouds. Specifically, we modify the search algorithm of nearest neighbors in tree structures (e.g., R-trees), such that the modified algorithm adapts to lightweight cryptographic primitives (e.g., Order-Preserving Encryption) without affecting the original faster-than-linear search complexity. Moreover, we propose an optimized algorithm on top of our modified search algorithm, where it can significantly save communication overheads of a client without introducing any additional information leakage. We devise an approximate algorithm to $k$ -nearest neighbor search to improve search efficiency by taking a tradeoff in the completeness of search results. In addition, we also demonstrate our design only leaks minimal privacy against advanced inference attacks. Our experimental results on Amazon EC2 show that our algorithms are extremely practical over massive datasets.},
  archive      = {J_TCC},
  author       = {Boyang Wang and Yantian Hou and Ming Li},
  doi          = {10.1109/TCC.2020.3009961},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2066-2078},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {QuickN: Practical and secure nearest neighbor search on encrypted large-scale data},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Publicly verifiable shared dynamic electronic health record
databases with functional commitment supporting privacy-preserving
integrity auditing. <em>TCC</em>, <em>10</em>(3), 2050–2065. (<a
href="https://doi.org/10.1109/TCC.2020.3002553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health record (EHR) is a system that collects patients&#39; digital health information and shares it with other healthcare providers in the cloud. Since EHR contains a large amount of significant and sensitive information about patients, it is required that the system ensures response correctness and storage integrity. Meanwhile, with the rise of IoT, more low-performance terminals are deployed for receiving and uploading patient data to the server, which increases the computational and communication burden of the EHR systems. The verifiable database (VDB), where a user outsources his large database to a cloud server and makes queries once he needs certain data, is proposed as an efficient updatable cloud storage model for resource-constrained users. To improve efficiency, most existing VDB schemes utilize proof reuse and proof updating technique to prove correctness of the query results. However, it ignores the “real-time” of proof generation, which results in an overhead that the user has to perform extra process (e.g., auditing schemes) to check storage integrity. In this article, we propose a publicly verifiable shared updatable EHR database scheme that supports privacy-preserving and batch integrity checking with minimum user communication cost. We modify the existing functional commitment (FC) scheme for the VDB design and construct a concrete FC under the computational l -BDHE assumption. In addition, the use of an efficient verifier-local revocation group signature scheme makes our scheme support dynamic group member operations, and gives nice features, such as traceability and non-frameability.},
  archive      = {J_TCC},
  author       = {Ye Su and Jiameng Sun and Jing Qin and Jiankun Hu},
  doi          = {10.1109/TCC.2020.3002553},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2050-2065},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Publicly verifiable shared dynamic electronic health record databases with functional commitment supporting privacy-preserving integrity auditing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Privacy-preserving subgraph matching scheme with
authentication in social networks. <em>TCC</em>, <em>10</em>(3),
2038–2049. (<a href="https://doi.org/10.1109/TCC.2020.3012999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of social networks, a great variety of new social applications have been generated for impromptu group formation and communications. Among those applications, the subgraph matching has become a hot research area in social networks. Due to the huge cost of managing and computing graph data, it may have to outsource the computations to the cloud server. However, the most critical problem is that the cloud server leaks the graph information during the processing of the graph data, and the external attackers modify the graph information during the transmission on the public channel. Thus, confidentiality and authentication have been critical attributes in the subgraph matching query service. In this article, we present an efficient and privacy-preserving subgraph matching scheme with authentication in social networks. Using the proposed scheme, the cloud can accomplish the subgraph matching query process without obtaining any sensitive information about the users. Additionally, we achieve data integrity verification and user authentication. Each receiver can verify if the received messages come from the legal sender and have not been tampered. The detailed security and efficiency analysis show that the proposed scheme not only satisfies security requirements but also achieves high-efficiency in local users, and it is suitable for many practical applications.},
  archive      = {J_TCC},
  author       = {Xiangjian Zuo and Lixiang Li and Haipeng Peng and Shoushan Luo and Yixian Yang},
  doi          = {10.1109/TCC.2020.3012999},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2038-2049},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Privacy-preserving subgraph matching scheme with authentication in social networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Privacy-preserving medical treatment system through
nondeterministic finite automata. <em>TCC</em>, <em>10</em>(3),
2020–2037. (<a href="https://doi.org/10.1109/TCC.2020.2999940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a privacy-preserving medical treatment system using nondeterministic finite automata (NFA), hereafter referred to as P-Med, designed for remote medical environment. P-Med makes use of the nondeterministic transition characteristic of NFA to flexibly represent medical model, which includes illness states, treatment methods and state transitions caused by exerting different treatment methods. A medical model is encrypted and outsourced to cloud to deliver telemedicine service. Using P-Med, patient-centric diagnosis and treatment can be made on-the-fly while protecting the confidentiality of patient’s illness states and treatment recommendation results. Moreover, a new privacy-preserving NFA evaluation method is given in P-Med to get a confidential match result for the evaluation of an encrypted NFA and an encrypted data set, which avoids the cumbersome inner state transition determination. We demonstrate that P-Med realizes treatment procedure recommendation without privacy leakage to unauthorized parties. We conduct extensive experiments and analysis to evaluate the efficiency.},
  archive      = {J_TCC},
  author       = {Yang Yang and Robert H. Deng and Ximeng Liu and Yongdong Wu and Jian Weng and Xianghan Zheng and Chunming Rong},
  doi          = {10.1109/TCC.2020.2999940},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2020-2037},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Privacy-preserving medical treatment system through nondeterministic finite automata},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Practical multi-keyword ranked search with access control
over encrypted cloud data. <em>TCC</em>, <em>10</em>(3), 2005–2019. (<a
href="https://doi.org/10.1109/TCC.2020.3024226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of data volume in the cloud computing environment, data owners are increasingly inclined to store their data on the cloud. Although data outsourcing reduces computation and storage costs for them, it inevitably brings new security and privacy concerns, as the data owners lose direct control of sensitive data. Meanwhile, most of the existing ranked keyword search schemes mainly focus on enriching search efficiency or functionality, but lack of providing efficient access control and formal security analysis simultaneously. To address these limitations, in this article we propose an efficient and privacy-preserving M ulti-keyword R anked S earch scheme with F ine-grained access control (MRSF). MRSF can realize highly accurate ciphertext retrieval by combining coordinate matching with Term Frequency-Inverse Document Frequency (TF-IDF) and improving the secure $k$ NN method. Besides, it can effectively refine users’ search privileges by utilizing the polynomial-based access strategy. Formal security analysis shows that MRSF is secure in terms of confidentiality of outsourced data and the privacy of index and tokens. Extensive experiments further show that, compared with existing schemes, MRSF achieves higher search accuracy and more functionalities efficiently.},
  archive      = {J_TCC},
  author       = {Jiayi Li and Jianfeng Ma and Yinbin Miao and Ruikang Yang and Ximeng Liu and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TCC.2020.3024226},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2005-2019},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Practical multi-keyword ranked search with access control over encrypted cloud data},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Personality- and value-aware scheduling of user requests in
cloud for profit maximization. <em>TCC</em>, <em>10</em>(3), 1991–2004.
(<a href="https://doi.org/10.1109/TCC.2020.3000792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main goal of a cloud provider is to make profits by providing services to users. Existing profit optimization strategies employ homogeneous user models in which user personality is ignored, resulting in fewer profits and particularly notably lower user satisfaction that in turn, leads to fewer users and reduced profits. In this article, we propose efficient personality-aware request scheduling schemes to maximize the profit of the cloud provider under the constraint of user satisfaction. Specifically, we first model the service requests at the granularity of individual personality and propose a personalized user satisfaction prediction model based on questionnaires. Subsequently, we design a personality-guided integer linear programming (ILP)-based request scheduling algorithm to maximize the profit under the constraint of user satisfaction, which is followed by an approximate but lightweight value assessment and cross entropy (VACE)-based profit improvement scheme. The VACE-based scheme is especially tailored for applications with high scheduling resolution. Extensive simulation results show that our satisfaction prediction model can achieve the accuracy of up to 83 percent, and our profit optimization schemes can improve the profit by at least 3.96 percent as compared to the benchmarking methods while still obtaining a speedup of at least 1.68x.},
  archive      = {J_TCC},
  author       = {Peijin Cong and Guo Xu and Junlong Zhou and Mingsong Chen and Tongquan Wei and Meikang Qiu},
  doi          = {10.1109/TCC.2020.3000792},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1991-2004},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Personality- and value-aware scheduling of user requests in cloud for profit maximization},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-intrusive and workflow-aware virtual network function
scheduling in user-space. <em>TCC</em>, <em>10</em>(3), 1975–1990. (<a
href="https://doi.org/10.1109/TCC.2020.3024232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The simple programming model and very low-overhead I/O capabilities of emerging packet processing techniques leveraging kernel-bypass I/O and poll-mode processing is gaining significant popularity for building high performance software middleboxes ( aka Virtual Network Functions (VNFs) ). However, existing OS schedulers fall short in rightsizing CPU allocation to poll-mode VNFs due to the schedulers’ shortcoming in capturing the actual processing cost of these VNFs. This issue is further exacerbated by their inability to consider VNF processing order when VNFs are chained to form Service Function Chains (SFCs). The state-of-the-art VNF schedulers proposed as an alternative to OS schedulers are intrusive , requiring the VNFs to be built with scheduler specific libraries or having carefully selected scheduling checkpoints. This highly restricts the VNFs that can properly work with these schedulers. In this article, we present UN i S , a U ser-space N on- i ntrusive work-flow aware VNF S cheduler. Unlike existing approaches, UNiS is non-intrusive, i.e., does not require VNF modifications and treats poll-mode VNFs as black boxes. UNiS is also workflow-aware, i.e., takes SFC processing order into account while scheduling VNFs. Testbed experiments show that UN i S is able to achieve a throughput within 90 and 98 percent of that achievable using an intrusive co-operative scheduler for synthetic and real data center traffic, respectively.},
  archive      = {J_TCC},
  author       = {Anthony and Shihabur Rahman Chowdhury and Tim Bai and Raouf Boutaba and Jérôme François},
  doi          = {10.1109/TCC.2020.3024232},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1975-1990},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Non-intrusive and workflow-aware virtual network function scheduling in user-space},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-user task offloading to heterogeneous processors with
communication delay and budget constraints. <em>TCC</em>,
<em>10</em>(3), 1958–1974. (<a
href="https://doi.org/10.1109/TCC.2020.3019952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study task scheduling and offloading in a cloud computing system with multiple users where tasks have different processing times, release times, communication times, and weights. Each user may schedule a task locally or offload it to a shared cloud with heterogeneous processors by paying a price for the resource usage. We consider four different models in this article: (i) zero task release and communication times; (ii) non-zero task release times and zero communication times; (iii) non-zero task release times and fixed communication times; and (iv) non-zero task release times and sequence-dependent communication times. Our article aims at identifying a task scheduling decision that minimizes the weighted sum completion time of all tasks, while satisfying the users’ budget constraints. We propose an efficient solution framework for this NP-hard problem. As a first step, we use a relaxation and a rounding technique to obtain an integer solution that is a constant factor approximation to the minimum weighted sum completion time. This solution violates the budget constraints, but the average budget violation decreases as the number of users increases. Thus, we develop a scalable algorithm termed Single-Task Unload for Budget Resolution (STUBR), which resolves budget violations and orders the tasks to obtain robust solutions. We prove performance bounds for the rounded solution as well as for the budget-resolved solution, for all four models considered. Via extensive trace-driven simulation for both chess and compute-intensive applications, we observe that STUBR exhibits robust performance under practical scenarios and outperforms existing alternatives. We also use simulation to study the scalability of STUBR algorithm as the number of tasks and the number of users in the system increases.},
  archive      = {J_TCC},
  author       = {Sowndarya Sundar and Jaya Prakash Champati and Ben Liang},
  doi          = {10.1109/TCC.2020.3019952},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1958-1974},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Multi-user task offloading to heterogeneous processors with communication delay and budget constraints},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Less provisioning: A hybrid resource scaling engine for
long-running services with tail latency guarantees. <em>TCC</em>,
<em>10</em>(3), 1941–1957. (<a
href="https://doi.org/10.1109/TCC.2020.3016345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern resource management frameworks guarantee low tail latency for long-running services using the resource over-provisioning method, resulting in serious waste of resources and increasing the service costs greatly. To reduce the over-provisioning cost, we present HRSE, a hybrid resource scaling engine that enables much more efficient resource provisioning for both periodic and non-periodic workloads of long-running services while guaranteeing the tail latency Service Level Objective (SLO). HRSE employs a convolution-based time series analysis to identify periodic patterns in workloads. If periodic patterns are discovered, HRSE estimates the just-right amount of resources based on the periodic features through a top- $K$ based collaborative filtering approach. Otherwise, it leverages wavelet-clustering to capture the short-term patterns in non-periodic workloads and predict the resource demands for the near future. To further enforce the tail latency SLO, HRSE uses an online reprovisioning mechanism that dynamically adjusts the resources to mitigate the performance uncertainty due to workload burstinesses. We fully implement HRSE on top of Docker and conduct extensive experiments using traces from production systems. Testbed experiments show that HRSE is able to increase the average resource utilization to 43 and 45 percent for periodic and non-periodic workloads respectively while guaranteeing the same tail latency objective.},
  archive      = {J_TCC},
  author       = {Binlei Cai and Keqiu Li and Laiping Zhao and Rongqi Zhang},
  doi          = {10.1109/TCC.2020.3016345},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1941-1957},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Less provisioning: A hybrid resource scaling engine for long-running services with tail latency guarantees},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Less is more: Service profit maximization in geo-distributed
clouds. <em>TCC</em>, <em>10</em>(3), 1925–1940. (<a
href="https://doi.org/10.1109/TCC.2020.3024616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays cloud providers purchase a good deal of bandwidth from Internet service providers to satisfy the growing requests from corporate customers for the exclusive use of inter-datacenter bandwidth. For exclusive bandwidth services, neither maximizing the revenue nor minimizing the cost can bring the maximal profit to cloud providers. The diversity of bandwidth prices and the random arrival time of user requests further increase the difficulty in economically scheduling the services to meet user requests from cloud providers. In this article, we propose to help cloud providers maximize their service profits by properly selecting user requests to serve rather than satisfying them all. We formulate the problem of service profit maximization and prove its NP-hardness. To handle offline request submission, we propose a solution that maximizes the service profit by alternately maximizing the service revenue and minimizing the service cost. To maximize service profit under online request submission, we propose an online scheduling algorithm that carefully handles the risk of not being able to pay off the incremental service cost and makes scheduling decisions in real time. Our extensive evaluations demonstrate that our solutions can achieve more than 1.6x the service profits of existing solutions.},
  archive      = {J_TCC},
  author       = {Zhenjie Yang and Yong Cui and Xin Wang and Minming Li and Yadong Liu},
  doi          = {10.1109/TCC.2020.3024616},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1925-1940},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Less is more: Service profit maximization in geo-distributed clouds},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning-driven cloud resource provision policy for content
providers with competitor. <em>TCC</em>, <em>10</em>(3), 1913–1924. (<a
href="https://doi.org/10.1109/TCC.2020.3020616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cloud resource provision policy of a content provider in the presence of competitors on globally distributed cloud platforms plays a significant role in maximizing its profit. However, developing an optimal resource provision policy is quite challenging, due to the difficulty to capture the competition relationship between two competitive CPs and to obtain the budget of the competitors which is usually kept private. To solve this problem, in this article, we propose a learning-driven cloud resource provision policy for a CP with competitors. We formulate the competition between the CPs as a lottery Colonel Blotto game in which the payoff of each region is positively related to the resource advantage achieved by the CP, formulate the budget allocation problem as a Markov decision process, and obtain the sub-optimal resource provision policy by reinforcement learning and deep reinforcement learning-based algorithms. We also prove the convergence of the sub-optimal solution. Finally, we validate our proposed method using real-world CPs statistics. The results show that the budget information is critical for a CP to make policy decisions, and it is better for CPs with smaller budget to focus their budget resources in regions with higher values.},
  archive      = {J_TCC},
  author       = {Xiaobo Zhou and Xiaodong Dong and Laiping Zhao and Keqiu Li and Tie Qiu},
  doi          = {10.1109/TCC.2020.3020616},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1913-1924},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Learning-driven cloud resource provision policy for content providers with competitor},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint optimization of VNF placement and flow scheduling in
mobile core network. <em>TCC</em>, <em>10</em>(3), 1900–1912. (<a
href="https://doi.org/10.1109/TCC.2020.3004301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the development of new generation mobile communication technology, the mobile core network also needs to be upgraded by new network technologies, e.g., software defined networking (SDN) and network function virtualization (NFV). With NFV, virtual network functions (VNFs) can be deployed on commodity devices to support various network function requirements and attain system&#39;s flexibility and elasticity in network edge. Meanwhile, a set of selected VNFs are usually chained as a service function chain (SFC) to serve a given flow in a specified order. Since the devices have heterogeneous execution environments and the VNFs have various requirements, one fundamental challenge is how to embed SFC for each flow on the shared NFV infrastructure (NFVI) with the goal of minimizing the flow completion time. Furthermore, multiple flows always compete for resources of those devices hosting SFCs. In this general setting, there is an urgent need to study efficient scheduling mechanism to minimize the total completion time of all flows. In this paper, by jointly considering VNF placement and flow scheduling, we first formulate this problem as an integer programming problem, and further prove that it is NP-hard in general case. We then design a PDG method to find the optimal solution in single flow case and an LRD method to achieve a high-quality feasible solution in multiple flows case. The extensive experiment results indicate that our LRD method can reduce the total completion time of all flows by 22.04, 60.99 and 39.95, percent against three compared methods, respectively.},
  archive      = {J_TCC},
  author       = {Bangbang Ren and Siyuan Gu and Deke Guo and Guoming Tang and Xu Lin},
  doi          = {10.1109/TCC.2020.3004301},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1900-1912},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Joint optimization of VNF placement and flow scheduling in mobile core network},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interference-aware SaaS user allocation game for edge
computing. <em>TCC</em>, <em>10</em>(3), 1888–1899. (<a
href="https://doi.org/10.1109/TCC.2020.3008448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge Computing, extending cloud computing, has emerged as a prospective computing paradigm. It allows a SaaS (Software-as-a-Service) vendor to allocate its users to nearby edge servers to minimize network latency and energy consumption on their devices. From the SaaS vendor’s perspective, a cost-effective SaaS user allocation (SUA) aims to allocate maximum SaaS users on minimum edge servers. However, the allocation of excessive SaaS users to an edge server may result in severe interference and consequently impact SaaS users’ data rates. In this article, we formally model this problem and prove that finding the optimal solution to this problem is NP-hard. Thus, we propose ISUAGame, a game-theoretic approach that formulates the interference-aware SUA (ISUA) problem as a potential game. We analyze the game and show that it admits a Nash equilibrium. Then, we design a novel decentralized algorithm for finding a Nash equilibrium in the game as a solution to the ISUA problem. The performance of this algorithm is theoretically analyzed and experimentally evaluated. The results show that the ISUA problem can be solved effectively and efficiently.},
  archive      = {J_TCC},
  author       = {Guangming Cui and Qiang He and Xiaoyu Xia and Phu Lai and Feifei Chen and Tao Gu and Yun Yang},
  doi          = {10.1109/TCC.2020.3008448},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1888-1899},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Interference-aware SaaS user allocation game for edge computing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integrated power anomaly defense: Towards
oversubscription-safe data centers. <em>TCC</em>, <em>10</em>(3),
1875–1887. (<a href="https://doi.org/10.1109/TCC.2020.3001454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy storage devices (e.g., batteries) are critical components for high-availability data center infrastructure today. Without resilient energy management of these devices, existing power-hungry data centers are largely unguarded targets for cyber criminals. Particularly for some of today&#39;s scale-out data centers, power infrastructure oversubscription unavoidably taxes the data center&#39;s backup energy resources (i.e., UPS), leaving very little room for dealing with power emergency. As a result, an attacker could manipulate the computing system to generate peak power demand and disrupt power-constrained server racks. This article aims at protecting data centers from malicious loads that seek to drain precious energy backup, overload server racks and compromise workload performance. We term such load as Elusive Power Peak (EPP) and demonstrate its basic three-phase attacking model. To defend against EPP, we propose IPAD, a remediation solution build on integrated software and hardware mechanisms. IPAD not only increases the attacking cost considerably by hiding vulnerable server racks from visible power peaks, but also strengthens the last line of defense against hidden power spikes with fine-grained power control strategy. We show that IPAD can effectively raise the bar of power-related attack, with reasonable design overhead.},
  archive      = {J_TCC},
  author       = {Xiaofeng Hou and Chao Li and Jinghang Yang and Wenli Zheng and Xiaoyao Liang and Minyi Guo},
  doi          = {10.1109/TCC.2020.3001454},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1875-1887},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Integrated power anomaly defense: Towards oversubscription-safe data centers},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geography-aware task scheduling for profit maximization in
distributed green data centers. <em>TCC</em>, <em>10</em>(3), 1864–1874.
(<a href="https://doi.org/10.1109/TCC.2020.3001051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrastructure in Distributed Green Data Centers (DGDCs) is concurrently shared by multiple different applications to flexibly provide a growing number of services to global users in a cost-effective way. A highly challenging problem is how to maximize the total profit of the DGDC provider in a market where Internet Service Provider (ISP) bandwidth price, availability of green energy, price of power grid, and revenue brought by the execution of tasks all vary with geographical locations. Unlike existing studies, this article proposes a Geography-Aware Task Scheduling (GATS) approach by considering spatial variations in DGDCs to maximize the total profit of the DGDC provider by intelligently scheduling tasks of all applications. In each time slot, the formulated profit maximization problem is solved as a convex optimization one via the interior point method. Trace-driven simulations show that GATS achieves larger total profit and higher throughput than two typical task scheduling approaches.},
  archive      = {J_TCC},
  author       = {Haitao Yuan and Jing Bi and MengChu Zhou},
  doi          = {10.1109/TCC.2020.3001051},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1864-1874},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Geography-aware task scheduling for profit maximization in distributed green data centers},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Forecasting cloud application workloads with CloudInsight
for predictive resource management. <em>TCC</em>, <em>10</em>(3),
1848–1863. (<a href="https://doi.org/10.1109/TCC.2020.2998017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive cloud resource management has been widely adopted to overcome the limitations of reactive cloud autoscaling. The predictive resource management is highly relying on workload predictors, which estimate short-/long-term fluctuations of cloud application workloads. These predictors tend to be pre-optimized for specific workload patterns. However, such predictors are still insufficient to handle real-world cloud workloads whose patterns may be unknown a priori, may dynamically change over time and may be irregular. As a result, these predictors often cause over-/under-provisioning of cloud resources. To address this problem, we have created CloudInsight, a novel cloud workload prediction framework, leveraging the combined power of multiple workload predictors. CloudInsight creates an ensemble model using multiple predictors to make accurate predictions for real workloads. The weights of the predictors in CloudInsight are determined at runtime with their accuracy for the current workload using multi-class regression. The ensemble model is periodically optimized to handle sudden changes in the workload. We evaluated CloudInsight with various real workload traces. The results show that CloudInsight has 13–27 percent higher accuracy than state-of-the-art predictors. Moreover, the results from trace-based simulations with a cloud resource manager show that CloudInsight has 15–20 percent less under-/over-provisioning periods, resulting in high cost-efficiency and low SLA violations.},
  archive      = {J_TCC},
  author       = {In Kee Kim and Wei Wang and Yanjun Qi and Marty Humphrey},
  doi          = {10.1109/TCC.2020.2998017},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1848-1863},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Forecasting cloud application workloads with CloudInsight for predictive resource management},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fine-grained query authorization with integrity verification
over encrypted spatial data in cloud storage. <em>TCC</em>,
<em>10</em>(3), 1831–1847. (<a
href="https://doi.org/10.1109/TCC.2020.3010915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a fine-grained query authorization scheme with integrity verification is proposed over encrypted spatial data for location-based services (LBS). The fine-grained query authorization is enabled based on a distribution of the spatial data by employing a non-uniform partition in the spatial domain to generate a density-based space filling curve (DSC), which can be used to generate index values for querying and transformation keys. The transformation keys can be used to generate query tokens for a secure spatial query as well as construct a transformation key tree whose subtree can be distributed by the LBS provider to an authorized user as transformation key for query tokens generation. Furthermore, the proposed scheme constructs a Merkle quad tree (MQ-tree) to support integrity verification by aggregating a digest of the spatial data based on the DSC and employing the MQ-tree as a verification structure. The LBS provider can share a subtree of the MQ-tree to authorized user as his verification structure, which corresponds to the transformation key of the authorized user. In this way, the authorized user can only generate the valid query tokens and verify the query results in his authorized region. The security properties of the proposed scheme is discussed, and extensive experimental results demonstrate the high efficiency of verification structure generation and verification operations.},
  archive      = {J_TCC},
  author       = {Feng Tian and Zhenqiang Wu and Xiaolin Gui and Jianbing Ni and Xuemin Sherman Shen},
  doi          = {10.1109/TCC.2020.3010915},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1831-1847},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Fine-grained query authorization with integrity verification over encrypted spatial data in cloud storage},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast secure and anonymous key agreement against bad
randomness for cloud computing. <em>TCC</em>, <em>10</em>(3), 1819–1830.
(<a href="https://doi.org/10.1109/TCC.2020.3008795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud computing, resources are usually in cloud service provider’s network and typically accessed remotely by the cloud users via public channels. Key agreement enables secure channel establishment over a public channel for the secure communications between a cloud user and a cloud service provider. Existing key agreement protocols for cloud computing suffer from some challenges, e.g., realizing low connection delay, eliminating certificate management problem, enhancing user privacy and avoiding bad randomness. To tackle these challenges, we propose a certificateless 0-RTT anonymous AKA protocol against bad randomness for secure channel establishment in cloud computing. As a 0-RTT protocol, it significantly speeds up the efficiency of the secure channel establishment process. Further, our protocol does not need for the certificates to bind a public key with an entity’s identity and hence solves the certificate management problem. Finally, concrete security analysis of the protocol is also proposed. The protocol not only satisfies the traditional security attributes (e.g., known-key security, unknown key-share), but also strong security guarantees, i.e., user privacy and bad randomness resistance.},
  archive      = {J_TCC},
  author       = {Xinyu Meng and Lei Zhang and Burong Kang},
  doi          = {10.1109/TCC.2020.3008795},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1819-1830},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Fast secure and anonymous key agreement against bad randomness for cloud computing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fairness-efficiency scheduling for cloud computing with soft
fairness guarantees. <em>TCC</em>, <em>10</em>(3), 1806–1818. (<a
href="https://doi.org/10.1109/TCC.2020.3021084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fairness and efficiency are two important metrics for users in modern data center computing system. Due to the heterogeneous resource demands of CPU, memory, and network I/O for users’ tasks, it cannot achieve the strict 100 percent fairness and the maximum efficiency at the same time. Existing fairness-efficiency schedulers (e.g., Tetris) can balance such a tradeoff elastically by relaxing fairness constraint for improved efficiency using the knob. However, their approaches are unaware of fairness degradation under different knob configurations, which makes several drawbacks. First, it cannot tell how much relaxed fairness can be guaranteed given a knob value. Second, it fails to meet several essential properties such as sharing incentive. To address these issues, we propose a new fairness-efficiency scheduler, QKnober , to balance the fairness and efficiency elastically and flexibly using a tunable fairness knob. QKnober is a fairness-sensitive scheduler that can maximize the system efficiency while guaranteeing the $\theta$ -soft fairness by modeling the whole allocation as a combination of fairness-oriented allocation and efficiency-oriented allocation. Moreover, QKnober satisfies fairness properties of sharing incentive, envy-freeness and pareto efficiency given a proper knob value. We have implemented QKnober in YARN and evaluated it using both testbed and simulated experiments. The results show that QKnober outperforms its alternatives DRF and Tetris by 31.2 and 4.5 percent, respectively.},
  archive      = {J_TCC},
  author       = {Shanjiang Tang and Ce Yu and Yusen Li},
  doi          = {10.1109/TCC.2020.3021084},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1806-1818},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Fairness-efficiency scheduling for cloud computing with soft fairness guarantees},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FADA: A cloud-fog-edge architecture and ontology for data
acquisition. <em>TCC</em>, <em>10</em>(3), 1792–1805. (<a
href="https://doi.org/10.1109/TCC.2020.3014110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large and complex machines are the backbone of manufacturing and will remain key to industrialization for the foreseeable future as will leveraging essential technologies, such as the Internet of Things. However, efforts to make this machinery as smart as it could be are falling behind the curve. Data quality is often too low or too heterogeneous for useful analytics making maintenance troublesome. Additionally, many machines are still dumb with no uniform way to extract data or monitor their operation. In part, the success of future concepts like intelligent manufacturing, cyber physical systems, and industry 4.0 depends on solving these problems. Hence, this article presents FADA the groundwork for an ontology and 3-layer cloud-fog-edge architecture for large and complex machines that places data acquisition and IoT at the forefront. The ontology provides a flexible framework for standardizing data. The fog nodes acquire data directly from smart machines, while the edge nodes harvest data from dumb equipment through a recognition model. The fog nodes are flexible and multi-threaded to provide faster higher-performance computing power. To evaluate the proposed architecture and concepts, we implemented FADA in two factory-based testbeds: one with IoT-enabled equipment, the other with mostly dumb machines. The response times and influence rates recorded are promising and indicate that the system is highly adaptable to many different scenarios. We also conducted comparative experiments between FADA and a conventional data acquisition system to compare the occupied disk space, processing time, and data uploading time, which show that the FADA can save 2.9 TB of disk space per day, and reduce the server’s processing time by 184.8 ms per time over the conventional data acquisition system(CDAS), when 20000 fog nodes simultaneously access the server. The results show improvements by FADA in all metrics.},
  archive      = {J_TCC},
  author       = {Huifeng Wu and Yi Yan and Baiping Chen and Feng Hou and Danfeng Sun},
  doi          = {10.1109/TCC.2020.3014110},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1792-1805},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {FADA: A cloud-fog-edge architecture and ontology for data acquisition},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evolutionary multi-objective workflow scheduling for
volatile resources in the cloud. <em>TCC</em>, <em>10</em>(3),
1780–1791. (<a href="https://doi.org/10.1109/TCC.2020.2993250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cloud has been widely used as a distributed computing platform for running scientific workflow applications. Most of the cloud providers encourage the use of their underutilized resources as spot instances for much cheaper prices compared with common resources as on-demand instances, however, the promise of lower costs for resources results in the volatility such that spot instances can be interrupted at any time by cloud providers. Many workflow scheduling algorithms have been proposed to deal with volatile resources. In this article, we consider the two most important features of the volatile resources namely fulfillment and interruption rates to fully model the instability of the cloud infrastructure. Subsequently, we propose a novel evolutionary multi-objective workflow scheduling approach to generate a set of trade-off solutions that outperform state-of-the-art algorithms in both makespan and economic costs. In addition, we explore the fluctuation of makespan and costs for our obtained schedules under different levels of fulfillment and interruption rates. Experimental results with the five well-known real-world workflows demonstrate that our evolutionary multi-objective workflow scheduling algorithm is competitive in terms of makespan and cost compared with state-of-the-art on-demand scheduling techniques.},
  archive      = {J_TCC},
  author       = {Thanh-Phuong Pham and Thomas Fahringer},
  doi          = {10.1109/TCC.2020.2993250},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1780-1791},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Evolutionary multi-objective workflow scheduling for volatile resources in the cloud},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enabling cost-effective, SLO-aware machine learning
inference serving on public cloud. <em>TCC</em>, <em>10</em>(3),
1765–1779. (<a href="https://doi.org/10.1109/TCC.2020.3006751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The remarkable advances of Machine Learning (ML) have spurred an increasing demand for ML- as-a-Service on public cloud: developers train and publish ML models as online services to provide low-latency inference for dynamic queries. The primary challenge of ML model serving is to meet the response-time Service-Level Objectives (SLOs) of inference workloads while minimizing serving cost. In this article, we proposes MArk (Model Ark), a general-purpose inference serving system, to tackle the dual challenge of SLO compliance and cost effectiveness. MArk employs three design choices tailored to inference workload. First, MArk dynamically batches requests and opportunistically serves them using expensive hardware accelerators (e.g., GPU) for improved performance-cost ratio. Second, instead of relying on feedback control scaling or over-provisioning to serve dynamic workload, which can be too slow or too expensive, MArk employs predictive autoscaling to hide the provisioning latency at low cost. Third, given the stateless nature of inference serving, MArk exploits the flexible, yet costly serverless instances to cover occasional load spikes that are hard to predict. We evaluated the performance of MArk using several state-of-the-art ML models trained in TensorFlow, MXNet, and Keras. Compared with the premier industrial ML serving platform SageMaker, MArk reduces the serving cost up to $7.8\times$ while achieving even better latency performance.},
  archive      = {J_TCC},
  author       = {Chengliang Zhang and Minchen Yu and Wei Wang and Feng Yan},
  doi          = {10.1109/TCC.2020.3006751},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1765-1779},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Enabling cost-effective, SLO-aware machine learning inference serving on public cloud},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficiently consolidating virtual data centers for
time-varying resource demands. <em>TCC</em>, <em>10</em>(3), 1751–1764.
(<a href="https://doi.org/10.1109/TCC.2020.2997403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data center virtualization is a flexible and efficient way to enable multiple users to share the common resources of a physical data center (DC). For efficient sharing, virtual data center (VDC) embedding is a vital problem that should be carefully addressed. However, existing studies on VDC embedding mostly assume that the capacity of each VDC is fixed, but do not consider the time-varying feature of resource demands. Considering the fact that the resource demands of most enterprise IT services exhibit the time-varying feature, resource allocation based on the fixed capacity assumption would cause a great inefficiency. To overcome this inefficiency, we propose a new VDC consolidation scheme that takes into account the time-varying feature of resource demands when embedding VDCs. We first develop a resource demand prediction model for each VDC using the Long Short-Term Memory (LSTM) neural network, which is used to predict the real-time resource demands of VDCs at different future moments. Based on the predicted resource demands, we then embed VDCs whose peaks and valleys of resource demands stagger each other onto common physical servers and links, such that the required physical resources can be minimized under the condition that all the resource demands of different VDCs are satisfied at all the different moments. An integer linear programming (ILP) model and a resource demand correlation-based heuristic algorithm are also developed for the proposed scheme. Simulation results show that the proposed consolidation scheme can significantly improve resource utilization in a DC. It can save up to 25 percent of physical servers and 29 percent of physical links used for accommodating the same requests as compared to a scheme assigning resources based on the fixed capacity assumption.},
  archive      = {J_TCC},
  author       = {Chao Guo and Yongcheng Li and Yonghu Yan and Wei Chen and Sanjay Kumar Bose and Gangxiang Shen},
  doi          = {10.1109/TCC.2020.2997403},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1751-1764},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficiently consolidating virtual data centers for time-varying resource demands},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient multi-channel computation offloading for mobile
edge computing: A game-theoretic approach. <em>TCC</em>, <em>10</em>(3),
1738–1750. (<a href="https://doi.org/10.1109/TCC.2020.2994145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing is emerging to provide cloud-computing capabilities to mobile users, so that they can offload computation intensive tasks to close proximity for execution. However, most existing works imply that a transmission-finished task still occupies the channel until all users on the same channel finish the transmission, leading to severe channel resource waste. To solve this problem, we propose an efficient computation offloading mechanism which releases the channel resources of transmission-finished tasks for transmission-unfinished tasks, and aims to minimize the response time and energy consumption for each user. Specifically, we formulate the computation offloading problem as a game, analyze its structural properties and show how it possesses a Nash equilibrium and admits the finite improvement property, in the cases of elastic cloud and non-elastic cloud respectively. We then propose a D istributed M ulti-channel C omputation O ffloading (DMCO) algorithm, which can converge to a Nash equilibrium, and find the upper bound of the convergence time. We further evaluate the performance of DMCO using the price of anarchy. Numerical results show that DMCO scales well with the number of users, and outperforms existing works, for example, benefits 13.3 percent more users and reduces cost by 23.7 percent than CO, one of the best existing works.},
  archive      = {J_TCC},
  author       = {Shuhui Chu and Zhiyi Fang and Shinan Song and Zhanyang Zhang and Chengxi Gao and Chengzhong Xu},
  doi          = {10.1109/TCC.2020.2994145},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1738-1750},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficient multi-channel computation offloading for mobile edge computing: A game-theoretic approach},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic resource orchestration for service capability
maximization in fog-enabled connected vehicle networks. <em>TCC</em>,
<em>10</em>(3), 1726–1737. (<a
href="https://doi.org/10.1109/TCC.2020.3001289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological advances in fog computing are precipitating an evolution in conventional vehicle networks to a new paradigm called fog-enabled connected vehicle networks (FCVNs). FCVNs provide communication efficiency for ensuring safe transportation through the massive Internet of vehicles. In FCVNs, massive vehicles tend to associate with roadside units and high power nodes, which act as fog nodes (FNs), when they have a good channel quality and/or popular contents. This circumstance may lead to a load imbalance among the FNs. This problem significantly decreases the resource utilization efficiency and service capability of the networks. In this article, we propose a dynamic resource orchestration (DRO) scheme to harmonize resource allocation for connected vehicles by migrating the offloaded services among FNs. A graph-theoretic approach is utilized to transform the FCVN into a directed graph model, where the maximum resource reduction obtained by service migrations is considered the weight of the link between every two FNs. Subsequently, the maximum weight matching solution is used to determine optimal pairs of FNs for migrating services to maximize network resource utilization. Our simulation results reveal that the proposed DRO scheme achieves significant improvements in terms of service capability, throughput, and resource utilization efficiency as compared with existing algorithms.},
  archive      = {J_TCC},
  author       = {Duc-Nghia Vu and Nhu-Ngoc Dao and Woongsoo Na and Sungrae Cho},
  doi          = {10.1109/TCC.2020.3001289},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1726-1737},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Dynamic resource orchestration for service capability maximization in fog-enabled connected vehicle networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic resource allocation method based on symbiotic
organism search algorithm in cloud computing. <em>TCC</em>,
<em>10</em>(3), 1714–1725. (<a
href="https://doi.org/10.1109/TCC.2020.3002205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing is a popular and powerful paradigm for leasing IT services over the Internet. It allows sharing the resources among a large number of consumers. This requires allocating and releasing resources dynamically over time. Since resources are paid as are used, ensuring a better level of service is a challenge for cloud providers. They must provide enough resources to meet user needs with maintaining the quality of service. Therefore, QoS is a serious problem that arises during the dynamic management of heterogeneous resources. This type of problem can be resolved by minimizing execution time and resource reservation prices. It is necessary to integrate a new method well enough to improve the performance of the cloud, in this area. This article presents a dynamic resource allocation model for the cloud computing environment. In addition, we propose a meta-heuristic approach named Multi-Objectives Symbiotic Organism Search algorithm (MOSOS) for resource allocation. We specifically designed MOSOS to minimize both the makespan and cost. Simulation results revealed that MOSOS gave a better result compared to other methods. It shown significant adaptation with the dynamic change of the cloud, as well as minimizing the execution time. Hence, improving the QoS given to cloud users.},
  archive      = {J_TCC},
  author       = {Ali Belgacem and Kadda Beghdad-Bey and Hassina Nacer},
  doi          = {10.1109/TCC.2020.3002205},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1714-1725},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Dynamic resource allocation method based on symbiotic organism search algorithm in cloud computing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cost-effective app user allocation in an edge computing
environment. <em>TCC</em>, <em>10</em>(3), 1701–1713. (<a
href="https://doi.org/10.1109/TCC.2020.3001570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing is a new distributed computing paradigm extending the cloud computing paradigm, offering much lower end-to-end latency, as real-time, latency-sensitive applications can now be deployed on edge servers that are much closer to end-users than distant cloud servers. In edge computing, edge user allocation (EUA) is a critical problem for any app vendors, who need to determine which edge servers will serve which users. This is to satisfy application-specific optimization objectives, e.g., maximizing users’ overall quality of experience, minimizing system costs, and so on. In this article, we focus on the cost-effectiveness of user allocation solutions with two optimization objectives. The primary one is to maximize the number of users allocated to edge servers. The secondary one is to minimize the number of required edge servers, which subsequently reduces the operating costs for app vendors. We first model this problem as a bin packing problem and introduce an approach for finding optimal solutions. However, finding optimal solutions to the $\mathcal {NP}$ -hard EUA problem in large-scale scenarios is intractable. Thus, we propose a heuristic to efficiently find sub-optimal solutions to large-scale EUA problems. Extensive experiments conducted on real-world data demonstrate that our heuristic can solve the EUA problem effectively and efficiently, outperforming the state-of-the-art and baseline approaches.},
  archive      = {J_TCC},
  author       = {Phu Lai and Qiang He and John Grundy and Feifei Chen and Mohamed Abdelrazek and John Hosking and Yun Yang},
  doi          = {10.1109/TCC.2020.3001570},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1701-1713},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cost-effective app user allocation in an edge computing environment},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Content caching and distribution at wireless mobile edge.
<em>TCC</em>, <em>10</em>(3), 1688–1700. (<a
href="https://doi.org/10.1109/TCC.2020.2995403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing can provision hierarchical cloud resources at the edge, and efficiently reduce the distance that the remote data need to travel towards the end-users. To empower content delivery at the edge and ultimately transform “shorter distance” to “less time”, instead of focusing only on the storage aspects (e.g., allocation of limited media caches) of mobile edge, the radio aspects (e.g., interference models caused by sharing limited RBs) should be jointly considered. We hence propose a novel comprehensive analytical content caching and delivery framework for the cloud enhanced mobile edge with hierarchical radio access points. To investigate the impacts of limited radio resources (e.g., power, spectrum, and time) and storage resources, we propose a joint user scheduling and caching (JSC) scheme to optimize the end to end performance in terms of throughput (i.e., the number of successful content requests). By leveraging the backhaul links among multi-tier access points, the proposed algorithm can tap on the potential of the coded multi-casting scheme, which is designed to reduce the traffic load among the network. Simulation results demonstrate that the JSC algorithm can improve the resource utilization efficiency and provide a significant sum throughput gain.},
  archive      = {J_TCC},
  author       = {Xueqing Huang and Nirwan Ansari},
  doi          = {10.1109/TCC.2020.2995403},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1688-1700},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Content caching and distribution at wireless mobile edge},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Congestion-aware traffic allocation for geo-distributed data
centers. <em>TCC</em>, <em>10</em>(3), 1675–1687. (<a
href="https://doi.org/10.1109/TCC.2020.3001192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Inter-datacenter transfer is a fundamental service for global cloud applications. Geo-distributed data centers become an essential resource for their application performance which may be destroyed by network congestion. Recent years, most inter-datacenter transfer methods focus on allocating transfers by bandwidth allocation to achieve low cost or high utilization. However, the congestion condition is rarely considered in these works. In this article, we introduce a congestion-aware traffic allocation method named CONA ( CON gestion- A ware), whose target is to maximize the profit of allocation transfer among multiple data centers. On this purpose, a maximizing optimization model is proposed, and an efficient link grading strategy is presented. A matrix transformation method is also introduced to simplify the optimization problem. Furthermore, the link congestion condition is considered by the controller, as well as the prediction on link congestion. To verify our proposed method, simulation model is established and comprehensive experiments are conducted. The experimental results show that our method brings higher profit than fair share and greedy traffic allocation method.},
  archive      = {J_TCC},
  author       = {Xiaoyi Tao and Kaoru Ota and Mianxiong Dong and Wuyunzhaola Borjigin and Heng Qi and Keqiu Li},
  doi          = {10.1109/TCC.2020.3001192},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1675-1687},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Congestion-aware traffic allocation for geo-distributed data centers},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Complying with data handling requirements in cloud storage
systems. <em>TCC</em>, <em>10</em>(3), 1661–1674. (<a
href="https://doi.org/10.1109/TCC.2020.3000336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In past years, cloud storage systems saw an enormous rise in usage. However, despite their popularity and importance as underlying infrastructure for more complex cloud services, today’s cloud storage systems do not account for compliance with regulatory, organizational, or contractual data handling requirements by design. Since legislation increasingly responds to rising data protection and privacy concerns, complying with data handling requirements becomes a crucial property for cloud storage systems. We present Prada , a practical approach to account for compliance with data handling requirements in key-value based cloud storage systems. To achieve this goal, Prada introduces a transparent data handling layer, which empowers clients to request specific data handling requirements and enables operators of cloud storage systems to comply with them. We implement Prada on top of the distributed database Cassandra and show in our evaluation that complying with data handling requirements in cloud storage systems is practical in real-world cloud deployments as used for microblogging, data sharing in the Internet of Things, and distributed email storage.},
  archive      = {J_TCC},
  author       = {Martin Henze and Roman Matzutt and Jens Hiller and Erik Mühmer and Jan Henrik Ziegeldorf and Johannes van der Giet and Klaus Wehrle},
  doi          = {10.1109/TCC.2020.3000336},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1661-1674},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Complying with data handling requirements in cloud storage systems},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cloud-based privacy-preserving collaborative consumption for
sharing economy. <em>TCC</em>, <em>10</em>(3), 1647–1660. (<a
href="https://doi.org/10.1109/TCC.2020.3010235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing has been a dominant paradigm for a variety of information processing platforms, particularly for enabling various popular applications of sharing economy. However, there is a major concern regarding data privacy on these cloud-based platforms. This work presents novel cloud-based privacy-preserving solutions to support collaborative consumption applications for sharing economy. In typical collaborative consumption, information processing platforms need to enable fair cost-sharing among multiple users for utilizing certain shared facilities and communal services. Our cloud-based privacy-preserving protocols, based on homomorphic Paillier cryptosystems, can ensure that the cloud-based operator can only obtain an aggregate schedule of all users in facility sharing, or a service schedule conforming to service provision rule in communal service sharing, but is unable to track the personal schedules or demands of individual users. More importantly, the participating users are still able to settle cost-sharing among themselves in a fair manner for the incurred costs, without knowing each other’s private schedules or demands. Our privacy-preserving protocols involve no other third party who may compromise privacy. We also provide an extensive evaluation study and a proof-of-concept system prototype of our protocols.},
  archive      = {J_TCC},
  author       = {Lingjuan Lyu and Sid Chi-Kin Chau and Nan Wang and Yifeng Zheng},
  doi          = {10.1109/TCC.2020.3010235},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1647-1660},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cloud-based privacy-preserving collaborative consumption for sharing economy},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cloud intrusion detection method based on stacked
contractive auto-encoder and support vector machine. <em>TCC</em>,
<em>10</em>(3), 1634–1646. (<a
href="https://doi.org/10.1109/TCC.2020.3001017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Security issues have resulted in severe damage to the cloud computing environment, adversely affecting the healthy and sustainable development of cloud computing. Intrusion detection is one of the technologies for protecting the cloud computing environment from malicious attacks. However, network traffic in the cloud computing environment is characterized by large scale, high dimensionality, and high redundancy, these characteristics pose serious challenges to the development of cloud intrusion detection systems. Deep learning technology has shown considerable potential for intrusion detection. Therefore, this study aims to use deep learning to extract essential feature representations automatically and realize high detection performance efficiently. An effective stacked contractive autoencoder (SCAE) method is presented for unsupervised feature extraction. By using the SCAE method, better and robust low-dimensional features can be automatically learned from raw network traffic. A novel cloud intrusion detection system is designed on the basis of the SCAE and support vector machine (SVM) classification algorithm. The SCAE+SVM approach combines both deep and shallow learning techniques, and it fully exploits their advantages to significantly reduce the analytical overhead. Experiments show that the proposed SCAE+SVM method achieves higher detection performance compared to three other state-of-the-art methods on two well-known intrusion detection evaluation datasets, namely KDD Cup 99 and NSL-KDD.},
  archive      = {J_TCC},
  author       = {Wenjuan Wang and Xuehui Du and Dibin Shan and Ruoxi Qin and Na Wang},
  doi          = {10.1109/TCC.2020.3001017},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1634-1646},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cloud intrusion detection method based on stacked contractive auto-encoder and support vector machine},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cloud firewall under bursty and correlated data traffic: A
theoretical analysis. <em>TCC</em>, <em>10</em>(3), 1620–1633. (<a
href="https://doi.org/10.1109/TCC.2020.3000674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud firewalls stand as one of the major building blocks of the cloud security framework protecting the Virtual Private Infrastructure against attacks such as the Distributed Denial of Service (DDoS). In order to fully characterize the cloud firewall operation and gain actionable insights on the design of cloud security, performance models for the cloud firewall become imperative. In this article, we propose a multi-dimensional Continuous-Time Markov Chain model for the cloud firewall that takes into account the burstiness and correlation features of the legitimate and malicious data traffic. By adopting the Markov-Modulated Poisson process (MMPP) and the Interrupted Poisson Process (IPP), we identify the workload conditions under which the cloud firewall might be subject to a loss of availability. Furthermore, by comparing the IPP and Poisson attacks, we numerically verify that the cloud firewall is inherently vulnerable to a burstiness-aware attack which might seriously compromise its operation. Additionally, we characterize the joint harmful impact of burstiness and correlation on the cloud firewall that might lead to performance degradation. Finally, we design an elastic cloud firewall by proposing a MMPP-driven load balancing procedure that provisions virtual firewalls dynamically while fulfilling a Service Level Agreement (SLA) latency specification.},
  archive      = {J_TCC},
  author       = {Glaucio H.S. Carvalho and Isaac Woungang and Alagan Anpalagan},
  doi          = {10.1109/TCC.2020.3000674},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1620-1633},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cloud firewall under bursty and correlated data traffic: A theoretical analysis},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic database troubleshooting of azure SQL databases.
<em>TCC</em>, <em>10</em>(3), 1604–1619. (<a
href="https://doi.org/10.1109/TCC.2020.3007016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exponentially growing number of workloads has been transferred from on-premises to the cloud environment during last decade. It incurs a constantly increasing load on the monitoring and troubleshooting capabilities of the platforms that host those applications nowadays. Data mining techniques and use of telemetry data have recently become an unavoidable means for tracking the behavior of cloud services. The research effort elaborated in this paper is focused on exploiting real-world data to build an automatic database troubleshooting system that exploits the combination of the more comprehensive statistical data science models and an expert system to perform the root cause analysis. An extensive evaluation study was conducted during the eight-month period with a plethora of Azure SQL production workloads for a varying number of databases. The obtained results confirmed the viability and cost-effectiveness of such an approach at the scale of the cloud.},
  archive      = {J_TCC},
  author       = {Dejan Dundjerski and Milo Tomašević},
  doi          = {10.1109/TCC.2020.3007016},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1604-1619},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Automatic database troubleshooting of azure SQL databases},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Authentication and key agreement based on anonymous identity
for peer-to-peer cloud. <em>TCC</em>, <em>10</em>(3), 1592–1603. (<a
href="https://doi.org/10.1109/TCC.2020.3004334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-cloud data migration is one of the prevailing challenges faced by mobile users, which is an essential process when users change their mobile phones to a different provider. However, due to the insufficient local storage and computational capabilities of the smart phones, it is often very difficult for users to backup all data from the original cloud servers to their mobile phones in order to further upload the downloaded data to the new cloud provider. To solve this problem, we propose an efficient data migration model between cloud providers and construct a mutual authentication and key agreement scheme based on elliptic curve certificate-free cryptography for peer-to-peer cloud. The proposed scheme helps to develop trust between different cloud providers and lays a foundation for the realization of cross-cloud data migration. Mathematical verification and security correctness of our scheme is evaluated against notable existing schemes of data migration, which demonstrate that our proposed scheme exhibits a better performance than other state-of-the-art scheme in terms of the achieved reduction in both the computational and communication cost.},
  archive      = {J_TCC},
  author       = {Hong Zhong and Chuanwang Zhang and Jie Cui and Yan Xu and Lu Liu},
  doi          = {10.1109/TCC.2020.3004334},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1592-1603},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Authentication and key agreement based on anonymous identity for peer-to-peer cloud},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Architectural design of cloud applications: A
performance-aware cost minimization approach. <em>TCC</em>,
<em>10</em>(3), 1571–1591. (<a
href="https://doi.org/10.1109/TCC.2020.3015703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud Computing has assumed a relevant role in the ICT, profoundly influencing the life-cycle of modern applications in the manner they are designed, developed, and deployed and operated. In this article, we tackle the problem of supporting the design-time analysis of Cloud applications to identify a cost-optimized strategy for allocating components onto Cloud Virtual Machine infrastructural services, taking performance requirements into account. We present an approach and a tool, SPACE4Cloud, that supports users in modeling the architecture of an application, in defining performance requirements as well as deployment constraints, and then in mapping each architecture component into a corresponding VM service, minimizing total costs. An optimization algorithm supports the mapping and determines the Cloud configuration that minimizes the execution costs of the application over a daily time horizon. The benefits of this approach are demonstrated in the context of an industrial case study. Furthermore, we show that SPACE4Cloud leads to a cost reduction up to 60 percent, when compared to a first-principle technique based on utilization thresholds, like the ones typically used in practice, and that our solution is able to solve large problem instances within a time frame compatible with a fast-paced design process (less than half an hour in the worst case). Finally, we show that SPACE4Cloud is suitable to model even microservice-based applications and to compute the corresponding optimized deployment configuration which is compared with a state-of-the art meta-heuristic alternative method, achieving savings between 21 and 85 percent.},
  archive      = {J_TCC},
  author       = {Michele Ciavotta and Giovanni Paolo Gibilisco and Danilo Ardagna and Elisabetta Di Nitto and Marco Lattuada and Marcos Aurélio Almeida da Silva},
  doi          = {10.1109/TCC.2020.3015703},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1571-1591},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Architectural design of cloud applications: A performance-aware cost minimization approach},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An edge computing matching framework with guaranteed quality
of service. <em>TCC</em>, <em>10</em>(3), 1557–1570. (<a
href="https://doi.org/10.1109/TCC.2020.3005539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing is a new computing paradigm, which aims at enhancing user experience by bringing computing resources closer to where data is produced by Internet of Things (IoT). Edge services are provided by small data centers located at the edge of the network, called cloudlets. However, IoT users often face strict Quality of Service (QoS) constraints for a proper remote execution of their applications on edge. Each user has specific resource requirements and budget limitations for her IoT application, while each cloudlet offers a limited number and types of resources, each with a specific cost. Therefore, a key challenge is how to efficiently match cloudlets to IoT applications and enable a convenient any-time access to edge computing services considering preferences and incentives of users and cloudlets. In this article, we address this problem by proposing a novel two-sided matching solution for edge services considering QoS requirements in terms of service response time. In addition, we determine dynamic pricing of edge services based on the preferences and incentives of cloudlets, IoT users, and the system. The proposed matching is incentive compatible, individually rational, weakly budget balanced, asymptotically allocative efficient, and computationally efficient. We perform a comprehensive assessment through extensive performance analysis experiments to evaluate our proposed matching and pricing solutions.},
  archive      = {J_TCC},
  author       = {Nafiseh Sharghivand and Farnaz Derakhshan and Lena Mashayekhy and Leyli Mohammadkhanli},
  doi          = {10.1109/TCC.2020.3005539},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1557-1570},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {An edge computing matching framework with guaranteed quality of service},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Admission control policies in fog computing using extensive
form game. <em>TCC</em>, <em>10</em>(3), 1543–1556. (<a
href="https://doi.org/10.1109/TCC.2020.3000800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to emergence of Internet of Things (IoT), fog computing is gaining momentum in the IT industry. The fog nodes are owned by the fog service providers (FSPs) and usually are not intended to provide their services for free. If FSP charges high for its services or does not stick with its promised quality of service, existing users of that FSP may leave early or may churn to some other FSP. In such competitive scenario, to survive and to maximize the profit in the long run, FSPs should accept the requests of the users considering both technical and non-technical parameters. Since both FSP and IoT users are strategic decision makers, game theoretic analysis may help FSPs to maximize their payoffs. With the change in the strategy of the player, equilibrium solution may change and therefore this dynamic scenario is formulated as an extensive game form. A subgame perfect equilibrium, obtained for this game using backward induction, makes admission control policies suitable for different environment which helps the FSPs in maximizing their profit in the long run. A comparative analysis of the proposed work with state of art indicates that the proposed work outperforms and generates better revenue to the FSPs.},
  archive      = {J_TCC},
  author       = {Gaurav Baranwal and Deo Prakash Vidyarthi},
  doi          = {10.1109/TCC.2020.3000800},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1543-1556},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Admission control policies in fog computing using extensive form game},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A remote memory sharing system for virtualized computing
infrastructures. <em>TCC</em>, <em>10</em>(3), 1532–1542. (<a
href="https://doi.org/10.1109/TCC.2020.3018089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resource management is a critical issue in today’s virtualized computing infrastructures. Consolidation is the main technique used to optimize such infrastructure. Regarding memory management, it allows gathering overloaded and underloaded VM on the same server so that memory can be mutualized. However, because of infrastructures constraints and complexity of managing multiple resources, consolidation can hardly optimize memory management. In this article, we propose to rely on a remote memory sharing for mutualizing memory. We implemented a system which monitors the working set of virtual machines, reclaims unused memory and makes it available (as a remote swap device) for virtual machines which need memory. Our evaluations with HPC and Big Data benchmarks demonstrate the effectiveness of this approach. We show that remote memory can improve the performance of a standard Spark benchmark by up the 17 percent with an average performance degradation of 1.5 percent (for the providing application).},
  archive      = {J_TCC},
  author       = {Aram Kocharyan and Brice Ekane and Boris Teabe and Giang Son Tran and Hrachya Astsatryan and Daniel Hagimont},
  doi          = {10.1109/TCC.2020.3018089},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1532-1542},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A remote memory sharing system for virtualized computing infrastructures},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A reliability-aware, delay guaranteed, and resource
efficient placement of service function chains in softwarized 5G
networks. <em>TCC</em>, <em>10</em>(3), 1515–1531. (<a
href="https://doi.org/10.1109/TCC.2020.3020269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network Functions Virtualization (NFV) allows flexibility, scalability, agility, and easy manageability of networks by leveraging the features of virtualization and cloud computing technologies. However, softwarization of network functions imposes many challenges. Reliability and latency are major challenges in NFV-enabled 5G networks that can lead to customer dissatisfaction and revenue loss. In general, redundancy is used to improve the reliability of communication services. However, redundancy requires the same amount of additional resources and thus increases cost. In this article, we address the reliability-aware, delay guaranteed, and resource efficient Service Function Chain (SFC) placement problem in softwarized 5G networks. First, we propose a novel SFC subchaining method to enhance the reliability of an SFC without backups. If reliability requirement is not met after subchaining method, we add backups to VNFs to meet the reliability requirement. Then, we formulate the reliable SFC placement problem as an Integer Linear Programming (ILP) problem in order to solve it optimally. Owing to high computational complexity of the ILP problem for solving large input instances, we propose a modified stable matching algorithm to provide near-optimal solution in polynomial time. By extensive simulations we show that our proposed solutions consume lesser physical resources compared to state-of-the-art solutions for provisioning reliable communication services.},
  archive      = {J_TCC},
  author       = {Prabhu Kaliyammal Thiruvasagam and Vijeth J. Kotagi and C Siva Ram Murthy},
  doi          = {10.1109/TCC.2020.3020269},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1515-1531},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A reliability-aware, delay guaranteed, and resource efficient placement of service function chains in softwarized 5G networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A cooperative coevolution genetic programming
hyper-heuristics approach for on-line resource allocation in
container-based clouds. <em>TCC</em>, <em>10</em>(3), 1500–1514. (<a
href="https://doi.org/10.1109/TCC.2020.3026338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Containers are lightweight and provide the potential to reduce more energy consumption of data centers than Virtual Machines (VMs) in container-based clouds. On-line resource allocation is the most common operation in clouds. However, the on-line Resource Allocation in Container-based clouds (RAC) is new and challenging because of its two-level architecture, i.e., the allocations of containers to VMs and the allocation of VMs to physical machines. These two allocations interact with each other, and hence cannot be made separately. Since on-line container allocation requires a real-time response, most current allocation techniques rely on heuristics (e.g., First Fit and Best Fit), which do not consider the comprehensive information such as workload patterns and VM types. As a result, resources are not used efficiently and the energy consumption is not sufficiently optimized. We first propose a novel model of the on-line RAC problem with the consideration of VM overheads, VM types and an affinity constraint. Then, we design a Cooperative Coevolution Genetic Programming (CCGP) hyper-heuristic approach to solve the RAC problem, named CCGP-RAC . CCGP-RAC can learn the workload patterns and VM types from historical workload traces and generate allocation rules. The experiments show significant improvement in energy consumption compared to the state-of-the-art algorithms.},
  archive      = {J_TCC},
  author       = {Boxiong Tan and Hui Ma and Yi Mei and Mengjie Zhang},
  doi          = {10.1109/TCC.2020.3026338},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1500-1514},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A cooperative coevolution genetic programming hyper-heuristics approach for on-line resource allocation in container-based clouds},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A cloud-edge collaboration framework for cognitive service.
<em>TCC</em>, <em>10</em>(3), 1489–1499. (<a
href="https://doi.org/10.1109/TCC.2020.2997008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile applications can leverage high-quality deep learning models such as convolutional neural networks and deep neural networks to provide high-performance cognitive services. Prior work on deep learning models-based mobile applications in a cloud-edge computing environment focuses on performing lightweight data pre-processing tasks on edge servers for cloud-hosted cognitive servers. These approaches have two major limitations. First, it is uneasy for the mobile applications to assure satisfactory user experience in terms of network communication delay, because the intermediary edge servers are used only to pre-process data (e.g., images and videos) and the cloud servers are used to complete the tasks. Second, these approaches assume the pre-trained deep learning models deployed on cloud servers are static, and will not attempt to automatically upgrade in a context-aware manner. In this article, we propose a cloud-edge collaboration framework that facilitates delivering cognitive services with long-lasting, fast response, and high accuracy properties. We fist deploy a shallow model (i.e., EdgeCNN) on the edge server and a deep model (i.e., CloudCNN) on the cloud server. EdgeCNN can provide durable and rapid response cognitive services, because edge servers not only provide computing resources for mobile applications, but also close to users. Then, we enable CloudCNN to assist in training EdgeCNN to improve the performance of the latter. Thus, EdgeCNN also provides high-accuracy cognitive services. Furthermore, because users may continue to upload data to edge servers in real-world scenarios, we propose to use the ongoing assistance of CloudCNN to further improve the accuracy of the shallow model. Experimental results show that EdgeCNN can reduce the average response time of cognitive services by up to 55.08 percent and improve accuracy by up to 26.70 percent.},
  archive      = {J_TCC},
  author       = {Chuntao Ding and Ao Zhou and Yunxin Liu and Rong N. Chang and Ching-Hsien Hsu and Shangguang Wang},
  doi          = {10.1109/TCC.2020.2997008},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {1489-1499},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A cloud-edge collaboration framework for cognitive service},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PKE-MET: Public-key encryption with multi-ciphertext
equality test in cloud computing. <em>TCC</em>, <em>10</em>(2),
1476–1488. (<a href="https://doi.org/10.1109/TCC.2020.2990201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing enables users to remove the necessity of the need of local hardware architecture, which removes the burden of the users from high computation costs. Therefore, it has attracted much attention and research has been conducted heavily on it. To protect users’ privacy, data is usually encrypted prior to being sent to the cloud server. As the resulting system is unusable, since the cloud can no longer search throughout the data, new cryptographic primitive such as public-key encryption with equality test (PKEET) has been introduced. In PKEET, users can test whether the underlying messages of two ciphertexts encrypted under different public keys are equal or not without the need to decrypt those ciphertexts. This is a very useful tool, especially for the cloud database, since PKEET mainly focuses on the equality test between two ciphertexts. However, in practice, the cloud server may need to verify the equivalence among more than two ciphertexts. This leads to disclosing unnecessary information of users and redundant computation cost will also occur when using traditional PKEET schemes. How to make this more efficient and practical remains an interesting research problem. In this article, to solve the aforementioned problems by providing a novel concept of public-key encryption with multi-ciphertext equality test (PKE-MET). In PKE-MET, each ciphertext can designate a number $s$ such that the cloud server can only perform equality test on this ciphertext with other $s-1$ ciphertexts, where all their designated numbers are $s$ . For PKE-MET, besides traditional OW-CPA and IND-CPA security, we specially define Number security. We instantiate PKE-MET to a concrete scheme and give its security proof. Furthermore, to enable the primitive to be more practical in applications, we extend it to the concept of PKE with flexible MET (PKE-FMET). In PKE-FMET, the cloud server can perform equality test on any number of ciphertexts as long as the maximum number of their designated numbers is less than or equal to the number of ciphertexts. We construct a PKE-FMET scheme based on our PKE-MET construction and prove its security under the defined security models. Besides, the performance analysis mainly of efficiency and security between our constructions and existing equality test schemes in cloud computing show that our proposed schemes are more efficient and secure in the multi-ciphertext scenario.},
  archive      = {J_TCC},
  author       = {Willy Susilo and Fuchun Guo and Zhen Zhao and Ge Wu},
  doi          = {10.1109/TCC.2020.2990201},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1476-1488},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {PKE-MET: Public-key encryption with multi-ciphertext equality test in cloud computing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A sketching approach for obtaining real-time statistics over
data streams in cloud. <em>TCC</em>, <em>10</em>(2), 1462–1475. (<a
href="https://doi.org/10.1109/TCC.2020.2987023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many applications of complex event processing (CEP) in Cloud can tolerate analytical errors to some extent, and it provides us an opportunity to optimize real-time analytics using methods of approximate query processing over big data streams. In this article, we present a novel rules-based sampling technique, which supports to construct sketch over one-pass and high-speed asynchronous data streams and provides accurate answers for different types of analytical queries. Moreover, we propose two methods of distributed sketching implementation, i.e., D-AQP $_b$ and D-AQP $_i$ , to make our approach to be compatible with batch processing and interactive processing architectures respectively, and be appropriate for stream processing systems in Cloud. Experimental results with real-world and synthetic datasets indicate that our approach can obtain more accurate estimates and improve two times of system throughput when compared with state-of-the-art Hadoop-based approximate engine BlinkDB. When compared with current batch processing systems Spark and stream processing system Spark-Streaming, our methods of D-AQP $_b$ and D-AQP $_i$ can achieve 2 and 4 orders of magnitude improvement on query response time respectively.},
  archive      = {J_TCC},
  author       = {Guangjun Wu and Xiaochun Yun and Yong Wang and Shupeng Wang and Binbin Li and Yong Liu},
  doi          = {10.1109/TCC.2020.2987023},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1462-1475},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A sketching approach for obtaining real-time statistics over data streams in cloud},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards secure and efficient equality conjunction search
over outsourced databases. <em>TCC</em>, <em>10</em>(2), 1445–1461. (<a
href="https://doi.org/10.1109/TCC.2020.2975175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searchable symmetric encryption enables a cloud server to answer queries directly over encrypted data. Two key requirements are a strong security guarantee and a sub-linear search performance. The bucketization approach in the literature addresses these requirements at the expense of downloading false positives and requiring the local search at the client side. In this article, we propose a novel approach to meet these requirements while minimizing the clients work and communication cost. First, a relaxed notion of ciphertext indistinguishability on partitioned data is formalized, called class indistinguishability, which provides a level of ciphertext indistinguishability similar to that of bucketization but allows the server to perform search of relevant data and filter false positives. We present a construction for achieving these goals through a two-phase search algorithm. The first phase finds a candidate set through a sub-linear search. The second phase finds the exact query result using a linear search applied to the candidate set. The experiment results on large real-world data-sets show that our approach outperforms the state-of-the-art. This article focuses on the class of equality conjunction search, but it applies to the general class of Boolean queries of equalities because the latter can be reduced to several equality conjunction queries.},
  archive      = {J_TCC},
  author       = {Weipeng Lin and Ke Wang and Zhilin Zhang and Ada Waichee Fu and Raymond Chi-Wing Wong and Cheng Long and Chunyan Miao},
  doi          = {10.1109/TCC.2020.2975175},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1445-1461},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Towards secure and efficient equality conjunction search over outsourced databases},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-synchronized non-blocking concurrent kernel cruising.
<em>TCC</em>, <em>10</em>(2), 1428–1444. (<a
href="https://doi.org/10.1109/TCC.2020.2970183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel heap buffer overflow vulnerabilities have been exposed for decades, but there are few practical countermeasures that can be applied to OS kernels. Previous solutions either suffer from high performance overhead or compatibility problems with mainstream kernels and hardware. In this article, we present Kruiser , a concurrent kernel heap buffer overflow monitor. Unlike conventional methods, the security enforcement of which is usually inlined into the kernel execution, Kruiser migrates security enforcement from the kernel’s normal execution to a concurrent monitor process, leveraging the increasingly popular multi-core architectures. To reduce the synchronization overhead between the monitor process and the running kernel, we design a novel semi-synchronized non-blocking monitoring algorithm, which enables efficient runtime detection on live memory without incurring false positives. To prevent the monitor process from being tampered and provide guaranteed performance isolation, we utilize the virtualization technology to run the monitor process out of the monitored VM, while heap memory allocation information is collected inside the monitored VM in a secure and efficient way. The hybrid VM monitoring technique combined with the secure canary that cannot be counterfeited by attackers provides guaranteed overflow detection with high efficiency. We have implemented a prototype of Kruiser based on Linux and the Xen/KVM hypervisor. The evaluation shows that Kruiser can detect realistic kernel heap buffer overflow attacks in cloud environment effectively with minimal cost.},
  archive      = {J_TCC},
  author       = {Donghai Tian and Qiang Zeng and Dinghao Wu and Peng Liu and Changzhen Hu},
  doi          = {10.1109/TCC.2020.2970183},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1428-1444},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Semi-synchronized non-blocking concurrent kernel cruising},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SecureAD: A secure video anomaly detection framework on
convolutional neural network in edge computing environment.
<em>TCC</em>, <em>10</em>(2), 1413–1427. (<a
href="https://doi.org/10.1109/TCC.2020.2990946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection offers a powerful approach to identifying unusual activities and uncommon behaviors in real-world video scenes. At present, convolutional neural networks (CNN) have been widely used to tackle anomalous events detection, which mainly rely on its stronger ability of feature representation than traditional hand-crafted features. However, massive video data and high cost of CNN model training are a challenge to achieve satisfactory detection results for resource-limited users. In this article, we propose a secure video anomaly detection framework (SecureAD) based on CNN. Specifically, we introduce additive secret sharing to design several calculation protocols for achieving safe CNN training and video anomaly detection. Besides, we propose a Bloom filter based fine-grained access control policy to authenticate legitimate users, without leaking the privacy of raw personal attributes. In addition, edge computing instead of cloud computing is integrated into the architecture to reduce response time between servers and users in an outsourced environment. Finally, we prove that the proposed SecureAD achieves secure video anomaly detection without compromising the privacy of the related data. Also, the simulation results demonstrate the effectiveness and security of our SecureAD.},
  archive      = {J_TCC},
  author       = {Hang Cheng and Ximeng Liu and Huaxiong Wang and Yan Fang and Meiqing Wang and Xiaopeng Zhao},
  doi          = {10.1109/TCC.2020.2990946},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1413-1427},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {SecureAD: A secure video anomaly detection framework on convolutional neural network in edge computing environment},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accurate fault location using deep neural evolution network
in cloud data center interconnection. <em>TCC</em>, <em>10</em>(2),
1402–1412. (<a href="https://doi.org/10.1109/TCC.2020.2974466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the threat of failure and the discrete distribution of data center users, the research of distributed cloud data center provides real-time cloud services with robustness, reliability and security. Faced with data center interconnection, network failures cause mass services delay and interruption, which do a great damage to cloud computing. Many researchers have studied fault location methods in data center interconnection, which are easy to trap in local optimum limited by search capability and reduce the accuracy of location, especially when confronted with large-scale alarm information. In this article, the deep neural evolution network is introduced to extract deep-hidden fault features from massive collected alarm information in cloud data center interconnection. It has the prominent capacity of global search without the constraint of gradient to realize the breakthrough of fault location accuracy. The fault location method based on deep neural evolution network (FL-DNEN) is applied which uses the alarm set and suspicious scope of fault getting from fault propagation model as input and export deterministic faults accurately. The emulations demonstrate that the proposed method dramatically improves the accuracy of fault location to 92 percent with large-scale alarm information, which improves the resilience of cloud data center interconnection dramatically.},
  archive      = {J_TCC},
  author       = {Hui Yang and Xudong Zhao and Qiuyan Yao and Ao Yu and Jie Zhang and Yuefeng Ji},
  doi          = {10.1109/TCC.2020.2974466},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1402-1412},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Accurate fault location using deep neural evolution network in cloud data center interconnection},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time prediction of docker container resource load based
on a hybrid model of ARIMA and triple exponential smoothing.
<em>TCC</em>, <em>10</em>(2), 1386–1401. (<a
href="https://doi.org/10.1109/TCC.2020.2989631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More and more enterprises are beginning to use Docker containers to build cloud platforms. Predicting the resource usage of container workload has been an important and challenging problem to improve the performance of cloud computing platform. The existing prediction models either incur large time overhead or have insufficient accuracy. This article proposes a hybrid model of the ARIMA and triple exponential smoothing. It can accurately predict both linear and nonlinear relationships in the container resource load sequence. To deal with the dynamic Docker container resource load, the weighting values of the two single models in the hybrid model are chosen according to the sum of squares of their predicted errors for a period of time. We also design and implement a real-time prediction system that consists of the collection, storage, prediction of Docker container resource load data and scheduling optimization of CPU and memory resource usage based on predicted values. The experimental results show that the predicting accuracy of the hybrid model improves by 52.64, 20.15, and 203.72 percent on average compared to the ARIMA, the triple exponential smoothing model and ANN+SaDE model respectively with a small time overhead.},
  archive      = {J_TCC},
  author       = {Yulai Xie and Minpeng Jin and Zhuping Zou and Gongming Xu and Dan Feng and Wenmao Liu and Darrell Long},
  doi          = {10.1109/TCC.2020.2989631},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1386-1401},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Real-time prediction of docker container resource load based on a hybrid model of ARIMA and triple exponential smoothing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FPGA-accelerated searchable encrypted database management
systems for cloud services. <em>TCC</em>, <em>10</em>(2), 1373–1385. (<a
href="https://doi.org/10.1109/TCC.2020.2969655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of database management systems (DBMSs) as a cloud service is rapidly expanding. Cloud DBMSs offer many advantages, such as easier management, lower costs, and greater scalability. However, there are still security concerns regarding attacks from adversaries. DBMSs that use searchable encryption have been investigated with regard to ensuring their security. Because searchable encryption allows query execution over encrypted data in the cloud, sensitive data can be securely stored there in the cloud. On the other hand, encrypted query processing is slower than query processing on plaintext data. In this article, we use a field-programmable gate array (FPGA) to accelerate query processing in a searchable encrypted DBMS. We also propose a new cache function to shorten the access time to database tables in a DBMS. According to an evaluation using basic queries, the proposed system has achieved up to 110.7 times speed-up compared with the central processing unit (CPU) processing of a single core. In addition, the proposed system can process queries faster than the plaintext processing on a CPU when processing large amounts of data.},
  archive      = {J_TCC},
  author       = {Mitsuhiro Okada and Takayuki Suzuki and Naoya Nishio and Hasitha Muthumala Waidyasooriya and Masanori Hariyama},
  doi          = {10.1109/TCC.2020.2969655},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1373-1385},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {FPGA-accelerated searchable encrypted database management systems for cloud services},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ES2: Building an efficient and responsive event path for i/o
virtualization. <em>TCC</em>, <em>10</em>(2), 1358–1372. (<a
href="https://doi.org/10.1109/TCC.2020.2969660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypervisor intervention in the virtual I/O event path is a main performance bottleneck for I/O virtualization because of the incurred costly VM exits. The shortcomings of prior software solutions against virtual interrupt delivery, a major source of VM exits, promoted the emergence of the hardware-based Posted-Interrupt (PI) technology. PI can provide non-exit interrupt delivery without compromising any virtualization benefit. However, it only acts on the half of the event path, i.e., the interrupt path, while guests I/O requests may also trigger a large amount of VM exits. Additionally, PI may still suffer a severe latency from the vCPU scheduling while delivering interrupts. Aiming at an optimal event path, we propose ES2 to simultaneously improve bidirectional I/O event delivery between guests and their devices. On the basis of PI, ES2 introduces hybrid I/O handling scheme for efficient I/O request delivery and intelligent interrupt redirection for enhanced I/O responsiveness. It does not require any modification to guest OS. We demonstrate that ES2 greatly reduces I/O-related VM exits with the exit handling time (EHT) below 2.5 percent for TCP streams and 0.1 percent for UDP streams, increases guest throughput by 1.9x for Memcached and 1.6x for Nginx, and keeps guest latency at a low level.},
  archive      = {J_TCC},
  author       = {Xiaokang Hu and Jian Li and Ruhui Ma and Haibing Guan},
  doi          = {10.1109/TCC.2020.2969660},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1358-1372},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {ES2: Building an efficient and responsive event path for I/O virtualization},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). When FPGA meets cloud: A first look at performance.
<em>TCC</em>, <em>10</em>(2), 1344–1357. (<a
href="https://doi.org/10.1109/TCC.2020.2992548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud service providers promote their new field programmable gate array (FPGA) infrastructure as a service (IaaS) as the new era of cloud product. This FPGA IaaS wraps virtualized compute resources with FPGA boards, e.g., Amazon AWS F1, and reserves acceleration capability for specific applications. Though this acceleration technique sounds promising, questions like real world performance, best-fit scenarios, portability, etc., still need further clarification. In this article, we present one of the first few empirical studies that take a close look at FPGA clouds from the tenants’ perspective. We have conducted measurement studies on Amazon AWS, Alibaba, and Huawei clouds for over one year. The experimental results show that: (1) Tenants experience severe performance-cost imbalance on FPGA IaaS platforms; (2) The inter-communication performance in FPGA clouds is tightly constrained by hardware drivers, e.g., small optimization of DMA drivers for PCIe can harvest significant performance gain; (3) The virtualized FPGA clouds are far from mature, e.g., small-sized jobs can greatly degrade the performance of FPGA clouds due to underutilized PCIe bandwidth. Our study not only provides useful hints to help tenants with FPGA service selection, but also sheds some lights for cloud providers to improve the performance of FPGA clouds.},
  archive      = {J_TCC},
  author       = {Xiuxiu Wang and Yipei Niu and Fangming Liu and Zichen Xu},
  doi          = {10.1109/TCC.2020.2992548},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1344-1357},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {When FPGA meets cloud: A first look at performance},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guaranteeing performance SLAs of cloud applications under
resource storms. <em>TCC</em>, <em>10</em>(2), 1329–1343. (<a
href="https://doi.org/10.1109/TCC.2020.2985372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern data centers, enterprise cloud instances run not only foreground applications like web and databases, but also different background services (e.g., backup, virus/compliance scan, batch) to manage the cloud instances securely and improve the overall resource utilization. These background services often incur resource storms that suddenly consume a lot of shared resources on cloud instances. The resource storms significantly degrade the performance of foreground applications by interfering in the preemption of the shared resources, resulting in frequent SLA violations. However, stock OS schedulers are not designed to handle these situations, and prior works are insufficient to address such resource storms under highly dynamic cloud workloads. This article presents Orchestra, a cloud-specific framework for controlling multiple applications in the user space, aiming at meeting corresponding SLAs. Orchestra takes an online approach with lightweight monitoring and performance models for both applications on the fly. It optimizes the resource allocations to meet corresponding SLAs. We evaluate the performance of Orchestra on a production cloud with a diverse range of SLAs. Orchestra guarantees the foreground application&#39;s performance SLAs at all times. At the same time, Orchestra maintains the background&#39;s performance by minimizing its performance penalty with proper allocation of the shared resources.},
  archive      = {J_TCC},
  author       = {In Kee Kim and Jinho Hwang and Wei Wang and Marty Humphrey},
  doi          = {10.1109/TCC.2020.2985372},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1329-1343},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Guaranteeing performance SLAs of cloud applications under resource storms},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A dynamic deep-learning-based virtual edge node placement
scheme for edge cloud systems in mobile environment. <em>TCC</em>,
<em>10</em>(2), 1317–1328. (<a
href="https://doi.org/10.1109/TCC.2020.2974948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge node placement is a key topic to edge cloud systems for that it affects their service performances significantly. Previous solutions based on the existing information are not suitable for the mobile environment due to the mobility and random Internet access of end users. In this article, we propose a dynamic virtual edge node placement scheme, in which the edge node placement strategy is generated based on the prediction information. Our placement scheme applies the pay-as-you-go and Spot Instance model of cloud computing, which may allocate the service resources with low cost conveniently and flexibly. What’s more, Long Short-Term Memory (LSTM) is implemented to predict the information of end users’ requests and the resources’ prices, endowing the generated placement strategy with the adaptability to the change of end users. At last, a set of hierarchical-clustering-based placement algorithms are proposed, which not only locate virtual edge nodes and allocate their corresponding service resources actively, but also guarantee the service quality of end users with low time complexity. The simulation with trace data shows that compared with K-means-clustering-based placement schemes, our virtual edge node placement scheme can provide users with high-quality service in terms of network delay with relatively low placement cost time-efficiently.},
  archive      = {J_TCC},
  author       = {Xiaoqun Yuan and Mengting Sun and Wenjing Lou},
  doi          = {10.1109/TCC.2020.2974948},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1317-1328},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A dynamic deep-learning-based virtual edge node placement scheme for edge cloud systems in mobile environment},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal resource allocation of cloud-based spark
applications. <em>TCC</em>, <em>10</em>(2), 1301–1316. (<a
href="https://doi.org/10.1109/TCC.2020.2985682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the big data paradigm is consolidating its central position in the industry, as well as in society at large. Lots of applications, across disparate domains, operate on huge amounts of data and offer great advantages both for business and research. According to analysts, cloud computing adoption is steadily increasing to support big data analyses and Spark is expected to take a prominent market position for the next decade. As big data applications gain more and more importance over time and given the dynamic nature of cloud resources, it is fundamental to develop an intelligent resource management system to provide Quality of Service guarantees to end-users. This article presents a set of run-time optimization-based resource management policies for advanced big data analytics. Users submit Spark applications characterized by a priority and by a hard or soft deadline. Optimization policies address two scenarios: i) identification of the minimum capacity to run a Spark application within the deadline; ii) re-balance of the cloud resources in case of heavy load, minimising the weighted soft deadline application tardiness. The solution relies on an initial non-linear programming model formulation and a search space exploration based on simulation-optimization procedures. Spark application execution times are estimated by relying on a gamut of techniques, including machine learning, approximated analyses, and simulation. The benefits of the approach are evaluated on Microsoft Azure HDInsight and on a private cloud cluster based on POWER8 by considering the TPC-DS industry benchmark and SparkBench. The results obtained in the first scenario demonstrate that the percentage error of the prediction of the optimal resource usage with respect to system measurement and exhaustive search is in the range 4–29 percent while literature-based techniques present an average error in the range 6–63 percent. Moreover, in the second scenario, the proposed algorithms can address complex problems like computing the optimal redistribution of resources among tens of applications in less than a minute with an error of 8 percent on average. On the same considered tests, literature-based approaches obtain an average error of about 57 percent.},
  archive      = {J_TCC},
  author       = {Marco Lattuada and Enrico Barbierato and Eugenio Gianniti and Danilo Ardagna},
  doi          = {10.1109/TCC.2020.2985682},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1301-1316},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Optimal resource allocation of cloud-based spark applications},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NetStor: Network and storage traffic management for ensuring
application QoS in a hyperconverged data-center. <em>TCC</em>,
<em>10</em>(2), 1287–1300. (<a
href="https://doi.org/10.1109/TCC.2020.2969154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the rapid increase in resource requirements in large scale Data Centers (DCs), enterprises have brought hyperconverged architecture where the storage pool is built up by the individual storage components associated with different servers, and it is shared among all the Virtual Machines (VMs) or containers through a common network infrastructure. Due to the sharing of network bandwidth among the application generated network traffic and the storage traffic from shared storage infrastructure, quality of service (QoS) performances of networking and storage intensive applications are affected, which further impacts VM or container migrations with dynamic workload scenarios. In this article, we propose NetStor to ensure QoS for various collocated network and storage intensive workloads over a hyperconverged architecture and ensure QoS during VM or container migration. NetStor uses a workload estimation strategy for VMs and containers, and applies the strategic decisions for resource allocation and migration based on environmental learning and workload characterization. NetStor supports a dynamic QoS provisioning that is workload-agnostic catering to both VMs and containers, and is unique to the best of our knowledge. We have implemented NetStor over a hyperconverged DC architecture on a testbed and found that NetStor can enhance network performance significantly compared to other related mechanisms discussed in the literature.},
  archive      = {J_TCC},
  author       = {Sumitro Bhaumik and Ravi Bansal and Raja Karmakar and Satish Kumar Mopur and Saikat Mukherjee and Mandar Jagannath Chitale and Sandip Chakraborty},
  doi          = {10.1109/TCC.2020.2969154},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1287-1300},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {NetStor: Network and storage traffic management for ensuring application QoS in a hyperconverged data-center},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding the security implication of aborting virtual
machine live migration. <em>TCC</em>, <em>10</em>(2), 1275–1286. (<a
href="https://doi.org/10.1109/TCC.2020.2982900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Live migration of Virtual machines (VMs) has become a regular tool for edge and cloud operators to facilitate system maintenance, fault tolerance, and load balancing, with little impact on running instances. However, the potential security risks of live migration of VMs are still obscure. In this article, we expose a new vulnerability in the existing VM live migration approaches, especially the post-copy approach. The entire live migration mechanism relies upon reliable TCP connectivity for the transfer of the VM state. We demonstrate that, if the host server is vulnerable to off-path TCP attacks, the loss of TCP reliability leads to VM live migration failure. We demonstrate that, by intentionally aborting the TCP connection, attackers can cause unrecoverable memory inconsistency for post-copy , leading to a significant increase in downtime and performance degradation of the running VM. Additionally, we present detailed techniques to reset the migration connection under heavy networking traffic. We also propose effective defenses to secure the VM live migration. Our experimental results demonstrate that memory inconsistencies could be devastating to some applications, and it only takes a few minutes to reset a heavy migration connection.},
  archive      = {J_TCC},
  author       = {Xing Gao and Jidong Xiao and Haining Wang and Angelos Stavrou},
  doi          = {10.1109/TCC.2020.2982900},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1275-1286},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Understanding the security implication of aborting virtual machine live migration},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards intelligent provisioning of virtualized network
functions in cloud of things: A deep reinforcement learning based
approach. <em>TCC</em>, <em>10</em>(2), 1262–1274. (<a
href="https://doi.org/10.1109/TCC.2020.2985651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud of Things (CoT) is an integration of Internet of Things (IoT) and cloud computing, where Network Function Virtualization (NFV) can dynamically provide Virtualized Network Functions (VNFs) for IoT devices based on service-specific requirements. The provisioning of VNFs in CoT is formulated as an online decision-making problem, but widely used methods mostly focus on characterizing the environment using simple models to obtain the optimal solution. Valuable historical experience on provisioning for the best long-term benefits is ignored and Quality of Service (QoS) requirements for different types of CoT services are also not considered, which leads to inefficient and coarse-gained provisioning. In this article, an intelligent provisioning framework of VNFs is proposed for adaptive CoT resource scheduling according to traffic identification of heterogeneous network services. The framework leverages a Deep Reinforcement Learning (DRL)-based model to make decisions based on the complexity of network environments and traffic variances. In this model, a policy gradient DRL algorithm, namely, Policy Optimization using Kronecker-Factored Trust Region (POKTR) is adopted to obtain the stable performance by a novel surrogate objective function. Experimental results verify that our framework improves the QoS in CoT by real-time VNFs provisioning. The DRL-based model with POKTR algorithm reduces network congestion and achieves higher throughput than other DRL algorithms.},
  archive      = {J_TCC},
  author       = {Bo He and Jingyu Wang and Qi Qi and Haifeng Sun and Jianxin Liao},
  doi          = {10.1109/TCC.2020.2985651},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1262-1274},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Towards intelligent provisioning of virtualized network functions in cloud of things: A deep reinforcement learning based approach},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Flexible threshold ring signature in chronological order for
privacy protection in edge computing. <em>TCC</em>, <em>10</em>(2),
1253–1261. (<a href="https://doi.org/10.1109/TCC.2020.2974954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing can process data at the network edge rather than in the remote cloud. To protect the security of the collected data, blockchain can be used in edge computing. Any request sent from an edge server requesting data storage is a transaction added into the blockchain, which should include the data owners’ signature. For protecting the privacy of data owners, a threshold ring signature can be utilized for the blockchain. In this article, we propose a flexible threshold ring signature scheme in chronological order that has two advantages for solving the update problem and chronological problem in practice. Our scheme is a nontrivial extension of Yuen et al. ’s signature scheme and has three security properties: unforgeability , anonymity, and chronological sorting . We prove our scheme without random oracles and test our scheme over the Intel Edison development platform simulating practical edge servers.},
  archive      = {J_TCC},
  author       = {Zhiwei Wang and Jiaxing Fan},
  doi          = {10.1109/TCC.2020.2974954},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1253-1261},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Flexible threshold ring signature in chronological order for privacy protection in edge computing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DNA similarity search with access control over encrypted
cloud data. <em>TCC</em>, <em>10</em>(2), 1233–1252. (<a
href="https://doi.org/10.1109/TCC.2020.2968893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DNA similarity search has been widely applied in human genomic studies including DNA marking, genomic sequencing and genetic disease prediction. Meanwhile, with the explosive growth of data, users are increasingly inclining to store DNA data on the cloud for saving local cost. However, the high sensitivity of DNA data has forced the government to strictly control its acquisition and utilization. One potential solution is to encrypt DNA data before outsourcing them to the cloud. Nevertheless, private DNA similarity query has been an active research issue, state-of-the-art results are still defective in security, functionality, and efficiency. In this article, we propose EFSS, an efficient and fine-grained similarity search scheme over encrypted DNA data. In specific, first, we design an approximation algorithm to efficiently calculate the edit distances between two sequences. Second, we put forward a novel Boolean search strategy to achieve complicated logic queries such as mixed “AND” and “NO” operations on genes. Third, data access control is also supported in our EFSS through a variant of polynomial based design. Moreover, the K-means clustering algorithm is exploited to further improve the efficiency of execution. In the end, security analysis and extensive experiments demonstrate the high performance of EFSS compared with existing schemes.},
  archive      = {J_TCC},
  author       = {Guowen Xu and Hongwei Li and Hao Ren and Xiaodong Lin and Xuemin Shen},
  doi          = {10.1109/TCC.2020.2968893},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1233-1252},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {DNA similarity search with access control over encrypted cloud data},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Malicious node detection scheme based on correlation of data
and network topology in fog computing-based VANETs. <em>TCC</em>,
<em>10</em>(2), 1215–1232. (<a
href="https://doi.org/10.1109/TCC.2020.2985050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In vehicle ad hoc networks (VANETs), if a legal vehicle node becomes malicious, then it is more likely to tamper with transferred data or provide false data easily. Because the malicious node is a valid internal user in VANETs, its behavior is difficult to be detected only through some cryptographic methods. Then the behavior may cause many serious traffic accidents. Based on the available (unencrypted) data only, how to detect out the internal malicious vehicle nodes by some lightweight methods needs to be researched in VANETs. Additionally, fog computing seamlessly integrates heterogeneous computing resources widely distributed in edge networks and then provides stronger computing services for users. Therefore, in this article, we propose a malicious node detection scheme in fog computing-based VANETs, where the fog server uses the reputation calculation to score each suspicious node based on the correlation of acquired data and network topology. In our proposed scheme, we build a reputation mechanism to score each suspicious node according to the correlation between outlier detection of acquired data and influence of nodes. Based on our proposed experiments, our proposed scheme can efficiently and effectively detect out malicious vehicle nodes so that fog server can acquire more true data.},
  archive      = {J_TCC},
  author       = {Ke Gu and XinYing Dong and WeiJia Jia},
  doi          = {10.1109/TCC.2020.2985050},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1215-1232},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Malicious node detection scheme based on correlation of data and network topology in fog computing-based VANETs},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tag-based verifiable delegated set intersection over
outsourced private datasets. <em>TCC</em>, <em>10</em>(2), 1201–1214.
(<a href="https://doi.org/10.1109/TCC.2020.2968320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Verifiable delegated set intersection over outsourced private datasets (VDPSI) enables two parties to outsource their private datasets and delegate the computation of set intersection to the cloud while being able to check the correctness of the result. In this process, the cloud learns nothing about the datasets and the intersection result. However, the existing VDPSI schemes suffer from three substantial shortcomings that limit their use: i) the whole dataset consists of only one subset, ii) they are designed for the static data, and iii) they cannot support other operations. To resolve these problems, we introduce a novel primitive called tag-based VDPSI (TVDPSI), which is designed for the multi-subset case where each subset is associated with one single tag for data classification. To protect privacy, the data is encrypted before being resided to the cloud. The tag is implicitly hidden in each encrypted element. As a result, the cloud cannot learn which data belongs to the same subset beyond the intersection set. Besides, the cloud cannot calculate the intersection except under the permissions of data owners. To the best of our knowledge, TVDPSI is the first VDPSI scheme supporting dynamic update and count operations. The detailed performance evaluation and simulation show that our protocol is more practical in cloud computing.},
  archive      = {J_TCC},
  author       = {Qiang Wang and Fucai Zhou and Jian Xu and Su Peng},
  doi          = {10.1109/TCC.2020.2968320},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1201-1214},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Tag-based verifiable delegated set intersection over outsourced private datasets},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bio-inspired formal model for space/time virtual machine
randomization and diversification. <em>TCC</em>, <em>10</em>(2),
1190–1200. (<a href="https://doi.org/10.1109/TCC.2020.2969353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies on resiliency against system attacks have contributed well established defensive techniques, sound protocols and paradigms in distributed systems’ literature. One of this contribution is credited to redundancy and replication techniques which is proven to be a double–edged–sword, by increasing the number of nodes inherently increases the system&#39;s attack-vector – the set of ways an attacker can compromise a system. To remedy this issue, system randomization and diversification has been considered as an effective defensive strategy, referred to as a Moving Target Defense (MTD). In this article, we introduce a bio-inspired formal model for space/time system randomization/diversification and a quantification scheme for virtual machines (VMs) in a cloud computing environment. We show the practicality of the model with a MTD framework (Mayflies) integrated into the cloud management software stack (OpenStack) and illustrate with realistic VM attacks and proactive defense use cases.},
  archive      = {J_TCC},
  author       = {Noor Ahmed and Bharat Bhargava},
  doi          = {10.1109/TCC.2020.2969353},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1190-1200},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Bio-inspired formal model for Space/Time virtual machine randomization and diversification},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy aware virtual network migration. <em>TCC</em>,
<em>10</em>(2), 1173–1189. (<a
href="https://doi.org/10.1109/TCC.2020.2976966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In network virtualization, one of the key problems is to embed a sequence of virtual networks with both node and link constraints onto the physical network, which is known to be NP-hard. Recent studies focus on how to minimize the energy cost while maximizing the revenue of the physical network when the VN request arrives. However, after a period of time, due to the dramatic dynamics of the resources of the physical network, the previous solution may become less energy efficient. In this article, we study how to re-optimize the energy cost by leveraging the migration technique. In particular, we first give the problem model of virtual network migration. Then we design two energy aware virtual network migration algorithms called EA-VNM and EA-VNM-G. For EA-VNM, it answers the following key questions: when to perform migration, migrate which virtual nodes to where, and how to perform migration. Especially, for EA-VNM-G, it further reduces the high time complexity problem of EA-VNM by grouping the virtual nodes to be migrated with fewer conflicts. Extensive simulations show that EA-VNM significantly reduces the energy cost by up to 25 percent over the state-of-the-art algorithm while maintaining similar revenue and EA-VNM-G reduces the running time significantly.},
  archive      = {J_TCC},
  author       = {Zhongbao Zhang and Huafeng Cao and Sen Su and Weitian Li},
  doi          = {10.1109/TCC.2020.2976966},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1173-1189},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Energy aware virtual network migration},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Failure aware semi-centralized virtual network embedding in
cloud computing fat-tree data center networks. <em>TCC</em>,
<em>10</em>(2), 1156–1172. (<a
href="https://doi.org/10.1109/TCC.2020.2984604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Cloud Computing, the tenants opting for the Infrastructure as a Service (IaaS) send the resource requirements to the Cloud Service Provider (CSP) in the form of Virtual Network (VN) consisting of a set of inter-connected Virtual Machines (VM). Embedding the VN onto the existing physical network is known as Virtual Network Embedding (VNE) problem. One of the major research challenges is to allocate the physical resources such that the failure of the physical resources would bring less impact onto the users’ service. Additionally, the major challenge is to handle the embedding process of growing number of incoming users’ VNs from the algorithm design point-of-view. Considering both of the above-mentioned research issues, a novel Failure aware Semi-Centralized VNE (FSC-VNE) algorithm is proposed for the Fat-Tree data center network with the goal to reduce the impact of the resource failure onto the existing users. The impact of failure of the Physical Machines (PMs), physical links and network devices are taken into account while allocating the resources to the users. The beauty of the proposed algorithm is that the VMs are assigned to different PMs in a semi-centralized manner. In other words, the embedding algorithm is executed by multiple physical servers in order to concurrently embed the VMs of a VN and reduces the embedding time. Extensive simulation results show that the proposed algorithm can outperform over other VNE algorithms.},
  archive      = {J_TCC},
  author       = {Chinmaya Kumar Dehury and Prasan Kumar Sahoo},
  doi          = {10.1109/TCC.2020.2984604},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1156-1172},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Failure aware semi-centralized virtual network embedding in cloud computing fat-tree data center networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Similarity search for encrypted images in secure cloud
computing. <em>TCC</em>, <em>10</em>(2), 1142–1155. (<a
href="https://doi.org/10.1109/TCC.2020.2989923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of intelligent terminals, the Content-Based Image Retrieval (CBIR) technique has attracted much attention from many areas (i.e., cloud computing, social networking services, etc.). Although existing privacy-preserving CBIR schemes can guarantee image privacy while supporting image retrieval, these schemes still have inherent defects (i.e., low search accuracy, low search efficiency, key leakage, etc.). To address these challenging issues, in this article we provide a similarity Search for Encrypted Images in secure cloud computing (called SEI). First, the feature descriptors extracted by the Convolutional Neural Network (CNN) model are used to improve search accuracy. Next, an encrypted hierarchical index tree by using $K$ -means clustering based on Affinity Propagation (AP) clustering is devised, which can improve search efficiency. Then, a limited key-leakage k-Nearest Neighbor (kNN) algorithm is proposed to protect key from being completely leaked to untrusted image users. Finally, SEI is extended to further prevent image users’ search information from being exposed to the cloud server. Our formal security analysis proves that SEI can protect image privacy as well as key privacy. Our empirical experiments using a real-world dataset illustrate the higher search accuracy and efficiency of SEI.},
  archive      = {J_TCC},
  author       = {Yingying Li and Jianfeng Ma and Yinbin Miao and Yue Wang and Ximeng Liu and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TCC.2020.2989923},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1142-1155},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Similarity search for encrypted images in secure cloud computing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Power of random choices made efficient for fog computing.
<em>TCC</em>, <em>10</em>(2), 1130–1141. (<a
href="https://doi.org/10.1109/TCC.2020.2968443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider a load balancing protocol based on the power of random choices that is adapted to a fog deploy in which several independent fog nodes equipped with a set of servers or VM are serving the same geographical area. The protocol is based on a simple but effective mechanism based on a threshold $T$ . When a fog node receives a unit of computation or a job, it immediately executes the job if the number of its occupied servers is lower than $T$ , otherwise the node executes a randomized algorithm by probing $F$ other fog nodes in the area, and delegates the execution of the job to the least loaded one, provided the workload is lower than the probing node. Through a mathematical analysis we show that probing just one node ( $F=1$ ) when there are less than two free VMs provides the same performance of the well known power-of-two random choices centralized algorithm, but at a much lower delay and control overhead costs. Also, simulations are used to address the node heterogeneity and, with a real testbed, we offer results that prove the effective benefit of the proposed solution in practical applications.},
  archive      = {J_TCC},
  author       = {Roberto Beraldi and Gabriele Proietti Mattia},
  doi          = {10.1109/TCC.2020.2968443},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1130-1141},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Power of random choices made efficient for fog computing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Resource allocation for cloud-based software services using
prediction-enabled feedback control with reinforcement learning.
<em>TCC</em>, <em>10</em>(2), 1117–1129. (<a
href="https://doi.org/10.1109/TCC.2020.2992537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With time-varying workloads and service requests, cloud-based software services necessitate adaptive resource allocation for guaranteeing Quality-of-Service (QoS) and reducing resource costs. However, due to the ever-changing system states, resource allocation for cloud-based software services faces huge challenges in dynamics and complexity. The traditional approaches mostly rely on expert knowledge or numerous iterations, which might lead to weak adaptiveness and extra costs. Moreover, existing RL-based methods target the environment with the fixed workload, and thus they are unable to effectively fit in the real-world scenarios with variable workloads. To address these important challenges, we propose a Prediction-enabled feedback Control with Reinforcement learning based resource Allocation (PCRA) method. First, a novel Q-value prediction model is designed to predict the values of management operations (by Q-values) at different system states. The model uses multiple prediction learners for making accurate Q-value prediction by integrating the Q-learning algorithm. Next, the objective resource allocation plans can be found by using a new feedback-control based decision-making algorithm. Using the RUBiS benchmark, simulation results demonstrate that the PCRA chooses the management operations of resource allocation with 93.7 percent correctness. Moreover, the PCRA achieves optimal/near-optimal performance, and it outperforms the classic ML-based and rule-based methods by 5 $\sim$ 7\% and 10 $\sim$ 13\%, respectively.},
  archive      = {J_TCC},
  author       = {Xing Chen and Fangning Zhu and Zheyi Chen and Geyong Min and Xianghan Zheng and Chunming Rong},
  doi          = {10.1109/TCC.2020.2992537},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1117-1129},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Resource allocation for cloud-based software services using prediction-enabled feedback control with reinforcement learning},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Microscaler: Cost-effective scaling for microservice
applications in the cloud with an online learning approach.
<em>TCC</em>, <em>10</em>(2), 1100–1116. (<a
href="https://doi.org/10.1109/TCC.2020.2985352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the microservice becomes a popular architecture to construct cloud native systems due to its agility. In cloud native systems, autoscaling is a key enabling technique to adapt to workload changes by acquiring or releasing the right amount of computing resources. However, it becomes a challenging problem in microservice applications, since such an application usually comprises a large number of different microservices with complex interactions. When the performance decreases due to an unpredictable workload peak, it is difficult to pinpoint the scaling-needed services which need to scale out and evaluate how many resources they need. In this article, we present a novel system named Microscaler to automatically identify the scaling-needed services and scale them to meet the Service Level Agreement (SLA) with an optimal cost for microservice applications. Microscaler first collects the quality of service (QoS) metrics in the service mesh enabled microservice infrastructure. Then, it determines under-provisioning or over-provisioning service instances along the service dependency graph with a novel scaling-needed service criterion named service power . The service dependency graph could be obtained by correlating each request flow in the service mesh. By combining an online learning approach and a step-by-step heuristic approach, Microscaler can precisely reach the optimal service scale meeting the SLA requirements. The experimental evaluations in a microservice benchmark show that Microscaler achieves an average 93 percent precision in scaling-needed service determination and converges to the optimal service scale faster than several state-of-the-art methods. Moreover, Microscaler is lightweight and flexible enough to work in a large-scale microservice system.},
  archive      = {J_TCC},
  author       = {Guangba Yu and Pengfei Chen and Zibin Zheng},
  doi          = {10.1109/TCC.2020.2985352},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1100-1116},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Microscaler: Cost-effective scaling for microservice applications in the cloud with an online learning approach},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). S-blocks: Lightweight and trusted virtual security function
with SGX. <em>TCC</em>, <em>10</em>(2), 1082–1099. (<a
href="https://doi.org/10.1109/TCC.2020.2985045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the advantages of scalability and flexibility, Security Function Virtualization (SFV) raises concerns about its own security. To enhance the security of SFV, a promising approach is to run critical components of off-the-shelf security software inside Software Guard Extensions (SGX) enclaves. This idea, however, is hardly practical due to the difficulty of detaching components from the monolithic security function and the unacceptable cost of executing them inside enclaves. In this article, we propose S-Blocks, an architecture to modularize virtual security functions (VSFs) and protect crucial modules with SGX in an efficient manner. S-Blocks decomposes VSFs into trusted and untrusted modules and provides dedicated APIs systematically. Only crucial VSF modules are hardened with enclaves. Furthermore, aiming at addressing state consistency and secure migration issues of security function scaling, we design a fine-grained state synchronization and migration mechanism to ensure loss-free, order-preserving, and state security for VSFs. To demonstrate the effectiveness of our approach, we prototype S-Blocks using Fast-Click on a real Skylake platform and implement three critical types of virtual security functions based on the S-Blocks architecture. Our evaluation results show that S-Blocks only imposes a manageable performance overhead, and low latency and resource consumption when protecting VSFs.},
  archive      = {J_TCC},
  author       = {Juan Wang and Shirong Hao and Hongxin Hu and Bo Zhao and Hongda Li and Wenhui Zhang and Jun Xu and Peng Liu and Jing Ma},
  doi          = {10.1109/TCC.2020.2985045},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1082-1099},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {S-blocks: Lightweight and trusted virtual security function with SGX},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The effects of soft errors and mitigation strategies for
virtualization servers. <em>TCC</em>, <em>10</em>(2), 1065–1081. (<a
href="https://doi.org/10.1109/TCC.2020.2973146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtualized servers compose the majority of cloud computing environments, where these nodes are used to host multiple clients over the same hardware. Many organizations run online applications by hiring elastic computing resources in order to match demand while reducing fixed costs. However, such organizations are unlikely to take advantage of these benefits for critical applications, as it would expose them to several risks. Among other threats, soft errors are a concern in large-scale reliable servers and are expected to become more frequent as a consequence of smaller transistors and lower operating voltages of integrated circuits. This article characterizes virtualized servers of cloud environments in presence of soft errors. Using fault injection, we collect experimental data to determine the failure modes of applications, operating systems, VMs, and hypervisor. The analysis exposes distinct failure modes, ranging from crash failures of a single virtual machine to silent data corruption in permanent storage. The most frequent failure mode, observed in 10–30 percent of injected errors, consists of a hang affecting multiple virtual machines. Given that such failures are a primary cause of downtime, we develop and evaluate a recovery mechanism which uses online testing and recovers a server from all hangs by rebooting its hypervisor.},
  archive      = {J_TCC},
  author       = {Frederico Cerveira and Raul Barbosa and Henrique Madeira and Filipe Araujo},
  doi          = {10.1109/TCC.2020.2973146},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1065-1081},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {The effects of soft errors and mitigation strategies for virtualization servers},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Privacy-preserving efficient verifiable deep packet
inspection for cloud-assisted middlebox. <em>TCC</em>, <em>10</em>(2),
1052–1064. (<a href="https://doi.org/10.1109/TCC.2020.2991167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing traffic volume, enterprises choose to outsource their middlebox services, such as deep packet inspection, to the cloud to acquire rich computational and communication resources. However, since the traffic is redirected to the public cloud, information leakages, such as packet payload and inspection rules, arouse privacy concerns of both middlebox owner and packet senders. To address the concerns, we propose an efficient verifiable deep packet inspection (EV-DPI) scheme with strong privacy guarantees. Specifically, a two-layer architecture is designed and deployed over two non-collusion cloud servers. The first layer fast filters out most of legitimate packets and the second layer supports exact rule matching. During the inspection, the privacy of packet payload and the confidentiality of inspection rules are well preserved. To improve the efficiency, only fast symmetric crypto-systems, such as hash functions, are used. Moreover, the proposed scheme allows the network administrator to verify the execution results, which offers a strong control of outsourced services. To validate the performance of the proposed EV-DPI scheme, we conduct extensive experiments on the Amazon Cloud. Large-scale dataset (millions of packets) is tested to obtain the key performance metrics. The experimental results demonstrate that EV-DPI not only preserves the packet privacy, but also achieves high packet inspection efficiency.},
  archive      = {J_TCC},
  author       = {Hao Ren and Hongwei Li and Dongxiao Liu and Guowen Xu and Nan Cheng and Xuemin Shen},
  doi          = {10.1109/TCC.2020.2991167},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1052-1064},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Privacy-preserving efficient verifiable deep packet inspection for cloud-assisted middlebox},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). APS: Adaptive packet spraying to isolate mix-flows in data
center network. <em>TCC</em>, <em>10</em>(2), 1038–1051. (<a
href="https://doi.org/10.1109/TCC.2020.2985037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern data centers host diverse applications, which generate a mix of short flows with stringent latency requirement and long flows requiring large sustained throughput. To solve the problem of resource competition between the mixed flows, we propose an adaptive traffic isolation scheme APS. Based on the packet spraying scheme in the multipath transmission, APS dynamically separates long flows from short ones on different paths to provide the low latency for the short flows. Meanwhile, to resolve the out-of-order problem, APS limits the long flows to a few paths with Equal Cost Multi Path (ECMP). Experimental results of NS2 simulation and testbed implementation show that, APS reduces the average completion time for short flows by up to 60 percent and increases the throughputs for long flows by about 1.68x over the state-of-the-art multipath transmission schemes.},
  archive      = {J_TCC},
  author       = {Jingling Liu and Jiawei Huang and Wenjun Lv and Jianxin Wang},
  doi          = {10.1109/TCC.2020.2985037},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1038-1051},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {APS: Adaptive packet spraying to isolate mix-flows in data center network},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bi-TCCS: Trustworthy cloud collaboration service scheme
based on bilateral social feedback. <em>TCC</em>, <em>10</em>(2),
1021–1037. (<a href="https://doi.org/10.1109/TCC.2020.2978810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a complementary technology to traditional network security, trust computing scheme has been playing an increasingly important role in providing cloud service. However, many organizations constantly face trust computing challenges; moreover, establishing a highly trustworthy cloud ecosystem can be costly and time-consuming. In this article, we originally propose the conceptual model and formal definitions for a trustworthy collaboration service ecosystem, and construct a Bi-trustworthy cloud collaboration service ( Bi-TCCS ), which is a scheme based on an innovative bilateral social feedback (referred to as “bi-feedback”) scheme. First, a trust-aware collaboration service model is proposed based on cloud service brokerages (CSBs), which can provide intermediation and aggregation capabilities to enable organizations to deploy their services across a collaborative cloud environment. Then, we propose a bi-feedback scheme based on the inherent social relationship among three network communities, which are composed of three types of network entities: cloud users, CSBs, and cloud service providers. The proposed scheme is effective and reliable against garnished and bad-mouthing attacks resulting from the traditional social feedback scheme. Moreover, we innovatively adopt an aggregating method for overall trust based on deviation analysis. This method can minimize errors and overcome the limitations of traditional schemes, where trust attributes are weighted manually. Theoretical analysis and experiments verify the effectiveness of Bi-TCCS . Compared with existing approaches, the service successful ratio of Bi-TCCS increased by 12 percent under highly dishonest cloud environment. These results also indicate that Bi-TCCS is more adaptable both in the random walk and cheating profiles, which represents a substantial improvement in tracking the dynamic behavior of cloud services.},
  archive      = {J_TCC},
  author       = {Chuanyi Liu and Xiaoyong Li and Mingliang Sun and Yali Gao and Jie Yuan and Shaoming Duan},
  doi          = {10.1109/TCC.2020.2978810},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1021-1037},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Bi-TCCS: Trustworthy cloud collaboration service scheme based on bilateral social feedback},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient IoT management with resilience to unauthorized
access to cloud storage. <em>TCC</em>, <em>10</em>(2), 1008–1020. (<a
href="https://doi.org/10.1109/TCC.2020.2985046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-based Internet of Things (IoT) management services are a promising means of ingesting data from globally dispersed devices. In this setting, it is important to regulate access to data managed by potentially untrusted cloud servers. Attribute-based encryption (ABE) is a highly effective tool for access control. However, applying ABE to IoT environments shows limitations in the following three aspects: First, the demands for storage resources increase in proportion to the complexity of the access control policies. Second, the computation cost of ABE is onerous for resource-limited devices. Lastly, ABE alone is intractable to prevent illegal key-sharing which leads to unauthorized access to data. In this article, we propose an efficient and secure cloud-based IoT data management scheme using ABE. First, we remove the storage-side dependency on the complexity of the access control policies. Second, a substantial part of computationally intensive operations is securely outsourced to the cloud servers. Lastly, unauthorized access to data via illegal key-sharing is strictly forbidden. Our security analysis and experimental results show the security and practicability of the proposed scheme.},
  archive      = {J_TCC},
  author       = {Changhee Hahn and Jongkil Kim and Hyunsoo Kwon and Junbeom Hur},
  doi          = {10.1109/TCC.2020.2985046},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1008-1020},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficient IoT management with resilience to unauthorized access to cloud storage},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed traffic flow consolidation for power efficiency
of large-scale data center network. <em>TCC</em>, <em>10</em>(2),
996–1007. (<a href="https://doi.org/10.1109/TCC.2020.2970403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power optimization for data center networks (DCNs) has recently received increasing research attention, since a DCN can account for 10 to 20 percent of the total power consumption of a data center. An effective power-saving approach for DCNs is traffic consolidation, which consolidates traffic flows onto a small set of links and switches such that unused network devices can be shut down dynamically for power savings. While this approach has shown great promise, existing solutions are mostly centralized and do not scale well for large-scale DCNs. In this article, we propose DISCO, a DIS tributed traffic flow CO nsolidation framework, with correlation analysis and delay constraints, for large-scale data center network. DISCO features two distributed traffic consolidation algorithms that provide different trade-offs (as desired by different DCN architectures) between scalability, power savings, and network performance. First, a flow-based algorithm is proposed to conduct consolidation for each flow individually, with greatly improved scalability. Second, an even more scalable switch-based algorithm is proposed to consolidate flows on each individual switch in a distributed fashion. We evaluate the DISCO algorithms both on a hardware testbed and in large-scale simulations with real DCN traces. The results show that, compared with state-of-the-art centralized solutions, DISCO can achieve nearly the same power savings while decomposing the global problem into sub-problems that are three orders of magnitude smaller. As a result, DISCO can run $\mathbf {10^4}$ to $\mathbf {10^6}$ times faster for a DCN at the scale of 10K servers. The convergence of DISCO has also been proven theoretically and examined experimentally.},
  archive      = {J_TCC},
  author       = {Kuangyu Zheng and Xiaorui Wang and Jia Liu},
  doi          = {10.1109/TCC.2020.2970403},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {996-1007},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Distributed traffic flow consolidation for power efficiency of large-scale data center network},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomic elasticity control for multi-server queues under
generic workload surges in cloud environments. <em>TCC</em>,
<em>10</em>(2), 984–995. (<a
href="https://doi.org/10.1109/TCC.2020.2992949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing environments and Internet datacenters consist of a multitude of servers that process user requests. Performance and scalability can suffer greatly when the workload surges to levels that cause a system to become unstable (i.e., when the arrival rate of requests exceeds the system’s capacity to process them). This article presents a detailed design and evaluation of an autonomic elasticity controller for surges of any shape. This controller uses an analytical model, derived by the authors, of a single-queue multiple-server system (G/G/c) subject to workload surges that cause the system to become unstable during finite time intervals. The controller is evaluated through extensive simulations and by using publicly available Google traces. The controller is further extended to take into account VM startup delays. The article also illustrates how fudge factors can be used to more aggressively react to surges at the expense of additional resources. Finally, our controller is compared with a hypothetical oracle controller that knows the exact shape of the surge when it starts to occur.},
  archive      = {J_TCC},
  author       = {Venkat Tadakamalla and Daniel A. Menascé},
  doi          = {10.1109/TCC.2020.2992949},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {984-995},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Autonomic elasticity control for multi-server queues under generic workload surges in cloud environments},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Infrastructure aware heterogeneous-workloads scheduling for
data center energy cost minimization. <em>TCC</em>, <em>10</em>(2),
972–983. (<a href="https://doi.org/10.1109/TCC.2020.2977040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A huge amount of energy consumption, the cost of this usage and environmental effects have become serious issues for commercial cloud providers. Solar energy is a promising clean energy source, to provide some portion of the Internet data center&#39;s (IDC&#39;s) energy usage which can reduce environmental effects and total energy costs. Moreover, due to the high energy consumption of the cooling system, considering cooling power in job scheduling can provide efficient solutions to reduce total energy consumption. In this article, we investigate the problem of minimizing the energy cost of an IDC and propose an algorithm which schedules heterogeneous IDC workloads, by considering available renewable energy, cooling subsystem, and electricity rate structure. We evaluate the effectiveness and feasibility of our algorithm using real and synthetic workload traces. The simulation results illustrate how our proposed solution reduces the data center&#39;s energy cost by up to 46 percent compared to previous solutions. Moreover, results show that our solution is capable of reducing energy cost of data centers under different weather conditions, and rate structures.},
  archive      = {J_TCC},
  author       = {Kawsar Haghshenas and Somayyeh Taheri and Maziar Goudarzi and Siamak Mohammadi},
  doi          = {10.1109/TCC.2020.2977040},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {972-983},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Infrastructure aware heterogeneous-workloads scheduling for data center energy cost minimization},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Proactive fault-tolerance technique to enhance reliability
of cloud service in cloud federation environment. <em>TCC</em>,
<em>10</em>(2), 957–971. (<a
href="https://doi.org/10.1109/TCC.2020.2968522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud federation is a new computing paradigm that has paved the way for cloud service providers (CSPs) to offer their unused resources (virtual machine) to other CSPs when their resource demands are low. Federation also allows CSPs to outsource their resource requests to other CSPs when their computing resources’ demands are high. Thus, in cloud federation environment reliability and availability of services offered by service providers increase as the CSPs are able to share their resources among themselves. Moreover, to maintain the reliability and availability of cloud services offered through federation, it is important that the computational environment of member CSPs within the federation is fault tolerant. Therefore, there is a need for fault tolerant system to guarantee cloud service reliability and availability in cloud federation environment. In this article, we propose a proactive fault tolerance system that preempts faults within the federation on the basis of CPU temperature. The fault tolerance system within the federation is modeled as a multi-objective optimization problem of maximizing profit and minimizing migration cost while redistributing resources (virtual machine) from faulty CSPs to non-faulty CSPs within the federation. To address this issue, we have also proposed an algorithm called Preference Based Fault Management (PBFM) to manage the federation in the event of faults. We perform extensive experiments to evaluate the effectiveness of our proposed mechanism and compare it with two other mechanisms MCAFM (Migration Cost Assured Fault Management) and PAFM (Profit Assured Fault Management). Results show that our proposed mechanism PBFM yields an optimized solution to the general problem of profit and migration cost trade-off in presence of faulty CSPs.},
  archive      = {J_TCC},
  author       = {Benay Kumar Ray and Avirup Saha and Sunirmal Khatua and Sarbani Roy},
  doi          = {10.1109/TCC.2020.2968522},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {957-971},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Proactive fault-tolerance technique to enhance reliability of cloud service in cloud federation environment},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An in-network replica selection framework for
latency-critical distributed data stores. <em>TCC</em>, <em>10</em>(2),
944–956. (<a href="https://doi.org/10.1109/TCC.2020.2976008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In distributed data stores, performance fluctuations generally occur across servers, especially when the servers are deployed in a cloud environment. Hence, the replica selected for a reading request will directly affect the response latency. However, replica selection is challenging in latency-critical data stores (e.g., key-value stores). Such data stores generally deal with small size data, and clients have to select replicas independently. Even the state-of-the-art algorithm of replica selection (C3) still has considerable room for improving the latency. According to our experiments, compared with C3, using the ideal replica selection (Oracle) reduces the 99th latency by about 34-60 percent. In this article, we propose NetRS to address the fundamental factors that prevent replica selection algorithms from being effective. NetRS is a framework that enables in-network replica selection for distributed data stores. It exploits emerging network devices, including programmable switches and network accelerators, to select replicas for requests. NetRS supports diverse algorithms of replica selection and is suited to the network topology of modern data centers. According to our extensive evaluations, compared with the conventional scheme of clients selecting replicas for requests, NetRS reduces the mean latency by up to 50.3 percent, and the 99th latency by up to 56.3 percent. Moreover, NetRS could effectively cut the response latency even when unexpected events (e.g., workload changes, network device failures), and network congestion occur.},
  archive      = {J_TCC},
  author       = {Yi Su and Dan Feng and Yu Hua and Zhan Shi and Tingwei Zhu},
  doi          = {10.1109/TCC.2020.2976008},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {944-956},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {An in-network replica selection framework for latency-critical distributed data stores},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Personality-guided cloud pricing via reinforcement learning.
<em>TCC</em>, <em>10</em>(2), 925–943. (<a
href="https://doi.org/10.1109/TCC.2020.2992461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an efficient commercial computing paradigm, cloud computing provides various computing and storage resources to users in a pay-as-you-go manner. However, existing cloud pricing models and mechanisms are deterministic to some degree, thus, may not work well in a real-world environment where user perceived values with respect to cloud services are dynamically changing and highly personalized. In this article, we develop a reinforcement learning (RL)-based dynamic cloud pricing scheme to optimize both cloud provider’s profit and costs of heterogeneous users with distinct personalities. Specifically, we first propose a novel personality-guided user perceived value prediction scheme to proactively capture the dynamics of the users’ perceived values with respect to cloud services. The prediction scheme models the relationship among user personality, service price, quality of service (QoS), user satisfaction and perceived value in the cloud service market. Second, on the basis of the prediction model, a RL-based cloud pricing mechanism is developed to learn sequential service pricing decision-making for profit and costs optimization. Particularly, the profit and costs optimization problem is modeled as a discrete-time Markov decision process (MDP) that is solved by using Q-learning. Finally, extensive simulation experiments have been conducted to verify our user perceived value prediction scheme and RL-based cloud service pricing mechanism. Simulation results show that our perceived value prediction scheme can achieve up to 87.50 percent prediction accuracy, and our RL-based pricing mechanism can obtain up to 19.39 percent more profit than the state-of-the-art scheme.},
  archive      = {J_TCC},
  author       = {Peijin Cong and Junlong Zhou and Mingsong Chen and Tongquan Wei},
  doi          = {10.1109/TCC.2020.2992461},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {925-943},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Personality-guided cloud pricing via reinforcement learning},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tight bound on incnetive ratio for sybil attack in resource
sharing system. <em>TCC</em>, <em>10</em>(2), 913–924. (<a
href="https://doi.org/10.1109/TCC.2020.2984760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we discuss the Sybil attack on a sharing-based economic system where each participant contributes its own resource for all to share. Such an attack is possible especially in the cloud computing model where agents can exchange information with the cloud and obtain aggregated information from it. We are interested in the robustness of the market equilibrium mechanism against such an attack. We adopt the incentive ratio to measure the gain that a participant can make by splitting its identity and reconstructing communication connections with others. On one hand, we show that no player can increase more than $\sqrt{2}$ times its original share in the market equilibrium solution by characterizing the worst case, in which a strategic agent can obtain the maximal gain in utility by playing the Sybil attack. On the other hand, the bound of $\sqrt{2}$ is proved to be tight by constructing a proper instance. We also simulate on a series of random graphs and observe that the incentive ratio was no more than two in the general setting.},
  archive      = {J_TCC},
  author       = {Zhou Chen and Yukun Cheng and Xiaotie Deng and Qi Qi and Xiang Yan},
  doi          = {10.1109/TCC.2020.2984760},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {913-924},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Tight bound on incnetive ratio for sybil attack in resource sharing system},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-tier workload consolidations in the cloud: Profiling,
modeling and optimization. <em>TCC</em>, <em>10</em>(2), 899–912. (<a
href="https://doi.org/10.1109/TCC.2020.2975788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reducing tail latency becomes increasingly important to improve the user-perceived service experience. User-facing latency-sensitive cloud applications typically contain multiple interactive tiers (e.g., Web, App, Database) running in different virtual machines (VMs) with complex interaction patterns. However, such interactions between VMs in different tiers are often neglected in previous VM consolidation methods, resulting in poor application performance. In this article, we study the consolidation of multi-tier interactive workloads from a new perspective of user-perceived tail latency. We propose a novel profiling-based consolidation methodology to satisfy tail latency requirements while reducing the number of used physical machines. To achieve such a goal, we first perform large-scale profiling experiments under various consolidation settings in a KVM virtualized private cluster to establish the empirical performance values. We consider two key factors that affect the tail latency of multi-tier workloads: interference with co-located VMs and interaction between tiers. We model the consolidation of multi-tier workloads as an optimization problem with different objectives and constraints, and derive the consolidation schedule. We implement and evaluate the proposed models, as well as comparing with other methods (i.e., without profiling or without considering interaction influence). Extensive experimental results show that the proposed method is able to reduce up to 5X tail latency, compared with the method without profiling and up to 1.3X tail latency, compared with the method without considering the interaction influence between different tiers.},
  archive      = {J_TCC},
  author       = {Kejiang Ye and Haiying Shen and Yang Wang and Cheng-Zhong Xu},
  doi          = {10.1109/TCC.2020.2975788},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {899-912},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Multi-tier workload consolidations in the cloud: Profiling, modeling and optimization},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stream processing on clustered edge devices. <em>TCC</em>,
<em>10</em>(2), 885–898. (<a
href="https://doi.org/10.1109/TCC.2020.2983402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things continuously generates avalanches of raw sensor data to be transferred to the Cloud for processing and storage. Due to network latency and limited bandwidth, this vertical offloading model, however, fails to meet requirements of time-critical data-intensive applications which must act upon generated data with minimum time delays. To address such a limitation, this article proposes a novel distributed architecture enabling stream data processing at the edge of the network, broadening the principle of enabling processing closer to data sources adopted by Fog and Edge Computing. Specifically, this architecture extends the Apache NiFi stream processing middleware with support for run-time clustering of heterogeneous edge devices, such that computational tasks can be horizontally offloaded to peer devices and executed in parallel. As opposed to vertical offloading on the Cloud, the proposed solution does not suffer from increased network latency and is thus able to offer 5-25 times faster response time, as demonstrated by the experiments on a run-time license plate recognition system.},
  archive      = {J_TCC},
  author       = {Rustem Dautov and Salvatore Distefano},
  doi          = {10.1109/TCC.2020.2983402},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {885-898},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Stream processing on clustered edge devices},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Praxi: Cloud software discovery that learns from practice.
<em>TCC</em>, <em>10</em>(2), 872–884. (<a
href="https://doi.org/10.1109/TCC.2020.2975439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With today’s rapidly-evolving cloud landscape embracing continuous integration and delivery, users of cloud systems must monitor software running on their containers and virtual machines (VMs) to ensure compliance, security, and efficiency. Traditional solutions to this problem rely on manually-created rules that identify software installations and modifications, but these require expert authors and are often unmaintainable. Recently, automated techniques for software discovery have emerged. Some techniques use examples of software to train machine learning models to predict which software has been installed on a system. Others leverage the knowledge of packaging practices to aid in discovery without requiring any pre-training, but these practice-based methods cannot provide precise-enough information to perform discovery by themselves. This article introduces Praxi, a new software discovery method that builds upon the strengths of prior approaches by combining the accuracy of learning-based methods with the efficiency of practice-based methods. In tests using samples collected on real-world cloud systems, Praxi correctly classifies installations at least 97.6 percent of the time, while running 14.8 times faster and using 87 percent less disk space than a similar learning-based method. Using a diverse software dataset, this article quantitatively compares Praxi to systematic rule-, learning-, and practice-based methods, and discusses the best uses for each.},
  archive      = {J_TCC},
  author       = {Anthony Byrne and Emre Ates and Ata Turk and Vladimir Pchelin and Sastry Duri and Shripad Nadgowda and Canturk Isci and Ayse K. Coskun},
  doi          = {10.1109/TCC.2020.2975439},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {872-884},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Praxi: Cloud software discovery that learns from practice},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MCDM-based parametric selection of cloud deployment models
for an academic organization. <em>TCC</em>, <em>10</em>(2), 863–871. (<a
href="https://doi.org/10.1109/TCC.2020.2980534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present scenario, academic organizations are looking toward the adoption of the cloud computing services to make improvements in teaching, learning, and research. Various cloud deployment models have been developed in past for effective implementation of the cloud computing. Due to such diversity of deployment models, the selection of these models becomes a very difficult task for the academic organizations. Further, this selection decision is dependent on various conflicting decision parameters. Therefore, the problem of cloud deployment model selection can be molded as a multi-criteria decision making (MCDM) problem. In this research article, a decision-making framework using Fuzzy-euclidean-Taxicab distance-based approach (Fuzzy-ETDBA) is developed and presented to solve the cloud deployment model selection problem. To demonstrate the presented framework, a case study is represented that involves the evaluation and selection of four cloud deployment models over three decision parameters consisting of seventeen sub-parameters. The selection results obtained depict that the public cloud deployment model is most preferable for the presented case study.},
  archive      = {J_TCC},
  author       = {Rakesh Garg},
  doi          = {10.1109/TCC.2020.2980534},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {863-871},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {MCDM-based parametric selection of cloud deployment models for an academic organization},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A queue waiting cost-aware control model for large scale
heterogeneous cloud datacenter. <em>TCC</em>, <em>10</em>(2), 849–862.
(<a href="https://doi.org/10.1109/TCC.2020.2990982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal load distribution and speed scaling in a heterogeneous data center that takes account of the tradeoff between the maintenance and operation costs and system performance are crucial issues of cloud computing. Due to the changing states of cloud centers and diversity of the task arrival rate, a static control model is infeasible for cloud computing. In this article, we aim to provide a novel multi-server control model with dynamic feedback to acquire dynamic states of the cloud system, and with queue waiting cost-awareness to optimize the queue wait time and load distribution in task assignment and server configuration management. Using the technology of speed scaling, each server in a data center is configured as an $M/M/1$ queue system with variable service rate, and the service rate is a function of the length of the task queue. We formulate two optimization problems, the optimal load distribution problem and the optimal service rate controlling problem, and provide algorithms to solve these problems, facilitating load distributions and service rate adjustments. We also present numerical simulations to validate our model. The results show our model to be efficient in multi-server dynamic configurations and task assignments according to the feedback information for the tradeoff between the system cost and performance.},
  archive      = {J_TCC},
  author       = {Weihua Bai and Jiaxian Zhu and Shaowei Huang and Huibing Zhang},
  doi          = {10.1109/TCC.2020.2990982},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {849-862},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A queue waiting cost-aware control model for large scale heterogeneous cloud datacenter},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Verifiable searchable encryption framework against insider
keyword-guessing attack in cloud storage. <em>TCC</em>, <em>10</em>(2),
835–848. (<a href="https://doi.org/10.1109/TCC.2020.2989296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searchable encryption (SE) allows cloud tenants to retrieve encrypted data while preserving data confidentiality securely. Many SE solutions have been designed to improve efficiency and security, but most of them are still susceptible to insider Keyword-Guessing Attacks (KGA), which implies that the internal attackers can guess the candidate keywords successfully in an off-line manner. Also in existing SE solutions, a semi-honest-but-curious cloud server may deliver incorrect search results by performing only a fraction of retrieval operations honestly (e.g., to save storage space). To address these two challenging issues, we first construct the basic Verifiable SE Framework (VSEF), which can withstand the inside KGA and achieve verifiable searchability. Based on the basic VSEF, we then present the enhanced VSEF to support multi-keyword search, multi-key encryption and dynamic updates (e.g., data modification, data insertion, and data deletion) at the same time, which highlights the importance of practicability and scalability of SE in real-world application scenarios. We conduct extensive experiments using the Enron email dataset to demonstrate that the enhanced VSEF achieves high efficiency while resisting to the inside KGA and supporting the verifiability of search results.},
  archive      = {J_TCC},
  author       = {Yinbin Miao and Qiuyun Tong and Robert H. Deng and Kim-Kwang Raymond Choo and Ximeng Liu and Hongwei Li},
  doi          = {10.1109/TCC.2020.2989296},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {835-848},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Verifiable searchable encryption framework against insider keyword-guessing attack in cloud storage},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed and collaborative high-speed inference deep
learning for mobile edge with topological dependencies. <em>TCC</em>,
<em>10</em>(2), 821–834. (<a
href="https://doi.org/10.1109/TCC.2020.2978846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ubiquitous computing has potentials to harness the flexibility of distributed computing systems including cloud, edge, and Internet of Things devices. Mobile edge computing (MEC) benefits time-critical applications by providing low latency connections. However, most of the resource-constrained edge devices are not computationally feasible to host deep learning (DL) solutions. Further, these edge devices if deployed under denser deployments result in topological dependencies which if not taken into consideration adversely affect the MEC performance. To bring more intelligence to the edge under topological dependencies, compared to optimization heuristics, this article proposes a novel collaborative distributed DL approach. The proposed approach exploits topological dependencies of the edge using a resource-optimized graph neural network (GNN) version with an accelerated inference. By exploiting edge collaborative learning using stochastic gradient (SGD), the proposed approach called CGNN-edge ensures fast convergence and high accuracy. Collaborative learning of the deployed CGNN-edge incurs extra communication overhead and latency. To cope, this article proposes compressed collaborative learning based on momentum correction called cCGNN-edge with better scalability while preserving accuracy. Performance evaluation under IEEE 802.11ax-high-density wireless local area networks deployment demonstrates that both the schemes outperform cloud-based GNN inference in response time, satisfaction of latency requirements, and communication overhead.},
  archive      = {J_TCC},
  author       = {Shagufta Henna and Alan Davy},
  doi          = {10.1109/TCC.2020.2978846},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {821-834},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Distributed and collaborative high-speed inference deep learning for mobile edge with topological dependencies},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asynchronous optical traffic offloading of hybrid
optical/electrical data center networks. <em>TCC</em>, <em>10</em>(2),
805–820. (<a href="https://doi.org/10.1109/TCC.2020.2992489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, hybrid optical/electrical data center network has been considered a promising interconnection technology for large-scale data centers, since it can efficiently provide sufficient bandwidth. A key issue in hybrid optical/electrical data center networks is finding a way to offload burst traffic through optical circuit switches (OCSs), such that the burst traffic can be offloaded timely while the system operation overhead is low. This article extends the idea of Mahout to propose a local-push asynchronous optical traffic offloading strategy, in which each ToR switch offloads the traffic via the OCSs if its queue length is larger than a preset threshold, and transfers the traffic back to electrical packet switches (EPSs) when the backlog is empty. To seek a proper threshold, we develop a fluid-flow model to analyze the performance of the proposed traffic offloading strategy, from which we demonstrate that there is a trade-off between the mean delay of the traffic and the system operation overhead in a typical data center network. Based on such a trade-off, we provide a rule to select the buffer threshold. We show via simulation that the proposed optical traffic offloading strategy with the threshold selection rule outperforms C-through.},
  archive      = {J_TCC},
  author       = {Tong Ye and Jianke Li and Xiaodan Pan and Tony T. Lee},
  doi          = {10.1109/TCC.2020.2992489},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {805-820},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Asynchronous optical traffic offloading of hybrid Optical/Electrical data center networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RES: Real-time video stream analytics using edge enhanced
clouds. <em>TCC</em>, <em>10</em>(2), 792–804. (<a
href="https://doi.org/10.1109/TCC.2020.2991748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing availability and use of Internet of Things (IoT) devices such as sensors and video cameras, large amounts of streaming data is now being produced at high velocity. Applications which require low latency response such as video surveillance, augmented reality and autonomous vehicles demand a swift and efficient analysis of this data. Existing approaches employ cloud infrastructure to store and perform machine learning-based analytics on this data. This centralized approach has limited ability to support real-time analysis of large-scale streaming data due to network bandwidth and latency constraints between data source and cloud. We propose RealEdgeStream (RES) an edge enhanced stream analytics system for large-scale, high performance data analytics. The proposed approach investigates the problem of video stream analytics by proposing (i) filtration and (ii) identification phases. The filtration phase reduces the amount of data by filtering low-value stream objects using configurable rules. The identification phase uses deep learning inference to perform analytics on the streams of interest. The phases consist of stages which are mapped onto available in-transit and cloud resources using a placement algorithm to satisfy the Quality of Service (QoS) constraints identified by a user. We demonstrate that for a 10K element data streams, with a frame rate of 15–100 per second, the job completion in the proposed system takes 49 percent less time and saves 99 percent bandwidth compared to a centralized cloud-only based approach.},
  archive      = {J_TCC},
  author       = {Muhammad Ali and Ashiq Anjum and Omer Rana and Ali Reza Zamani and Daniel Balouek-Thomert and Manish Parashar},
  doi          = {10.1109/TCC.2020.2991748},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {792-804},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {RES: Real-time video stream analytics using edge enhanced clouds},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Critical path analysis through hierarchical distributed
virtualized environments using host kernel tracing. <em>TCC</em>,
<em>10</em>(2), 774–791. (<a
href="https://doi.org/10.1109/TCC.2019.2953258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamic nature of applications in Virtual Machines (VMs) and the increasing demand for virtualized systems make the analysis of dynamic environments critical to achieve efficient operation of such complex distributed systems. In this article, we propose a precise host-based tracing and analysis method to retrieve execution flows, and dependency flows from virtualized environments, regardless of the level of nested virtualization. Given a host operating system level trace, the Any-Level vCPU Detection (ASD) algorithm and Guest Thread-state Analysis (GTA) algorithm detect the different states of vCPUs and threads for arbitrary nesting depths. Then, the Execution-graph Construction (HEC) algorithm extracts the waiting / wake-up dependencies chains out of the running processes across VMs, for any level of virtualization in a transparent manner. The process dependency graph, vCPU state, and VM process state are displayed in an interactive trace viewer, Trace Compass, for further inspection. Our proposed VM trace analysis algorithms have been open-sourced for further enhancements and collaborative research and development. Our new techniques were evaluated with workloads generated using several well-known server applications (e.g., Hadoop, Apache, MySQL, Linux apt-get, and IMS network). The proposed approaches are based on host hypervisor tracing, which brings a lower tracing overhead (around 1 percent), is easier to deploy, and presents fewer security issues as compared to other approaches.},
  archive      = {J_TCC},
  author       = {Hani Nemati and Francois Tetreault and Jason Puncher and Michel Dagenais},
  doi          = {10.1109/TCC.2019.2953258},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {774-791},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Critical path analysis through hierarchical distributed virtualized environments using host kernel tracing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attribute based encryption with privacy protection and
accountability for CloudIoT. <em>TCC</em>, <em>10</em>(2), 762–773. (<a
href="https://doi.org/10.1109/TCC.2020.2975184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pervasive, ubiquitous, and heterogeneous properties of IoT make securing IoT systems a very challenging task. More so when access and storage are performed through a cloud-based IoT system. IoT data stored on cloud should be encrypted to ensure data privacy. It is also crucial to allow only authorized entities to access and decrypt the encrypted data. In this article, we propose a ciphertext-policy attribute-based encryption (CP-ABE) scheme that enables fine-grained access control of encrypted IoT data on cloud. CP-ABE is regarded as a highly promising approach to provide flexible and fine-grained access control, which is quite suited to secure cloud based IoT systems. We first present an access control system model of CloudIoT platform based on ABE. Based on the presented system model, we construct a ciphertext-policy hiding CP-ABE scheme, which guarantees the privacy of the users. We further construct a white-box traceable CP-ABE scheme with accountability in order to address the user key abuse and authorization center key abuse. Experiment illustrates the proposed systems are efficient.},
  archive      = {J_TCC},
  author       = {Jiguo Li and Yichen Zhang and Jianting Ning and Xinyi Huang and Geong Sen Poh and Debang Wang},
  doi          = {10.1109/TCC.2020.2975184},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {762-773},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Attribute based encryption with privacy protection and accountability for CloudIoT},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Designing secure and efficient biometric-based access
mechanism for cloud services. <em>TCC</em>, <em>10</em>(2), 749–761. (<a
href="https://doi.org/10.1109/TCC.2020.2987564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for remote data storage and computation services is increasing exponentially in our data-driven society; thus, the need for secure access to such data and services. In this article, we design a new biometric-based authentication protocol to provide secure access to a remote (cloud) server. In the proposed approach, we consider biometric data of a user as a secret credential. We then derive a unique identity from the user&#39;s biometric data, which is further used to generate the user&#39;s private key. In addition, we propose an efficient approach to generate a session key between two communicating parties using two biometric templates for a secure message transmission. In other words, there is no need to store the user&#39;s private key anywhere and the session key is generated without sharing any prior information. A detailed Real-Or-Random (ROR) model based formal security analysis, informal (non-mathematical) security analysis and also formal security verification using the broadly-accepted Automated Validation of Internet Security Protocols and Applications (AVISPA) tool reveal that the proposed approach can resist several known attacks against (passive/active) adversary. Finally, extensive experiments and a comparative study demonstrate the efficiency and utility of the proposed approach.},
  archive      = {J_TCC},
  author       = {Gaurang Panchal and Debasis Samanta and Ashok Kumar Das and Neeraj Kumar and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TCC.2020.2987564},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {749-761},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Designing secure and efficient biometric-based access mechanism for cloud services},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). User-centric interference-aware load balancing for
cloud-deployed applications. <em>TCC</em>, <em>10</em>(1), 736–748. (<a
href="https://doi.org/10.1109/TCC.2019.2943560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {VMs deployed in cloud environments are prone to performance interference due to dynamic and unpredictable contention for shared physical resources among colocated tenants. Current provider-centric solutions, such as careful co-scheduling of VMs and/or VM migration, require a priori profiling of customer VMs, which is infeasible in public clouds. Further, such solutions are not always aware of the user&#39;s SLO requirements or application bottlenecks. This paper presents DIAL, an interference-aware load balancing framework that can directly be employed by cloud users without requiring any assistance from the provider. The key idea behind DIAL is to infer the demand for contended resources on the physical hosts, which is otherwise hidden from users. Estimates of the colocated load are then used to dynamically shift load away from compromised VMs without violating the application&#39;s tail latency SLOs. We implement DIAL for web and online analytical processing applications, and show, via experimental results on OpenStack and AWS clouds, that DIAL can reduce tail latencies by as much as 70 percent compared to existing solutions.},
  archive      = {J_TCC},
  author       = {Seyyed Ahmad Javadi and Anshul Gandhi},
  doi          = {10.1109/TCC.2019.2943560},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {736-748},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {User-centric interference-aware load balancing for cloud-deployed applications},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-tier sharing in electric vehicle service market.
<em>TCC</em>, <em>10</em>(1), 724–735. (<a
href="https://doi.org/10.1109/TCC.2019.2941484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transportation sharing in goods (bike sharing), distinguished from service sharing (ride), has been one of the most active sectors of the sharing economy recently. Such a business model, facilitated by the mobile Internet and cloud computing platforms, seeks supplies in vehicles on demands at matched times and rental locations. The success has attracted more competitors into it, counter-effectively resulted in redundancies in total supplies, reducing social efficiency. In this work, we take electric car sharing as an example to propose a business solution to deal with such a dilemma. Our main idea is to set up a joint venture to provide shared electric cars for different competitors to operate on. These competitors provide their differentiated service for their customers through their own electric mobile Apps, while reducing the infrastructure cost through the joint venture. We study this business model as a two-stage Stackelberg game to analyze the optimal pricing and the sharing scheme of the leader (joint venture) and its followers (car sharing operators). Our model places the sharing of the electric vehicles in two tiers: One among the customers (to reduce the cost of time sharing) and the other among the operators (to reduce the cost of space sharing).},
  archive      = {J_TCC},
  author       = {Yukun Cheng and Xiaotie Deng and Mengqian Zhang},
  doi          = {10.1109/TCC.2019.2941484},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {724-735},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Two-tier sharing in electric vehicle service market},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards dependency-aware cache management for data analytics
applications. <em>TCC</em>, <em>10</em>(1), 706–723. (<a
href="https://doi.org/10.1109/TCC.2019.2945015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory caches are being used aggressively in today&#39;s data analytics systems such as Spark, Tez, and Piccolo. The significant performance impact of caches and their limited sizes call for efficient cache management in data analytics clusters. However, prevalent data analytics systems employ rather simple cache management policies—notably Least Recently Used (LRU) and Least Frequently Used (LFU)—that are oblivious to the application semantics of data dependency, expressed as directed acyclic graphs (DAGs). Without this knowledge, cache management can, at best, be performed by “guessing” the future data access patterns based on history, which frequently results in inefficient, erroneous caching with a low hit rate and a long response time. Worse still, the lack of data dependency knowledge makes it impossible to retain the all-or-nothing cache property of cluster applications, in that a compute task cannot be sped up unless all the dependent data has been kept in the main memory. In this paper, we propose a novel cache replacement policy, named Least Reference Count (LRC), which exploits the application&#39;s data dependency information to optimize the cache management. LRC keeps track of the reference count of each data block, defined as the number of dependent child blocks that have not been computed yet, and always evicts the block with the smallest reference count. Furthermore, we incorporate the all-or-nothing requirement into LRC by coordinately managing the reference counts of all the input data blocks for the same computation. We demonstrate the efficacy of LRC through both empirical analysis and cluster deployments against popular benchmarking workloads. Our Spark implementation shows that, the proposed policies well address the all-or-nothing requirement and significantly improve the cache performance. Compared with LRU and a recently proposed caching policy called MEMTUNE, LRC improves the caching performance of typical workloads in production clusters by 22 and 284 percent, respectively.},
  archive      = {J_TCC},
  author       = {Yinghao Yu and Chengliang Zhang and Wei Wang and Jun Zhang and Khaled Ben Letaief},
  doi          = {10.1109/TCC.2019.2945015},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {706-723},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Towards dependency-aware cache management for data analytics applications},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SecEDMO: Enabling efficient data mining with strong privacy
protection in cloud computing. <em>TCC</em>, <em>10</em>(1), 691–705.
(<a href="https://doi.org/10.1109/TCC.2019.2932065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequent itemsets mining and association rules mining are among the top used algorithms in the area of data mining. Secure outsourcing of data mining tasks to the third-party cloud is an effective option for data owners. However, due to the untrust cloud and the distrust between data owners, the traditional algorithms which only work over plaintext should be re-considered to take security and privacy concerns into account. For example, each data owner may not be willing to disclose their own private data to others during the cooperative data mining process. The previous solutions are either not sufficiently secure or not efficient. Therefore, we propose a Sec ure and E fficient D ata M ining O utsourcing (SecEDMO) scheme for secure outsourcing of frequent itemsets mining and association rules mining over the joint database (i.e., database aggregated from multiple data owners) in the paradigm of cloud computing. Based on our customized lightweight symmetric homomorphic encryption algorithm and a secure comparison algorithm, SecEDMO can ensure strong privacy protection and low data mining latency simultaneously. Moreover, the well-designed virtual transaction insertion algorithm can hide the information of the original database while still preserving the cloud’s ability to perform data mining over the obfuscated data. By evaluation of a numerical experiment and theoretical comparisons, the correctness, security, and efficiency of SecEDMO are confirmed.},
  archive      = {J_TCC},
  author       = {Jiahui Wu and Nankun Mu and Xinyu Lei and Junqing Le and Di Zhang and Xiaofeng Liao},
  doi          = {10.1109/TCC.2019.2932065},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {691-705},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {SecEDMO: Enabling efficient data mining with strong privacy protection in cloud computing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SDN-based traffic matrix estimation in data center networks
through large size flow identification. <em>TCC</em>, <em>10</em>(1),
675–690. (<a href="https://doi.org/10.1109/TCC.2019.2944823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defined networking (SDN) with separated control plane and data plane brings new opportunities for traffic measurement in data center networks. However, in the SDN-enabled switches, available TCAM (Ternary Content Addressable Memory) resources for traffic measurement are limited. Thus, it is necessary to utilize traffic matrix (TM) estimation to derive a hybrid network monitoring scheme through combining the partial direct measurement offered by SDN with some inference techniques. Although large size flows play an important role in improving TM estimation accuracy, directly monitoring each flow and finding out large size flows consume massive channel bandwidth resource between control plane and data plane. Therefore, in this paper, we identify large size flows from multiple historical TMs instead of monitoring each flow. First, we analyze multiple historical TMs and observe that origin-to-destination (OD) pair whose flow size is selected as large size flow at last time slot is most likely to be selected for per-flow monitoring at next time slot, so these OD pairs are identified by gradient boosting machine and are directly regarded as sampled OD pairs in order to reduce resource consumption. Then, we propose a greedy heuristic algorithm to solve SDN-enabled switch selection problem to best utilize the TCAM resources and guarantee that most of sampled OD pairs are measured in the flow table. We also present a source node prefix tree based bit merging aggregation (SPTBMA) scheme to design feasible forwarding rules to be inserted in TCAM of SDN-enabled switches and reserve more TCAM space for sampled OD pairs. Finally, the experimental results based on real traffic dataset demonstrate that our proposed scheme outperforms the existing algorithms in terms of improving TM estimation accuracy and overcoming limitation of TCAM resources.},
  archive      = {J_TCC},
  author       = {Guiyan Liu and Songtao Guo and Bin Xiao and Yuanyuan Yang},
  doi          = {10.1109/TCC.2019.2944823},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {675-690},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {SDN-based traffic matrix estimation in data center networks through large size flow identification},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scheduling real-time parallel applications in cloud to
minimize energy consumption. <em>TCC</em>, <em>10</em>(1), 662–674. (<a
href="https://doi.org/10.1109/TCC.2019.2956498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing has become an important paradigm in which scalable resources such as CPU, memory, disk and IO devices can be provided to users to remotely process their applications. In a cloud computing platform, energy consumption accounts for a significant cost portion. This article thus aims to present an energy-efficient scheduling algorithm for processing a user application with a real-time requirement. This problem is formulated as a non-linear mixed integer programming problem. We start with providing an optimal closed-form solution to its relaxation problem that aims to minimize the energy consumption without considering real-time requirements. To meet real-time requirements, we propose how to adjust task placement and resource allocation by making a good tradeoff between energy consumption and task execution time. Lastly, we find two equivalent optimal resource allocation strategies once task placement has been done. We then propose to adjust the start time of task execution such that an application’s completion time can be further shortened. Experimental results on two real-case enchmarks and extensive synthetic applications demonstrate that our proposed method finds a schedule that generally has 30 and 20 percent less energy consumption than enhancement heterogeneous earliest finish time (E-HEFT) and genetic algorithm, respectively. Besides, the proposed method has a higher rate to successfully find a feasible schedule than them, and its computation time is close to E-HEFT’s, but far less than the genetic algorithm&#39;s.},
  archive      = {J_TCC},
  author       = {Biao Hu and Zhengcai Cao and Mengchu Zhou},
  doi          = {10.1109/TCC.2019.2956498},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {662-674},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Scheduling real-time parallel applications in cloud to minimize energy consumption},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Resource price-aware offloading for edge-cloud
collaboration: A two-timescale online control approach. <em>TCC</em>,
<em>10</em>(1), 648–661. (<a
href="https://doi.org/10.1109/TCC.2019.2937928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computation offloading is envisioned as a promising technique for prolonging the battery lives and enhancing the computation capability of mobile devices. In this paper, we study the task offloading and resource purchasing problems in an edge-cloud collaborative system. The purpose of this system is to minimize the cost of task offloading while ensuring that the tasks can be served before their maximum acceptable delays. Due to the uncertainty of both the task arrival rates and the prices of the computing resources, it is impossible to make an optimal decision online for a long-running time. Therefore, we propose a two-timescale Lyapunov optimization algorithm to overcome the uncertainty of the system’s future information and make the optimal decisions only based on the system’s current states. By purchasing computation resources in different timescales from the public cloud and making online decisions on where and how many requests should be offloaded, we can achieve an efficient outcome such that the system performance can approach the offline optimum without requiring a priori knowledge of system statistics. Rigorous theoretical analysis confirms the effectiveness of the proposed two-timescale Lyapunov optimization algorithm and extensive trace-driven experimental results show that the algorithm achieves outstanding performance gains over existing benchmarks.},
  archive      = {J_TCC},
  author       = {Rui Li and Zhi Zhou and Xu Chen and Qing Ling},
  doi          = {10.1109/TCC.2019.2937928},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {648-661},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Resource price-aware offloading for edge-cloud collaboration: A two-timescale online control approach},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Redundancy minimization and cost reduction for workflows
with reliability requirements in cloud-based services. <em>TCC</em>,
<em>10</em>(1), 633–647. (<a
href="https://doi.org/10.1109/TCC.2019.2937933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliability requirement assurance is an important quality of service (QoS) for workflow execution in cloud-based services. For a workflow with a reliability requirement, the enough replication for redundancy minimization (ERRM) and quantitative fault-tolerance with minimum execution cost + (QFEC+) algorithms are state-of-the-art algorithms to reduce the redundancy and cost, respectively. In this work, we define the reliability increment ratio (RIR) and propose the redundancy minimization using RIR (R_RIR) algorithm. In addition, we introduce the geometric mean and propose the cost reduction using geometric mean (C_GM) algorithm based on redundancy minimization. Experimental results show the proposed R_RIR and C_GM algorithms are superior to state-of-the-art algorithms: (1) although both R_RIR and ERRM show the same redundancy results, R_RIR is proven to generate minimal redundancy, whereas ERRM cannot; (2) R_RIR only consumes a few seconds to achieve minimal redundancy for large-scale workflows, and it has much higher time efficiency than ERRM; and (3) C_GM generates less cost than QFEC+ in a large part of cases.},
  archive      = {J_TCC},
  author       = {Guoqi Xie and Yehua Wei and Yi Le and Renfa Li},
  doi          = {10.1109/TCC.2019.2937933},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {633-647},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Redundancy minimization and cost reduction for workflows with reliability requirements in cloud-based services},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantifying the influence of intermittent connectivity on
mobile edge computing. <em>TCC</em>, <em>10</em>(1), 619–632. (<a
href="https://doi.org/10.1109/TCC.2019.2926702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) is a key technology that enables the deployment of applications (or services) at the proximity of mobile users. However, the performance of mobile edge computing is sensitive to the quality and availability of underlying connection links. It is still unclear to what extent intermittent connectivity affects the performance of mobile edge computing. In this paper, we make the first attempt to quantify the influence of intermittent connectivity on mobile edge computing from a theoretical perspective. Specifically, we propose an analytical framework based on discrete-time Markov chain and derive a closed-form expression of the task processing time under different network conditions. Our model can be further extended to account for the case with group task arrivals. We also conduct extensive simulations to examine the accuracy of our proposed analytical models with both synthetic and real-world user mobility traces. The results show that our model can well capture the influence of intermittent connectivity on MEC. Our model sheds important insights into the impact of intermittent connectivity on task processing in MEC, which we believe should be taken into account when designing future MEC systems.},
  archive      = {J_TCC},
  author       = {Miao Hu and Di Wu and Weigang Wu and Julian Cheng and Min Chen},
  doi          = {10.1109/TCC.2019.2926702},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {619-632},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Quantifying the influence of intermittent connectivity on mobile edge computing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Probability-based online algorithm for switch operation of
energy efficient data center. <em>TCC</em>, <em>10</em>(1), 608–618. (<a
href="https://doi.org/10.1109/TCC.2019.2931961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The huge amount of energy consumed by the data centers around the world every year motivates the cloud service providers to operate data centers in a more energy efficient way. A promising solution is to turn off the idle servers, which, however, may be turned on later, incurring a significant startup cost. The problem turns to dynamically provisioning the workload, and cutting down the energy cost which includes the power to support the running of data center and the startup cost. Different from previous studies that usually consider the worst-case performance guarantee when designing online algorithms, this paper considers the average case which is more practical. We propose a simple online algorithm based on the expectation of job interval of workload, which is proven to be optimal for exponential and uniform distributions and achieves tight competitive ratio $\frac{e}{e-1}$ and $\frac{4}{3}$ , respectively, for them. Simulations using the synthetic data verify our theoretical analysis. Numerical results employing Google’s data center workload trace demonstrate that the proposed algorithm outperforms the worst case-based algorithm in terms of operation cost reduction.},
  archive      = {J_TCC},
  author       = {Jun Sun and Qinmin Yang and Zaiyue Yang},
  doi          = {10.1109/TCC.2019.2931961},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {608-618},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Probability-based online algorithm for switch operation of energy efficient data center},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predictive auto-scaling of multi-tier applications using
performance varying cloud resources. <em>TCC</em>, <em>10</em>(1),
595–607. (<a href="https://doi.org/10.1109/TCC.2019.2944364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of the same type of cloud resources, such as virtual machines (VMs), varies over time mainly due to hardware heterogeneity, resource contention among co-located VMs, and virtualization overhead. The performance variation can be significant, introducing challenges to learn workload-specific resource provisioning policies to automatically scale the cloud-hosted applications to maintain the desired response time. Moreover, auto-scaling multi-tier applications using minimal resources is even more challenging because bottlenecks may occur on multiple tiers concurrently. In this paper, we address the problem of using performance varying VMs for gracefully auto-scaling a multi-tier application using minimal resources to handle dynamically increasing workloads and satisfy the response time requirements. The proposed system uses a supervised learning method to identify the appropriate resources provisioning for multi-tier applications based on the prediction of the application response time and the request arrival rate. The supervised learning method learns a state transition configuration map which encodes a resource allocation states invariant to the underlying VMs performance variations. This configuration map helps to use performance varying resources in predictive autoscaling method. Our experimental evaluation using a real-world multi-tier web application hosted on a public cloud shows an improved application performance with minimal resources compared to conventional predictive auto-scaling methods.},
  archive      = {J_TCC},
  author       = {Waheed Iqbal and Abdelkarim Erradi and Muhammad Abdullah and Arif Mahmood},
  doi          = {10.1109/TCC.2019.2944364},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {595-607},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Predictive auto-scaling of multi-tier applications using performance varying cloud resources},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Petascale cloud supercomputing for terapixel visualization
of a digital twin. <em>TCC</em>, <em>10</em>(1), 583–594. (<a
href="https://doi.org/10.1109/TCC.2019.2958087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background—Photo-realistic terapixel visualization is computationally intensive and to date there have been no such visualizations of urban digital twins, the few terapixel visualizations that exist have looked towards space rather than earth. Objective—Our aims are: creating a scalable cloud supercomputer software architecture for visualization; a photo-realistic terapixel 3D visualization of urban IoT data supporting daily updates; a rigorous evaluation of cloud supercomputing for our application. Method—We migrated the Blender Cycles path tracer to the public cloud within a new software framework designed to scale to petaFLOP performance. Results—We demonstrate that we can compute a terapixel visualization in under one hour, the system scaling at 98 percent efficiency to use 1024 public cloud GPU nodes delivering 14 petaFLOPS. The resulting terapixel image supports interactive browsing of the city and its data at a wide range of sensing scales. Conclusion—The GPU compute resource available in the cloud is greater than anything available on our national supercomputers providing access to the globally competitive resources. The direct financial cost of access, compared to procuring and running these systems, was low. The indirect cost, in overcoming teething issues with cloud software development, should reduce significantly over time.},
  archive      = {J_TCC},
  author       = {Nicolas S. Holliman and Manu Antony and James Charlton and Stephen Dowsland and Philip James and Mark Turner},
  doi          = {10.1109/TCC.2019.2958087},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {583-594},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Petascale cloud supercomputing for terapixel visualization of a digital twin},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimizing speculative execution in spark heterogeneous
environments. <em>TCC</em>, <em>10</em>(1), 568–582. (<a
href="https://doi.org/10.1109/TCC.2019.2947674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The execution time of a stage is extended by a few slow running tasks in Spark computing environments. To tackle this so-called straggler problem, Spark adopts speculative execution mechanism under which the scheduler speculatively launch additional backup for the straggler with the hope to complete early. However, due to the characteristics of tasks and the complexity of runtime environments, the Spark original speculative execution strategy and its improved versions cannot deal with this problem effectively. In this paper, we propose a novel strategy called ETWR to improve the efficiency of speculative execution in Spark. We consider the heterogeneous environment when we around to tackle the three key points of speculative execution: straggler identification, backup node selection and effectiveness guarantee. Based on the task type classification, first, we divide the task into sub-phases and use both the process speed and progress rate within a phase to find the straggler promptly. Second, we use the Locally Weighted Regression model to estimate the execution time of the task, which will be used to calculate the task&#39;s remaining time and backup time. Third, we present iMCP model to guarantee the effectiveness of speculative tasks, which can additionally keep load balancing for nodes. Finally, the factors of fast node and better location are considered when choosing proper backup nodes. Extensive experiments show that ETWR can reduce the job execution time by 23.8 percent, and improve the cluster throughput by 33.2 percent compared with Spark-2.2.0.},
  archive      = {J_TCC},
  author       = {Zhongming Fu and Zhuo Tang},
  doi          = {10.1109/TCC.2019.2947674},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {568-582},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Optimizing speculative execution in spark heterogeneous environments},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Near-optimal energy-efficient algorithm for virtual network
function placement. <em>TCC</em>, <em>10</em>(1), 553–567. (<a
href="https://doi.org/10.1109/TCC.2019.2947554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To accommodate heterogeneous and sophisticated network services, Network Function Virtualization (NFV) is invented as a hopeful networking technology. The most distinct feature of NFV is that it separates network functions from physical hardware. In the NFV architecture, various types of Virtual Network Functions (VNFs) are placed on specific software-based middleboxes by telecom providers. Traffic traverses through a sequence of Virtual Network Functions (VNFs) in pre-defined order, which is named as Service Function Chain (SFC). However, how to effectively place VNFs at different locations and steer SFC requests while minimizing energy consumption is still an open problem. Accordingly, we investigate on the joint optimization of VNF placement and traffic steering for energy efficiency in telecom networks. We first present the power consumption model in NFV-enabled telecom networks, and then formulate the studied problem as an Integer Linear Programming (ILP) model. Since the problem is proved as NP-hard, we design a polynomial algorithm that can achieve near-optimal performances based on the Markov approximation technique. In addition, our algorithm can be extended to an online version to serve dynamic arriving SFC requests. The online algorithm achieves a near-optimal long-term averaged performance. Extensive simulation results show that compared with the benchmark algorithms, in the offline and online scenario, our algorithm can reduce up to 14.08 and 13.72 percent power consumption in telecom networks, respectively.},
  archive      = {J_TCC},
  author       = {Xiaoning Zhang and Zhichao Xu and Lang Fan and Shui Yu and Youyang Qu},
  doi          = {10.1109/TCC.2019.2947554},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {553-567},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Near-optimal energy-efficient algorithm for virtual network function placement},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-access filtering for privacy-preserving fog computing.
<em>TCC</em>, <em>10</em>(1), 539–552. (<a
href="https://doi.org/10.1109/TCC.2019.2942293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interest in fog computing is growing, including in traditionally conservative and sensitive areas such as military and governments. This is partly driven by the interconnectivity of our society, and advances in technologies such as Internet-of-Things (IoT). However, protecting against privacy leakage is one of several key considerations in fog computing deployment. Therefore, in this paper, we present a privacy-preserving multi-layer access filtering model, designed for a fog computing environment; hence, coined fog-based access filter (FAF). FAF comprises three key algorithms, namely: access filter initialization algorithm, optimal privacy-energy-time algorithm, and tuple reduction algorithm. Also, a hierarchical classification is used to distinguish the protection objectives. Findings from our experimental evaluation demonstrate that FAF allows one to achieve an optimal balance between privacy protection and computational costs.},
  archive      = {J_TCC},
  author       = {Keke Gai and Liehuang Zhu and Meikang Qiu and Kai Xu and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TCC.2019.2942293},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {539-552},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Multi-access filtering for privacy-preserving fog computing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MPEC: Distributed matrix multiplication performance modeling
on a scale-out cloud environment for data mining jobs. <em>TCC</em>,
<em>10</em>(1), 521–538. (<a
href="https://doi.org/10.1109/TCC.2019.2950400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many data mining workloads are being analyzed in large-scale distributed cloud computing environments which provide nearly infinite resources with diverse hardware configurations. To maintain cost-efficiency in such environments, understanding the characteristics and estimating the overheads of a distributed matrix multiplication task that is a core computation kernel in many machine learning algorithms are essential. This article aims to propose a Matrix Multiplication Performance Estimator on Cloud (MPEC) algorithm. The proposed algorithm predicts the latency incurred when executing distributed matrix multiplication tasks of various input sizes and shapes with diverse instance types and a different number of worker nodes on cloud computing environments. To achieve this goal, we first analyze the characteristics of distributed matrix multiplication tasks. With characteristics generated from qualitative analysis, we propose to apply an ensemble of non-linear regression algorithm to predict the execution time of arbitrary matrix multiplication tasks. Thorough experimental results reveal that the proposed algorithm demonstrates higher accuracy than a state-of-the-art machine learning task performance estimation engine, Ernest, by decreasing the Mean Absolute Percentage Error (MAPE) in half.},
  archive      = {J_TCC},
  author       = {Jeongchul Kim and Myungjun Son and Kyungyong Lee},
  doi          = {10.1109/TCC.2019.2950400},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {521-538},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {MPEC: Distributed matrix multiplication performance modeling on a scale-out cloud environment for data mining jobs},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MarVeLScaler: A multi-view learning-based auto-scaling
system for MapReduce. <em>TCC</em>, <em>10</em>(1), 506–520. (<a
href="https://doi.org/10.1109/TCC.2019.2944916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To promote cloud computing from current pay-per-request model to truly pay-per-use, tenants are crying for automatic tools to auto-estimate the amount of resources for MapReduce jobs. Such tools call for accurately quantifying the relationship among workload, resources and completion time. Various prediction models have been proposed. However, none of these models takes virtual machines’ (VMs) performance variance during a job&#39;s execution into consideration, leading to underestimate the required resources and exceed the job&#39;s deadline. To address this problem, we propose a multi-view deep learning model to capture real-time performance variance and automatically scale out the cloud cluster whenever necessary. We implement MarVeLScaler , a prototype system including two useful modules, namely, Scale Estimator and Scale Controller . Scale Estimator preliminarily estimates the required cluster size for a MapReduce job with given a concrete workload and deadline. During the runtime, Scale Controller adjusts the scale of the cluster according to its real-time running status to guarantee the job finished on time. We evaluate the performance of MarVeLScaler based on Hadoop in Alibaba Cloud. Experiments show that MarVeLScaler can provide 98.4 percent accuracy of prediction in determining initial cluster size, and save 30.8 percent of expense while still guaranteeing similar performance compared with the state-of-the-art methods.},
  archive      = {J_TCC},
  author       = {Yi Li and Fangming Liu and Qiong Chen and Yibing Sheng and Miao Zhao and Jianping Wang},
  doi          = {10.1109/TCC.2019.2944916},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {506-520},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {MarVeLScaler: A multi-view learning-based auto-scaling system for MapReduce},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lyapunov optimization-based trade-off policy for mobile
cloud offloading in heterogeneous wireless networks. <em>TCC</em>,
<em>10</em>(1), 491–505. (<a
href="https://doi.org/10.1109/TCC.2019.2938504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve mobile users’ service experience, mobile cloud computing (MCC) is promoted. Although MCC can alleviate the burdens of Smart mobile devices (SMDs) by offloading computation-intensive applications to the cloud, it also aggravates computing and storage overheads in cloud centers and bandwidth overhead on wireless links for offloading workloads of mobile users. Therefore, we should carefully design the offloading policy to decrease these overheads while easing the burdens of SMDs. To this end, we investigate the offloading policy in heterogeneous wireless networks. In this paper, a queue model is built to formulate the mobile users’ workload offloading problem and Lyapunov optimization framework is proposed to make trade-off between system offloading utility and queue backlog. For deterministic WiFi connections, a Lagrangian optimization method is proposed to decide the optimal offloading workloads. Furthermore, considering random WiFi connection durations, a multi-stage stochastic programming method is proposed. The experimental results show effectiveness of the Lagrangian optimization offloading method for deterministic WiFi connection and the multi-stage stochastic programming method for random WiFi connection.},
  archive      = {J_TCC},
  author       = {Yun Li and Shichao Xia and Mengyan Zheng and Bin Cao and Qilie Liu},
  doi          = {10.1109/TCC.2019.2938504},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {491-505},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Lyapunov optimization-based trade-off policy for mobile cloud offloading in heterogeneous wireless networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight and expressive fine-grained access control for
healthcare internet-of-things. <em>TCC</em>, <em>10</em>(1), 474–490.
(<a href="https://doi.org/10.1109/TCC.2019.2936481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare Internet-of-Things (IoT) is an emerging paradigm that enables embedded devices to monitor patients vital signals and allows these data to be aggregated and outsourced to the cloud. The cloud enables authorized users to store and share data to enjoy on-demand services. Nevertheless, it also causes many security concerns because of the untrusted network environment, dishonest cloud service providers and resource-limited devices. To preserve patients&amp;#x2019; privacy, existing solutions usually apply cryptographic tools to offer access controls. However, fine-grained access control among authorized users is still a challenge, especially for lightweight and resource-limited end-devices. In this paper, we propose a novel healthcare IoT system fusing advantages of attribute-based encryption, cloud and edge computing, which provides an efficient, flexible, secure fine-grained access control mechanism with data verification in healthcare IoT network without any secure channel and enables data users to enjoy the lightweight decryption. We also define the formal security models and present security proofs for our proposed scheme. The extensive comparison and experimental simulation demonstrate that our scheme has better performance than existing solutions.},
  archive      = {J_TCC},
  author       = {Shengmin Xu and Yingjiu Li and Robert H. Deng and Yinghui Zhang and Xiangyang Luo and Ximeng Liu},
  doi          = {10.1109/TCC.2019.2936481},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {474-490},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Lightweight and expressive fine-grained access control for healthcare internet-of-things},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint optimization of MapReduce scheduling and network
policy in hierarchical data centers. <em>TCC</em>, <em>10</em>(1),
461–473. (<a href="https://doi.org/10.1109/TCC.2019.2961653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As large-scale data analytic becomes norm in various industries, using MapReduce frameworks to analyze ever-increasing volumes of data will keep growing. In turn, this trend drives up the intention to move MapReduce into multi-tenant clouds. However, the application performance of MapReduce can be significantly affected by the time-varying network bandwidth in a shared cluster. Although many recent studies improve MapReduce performance by dynamic scheduling to reduce the shuffle traffic, most of them do not consider the impact by widely existing hierarchical network architectures in data centers. In this article, we propose and design a hierarchical topology (Hit) aware MapReduce scheduler to minimize overall data traffic cost and hence to reduce job execution time. We first formulate the problem as a Topology Aware Assignment (TAA) optimization problem while considering dynamic computing and communication resources in the cloud with hierarchical network architecture. We further develop a synergistic strategy to solve the TAA problem by using the stable matching theory, which ensures the preference of both individual tasks and hosting machines. Finally, we implement the proposed scheduler as a pluggable module on Hadoop YARN and evaluate its performance by testbed experiments and simulations. The testbed experimental results show that Hit-scheduler can improve job completion time by 28 and 11 percent compared to Capacity Scheduler and Probabilistic Network-Aware scheduler, respectively. Our simulations further demonstrate that Hit-scheduler can reduce the traffic cost by 38 percent at most and the average shuffle flow traffic time by 32 percent compared to capacity scheduler. In this article, we have extended Hit-scheduler to a decentralized heuristic scheme to perform the policy-aware allocation in data center environments. Many existing centralized approximation approaches are too complex and infeasible to implement over a data center, which typically include large amounts of servers, containers, switches, and traffic flows. In the extension, we have designed a decentralized heuristic scheme to perform the Policy-Aware Task (PAT) allocation by using existing centralize algorithm to approximately maximize the total gained utility. Finally, the simulation based experimental results show that the proposed PAT policy reduces the communication cost by 33.6 percent compared with the default scheduler in data centers.},
  archive      = {J_TCC},
  author       = {Donglin Yang and Dazhao Cheng and Wei Rang and Yu Wang},
  doi          = {10.1109/TCC.2019.2961653},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {461-473},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Joint optimization of MapReduce scheduling and network policy in hierarchical data centers},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint computation offloading and bandwidth assignment in
cloud-assisted edge computing. <em>TCC</em>, <em>10</em>(1), 451–460.
(<a href="https://doi.org/10.1109/TCC.2019.2950395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offloading computation based on mobile edge computing paradigms can augment the computational capabilities of resource-scarce mobile devices. However, the capacity limitations of edge servers constrain the performance improvement achieved through computation offloading. In this article, we consider a three-tier computation offloading schema with multiple users, edge servers, and cloud servers. Computation can be offloaded from mobile devices to edge servers or can be further offloaded to remote cloud servers if necessary. Since a number of mobile devices connected to edge servers will share a common wireless communication network, which may contain both uplink and downlink channels, the assignment of bandwidth assignment the channels also constrains the performance improvement of computation offloading. In this paper, the problems of how to determine the offloading strategy and of how to assign the bandwidth are jointly studied and formulated as a programming problem to minimize the average application response time. We analyze the joint problem and further transform it into a piecewise convex programming problem. We propose an efficient algorithm that can find the optimal solution. Extensive experiments demonstrate that our algorithm significantly outperforms previous algorithms. The experimental results also show that the performance of our algorithm is highly robust.},
  archive      = {J_TCC},
  author       = {Kai Guo and Mingcong Yang and Yongbing Zhang and Jiannong Cao},
  doi          = {10.1109/TCC.2019.2950395},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {451-460},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Joint computation offloading and bandwidth assignment in cloud-assisted edge computing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving performance and capacity utilization in cloud
storage for content delivery and sharing services. <em>TCC</em>,
<em>10</em>(1), 439–450. (<a
href="https://doi.org/10.1109/TCC.2020.2968444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Content delivery and sharing (CDS) is a popular and cost effective cloud-based service for organizations to deliver/share contents to/with end-users, partners and insider users. This type of service improves the data availability and I/O performance by producing and distributing replicas of shared contents. However, such a technique increases overhead on the storage/network resources. This article introduces a threefold methodology to improve the trade-off between I/O performance and capacity utilization of cloud storage for CDS services. This methodology includes: i) Definition of a classification model for identifying types of users and contents by analyzing their consumption/ demand and sharing patterns, ii) Usage of the classification model for defining content availability and load balancing schemes, and iii) Integration of a dynamic availability scheme into a cloud-based CDS system. Our method was implemented on both a simulator and a real-world CDS service, supporting information sharing operations performed in a cloud storage. An experimental evaluation, conducted in a private cloud through simulation and emulation of workloads, showed the feasibility of this methodology in terms of storage capacity utilization, whereas the real-world implementation revealed the efficiency of applying a classification model to information sharing patterns in terms of I/O performance.},
  archive      = {J_TCC},
  author       = {Victor J. Sosa-Sosa and Alfredo Barron and Jose Luis Gonzalez-Compean and Jesus Carretero and Ivan Lopez-Arevalo},
  doi          = {10.1109/TCC.2020.2968444},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {439-450},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Improving performance and capacity utilization in cloud storage for content delivery and sharing services},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Forward secure public key encryption with keyword search for
outsourced cloud storage. <em>TCC</em>, <em>10</em>(1), 426–438. (<a
href="https://doi.org/10.1109/TCC.2019.2944367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud storage has become a primary industry in remote data management service but also attracts security concerns, where the best available approach for preventing data disclosure is encryption. Among them the public key encryption with keyword search (PKSE) is considered to be a promising technique, since clients can efficiently search over encrypted data files. That is, a client first generates a search token when to query data files, the cloud server uses the search token to proceed the query over encrypted data files. However, a serious attack is raised when PKSE meets cloud. Formally speaking, the cloud server can learn the information of a newly added encrypted data file containing the keyword that previously queried by using the search tokens it has received, and can further discover the privacy information. To address this issue, we propose a forward secure public key searchable encryption scheme, in which a cloud server cannot learn any information about a newly added encrypted data file containing the keyword that previously queried. To better understand the design principle, we introduce a framework for constructing forward secure public key searchable encryption schemes based on attribute-based searchable encryption. Finally, the experiments show our scheme is efficient.},
  archive      = {J_TCC},
  author       = {Ming Zeng and Haifeng Qian and Jie Chen and Kai Zhang},
  doi          = {10.1109/TCC.2019.2944367},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {426-438},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Forward secure public key encryption with keyword search for outsourced cloud storage},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring potential and feasibility of binary code sharing
in mobile computing. <em>TCC</em>, <em>10</em>(1), 411–425. (<a
href="https://doi.org/10.1109/TCC.2019.2932386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While tremendous growing mobile apps offer users rich services and functionalities, they also bring significant performance and energy issues. Code sharing is promising to address these issues, but existing application-level code sharing is rather restrictive. This paper develops the a transparent machine code sharing for mobile devices, and presents its design, implementation, and deployment. SnapCode enables machine code sharing across a wide variety of commercial off-the-shelf Android devices. By sharing and running machine code, SnapCode can offer significant speed-ups: an average speed-up of 9.9X for one-time trial apps, and up to 120X in apps’ regular uses. In addition, it can save more than 80 percent energy consumption.},
  archive      = {J_TCC},
  author       = {Chao Wu and Lan Zhang and Zhenhua Li and Qiushi Li and Yaoxue Zhang},
  doi          = {10.1109/TCC.2019.2932386},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {411-425},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Exploring potential and feasibility of binary code sharing in mobile computing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy-efficient device activation, rule installation and
data transmission in software defined DCNs. <em>TCC</em>,
<em>10</em>(1), 396–410. (<a
href="https://doi.org/10.1109/TCC.2019.2947900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the prosperity of cloud computing and video services, the demand for network resources has increased dramatically, leading to the remarkable growth in the amount of network energy consumption, a key factor restricting the development of data centers. Numerous existing works reduce network energy consumption by optimizing data transmission, but they ignore the energy consumption for data transmission preparation, such as activating devices and installing rules. In this paper, we jointly optimize device activation, rule installation and data transmission to minimize network energy consumption. Specifically, we first formulate the minimization problem of the energy consumption of device activation, rule installation, and data transmission. We then prove that it is NP-complete to get the optimal solution of the minimization problem, furthermore, we propose a heuristic algorithm to plan the path with minimum network energy consumption for each flow. The simulation results show that the energy consumption of our algorithm is close to the optimal solution solved by Gurobi, and our algorithm has lower complexity. Compared with the state-of-the-art algorithm, our algorithm always consumes less energy and has shorter flow completion time.},
  archive      = {J_TCC},
  author       = {Yue Zeng and Songtao Guo and Guiyan Liu and Pan Li and Yuanyuan Yang},
  doi          = {10.1109/TCC.2019.2947900},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {396-410},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Energy-efficient device activation, rule installation and data transmission in software defined DCNs},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient provision of service function chains in overlay
networks using reinforcement learning. <em>TCC</em>, <em>10</em>(1),
383–395. (<a href="https://doi.org/10.1109/TCC.2019.2961093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-Defined Networking (SDN) and Network Functions Virtualization (NFV) technologies facilitate deploying Service Function Chains (SFCs) at clouds in efficiency and flexibility. However, it is still challenging to efficiently chain Virtualized Network Functions (VNFs) in overlay networks without knowledge of underlying network configurations. Although there are many deterministic approaches for VNF placement and chaining, they have high complexity and depend on state information of substrate networks. Fortunately, Reinforcement Learning (RL) brings opportunities to alleviate this challenge as it can learn to make suitable decisions without prior knowledge. Therefore, in this article, we propose an RL approach for efficient SFC provision in overlay networks, where the same VNFs provided by multiple vendors are with different performance. Specifically, we first formulate the problem into an Integer Linear Programming (ILP) model for benchmarking. Then, we present the online SFC path selection into a Markov Decision Process (MDP) and propose a corresponding policy-gradient-based solution. Finally, we evaluate our proposed approach with extensive simulations with randomly generated SFC requests and a real-world video streaming dataset, and implement an emulation system for feasibility verification. Related results demonstrate that performance of our approach is close to the ILP-based method and better than deep Q-learning, random, and load-least-greedy methods.},
  archive      = {J_TCC},
  author       = {Guanglei Li and Huachun Zhou and Bohao Feng and Yuming Zhang and Shui Yu},
  doi          = {10.1109/TCC.2019.2961093},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {383-395},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficient provision of service function chains in overlay networks using reinforcement learning},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient privacy-preserving outsourced discrete wavelet
transform in the encrypted domain. <em>TCC</em>, <em>10</em>(1),
366–382. (<a href="https://doi.org/10.1109/TCC.2019.2948012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Signal processing in the encrypted domain is a potential tool to protect sensitive signals against untrusted cloud servers and unauthorized users in the delegated computing setting, without affecting the accuracy of large volume of signal analyzing and processing. Most existing approaches use Paillier’s public key additively homomorphic encryption to encrypt each signal in a large bundle; thus, incurring significant computational costs at local, often resource-constrained, devices while guaranteeing only signal input privacy. To address these limitations, in this paper, an efficient privacy-preserving outsourced discrete wavelet transform scheme (PPDWT), comprising PPDWT-1 and PPDWT-2, without leveraging public key (fully) homomorphic encryption is proposed. Specifically, PPDWT-1 is proposed to achieve signal input privacy against the collusion between the honest-but-curious cloud and unauthorized users, and the proposed PPDWT-2 protects both signal input privacy and coefficient privacy against collusion attacks. Both constructions leverage the offline execution of any one-way trapdoor permutation only once to encrypt batch signals, and permit signal processing in the encrypted domain. In our approach, only authorized users can successfully decipher the result of discrete wavelet transform. Compared to the $O(|l|)$ computational complexity on the user’s end in existing state-of-the-art public key homomorphic encryption-based techniques, our approach only incurs $O(1)$ computational complexity which is independent to size of the signal inputs $|l|$ . We also discuss the expanding factor, the upper bound and various extensions to privacy-preserving discrete cosine/fourier transform in the encrypted domain. Finally, our proposed PPDWT is formally proved secure under the universal composability (UC) model. We then evaluate the proposed approach using case studies to demonstrate its effectiveness and practicability.},
  archive      = {J_TCC},
  author       = {Jun Zhou and Zhenfu Cao and Xiaolei Dong and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TCC.2019.2948012},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {366-382},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficient privacy-preserving outsourced discrete wavelet transform in the encrypted domain},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient identity-based provable multi-copy data possession
in multi-cloud storage. <em>TCC</em>, <em>10</em>(1), 356–365. (<a
href="https://doi.org/10.1109/TCC.2019.2929045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To increase the availability and durability of the outsourced data, many customers store multiple copies on multiple cloud servers. To guarantee the integrity of multi-copies, some provable data possession (PDP) protocols for multi-copy are presented. However, most of previous PDP protocols consider all copies to be stored on only one cloud storage server. In some degree, multi-copy makes little sense in such circumstance. Furthermore, many PDP protocols depend on the technique of public key infrastructure (PKI), which suffers many types of security vulnerabilities and also brings heavy communicational and computational cost. To increase the security and efficiency, we provide a novel identity-based PDP scheme of multi-copy on multiple cloud storage servers. In our scheme, all copies are delivered to different cloud storage servers, which work cooperatively to store the customer&#39;s data. By the homomorphic verifiable tags, the integrity of all copies can be checked simultaneously. The system model and security model of our scheme are provided in the paper. The security for our scheme is proved based on the computation Diffie-Hellman (CDH) hard problem. Analysis and experimental evaluation show that our scheme is efficient and practical. The proposed scheme is the first identity-based PDP scheme for multi-copy and multi-cloud servers.},
  archive      = {J_TCC},
  author       = {Jiguo Li and Hao Yan and Yichen Zhang},
  doi          = {10.1109/TCC.2019.2929045},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {356-365},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficient identity-based provable multi-copy data possession in multi-cloud storage},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficiency measurement of cloud service providers using
network data envelopment analysis. <em>TCC</em>, <em>10</em>(1),
348–355. (<a href="https://doi.org/10.1109/TCC.2019.2927340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An increasing number of organizations and businesses around the world use cloud computing services to improve their performance in the competitive marketplace. However, one of the biggest challenges in using cloud computing services is performance measurement and the selection of the best cloud service providers (CSPs) based on quality of service (QoS) requirements [13] . To address this shortcoming in this article we propose a network data envelopment analysis (DEA) method in measuring the efficiency of CSPs. When network dimensions are taken into consideration, a more comprehensive analysis is enabled where divisional efficiency is reflected in overall efficiency estimates. This helps managers and decision makers in organizations to make accurate decisions in selecting cloud services. In the current study, the non-oriented network slacks-based measure (SBM) model and conventional SBM model with the assumptions of constant returns to scale (CRS) and variable returns to scale (VRS) are applied to measure the performance of 18 CSPs. The obtained results show the superiority of the network DEA model and they also demonstrate that the proposed model can evaluate and rank CSPs much better than compared to traditional DEA models.},
  archive      = {J_TCC},
  author       = {Majid Azadi and Ali Emrouznejad and Fahimeh Ramezani and Farookh Khadeer Hussain},
  doi          = {10.1109/TCC.2019.2927340},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {348-355},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficiency measurement of cloud service providers using network data envelopment analysis},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic pricing for sensor-cloud platform in the presence of
dumb nodes. <em>TCC</em>, <em>10</em>(1), 334–347. (<a
href="https://doi.org/10.1109/TCC.2019.2950396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of dumb nodes in sensor-cloud environment leads to degraded system performance. In this article, we consider the presence of dumb nodes in the sensor-cloud platform, and thereafter, propose a dynamic pricing scheme, while considering the existence of such nodes in the networks. The existing literature addresses the problem of pricing in sensor-cloud with the assumption of an ideal environment with normally functioning sensor nodes. The proposed pricing model considers the realistic existence of dumb nodes in sensor-cloud platforms. Further, the dumb behavior of a sensor node is dynamic in nature, as it is dependent on environmental conditions such as the occurrence of heavy rainfall, high temperature, and the presence of fog. However, in the absence of such adverse environmental conditions, the erstwhile dumb nodes resume normal behavior. The permanent removal of a dumb node from sensor-cloud is not always a feasible solution. When a dumb node is assigned to a virtual sensor, the existing pricing scheme in sensor-cloud charges same as other normal nodes. Thus, in such a situation, a user pays the normal price for a dumb node to the Sensor-Cloud Service Provider (SCSP). Consequently, the sensor owner of dumb node earn same profit as the owner of a normal node. Therefore, we formulate a scheme for Dynamic pricing in sensor-cloud environment in the presence of dumb nodes ( DISCLOUD ). As the presence of dumb nodes in sensor-cloud affects the Quality of Service (QoS), we propose a scheme considering QoS of the sensor-cloud. The proposed scheme, DISCLOUD, enables profit maximization of the SCSP, while considering the price required to be paid by end-user based on QoS.},
  archive      = {J_TCC},
  author       = {Arijit Roy and Sudip Misra and Prerona Dutta},
  doi          = {10.1109/TCC.2019.2950396},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {334-347},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Dynamic pricing for sensor-cloud platform in the presence of dumb nodes},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual-server public-key authenticated encryption with keyword
search. <em>TCC</em>, <em>10</em>(1), 322–333. (<a
href="https://doi.org/10.1109/TCC.2019.2945714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud storage, how to search sensitive data efficiently and securely is a challenging problem. The searchable encryption technique provides a secure storage method without loss of data confidentiality and usability. As an important branch of searchable encryption, public-key encryption with keyword search (PEKS) is widely studied by scholars. However, most of the traditional PEKS schemes are vulnerable to the inside keyword guessing attack (IKGA). Resisting the inside keyword guessing attack is likely to become an essential property of all new PEKS schemes. For a long time, mitigating IKGA has been inefficient and difficult, and most existing PEKS schemes fail in achieving their security goals. To address the above problems, we define the notion of D ual-server P ublic-key A uthenticated E ncryption with K eyword S earch (DPAEKS), which protects against IKGA by leveraging two servers that do not cooperate, and supports the authentication property. Then, we provide a construction of DPAEKS without bilinear pairings. Experimental results obtained using a real-world dataset show that our scheme is highly efficient and provides strong security, making it suitable for deployment in practical applications.},
  archive      = {J_TCC},
  author       = {Biwen Chen and Libing Wu and Sherali Zeadally and Debiao He},
  doi          = {10.1109/TCC.2019.2945714},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {322-333},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Dual-server public-key authenticated encryption with keyword search},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deadline-aware fast one-to-many bulk transfers over
inter-datacenter networks. <em>TCC</em>, <em>10</em>(1), 304–321. (<a
href="https://doi.org/10.1109/TCC.2019.2935435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An increasing number of cloud services are operated globally, where the service data are frequently replicated across geographically distributed datacenters to improve service quality and reliability. Such replication generates many one-to-many bulk data transfers over inter-datacenter networks from one datacenter to many receiver datacenters. To provide end-users with guaranteed services, these data transfers are usually required to be completed within designated deadlines. Despite the exponential growth in data demand, there has been little work on guaranteeing deadlines for one-to-many transfers, which is the subject of this paper. This paper proposes a centralized admission control coupled with a scheduling algorithm, named deAdline-Guaranteed transfEr (AGE), to guarantee the deadline of admitted data transfers and utilize the network capacity efficiently. The key idea is to flexibly select the source datacenter for receiver datacenters and allow the remaining receivers to obtain a replica from either the original source or the other receivers that have already received a copy. By jointly allocating the source for receivers and the bandwidth and routing paths for every data transfer, AGE maximizes the number of deadline-satisfied transfers. Our simulations show that compared to the state-of-the-art, AGE guarantees the deadline for up to 70 percent more transfers, achieves at least 2× higher network throughput, and reduces the completion time up to 80 percent.},
  archive      = {J_TCC},
  author       = {Long Luo and Yijing Kong and Mohammad Noormohammadpour and Zilong Ye and Gang Sun and Hongfang Yu and Bo Li},
  doi          = {10.1109/TCC.2019.2935435},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {304-321},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Deadline-aware fast one-to-many bulk transfers over inter-datacenter networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cutting the unnecessary long tail: Cost-effective big data
clustering in the cloud. <em>TCC</em>, <em>10</em>(1), 292–303. (<a
href="https://doi.org/10.1109/TCC.2019.2947678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering big data often requires tremendous computational resources where cloud computing is undoubtedly one of the promising solutions. However, the computation cost in the cloud can be unexpectedly high if it cannot be managed properly. The long tail phenomenon has been observed widely in the big data clustering area, which indicates that the majority of time is often consumed in the middle to late stages in the clustering process. In this research, we try to cut the unnecessary long tail in the clustering process to achieve a sufficiently satisfactory accuracy at the lowest possible computation cost. A novel approach is proposed to achieve cost-effective big data clustering in the cloud. By training the regression model with the sampling data, we can make widely used k-means and EM (Expectation-Maximization) algorithms stop automatically at an early point when the desired accuracy is obtained. Experiments are conducted on four popular data sets and the results demonstrate that both k-means and EM algorithms can achieve high cost-effectiveness in the cloud with our proposed approach. For example, in the case studies with the much more efficient k-means algorithm, we find that achieving a 99 percent accuracy needs only 47.71-71.14 percent of the computation cost required for achieving a 100 percent accuracy while the less efficient EM algorithm needs 16.69-32.04 percent of the computation cost. To put that into perspective, in the United States land use classification example, our approach can save up to $94,687.49 for the government in each use.},
  archive      = {J_TCC},
  author       = {Dongwei Li and Shuliang Wang and Nan Gao and Qiang He and Yun Yang},
  doi          = {10.1109/TCC.2019.2947678},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {292-303},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cutting the unnecessary long tail: Cost-effective big data clustering in the cloud},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context-aware policy enforcement for PaaS-enabled access
control. <em>TCC</em>, <em>10</em>(1), 276–291. (<a
href="https://doi.org/10.1109/TCC.2019.2927341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is generally conceded that, due to security and privacy concerns, enterprises and users are reluctant to embrace the cloud computing paradigm and hence benefit from the cost reductions and the increased flexibility or business agility that this paradigm brings about. These concerns stem mainly from the significantly-expanded attack surfaces that result from the heterogeneous nature of cloud services and the dynamicity inherent in cloud environments. In order to alleviate these concerns, effective and flexible access control approaches are required to consider the contextual parameters that characterise data access requests in the cloud. In this respect, this work presents PaaSword: a novel holistic access control framework—essentially a PaaS offering—that extends the popular XACML standard with semantic reasoning capabilities that support the federation of effective context-aware access control policies and their infusion into cloud applications with minimal manual intervention and effort. To determine the performance of our solution, a comparative evaluation test is presented and discussed, against a well-known reference implementation of the XACML standard, namely the open source WSO2 Balana engine.},
  archive      = {J_TCC},
  author       = {Yiannis Verginadis and Ioannis Patiniotakis and Panagiotis Gouvas and Spyros Mantzouratos and Simeon Veloudis and Sebastian Thomas Schork and Ludwig Seitzluwig and Iraklis Paraskakis and Gregoris Mentzas},
  doi          = {10.1109/TCC.2019.2927341},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {276-291},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Context-aware policy enforcement for PaaS-enabled access control},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cloud-based framework for spatio-temporal trajectory data
segmentation and query. <em>TCC</em>, <em>10</em>(1), 258–275. (<a
href="https://doi.org/10.1109/TCC.2019.2949987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory segmentation is a technique of dividing sequential trajectory into segments. These segments are building blocks to various applications. Hence a system framework is essential to support trajectory segment indexing, storage, and query. When the size of segments is beyond the computing capacity of a single processing node, a distributed solution is proposed. In this article, we develop a distributed trajectory segmentation framework that includes a greedy-split segmentation method. This framework consists of distributed in-memory processing and a cluster of graph storage respectively. For fast trajectory queries, we design a distributed spatial R-tree index of trajectory segments. Using the indexes, we build scalable query operations from both in-memory processing and access to graph storage. Based on this framework, we define two metrics to measure trajectory similarity and chance of collision. These two metrics are further applied to identify moving groups of trajectories. We quantitatively evaluate the effects of data partition, parallelism, and data size on the system. We identify the bottleneck factors at the data partition stage and validate two mitigation techniques to data skew. The evaluation demonstrates our distributed segmentation method and the system framework scale as the growth of the workload and the size of the parallel cluster.},
  archive      = {J_TCC},
  author       = {Huaqiang Kang and Yan Liu and Weishan Zhang},
  doi          = {10.1109/TCC.2019.2949987},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {258-275},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cloud-based framework for spatio-temporal trajectory data segmentation and query},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cloud computing assisted blockchain-enabled internet of
things. <em>TCC</em>, <em>10</em>(1), 247–257. (<a
href="https://doi.org/10.1109/TCC.2019.2930259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the term ‘Internet of Things’ (IoT) has garnered great attention. As a trusted, dependable, and decentralized approach, blockchain has already been used in IoT. However, the existing blockchain has a number of drawbacks that prevent it from being used as a generic platform for IoT. The nodes in IoT are heavily resource-limited, especially computing and networking resources. Unfortunately, they are necessary for the blockchain to solve complicated puzzles and propagate blocks. In this paper, we propose agent mining and cloud mining approaches to solve the above problem in the blockchain-enabled IoT. To be specific, miners act as mining agents for nodes in IoT, offload mining tasks to cloud computing servers, and use networking resources dynamically. Furthermore, in order to enhance the performance, the access selection of users, computing resources allocation, and networking resources allocation are formulated as a joint optimization problem. We then propose a dueling deep reinforcement learning approach to address this problem. Numerical results justify the effectiveness of our proposed scheme.},
  archive      = {J_TCC},
  author       = {Chao Qiu and Haipeng Yao and Chunxiao Jiang and Song Guo and Fangmin Xu},
  doi          = {10.1109/TCC.2019.2930259},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {247-257},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cloud computing assisted blockchain-enabled internet of things},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ClassyTune: A performance auto-tuner for systems in the
cloud. <em>TCC</em>, <em>10</em>(1), 234–246. (<a
href="https://doi.org/10.1109/TCC.2019.2936567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance tuning can improve the system performance and thus enable the reduction of cloud computing resources needed to support an application. Due to the ever increasing number of parameters and complexity of systems, there is a necessity to automate performance tuning for the complicated systems in the cloud. The state-of-the-art tuning methods are adopting either the experience-driven tuning approach or the data-driven one. Data-driven tuning is attracting increasing attentions, as it has wider applicability. But existing data-driven methods cannot fully address the challenges of sample scarcity and high dimensionality simultaneously. We present ClassyTune, a data-driven automatic configuration tuning tool for cloud systems. ClassyTune exploits the machine learning model of classi fication for auto-tuning. This exploitation enables the induction of more training samples without increasing the input dimension. Experiments on seven popular systems in the cloud show that ClassyTune can effectively tune system performance to seven times higher for high-dimensional configuration space, outperforming expert tuning and the state-of-the-art auto-tuning solutions. We also describe a use case in which performance tuning enables the reduction of 33 percent computing resources needed to run an online stateless service.},
  archive      = {J_TCC},
  author       = {Yuqing Zhu and Jianxun Liu},
  doi          = {10.1109/TCC.2019.2936567},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {234-246},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {ClassyTune: A performance auto-tuner for systems in the cloud},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Authorized keyword search over outsourced encrypted data in
cloud environment. <em>TCC</em>, <em>10</em>(1), 216–233. (<a
href="https://doi.org/10.1109/TCC.2019.2931896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For better data availability and accessibility while ensuring data secrecy, end-users often tend to outsource their data to the cloud servers in an encrypted form. However, this brings a major challenge to perform the search for some keywords over encrypted content without disclosing any information to unintended entities. This paper proposes a novel expressive authorized keyword search scheme relying on the concept of ciphertext-policy attribute-based encryption. The originality of the proposed scheme is multifold. First, it supports the generic and convenient multi-owner and multi-user scenario, where the encrypted data are outsourced by several data owners and searchable by multiple users. Second, the formal security analysis proves that the proposed scheme is semantically secure against chosen keyword and outsiders keyword guessing attacks. Third, an interactive protocol is introduced which avoids the need of any secure-channels between users and service provider. Fourth, due to the concept of bilinear-map accumulator, the system can efficiently revoke users and/or their attributes, and authenticate them prior to launching any expensive search operations. Fifth, conjunctive keyword search is provided thus enabling to search for multiple keywords simultaneously, with minimal cost. Sixth, the performance analysis shows that the proposed scheme outperforms closely-related works.},
  archive      = {J_TCC},
  author       = {Nazatul Haque Sultan and Nesrine Kaaniche and Maryline Laurent and Ferdous Ahmed Barbhuiya},
  doi          = {10.1109/TCC.2019.2931896},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {216-233},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Authorized keyword search over outsourced encrypted data in cloud environment},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An online cost-efficient transmission scheme for
information-agnostic traffic in inter-datacenter networks. <em>TCC</em>,
<em>10</em>(1), 202–215. (<a
href="https://doi.org/10.1109/TCC.2019.2941688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of cloud computing, network services are deployed on geographically distributed cloud platforms, which results in a large amount of inter-datacenter traffic. Multi-tier pricing schemes are widely adopted by cloud service providers (CSPs) to charge cloud users for inter-datacenter transmission services. To avoid a severe penalty associated with missing a deadline, cloud users are prone to selecting a sufficiently high service level. However, they are usually unaware of the total traffic volume before accessing the network; hence, a high transmission cost is introduced. In this paper, we propose an online cost-efficient transmission scheme for cloud users with information-agnostic traffic. The basic idea is to split a long-term transmission request into a series of short-term ones. In this scheme, we take into account the CSP’s countermeasures, and model the interactions between the cloud users and the CSP as a Stackelberg game. We show that the optimal number of short-term requests and the associated transmission service levels can be determined with an online algorithm based on Lyapunov optimization. The experimental results reveal that the CSP and the cloud users can achieve a win-win outcome, whereby the transmission cost of cloud users can be reduced by 59 percent.},
  archive      = {J_TCC},
  author       = {Xiaodong Dong and Laiping Zhao and Xiaobo Zhou and Keqiu Li and Deke Guo and Tie Qiu},
  doi          = {10.1109/TCC.2019.2941688},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {202-215},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {An online cost-efficient transmission scheme for information-agnostic traffic in inter-datacenter networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-task oriented framework for mobile computation
offloading. <em>TCC</em>, <em>10</em>(1), 187–201. (<a
href="https://doi.org/10.1109/TCC.2019.2952346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computation offloading has become popular in recent years as it is an effective way to reduce the energy consumption and enhance the performance of smartphones. To deal with the heterogeneous architectures between the smartphone and the server, and to simplify deployment of the server, we propose and implement a lightweight offloading framework which supports offloading of compute-intensive tasks and deploying the server efficiently. Based on this framework, generic and developer-customized offloading services could be provided for different third-party applications. Furthermore, we design a multi-task offloading tactic for the framework to deal with intensive offloading requests from various mobile devices. When receiving an offloading request, the master node in server-side determines whether this task should be offloaded or not and which VM should handle this task, so that the overall execution time and energy consumption are optimized. We implement this framework and evaluate it by comparing the execution time, energy consumption and CPU utilization rate among three execution modes with three applications. We also conduct experiments of the multi-task offloading tactic in simulation environment. Experimental results indicate that this framework effectively reduces energy consumption and boosts performance for compute-intensive tasks, and the multi-task offloading tactic is valid for intensive offloading requests.},
  archive      = {J_TCC},
  author       = {Junyu Lu and Qiang Li and Bing Guo and Jie Li and Yan Shen and Gongliang Li and Hong Su},
  doi          = {10.1109/TCC.2019.2952346},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {187-201},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A multi-task oriented framework for mobile computation offloading},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-objective optimization scheme for job scheduling in
sustainable cloud data centers. <em>TCC</em>, <em>10</em>(1), 172–186.
(<a href="https://doi.org/10.1109/TCC.2019.2950002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a number of years, due to an exponential increase in the demand for an eco-friendly environment, there has been a rapid increase in the green city revolution across the globe. Subsequently, load shifting of major energy consumers from conventional power grids to renewable energy sources (RES) has become inevitable. Towards this end, cloud data centers (DCs) have emerged as significant consumers of energy that solely rely on power grids to fuel their day-to-day operations. Nevertheless, their energy consumption has increased significantly which in turn has substantially raised the global carbon footprint rate. These challenges can be best addressed by the judicious utilization of RES which have well established advantages like reduced operational costs and carbon emissions. Keeping in view of the above facts, the ultimate goal of the proposed work is to design a comprehensive workload classification; and job scheduling and Vitual machine placement architecture for cloud DCs powered by RES and power grids. For this, a multi-objective optimization scheme is proposed which operates in two phases. In phase I, a random forest-based wrapper scheme known as Boruta, is used for relevant feature set selection for the incoming workload. This is followed by classification of the workload using a locality sensitive hashing-based support vector machines approach. In phase II, a multi-objective optimization problem for job scheduling and VM placement is formulated with respect to parameters such as service level agreement (SLA), energy cost, carbon footprint rate (CFR), and availability of RES. It is further solved using an enhanced heuristic approach based on a greedy strategy. Our experimental evaluations show an average improvement of approximately 31 percent in energy utilization, 28 percent in energy cost, and 36 percent in CFR, with a slight degradation in SLA assurance (about 2 percent) compared with the existing schemes.},
  archive      = {J_TCC},
  author       = {Kuljeet Kaur and Sahil Garg and Gagangeet Singh Aujla and Neeraj Kumar and Albert Y. Zomaya},
  doi          = {10.1109/TCC.2019.2950002},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {172-186},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A multi-objective optimization scheme for job scheduling in sustainable cloud data centers},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A many-to-many demand and response hybrid game method for
cloud environments. <em>TCC</em>, <em>10</em>(1), 158–171. (<a
href="https://doi.org/10.1109/TCC.2019.2956134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we design a service mechanism for profits optimization between multiple cloud providers and multiple cloud customers ( many-to-many ). We explore this problem from the perspective of game theory but take a different approach compared with existing cloud resource pricing game methods. First, we regard the relationships among multiple cloud customers as an evolutionary game, and formulate the competitions among the multiple cloud providers as a noncooperative game. Eventually, we form a hybrid game model in which the strategy of each customer and each cloud provider is affected not only by the other side but also by customers or cloud providers other than themselves. Second, based on the hybrid game model, we simulate the bargaining process between cloud providers and customers by controlling supply and demand allocation, and try to ultimately achieve a balanced supply and demand state, i.e., a win-win situation. For each cloud customer and provider, we design a utility function. A customer’s utility involves net profits and the cloud providers’ bidding strategies, and a cloud provider’s utility involves net profits and the cloud customers’ demand strategies. Both sides attempt to maximize their own profits under the influences of each other. We prove that our proposed strategies enable each of the two games to converge to their own equilibrium. Finally, the strategies of cloud customers and providers can be implemented through an iterative proximal algorithm ( $\mathcal {IPA}$ ) and a distributed iterative algorithm ( $\mathcal {DIA}$ ). The experimental results validate our methods and show that the proposed method can benefit both multiple cloud providers and customers.},
  archive      = {J_TCC},
  author       = {Gang Liu and Zheng Xiao and Anthony Theodore Chronopoulos and Chubo Liu and Zhuo Tang},
  doi          = {10.1109/TCC.2019.2956134},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {158-171},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A many-to-many demand and response hybrid game method for cloud environments},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A learning-based data placement framework for low latency in
data center networks. <em>TCC</em>, <em>10</em>(1), 146–157. (<a
href="https://doi.org/10.1109/TCC.2019.2940953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-latency data service is an increasingly critical challenge for data center applications. In modern distributed storage systems, proper data placement helps reduce the data movement delay, which can contribute to the service latency reduction tremendously. Existing data placement solutions have often assumed the prior distribution of data requests or discovered it via trace analysis. However, data placement is a difficult online decision-making problem faced with dynamic network conditions and time-varying user request patterns. The conventional static model-based solutions are less effective to handle the dynamic system. With an overall consideration of data movement and analytical latency, we develop a reinforcement learning-based framework DataBot+, automatically learning the optimal placement policies. DataBot+ adopts neural networks, trained with a variant of $Q$ -learning, whose input is the real-time data flow measurements and whose output is a value function estimating the near-future latency. For instantaneous decision making, DataBot+ is decoupled into two asynchronous production and training components, ensuring that the training delay will not introduce extra overheads to handle the data flows. Evaluation results driven by real-world traces demonstrate the effectiveness of our design.},
  archive      = {J_TCC},
  author       = {Kaiyang Liu and Jun Peng and Jingrong Wang and Boyang Yu and Zhuofan Liao and Zhiwu Huang and Jianping Pan},
  doi          = {10.1109/TCC.2019.2940953},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {146-157},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A learning-based data placement framework for low latency in data center networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A docker container anomaly monitoring system based on
optimized isolation forest. <em>TCC</em>, <em>10</em>(1), 134–145. (<a
href="https://doi.org/10.1109/TCC.2019.2935724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Container-based virtualization has gradually become a main solution in today‘s cloud computing environments. Detecting and analyzing anomaly in containers present a major challenge for cloud vendors and users. This paper proposes an online container anomaly detection system by monitoring and analyzing multidimensional resource metrics of the containers based on the optimized isolation forest algorithm. To improve the detection accuracy, it assigns each resource metric a weight and changes the random feature selection in the isolation forest algorithm to the weighted feature selection according to the resource bias of the container. In addition, it can identify abnormal resource metrics and automatically adjust the monitoring period to reduce the monitoring delay and system overhead. Moreover, it can locate the cause of the anomalies via analyzing and exploring the container log. The experimental results demonstrate the performance and efficiency of the system on detecting the typical anomalies in containers in both simulated and real cloud environments.},
  archive      = {J_TCC},
  author       = {Zhuping Zou and Yulai Xie and Kai Huang and Gongming Xu and Dan Feng and Darrell Long},
  doi          = {10.1109/TCC.2019.2935724},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {134-145},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A docker container anomaly monitoring system based on optimized isolation forest},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cloud-edge interoperability for demand response-enabled fast
frequency response service provision. <em>TCC</em>, <em>10</em>(1),
123–133. (<a href="https://doi.org/10.1109/TCC.2021.3117717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The massive penetration of Renewables into the energy mix and the existence of IoT-enabled Distributed Energy Resources (DERs) in the emerging smart grid, while a blessing towards de-carbonization, increase considerably the operations and planning functions of the grid. Cloud processing of IoT/DER data facilitates the deployment of various Demand-Response (DR) and other DER asset scenarios, the organization of distribution grids into Local Energy Markets (LEM) and the efficient computation of load forecasting and power flow. Cloud computing enables a plethora of service provisions to the grid including frequency response. The decentralized nature of DERs at the edge of the distribution grid requires nodal approaches for the computation of power grid congestion constraints and power flow solutions. We present here a cloud-edge continuum approach, anchored on the new generation of communications infrastructure, which expedites the computation time of the load and DER forecasting and optimal power flow calculations. The proposed approach allows the LEM operator to respond to Fast Frequency Response service procurement signals issued by the balancing authority requiring even sub-second latency for service settlement. The proposed cloud-edge architecture has been tested on the IEEE European Low Voltage Benchmark model and provides scalability and elasticity for various DR/DER configurations.},
  archive      = {J_TCC},
  author       = {Athanasios Bachoumis and Nikos Andriopoulos and Konstantinos Plakas and Aristeidis Magklaras and Panayiotis Alefragis and Georgios Goulas and Alexios Birbas and Alex Papalexopoulos},
  doi          = {10.1109/TCC.2021.3117717},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {123-133},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cloud-edge interoperability for demand response-enabled fast frequency response service provision},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic price-enabled strategic energy management scheme in
cloud-enabled smart grid. <em>TCC</em>, <em>10</em>(1), 111–122. (<a
href="https://doi.org/10.1109/TCC.2021.3118637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, the problem of high-quality energy service provisioning in the presence of competitive prosumers and micro-grids in cloud-enabled smart grid is studied. Oligopolistic prosumers behave non-cooperatively and store the excess generated energy for future use, which increases the load on the main grid and degrades the performance of the smart grid. To address this issue, we propose a dynamic cloud-based pricing scheme, named SmartPrice, to enforce cooperation among the prosumers for ensuring high quality of service provided by the micro-grids. In SmartPrice, using cloud infrastructure, each micro-grid calculates a reward factor for each prosumer based on his/her behavior to enforce cooperation among them. We model the interaction between each micro-grid and the prosumers using a single-leader-multiple-followers Stackelberg game, where the micro-grids and the prosumers act as the leaders and the followers, respectively. Each micro-grid determines the unit energy price to be charged/paid and each prosumer determines the quantity of excess energy to be supplied for ensuring high revenue. Thus, SmartPrice enforces cooperation among the micro-grids and prosumers. Additionally, using SmartPrice, the price for unit energy charged from the prosumers reduces by 23.37- $35.63\%$ , thereby ensuring high revenue and the number of prosumers served by the micro-grids increases by 38.19- $53.14\%$ .},
  archive      = {J_TCC},
  author       = {Ayan Mondal and Sudip Misra and Aishwariya Chakraborty},
  doi          = {10.1109/TCC.2021.3118637},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {111-122},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Dynamic price-enabled strategic energy management scheme in cloud-enabled smart grid},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evolutionary game based demand response bidding strategy for
end-users using q-learning and compound differential evolution.
<em>TCC</em>, <em>10</em>(1), 97–110. (<a
href="https://doi.org/10.1109/TCC.2021.3117956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Load aggregators (LAs) play a key role in fully tapping the demand response (DR) resources of small and medium-sized end-users to enable a more flexible power grid. In the ancillary service market, the LA can provide DR to the system by aggregating the resources of its users. In response to the issued DR program, end-users offer to provide DR resources. To help optimize the user bidding strategy, an evolutionary game model is presented here in view of the bounded rationality of bidders. A combined Q-learning and compound differential evolution (CDE) algorithm is proposed to deal with the problems of incomplete information and uncertainties in the opponents’ decision-making, and prevent the evolutionary stable strategy (ESS) from falling into a local optimum. Moreover, a cloud-computing-based framework is designed and agent servers are introduced to protect data privacy. Numerical results show that by adopting the proposed algorithm, the user&#39;s bidding price keeps slightly lower than the opponents’ price which guarantees its revenue remains on a high level. This indicates that the proposed algorithm has good adaptability for addressing incomplete information and uncertainties in opponents’ decision-making.},
  archive      = {J_TCC},
  author       = {Ouzhu Han and Tao Ding and Linquan Bai and Yuankang He and Fangxing Li and Mohammad Shahidehpour},
  doi          = {10.1109/TCC.2021.3117956},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {97-110},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Evolutionary game based demand response bidding strategy for end-users using Q-learning and compound differential evolution},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Demand response as a service: Clearing multiple
distribution-level markets. <em>TCC</em>, <em>10</em>(1), 82–96. (<a
href="https://doi.org/10.1109/TCC.2021.3117598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The uncertain and non-dispatchable nature of renewable energy sources renders Demand Response (DR) a critical component of modern electricity distribution systems. Demand Response (DR) service provision takes place via aggregators and special distribution-level markets (e.g., flexibility markets), where small, distributed DR resources, such as building energy management systems, electric vehicle charging stations, micro-generation and storage, connected to the low-voltage distribution grid, offer DR services. In such systems, energy balancing (and thus, also DR decisions) have to be made close to real-time. Thus, market clearing algorithms for DR service provision must fulfill several requirements related to the efficiency of their operation. More specifically, a DR market clearing algorithm needs to be optimal in terms of cost-efficiency, scalable in terms of number of assets and locations, and able to satisfy real-time constraints. In order to cope with these challenges, this article presents a distributed DR market clearing algorithm based on Lagrangian decomposition, combined with an optimal cloud resource allocation algorithm for assigning the required computation power. A heuristic algorithm is also presented, able to achieve a near-optimal solution, within negligible computational time. Simulations, performed on a testbed, demonstrate the computational burden introduced by various DR models, as well as the heuristic algorithm&#39;s near-optimal performance. The resource allocation algorithm is able to service multiple DR requests (e.g., in multiple distribution networks), and minimize the cost of computational resources while respecting the execution time constraints of each request. This enables third parties to offer cost-efficient and competitive DR operation as a service.},
  archive      = {J_TCC},
  author       = {Georgios Tsaousoglou and Polyzois Soumplis and Nikolaos Efthymiopoulos and Konstantinos Steriotis and Aristotelis Kretsis and Prodromos Makris and Panagiotis Kokkinos and Emmanouel Varvarigos},
  doi          = {10.1109/TCC.2021.3117598},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {82-96},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Demand response as a service: Clearing multiple distribution-level markets},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cloud computing based demand response management using deep
reinforcement learning. <em>TCC</em>, <em>10</em>(1), 72–81. (<a
href="https://doi.org/10.1109/TCC.2021.3117604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Demand response is an effective way for ensuring safety and stabilization of power grid by maintaining the balance between the supply and the demand of power grid, and this article focuses on using electric water heaters for demand response. In addition to considering comfort and price factors as did in previous works, this article considers the overshoot temperature and its influence on demand response. First, a theoretical model of the heating and cooling processes of the electric water heater is established; second, the demand response process using electric water heaters is analyzed, including the influences of the physical parameters and the settings of electric water heaters on the demand response process; third, a model is established considering the demand response requirement, the comfort of owners of electric water heaters, and the electricity price, simultaneously; fourth, an optimization method based on deep reinforcement learning is proposed for demand response using electric water heaters. Meanwhile, the influence of parameters on the results of demand response is discussed in details. Experimental results show the effectiveness of the proposed method.},
  archive      = {J_TCC},
  author       = {Chunhe Song and Guangjie Han and Peng Zeng},
  doi          = {10.1109/TCC.2021.3117604},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {72-81},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cloud computing based demand response management using deep reinforcement learning},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid cloud and edge control strategy for demand
responses using deep reinforcement learning and transfer learning.
<em>TCC</em>, <em>10</em>(1), 56–71. (<a
href="https://doi.org/10.1109/TCC.2021.3117580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A number of electric devices in buildings can be considered as important demand response (DR) resources, for instance, the battery energy storage system (BESS) and the heat, ventilation, and air conditioning (HVAC) systems. The conventional model-based DR methods rely on efficient on-demand computing resources. However, the current buildings suffer from the high cost of computing resources and lack a cost-effective automation system, which becomes the main obstacle to the popularization and implementation of the DR program. Therefore, in this paper, we present a hybrid cloud and edge control strategy for BESS and HVAC based on deep reinforcement learning (DRL). On the cloud infrastructure, the agent learns the control strategy online based on the proposed continuous dueling deep Q-learning (C-DDQN) algorithm, and the learned strategy is distributed to the edge devices for execution. Under this framework, the data-intensive application of cloud computing in real-time DR shows advantages in high processing speed, unlimited data aggregation, fault-tolerant, cost-saving, security, and confidentiality. However, if every controller is trained from the beginning, the cloud resources are wasted to a large extent. Therefore, we propose a transfer deep reinforcement learning methodology to transfer the control strategies between BESS and HVAC units. The transfer learning is realized based on fine-tuning and the proposed Evolving Domain Adaptation Network (EDAN). In case studies, it is verified that the proposed transfer deep reinforcement learning algorithm shows better convergence and learning capability compared with not applying transfer learning technologies. Compared with the conventional model-based method, the proposed methodology speeds up the decision-making time by 10 5 times.},
  archive      = {J_TCC},
  author       = {Yuechuan Tao and Jing Qiu and Shuying Lai},
  doi          = {10.1109/TCC.2021.3117580},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {56-71},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A hybrid cloud and edge control strategy for demand responses using deep reinforcement learning and transfer learning},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Demand response control of smart buildings integrated with
security interconnection. <em>TCC</em>, <em>10</em>(1), 43–55. (<a
href="https://doi.org/10.1109/TCC.2021.3117592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Demand response (DR) of power grids services an increasing key component of smart grids. This article is devoted to the DR active power control for smart buildings based on a cloud platform for security interconnection with power grids. The objective of the DR control is to cope with the intermittent renewable power injection and power fluctuation of the tie-line between the building and the distribution grid. The renewable energy power includes photovoltaic panel and on-site wind power generators on the building roof, and the flexible loads includes electric vehicles (EVs) connected to the building, and thermostatically controlled loads (TCLs) in the building. We propose two fair and flexible indexes, comfortable levels for TCLs and the charging priority for EVs, and design two kinds of DR control algorithm, i.e., centralized load reduction control algorithm and distributed load following control algorithm, to coordinate the power balance of the building while maintain the grid exchange power is distributed in a probabilistic security controllable interval such that the building turns out to be a grid-friendly building. The case study is tested on a smart building with random renewable power injection and flowing EVs, civil electric water heaters (EWHs) thermostatically controlled loads and the simulations results shows the effectiveness of the proposed DR control algorithms.},
  archive      = {J_TCC},
  author       = {Jianqiang Hu and Zhixun Zhang and Jianquan Lu and Jie Yu and Jinde Cao},
  doi          = {10.1109/TCC.2021.3117592},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {43-55},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Demand response control of smart buildings integrated with security interconnection},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal incentive strategy in cloud-edge integrated demand
response framework for residential air conditioning loads. <em>TCC</em>,
<em>10</em>(1), 31–42. (<a
href="https://doi.org/10.1109/TCC.2021.3118597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the residential demand response area, currently the incentive-based method (e.g., direct load control, DLC) may impair users’ comfort and autonomy, while the price-based method can hardly guarantee users’ engagements. This paper proposes an edge-cloud integrated demand response framework to achieve an effect-predictable residential demand response without harming users’ benefits. First, we combine the cloud-computing resource (cloud) and the home-installed smart thermostats (edges) to formulate an efficient, cost-effective, and data-secured infrastructure to implement the demand response program. Then, we model the demand response problem between the load aggregator and its served residential users as a bi-level optimization problem, and the key is for the load aggregator to find the optimal incentive strategy. To solve this problem, we introduce an RL algorithm, i.e., Continuous Action Reinforcement Learning Automata, to quickly obtain the optimal incentive strategy under an incomplete information scenario. Simulation results based on 136 real-world residential users in Austin area demonstrate that the proposed CEI-DR framework can increase the social welfare by about $8.6/h compared to the traditional DLC method during a normal DR event.},
  archive      = {J_TCC},
  author       = {Qiangang Jia and Sijie Chen and Zheng Yan and Yiyan Li},
  doi          = {10.1109/TCC.2021.3118597},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {31-42},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Optimal incentive strategy in cloud-edge integrated demand response framework for residential air conditioning loads},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal energy trading with demand responses in cloud
computing enabled virtual power plant in smart grids. <em>TCC</em>,
<em>10</em>(1), 17–30. (<a
href="https://doi.org/10.1109/TCC.2021.3118563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing penetration of renewable energy sources and electric vehicles (EVs) poses a significant challenge for the power grid operator in terms of increasing peak load and power quality reduction. Moreover, there is a growing demand for fast charging services in smart grids. Addressing the growing demand from fast charging services is challenging. To overcome this challenge, in this article, we propose a new computational architecture combining energy trading and demand responses based on cloud computing for managing virtual power plants (VPPs) in smart grids. In the proposed system, EVs can be charged at high charging rates without affecting the operation of the power grid by purchasing energy through the energy trading platform in the cloud. In addition, users with storage devices can sell energy surplus to the market. On the one hand, the energy trading platform can be regarded as an internal market of the VPP that aims to maximize its revenue. The interest of the EV owners, on the other hand, is to minimize the cost for charging. Therefore, we model the interactions between the EV owners and the VPP as a non-cooperative game. To search for the Nash equilibrium (NE) of the game, we design an algorithm and then analyze its computational complexity and communication overhead. We utilize real data from the California Independent System Operator (CAISO) to evaluate the performance of the proposed algorithm. Our results illustrate that the users with only storage devices can obtain nearly $200\%$ higher revenue on average by participating in the proposed internal market. Moreover, users with only EVs can reduce their charging costs by nearly $50\%$ in average. Users with both EVs and storage devices can reduce the charging costs even further by approximately $120\%$ where the users get profit by utilizing the internal market.},
  archive      = {J_TCC},
  author       = {Hwei-Ming Chung and Sabita Maharjan and Yan Zhang and Frank Eliassen and Kai Strunz},
  doi          = {10.1109/TCC.2021.3118563},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {17-30},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Optimal energy trading with demand responses in cloud computing enabled virtual power plant in smart grids},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-sufficient participation in cloud-based demand
response. <em>TCC</em>, <em>10</em>(1), 4–16. (<a
href="https://doi.org/10.1109/TCC.2021.3118212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Demand response and distributed energy storage present significant opportunities for improving the reliability of power systems under increasing threats of natural disasters and uncertainty in sources and loads. In addition, the distributed control and resources which these technologies bring to the customer open promising potential of resilience for individual customers and their critical loads. This article proposes a unification of the problems of system reliability and individual resilience for critical loads through a cloud-based framework for control and optimization relying on centralized decision-making and distributed emergency control. A formulation is presented for optimal power flow with demand response and storage scheduling including N-1 contingencies and guarantees of serving critical loads even when they are isolated in a contingency. The local platform for control and communication under demand response and islanded emergency operations is discussed. A case study with simulation results for a 24 hour period demonstrates the advantages of this framework for the overall power system in terms of operation cost and for an individual customer in terms of serving a critical load in an emergency scenario.},
  archive      = {J_TCC},
  author       = {David Sehloff and Maitreyee Marathe and Ashray Manur and Giri Venkataramanan},
  doi          = {10.1109/TCC.2021.3118212},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {4-16},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Self-sufficient participation in cloud-based demand response},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guest editorial: Special section on demand response
applications of cloud computing technologies. <em>TCC</em>,
<em>10</em>(1), 1–3. (<a
href="https://doi.org/10.1109/TCC.2021.3119217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special section focus on demand response applications in cloud computing technologie.s The use of distributed energy resources for self-generation and self-consumption along with Information and Communications Technologies and the Internet of Things is rapidly increasing the ability of the consumers and prosumers to actively engage with the electric energy system. Sustained consumer and prosumer engagement in demand response programs has been identified as a key factor in future electric energy systems, especially with a high penetration of renewable energy sources. This engagement has allowed demand-side resources to play a larger role in energy and reserve markets, whether by generating, storing or participating in demand response programs through increased flexibility, towards the consumer-driven energy transition. However, in real life, there is still a long way to go until demand response solutions take off and become entirely integrated into the daily life of the consumers, thus utilizing their full potential. Stronger engagement of consumers and prosumers is needed, as well as more flexibility services for system operation, benefiting Smart Grid developments.},
  archive      = {J_TCC},
  author       = {João P. S. Catalão and Young-Jin Kim and Jamshid Aghaei and Joel J. P. C. Rodrigues and Miadreza Shafie-Khah},
  doi          = {10.1109/TCC.2021.3119217},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {1-3},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Guest editorial: Special section on demand response applications of cloud computing technologies},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
