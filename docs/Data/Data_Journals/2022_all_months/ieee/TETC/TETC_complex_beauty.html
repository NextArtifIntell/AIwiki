<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TETC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tetc---164">TETC - 164</h2>
<ul>
<li><details>
<summary>
(2022). Fuzzy clustered federated learning algorithm for solar power
generation forecasting. <em>TETC</em>, <em>10</em>(4), 2092–2098. (<a
href="https://doi.org/10.1109/TETC.2022.3142886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a promising technique to construct a solar power generation forecasting model based on data collected from local generators. However, a set of local generators (i.e., cluster) for FL should be carefully defined to construct a high-accuracy forecasting model. Herein, we propose a fuzzy clustered FL algorithm (FCFLA) where each local generator can be included in more than one cluster. In FCFLA, a local generator has its own membership degree representing its sense of belonging to a specific cluster. Based on this membership degree, FCFLA can generate the high-accuracy forecasting model by catching different characteristics of the data of local generators while addressing the training data shortage problem. Evaluation results demonstrate that FCFLA has the fastest convergence time in achieving the desired accuracy.},
  archive      = {J_TETC},
  author       = {Eungeun Yoo and Haneul Ko and Sangheon Pack},
  doi          = {10.1109/TETC.2022.3142886},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {2092-2098},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Fuzzy clustered federated learning algorithm for solar power generation forecasting},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reliability risks due to faults affecting selectors of
ReRAMs and possible solutions. <em>TETC</em>, <em>10</em>(4), 2086–2091.
(<a href="https://doi.org/10.1109/TETC.2021.3114961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resistive Random Access Memories (ReRAMs) are considered amongst the most promising candidates to replace silicon-based memories in the near future, when huge amount of data have to be stored. However, ReRAMs suffer from reliability issues associated to faults affecting both their resistive elements and their selectors. While some solutions have been presented in literature to detect, or tolerate, faults affecting the resistive element, so far no solution has been yet proposed to detect, or tolerate, faults affecting the selector, albeit these faults have been proven to be likely and possibly compromising the reliability of ReRAMs. In this paper, we analyze the effects of the most likely faults (i.e., shorts and opens) possibly affecting the selectors of ReRAM cells on the operation of the crossbar memory array. We show that a selector failing as a short can give rise to numerous errors on the ReRAM crossbar array. As an example, for the case of a crossbar array of 128x128 cells, we show that a single selector failing as a short may cause up to 64 errors in memory cells sharing the same word line of the cell containing the faulty selector. Such a large number of errors exceeds the correction capability of ECCs usually adopted for ReRAM arrays. Therefore, new solutions enabling to tolerate the large number of errors caused by selector faults in crossbar arrays are needed, to enable the use of this kind of promising memories in a reliable way. We then propose a low-cost approach to detect short faults affecting selectors of ReRAMs and to identify the bitline containing the cell whose selector is faulty. Such a bitline can then be properly deactivated and replaced by a spare one, in order to guarantee the memory correct operation in the field.},
  archive      = {J_TETC},
  author       = {Martin Omaña and Sejuti Bardhan and Cecilia Metra},
  doi          = {10.1109/TETC.2021.3114961},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {2086-2091},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Reliability risks due to faults affecting selectors of ReRAMs and possible solutions},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PLAM: A posit logarithm-approximate multiplier.
<em>TETC</em>, <em>10</em>(4), 2079–2085. (<a
href="https://doi.org/10.1109/TETC.2021.3109127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Posit™ Number System was introduced in 2017 as a replacement for floating-point numbers. Since then, the community has explored its application in several areas, such as deep learning, and produced some unit designs which are still far from being competitive with their floating-point counterparts. This article proposes a Posit Logarithm-Approximate Multiplication (PLAM) scheme to significantly reduce the complexity of posit multipliers, one of the most power-hungry arithmetic units. The impact of this approach is evaluated in deep neural network inference, where there are no significant accuracy drops. Compared with state-of-the-art posit multipliers, experiments show that the proposed technique reduces the area, power, and delay of 32-bit hardware multipliers up to 72.86%, 81.79%, and 17.01%, respectively.},
  archive      = {J_TETC},
  author       = {Raul Murillo and Alberto A. Del Barrio and Guillermo Botella and Min Soo Kim and HyunJin Kim and Nader Bagherzadeh},
  doi          = {10.1109/TETC.2021.3109127},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {2079-2085},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {PLAM: A posit logarithm-approximate multiplier},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ACBAM-accuracy-configurable sign inclusive broken array
booth multiplier design. <em>TETC</em>, <em>10</em>(4), 2072–2078. (<a
href="https://doi.org/10.1109/TETC.2021.3107509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate computing technique has been adopted in recent years to develop low power design solutions targeted towards error-tolerant applications. Since the accuracy requirements of an application can vary dynamically at run-time, there is a justified need to design reconfigurable approximate circuits with varying power requirements proportional to computational accuracy. In this article, we have proposed a novel accuracy reconfigurable approximate booth multiplier circuit. The design involves partial error correction through the addition of sign bits in a broken array multiplier. The design space of sign inclusive 16-bit broken array multipliers is thoroughly analyzed to include only non-redundant accuracy modes. The horizontal and vertical breaks introduced in the booth multiplier circuit are controlled using external control signals stored in a ROM. The proposed reconfigurable multiplier achieves significant power savings compared to accurate multiplier circuit and state-of-the-art accuracy configurable multiplier designs.},
  archive      = {J_TETC},
  author       = {Avishek Sinha Roy and Hardik Agrawal and Anindya Sundar Dhar},
  doi          = {10.1109/TETC.2021.3107509},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {2072-2078},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {ACBAM-accuracy-configurable sign inclusive broken array booth multiplier design},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CRAM-seq: Accelerating RNA-seq abundance quantification
using computational RAM. <em>TETC</em>, <em>10</em>(4), 2055–2071. (<a
href="https://doi.org/10.1109/TETC.2022.3153613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RNA Sequence (RNA-Seq) abundance quantification is an important application in different fields of genomic studies, e.g., analysis offunctionally similar genes in a biological sample. This application depends on the availability of high volume of sequence data for high accuracy abundance estimation, which is made possible by next generation sequencing platforms. Large scale data processing requirements of this quantification application push conventional computing systems to their limits due to excessive data movement required between processing and memory elements. Processing-In-memory presents a viable solution to this drawback, through in-situ processing of the genomic data. In this paper, we present CRAM-Seq, an accelerator for RNA-Seq abundance quantification based on Computational RAM (CRAM) – an in-memory processing substrate capable of high degree of parallel processing with very low energy consumption. Through hardware/software co-design, we demonstrate that CRAM-Seq outperforms a commonly used state-of-the-art software abundance quantification algorithm, Kallisto – in terms of throughput and energy efficiency, while being highly scalable.},
  archive      = {J_TETC},
  author       = {Zamshed I. Chowdhury and S. Karen Khatamifard and Salonik Resch and Hüsrev Cılasun and Zhengyang Zhao and Masoud Zabihi and Meisam Razaviyayn and Jian-Ping Wang and Sachin S. Sapatnekar and Ulya R. Karpuzcu},
  doi          = {10.1109/TETC.2022.3153613},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {2055-2071},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {CRAM-seq: Accelerating RNA-seq abundance quantification using computational RAM},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Child-computer interaction with mobile devices: Recent
works, new dataset, and age detection. <em>TETC</em>, <em>10</em>(4),
2042–2054. (<a href="https://doi.org/10.1109/TETC.2022.3150836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article provides an overview of recent research in Child-Computer Interaction with mobile devices and describe our framework ChildCI intended for: i) overcoming the lack of large-scale publicly available databases in the area, ii) generating a better understanding of the cognitive and neuromotor development of children along time, contrary to most previous studies in the literature focused on a single-session acquisition, and iii) enabling new applications in e-Learning and e-Health through the acquisition of additional information such as the school grades and children&#39;s disorders, among others. Our framework includes a new mobile application, specific data acquisition protocols, and a first release of the ChildCI dataset 1 (ChildCIdb v1), which is planned to be extended yearly to enable longitudinal studies. In our framework children interact with a tablet device, using both a pen stylus and the finger, performing different tasks that require different levels of neuromotor and cognitive skills. ChildCIdb is the first database in the literature that comprises more than 400 children from 18 months to 8 years old, considering therefore the first three development stages of the Piaget&#39;s theory. In addition, and as a demonstration of the potential of the ChildCI framework, we include experimental results for one of the many applications enabled by ChildCIdb: children age detection based on device interaction.},
  archive      = {J_TETC},
  author       = {Ruben Tolosana and Juan Carlos Ruiz-Garcia and Ruben Vera-Rodriguez and Jaime Herreros-Rodriguez and Sergio Romero-Tapiador and Aythami Morales and Julian Fierrez},
  doi          = {10.1109/TETC.2022.3150836},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {2042-2054},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Child-computer interaction with mobile devices: Recent works, new dataset, and age detection},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SATAM: A SAT attack resistant active metering against IC
overbuilding. <em>TETC</em>, <em>10</em>(4), 2025–2041. (<a
href="https://doi.org/10.1109/TETC.2022.3148244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the horizontal semiconductor business model, a foundry can tamper or overbuild the integrated circuits (ICs) while the IC owner knows nothing about it. Active IC metering scheme has been proposed to solve this problem by enabling IC owners to uniquely identify each manufactured chip. We propose a new active metering scheme, SATAM. In the scheme, a new cell of switchable scannable flip-flop (WFF) is introduced to be inserted in the non-critical paths or replace some original scan cells. Without a correct key on these WFFs, the synchronization status of the original design is violated and hence the circuit logic is locked (obfuscated). The scan design is also obfuscated by the introduction of WFFs, which helps the locking scheme to resist the typical SAT attack and other existing attacks on logic locking. To enable each fabricated chip to have a unique key to unlock, a via-based physical unclonable function (PUF) is adopted due to its perfect reliability. A new post-processing method is proposed to improve its uniqueness and randomness. Also, we propose to modify the original finite state machine (FSM) so that the PUF response can only be retrieved by the chip designer in a secure way. The experimental results show that the locking scheme based on WFFs can well resist the SAT attack and other existing attacks on logic obfuscation. The PUF design can achieve satisfactory uniqueness and randomness while incurring lower overhead. The FSM-based retrieval scheme can secure the PUF response by slightly modifying the original FSM. The overhead due to the overall metering method is smaller than most of the existing metering methods while it can resist typical attacks.},
  archive      = {J_TETC},
  author       = {Aijiao Cui and Zhen Weng and Hui Zhang and Gang Qu and Huawei Li},
  doi          = {10.1109/TETC.2022.3148244},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {2025-2041},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {SATAM: A SAT attack resistant active metering against IC overbuilding},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning assisted security analysis of
5G-network-connected systems. <em>TETC</em>, <em>10</em>(4), 2006–2024.
(<a href="https://doi.org/10.1109/TETC.2022.3147192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The core network architecture of telecommunication systems has undergone a paradigm shift in the fifth-generation (5G) networks. 5G networks have transitioned to software-defined infrastructures, thereby reducing their dependence on hardware-based network functions. New technologies, like network function virtualization and software-defined networking, have been incorporated in the 5G core network (5GCN) architecture to enable this transition. This transition has significantly improved network efficiency, performance, and robustness. However, this has also made the core network more vulnerable, as software systems are generally easier to compromise than hardware systems. This article presents a comprehensive security analysis framework for the 5GCN. The novelty of this approach lies in the creation and analysis of attack graphs of the software-defined and virtualized 5GCN through machine learning. This analysis points to 119 novel possible exploits in the 5GCN. We demonstrate that these potential exploits of 5GCN vulnerabilities generate five novel attacks on the 5G Authentication and Key Agreement protocol. We combine the attacks at the network, protocol, and application layers to generate complex attack vectors. In a case study, we use these attack vectors to find four novel security loopholes in WhatsApp running on a 5G network.},
  archive      = {J_TETC},
  author       = {Tanujay Saha and Najwa Aaraj and Niraj K. Jha},
  doi          = {10.1109/TETC.2022.3147192},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {2006-2024},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Machine learning assisted security analysis of 5G-network-connected systems},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ultra high-speed polynomial multiplications for
lattice-based cryptography on FPGAs. <em>TETC</em>, <em>10</em>(4),
1993–2005. (<a href="https://doi.org/10.1109/TETC.2022.3144101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lattice-based cryptography (LBC) has emerged as the most viable substitutes to the classical cryptographic schemes as 5 out of 7 finalist schemes in the 3rd round of the NIST post-quantum cryptography (PQC) standardization process are lattice based in construction. This work explores novel architectural optimizations in the FPGA-based hardware implementation of polynomial multiplication, which is a bottleneck in every LBC construction. To target ultra-high throughput, both schoolbook polynomial multiplication (SPM) and number theoretic transform (NTT) are explored: a completely parallel architecture of an SPM is undertaken while for NTT, radix-2 and radix- &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$2^2$&lt;/tex-math&gt;&lt;/inline-formula&gt; multi-path delay commutator (MDC) based pipelined architectures are adopted. Our proposed high-speed SPM (HSPM) structure on latest Xilinx UltraScale+ FPGA is 5× faster than the state-of-the-art LBC designs. Whereas, the proposed high-speed NTT (HNTT) structure (i.e., R &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$2^2$&lt;/tex-math&gt;&lt;/inline-formula&gt; MDC) takes only 0.63 &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\mu$&lt;/tex-math&gt;&lt;/inline-formula&gt; s for the encryption, hence achieving the highest throughput of 408 Mbps. Moreover, all of the proposed designs achieve highest design efficiencies (i.e., throughput per slice (TPS)) in comparison to available LBC designs.},
  archive      = {J_TETC},
  author       = {Dur-e-Shahwar Kundi and Yuqing Zhang and Chenghua Wang and Ayesha Khalid and Máire O’Neill and Weiqiang Liu},
  doi          = {10.1109/TETC.2022.3144101},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1993-2005},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Ultra high-speed polynomial multiplications for lattice-based cryptography on FPGAs},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantifying the effects of anonymization techniques over
micro-databases. <em>TETC</em>, <em>10</em>(4), 1979–1992. (<a
href="https://doi.org/10.1109/TETC.2022.3141754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-databases are unique datasets that contain person-specific information about individuals. Preserving the privacy of such datasets has become a cause for serious concern since this massive repository of personalized data regularly gets published in the public domain. Sanitization mechanisms are specialized techniques that provide the required privacy guarantees to the published data. The work in this article establishes an efficient framework for quantitatively estimating the effectiveness of any privacy-preservation scheme which employs the anonymization principle. In our study, we have introduced an information-theoretic metric termed as Sanitization Degree &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$(\eta)$&lt;/tex-math&gt;&lt;/inline-formula&gt; which assigns a cumulative score in the range [0,1] for a generic anonymization process. The design of our proposed metric is based on the fundamental fact that any sanitization mechanism attempts to reduce the amount of correlated information within the database attributes while simultaneously preserving the utility of the original dataset. Furthermore, we have characterized the privacy-utility tradeoff associated with our model by establishing a working relationship between these two fundamental quantities. We have empirically computed the value of &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\eta$&lt;/tex-math&gt;&lt;/inline-formula&gt; for three popular anonymization models ( &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; -anonymity, &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$l$&lt;/tex-math&gt;&lt;/inline-formula&gt; -diversity, and &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$t$&lt;/tex-math&gt;&lt;/inline-formula&gt; -closeness) over a couple of real-life micro-databases, thereby demonstrating the practicability of our study.},
  archive      = {J_TETC},
  author       = {Debanjan Sadhya and Bodhi Chakraborty},
  doi          = {10.1109/TETC.2022.3141754},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1979-1992},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Quantifying the effects of anonymization techniques over micro-databases},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep recommendation with adversarial training.
<em>TETC</em>, <em>10</em>(4), 1966–1978. (<a
href="https://doi.org/10.1109/TETC.2022.3141422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems provide an effective solution to the information overload and have become a research hotspot in both industry and academia. Some existing work on implicit feedback has made great progress. However, there are still the following shortcomings: 1) existing pairwise ranking methods mostly sample unobserved items uniformly to obtain negative samples, which brings a biased solution and insufficient convergence; 2) the recommendation result comes from a complicated process, which makes it less interpretable. Therefore, we put forward a Deep Recommendation with Adversarial Training (DRAT) model, which provides users with personalized recommendations by utilizing an encoder-decoder structure and adversarial training. It consists of two components: one is feature learning module in which the encoder captures the text features of items from user-generated reviews, and the decoder reconstructs the text with the captured features; the other is rating prediction module which utilizes adversarial training to generate personalized negative samples tackling the drawbacks of uniform negative sampling. Extensive experiments on five real-world datasets show that our proposed model significantly outperforms the baseline methods.},
  archive      = {J_TETC},
  author       = {Chenyan Zhang and Jing Li and Jia Wu and Donghua Liu and Jun Chang and Rong Gao},
  doi          = {10.1109/TETC.2022.3141422},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1966-1978},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Deep recommendation with adversarial training},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tight bound on NewHope failure probability. <em>TETC</em>,
<em>10</em>(4), 1955–1965. (<a
href="https://doi.org/10.1109/TETC.2021.3138951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NewHope Key Encapsulation Mechanism (KEM) has been presented at USENIX 2016 by Alkim et al. and was one of the lattice-based candidates to the post-quantum standardization initiated by the NIST. However, despite the relative simplicity of the protocol, the bound on the decapsulation failure probability resulting from the original analysis is not tight. In this work we refine this analysis to get a tight upper-bound on this probability which happens to be much lower than what was originally evaluated. As a consequence, we propose a set of alternative parameters, increasing the security and the compactness of the scheme. However using a smaller modulus prevent the use of a full NTT algorithm to perform multiplications of elements in dimension 512 or 1024. Nonetheless, similarly to previous works, we combine different multiplication algorithms and show that our new parameters are competitive on a constant time vectorized implementation. Our most compact parameters bring a speed-up of &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$17\%$&lt;/tex-math&gt;&lt;/inline-formula&gt; (resp. &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$11\%$&lt;/tex-math&gt;&lt;/inline-formula&gt; ) in performance but allow to gain more than &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$19\%$&lt;/tex-math&gt;&lt;/inline-formula&gt; over the bandwidth requirements and to increase the security of &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$10\%$&lt;/tex-math&gt;&lt;/inline-formula&gt; (resp. &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$7\%$&lt;/tex-math&gt;&lt;/inline-formula&gt; ) in dimension 512 (resp. 1024).},
  archive      = {J_TETC},
  author       = {Thomas Plantard and Arnaud Sipasseuth and Willy Susilo and Vincent Zucca},
  doi          = {10.1109/TETC.2021.3138951},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1955-1965},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Tight bound on NewHope failure probability},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy-minimized partial computation offloading for
delay-sensitive applications in heterogeneous edge networks.
<em>TETC</em>, <em>10</em>(4), 1941–1954. (<a
href="https://doi.org/10.1109/TETC.2021.3137980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Devices (MDs) support various delay-sensitive and computation-intensive applications. Yet they only have limited battery energy and computing resources, thereby failing to totally run all applications. A mobile edge computing (MEC) paradigm has been proposed to provide additional computation, storage, and networking resources for MDs. Servers in MEC are often deployed in both macro base stations (MBSs) and small base stations (SBSs). Thus, it is highly challenging to associate resource-limited MDs to them with high performance, and realize partial computation offloading among them for minimizing total energy consumption of an MEC system. To tackle these challenges, this work proposes a novel computation offloading approach for delay-sensitive applications with multiple separable tasks in hybrid networks including MDs, SBSs, and an MBS. To achieve it, this work formulates total energy consumption minimization as a constrained mixed integer non-linear program. To solve it, this work designs an improved meta-heuristic optimization algorithm called P article swarm optimization based on G enetic L earning (PGL), which integrates strong local search capacity of a particle swarm optimizer, and genetic operations of a genetic algorithm. PGL jointly optimizes task offloading among MDs, SBSs, and MBS, users’ connection to SBSs, MDs’ CPU speeds and transmission power, SBSs and MBS, and bandwidth allocation of available channels. Simulations with real-world data collected from Google cluster trace demonstrate that PGL significantly outperforms other existing methods in total energy consumption of an entire system.},
  archive      = {J_TETC},
  author       = {Jing Bi and Haitao Yuan and Kaiyi Zhang and MengChu Zhou},
  doi          = {10.1109/TETC.2021.3137980},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1941-1954},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Energy-minimized partial computation offloading for delay-sensitive applications in heterogeneous edge networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spin wave based approximate computing. <em>TETC</em>,
<em>10</em>(4), 1932–1940. (<a
href="https://doi.org/10.1109/TETC.2021.3136299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By their very nature Spin Waves (SWs) enable the realization of energy efficient circuits, as they propagate and interfere within waveguides without consuming noticeable energy. However, SW computing can be even more energy efficient by taking advantage of the approximate computing paradigm as many applications, e.g., multimedia and social media, are error-tolerant. In this paper, we propose an ultra-low energy Approximate Full Adder (AFA) and an Approximate 2-bit inputs Multiplier (AMUL). AFA consists of one Majority gate whereas AMUL is built by means of 3 AND gates. We validate the correct functionality of our proposal by means of micromagnetic simulations and evaluate AFA&#39;s figures of merit against state-of-the-art accurate SW, 7nm CMOS, Spin Hall Effect (SHE), Domain Wall Motion (DWM), accurate and approximate 45nm CMOS, Magnetic Tunnel Junction (MTJ), and Spin-CMOS FA implementations. Our results indicate that AFA consumes 38% and 6% less energy than state-of-the-art accurate SW and 7nm CMOS FA implementations, respectively. Moreover, it saves 56% and 20% energy when compared with accurate and approximate 45nm CMOS counterparts, respectively. Furthermore, it provides 2 orders of magnitude energy reduction when compared with accurate SHE, accurate and approximate DWM, MTJ, and Spin-CMOS, counterparts. In addition, it achieves the same error rate as approximate 45nm CMOS and Spin-CMOS FAs whereas it exhibits 50% less error rate than the approximate DWM FA. Last but not least, it outperforms its contenders in terms of area by saving at least 29% chip real-estate. AMUL is evaluated and compared with state-of-the-art SW and 16nm CMOS accurate and approximate designs. The evaluation results indicate that AMUL energy consumption is at least 2.8x and 2.6x smaller than the one of state-of-the-art SW and 16nm CMOS accurate and approximate designs, respectively. AMUL has an error rate of 25%, whereas the approximate CMOS multiplier has an error rate of 38%, and requires at least 64% less chip real-estate than the CMOS counterpart.},
  archive      = {J_TETC},
  author       = {Abdulqader Mahmoud and Frederic Vanderveken and Florin Ciubotaru and Christoph Adelmann and Said Hamdioui and Sorin Cotofana},
  doi          = {10.1109/TETC.2021.3136299},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1932-1940},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Spin wave based approximate computing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel technique for fingerprint based secure user
authentication. <em>TETC</em>, <em>10</em>(4), 1918–1931. (<a
href="https://doi.org/10.1109/TETC.2021.3130126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fingerprint biometrics are extremely popular among digital identity systems for authentication. Most fingerprint based authentication systems store unique minutiae points information of a user directly in the database as their template. An adversary can potentially attack the user template that could compromise information of minutiae points. It has been proven that a fingerprint can be reconstructed from minutiae points and hence there is an important requirement in fingerprint based biometric systems to provide protection and privacy. This article introduces a novel technique to provide enhanced protection for fingerprint based biometrics, taking into account the attributes of minutiae point, the ridge count between the neighbouring minutiae points and the local minutiae point structure. The proposed technique has been implemented and tested on a total of nine fingerprint databases of varying quality. In comparison to the other existing techniques under different scenarios, the proposed technique exhibits superior recognition performance. The proposed technique is a singular point independent and has shown high stability, robustness, and security.},
  archive      = {J_TETC},
  author       = {Syed Sadaf Ali and Vivek Singh Baghel and Iyyakutti Iyappan Ganapathi and Surya Prakash and Ngoc-Son Vu and Naoufel Werghi},
  doi          = {10.1109/TETC.2021.3130126},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1918-1931},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A novel technique for fingerprint based secure user authentication},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). UAV-aided information diffusion for vehicle-to-vehicle (V2V)
in disaster scenarios. <em>TETC</em>, <em>10</em>(4), 1909–1917. (<a
href="https://doi.org/10.1109/TETC.2021.3120551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To support safe driving and automatic operation, collecting and providing information are important in intelligent transport systems (ITSs). Information and communication technology utilization is needed even when disasters occur because it is possible to collect and analyze vehicle position information via roadside units and thus locate accidents and traffic jams. However, if the roadside units are damaged by disasters or power outages, these functions cannot be provided. Therefore, we consider ITS function assistance using unmanned aerial vehicles (UAVs), which can spread information without being affected by the availability of roads and traffic congestion. Simply flying UAVs without a plan will cause problems such as delays due to unnecessary movement and information transmission to areas where vehicles do not exist. Therefore, we propose a method for predicting information diffusion by vehicle-to-vehicle communication to control the flight trajectory of UAVs, with the aim of quickly spreading information to many vehicles. To predict information diffusion, we use a reaction-diffusion model, which is a partial differential equation, and construct a model that considers the effects of vehicle-to-vehicle communication. The effectiveness of the proposed method is evaluated through the simulation, and the results show that the method performs better than the comparison methods.},
  archive      = {J_TETC},
  author       = {Yuichi Kawamoto and Takuto Mitsuhashi and Nei Kato},
  doi          = {10.1109/TETC.2021.3120551},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1909-1917},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {UAV-aided information diffusion for vehicle-to-vehicle (V2V) in disaster scenarios},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A lightweight posit processing unit for RISC-v processors in
deep neural network applications. <em>TETC</em>, <em>10</em>(4),
1898–1908. (<a href="https://doi.org/10.1109/TETC.2021.3120538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, two groundbreaking factors are emerging in neural networks. First, there is the RISC-V open instruction set architecture (ISA) that allows a seamless implementation of custom instruction sets. Second, there are several novel formats for real number arithmetic. In this work, we combined these two key aspects using the very promising posit format, developing a light Posit Processing Unit (PPU-light). We present an extension of the base RISC-V ISA that allows the conversion between 8 or 16-bit posits and 32-bit IEEE Floats or fixed point formats in order to offer a compressed representation of real numbers with little-to-none accuracy degradation. Then we elaborate on the hardware and software toolchain integration of our PPU-light inside the Ariane RISC-V core and its toolchain, showing how little it impacts in terms of circuit complexity and power consumption. Indeed, only 0.36% of the circuit is devoted to the PPU-light while the full RISC-V core occupies the 33% of the overall circuit complexity. Finally we present the impact of our PPU-light on a deep neural network task, reporting speedups up to 10 on sample inference processing time.},
  archive      = {J_TETC},
  author       = {Marco Cococcioni and Federico Rossi and Emanuele Ruffaldi and Sergio Saponara},
  doi          = {10.1109/TETC.2021.3120538},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1898-1908},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A lightweight posit processing unit for RISC-V processors in deep neural network applications},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Thermal-aware standby-sparing technique on heterogeneous
real-time embedded systems. <em>TETC</em>, <em>10</em>(4), 1883–1897.
(<a href="https://doi.org/10.1109/TETC.2021.3120084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low power consumption, real-time computing, and high reliability are three key requirements/design objectives of real-time embedded systems. The standby-sparing technique can improve system reliability while it might increase the temperature of the system beyond safe limits. In this paper, we propose a t hermal- a ware s tandby- s paring (TASS) technique that aims at maximizing the Quality of Service (QoS) of soft real-time tasks, which is defined as a function of the finishing time of running tasks. The proposed technique tolerates permanent and transient faults for multicore real-time embedded systems while meeting the Thermal Safe Power (TSP) as the core-level power constraint, which avoids thermal emergencies in on-chip systems. Executing the main and backup tasks on the cores at any power consumption below TSP guarantees that no thermal violation occurs. Our TASS proposed method provides an opportunity to remove the overlaps of the execution of main and backup tasks to prevent extra power consumption due to applying the fault-tolerant technique. Meanwhile, in order to maximize the QoS, we employ a heterogeneous platform to execute the main tasks as soon as possible on high-performance cores with more power budget. The backup tasks are executed on low power cores after finishing the main tasks. In this case, when the main task finishes successfully, the whole of its corresponding backup task can be dropped, resulting in a significant amount of power and temperature reduction. Therefore, in the fault-free scenarios, the spare cores can be powered down, and only the main tasks are scheduled and executed on the primary cores. Experiments show that our proposed method improves QoS up to 39.78% (on average by 18.40%) and reduces the peak power consumption and temperature by up to 40.21% and 15.47˚C (on average 28.31% and 13.60˚C), respectively, at runtime, while keeping the system reliability at the required level.},
  archive      = {J_TETC},
  author       = {Mohsen Ansari and Sepideh Safari and Sina Yari-Karin and Pourya Gohari-Nazari and Heba Khdr and Muhammad Shafique and Jörg Henkel and Alireza Ejlali},
  doi          = {10.1109/TETC.2021.3120084},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1883-1897},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Thermal-aware standby-sparing technique on heterogeneous real-time embedded systems},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Emulating the effects of radiation-induced soft-errors for
the reliability assessment of neural networks. <em>TETC</em>,
<em>10</em>(4), 1867–1882. (<a
href="https://doi.org/10.1109/TETC.2021.3116999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) are currently one of the most widely used predictive models in machine learning. Recent studies have demonstrated that hardware faults induced by radiation fields, including cosmic rays, may significantly impact the CNN inference leading to wrong predictions. Therefore, ensuring the reliability of CNNs is crucial, especially for safety-critical systems. In the literature, several works propose reliability assessments of CNNs mainly based on statistically injected faults. This work presents a software emulator capable of injecting real faults retrieved from radiation tests. Specifically, from the device characterisation of a DRAM memory, we extracted event rates and fault models. The software emulator can reproduce their incidence and access their effect on CNN applications with a reliability assessment precision close to the physical one. Radiation-based physical injections and emulator-based injections are performed on three CNNs (LeNet-5) exploiting different data representations. Their outcomes are compared, and the software results evidence that the emulator is able to reproduce the faulty behaviours observed during the radiation tests for the targeted CNNs. This approach leads to a more concise use of radiation experiments since the extracted fault models can be reused to explore different scenarios (e.g., impact on a different application).},
  archive      = {J_TETC},
  author       = {Lucas Matana Luza and Annachiara Ruospo and Daniel Söderström and Carlo Cazzaniga and Maria Kastriotou and Ernesto Sanchez and Alberto Bosio and Luigi Dilillo},
  doi          = {10.1109/TETC.2021.3116999},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1867-1882},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Emulating the effects of radiation-induced soft-errors for the reliability assessment of neural networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy efficient approximate 3D image reconstruction.
<em>TETC</em>, <em>10</em>(4), 1854–1866. (<a
href="https://doi.org/10.1109/TETC.2021.3116471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We demonstrate an efficient and accelerated parallel, sparse depth reconstruction framework using compressed sensing (compressed sensing (CS)) and approximate computing. Employing data parallelism for rapid image formation, the depth image is reconstructed from sparsely sampled scenes using convex optimization. Coupled with faster imaging, this sparse sampling reduces significantly the projected laser power in active systems such as light detection and ranging (LiDAR) to allow eye safe operation at longer range. We also demonstrate how reduced precision is leveraged to reduce the number of logic units in field-programmable gate array (FPGA) implementations for such sparse imaging systems. It enables significant reduction in logic units, memory requirements and power consumption by over 80% with minimal impact on the quality of reconstruction. To further accelerate processing, pre-computed, important components of the lower-upper (LU) decomposition and other linear algebraic computations are used to solve the convex optimization problems. Our methodology is demonstrated by the application of the alternating direction method of multipliers (ADMM) and proximal gradient descent (PGD) algorithms. For comparison, a fully discrete least square reconstruction method ( &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$d$&lt;/tex-math&gt;&lt;/inline-formula&gt; Sparse) is also presented. This demonstrates the feasibility of novel, high resolution, low power and high frame rate LiDAR depth imagers based on sparse illumination for use in applications where resources are strictly limited.},
  archive      = {J_TETC},
  author       = {Yun Wu and Andreas Aßmann and Brian D. Stewart and Andrew M. Wallace},
  doi          = {10.1109/TETC.2021.3116471},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1854-1866},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Energy efficient approximate 3D image reconstruction},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning-based hardware trojan detection with
block-based netlist information extraction. <em>TETC</em>,
<em>10</em>(4), 1837–1853. (<a
href="https://doi.org/10.1109/TETC.2021.3116484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the globalization of the semiconductor industry, hardware Trojans (HTs) are an emergent security threat in modern integrated circuit (IC) production. Research is now being conducted into designing more accurate and efficient methods to detect HTs. Recently, a number of machine learning (ML)-based HT detection approaches have been proposed; however, most of them still use knowledge-driven approaches to design features and often use engineering intuition to carefully craft the detection model to improve accuracy. Therefore, in this work, we propose a data-driven HT detection system based on gate-level netlists. The system consists of four main parts: 1) Information extraction from netlist block; 2) Natural language processing (NLP) for translating netlist information; 3) Deel learning (DL)-based HT detection model; 4) HT component final voter. In the experiments, both a long short-term memory networks (LSTM) model and convolutional neural network (CNN) model are used as our detection models. We performed the experiments on the HT benchmarks from Trust-hub and K-fold crossing verification has been applied to evaluate different parameter settings in the training procedure. The experimental results show that the proposed HT detection system can achieve 79.29% TPR, 99.97% TNR, 87.75% PPV and 99.94% NPV for combinational Trojan detection and 93.46% TPR, 99.99% TNR, 98.92% PPV and 99.92% NPV for sequential Trojan detection after voting-based optimization using the LEDA library-based HT benchmarks ( logic_level =4, upsampling, LSTM, 5 epochs).},
  archive      = {J_TETC},
  author       = {Shichao Yu and Chongyan Gu and Weiqiang Liu and Máire O’Neill},
  doi          = {10.1109/TETC.2021.3116484},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1837-1853},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Deep learning-based hardware trojan detection with block-based netlist information extraction},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quick generation of SSD performance models using machine
learning. <em>TETC</em>, <em>10</em>(4), 1821–1836. (<a
href="https://doi.org/10.1109/TETC.2021.3116197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing usage of Solid-State Drives (SSDs) has greatly boosted the performance of storage backends. SSDs perform many internal processes such as out-of-place writes, wear-leveling, and garbage collection. These operations are complex and not well documented which make it difficult to create accurate SSD simulators. Our survey indicates that aside from complex configuration, available SSD simulators do not support both sync and discard requests. Past performance models also ignore the long term effect of I/O requests on SSD performance, which has been demonstrated to be significant. In this article, we utilize a methodology based on machine learning that extracts history-aware features at low cost to train SSD performance models that predict request response times. A key goal of our work is to achieve real-time or near-real time feature extraction and to achieve practical training times so our work can be considered as part of solutions that perform online or periodical characterization such as adaptive storage algorithms. Thus, we extract features from individual read, write, sync , and discard I/O requests and use structures such as exponentially decaying counters to track past activity using &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$O(1)$&lt;/tex-math&gt;&lt;/inline-formula&gt; memory and processing cost. To make our methodology accessible and usable in real-world online scenarios, we focus on machine learning models that can be trained quickly on a single machine. To massively reduce processing and memory cost, we utilize feature selection to reduce feature count by up to 63%, allowing a feature extraction rate of 313,000 requests per second using a single thread. Our dataset contains 580M requests taken from 35 workloads. We experiment with three families of machine learning models, a) decision trees, b) ensemble methods utilizing decision trees, and c) Feedforward Neural Networks (FNN). Based on these experiments, FNN achieves an average &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$R^2$&lt;/tex-math&gt;&lt;/inline-formula&gt; score of 0.72 compared to 0.61 and 0.45 for the Random Forest and Bagging, respectively, where &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$R^2 \in (-\inf, 1)$&lt;/tex-math&gt;&lt;/inline-formula&gt; of 1 indicates a perfect fit. However, while the random forest model has lower accuracy, it uses general processing hardware and can be trained much faster, making it viable for use in online scenarios.},
  archive      = {J_TETC},
  author       = {Mojtaba Tarihi and Soheil Azadvar and Arash Tavakkol and Hossein Asadi and Hamid Sarbazi-Azad},
  doi          = {10.1109/TETC.2021.3116197},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1821-1836},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Quick generation of SSD performance models using machine learning},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SACC: A size adaptive content caching algorithm in fog/edge
computing using deep reinforcement learning. <em>TETC</em>,
<em>10</em>(4), 1810–1820. (<a
href="https://doi.org/10.1109/TETC.2021.3115793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge/Fog caching is promising to mitigate the data traffic problem in both traditional wireline/wireless networks and the 5G network. Recently, deep reinforcement learning (DRL) has been adopted to provide a more powerful content caching policy. The current DRL-based scheme considers the requests for the same size and updates the caching for each request. However, the real-world data delivery systems usually refresh the content cache periodically, with different sizes of requests. To satisfy the real-world requirements, this study proposes a novel size adaptive content caching algorithm using DRL, termed SACC. SACC models the requests with random sizes and updates the cache after a batch of requests. Technically, SACC utilizes the Actor-Critic framework, which is able to process large discrete action space. SACC comprehensively considers the short-, medium- and long-term requests as the state to train the actor network. The reward is modeled as the cache hit rate. Once an action is selected from the policy network, it is expended to its k nearest neighbors. The critic network finds the action with the best reward from the k actions. The performance of the proposed SACC is evaluated through computer simulation. The experimental results showed that SACC could train the network much more efficiently and improve the cache hit rate by as much as 4% when comparing to the state-of-art DRL-based scheme.},
  archive      = {J_TETC},
  author       = {Xiaoping Zhou and Zhenlong Liu and Maozu Guo and Jichao Zhao and Jialin Wang},
  doi          = {10.1109/TETC.2021.3115793},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1810-1820},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {SACC: A size adaptive content caching algorithm in Fog/Edge computing using deep reinforcement learning},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards execution-efficient LSTMs via hardware-guided
grow-and-prune paradigm. <em>TETC</em>, <em>10</em>(4), 1799–1809. (<a
href="https://doi.org/10.1109/TETC.2021.3115475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long short-term memory (LSTM) applications need fast yet compact models. Neural network compression approaches, e.g., the grow-and-prune paradigm, have proved to be promising for cutting down network complexity by skipping insignificant weights. However, current compression strategies remain mostly hardware-agnostic and network complexity reduction does not always translate to execution efficiency. In this work, we propose a hardware-guided symbiotic training methodology for compact, accurate, yet execution-efficient inference models. It is based on our observation that hardware may introduce substantial non-monotonic behavior, which we call the latency hysteresis effect, when evaluating network size versus inference latency. This observation raises question about the mainstream smaller-dimension-is-better compression strategy, which often leads to a sub-optimal model architecture. Leveraging the hardware-impacted hysteresis effect and sparsity, we enable a symbiosis of model compactness and accuracy with execution efficiency, thus reducing LSTM latency while increasing its accuracy. We have evaluated our approach on language modeling and speech recognition applications. Relative to the traditional stacked LSTM architecture obtained for the Penn Treebank dataset, we reduce the number of parameters by 18.0× (30.5×) and measured run-time latency by up to 2.4× (5.2×) on Nvidia GPUs (Intel Xeon CPUs) without any accuracy degradation. For the DeepSpeech2 architecture obtained for the AN4 dataset, we reduce the model size by 7.0× (19.4×), word error rate from 12.9% to 9.9% (10.4%), and measured run-time latency by up to 1.7× (2.4×) on Nvidia GPUs (Intel Xeon CPUs). Our method consistently outperforms prior art for both applications, with compact, accurate, and execution-efficient inference models.},
  archive      = {J_TETC},
  author       = {Hongxu Yin and Guoyang Chen and Yingmin Li and Shuai Che and Weifeng Zhang and Niraj K. Jha},
  doi          = {10.1109/TETC.2021.3115475},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1799-1809},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Towards execution-efficient LSTMs via hardware-guided grow-and-prune paradigm},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly supervised class-agnostic image similarity search
based on convolutional neural network. <em>TETC</em>, <em>10</em>(4),
1789–1798. (<a href="https://doi.org/10.1109/TETC.2022.3157851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For vision-based localization methods, the efficient and accurate image retrieval in a database with a large number of images is an important prerequisite and guarantee. The traditional content-based image retrieval method can retrieve database images that have similar details and features with the query image. However, there may be significant differences in semantics between them, which may result in mismatches. The method based on convolutional neural network can avoid such problems to a certain extent. In this paper, we propose an end-to-end weakly supervised class-agnostic image retrieval method based on convolutional neural networks. In the offline stage, the database images are preprocessed to separate the foregrounds and backgrounds, and the foregrounds are clustered for storage. The aim is to reduce the amount of data calculation in the online stage and avoid mismatches caused by backgrounds mixing. In the online stage, the same processing is performed on the query image, and the improved method based on the relational network is used to compare the similarity between the input objects to obtain the optimal retrieval result. The test results on Holidays, Paris 6k, Oxford 5k and three large-scale datasets show that the proposed method has better performance than existing image retrieval methods.},
  archive      = {J_TETC},
  author       = {Songxiang Yang and Lin Ma and Xuezhi Tan},
  doi          = {10.1109/TETC.2022.3157851},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1789-1798},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Weakly supervised class-agnostic image similarity search based on convolutional neural network},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluating the security and economic effects of moving
target defense techniques on the cloud. <em>TETC</em>, <em>10</em>(4),
1772–1788. (<a href="https://doi.org/10.1109/TETC.2022.3155272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moving Target Defense (MTD) is a proactive security mechanism that changes the attack surface with the aim of confusing attackers. Cloud computing leverages MTD techniques to enhance the cloud security posture against cyber threats. While many MTD techniques have been applied to cloud computing, there has so far been no joint evaluation of the effectiveness of MTD techniques with respect to security and economic metrics. In this paper, we first introduce mathematical definitions for the combination of three MTD techniques: Shuffle, Diversity, and Redundancy. Then, we utilize four security metrics – namely, system risk, attack cost, return on attack, and reliability – to assess the effectiveness of the combined MTD techniques applied to large-scale cloud models. Second, we focus on a specific context based on a cloud model for e-health applications to evaluate the effectiveness of the MTD techniques using security and economic metrics. We introduce (1) a strategy to effectively deploy the Shuffle MTD technique using a virtual machine placement technique, and (2) two strategies to deploy the Diversity MTD technique through operating system diversification. As deploying the Diversity technique incurs costs, we formulate the optimal diversity assignment problem (O-DAP), and solve it as a binary linear programming model to obtain the assignment that maximizes the expected net benefit.},
  archive      = {J_TETC},
  author       = {Hooman Alavizadeh and Samin Aref and Dong Seong Kim and Julian Jang-Jaccard},
  doi          = {10.1109/TETC.2022.3155272},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1772-1788},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Evaluating the security and economic effects of moving target defense techniques on the cloud},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A dynamic task scheduling strategy for multi-access edge
computing in IRS-aided vehicular networks. <em>TETC</em>,
<em>10</em>(4), 1761–1771. (<a
href="https://doi.org/10.1109/TETC.2022.3153494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access Edge Computing (MEC) has played an important role in realizing intelligent beyond 5G (B5G) vehicular networks. The computation tasks of intelligent applications can be offloaded to and processed by near-end-user MEC servers to meet strict latency requirements. However, the latency of provided services is dependent on MEC processor scheduling and millimeter wave (mmWave) transmission conditions for the urban B5G vehicular networks. To alleviate the mmWave signal attenuation caused by buildings, Intelligent Reflecting Surface (IRS) has been regarded as efficient and prospective infrastructure. In this article, we study the IRS-aided MEC-served vehicular networks and analyze the relationship between computation resource allocation and offloading policy at an intersection. Considering the vehicle mobility patterns, transmission conditions, and task sizes, we optimize the task scheduling by improving the allocation of limited processors and IRS resource. Moreover, the mutual interference among concurrent transmissions is taken into account. Assuming the moving directions available, a dynamic task scheduling algorithm is proposed which considers both the communications and computations. The simulation results illustrate that our proposal outperforms benchmark methods in terms of task offloading rate, computing rate, and finish rate for the IRS-aided MEC-served vehicular networks.},
  archive      = {J_TETC},
  author       = {Yishi Zhu and Bomin Mao and Nei Kato},
  doi          = {10.1109/TETC.2022.3153494},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1761-1771},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A dynamic task scheduling strategy for multi-access edge computing in IRS-aided vehicular networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The role of explainability in assuring safety of machine
learning in healthcare. <em>TETC</em>, <em>10</em>(4), 1746–1760. (<a
href="https://doi.org/10.1109/TETC.2022.3171314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Established approaches to assuring safety-critical systems and software are difficult to apply to systems employing ML where there is no clear, pre-defined specification against which to assess validity. This problem is exacerbated by the “opaque” nature of ML where the learnt model is not amenable to human scrutiny. Explainable AI (XAI) methods have been proposed to tackle this issue by producing human-interpretable representations of ML models which can help users to gain confidence and build trust in the ML system. However, little work explicitly investigates the role of explainability for safety assurance in the context of ML development. This paper identifies ways in which XAI methods can contribute to safety assurance of ML-based systems. It then uses a concrete ML-based clinical decision support system, concerning weaning of patients from mechanical ventilation, to demonstrate how XAI methods can be employed to produce evidence to support safety assurance. The results are also represented in a safety argument to show where, and in what way, XAI methods can contribute to a safety case. Overall, we conclude that XAI methods have a valuable role in safety assurance of ML-based systems in healthcare but that they are not sufficient in themselves to assure safety.},
  archive      = {J_TETC},
  author       = {Yan Jia and John McDermid and Tom Lawton and Ibrahim Habli},
  doi          = {10.1109/TETC.2022.3171314},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1746-1760},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {The role of explainability in assuring safety of machine learning in healthcare},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An approximate memory based defense against model inversion
attacks to neural networks. <em>TETC</em>, <em>10</em>(4), 1733–1745.
(<a href="https://doi.org/10.1109/TETC.2022.3179980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diverse and comprehensive training data is critical in building robust machine learning (ML) models. However, model inversion attacks (MIA) have demonstrated that an ML model can leak important information about its training dataset. This work examines the existing MIAs and proposes a hardware-oriented solution to protect the training data from such attacks. Our proposed solution – MIDAS: Model Inversion Defenses with an Approximate memory System – intentionally introduces memory faults to thwart MIA without compromising the original ML model. We use detailed SPICE simulations to build the DRAM fault model with voltage overscaling, implement the state-of-the-art MIAs, and evaluate our proposed solution. Our experiments demonstrate that MIDAS can effectively protect training data from run-time adversarial attacks. In terms of the Pearson Correlation Coefficient (PCC) similarity measure (between the original and the recovered training data), MIDAS reduces the PCC value for shallow and deep neural networks.},
  archive      = {J_TETC},
  author       = {Qian Xu and Md Tanvir Arafin and Gang Qu},
  doi          = {10.1109/TETC.2022.3179980},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1733-1745},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {An approximate memory based defense against model inversion attacks to neural networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving the reliability of network intrusion detection
systems through dataset integration. <em>TETC</em>, <em>10</em>(4),
1717–1732. (<a href="https://doi.org/10.1109/TETC.2022.3178283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents Reliable-NIDS (R-NIDS), a novel methodology for Machine Learning (ML) based Network Intrusion Detection Systems (NIDSs) that allows ML models to work on integrated datasets, empowering the learning process with diverse information from different datasets. We also propose a new dataset, called UNK22. It is built from three of the most well-known network datasets (UGR’16, USNW-NB15 and NLS-KDD), each one gathered from its own network environment, with different features and classes, by using a data aggregation approach present in R-NIDS . Therefore, R-NIDS targets the design of more robust models that generalize better than traditional approaches. Following R-NIDS, in this work we propose to build two well-known ML models for reliable predictions thanks to the meaningful information integrated in UNK22. The results show how these models benefit from the proposed approach, being able to generalize better when using UNK22 in the training process, in comparison to individually using the datasets composing it. Furthermore, these results are carefully analyzed with statistical tools that provide high confidence on our conclusions. Finally, the proposed solution is feasible to be deployed in network production environments, not usually taken into account in the literature.},
  archive      = {J_TETC},
  author       = {Roberto Magán-Carrión and Daniel Urda and Ignacio Diaz-Cano and Bernabé Dorronsoro},
  doi          = {10.1109/TETC.2022.3178283},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1717-1732},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Improving the reliability of network intrusion detection systems through dataset integration},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An interrelated imitation learning method for heterogeneous
drone swarm coordination. <em>TETC</em>, <em>10</em>(4), 1704–1716. (<a
href="https://doi.org/10.1109/TETC.2022.3202297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of small drones has boosted diverse intelligent services, in which the effective swarm coordination plays a vital role in enhancing execution efficiency. However, owing to unreliable air communication and heterogeneous computation capabilities, it is difficult to achieve coordinated actions particularly in distributed scenarios with incomplete observations. In this article, we utilize the generative adversarial imitation learning (GAIL) model to coordinate the drones’ maneuvers by imitating the peer&#39;s demonstrations. However, incomplete observations will lead to inaccurate imitation policies. In order to recover true environment states, we encode historical observation-action trajectories into latent belief representations, which are trained in correlation to imitation policies. Moreover, by merging the trace of historical contexts, the prediction of future states and the action-assisted guidance information, we gain robust belief representations, which lead to more accurate imitation policies. We evaluate the algorithm performance via the drones’ formation control task. Experiment results display the superiorities on imitation accuracy, execution time and energy cost.},
  archive      = {J_TETC},
  author       = {Bo Yang and Chaofan Ma and Xiaofang Xia},
  doi          = {10.1109/TETC.2022.3202297},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1704-1716},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {An interrelated imitation learning method for heterogeneous drone swarm coordination},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Secrecy driven federated learning via cooperative jamming:
An approach of latency minimization. <em>TETC</em>, <em>10</em>(4),
1687–1703. (<a href="https://doi.org/10.1109/TETC.2022.3159282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) provides a promising framework for enabling distributed machine learning based services without revealing users’ private data. In the scenario of wireless FL, to counter the eavesdropping attack when the parameter-server (PS, which is co-located with a base station, BS) sends the model-data to the wireless devices, we propose a secrecy driven FL via cooperative jamming, in which wireless devices cooperatively provide jamming to the eavesdropper to enhance the PS’s secure throughput based on the measure of physical layer security. We formulate a joint optimization of the PS’s downloading-transmission duration, all wireless devices’ uploading-transmission duration as a non-orthogonal multiple access cluster, each device’s local processing-rate and transmit powers for its uploading NOMA-transmission and jamming to the eavesdropper, with the objective of minimizing the overall latency for each round of FL iteration. Despite the non-convexity of the joint optimization problem, a layered algorithm is proposed to solve it. Taking into account the special feature of the optimal jamming solution, we further propose a benefit-sharing scheme, which is based on the principle of Nash bargaining solution, such that all wireless devices can benefit from reducing the FL latency via cooperative jamming in a fairness manner. Numerical results are provided to validate the effectiveness of our proposed algorithms as well as the performance advantage of our proposed secrecy driven FL via cooperative jamming. Experimental results based on the real data-sets and training models demonstrate that our scheme can reduce the latency by more than 35% compared to the case without using the jamming.},
  archive      = {J_TETC},
  author       = {Tianshun Wang and Yang Li and Yuan Wu and Tony Q.S. Quek},
  doi          = {10.1109/TETC.2022.3159282},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1687-1703},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Secrecy driven federated learning via cooperative jamming: An approach of latency minimization},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the properness of incorporating binary classification
machine learning algorithms into safety-critical systems. <em>TETC</em>,
<em>10</em>(4), 1671–1686. (<a
href="https://doi.org/10.1109/TETC.2022.3178631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manufacturers are willing to incorporate Machine Learning (ML) algorithms into their systems, especially those considered as Safety-Critical Systems (SCS). ML algorithms that perform binary classification (i.e., Binary Classifiers (BCs)) find a wide applicability as error, intrusion or failure detectors, provided that their performance complies with SCS safety requirements. However, the performance analysis of BCs relies on metrics that were not developed with safety in mind and consequently may not provide meaningful evidence to decide whether to incorporate a BC into a SCS. In this paper, we empirically assess the properness of such incorporation by analyzing the distribution of misclassifications of BCs instead of simply counting misclassifications. This allows us to better assess the adequacy of a given BC by identifying areas of the classification space where the BC is likely to misclassify and therefore constitutes actionable information to deal with the SCS. Our assessment takes a deeper view of the classification performance concerning safety by using new metrics that consider the proportions of predictions that are/are not considered sufficiently safe to be used by incorporating SCS. The results of our experiment allow discussing the potential of such distribution analysis for deciding if a BC can be incorporated into a SCS.},
  archive      = {J_TETC},
  author       = {Mohamad Gharib and Tommaso Zoppi and Andrea Bondavalli},
  doi          = {10.1109/TETC.2022.3178631},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1671-1686},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {On the properness of incorporating binary classification machine learning algorithms into safety-critical systems},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guest editorial: Special section on “to be safe and
dependable in the era of artificial intelligence: Emerging techniques
for trusted and reliable machine learning.” <em>TETC</em>,
<em>10</em>(4), 1668–1670. (<a
href="https://doi.org/10.1109/TETC.2022.3210449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special section focus on the application of artificial intelligence in emerging trusted and reliability machine learning systems. During the last decade, advances in areas such as convolutional neural networks, deep learning, and hardware accelerators among others, have enabled the widespread and ubiquitous adoption of machine learning in real-world systems. This trend is expected to continue and expand in coming years leading to a world that depends heavily on machine learning-based systems.},
  archive      = {J_TETC},
  author       = {Shanshan Liu and Pedro Reviriego and Fabrizio Lombardi and Patrick Girard},
  doi          = {10.1109/TETC.2022.3210449},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1668-1670},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Guest editorial: special section on “To be safe and dependable in the era of artificial intelligence: emerging techniques for trusted and reliable machine learning”},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Near volatile and non-volatile memory processing in 3D
systems. <em>TETC</em>, <em>10</em>(3), 1657–1664. (<a
href="https://doi.org/10.1109/TETC.2021.3115495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cost of transferring data between the off-chip memory system and compute unit is the fundamental energy and performance bottleneck in modern computing systems. Furthermore, with the advent of emerging data-intensive applications and technology scaling, this bottleneck has continuously increased. To overcome these difficulties, Near Memory Processing (NMP) based on 3D die stacking becomes a potential technology to transform the computation-centric system towards memory-centric system. In this work, we explore the feasibility and efficacy of a NMP architecture based on an emerging Non-Volatile Memory technology (NVM) for data-intensive applications and compare it with the conventional 3D-stacked NMP architecture based on DRAM. We demonstrate the effectiveness of our approach with experimental results.},
  archive      = {J_TETC},
  author       = {Maryam S. Hosseini and Masoumeh Ebrahimi and Pooria Yaghini and Nader Bagherzadeh},
  doi          = {10.1109/TETC.2021.3115495},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1657-1664},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Near volatile and non-volatile memory processing in 3D systems},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SPACE: Finding key-speaker in complex multi-person scenes.
<em>TETC</em>, <em>10</em>(3), 1645–1656. (<a
href="https://doi.org/10.1109/TETC.2021.3115625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent image processing based on deep learning is a recently emerging artificial Intelligence method which is critical in the next generation computing systems. Key-speaker detection using image processing aims to find the speaker who plays the most important role in complex multi-person scenes. However, existing speaker detection methods just judge whether there is somebody talking. To overcome this problem, this paper proposes a novel key-speaker detection approach named Speaker Pose-Attention deteCtion modEl (SPACE). This approach extracts the space information and analyze people&#39;s attention gathering to find the key-speaker. The method consists of two main parts. The first part is our proposed Importance Detection Model (IDM) using human pose estimation and an effective evaluation strategy to find candidates of important speaker. The second part consists of Pose-Attention Graph (PAG) and link analysis algorithm. The PAG represents people&#39;s attention graph. The link analysis algorithm analyzes the connectivity of this graph to find the key-speaker. Due to the lack of available key-speaker detection datasets, a self-collected dataset containing speech and meeting scenes has been built to estimate SPACE model. The experimental results show that the proposed method can obtain the best detection accuracy compared with other existing methods.},
  archive      = {J_TETC},
  author       = {Haoyu Zhao and Weidong Min and Jianqiang Xu and Qing Han and Wei Li and Qi Wang and Ziyuan Yang and Linghua Zhou},
  doi          = {10.1109/TETC.2021.3115625},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1645-1656},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {SPACE: Finding key-speaker in complex multi-person scenes},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intelligent reflecting surface (IRS) allocation scheduling
method using combinatorial optimization by quantum computing.
<em>TETC</em>, <em>10</em>(3), 1633–1644. (<a
href="https://doi.org/10.1109/TETC.2021.3115107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent Reflecting Surface (IRS) significantly improves the energy utilization efficiency in 6th generation cellular communication systems. Here, we consider a system with multiple IRS and users, with one user communicating via several IRSs. In such a system, the user to which an IRS is assigned for each unit time must be determined to realize efficient communication. The previous studies on the optimization of various parameters for IRS based wireless systems did not consider the optimization of such IRS allocation scheduling. Therefore, we propose an IRS allocation scheduling method that limits the number of users who allocate each IRS to one unit time and sets the reflection coefficients of the IRS specifically to the assigned user resulting in the maximum IRS array gain. Additionally, as the proposed method is a combinatorial optimization problem, we develop a quadratic unconstrained binary optimization formulation to solve this using quantum computing. This will lead to the optimization of the entire system at a high speed and low power consumption in the future. Using computer simulation, we clarified that the proposed method realizes a more efficient communication compared to the method where one IRS is simultaneously used by multiple users.},
  archive      = {J_TETC},
  author       = {Takahiro Ohyama and Yuichi Kawamoto and Nei Kato},
  doi          = {10.1109/TETC.2021.3115107},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1633-1644},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Intelligent reflecting surface (IRS) allocation scheduling method using combinatorial optimization by quantum computing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PUFs physical learning: Accelerating the enrollment via
delay-based model extraction. <em>TETC</em>, <em>10</em>(3), 1621–1632.
(<a href="https://doi.org/10.1109/TETC.2021.3115176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The introduction of Physical Unclonable Functions (PUFs) has been originally motivated by their ability to resist physical attacks, particularly in anti-counterfeiting scenarios. In these one-way functions, machine learning, cryptanalysis, and side-channel attacks are common attack vectors threatening the promised PUF&#39;s property of unclonability. These attacks often emulate a PUF by employing a large number of Challenge-Response Pairs (CRPs). Some solutions to defeat such attacks are based on a protocol, where a model of the underlying PUF primitives should be extracted during the enrollment phase. In this article, we introduce a novel physical cloning approach applicable to FPGA-based implementations, which allows extracting the PUF&#39;s unique physical characteristics with a few number of Challenge-Response Pairs (CRPs), that increases only linearly for a higher number of PUF components. Indeed, our proposed approach significantly accelerates the enrollment phase and makes complex enrollment protocols feasible. Our core idea relies on an on-chip delay sensor, which can be realized by ordinary FPGA components, measuring the unique characteristic of the PUF elements. We demonstrate the feasibility of our introduced technique by practical experiments on different FPGA platforms, cloning a couple of (complex) PUF constructions, i.e., XOR APUF, iPUF, composed of delay-based Arbiter PUFs.},
  archive      = {J_TETC},
  author       = {Anita Aghaie and Maik Ender and Amir Moradi},
  doi          = {10.1109/TETC.2021.3115176},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1621-1632},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {PUFs physical learning: Accelerating the enrollment via delay-based model extraction},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SIRDAM4.0: A support infrastructure for reliable data
acquisition and management in industry 4.0. <em>TETC</em>,
<em>10</em>(3), 1605–1620. (<a
href="https://doi.org/10.1109/TETC.2021.3111974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main claim of the Industry 4.0 manifesto (I4.0) is the promise of a significant transformation of production and delivery processes due to the digitization of manufacturing. A mandatory requirement of the I4.0 transition is the pervasive adoption of Information Technologies (IT), such as Industrial Internet of Things (IIoT), Cyber-physical Systems (CPS), and Big Data, starting with business management departments down to production sites. Despite IT has reached a consistent maturity level, its integration with Operation Technology (OT) is still a mostly unresolved challenge. In this article, we propose the design of a platform that supports reliable data gathering and sharing among OT and IT layers of an industrial manufacturing company, while also putting the basis for straightforward integration of business stakeholders in the data sharing loop. By leveraging widely used open-source tools, we implemented a software prototype of the platform that Small and Medium Enterprises (SME) can use to face the IT/OT convergence challenge in an affordable way. Results collected from an extensive assessment in real manufacturing settings show that the proposed solution allows meeting both functional and non-functional requirements of a typical data gathering/sharing process in a near-real-time scenario, by granting reliability with very low overhead cost.},
  archive      = {J_TETC},
  author       = {Antonio Corradi and Giuseppe Di Modica and Luca Foschini and Lorenzo Patera and Michele Solimando},
  doi          = {10.1109/TETC.2021.3111974},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1605-1620},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {SIRDAM4.0: A support infrastructure for reliable data acquisition and management in industry 4.0},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computing the similarity estimate using approximate memory.
<em>TETC</em>, <em>10</em>(3), 1593–1604. (<a
href="https://doi.org/10.1109/TETC.2021.3109559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many computing applications there is a need to compute the similarity of sets of elements. When the sets have many elements or comparison involves many sets, computing the similarity requires significant computational effort and storage capacity. As in most cases, a reasonably accurate estimate is sufficient, many algorithms for similarity estimation have been proposed during the last decades. Those algorithms compute signatures for the sets and use them to estimate similarity. However, as the number of sets that need to be compared grows, even these similarity estimation algorithms require significant memory with its associated power dissipation. This article for the first time considers the use of approximate memories for similarity estimation. A theoretical analysis and simulation results are provided; initially it is shown that similarity sketches can tolerate large bit error rates and thus, they can benefit from using approximate memories without substantially compromising the accuracy of the similarity estimate. An understanding of the effect of errors in the stored signatures on the similarity estimate is pursued. A scheme to mitigate the impact of errors is presented; the proposed scheme tolerates even larger bit error rates and does not need additional memory. For example, bit error rates of up to &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$10^{-4}$&lt;/tex-math&gt;&lt;/inline-formula&gt; have less than a 1% impact on the accuracy of the estimate when the memory is unprotected, and larger bit errors rates can be tolerated if the memory is parity protected. These findings can be used for voltage supply scaling and increasing the refresh time in SRAMs and DRAMs. Based on those initial results, an enhanced implementation is further proposed for unprotected memories that further extends the range of tolerated BERs and enables power savings of up to 61.31% for SRAMs. In conclusion, this article shows that the use of approximate memories in sketches for similarity estimation provides significant benefits with a negligible impact on accuracy.},
  archive      = {J_TETC},
  author       = {Pedro Reviriego and Shanshan Liu and Otmar Ertl and Farzad Niknia and Fabrizio Lombardi},
  doi          = {10.1109/TETC.2021.3109559},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1593-1604},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Computing the similarity estimate using approximate memory},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GNNUnlock+: A systematic methodology for designing graph
neural networks-based oracle-less unlocking schemes for provably secure
logic locking. <em>TETC</em>, <em>10</em>(3), 1575–1592. (<a
href="https://doi.org/10.1109/TETC.2021.3108487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leading-edge design houses outsource the fabrication process to pure-play foundries eliminating the expenses of owning and maintaining a fab. The intellectual property (IP) of an outsourced design is now subject to IP piracy, which drives the need for a protection mechanism. Logic locking is a technique that aims to thwart IP piracy throughout the supply chain. However, state-of-the-art, provably secure logic locking (PSLL) techniques are vulnerable to functional and structural analysis-based attacks. Few removal attack protection mechanisms have been developed, such as diversified tree logic and wire entanglement, to protect PSLL against structural attacks. In this work, we significantly enhance GNNUnlock ( GNNUnlock+ ) and demonstrate how the most advanced PSLL techniques armed with removal attack protection have no impact on its effectiveness. Our evaluation demonstrates that GNNUnlock+ is &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$89.66\%-100\%$&lt;/tex-math&gt;&lt;/inline-formula&gt; successful in breaking benchmarks locked using 9 different PSLL techniques—Stripped functionality logic locking, tenacious and traceless logic locking, Anti-SAT, SAT attack resistant logic locking (SARLock), Anti-SAT with diversified tree logic (Anti-SAT-DTL), Anti-SAT with wire entanglement, SARLock-DTL, corrupt and correct (CAC) and CAC-DTL. GNNUnlock+ can break the considered techniques under different parameters, synthesis settings, and technology nodes. Moreover, GNNUnlock+ successfully breaks corner cases where even the most advanced state-of-the-art attacks fail.},
  archive      = {J_TETC},
  author       = {Lilas Alrahis and Satwik Patnaik and Muhammad Abdullah Hanif and Hani Saleh and Muhammad Shafique and Ozgur Sinanoglu},
  doi          = {10.1109/TETC.2021.3108487},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1575-1592},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {GNNUnlock+: A systematic methodology for designing graph neural networks-based oracle-less unlocking schemes for provably secure logic locking},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PM-AIO: An effective asynchronous i/o system for persistent
memory. <em>TETC</em>, <em>10</em>(3), 1558–1574. (<a
href="https://doi.org/10.1109/TETC.2021.3109047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the expected near DRAM performance, popular async I/O systems are disabled on local persistent memory (PM) file systems, which instead use the pseudo-async I/O path (namely the sync one), to serve async I/O (AIO) requests of applications. This paper first identifies the performance shortcomings of this kind of I/O method and argues the necessity of applying the real async I/O on PM device. Then, this paper proposes PM-AIO, a general method to create an effective async I/O path on PM file systems. PM-AIO leverages kernel-level threads to achieve real asynchronism and concurrency. We implement PM-AIO in the Native-AIO of PMFS and NOVA respectively. Extensive experiments are conducted to verify the advantages of PM-AIO on a real PM platform. Compared with the original I/O methods of PM file systems, the results show that PM-AIO can reduce the latencies of AIO requests up to 3 orders of magnitude while reaping up to 2.11× IOPS in realistic workloads which contain relatively large I/O operations (often &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\geq$&lt;/tex-math&gt;&lt;/inline-formula&gt; 4 KB). Meanwhile, PM-AIO incurs up to 4% overhead when handling small I/O operations (often &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$&amp;lt;$&lt;/tex-math&gt;&lt;/inline-formula&gt; 4 KB) because of the inherent overhead.},
  archive      = {J_TETC},
  author       = {Dingding Li and Niyang Zhang and Mianxiong Dong and Hao Chen and Kaoru Ota and Yong Tang},
  doi          = {10.1109/TETC.2021.3109047},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1558-1574},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {PM-AIO: An effective asynchronous I/O system for persistent memory},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online remaining useful lifetime prediction using support
vector regression. <em>TETC</em>, <em>10</em>(3), 1546–1557. (<a
href="https://doi.org/10.1109/TETC.2021.3106252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An accurate prediction of remaining useful lifetime (RUL) in high reliability and safety electronic systems is required due to its wide use in industrial applications. In this paper, we propose a novel methodology for online RUL prediction, using support vector regression (SVR) model. Through Cadence simulations with 22nm CMOS technology library, we demonstrate that frequency degradation follows a trackable path and depends on temperature, voltage and aging. This characteristic is exploited for training the SVR model, validated over 20 years of aging degradation. Our methodology is capable of highly accurate RUL estimation, requiring a ring oscillator (RO), temperature sensor and trained SVR software model. Using a supply voltage of 0.9 V and variation in temperature from 0°C to 100°C, 13 and 21 stage RO show 90 percent cases with a RUL prediction deviation of ±0.2 years, and the remaining between ±0.75 and ±0.8 years, respectively. Furthermore, with voltage variation from 0.7 to 0.9V, with steps of 0.05V and four representative temperatures (25, 50, 75 and 100°C), the 13-RO shows 52 percent cases between ±0.2 years, 21-RO has 80.5 percent cases concentrated between ±0.2 years of RUL prediction deviation and remaining cases for both ROs are located between ±0.8 years.},
  archive      = {J_TETC},
  author       = {Antonio Leonel Hernández Martínez and Saqib Khursheed and Turki Alnuayri and Daniele Rossi},
  doi          = {10.1109/TETC.2021.3106252},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1546-1557},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Online remaining useful lifetime prediction using support vector regression},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Goal and elite-data-driven anthropomorphic learning for
streaming-image analytics. <em>TETC</em>, <em>10</em>(3), 1532–1545. (<a
href="https://doi.org/10.1109/TETC.2021.3104632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of the Internet of Things (IoT), many edge cameras (smart cameras that leverage edge computing) have provided real-time streaming-image analytics using anthropomorphic learning. However, previous anthropomorphic-learning studies did not allow the setting of customized goals to filter out concerned features during online model learning. In addition, when any concept drift occurs, the existing studies using an edge camera cannot adapt efficiently using only representative images. Our study therefore proposes solutions to the above issues based on the self-regulated learning framework, which divides a learning procedure into before, during, and after stages to improve overall learning performance, just like human beings. In the before learning stage of a model on an edge camera, we propose a generative adversarial network (GAN)-based customized feature filtering to screen out users’ concerned features before real-time model learning of image analytics. Next, during the learning stage of the model, we also propose keyframe-oriented model adaptation that can effectively respond to concept drift while reducing image storage space on an edge camera. The experimental results show that customized concerned feature filtering can improve the adversarial effect of the concerned features by 24.26 percent on average, meaning it can avoid using unwanted features from learning an undesirable model. The keyframe-oriented model adaptation can accelerate the overall adjustment time by 52 percent and save the required storage space by 54.7 percent at the cost of compromising precision by 2 percent at most.},
  archive      = {J_TETC},
  author       = {Ching-Hu Lu and Hao-Chung Ku},
  doi          = {10.1109/TETC.2021.3104632},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1532-1545},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Goal and elite-data-driven anthropomorphic learning for streaming-image analytics},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast and robust UAV to UAV detection and tracking from
video. <em>TETC</em>, <em>10</em>(3), 1519–1531. (<a
href="https://doi.org/10.1109/TETC.2021.3104555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicle (UAV) technology is being increasingly used in a wide variety of applications ranging from remote sensing, to delivery and security. As the number of UAVs increases, there is a growing need for UAV to UAV detection and tracking systems for both collision avoidance and coordination. Among possible solutions, autonomous “see-and-avoid” systems based on low-cost high-resolution video cameras offer important advantages in terms of light weight and low power consumption. However, in order to be effective, camera based “see-and-avoid” systems require sensitive, robust, and computationally efficient algorithms for autonomous detection and tracking of UAVs from a moving camera. In this article, we propose a general architecture for a highly accurate and computationally efficient UAV to UAV detection and tracking (U2U-D&amp;T) algorithm from a camera mounted on a moving UAV platform. The system is based on a computationally efficient pipeline consisting of a moving target detector, followed by a target tracker. The algorithm is validated using video data collected from multiple fixed-wing UAVs that is manually ground-truthed and is publicly available. Results indicate that the proposed algorithm can be implemented on commodity hardware and robustly achieves highly accurate detection and tracking of even distant and faint UAVs. Open source code for the U2U-D&amp;T algorithm is available at: https://github.com/jingliinpurdue/Fast-and-Robust-UAV-to-UAV-Detection-and-Tracking.git.},
  archive      = {J_TETC},
  author       = {Jing Li and Dong Hye Ye and Mathias Kolsch and Juan P. Wachs and Charles A. Bouman},
  doi          = {10.1109/TETC.2021.3104555},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1519-1531},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Fast and robust UAV to UAV detection and tracking from video},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AntiDOTE: Protecting debug against outsourced test entities.
<em>TETC</em>, <em>10</em>(3), 1507–1518. (<a
href="https://doi.org/10.1109/TETC.2021.3102832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a variety of business and technical reasons, semiconductor companies might choose to outsource fabrication, assembly, and testing. To fulfill their obligations, external foundries and test houses need access to chip test interfaces to ensure basic functionality. These test interfaces must be protected against illegitimate accesses to prevent extraction of sensitive original equipment manufacturer (OEM) and end-user data from devices in the field as well as thwart intellectual property (IP) piracy and overproduction of integrated circuits. In this article, we present AntiDOTE, a low-cost and robust access control mechanism for test interfaces that supports outsourced fabrication, assembly, and testing with untrusted foundries and testers without compromising security. We showcase AntiDOTE on an ARM Cortex M0-based micro-controller fabricated using commercially available 55nm technology.},
  archive      = {J_TETC},
  author       = {Nimisha Limaye and Christian Wachsmann and Mohammed Nabeel and Mohammed Ashraf and Arun Kanuparthi and Ozgur Sinanoglu},
  doi          = {10.1109/TETC.2021.3102832},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1507-1518},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {AntiDOTE: Protecting debug against outsourced test entities},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RC-RNN: Reconfigurable cache architecture for storage
systems using recurrent neural networks. <em>TETC</em>, <em>10</em>(3),
1492–1506. (<a href="https://doi.org/10.1109/TETC.2021.3102041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solid-State Drives (SSDs) have significant performance advantages over traditional Hard Disk Drives (HDDs) such as lower latency and higher throughput. Significantly higher price per capacity and limited lifetime, however, prevents designers to completely substitute HDDs by SSDs in enterprise storage systems. SSD-based caching has recently been suggested for storage systems to benefit from higher performance of SSDs while minimizing the overall cost. While conventional caching algorithms such as Least Recently Used (LRU) provide high hit ratio in processors, due to the highly random behavior of Input/Output (I/O) workloads, they hardly provide the required performance level for storage systems. In addition to poor performance, inefficient algorithms also shorten SSD lifetime with unnecessary cache replacements. Such shortcomings motivate us to benefit from more complex non-linear algorithms to achieve higher cache performance and extend SSD lifetime. In this article, we propose RC-RNN , the first reconfigurable SSD-based cache architecture for storage systems that utilizes machine learning to identify performance-critical data pages for I/O caching. The proposed architecture uses Recurrent Neural Networks (RNN) to characterize ongoing workloads and optimize itself towards higher cache performance while improving SSD lifetime. RC-RNN attempts to learn characteristics of the running workload to predict its behavior and then uses the collected information to identify performance-critical data pages to fetch into the cache. We implement the proposed architecture on a physical server equipped with a Core-i7 CPU, 256GB SSD, and a 2TB HDD running Linux kernel 4.4.0. Experimental results show that RC-RNN characterizes workloads with an accuracy up to 94.6 percent for SNIA I/O workloads. RC-RNN can perform similarly to the optimal cache algorithm by an accuracy of 95 percent on average, and outperforms previous SSD caching architectures by providing up to 7x higher hit ratio and decreasing cache replacements by up to 2x.},
  archive      = {J_TETC},
  author       = {Shahriar Ebrahimi and Reza Salkhordeh and Seyed Ali Osia and Ali Taheri and Hamid R. Rabiee and Hossen Asadi},
  doi          = {10.1109/TETC.2021.3102041},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1492-1506},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {RC-RNN: Reconfigurable cache architecture for storage systems using recurrent neural networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting zero knowledge proof and blockchains towards the
enforcement of anonymity, data integrity and privacy (ADIP) in the IoT.
<em>TETC</em>, <em>10</em>(3), 1476–1491. (<a
href="https://doi.org/10.1109/TETC.2021.3099701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the Internet of Things (IoT) has been contemplated as the next technological advancement in the era of data communication and networking. However, although hundreds of new IoT platforms are introduced to the market every few months, the security of IoT ecosystems is still not fully understood. This paper discloses the architecture of a multilayer, multimode security system for the IoT. The proposed system is capable of providing multiple security solutions that support anonymous authentication, device privacy, data integrity, device sybil attack detection and IoT server spoofing attack detection. For IoT access control and authentication, our system can support two modes of operations, with one mode endorsing device privacy protection over the network and the second mode relinquishing device identity to establish data tracing during safety-critical IoT events. The new security system includes two innovative crypto approaches, zero knowledge proof (ZKP) and blockchains. IoT device anonymity was achieved via the multimode ZKP protocol, while data integrity and protection against sybil and IoT spoofing attacks were maintained via blockchains. Our threat analysis models showed that data modification and data injection attacks are not feasible. Probabilistic modeling of an IoT spoofing attack was performed in this paper, and the results show that our security system provides high resiliency against such attacks, with a probability approaching 1.},
  archive      = {J_TETC},
  author       = {Amar Rasheed and Rabi N. Mahapatra and Cihan Varol and Karpoor Narashimha},
  doi          = {10.1109/TETC.2021.3099701},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1476-1491},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Exploiting zero knowledge proof and blockchains towards the enforcement of anonymity, data integrity and privacy (ADIP) in the IoT},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ALPHA: A novel algorithm-hardware co-design for accelerating
DNA seed location filtering. <em>TETC</em>, <em>10</em>(3), 1464–1475.
(<a href="https://doi.org/10.1109/TETC.2021.3093840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequence alignment is a fundamental operation in genomic analysis where DNA fragments called reads are mapped to a long reference DNA sequence. There exist a number of (in)exact alignment algorithms with varying sensitivity for both local and global alignments, however, they are all computationally expensive. With the advent of high-throughput sequencing (HTS) technologies that generate a mammoth amount of data, there is increased pressure on improving the performance and capacity of the analysis algorithms in general and the mapping algorithms in particular. While many works focus on improving the performance of the aligner themselves, recently it has been demonstrated that restricting the mapping space for input reads and filtering out mapping positions that will result in a poor match can significantly improve the performance of the alignment operation. However, this is only true if it is guaranteed that the filtering operation can be performed significantly faster. Otherwise, it can easily outweigh the benefits of the aligner. To expedite this pre-alignment filtering, among others, the recently proposed GRIM-Filter uses highly-parallel processing-in-memory operations benefiting from light-weight computational units on the logic-in-memory layer. However, the significant amount of data transferring between the memory and logic-in-memory layers quickly becomes a performance and energy bottleneck for the memory subsystem and ultimately for the overall system. By analyzing input genomes, we found that there are unexpected data-reuse opportunities in the filtering operation. We propose an algorithm-hardware co-design that exploits the data-reuse in the seed location filtering operation and, compared to the GRIM-Filter, cuts the number of memory accesses by 22-54 percent. This reduction in memory accesses improves the overall performance and energy consumption by 19-44 and 21-49 percent, respectively.},
  archive      = {J_TETC},
  author       = {Fazal Hameed and Asif Ali Khan and Jeronimo Castrillon},
  doi          = {10.1109/TETC.2021.3093840},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1464-1475},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {ALPHA: A novel algorithm-hardware co-design for accelerating DNA seed location filtering},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A cybersecurity educated community. <em>TETC</em>,
<em>10</em>(3), 1456–1463. (<a
href="https://doi.org/10.1109/TETC.2021.3093444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for cybersecurity education has grown significantly for all citizens. However, cybersecurity education has not been adequately addressed in fields outside of this domain. Cybersecurity concerns increase every time a new device connects to a network. As such, we need to increase involvement and advancement of cybersecurity education at all levels and in every discipline. Education, audit, and enforcement regarding cybersecurity are critical aspects and are the responsibility of all system stakeholders. It is no longer enough to rely on the security team to be responsible for combating cyber threats. Involvement and commitment from all stakeholders are essential and universal and begins with a cross-disciplinary reform in education. In this paper, the need to deliver cybersecurity education across all disciplines and at all levels is discussed. Specifically, the focus is at the following four types of users: K-12, college, technical professionals, and all other citizens. A curriculum roadmap that integrates cybersecurity throughout technical and nontechnical curricula is proposed as a foundation for future cyber education planning. The goal is to introduce students and citizens to the concept of cybersecurity, to ensure they can apply cybersecurity concepts, and to expose them to multiple approaches to solving cybersecurity related problems.},
  archive      = {J_TETC},
  author       = {Norita Ahmad and Phillip A. Laplante and Joanna F. DeFranco and Mohamad Kassab},
  doi          = {10.1109/TETC.2021.3093444},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1456-1463},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A cybersecurity educated community},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PolyWorm: Leveraging polymorphic behavior to implant
hardware trojans. <em>TETC</em>, <em>10</em>(3), 1443–1455. (<a
href="https://doi.org/10.1109/TETC.2021.3090060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the various challenges looming over the integrated circuit (IC) supply chain for the sub-nm technology nodes, researchers are looking into emerging devices to design the next generation of high performance and low-power chips. The magnetoelectric spin-orbit (MESO) switch, developed by Intel™, is a front-runner in this search and is currently in a state of advanced experimental research. To ensure faster time-to-market and reap the existing supply chain’s benefits, it is envisioned that the upcoming MESO devices will be coupled and hybridized with the existing complementary metal-oxide-semiconductor (CMOS) industrial framework. However, adopting the existing CMOS framework and incorporating it’s outsourced supply chain increases exposure to various security threats such as design intellectual property (IP) piracy, overproduction of ICs, and insertion of hardware Trojans (HT) to leak information or cause denial-of-service. This article proposes and investigates a stealthy HT insertion technique, PolyWorm , leveraging the polymorphic capabilities of MESO gates in hybrid MESO-CMOS architectures. We evaluate the efficacy of PolyWorm on ITC-99 benchmarks and demonstrate two use-case scenarios, viz., corrupting the intended design functionality and leaking the secret key from a cryptographic core. We also present a low-footprint domain wall-based trigger for our polymorphic Trojan to evade structural and power-based testing.},
  archive      = {J_TETC},
  author       = {Nimisha Limaye and Nikhil Rangarajan and Satwik Patnaik and Ozgur Sinanoglu and Kanad Basu},
  doi          = {10.1109/TETC.2021.3090060},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1443-1455},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {PolyWorm: Leveraging polymorphic behavior to implant hardware trojans},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AutoRelax: HW-SW co-optimization for efficient SpGEMM
operations with automated relaxation in deep learning. <em>TETC</em>,
<em>10</em>(3), 1428–1442. (<a
href="https://doi.org/10.1109/TETC.2021.3089848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a HW-SW co-optimization technique to perform energy-efficient spGEMM operations for deep learning. First, we present an automated pruning algorithm, named AutoRelax, that allows some level of relaxation to achieve higher compression ratio. As the benefit of the proposed pruning algorithm may be limited by the sparsity level of a given weight matrix, we present additional steps to further improve its efficiency. Along with the software approach, we also present a hardware architecture for processing sparse GEMM operations to maximize the benefit of the proposed pruning algorithm and sparse matrix format. To validate the efficiency of our co-optimization methodology, we evaluated the proposed method on three benchmarks, language modeling, speech recognition and image classification. As a result, our approach improves the on-chip performance on spGEMM operations by 9.50 &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\sim$&lt;/tex-math&gt;&lt;/inline-formula&gt; 27.57% and achieves 15.35 &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\sim$&lt;/tex-math&gt;&lt;/inline-formula&gt; 33.28% energy reduction considering DRAM accesses over other sparse accelerators.},
  archive      = {J_TETC},
  author       = {Sehun Park and Jae-Joon Kim and Jaeha Kung},
  doi          = {10.1109/TETC.2021.3089848},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1428-1442},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {AutoRelax: HW-SW co-optimization for efficient SpGEMM operations with automated relaxation in deep learning},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Offloading decision for mobile multi-access edge computing
in a multi-tiered 6G network. <em>TETC</em>, <em>10</em>(3), 1414–1427.
(<a href="https://doi.org/10.1109/TETC.2021.3090061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Future telecommunication systems in beyond 5G/6G networks, will include a massive amount of devices and a high variation of applications, many of which with steep processing requirements and strict latency limitations. To satisfy such demands, Multi-access Edge Computing will play a key role in the future of cloud systems. Users can offload their applications to edge cloud servers, capable of processing their tasks and responding with an output quickly. However, for this to become a reality, it is important to carefully choose a server for each user. This decision is complicated by user mobility and how users could alternatively connect to the remote cloud or execute applications locally. In this article, we propose a heuristic algorithm for determining the best server for each user in a multiple mobile users, multiple servers, multi-tiered scenario. Our proposal considers the time needed for transmitting and processing tasks when minimizing the total service delay as well as the time needed for service setup and migration of data between servers. Moreover, our method attempts to mimic as faithfully as possible real-life scenarios. Finally, analysis shows that our proposal is vastly superior to benchmark methods and even improves upon a solution commonly used in the literature.},
  archive      = {J_TETC},
  author       = {Tiago Koketsu Rodrigues and Jiajia Liu and Nei Kato},
  doi          = {10.1109/TETC.2021.3090061},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1414-1427},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Offloading decision for mobile multi-access edge computing in a multi-tiered 6G network},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph-embedded convolutional neural network for image-based
EEG emotion recognition. <em>TETC</em>, <em>10</em>(3), 1399–1413. (<a
href="https://doi.org/10.1109/TETC.2021.3087174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition from electroencephalograph (EEG) signals has long been essential for affective computing. In this article, we evaluate EEG emotion recognition by converting EEG signals from multiple channels into images such that richer spatial information can be considered and the question of EEG-based emotion recognition can be converted into image recognition. To this end, we propose a novel method to generate continuous images from discrete EEG signals by introducing offset variables following a Gaussian distribution for each EEG channel to alleviate the biased electrode coordinates during image generation. In addition, a novel graph-embedded convolutional neural network (GECNN) method is proposed to combine the local convolutional neural network (CNN) features with global functional features to provide complementary emotion information. In GECNN, the attention mechanism is applied to extract more discriminative local features. Simultaneously, dynamical graph filtering explores the intrinsic relationships between different EEG regions. The local and global functional features are finally fused for emotion recognition. Extensive experiments in subject-dependent and subject-independent protocols are conducted to evaluate the performance of the proposed GECNN model on four datasets, i.e., SEED, SDEA, DREAMER, and MPED.},
  archive      = {J_TETC},
  author       = {Tengfei Song and Wenming Zheng and Suyuan Liu and Yuan Zong and Zhen Cui and Yang Li},
  doi          = {10.1109/TETC.2021.3087174},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1399-1413},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Graph-embedded convolutional neural network for image-based EEG emotion recognition},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improvement of battery lifetime based on communication
resource control in low-earth-orbit satellite constellations.
<em>TETC</em>, <em>10</em>(3), 1388–1398. (<a
href="https://doi.org/10.1109/TETC.2021.3087489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The market needs for satellite communication networks have changed recently. Accordingly, low-earth-orbit (LEO) satellite constellations, which are expected to meet these demands, have been attracting increasing attention. In the existing communication method that uses LEO satellite constellations, the satellite located in the vicinity of the satellite terminal that issues the communication request processes it, regardless of the state of its battery. However, this communication method shortens the lifetime of the satellite in instances of severe battery deterioration. Thus, this communication method is ineffective in large-scale satellite constellations wherein running costs are an issue. Therefore, in this study, we develop a communication method that controls the transmission power and transmission gain of a satellite antenna based on the deterioration state of the battery to increase the battery&#39;s lifetime. The reduction in running costs following the prolongation of the battery&#39;s lifetime will allow the development and use of large-scale LEO satellite constellations. The implemented system is expected to be able to meet future satellite communication demands. The effectiveness of the proposed method is verified through simulation.},
  archive      = {J_TETC},
  author       = {Hikaru Tsuchida and Yuichi Kawamoto and Nei Kato and Kazuma Kaneko and Shigenori Tani and Masatake Hangai and Hiroshi Aruga},
  doi          = {10.1109/TETC.2021.3087489},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1388-1398},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Improvement of battery lifetime based on communication resource control in low-earth-orbit satellite constellations},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal energy-aware task scheduling for batteryless IoT
devices. <em>TETC</em>, <em>10</em>(3), 1374–1387. (<a
href="https://doi.org/10.1109/TETC.2021.3086144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today’s IoT devices rely on batteries, which offer stable energy storage but contain harmful chemicals. Having billions of IoT devices powered by batteries is not sustainable for the future. As an alternative, batteryless devices run on long-lived capacitors charged using energy harvesters. The small energy storage capacity of capacitors results in intermittent on-off behaviour. Traditional computing schedulers can not handle this intermittency, and in this article we propose a first step towards an energy-aware task scheduler for constrained batteryless devices. We present a new energy-aware task scheduling algorithm that is able to optimally schedule application tasks to avoid power failures, and that will allow us to provide insights on the optimal look-ahead time for energy prediction. Our insights can be used as a basis for practical energy-aware scheduling and energy a vailability prediction algorithms. We formulate the scheduling problem as a Mixed Integer Linear Program. We evaluate its performance improvement when comparing it with state-of-the-art schedulers for batteryless IoT devices. Our results show that making the task scheduler energy aware avoids power failures and allows more tasks to successfully execute. Moreover, we conclude that a relatively short look-ahead energy prediction time of 8 future task executions is enough to achieve optimality.},
  archive      = {J_TETC},
  author       = {Carmen Delgado and Jeroen Famaey},
  doi          = {10.1109/TETC.2021.3086144},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1374-1387},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Optimal energy-aware task scheduling for batteryless IoT devices},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Co-evolutionary fuzzy deep transfer learning for disaster
relief demand forecasting. <em>TETC</em>, <em>10</em>(3), 1361–1373. (<a
href="https://doi.org/10.1109/TETC.2021.3085337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relief demand forecasting is vital to the success of disaster relief operations, but it is associated with challenges including insufficient training samples, incomplete and imprecise inputs, and inaccurate demands. This article presents a co-evolutionary fuzzy deep transfer learning (CoFDTL) method for relief demand forecasting where different types of disasters (e.g., earthquake, typhoon, and flood) are considered as different tasks. CoFDTL consists of three stages. First, a deep fuzzy learning model is used to learn latent representation of the shared inputs of all tasks. Second, a co-evolutionary algorithm is used to simultaneously learn task-specific features and the shared regressor. Third, the shared regressor is re-trained based on the best solutions obtained for different tasks in the second stage. Experiments demonstrate that CoFDTL exhibits significant performance improvements over the selected popular fuzzy learning, deep learning, and transfer learning models. This article also reports the application of CoFDTL to two real-world disasters in China, 2018. The proposed CoFDTL that integrates fuzzy deep learning, transfer learning, and co-evolutionary learning can be used for many other complex multi-task transfer learning problems with insufficient samples and uncertain information.},
  archive      = {J_TETC},
  author       = {Yu-Jun Zheng and Si-Lan Yu and Qin Song and Yu-Jiao Huang and Wei-Guo Sheng and Sheng-Yong Chen},
  doi          = {10.1109/TETC.2021.3085337},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1361-1373},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Co-evolutionary fuzzy deep transfer learning for disaster relief demand forecasting},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-speed memristive ternary content addressable memory.
<em>TETC</em>, <em>10</em>(3), 1349–1360. (<a
href="https://doi.org/10.1109/TETC.2021.3085252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an ultra-high-speed memristor-based non-volatile Ternary Content Addressable Memory (TCAM) for use in real-time and big-data applications that require low power dissipation and fast data retrieval. It proposes a novel memristive TCAM (MTCAM) cell using memristors as a bit storage device along with an ultra-fast match line sense amplifier to minimize search time. SPICE simulation on 45 nm technology show that the search delay on a 144-bit proposed MTCAM at a supply voltage of 1 V and a sense margin of 140 mV is 175 ps with per bit search energy of 1.2 fJ. It is 1.12× times faster and dissipates 67 percent less search energy per bit than the fastest existing 144-bit MTCAM design.},
  archive      = {J_TETC},
  author       = {Krishna P. Gnawali and Spyros Tragoudas},
  doi          = {10.1109/TETC.2021.3085252},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1349-1360},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {High-speed memristive ternary content addressable memory},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GRAVITAS: Graphical reticulated attack vectors for
internet-of-things aggregate security. <em>TETC</em>, <em>10</em>(3),
1331–1348. (<a href="https://doi.org/10.1109/TETC.2021.3082525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet-of-Things (IoT) and cyber-physical systems (CPSs) may consist of thousands of devices connected in a complex network topology. The diversity and complexity of these components present an enormous attack surface, allowing an adversary to exploit security vulnerabilities of different devices to execute a potent attack. Though significant efforts have been made to improve the security of individual devices in these systems, little attention has been paid to security at the aggregate level. In this article, we describe a comprehensive risk management system, called GRAVITAS, for IoT/CPS that can identify undiscovered attack vectors and optimize the placement of defenses within the system for optimal performance and cost. While existing risk management systems consider only known attacks, our model employs a machine learning approach to extrapolate undiscovered exploits, enabling us to identify attacks overlooked by manual penetration testing (pen-testing). The model is flexible enough to analyze practically any IoT/CPS and provide the system administrator with a concrete list of suggested defenses that can reduce system vulnerability at optimal cost. GRAVITAS can be employed by governments, companies, and system administrators to design secure IoT/CPS at scale, providing a quantitative measure of security and efficiency in a world where IoT/CPS devices will soon be ubiquitous.},
  archive      = {J_TETC},
  author       = {Jacob Brown and Tanujay Saha and Niraj K. Jha},
  doi          = {10.1109/TETC.2021.3082525},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1331-1348},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {GRAVITAS: Graphical reticulated attack vectors for internet-of-things aggregate security},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximate recursive multipliers using low power building
blocks. <em>TETC</em>, <em>10</em>(3), 1315–1330. (<a
href="https://doi.org/10.1109/TETC.2022.3186240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate computing, frequently used in error tolerant applications, aims to achieve higher circuit performances by allowing the possibility of inaccurate results, rather than guaranteeing a correct outcome. Many contributions target the binary multiplier aiming to minimize the complexity of this common yet power-hungry circuit. Approximate recursive multipliers are low-power designs that exploit approximate building blocks to scale up to their final size. In this paper, we present two novel 4×4 approximate multipliers obtained by carry manipulation. They are used to compose 8×8 designs with different error-performance trade-off. The final circuits exhibit a competitive behavior in terms of error while reducing the power dissipation when compared to state-of-the-art proposals. The proposed multipliers and state-of-the-art designs found in the literature, have been synthesized targeting a 14nm FinFET technology to determine the electrical characteristics. Compared with an exact 8×8 multiplier, the least dissipative design proposed in this paper reduces power consumption and silicon area by 46%, and minimum delay by 21%. It also consumes 14% less power than the least power-hungry recursive circuit found in the literature, while offering 81% higher accuracy. Ιmage processing applications and a convolutional neural network are shown to demonstrate the effectiveness of the proposed multipliers.},
  archive      = {J_TETC},
  author       = {Efstratios Zacharelos and Italo Nunziata and Gerardo Saggese and Antonio G.M. Strollo and Ettore Napoli},
  doi          = {10.1109/TETC.2022.3186240},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1315-1330},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Approximate recursive multipliers using low power building blocks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A BF16 FMA is all you need for DNN training. <em>TETC</em>,
<em>10</em>(3), 1302–1314. (<a
href="https://doi.org/10.1109/TETC.2022.3187770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fused Multiply-Add (FMA) functional units constitute a fundamental hardware component to train Deep Neural Networks (DNNs). Its silicon area grows quadratically with the mantissa bit count of the computer number format, which has motivated the adoption of the BrainFloat16 format (BF16). BF16 features 1 sign, 8 exponent and 7 explicit mantissa bits. Some approaches to train DNNs achieve significant performance benefits by using the BF16 format. However, these approaches must combine BF16 with the standard IEEE 754 Floating-Point 32-bit (FP32) format to achieve state-of-the-art training accuracy, which limits the impact of adopting BF16. This article proposes the first approach able to train complex DNNs entirely using the BF16 format. We propose a new class of FMA operators, &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\mathrm{FMA}^{\mathrm {bf}16}_{\mathrm{n}\_\mathrm{m}}$&lt;/tex-math&gt;&lt;/inline-formula&gt; , that entirely rely on BF16 FMA hardware instructions and deliver the same accuracy as FP32. &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\mathrm{FMA}^{\mathrm {bf}16}_{\mathrm{n}\_\mathrm{m}}$&lt;/tex-math&gt;&lt;/inline-formula&gt; operators achieve performance improvements within the 1.28-1.35× range on ResNet101 with respect to FP32. &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\mathrm{FMA}^{\mathrm {bf}16}_{\mathrm{n}\_\mathrm{m}}$&lt;/tex-math&gt;&lt;/inline-formula&gt; enables training complex DNNs on simple low-end hardware devices without requiring expensive FP32 FMA functional units.},
  archive      = {J_TETC},
  author       = {John Osorio and Adrià Armejach and Eric Petit and Greg Henry and Marc Casas},
  doi          = {10.1109/TETC.2022.3187770},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1302-1314},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A BF16 FMA is all you need for DNN training},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generating very large RNS bases. <em>TETC</em>,
<em>10</em>(3), 1289–1301. (<a
href="https://doi.org/10.1109/TETC.2022.3187072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Residue Number Systems (RNS) are proven to be effective in speeding up computations involving additions and products. For these representations, there exists efficient modular reduction algorithms that can be used in the context of arithmetic over finite fields or modulo large numbers, especially when used in the context of cryptographic engineering. Their independence allows random draws of bases, which also makes it possible to protect against side-channel attacks, or even to detect them using redundancy. These systems are easily scalable, however the existence of large bases for some specific uses remains a difficult question. In this article, we present four techniques to extract RNS bases from specific sets of integers, giving better performance and flexibility to previous works in the litterature. While our techniques do not allow to solve efficiently every possible case, we provide techniques to provably and efficiently find the largest possible available RNS bases in several cases, improving the state-of-the-art on various works of the recent literature.},
  archive      = {J_TETC},
  author       = {Jean Claude Bajard and Kazuhide Fukushima and Thomas Plantard and Arnaud Sipasseuth},
  doi          = {10.1109/TETC.2022.3187072},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1289-1301},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Generating very large RNS bases},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An alternative approach to polynomial modular number system
internal reduction. <em>TETC</em>, <em>10</em>(3), 1278–1288. (<a
href="https://doi.org/10.1109/TETC.2022.3190368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Polynomial Modular Number System (PMNS) is an alternative to the binary multi-precision representation that allows to transport the arithmetic of a finite field to a polynomial ring. The most important operation in that system is the internal reduction that follows any arithmetic operation. All recent works on the subject use the same algorithm derived from Montgomery&#39;s modular multiplications to perform this internal reduction. This paper designs and analyzes two alternative algorithms to perform the internal reduction, both based on Babai&#39;s Closest Vector algorithms. It allows to significantly reduce the number of additions needed to perform this operation. A comprehensive experimental analysis shows that one of those algorithms is also faster in practice. For that matter, a C code generation tool has been developed in order to produce implementations for any prime field.},
  archive      = {J_TETC},
  author       = {Nicolas Méloni},
  doi          = {10.1109/TETC.2022.3190368},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1278-1288},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {An alternative approach to polynomial modular number system internal reduction},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PMNS for efficient arithmetic and small memory cost.
<em>TETC</em>, <em>10</em>(3), 1263–1277. (<a
href="https://doi.org/10.1109/TETC.2022.3187786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Polynomial Modular Number System (PMNS) is an integer number system which aims to speed up arithmetic operations modulo a prime &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$p$&lt;/tex-math&gt;&lt;/inline-formula&gt; . Such a system is defined by a tuple &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$(p, n, \gamma, \rho, E)$&lt;/tex-math&gt;&lt;/inline-formula&gt; , where &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$p$&lt;/tex-math&gt;&lt;/inline-formula&gt; , &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$n$&lt;/tex-math&gt;&lt;/inline-formula&gt; , &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\gamma$&lt;/tex-math&gt;&lt;/inline-formula&gt; and &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\rho$&lt;/tex-math&gt;&lt;/inline-formula&gt; are positive integers, &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$E\in \mathbb {Z}[X]$&lt;/tex-math&gt;&lt;/inline-formula&gt; , with &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$E(\gamma) \equiv 0 \pmod p$&lt;/tex-math&gt;&lt;/inline-formula&gt; . In (Didier, et al. 2020) conditions required to build efficient AMNS (PMNS with &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$E(X)=X^{n} - \lambda$&lt;/tex-math&gt;&lt;/inline-formula&gt; , where &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\lambda \in \mathbb {Z}\setminus \lbrace 0\rbrace$&lt;/tex-math&gt;&lt;/inline-formula&gt; ) are provided. In this paper, we generalise their approach for any monic polynomial &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$E\in \mathbb {Z}[X]$&lt;/tex-math&gt;&lt;/inline-formula&gt; of degree &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$n$&lt;/tex-math&gt;&lt;/inline-formula&gt; . We present new bounds and highlight a set of polynomials &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$E$&lt;/tex-math&gt;&lt;/inline-formula&gt; for very efficient operations in the PMNS and low memory requirement. We also provide AMNS and PMNS modular multiplication implementations, for a prime of size 256 bits, in classic C. We also provide, for the same prime, the first implementation taking advantage of the SIMD AVX512 instruction set. The AVX512 PMNS is 72 % faster than its AMNS counterpart (classical C version). This version presents a more than 60 % speed-up in comparison with the state-of-the-art Montgomery-CIOS modular multiplication of the GMP library.},
  archive      = {J_TETC},
  author       = {Fangan Yssouf Dosso and Jean-Marc Robert and Pascal Véron},
  doi          = {10.1109/TETC.2022.3187786},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1263-1277},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {PMNS for efficient arithmetic and small memory cost},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bounding the round-off error of the upwind scheme for
advection. <em>TETC</em>, <em>10</em>(3), 1253–1262. (<a
href="https://doi.org/10.1109/TETC.2022.3191472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical simulations are carefully-written programs, and their correctness is based on mathematical results. Nevertheless, those programs rely on floating-point arithmetic and the corresponding round-off errors are often ignored. This article deals with a specific simple scheme applied to advection, that is a particular equation from hydrodynamics dedicated to the transport of a substance. It shows a tight bound on the round-off error of the 1D and 2D upwind scheme, with an error roughly proportional to the number of steps. The error bounds are generic with respect to the format and exceptional behaviors are taken into account. Some experiments give an insight of the quality of the bounds.},
  archive      = {J_TETC},
  author       = {Louise Ben Salem-Knapp and Sylvie Boldo and William Weens},
  doi          = {10.1109/TETC.2022.3191472},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1253-1262},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Bounding the round-off error of the upwind scheme for advection},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PERCIVAL: Open-source posit RISC-v core with quire
capability. <em>TETC</em>, <em>10</em>(3), 1241–1252. (<a
href="https://doi.org/10.1109/TETC.2022.3187199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The posit representation for real numbers is an alternative to the ubiquitous IEEE 754 floating-point standard. In this work, we present PERCIVAL, an application-level posit RISC-V core based on CVA6 that can execute all posit instructions, including the quire fused operations. This solves the obstacle encountered by previous works, which only included partial posit support or which had to emulate posits in software. In addition, Xposit, a RISC-V extension for posit instructions is incorporated into LLVM. Therefore, PERCIVAL is the first work that integrates the complete posit instruction set in hardware. These elements allow for the native execution of posit instructions as well as the standard floating-point ones, further permitting the comparison of these representations. FPGA and ASIC synthesis show the hardware cost of implementing 32-bit posits and highlight the significant overhead of including a quire accumulator. However, results show that the quire enables a more accurate execution of dot products. In general matrix multiplications, the accuracy error is reduced up to 4 orders of magnitude. Furthermore, performance comparisons show that these accuracy improvements do not hinder their execution, as posits run as fast as single-precision floats and exhibit better timing than double-precision floats, thus potentially providing an alternative representation.},
  archive      = {J_TETC},
  author       = {David Mallasén and Raul Murillo and Alberto A. Del Barrio and Guillermo Botella and Luis Piñuel and Manuel Prieto-Matias},
  doi          = {10.1109/TETC.2022.3187199},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1241-1252},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {PERCIVAL: Open-source posit RISC-V core with quire capability},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guest editorial: Special section on emerging and impacting
trends on computer arithmetic. <em>TETC</em>, <em>10</em>(3), 1239–1240.
(<a href="https://doi.org/10.1109/TETC.2022.3195414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special section focus on emerging and impacting trends on computer arithmetic The computer arithmetic field encompasses the definition and standardization of arithmetic systems for computers. It also deals with issues pertaining to hardware and software implementations, testing, and verification. Researchers and practitioners of this field also work on challenges associated with using Computer Arithmetic to perform scientific and engineering calculations. As such, Computer Arithmetic can be regarded as a truly multidisciplinary field, which builds upon mathematics, computer science and electrical engineering. Thus, the range of topics addressed by Computer Arithmetic is generally very broad, spanning from highly theoretical to extremely practical contributions. Computer Arithmetic has been an active research field since the advent of computers, and it is progressively evolving following continuous advancements in technology.},
  archive      = {J_TETC},
  author       = {Stuart Oberman and Leonel Sousa and Bogdan Pasca and Alberto Nannarelli},
  doi          = {10.1109/TETC.2022.3195414},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {1239-1240},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Guest editorial: Special section on emerging and impacting trends on computer arithmetic},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AxRMs: Approximate recursive multipliers using
high-performance building blocks. <em>TETC</em>, <em>10</em>(2),
1229–1235. (<a href="https://doi.org/10.1109/TETC.2021.3096515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recursive multipliers (RMs) have been classified as a class of low-power multipliers because they provide a wide-range of power-quality configuration options. 2×2 multipliers are the constitutional building blocks of this recursive topology; however, most of the state-of-the-art approximate recursive designs are based on a 4×4 building blocks. Therefore, the design space exploration of AxRMs using 2×2 multipliers is still an open-research problem. To add the configurability and flexibility in the design of AxRMs such 2-bit multipliers are required that exhibit high-performance and low-area. In this article, two approximate 2×2 multipliers are proposed that exhibit double-sided error distribution feature. Compared to the existing best-approximated 2×2 multiplier, the proposed design achieves a reduction of 52 percent in area and exhibits an improvement of 25 percent in terms of delay while having a bounded error behavior. Then, three 8×8 multipliers of variable accuracy are designed using different configurations of approximate 2×2 multiplier. AxRM1 is the most-accurate design; an improvement of 50 percent in terms of mean relative error distance (MRED) is achieved compared to the existing best MRED-optimized design. AxRM3 has similar MRED compared to the previous best 2×2-based AxRM (called MACISH); however, AxRM3 exhibits 13 percent better PDP than MACISH due to the use of low-power and high-performance 2×2 multipliers in building larger multipliers. The proposed approximate multipliers are applied to cutting-edge error-tolerant application, i.e., convolutional neural networks. AxRM2 provides the best quality-power trade-off, 32.64 percent power savings are achieved with 1.10 percent better classification accuracy.},
  archive      = {J_TETC},
  author       = {Haroon Waris and Chenghua Wang and Chenyu Xu and Weiqiang Liu},
  doi          = {10.1109/TETC.2021.3096515},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {1229-1235},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {AxRMs: Approximate recursive multipliers using high-performance building blocks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient hardware implementation of finite field arithmetic
&lt;inline-formula&gt;&lt;tex-math notation=“LaTeX”&gt;<span
class="math inline"><em>A</em><em>B</em> + <em>C</em></span>&lt;/tex-math&gt;&lt;mml:math
xmlns:mml=“http://www.w3.org/1998/math/MathML”&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic
xlink:href=“xie-ieq1-3091982.gif”
xmlns:xlink=“http://www.w3.org/1999/xlink”/&gt;&lt;/inline-formula&gt;
for binary ring-LWE based post-quantum cryptography. <em>TETC</em>,
<em>10</em>(2), 1222–1228. (<a
href="https://doi.org/10.1109/TETC.2021.3091982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-quantum cryptography (PQC) has gained significant attention from the community recently as it is proven that the existing public-key cryptosystems are vulnerable to the attacks launched from the well-developed quantum computers. The finite field arithmetic &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$AB+C$&lt;/tex-math&gt;&lt;/inline-formula&gt; , where &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$A$&lt;/tex-math&gt;&lt;/inline-formula&gt; and &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$C$&lt;/tex-math&gt;&lt;/inline-formula&gt; are integer polynomials and &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$B$&lt;/tex-math&gt;&lt;/inline-formula&gt; is a binary polynomial, is the key component for the binary Ring-learning-with-errors (BRLWE)-based encryption scheme (a low-complexity PQC suitable for emerging lightweight applications). In this paper, we propose a novel hardware implementation of the finite field arithmetic &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$AB+C$&lt;/tex-math&gt;&lt;/inline-formula&gt; through three stages of interdependent efforts: (i) a rigorous mathematical formulation process is presented first; (ii) an efficient hardware architecture is then presented with detailed description; (iii) a thorough implementation has also been given along with the comparison. Overall, (i) the proposed basic structure ( &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$u=1$&lt;/tex-math&gt;&lt;/inline-formula&gt; ) outperforms the existing designs, e.g., it involves 55.9% less area-delay product (ADP) than [13] for &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$n=512$&lt;/tex-math&gt;&lt;/inline-formula&gt; ; (ii) the proposed design also offers very efficient performance in time-complexity and can be used in many future applications.},
  archive      = {J_TETC},
  author       = {Jiafeng Xie and Pengzhou He and Xiaofang Wang and José L. Imaña},
  doi          = {10.1109/TETC.2021.3091982},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {1222-1228},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Efficient hardware implementation of finite field arithmetic &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$AB+C$&lt;/tex-math&gt;&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=&quot;xie-ieq1-3091982.gif&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;/&gt;&lt;/inline-formula&gt; for binary ring-LWE based post-quantum cryptography},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parallel computation in the racetrack memory. <em>TETC</em>,
<em>10</em>(2), 1216–1221. (<a
href="https://doi.org/10.1109/TETC.2021.3078061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Racetrack memories are promising candidates for next-generation solid-state storage devices. Various racetrack memories have been proposed in the literature, skyrmion based or domain wall based. However, none of them show integrated computing capabilities. Here, we introduce a new domain wall based racetrack concept that can operate both as a memory and as a computing device. The computation is defined by changing locally the anisotropy of the film. Stray fields from nearby cells are exploited to implement reconfigurable logic gates. We demonstrate that the racetrack array can operate in parallel in every cell. This is achieved by an external out-of-plane Zeeman field applied to the array. As proof-of-principle, we verified the single computing cell and multiple connected cells operating in parallel by micromagnetic simulations. Logic NAND/NOR is implemented independently in every computing cell. This study provides the guidelines for the development and optimization of this family of logic gates.},
  archive      = {J_TETC},
  author       = {Fabrizio Riente and Giovanna Turvani and Marco Vacca and Mariagrazia Graziano},
  doi          = {10.1109/TETC.2021.3078061},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {1216-1221},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Parallel computation in the racetrack memory},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hardware constructions for error detection in lightweight
welch-gong (WG)-oriented streamcipher WAGE benchmarked on FPGA.
<em>TETC</em>, <em>10</em>(2), 1208–1215. (<a
href="https://doi.org/10.1109/TETC.2021.3073163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Providing an acceptable level of security at low cost becomes a challenge in embedded systems that have limited resources, e.g., Internet of Things application devices. Lightweight cryptography aims to overcome this challenge by adopting security measures which are well suited for resource-constrained usage models. As we move towards standardizing the approach for lightweight cryptography, several cipher implementations were proposed to NIST, out of which WAGE is one of the algorithms advanced to the next round. WAGE is a 259-bit lightweight stream cipher that derives its cryptographic properties from the WG stream cipher and is designed to provide Authenticated Encryption with Associated Data (AEAD) in hardware implementation. In this article, we present error detection schemes for the nonlinear sub-blocks of WAGE cipher for secure implementations of the cipher on hardware. The proposed signature-based error detection schemes are platform oblivious. Derivations for signature and interleaved signature schemes for both logic- and LUT-based implementations are presented for the 7-bit S-Box and Welch- Gong permutation (WGP) of WAGE. The presented schemes are evaluated for error coverage and are benchmarked on field-programmable gate array (FPGA) which show acceptable overheads and practical error coverage.},
  archive      = {J_TETC},
  author       = {Jasmin Kaur and Ausmita Sarker and Mehran Mozaffari Kermani and Reza Azarderakhsh},
  doi          = {10.1109/TETC.2021.3073163},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {1208-1215},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Hardware constructions for error detection in lightweight welch-gong (WG)-oriented streamcipher WAGE benchmarked on FPGA},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An inexact newton method for unconstrained total
variation-based image denoising by approximate addition. <em>TETC</em>,
<em>10</em>(2), 1192–1207. (<a
href="https://doi.org/10.1109/TETC.2021.3079715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Inexact Newton method using the conjugate gradient has been widely used to solve large-scale unconstrained optimization problems, such as the total variation-based image denoising. To improve energy and latency, this paper initially proposes an additional step length parameter ( &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\alpha $&lt;/tex-math&gt;&lt;/inline-formula&gt; ), such that the required number of iterations (and therefore its energy dissipation) decreases. Then, a floating-point adder (32-bits) made of approximate or truncated cells is utilized to reduce the energy dissipation in each iteration. These two techniques are finally combined to reduce the total processing time and energy dissipation for image denoising. The results show that &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\ \alpha $&lt;/tex-math&gt;&lt;/inline-formula&gt; significantly reduces the number of iterations; the proposed technique is tested on a set of images taken from a public domain library and is found that when 1.39&lt; &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\alpha $&lt;/tex-math&gt;&lt;/inline-formula&gt; &lt;1.45, the number of iterations is the lowest. Moreover, the energy dissipation of the denoising algorithm decreases by applying an approximate adder at a very small loss of accuracy and quality of the output image; the number of iterations remains constant when the number of approximate or truncated cells in the least significant positions (given by so-called NAB) is below 10. Irrespective of the noise level and approximate cell type, the quality of the output images does not incur in a significant degradation when NAB&lt;18.},
  archive      = {J_TETC},
  author       = {Junqi Huang and Haider A.F. Almurib and T. Nandha Kumar and Fabrizio Lombardi},
  doi          = {10.1109/TETC.2021.3079715},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {1192-1207},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {An inexact newton method for unconstrained total variation-based image denoising by approximate addition},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient error-resilient bus coding method using bit-basis
orthogonal integrative multiplexing. <em>TETC</em>, <em>10</em>(2),
1178–1191. (<a href="https://doi.org/10.1109/TETC.2021.3078085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interconnection technologies must consider reliability problems in dense on-chip networks, where a large number of routing elements constitute the global tracks for bus transactions through high-speed and low-swing signals. As the density of device integration continues to evolve and even employ multilevel states, technologies must effectively cope with dynamic errors to maintain system reliability. The primary objective of this study is to develop an atomic hardware multiplexer to decrease signal errors that can be applied to fault-tolerant routing resources in the digital domain. We propose a new bus coding method named bit-basis code division multiplexing (BCDM) and multidimensional expansions to integrate multi-source digital signals. The concurrent multiplexing techniques offered here can enhance the random noise immunity of the existing differential and duplicating bus transactions. Additionally, we demonstrate that simple parity and receiving integration improve the applicability of the BCDM. In numerical simulations, the proposed BCDM variants exhibited lower error rates and fewer transmission costs than existing methods. In a mesh-type spiking neural network, these methods can be used to multiplex the individual router taps and provide immunity from random noise. In a 32-nm CMOS technology library, the gate-count overhead of the multiplexing operators was only 3.6 percent of the baseline design.},
  archive      = {J_TETC},
  author       = {Jong Kang Park and Duckyong Kim and Jong Tae Kim},
  doi          = {10.1109/TETC.2021.3078085},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {1178-1191},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Efficient error-resilient bus coding method using bit-basis orthogonal integrative multiplexing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PVMC: Task mapping and scheduling under process variation
heterogeneity in mixed-criticality systems. <em>TETC</em>,
<em>10</em>(2), 1166–1177. (<a
href="https://doi.org/10.1109/TETC.2021.3072286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embedded Systems (ESs) have migrated from special-purpose hardware to commodity hardware. These systems have also tended to Mixed-Criticality (MC) implementations, executing applications of different criticalities upon a shared platform. Multi-cores, which are commonly used to design MC Systems (MCSs), bring out new challenges due to the Process Variation (PV). Power and frequency asymmetry affects the predictability of ESs. In this work, variation-aware techniques are explored to not only improve the reliability of MCSs, but also aid the scheduling and energy saving of them. We leverage the Core-to-Core (C2C) variations to protect high-criticality tasks and provide full service for a high percentage of low-criticality tasks. We formulate a constrained Integer Linear Program (ILP) and propose an optimization heuristic for task mapping and scheduling under PV in Mixed-Criticality systems (PVMC). Our proposed techniques also guarantee timing, reliability, and Thermal Design Power (TDP) constraints by considering the impact of task mapping in variation-affected platforms on system reliability and peak power consumption. Experiments demonstrate that our ILP framework and PVMC algorithm can greatly improve the schedulability and the overall Quality-of-Service (QoS), and provide energy saving up to 27.1 percent under different quantities of PV compared with a state-of-the-art algorithm.},
  archive      = {J_TETC},
  author       = {Fahimeh Bahrami and Behnaz Ranjbar and Nezam Rohbani and Alireza Ejlali},
  doi          = {10.1109/TETC.2021.3072286},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {1166-1177},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {PVMC: Task mapping and scheduling under process variation heterogeneity in mixed-criticality systems},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Application of elastic shape analysis to user authentication
and identification. <em>TETC</em>, <em>10</em>(2), 1157–1165. (<a
href="https://doi.org/10.1109/TETC.2021.3074242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different methods of user identification and authentication are widely used in security systems. Touch screen-based methods are very convenient for users and do not require specialized equipment, although commonly used pattern locks alone provide a low level of security. Analysis of biometric features of performed gestures can greatly improve the security of touch screen-based identification and authentication, however typically a large amount of training samples is needed to build an accurate model. In this study a novel method based on Elastic Shape Analysis and covariance shrinkage is introduced to overcome this limitation. Two dataset (one with finger gestures and one with stylus gestures) were used. It is shown that 4 training samples are sufficient for identification among 24 people with accuracy of 84.7 percent using elastic Linear Discriminant Analysis. In user authentication, the area under the curve (AUC) score of 0.986 was reached using an elastic variant of Principal Component Analysis and 8 training samples per user. The approach can be easily adapted as a part of a touch-screen based user authentication or identification system.},
  archive      = {J_TETC},
  author       = {Krzysztof Rzecki and Mateusz Baran},
  doi          = {10.1109/TETC.2021.3074242},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {1157-1165},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Application of elastic shape analysis to user authentication and identification},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reinforcement learning-based mobile edge computing and
transmission scheduling for video surveillance. <em>TETC</em>,
<em>10</em>(2), 1142–1156. (<a
href="https://doi.org/10.1109/TETC.2021.3073744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video surveillance plays an important role in Industrial Internet of Things (IIOT). The entire system is demanded to output useful surveillance information in a timely and accurate manner, thus having high requirements of enough computation and communication resources. In this work, we propose a mobile edge computing (MEC) based video surveillance network for face recognition applications, consisting of multiple camera sensors, relays, and the mobile edge server. Considering the limited computation and communication resources, we make joint decision for task offloading, wireless channel allocation, and image compression rate selection to obtain high average recognition accuracy and low average process delay. We adopt different image recognition algorithms for both camera sensors and the MEC server according to their own computation ability, and utilize the deep Q-network (DQN) algorithm as the decision-making module. Besides, we propose a two-layer hierarchical learning framework, i.e.,DQN and layers based on back propagation neural network (DQN+NN) algorithm, to reduce the curse of dimensionality. Experimental results show that both the DQN and DQN+NN algorithms can efficiently handle multiple computation tasks with limited computation and communication resources in intelligence video surveillance scenarios, and the DQN+NN algorithm performs better in terms of the convergence and training efficiency.},
  archive      = {J_TETC},
  author       = {Kunpeng Yan and Hangguan Shan and Tengxu Sun and Haoji Hu and Yingxiao Wu and Lu Yu and Zhaoyang Zhang and Tony Q. S. Quek},
  doi          = {10.1109/TETC.2021.3073744},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {1142-1156},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Reinforcement learning-based mobile edge computing and transmission scheduling for video surveillance},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-person activity recognition in continuously monitored
smart homes. <em>TETC</em>, <em>10</em>(2), 1130–1141. (<a
href="https://doi.org/10.1109/TETC.2021.3072980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Activity recognizers are challenging to design for continuous, in-home settings. However, they are notoriously difficult to create when there is more than one resident in the home. Despite recent efforts, there remains a need for an algorithm that can estimate the number of residents in the house, split a time series stream into separate substreams, and accurately identify each resident&#39;s activities. To address this challenge, we introduce Gamut . This novel unsupervised method jointly estimates the number of residents and associates sensor readings with those residents, based on a multi-target Gaussian mixture probability hypothesis density filter. We hypothesize that the proposed method will offer robust recognition for homes with two or more residents. In experiments with labeled data collected from 50 single-resident and 11 multi-resident homes, we observe that Gamut outperforms previous unsupervised and supervised methods, offering a robust strategy to track behavioral routines in complex settings.},
  archive      = {J_TETC},
  author       = {Tinghui Wang and Diane J. Cook},
  doi          = {10.1109/TETC.2021.3072980},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {1130-1141},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Multi-person activity recognition in continuously monitored smart homes},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DAICS: A deep learning solution for anomaly detection in
industrial control systems. <em>TETC</em>, <em>10</em>(2), 1117–1129.
(<a href="https://doi.org/10.1109/TETC.2021.3073017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning is emerging as an effective technique to detect sophisticated cyber-attacks targeting Industrial Control Systems (ICSs). The conventional detection approach is to learn the “normal” behaviour of the system, to be then able to label noteworthy deviations from it as anomalies. However, the normal behaviour of ICSs continuously evolves over time for multiple reasons, such as update/replacement of devices, workflow modifications or others. As a consequence, the accuracy of the anomaly detection process may be dramatically affected with a considerable amount of false alarms being generated. This article presents DAICS , a novel deep learning framework with a modular design to fit in large ICSs. The key component of the framework is a 2-branch neural network that learns the changes in the ICS behaviour with a small number of data samples and a few gradient updates. This is supported by an automatic tuning mechanism of the detection threshold that takes into account the changes in the prediction error under normal operating conditions. In this regard, no specialised human intervention is needed to update the other parameters of the system. DAICS has been evaluated using publicly available datasets and shows an increased detection rate and accuracy compared to state-of-the-art approaches, as well as higher robustness to additive noise.},
  archive      = {J_TETC},
  author       = {Maged Abdelaty and Roberto Doriguzzi-Corin and Domenico Siracusa},
  doi          = {10.1109/TETC.2021.3073017},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {1117-1129},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {DAICS: A deep learning solution for anomaly detection in industrial control systems},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy- and quality-efficient approximate multipliers for
neural network and image processing applications. <em>TETC</em>,
<em>10</em>(2), 1105–1116. (<a
href="https://doi.org/10.1109/TETC.2021.3072666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate computing is a new trend that trades off computational accuracy for lower energy dissipation and design complexity in various applications, where high precision is not a critical need. In this paper, energy- and quality- efficient approximate multipliers based on new approximate compressors are proposed. We use NAND gates for generating the complemented partial products, which reduces the number of transistors. Furthermore, new approximate compressors with different accuracy and performance characteristics are designed. Accordingly, three hybrid approximate multipliers offering different trade-offs between accuracy and hardware efficiency are proposed. The proposed designs are simulated using HSPICE with the 7nm FinFET model as a modern technology. Furthermore, the efficacies of the approximate multipliers in the neural network and image processing applications are evaluated using MATLAB. According to the results, the proposed designs provide far better compromises between the quality and energy metrics in comparison with the previous designs and can be considered as efficient alternatives for the exact multipliers in neural network and image processing applications.},
  archive      = {J_TETC},
  author       = {Mohammad Ahmadinejad and Mohammad Hossein Moaiyeri},
  doi          = {10.1109/TETC.2021.3072666},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {1105-1116},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Energy- and quality-efficient approximate multipliers for neural network and image processing applications},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A dynamically configurable PUF and dynamic matching
authentication protocol. <em>TETC</em>, <em>10</em>(2), 1091–1104. (<a
href="https://doi.org/10.1109/TETC.2021.3072421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A physical unclonable function (PUF) is a hardware security primitive, which can be used secure various hardware-based applications. As a type of PUFs, strong PUFs have a large number of challenge-response pairs (CRPs), which can be used for authentication. At present, most strong PUF structures follow a one-to-one input/output relationship, i.e., linear function. As such, strong PUF designs are vulnerable to machine learning (ML) based modeling attacks. To address the issue, a dynamically configurable PUF structure is proposed in this article. A mathematical model of the proposed dynamic PUF is presented and the design is proposed against the effective ML based attacks, such as deep neural network (DNN), logistic regression (LR) and reliability-based covariance matrix adaptation evolution strategies (CMA-ES). Experimental results on field programmable gate arrays (FPGAs) show that the proposed dynamic structure has achived good uniqueness and reliability. It is also shown that the dynamic PUF has a strong resistance to the CMA-ES attack. Due to the dynamic nature of the proposed PUF structure, an authentication protocol is also designed to generate recognizable authentication bits string. The protocol shows strong resistance to classical machine learning attacks including the new variant of CMA-ES.},
  archive      = {J_TETC},
  author       = {Yale Wang and Chenghua Wang and Chongyan Gu and Yijun Cui and Máire O&#39;Neill and Weiqiang Liu},
  doi          = {10.1109/TETC.2021.3072421},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {1091-1104},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A dynamically configurable PUF and dynamic matching authentication protocol},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Manifold-inspired search-based algorithm for automated test
case generation. <em>TETC</em>, <em>10</em>(2), 1075–1090. (<a
href="https://doi.org/10.1109/TETC.2021.3070968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated test case generation based on path coverage (ATCG-PC) is a black-box optimization problem whose difficulty is attributed to the one-to-many relationship between path and test cases. It results in a large number of redundant function evaluations in the search process of algorithms for ATCG-PC. To minimize the redundant function evaluations for solving ATCG-PC, the equivalent mapping subspaces are defined to decompose the search space according to the paths. Inspired by the data distribution hypothesis, we assume that the target path can be covered by searching in one neighborhood of a test case (equivalent mapping subspace) instead of the whole search space. This article presents a manifold-inspired search-based algorithm that finds the equivalent mapping subspaces with the test-case-path relationship matrix. Furthermore, the algorithm generates test cases covering all possible paths by searching in the found subspaces. The experimental results show the proposed algorithm has significantly lower function evaluation consumption than the state-of-the-art algorithms with the highest path coverage rate in two open-source toolkits and 16 open-source real-world programs. Several orders of magnitude of function evaluations can be saved by searching in the found equivalent mapping subspaces instead of exploring the whole search space.},
  archive      = {J_TETC},
  author       = {Fangqing Liu and Han Huang and Junpeng Su and Stuart D. Semujju and Zhongming Yang and Zhifeng Hao},
  doi          = {10.1109/TETC.2021.3070968},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {1075-1090},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Manifold-inspired search-based algorithm for automated test case generation},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mining implicit equations from data using gene expression
programming. <em>TETC</em>, <em>10</em>(2), 1058–1074. (<a
href="https://doi.org/10.1109/TETC.2021.3068651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbolic regression is an active research topic that has various applications in data mining and knowledge discovery. Existing methods for symbolic regression mainly focus on mining explicit equations. In contrast, implicit equations are more flexible and powerful than explicit equations with regard to describing the relationships between variables in the given dataset. However, evaluating the quality of an implicit equation is more difficult than evaluating an explicit equation. The traditional method for implicit equation evaluation is based on derivative calculation, which requires much time consumption and dense training data. To address the above issues, this article proposes a new evolutionary framework to efficiently mine implicit equations from data. In the proposed framework, a new mechanism (named CL-FEM) is proposed to evaluate implicit equations, and it can efficiently evaluate the accuracy and validity of implicit equations. In addition, a multiple sub-chromosome encoding method based on a least-squares estimator is proposed in the framework to further improve its search efficiency. Based on the proposed evolutionary framework, an efficient algorithm named gene expression programming with a least-squares estimator (LSE-GEP) is developed to mine implicit equations from data. Experimental results demonstrate that the proposed LSE-GEP method performs much better than the recently published methods, in terms of its success rate, accuracy and readability.},
  archive      = {J_TETC},
  author       = {Jinghui Zhong and Jiaquan Yang and Yongliang Chen and Wei-Li Liu and Liang Feng},
  doi          = {10.1109/TETC.2021.3068651},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {1058-1074},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Mining implicit equations from data using gene expression programming},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convolutional autoencoder-based transfer learning for
multi-task image inferences. <em>TETC</em>, <em>10</em>(2), 1045–1057.
(<a href="https://doi.org/10.1109/TETC.2021.3068063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern-recognition algorithms from machine learning play a prominent role in embedded sensing systems to derive inferences from sensor data. Very often, such systems face severe energy constraints, especially when dealing with high-dimensional data, such as images. The focus of this study is on reducing computational energy by exploiting the concept of transfer learning and energy-efficient dataflow accelerators. We show that the use of convolutional autoencoders can enable various opportunities to reduce computational energy and avoid significant reduction in inference performance when multiple task categories are targeted for inference. We validate our approach through a multi-task case study. The study targets a set of pictures with each picture containing four different task categories: gender, smile, glasses, and pose. In order to minimize inference time and computational energy, a convolutional autoencoder is used for learning a generalized representation of the images. Three scenarios are analyzed: transferring layers using convolutional autoencoders, transferring layers using convolutional neural networks trained on different tasks, and no layer transfer. We show that when the convolutional layers with one fully-connected layer are transferred using convolutional autoencoders, we can achieve a reduction of &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$6.58\times$&lt;/tex-math&gt;&lt;/inline-formula&gt; in computational energy, while improving performance by 1.98, 1.88, 4.11, and 1.47 percent for gender, smile, glasses, and pose inferences, respectively, as compared to the no-transfer method, when the number of training samples is small.},
  archive      = {J_TETC},
  author       = {Jie Lu and Naveen Verma and Niraj K. Jha},
  doi          = {10.1109/TETC.2021.3068063},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {1045-1057},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Convolutional autoencoder-based transfer learning for multi-task image inferences},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey of incentive mechanism design for federated
learning. <em>TETC</em>, <em>10</em>(2), 1035–1044. (<a
href="https://doi.org/10.1109/TETC.2021.3063517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is promising in enabling large-scale machine learning by massive clients without exposing their raw data. It can not only enable the clients to preserve the privacy information, but also achieve high learning performance. Existing works of federated learning mainly focus on improving learning performance in terms of model accuracy and learning task completion time. However, in practice, clients are reluctant to participate in the learning process without receiving compensation. Therefore, how to effectively motivate the clients to actively and reliably participate in federated learning is paramount. As compared to the current incentive mechanism design in other fields, such as crowdsourcing, cloud computing, smart grid, etc., the incentive mechanism for federated learning is more challenging. First, it is hard to evaluate the training data value of each client. Second, it is difficult to model the learning performance of different federated learning algorithms. In this article, we survey the incentive mechanism design for federated learning. In particular, we present a taxonomy of existing incentive mechanisms for federated learning, which are subsequently discussed in depth by comparing and contrasting different approaches. Finally, some future directions of how to incentivize clients in federated learning have been discussed.},
  archive      = {J_TETC},
  author       = {Yufeng Zhan and Jie Zhang and Zicong Hong and Leijie Wu and Peng Li and Song Guo},
  doi          = {10.1109/TETC.2021.3063517},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {1035-1044},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A survey of incentive mechanism design for federated learning},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A budget and deadline aware task assignment scheme for
crowdsourcing environment. <em>TETC</em>, <em>10</em>(2), 1020–1034. (<a
href="https://doi.org/10.1109/TETC.2021.3062843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the problem of assigning heterogeneous tasks with multiple skill requirements in crowdsourcing platforms. The aim is to find mutually exclusive, a highly-productive set of workers who can successfully complete the tasks within a given deadline and budget. We propose a timeline based weighted aggregation ( TWA ) technique to quantify the per-skill score of a worker. The score is computed based on the worker’s profile and past work experiences. Given the worker’s score, we formulate the problem as one of maximizing the productivity of all the &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$N$&lt;/tex-math&gt;&lt;/inline-formula&gt; given tasks. A two-stage approximation solution is proposed. In the first stage, we offer a greedy-based 2-approximation algorithm for a single task. In the second stage, a local ratio based algorithm is proposed to extend the solution for multiple tasks. The overall solution is shown to be 3-approximate. Finally, simulation results using real-world data are presented to highlight the efficacy of our proposed schemes.},
  archive      = {J_TETC},
  author       = {Akash Yadav and Joydeep Chandra and Ashok Singh Sairam},
  doi          = {10.1109/TETC.2021.3062843},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {1020-1034},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A budget and deadline aware task assignment scheme for crowdsourcing environment},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blockchain-based edge computing data storage protocol under
simplified group signature. <em>TETC</em>, <em>10</em>(2), 1009–1019.
(<a href="https://doi.org/10.1109/TETC.2021.3062348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contrast to the traditional cloud-based IoT structure, which imposes high computation and storage demands on the central cloud server, edge computing can process data at the network edge. In this article, we present an edge computing data storage protocol employing the blockchain and the group signature, where the blockchain works as the trusted third party, offering a convenient platform for the data storage and protection, and the group signature is utilized for privacy identity information protection. Unlike the common group signature scheme, we propose a notion of simplified group signature , which removes the traceable property to improve the computational efficiency. In edge computing, the edge devices and the end devices can easily set up a long-term trust relationship through the cloud services, so the traceability may be of little utility in this scenario. In our protocol, we also design a new data validation scheme that has a proof size of only &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$O(1)$&lt;/tex-math&gt;&lt;/inline-formula&gt; .},
  archive      = {J_TETC},
  author       = {Zhiwei Wang},
  doi          = {10.1109/TETC.2021.3062348},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {1009-1019},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Blockchain-based edge computing data storage protocol under simplified group signature},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-technique redirected walking method. <em>TETC</em>,
<em>10</em>(2), 997–1008. (<a
href="https://doi.org/10.1109/TETC.2021.3062285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Room-Scale locomotion method is one of the most realistic locomotion methods used in virtual reality technologies. This is due to the natural interaction obtained through the tracking of its controllers and the head-mounted display with six degrees of freedom. However, its mapping by position between the physical and the virtual world limits the user’s movement to the physical workspace provided by the corresponding device. Redirected Walking methods use gain algorithms that add modifications to the virtual movement of the user, optimizing the virtual workspace in the same physical workspace. In this article, we develop a new Redirected Walking method, which combines the modification of three gain algorithms (curvature gain, rotation gain, and translation gain), a new algorithm (deviation gain), a path predictive method of the user’s locomotion, a proportional distance to the center function, and a user direction smoothing function that softens the effect of the algorithms. Complementing the new method, we offer an automatic calibration system that allows the user to use our method in a personalized way.},
  archive      = {J_TETC},
  author       = {Jesus Mayor and Laura Raya and Sofia Bayona and Alberto Sanchez},
  doi          = {10.1109/TETC.2021.3062285},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {997-1008},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Multi-technique redirected walking method},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tolerating permanent faults with low-energy overhead in
multicore mixed-criticality systems. <em>TETC</em>, <em>10</em>(2),
985–996. (<a href="https://doi.org/10.1109/TETC.2021.3059724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the battery-operated nature of some embedded Mixed-Criticality Systems, simultaneous energy and reliability management is a crucial issue in designing these systems. We propose two comprehensive schemes, MC-2S and MC-4S, which exploit the standby-sparing technique to tolerate permanent faults through inherent redundancy of multicore systems and maintain the system&#39;s reliability against transient faults with low energy overhead. In these schemes, two copies of each high-criticality task are scheduled on different cores to guarantee their timeliness in case of permanent fault occurrence. To guarantee the quality of service of low-criticality tasks, in the MC-2S scheme, one backup copy is considered for each low-criticality task on another core at the design time and partitioned scheduling is employed. The MC-4S scheme exploits semi-partitioned scheduling in which low-criticality tasks migrate to other cores in case of permanent fault or overrun occurrence on one of the cores. We also develop a Demand Bound Function schedulability analysis to guarantee timeliness and propose a preference-oriented scheduling algorithm along with a reliability-aware DVFS method for energy saving. The proposed schemes provide up to 57 percent (39 percent on average) energy saving in comparison to other state-of-the-art methods and enhance the acceptance ratio of the system significantly.},
  archive      = {J_TETC},
  author       = {Amin Naghavi and Sepideh Safari and Shaahin Hessabi},
  doi          = {10.1109/TETC.2021.3059724},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {985-996},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Tolerating permanent faults with low-energy overhead in multicore mixed-criticality systems},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reformative noise-immune neural network for
equality-constrained optimization applied to image target detection.
<em>TETC</em>, <em>10</em>(2), 973–984. (<a
href="https://doi.org/10.1109/TETC.2021.3057395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Equality-constrained optimization problem captures increasing attention in the fields of computer science, control engineering, and applied mathematics. Almost all of the relevant issues suffer from kinds of intense or weak noises during the solving process, so that how to realize the noise deduction even noise elimination has increasingly become a sticky and significant problem. A lot of corresponding solving models are established for the equality-constrained optimization problem. However, the majority of them can find the optimal solution to a certain extent in the absence of noise disturbance, but few can behave a brilliant noise-resistance proficiency. On account of this discovery, a reformative noise-immune neural network (RNINN) model is constructed. In addition, the conventional gradient-based recursive neural network model and the zeroing recursive neural network model are presented to compare with the proposed RNINN model on convergence properties and noise-resistance capabilities. Lastly, the relative numerical experiment simulation and image target detection application are implemented to further elaborate on the robustness and efficiency of the RNINN model.},
  archive      = {J_TETC},
  author       = {Ying Liufu and Long Jin and Jinqiang Xu and Xiuchun Xiao and Dongyang Fu},
  doi          = {10.1109/TETC.2021.3057395},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {973-984},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Reformative noise-immune neural network for equality-constrained optimization applied to image target detection},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fully dynamic inference with deep neural networks.
<em>TETC</em>, <em>10</em>(2), 962–972. (<a
href="https://doi.org/10.1109/TETC.2021.3056031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern deep neural networks are powerful and widely applicable models that extract task-relevant information through multi-level abstraction. Their cross-domain success, however, is often achieved at the expense of computational cost, high memory bandwidth, and long inference latency, which prevents their deployment in resource-constrained and time-sensitive scenarios, such as edge-side inference and self-driving cars. While recently developed methods for creating efficient deep neural networks are making their real-world deployment more feasible by reducing model size, they do not fully exploit input properties on a per-instance basis to maximize computational efficiency and task accuracy. In particular, most existing methods typically use a one-size-fits-all approach that identically processes all inputs. Motivated by the fact that different images require different feature embeddings to be accurately classified, we propose a fully dynamic paradigm that imparts deep convolutional neural networks with hierarchical inference dynamics at the level of layers and individual convolutional filters/channels. Two compact networks, called Layer-Net (L-Net) and Channel-Net (C-Net), predict on a per-instance basis which layers or filters/channels are redundant and therefore should be skipped. L-Net and C-Net also learn how to scale retained computation outputs to maximize task accuracy. By integrating L-Net and C-Net into a joint design framework, called LC-Net, we consistently outperform state-of-the-art dynamic frameworks with respect to both efficiency and classification accuracy. On the CIFAR-10 dataset, LC-Net results in up to &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$11.9\times$&lt;/tex-math&gt;&lt;/inline-formula&gt; fewer floating-point operations (FLOPs) and up to 3.3 percent higher accuracy compared to other dynamic inference methods. On the ImageNet dataset, LC-Net achieves up to &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$1.4\times$&lt;/tex-math&gt;&lt;/inline-formula&gt; fewer FLOPs and up to 4.6 percent higher Top-1 accuracy than the other methods.},
  archive      = {J_TETC},
  author       = {Wenhan Xia and Hongxu Yin and Xiaoliang Dai and Niraj K. Jha},
  doi          = {10.1109/TETC.2021.3056031},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {962-972},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Fully dynamic inference with deep neural networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Developing practical multi-view learning for clinical
analytics in p4 medicine. <em>TETC</em>, <em>10</em>(2), 948–961. (<a
href="https://doi.org/10.1109/TETC.2021.3054761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the immersive experience of P4 medicine (predictive, preventive, personalized and participatory), various medical applications are now transforming from periodic in-hospital service into anytime self-monitoring systems, which generates rich personal medical data required to be processed. The emergence of edge computing enables the possibility of performing on-site processing for such data while preserving individual privacy. To automatically analyzing the data, machine learning methods were implemented to extract features from sensor readings and then performed on a concentrated feature space with all features. However, these approaches can easily suffer from over-fitting issues and ignore the differences of physical interpretation between different feature groups. In this article, we proposed a lightweight multi-stage, multi-view learning approach, called M3E, for processing data on the edge. Every stage of M3E helps the desired features to be finally acquired from the raw signals, as well as exploits the consistency and complementary proprieties of different views for getting better learning results. To study the performance, we evaluated the M3E approach on a real-world system and two real-world datasets, and the accuracy of prediction can reach 80.1, 79.11 and 72.63 percent respectively. Moreover, the experimental results also show that our multi-view approach outperforms single-view ones and can be easily extended to other medical cases.},
  archive      = {J_TETC},
  author       = {Yucen Nan and Wei Li and Lu Feng and Chengwen Luo and Jianqiang Li and Albert Y. Zomaya},
  doi          = {10.1109/TETC.2021.3054761},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {948-961},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Developing practical multi-view learning for clinical analytics in p4 medicine},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Secure data sharing with flexible user access privilege
update in cloud-assisted IoMT. <em>TETC</em>, <em>10</em>(2), 933–947.
(<a href="https://doi.org/10.1109/TETC.2021.3052377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-assisted Internet of Medical Things (IoMT) is becoming an emerging paradigm in the healthcare domain, which involves collection, storage and usage of the medical data. Considering the confidentiality and accessibility of the outsourced data, secure and fine-grained data sharing is a crucial requirement for the patients. Attribute-based encryption (ABE) is a promising solution to deal with this issue, but considering its property of each attribute sharing with multiple users, how to flexibly and efficiently update access privileges of certain users without affecting others is still a serious challenge. In this article, we propose a secure and fine-grained data sharing scheme with flexible user access privilege update in cloud-assisted IoMT environment. Specifically, we take ABE as the basic building block, and utilize proxy re-encryption and key blinding techniques to empower the cloud server to re-encrypt the ciphertext affected by revocation and update keys for unrevoked users. In addition, adding attributes for users to extend their access rights is realized only based on few key components stored in cloud without entirely re-computing and re-issuing keys for them. As a result, the patients are able to flexibly and efficiently share their data and manage users’ privileges. Formal proof and detailed performance evaluation demonstrate the security and efficiency of the proposed scheme.},
  archive      = {J_TETC},
  author       = {Jialu Hao and Wenjuan Tang and Cheng Huang and Jian Liu and Huimei Wang and Ming Xian},
  doi          = {10.1109/TETC.2021.3052377},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {933-947},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Secure data sharing with flexible user access privilege update in cloud-assisted IoMT},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DWES: A dynamic weighted evaluation system for scratch based
on computational thinking. <em>TETC</em>, <em>10</em>(2), 917–932. (<a
href="https://doi.org/10.1109/TETC.2020.3044588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scratch is a programming environment, which is widely adopted as the first language to enter the programming world. Since Scratch projects are rich in multimedia resources, it can be time-consuming to judge them by hand, requiring manual execution with the understanding of each project. Auto-judgment tools come into being, they can rapidly evaluate the project in some way. Unfortunately, the existing tools are too rigid and they ignore the diversity of Scratch projects. To address this issue, we propose a new dynamic weighted evaluation system (DWES). First, we define a new computational thinking (CT) evaluation criteria from eight aspects. Second, based on the proposed criteria, we present an analysis tool that automatically assesses the CT skills of the learners’ projects. Third, considering the characteristics of projects, we dynamically adjust the evaluation results according to types, so that a single standard is no longer applied roughly to all. This process is data-driven, we consider the CT performance and scripts of projects. We prove the rationality of the evaluation criteria from the aspect of program complexity. Through the correlation analysis between DWES CT scores and experts’ ratings, we find that compared with Dr. Scratch, the correlation coefficient has increased.},
  archive      = {J_TETC},
  author       = {Xiaolin Chai and Yan Sun and Hong Luo and Mohsen Guizani},
  doi          = {10.1109/TETC.2020.3044588},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {917-932},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {DWES: A dynamic weighted evaluation system for scratch based on computational thinking},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The effects of approximate multiplication on convolutional
neural networks. <em>TETC</em>, <em>10</em>(2), 904–916. (<a
href="https://doi.org/10.1109/TETC.2021.3050989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article analyzes the effects of approximate multiplication when performing inferences on deep convolutional neural networks (CNNs). The approximate multiplication can reduce the cost of the underlying circuits so that CNN inferences can be performed more efficiently in hardware accelerators. The study identifies the critical factors in the convolution, fully-connected, and batch normalization layers that allow more accurate CNN predictions despite the errors from approximate multiplication. The same factors also provide an arithmetic explanation of why bfloat16 multiplication performs well on CNNs. The experiments are performed with recognized network architectures to show that the approximate multipliers can produce predictions that are nearly as accurate as the FP32 references, without additional training. For example, the ResNet and Inception-v4 models with Mitch- &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$w$&lt;/tex-math&gt;&lt;/inline-formula&gt; 6 multiplication produces Top-5 errors that are within 0.2 percent compared to the FP32 references. A brief cost comparison of Mitch- &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$w$&lt;/tex-math&gt;&lt;/inline-formula&gt; 6 against bfloat16 is presented where a MAC operation saves up to 80 percent of energy compared to the bfloat16 arithmetic. The most far-reaching contribution of this article is the analytical justification that multiplications can be approximated while additions need to be exact in CNN MAC operations.},
  archive      = {J_TETC},
  author       = {Min Soo Kim and Alberto A. Del Barrio and HyunJin Kim and Nader Bagherzadeh},
  doi          = {10.1109/TETC.2021.3050989},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {904-916},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {The effects of approximate multiplication on convolutional neural networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning low resource consumption CNN through pruning and
quantization. <em>TETC</em>, <em>10</em>(2), 886–903. (<a
href="https://doi.org/10.1109/TETC.2021.3050770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models have evolved into powerful tools that can be used for many artificial intelligence tasks. However, deploying deep neural networks into real-world applications is still challenging due to their high computational complexity and storage overhead. Fortunately, a densely connected neural network can be converted into a sparsely connected network with low resource demand by the neural network compression. Since deep neural networks are complicated, compression mechanism should find a tradeoff between compression ratio and model accuracy. In this article, by analyzing the statistics of channel connection, we propose an interactive neural network compression mechanism including out-in-channel pruning and neural network quantization. Many channel pruning works apply structured sparsity regularization on each layer separately. We consider correlations between successive layers to retain predictive power of the compact network. A global greedy pruning algorithm is designed to remove redundant out-in-channels in an iterative way. Moreover, in order to solve the shortcomings of the one-shot quantization, we propose the incremental quantization algorithm in the dimension of the output channel, which can smooth network fluctuations and recover accuracy better during retraining. Our mechanism is comprehensively evaluated with various Convolutional Neural Networks (CNN) architectures on popular datasets. Notably, on ImageNet-1K, the out-in-channel pruning reduce 54.0 percent FLOPS on AlexNet and 50.0 percent FLOPs on ResNet-50 with only 0.15 and 0.37 percent top-1 accuracy drop respectively. On classification and style transfer tasks, the superiority of incremental quantization increases with the decrease of the number of quantization bits.},
  archive      = {J_TETC},
  author       = {Qi Qi and Yan Lu and Jiashi Li and Jingyu Wang and Haifeng Sun and Jianxin Liao},
  doi          = {10.1109/TETC.2021.3050770},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {886-903},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Learning low resource consumption CNN through pruning and quantization},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SHARKS: Smart hacking approaches for RisK scanning in
internet-of-things and cyber-physical systems based on machine learning.
<em>TETC</em>, <em>10</em>(2), 870–885. (<a
href="https://doi.org/10.1109/TETC.2021.3050733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber-physical systems (CPS) and Internet-of-Things (IoT) devices are increasingly being deployed across multiple functionalities, ranging from healthcare devices and wearables to critical infrastructures, e.g., nuclear power plants, autonomous vehicles, smart cities, and smart homes. These devices are inherently not secure across their comprehensive software, hardware, and network stacks, thus presenting a large attack surface that can be exploited by hackers. In this article, we present an innovative technique for detecting unknown system vulnerabilities, managing these vulnerabilities, and improving incident response when such vulnerabilities are exploited. The novelty of this approach lies in extracting intelligence from known real-world CPS/IoT attacks, representing them in the form of regular expressions, and employing machine learning (ML) techniques on this ensemble of regular expressions to generate new attack vectors and security vulnerabilities. Our results show that 10 new attack vectors and 122 new vulnerability exploits can be successfully generated that have the potential to exploit a CPS or an IoT ecosystem. The ML methodology achieves an accuracy of 97.4 percent and enables us to predict these attacks efficiently with an 87.2 percent reduction in the search space. We demonstrate the application of our method to the hacking of the in-vehicle network of a connected car. To defend against the known attacks and possible novel exploits, we discuss a defense-in-depth mechanism for various classes of attacks and the classification of data targeted by such attacks. This defense mechanism optimizes the cost of security measures based on the sensitivity of the protected resource, thus incentivizing its adoption in real-world CPS/IoT by cybersecurity practitioners.},
  archive      = {J_TETC},
  author       = {Tanujay Saha and Najwa Aaraj and Neel Ajjarapu and Niraj K. Jha},
  doi          = {10.1109/TETC.2021.3050733},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {870-885},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {SHARKS: Smart hacking approaches for RisK scanning in internet-of-things and cyber-physical systems based on machine learning},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Securing an accelerator-rich system from flooding-based
denial-of-service attacks. <em>TETC</em>, <em>10</em>(2), 855–869. (<a
href="https://doi.org/10.1109/TETC.2021.3049826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing demand for high performance and energy efficiency along with time-to-market pressures have led to a growing number of third-party accelerators in modern System-on-Chips (SoCs). However, the third-party accelerators may introduce vulnerabilities due to malicious intent or elusive design bugs that escape through verification processes. These vulnerabilities can be exploited by various attackers which can corrupt the entire system. Flooding is one such attack that is triggered by a malicious third-party accelerator to obstruct on-chip network communication by injecting useless packets leading to Denial-of-Service. To secure accelerator-rich SoCs from flooding attacks, we propose a two-step attack detection framework that leverages machine learning (ML) models to detect an attack scenario. The ML-based solution along with hierarchical feature data aggregation and feature packet prioritization provides an accurate attack detection with minimal performance overheads. Experimental evaluations with real accelerator benchmarks show the effectiveness of our framework achieving a detection accuracy of up to 97 percent across various ML classifiers.},
  archive      = {J_TETC},
  author       = {Mitali Sinha and Pramit Bhattacharyya and Sidhartha Sankar Rout and Neha Bhairavi Prakriya and Sujay Deb},
  doi          = {10.1109/TETC.2021.3049826},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {855-869},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Securing an accelerator-rich system from flooding-based denial-of-service attacks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A service-based joint model used for distributed learning:
Application for smart agriculture. <em>TETC</em>, <em>10</em>(2),
838–854. (<a href="https://doi.org/10.1109/TETC.2020.3048671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed analytics facilitate to make the data-driven services smarter for a wider range of applications in many domains, including agriculture. The key to producing services at such level is timely analysis for deriving insights from reliable data. Centralized data analytic services are becoming infeasible due to limitations in the Information and Communication Technologies (ICT) infrastructure, timeliness of the information, and data ownership. Distributed Machine Learning (DML) platforms facilitate efficient data analysis and overcome such limitations effectively. Federated Learning (FL) is a DML methodology, which enables optimizing resource consumption while performing privacy-preserved timely analytics. In order to create such services through FL, there needs to be innovative machine learning (ML) models as data complexity as well as application requirements limit the applicability of existing ML models. Even though NN-based models are highly advantageous, use of NN in FL settings is limited with thin clients (with less computational capabilities) and high-dimensional data (with large number of model parameters). Therefore, in this paper, we propose a novel Neural Network (NN)- and Partial Least Square (PLS) regression- based joint FL model (FL-NNPLS). Its predictive performance is evaluated under sequential- and parallel-updating based FL algorithms in a smart farming context for milk quality analysis. Smart farming is a fast-growing industrial sector which requires effective analytics platforms to enable sustainable farming practices. However, the use of advanced ML techniques are still at a early stage for improving the effectiveness of farming practices. Our FL-NNPLS approach performs and compares well with a centralized approach and demonstrates state-of-the-art performance.},
  archive      = {J_TETC},
  author       = {Dixon Vimalajeewa and Chamil Kulatunga and Donagh P. Berry and Sasitharan Balasubramaniam},
  doi          = {10.1109/TETC.2020.3048671},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {838-854},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A service-based joint model used for distributed learning: Application for smart agriculture},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning-based similarity attacks for chaos-based
cryptosystems. <em>TETC</em>, <em>10</em>(2), 824–837. (<a
href="https://doi.org/10.1109/TETC.2020.3048498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the chaotic block cryptographic algorithms are performed on hardware devices, the leakages of power consumption etc. are crucial information which can be used to analyse the security of the cryptosystems. Template Attack (TA) can recover the secret key. However, there are still some challenges for TA such as irreversible covariance matrix and exponentiation calculation overflow. Machine Learning-based Similarity Attacks (MLSAs) are proposed to effectively analyse the sensitive information of the chaotic block cryptosystem. The proposed method consists of three steps: parameter tuning, learning and attacking. For the parameter tuning, the profiling traces are categorised according to the Hamming weights of sensitive intermediate data. Then a 10-fold cross-validation is executed to determine the corresponding parameter settings for learning algorithms. In the learning step, the profiling traces and Hamming weight labels are used to train machine learning models, and in the attacking step different similarity measure methods are used to calculate similarities between actual and hypothetical Hamming weight labels to attack the secret keys. Performance analyses demonstrate that the proposed MLSAs have higher success rates than TA and lower computational time consumptions under most of scenarios. Therefore, the MLSAs can efficiently attack and analyse hardware security of chaotic block cryptosystems.},
  archive      = {J_TETC},
  author       = {Junxiu Liu and Shunsheng Zhang and Yuling Luo and Lvchen Cao},
  doi          = {10.1109/TETC.2020.3048498},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {824-837},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Machine learning-based similarity attacks for chaos-based cryptosystems},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A cellular automata guided finite-state-machine watermarking
strategy for IP protection of sequential circuits. <em>TETC</em>,
<em>10</em>(2), 806–823. (<a
href="https://doi.org/10.1109/TETC.2020.3046662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preventing the illegal use of Intellectual Property (IP) cores has been one of the most fundamental challenges for the semiconductor industry. Hardware watermarking is a viable and widely adopted solution to this problem, which can detect IP piracy by verifying the authorship proof of the design. Easy detectability and resilience against various attacks are the two most fundamental criteria of a robust watermarking scheme. However, despite offering significant protection against numerous attacks, field authentication of watermark has been a substantial problem for the state-of-the-art Finite State Machine (FSM) watermarking schemes. In this article, we address this problem by proposing a novel Cellular Automata (CA) guided FSM watermarking strategy, which offers easy in-field authentication, without compromising on the security of the design. Self-testability feature and unique state-transition property of a special class of CA ( &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$D1*CA$&lt;/tex-math&gt;&lt;/inline-formula&gt; ), and its dual counterpart ( &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$D1*CA_{dual}$&lt;/tex-math&gt;&lt;/inline-formula&gt; ) is the backbone of the proposed FSM watermarking scheme. Unlike the existing FSM watermarking strategies, watermarking in the proposed technique neither depends on the existence of the free state-transitions nor requires pseudo-inputs. Experimental results show that although the design overheads of CA-based FSM designs are relatively high, the additional overheads of the proposed watermarking scheme is minimal. Unlike the existing techniques, the overhead of the proposed strategy is independent of the watermark length, which enables embedding a significantly large authorship signature into the IP.},
  archive      = {J_TETC},
  author       = {Rajit Karmakar and Suman Sekhar Jana and Santanu Chattopadhyay},
  doi          = {10.1109/TETC.2020.3046662},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {806-823},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A cellular automata guided finite-state-machine watermarking strategy for IP protection of sequential circuits},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Chaotic clock driven cryptographic chip: Towards a DPA
resistant AES processor. <em>TETC</em>, <em>10</em>(2), 792–805. (<a
href="https://doi.org/10.1109/TETC.2020.3045802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing a tamper-resistant microchip for small embedded systems is one of the urgent demands of the computing community nowadays due to the immense security challenges arising particularly in massively connected networks. One of the major threats to secure smart card chips is the ability of Side Channel Attacks (SCA) , such as Correlation Power Analysis (CPA) and Correlation Instantaneous Frequency Analysis (CIFA) to increase the vulnerability of the secured cipher text to attacks even when the state of the art Advanced Encryption Standard (AES) is used. In this paper we explore the possibility of using chaotic clocking to protect AES chips against CPA and CIFA attacks. Our findings reveal that chaotic clocks, although not random, can effectively provide this protection with a low power envelope. Chaotic clocks derived from two different chaotic systems were used for testing in order to confirm the findings. Two FPGA boards running AES were driven using these chaotic clocks in order to prove the applicability of the proposed security enhancement technique.},
  archive      = {J_TETC},
  author       = {Ali A. El-Moursy and Abdollah M. Darya and Ahmed S. Elwakil and Abhinand Jha and Sohaib Majzoub},
  doi          = {10.1109/TETC.2020.3045802},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {792-805},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Chaotic clock driven cryptographic chip: Towards a DPA resistant AES processor},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intermittent pulling with local compensation for
communication-efficient distributed learning. <em>TETC</em>,
<em>10</em>(2), 779–791. (<a
href="https://doi.org/10.1109/TETC.2020.3043300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a widely used iterative algorithm, the distributed Stochastic Gradient Descent (SGD) has shown great advances in training machine learning models due to the reduced time of the gradients computation. However, the huge number of iterations of SGD usually incurs huge communication cost on pushing local gradients and pulling global model that prohibits its further improvement over performance. In this article, to reduce the number of pulling operations, a novel approach named Pulling Reduction with Local Compensation (PRLC) is proposed, in which each worker intermittently pulls the global model from the server and uses its local update to compensate the gap between the local model and the global model. Our rigorous theoretical analysis shows that the convergence rate of PRLC preserves the same order as the classical synchronous SGD for both strongly-convex and non-convex cases with good scalability due to the linear speedup with respect to the number of training nodes. Moreover, we also show that PRLC admits lower pulling frequency than the pulling reduction method without local compensation. The extensive experiments conducted on various models exhibit that our approach achieves a significant pulling reduction over the state-of-the-art methods, e.g., requiring only half of the pulling operations of LAG.},
  archive      = {J_TETC},
  author       = {Haozhao Wang and Zhihao Qu and Song Guo and Xin Gao and Ruixuan Li and Baoliu Ye},
  doi          = {10.1109/TETC.2020.3043300},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {779-791},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Intermittent pulling with local compensation for communication-efficient distributed learning},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Security promises and vulnerabilities in emerging
reconfigurable nanotechnology-based circuits. <em>TETC</em>,
<em>10</em>(2), 763–778. (<a
href="https://doi.org/10.1109/TETC.2020.3039375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconfigurable field-effect transistors (RFETs) based on emerging nanotechnologies allow switching between p-type and n-type behavior at runtime upon applying different bias potentials. While prior works have focused on particular security schemes using RFETs, here we first revisit the underlying security promises, and further showcase specific circuit vulnerabilities which can lead to adversarial scenarios. More specifically, first, we explore how transistor-level reconfigurability can be leveraged for logic locking and split manufacturing in the pretext of RFET-based modeling of the ITC-99 benchmarks. We find that with only 30 percent reconfigurable logic gates, we can induce a 100 percent output error rate (OER) and 31 percent Hamming distance (HD) on split manufacturing schemes. Second, arguably more disruptive, we explore how the very reconfigurability can be exploited to induce either short-circuit currents or open-circuit configurations, essentially destroying the reliability as well as electrical or functional characteristics of the chip. We apply detailed circuit evaluation and fault modeling toward this end. The novelty and severity of such disruptive scenarios lie in the fact that they can be readily realized in an actual on-field RFET-based chip, either as an adversarial or a fail-safe measure.},
  archive      = {J_TETC},
  author       = {Shubham Rai and Satwik Patnaik and Ansh Rupani and Johann Knechtel and Ozgur Sinanoglu and Akash Kumar},
  doi          = {10.1109/TETC.2020.3039375},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {763-778},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Security promises and vulnerabilities in emerging reconfigurable nanotechnology-based circuits},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incremental learning using a grow-and-prune paradigm with
efficient neural networks. <em>TETC</em>, <em>10</em>(2), 752–762. (<a
href="https://doi.org/10.1109/TETC.2020.3037052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have become a widely deployed model for numerous machine learning applications. However, their fixed architecture, substantial training cost, and significant model redundancy make it difficult to efficiently update them to accommodate previously unseen data. To solve these problems, we propose an incremental learning framework based on a grow-and-prune neural network synthesis paradigm. When new data arrive, the neural network first grows new connections based on the gradients to increase the network capacity to accommodate new data. Then, the framework iteratively prunes away connections based on the magnitude of weights to enhance network compactness, and hence recover efficiency. Finally, the model rests at a lightweight DNN that is both ready for inference and suitable for future grow- and-prune updates. The proposed framework improves accuracy, shrinks network size, and significantly reduces the additional training cost for incoming data compared to conventional approaches, such as training from scratch and network fine-tuning. For the LeNet-300-100 (LeNet-5) neural network architectures derived for the MNIST dataset, the framework reduces training cost by up to 64 (67), 63 (63), and 69 (73 percent) compared to training from scratch, network fine-tuning, and grow-and-prune from scratch, respectively. For the ResNet-18 architecture derived for the ImageNet dataset (DeepSpeech2 for the AN4 dataset), the corresponding training cost reductions against training from scratch, network fine-tunning, and grow-and-prune from scratch are 64 (67), 60 (62), and 72 (71 percent), respectively. Our derived models contain fewer network parameters but achieve higher accuracy relative to conventional baselines.},
  archive      = {J_TETC},
  author       = {Xiaoliang Dai and Hongxu Yin and Niraj K. Jha},
  doi          = {10.1109/TETC.2020.3037052},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {752-762},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Incremental learning using a grow-and-prune paradigm with efficient neural networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hardware trojan detection on a PCB through differential
power monitoring. <em>TETC</em>, <em>10</em>(2), 740–751. (<a
href="https://doi.org/10.1109/TETC.2020.3035521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a general consensus that contemporary electronics are at risk of cyber-attacks or malicious modifications, such as Hardware Trojans (HT). This makes it crucial to develop reliable countermeasures at both Integrated Circuit (IC) and Printed Circuit Board (PCB) levels. While HT detection at IC level has been widely studied in the past several years, there is still very limited research carried out to tackle HTs on PCBs. We propose a power analysis method for detecting HT components implanted on PCBs. An experimental setup, using a hardware prototype, is built and tested for verification of the methodology, taking process and temperature variations into account. The results confirm the ability to detect alien components on a PCB and provide directions for further research. The performance degradation of the original PCB due to the implementation of the proposed approach is negligible. The area overhead of the proposed method is small, related to the original PCB design, and consists of Sub Power Monitors of individual ICs on the PCB and Main Power Monitor for the overall power measurement of the PCB. To the best of our knowledge this research is the first to develop a PCB HT detection methodology using power analysis.},
  archive      = {J_TETC},
  author       = {Gor Piliposyan and Saqib Khursheed and Daniele Rossi},
  doi          = {10.1109/TETC.2020.3035521},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {740-751},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Hardware trojan detection on a PCB through differential power monitoring},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic generation of assertions for detection of firmware
vulnerabilities through alignment of symbolic sequences. <em>TETC</em>,
<em>10</em>(2), 728–739. (<a
href="https://doi.org/10.1109/TETC.2020.3035187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbolic simulation of firmware allows to automatically find execution paths triggering undesired behaviors that could hide vulnerabilities. However, once an unexpected behavior is identified, understanding its origin is an even more challenging task for the verification engineer. While several static and dynamic tools exist for detecting vulnerabilities, the same is not true for identifying their causes. This article is intended to fill in the gap by proposing an automatic framework for catching the source of IP vulnerabilities. Given an unwanted behavior, in the form of a propositional logic assertion, the framework exploits symbolic simulation and a sequence alignment algorithm to generate a set of temporal assertions that represent the minimum sequence of firmware instructions necessary for triggering the related vulnerability. In this way, the designer can effectively identify the cause of the vulnerability and fix it. Experimental results show the efficacy of the methodology in terms of efficiency and effectiveness.},
  archive      = {J_TETC},
  author       = {Samuele Germiniani and Alessandro Danese and Graziano Pravadelli},
  doi          = {10.1109/TETC.2020.3035187},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {728-739},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Automatic generation of assertions for detection of firmware vulnerabilities through alignment of symbolic sequences},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-stage deep regression enhanced depth estimation from a
single RGB image. <em>TETC</em>, <em>10</em>(2), 719–727. (<a
href="https://doi.org/10.1109/TETC.2020.3034559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth estimation plays a significant role in industrial applications, e.g., augmented reality, robotic mapping and autonomous driving. Traditional approaches for capturing depth, such as laser or depth sensor based methods, are difficult to use in most scenarios due to the limitations of high system cost and limited operational conditions. As an inexpensive and convenient approach, using the computational models to estimate depth from a single RGB image offers a preferable way for the depth prediction. Although the design of computational models to estimate the depth map has been widely investigated, the majority of models suffers from low prediction accuracy due to the sole utilization of a one-stage regression strategy. Inspired by both theoretical and practical success of two-stage regression, we propose a two-stage deep regression model, which is composed of two state-of-the-art network architectures, i.e., the fully convolutional residual network (FCRN) and the conditional generation adversarial network (cGAN). FCRN has been proved to possess a strong prediction ability for depth prediction, but fine details in the depth map are still incomplete. Accordingly, we have improved the existing cGAN model to refine the FCRN-based depth prediction. The experimental results show that the proposed two-stage deep regression model outperforms existing state-of-the-art methods.},
  archive      = {J_TETC},
  author       = {Jianyuan Sun and Zidong Wang and Hui Yu and Shu Zhang and Junyu Dong and Pengxiang Gao},
  doi          = {10.1109/TETC.2020.3034559},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {719-727},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Two-stage deep regression enhanced depth estimation from a single RGB image},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiclass classification on high dimension and low sample
size data using genetic programming. <em>TETC</em>, <em>10</em>(2),
704–718. (<a href="https://doi.org/10.1109/TETC.2020.3034495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiclass classification is one of the most fundamental tasks in data mining. However, traditional data mining methods rely on the model assumption, they generally can suffer from the overfitting problem on high dimension and low sample size (HDLSS) data. Trying to address multiclass classification problems on HDLSS data from another perspective, we utilize Genetic Programming (GP), an intrinsic evolutionary classification algorithm that can implement feature construction automatically without model assumption. This article develops an ensemble-based genetic programming classification framework, the Sigmoid-based Ensemble Gene Expression Programming (SE-GEP). To relieve the problem of output conflict in GP-based multiclass classifiers, the proposed method employs a flexible probability representation with continuous relaxation to better integrate the output of all the binary classifiers, an effective data division strategy to further enhance the ensemble performance, and a novel sampling strategy to refine the existing GP-based binary classifier. The experiment results indicate that SE-GEP can attain better classification accuracy compared to other GP methods. Moreover, the comparison with other representative machine learning methods indicates that SE-GEP is a competitive method for multiclass classification in HDLSS data.},
  archive      = {J_TETC},
  author       = {Tingyang Wei and Wei-Li Liu and Jinghui Zhong and Yue-Jiao Gong},
  doi          = {10.1109/TETC.2020.3034495},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {704-718},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Multiclass classification on high dimension and low sample size data using genetic programming},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A rumor &amp; anti-rumor propagation model based on data
enhancement and evolutionary game. <em>TETC</em>, <em>10</em>(2),
690–703. (<a href="https://doi.org/10.1109/TETC.2020.3034188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In social topics, the study of rumor propagation mechanisms is essential to master social behavior. In this paper, aiming to explore the adversarial game relationship between rumor and anti-rumor in the propagation process and considering the sparse of real and effective data in the process of rumor propagation, we propose a rumor propagation model. First, considering the sparse data samples of effective cross-countermeasures in the process of spreading rumor and anti-rumor, a model of unsupervised GAN is used to enhance homomorphism data in the sample space. Second, aiming at the symbiotic and antagonistic game relationship between rumor and anti-rumor, evolutionary game theory is introduced. The internal and external factors that affect user behavior within rumor propagation are comprehensively considered, and a quantitative rumor &amp; anti-rumor mutual influence model is proposed. Finally, considering the dynamic time limit of rumor and anti-rumor propagation life cycle, we herein propose a simple prediction method of rumor topic group behavior based on a dynamic iteration mechanism. Experiments show that our method gets 9-29 percent precision gains over the current methods on different datasets.},
  archive      = {J_TETC},
  author       = {Yunpeng Xiao and Wen Li and Shuai Qiang and Qian Li and Hanchun Xiao and Yanbing Liu},
  doi          = {10.1109/TETC.2020.3034188},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {690-703},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A rumor &amp; anti-rumor propagation model based on data enhancement and evolutionary game},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Algebraic attacks on block ciphers using quantum annealing.
<em>TETC</em>, <em>10</em>(2), 678–689. (<a
href="https://doi.org/10.1109/TETC.2022.3143152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the transformation method of the system of algebraic equations describing the symmetric cipher into the QUBO problem. After transformation of given equations &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$f_0, f_1, \ldots, f_{n-1}$&lt;/tex-math&gt;&lt;/inline-formula&gt; to equations over integers &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$f^{\prime }_0, f^{\prime }_1, \ldots, f^{\prime }_{n-1}$&lt;/tex-math&gt;&lt;/inline-formula&gt; , one can linearize each, obtaining &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$f^{\prime }_{lin_i}=lin(f^{\prime }_i)$&lt;/tex-math&gt;&lt;/inline-formula&gt; , for &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$i=\overline{0, n-1}$&lt;/tex-math&gt;&lt;/inline-formula&gt; , where &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$lin$&lt;/tex-math&gt;&lt;/inline-formula&gt; denotes linearization operation. Finally, one can obtain problem in the QUBO form as &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$(f^{\prime }_{lin_0} )^2+\cdots +(f^{\prime }_{lin_{n-1}} )^2+Pen-C$&lt;/tex-math&gt;&lt;/inline-formula&gt; , where &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$Pen$&lt;/tex-math&gt;&lt;/inline-formula&gt; denotes penalties obtained during linearization of equations, &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$n$&lt;/tex-math&gt;&lt;/inline-formula&gt; is the number of equations and &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$C$&lt;/tex-math&gt;&lt;/inline-formula&gt; is constant appearing in the polynomial &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$(f^{\prime }_{lin_0} )^2+\cdots +(f^{\prime }_{lin_{n-1}} )^2+Pen$&lt;/tex-math&gt;&lt;/inline-formula&gt; . This paper presents the transformation method of SPN block ciphers to the QUBO problem. What is more, we present the results of the transformation of the complete AES-128 cipher to the QUBO problem, where the number of variables of the equivalent QUBO problem equals approximately 30,026. It is worth noting that AES-128 is much easier to solve using quantum annealing than the factorization problem and the discrete logarithm problem of a similar level of security. For example, factorizing a 3072 bit long RSA integer using quantum annealing requires a QUBO problem of about 2,360,000 variables.},
  archive      = {J_TETC},
  author       = {Elżbieta Burek and Michał Wroński and Krzysztof Mańk and Michał Misztal},
  doi          = {10.1109/TETC.2022.3143152},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {678-689},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Algebraic attacks on block ciphers using quantum annealing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and evaluate recomposited OR-AND-XOR-PUF.
<em>TETC</em>, <em>10</em>(2), 662–677. (<a
href="https://doi.org/10.1109/TETC.2022.3170320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical Unclonable Function (PUF) is a hardware security primitive with a desirable feature of low cost. Considering the space of challenge-response pairs (CRPs), there are two PUF categories: weak PUF and strong PUF. Compared to a weak PUF, a strong PUF has a wider range of applications. However, it is challenging to design a reliable and secure lightweight strong PUF. To address this challenge, PUF recomposition built upon multiple simple PUF instances has received a lot of attention in research, such as the most popular XOR-APUF, the recent MPUF in IEEE TC 2017 (Sahoo et al. , 2018), XOR-FF-APUF in IEEE TIFS 2020 (Avvaru et al. , 2020) and IPUF in TCHES 2020 (Nguyen et al. , 2019). When a combination of MAX and MIN (equal to AND and OR) bitwise operations are used in PUF recomposition (Rührmair et al. , 2010), (Rührmair et al. , 2013) and (Gao et al. , 2020), its resilience against model attacks was expected to be improved markedly, because one bitwise operation might be vulnerable to one type of modeling attack and combining them can yield improved resilience. To our knowledge, there is no explicit evaluation of this recomposition; thus, this study is the first to evaluate the uniformity and reliability of the O R- A ND- X OR -PUF (OAX-PUF)—( &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$x,y,z$&lt;/tex-math&gt;&lt;/inline-formula&gt; )-OAX-PUF. Compared to the most used &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$l$&lt;/tex-math&gt;&lt;/inline-formula&gt; -XOR-PUF, the ( &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$x,y,z$&lt;/tex-math&gt;&lt;/inline-formula&gt; )-OAX-PUF shows better reliability given &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$l=x+y+z$&lt;/tex-math&gt;&lt;/inline-formula&gt; without degrading the uniformity (i.e., retain to be 50%). As APUF is a compact PUF instance for constructing lightweight strong PUF candidates, e.g., XOR-APUF, MUXPUF and IPUF, we further examine the modeling resilience of the ( &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$x,y,z$&lt;/tex-math&gt;&lt;/inline-formula&gt; )-OAX-APUF using four powerful attacks, i.e., logistic regression (LR), reliability assisted CMA-ES, multilayer perceptron (MLP), and the most recent hybrid LR-reliability. Compared to the XOR-APUF, the OAX-APUF successfully defeats the CMA-ES attack. It often shows improved modeling resilience against LR and hybrid LR-reliability attacks while always increasing the attacking time costs of these two attacks. However, OAX-APUF exhibits lower modeling resilience against the MLP attack unless the &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$x,y,z$&lt;/tex-math&gt;&lt;/inline-formula&gt; are carefully tuned. Overall, the OAX recomposition could be an alternative lightweight recomposition approach in constructing strong PUFs if the underlying PUF (e.g., FF-APUF), has shown improved resilience against modeling attacks, because the OAX incurs smaller reliability degradation compared to XOR.},
  archive      = {J_TETC},
  author       = {Jianrong Yao and Lihui Pang and Yang Su and Zhi Zhang and Wei Yang and Anmin Fu and Yansong Gao},
  doi          = {10.1109/TETC.2022.3170320},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {662-677},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Design and evaluate recomposited OR-AND-XOR-PUF},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximation-based fault tolerance in image processing
applications. <em>TETC</em>, <em>10</em>(2), 648–661. (<a
href="https://doi.org/10.1109/TETC.2021.3100623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image processing applications exhibit an intrinsic degree of fault tolerance due to i) the redundant nature of images, and ii) the possible ability of the consumers of the application output to effectively carry out their task even when it is slightly corrupted. In this application scenario the classical Duplication with Comparison (DWC) scheme, that rejects images (and requires re-executions) when the two replicas’ outputs differ in a per-pixel comparison, may be over-conservative. In this article, we propose a novel lightweight fault tolerant scheme specifically tailored for image processing applications. The proposed scheme enhances the state-of-the-art by: i) improving the DWC scheme by replacing one of the two exact replicas with an approximated counterpart, and ii) allowing to distinguish between usable and unusable images instead of corrupted and uncorrupted ones by means of a Convolutional Neural Network-based checker. To tune the proposed scheme we introduce a specific design methodology that optimizes both execution time and fault detection capability of the hardened system. We report the results of the application of the proposed approach on two case studies; our proposal achieves an average execution time reduction larger than 30% w.r.t. the DWC with re-execution, and less than 4% misclassified unusable images.},
  archive      = {J_TETC},
  author       = {Matteo Biasielli and Cristiana Bolchini and Luca Cassano and Andrea Mazzeo and Antonio Miele},
  doi          = {10.1109/TETC.2021.3100623},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {648-661},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Approximation-based fault tolerance in image processing applications},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reducing the impact of defects in quantum-dot cellular
automata (QCA) approximate adders at nano scale. <em>TETC</em>,
<em>10</em>(2), 635–647. (<a
href="https://doi.org/10.1109/TETC.2021.3136204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum-dot Cellular Automata (QCA) has been studied for some time as a candidate to replace traditional CMOS circuits. QCA circuits are implemented using majority-logic gates and inverters as primitive elements. Different types of defects affecting QCA during synthesis and manufacturing have been identified. Due to the use of the majority logic gate as the main building block of logic designs, the probability and impact of a defect affecting this type of element is significant. The effect can be translated into a change of the boolean expression implemented by the gate. QCA Approximate adders have been proposed for image processing and other applications. This paper analyzes the effect of QCA majority gate defects in the processing of images when using approximate adders. This is done by evaluating the variation in the value of common error distance metrics in the presence of defects. Mitigation of these defects by combining approximate and exact adders and selective introduction of fault-tolerant majority gates in different bits is analyzed. The technique achieves a reduction of around 75% of the average normalized mean error distance (NMED). Specific image-based metrics such as PSNR and SSIM are also evaluated in two different experiments, with an increase of up to 225% and 226% respectively. The increase in area is limited to around 40%.},
  archive      = {J_TETC},
  author       = {Alfonso Sánchez-Macián and Alonso Martín-Toledano and Jefferson Andres Bravo-Montes and Francisco García-Herrero and Juan Antonio Maestro},
  doi          = {10.1109/TETC.2021.3136204},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {635-647},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Reducing the impact of defects in quantum-dot cellular automata (QCA) approximate adders at nano scale},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Achieving diverse redundancy for GPU kernels. <em>TETC</em>,
<em>10</em>(2), 618–634. (<a
href="https://doi.org/10.1109/TETC.2021.3101922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous driving requires high-performance computing devices including general-purpose CPUs as well as specific accelerators, with GPUs having a key role due to their flexibility. Safety-critical microcontrollers have achieved ASIL-D compliance by implementing diverse redundancy with lockstep execution on-chip. However, a GPU does not provide diverse redundancy natively, thus failing to reach ASIL-D, which could only be reached with fully redundant lockstepped GPUs (2 GPUs) or pairing a GPU with another accelerator. However, both options may be infeasible due to procurement costs, and additional power, space and reliability costs to accomodate two devices. In this work, we present a variety of solutions to enable diverse redundant execution using only one GPU by taking advantage of the already internal redundancy of GPUs. We provide two lowly-intrusive hardware solutions and a software-only solution, with the latter evaluated directly on a real platform. In the case of the software-only solution, kernel execution on the GPU may require tailoring some parameters. With that objective, we also propose an algorithm that performs such tailoring automatically to guarantee software-only diverse redundancy on GPUs. Overall, our solutions allow achieving ASIL-D with a single GPU either with software-only solutions on a Commercial off-the-shelf GPU, or in a more efficient manner by introducing minor changes in the GPU design.},
  archive      = {J_TETC},
  author       = {Sergi Alcaide and Leonidas Kosmidis and Carles Hernandez and Jaume Abella},
  doi          = {10.1109/TETC.2021.3101922},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {618-634},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Achieving diverse redundancy for GPU kernels},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MigSpike: A migration based algorithms and architecture for
scalable robust neuromorphic systems. <em>TETC</em>, <em>10</em>(2),
602–617. (<a href="https://doi.org/10.1109/TETC.2021.3136028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While conventional hardware neuromorphic systems usually consist of multiple clusters of neurons that communicate via an interconnect infrastructure, scaling up them confronts the reliability issue when faults in the neuron circuits and synaptic weight memories can cause faulty outputs. This work presents a method named MigSpike that allows placing spare neurons for repairing with the support of enhanced migrating methods and the built-in hardware architecture for migrating neurons between nodes (clusters of neurons). MigSpike architecture supports migrating the unmapped neurons from their nodes to suitable ones within the system by creating chains of migrations. Furthermore, a max-flow min-cut adaptation and a genetic algorithm approach are presented to solve the aforementioned problem. The evaluation results show that the proposed methods support recovery up to 100% of spare neurons. While the max-flow min-cut adaption can execute milliseconds, the genetic algorithm can help reduce the migration cost with a graceful degradation on communication cost. With a system of 256 neurons per node and a 20% fault rate, our approach minimizes the migration cost from remapping by 10.19× and 96.13× under Networks-on-Chip of &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$4\times 4$&lt;/tex-math&gt;&lt;/inline-formula&gt; (smallest) and &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$16\times 16 \times 16$&lt;/tex-math&gt;&lt;/inline-formula&gt; (largest), respectively. The Mean-Time-to-Failure evaluation also shows an approximate 10× of lifetime expectancy by having a 20% spare rate.},
  archive      = {J_TETC},
  author       = {Khanh N. Dang and Nguyen Anh Vu Doan and Abderazek Ben Abdallah},
  doi          = {10.1109/TETC.2021.3136028},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {602-617},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {MigSpike: A migration based algorithms and architecture for scalable robust neuromorphic systems},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fault tolerant polyphase filters-based decimators for
SRAM-based FPGA implementations. <em>TETC</em>, <em>10</em>(2), 591–601.
(<a href="https://doi.org/10.1109/TETC.2021.3108556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To reduce the oversampling rate of baseband signals, decimation is widely used in digital communication systems. Polyphase filters (PPFs) can be used to efficiently implement decimators. SRAM-based FPGAs provide large amounts of resources combined with flexibility and are a popular option for the implementation of communication receivers. However, they are sensitive to soft errors, which limit their application in harsh environments, such as space. An initial reliability study on SRAM-based FPGA implemented decimation shows that the soft errors on around 5% of the critical bits in the configuration memory of the decimator would degrade the decimated signal dramatically. Based on this result, this paper proposes an efficient fault tolerance scheme, in which the high correlation between adjacent PPFs outputs is utilized to tolerate the fault of a single-phase filter, and a duplicate and comparison structure is used to protect the fault tolerance logic. Hardware implementation and fault injection experiments show that the proposed scheme can drastically reduce the number of critical bits that cause severe output degradation with 1.5x resource usage and 0.75x maximum frequency relative to the unprotected decimator. Therefore, the proposed scheme can be an alternative to Triple Modular Redundancy that more than triples the use of resources.},
  archive      = {J_TETC},
  author       = {Zhen Gao and Jinhua Zhu and Tong Yan Tyan and Anees Ullah and Pedro Reviriego},
  doi          = {10.1109/TETC.2021.3108556},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {591-601},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Fault tolerant polyphase filters-based decimators for SRAM-based FPGA implementations},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design-time exploration for process, environment and aging
compensation techniques for low power reliable-aware design.
<em>TETC</em>, <em>10</em>(2), 581–590. (<a
href="https://doi.org/10.1109/TETC.2021.3136288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern CMOS technologies such as advanced process FDSOI are affected by aging effects which can impact circuit functionality during lifetime operation. Indeed, due to system activity and usage dependence it is extremely difficult to establish a-priori sufficient guard bands for performance estimations, which leads either to large delay overestimation (and therefore loss of performances) or to reduced operating lifetime. In this paper, we propose an exploration of full adaptive AVS, ABB and combined supply and body bias techniques based on embedded monitors to obtain reliable functional operation, increased lifetime, while reducing the power consumption for the duration of the runtime. SPICE simulations performed on ARM processors show that large design margins in terms of area and power can be reduced, while keeping the target reliability and performance for the entire expected lifetime.},
  archive      = {J_TETC},
  author       = {Lorena Anghel and Florian Cacho},
  doi          = {10.1109/TETC.2021.3136288},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {581-590},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Design-time exploration for process, environment and aging compensation techniques for low power reliable-aware design},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Solar particle event and single event upset prediction from
SRAM-based monitor and supervised machine learning. <em>TETC</em>,
<em>10</em>(2), 564–580. (<a
href="https://doi.org/10.1109/TETC.2022.3147376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intensity of cosmic radiation may differ over five orders of magnitude within a few hours or days during the Solar Particle Events (SPEs), thus increasing for several orders of magnitude the probability of Single Event Upsets (SEUs) in space-borne electronic systems. Therefore, it is vital to enable the early detection of the SEU rate changes in order to ensure timely activation of dynamic radiation hardening measures. In this paper, an embedded approach for the prediction of SPEs and SRAM SEU rate is presented. The proposed solution combines the real-time SRAM-based SEU monitor, the offline-trained machine learning model and online learning algorithm for the prediction. With respect to the state-of-the-art, our solution brings the following benefits: (1) Use of existing on-chip data storage SRAM as a particle detector, thus minimizing the hardware and power overhead, (2) Prediction of SRAM SEU rate one hour in advance, with the fine-grained hourly tracking of SEU variations during SPEs as well as under normal conditions, (3) Online optimization of the prediction model for enhancing the prediction accuracy during run-time, (4) Negligible cost of hardware accelerator design for the implementation of selected machine learning model and online learning algorithm. The proposed design is intended for a highly dependable and self-adaptive multiprocessing system employed in space applications, allowing to trigger the radiation mitigation mechanisms before the onset of high radiation levels.},
  archive      = {J_TETC},
  author       = {Junchao Chen and Thomas Lange and Marko Andjelkovic and Aleksandar Simevski and Li Lu and Milos Krstic},
  doi          = {10.1109/TETC.2022.3147376},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {564-580},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Solar particle event and single event upset prediction from SRAM-based monitor and supervised machine learning},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FireNN: Neural networks reliability evaluation on hybrid
platforms. <em>TETC</em>, <em>10</em>(2), 549–563. (<a
href="https://doi.org/10.1109/TETC.2022.3152668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern neural network complexity has grown dramatically in recent years, leading to the adoption of hardware-accelerated solutions to cope with the computational power required by the new network architectures. The possibility to adapt the network size and performance to different platforms enhanced the interests of safety-critical applications such as automotive and avionic. Today, the reliability evaluation of neural networks is still premature and requires platforms to measure the safety standards required by mission-critical applications. For this reason, the interest in studying the reliability of neural networks is growing. In this work, we propose a new approach for evaluating the resiliency of neural networks by using programmable hardware of hybrid platforms. The approach relies on the reconfigurable hardware for emulating the target hardware platform and performing the fault injection process. The main advantage of the proposed approach is to involve the on-hardware execution of the neural network in the reliability analysis without modifying the hardware implementation of the network under analysis, and addressing specific fault models. The implementation of FireNN, the platform based on the proposed approach is detailly described in the paper. Experimental analyses are performed using fault injection on the AlexNet Convolutional Neural Network. The analyses are carried out by means of the FireNN platform and the obtained results are compared with the outcome of traditional software-level evaluations. Results are commented taking into account the insight into the hardware level achieved by using the FireNN platform.},
  archive      = {J_TETC},
  author       = {Corrado De Sio and Sarah Azimi and Luca Sterpone},
  doi          = {10.1109/TETC.2022.3152668},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {549-563},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {FireNN: Neural networks reliability evaluation on hybrid platforms},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HREN: A hybrid reliable and energy-efficient network-on-chip
architecture. <em>TETC</em>, <em>10</em>(2), 537–548. (<a
href="https://doi.org/10.1109/TETC.2022.3147407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As transistor scales down to sub-nanometer and processing cores with billions of transistors are integrated, reliable and energy-efficient Network-on-Chip (NoC) architectures are critical for improving performance of multicores. Near Threshold Voltage (NTV) scaling and approximate communication are popular techniques to reduce the energy consumption of NoC. Applications, which are insensitive to lower precision, can tolerate some loss in quality and take advantage of approximate communication. While approximate communication can improve energy-efficiency, these techniques are vulnerable to faults which in turn compromises reliability. In this article, we propose HREN: A Hybrid Reliable and Energy-efficient Network-on-Chip architecture that improves the reliability of NoC while utilizing both approximate communication and NTV scaling techniques in a multi-layered reliability model. HREN architecture facilitates two-levels of data approximation by identifying and compressing frequently repetitive patterns in the application data, thereby reducing the number of packet transmissions in NoC. As applications exhibit different traffic patterns in NoC, HREN switches the voltage mode of the network globally at runtime, thereby reducing the dynamic energy consumption while performing data approximation. HREN carefully monitors and handles the faults occurred due to NTV scaling and approximation while maintaining the fine balance between energy consumption and error rate. From our simulation results, HREN demonstrates up to 2.8x dynamic energy savings while reducing latency up to 2x. HREN shows an improvement of 4x to 5.5x in Energy-Delay Product over the baseline model for AxBench approximation benchmark suit on a 4 × 4 concentrated mesh (CMESH) architecture.},
  archive      = {J_TETC},
  author       = {Padmaja Bhamidipati and Avinash Karanth},
  doi          = {10.1109/TETC.2022.3147407},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {537-548},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {HREN: A hybrid reliable and energy-efficient network-on-chip architecture},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A real-time error detection (RTD) architecture and its use
for reliability and post-silicon validation for f/f based memory arrays.
<em>TETC</em>, <em>10</em>(2), 524–536. (<a
href="https://doi.org/10.1109/TETC.2022.3141486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes in-situ Real-Time Error Detection (RTD): embedding hardware in a memory array for detecting a fault in the array when it occurs, rather than when it is read. RTD breaks the serialization between data access and error-detection and, thus, it can speed-up the access-time of arrays that use in-line error-correction. The approach can also reduce the time needed to root-cause array related bugs during post-silicon validation and product testing. The paper introduces a two-dimensional error-correction scheme based on RTD and, also, presents a proactive error-correction method that combines RTD with demand-scrubbing. The work describes how to build RTD into a memory array with flip-flops to track in real-time the column-parity. A comparison of the proposed two-dimensional ECC scheme, as compared to single-error-correction-double-error-detection, shows that the RTD design has comparable error-detection-and-correction strength and, depending on the array dimensions and configuration, RTD reduces access time by 4% to 26% at an area and power overhead (negative is a reduction) between -7% to 33% and -42% to 86% respectively.},
  archive      = {J_TETC},
  author       = {Yiannakis Sazeides and Arkady Bramnik and Ron Gabor and Ramon Canal},
  doi          = {10.1109/TETC.2022.3141486},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {524-536},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A real-time error detection (RTD) architecture and its use for reliability and post-silicon validation for F/F based memory arrays},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IEEE transactions on emerging topics in computing.
<em>TETC</em>, <em>10</em>(1), C2. (<a
href="https://doi.org/10.1109/TETC.2022.3151016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TETC},
  doi          = {10.1109/TETC.2022.3151016},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {C2},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {IEEE transactions on emerging topics in computing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hardware constructions for lightweight cryptographic block
cipher QARMA with error detection mechanisms. <em>TETC</em>,
<em>10</em>(1), 514–519. (<a
href="https://doi.org/10.1109/TETC.2020.3027789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cryptographic algorithm QARMA is a family of lightweight tweakable block ciphers targeted at applications such as memory encryption and construction of keyed hash functions. Utilizing lightweight security in hardware has the advantage of adopting the mechanisms to battery-constrained usage models including implantable and wearable medical devices. This lightweight block cipher utilizes a substitution permutation network (SPN) which is inspired by block ciphers such as PRINCE, MANTIS, and MIDORI. Moreover, it uses a three-round Even-Mansour scheme instead of the FX-construction, with its central permutation being non-involutory and keyed. In this article, we introduce error detection schemes on variations of QARMA, namely QARMA-64 and QARMA-128, which to the best of authors’ knowledge, have not been presented to date. We present our derivations for the logic-gate-based implementation, following which, we present the derivations for signature-based and interleaved signature-based schemes for the LUT-based approach. The presented, new signature-based error detection schemes, including cyclic redundancy check (CRC), are provided for the compact, involutory, and optimized S-box. Besides, recomputations through encoding the operands allow for the architectures to counter both transient and permanent faults. Also, the schemes are benchmarked on a field-programmable gate array (FPGA) hardware platform, where performance and implementation metrics show acceptable overheads and degradations. The proposed schemes are aimed to make the implementations of this lightweight tweakable block cipher more reliable.},
  archive      = {J_TETC},
  author       = {Jasmin Kaur and Mehran Mozaffari Kermani and Reza Azarderakhsh},
  doi          = {10.1109/TETC.2020.3027789},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {514-519},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Hardware constructions for lightweight cryptographic block cipher QARMA with error detection mechanisms},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid partial product-based high-performance approximate
recursive multipliers. <em>TETC</em>, <em>10</em>(1), 507–513. (<a
href="https://doi.org/10.1109/TETC.2020.3013977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate recursive multipliers exhibit low-power operation because they are designed using smaller power-efficient approximate multiplier blocks. These building blocks can be configured by varying the approximation levels for a wide range of larger multiplier sizes. However, most of the building blocks proposed for recursive multipliers are either slightly inaccurate or hardware-efficient with limited accuracy. In this brief, hybrid partial product-based building blocks are proposed by considering the probability distribution of the input operands. An efficient hardware implementation of approximate 4×4 multipliers is achieved, while maintaining the required accuracy. Moreover, high-performance approximate NOR-based half adder (NxHA) and full adder (NxFA) cells are proposed for use in a 4×4 multiplier. Three different strategies (Ax8-1/2/3) are further proposed and analyzed for utilizing the 4×4 multipliers when designing larger multipliers. Ax8-2 provides the best trade-off among the designs with a moderate MRED. A reduction of 30 and 17 percent in the MRED is achieved compared to previous best energy-optimized and MRED-optimized designs. Among the designs with higher MREDs, Ax8-3 exhibits the smallest MRED and PDP. Moreover, it shows an improvement of 7 to 28 percent in delay compared to existing approximate recursive designs. As a case study, image multiplication is evaluated; a high peak signal-to-noise ratio (PSNR) with a value close to 50dB is obtained for the proposed multiplier designs.},
  archive      = {J_TETC},
  author       = {Haroon Waris and Chenghua Wang and Weiqiang Liu and Jie Han and Fabrizio Lombardi},
  doi          = {10.1109/TETC.2020.3013977},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {507-513},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Hybrid partial product-based high-performance approximate recursive multipliers},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-power approximate logarithmic squaring circuit design
for DSP applications. <em>TETC</em>, <em>10</em>(1), 500–506. (<a
href="https://doi.org/10.1109/TETC.2020.2989699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The squaring function is widely used in Digital Signal Processing (DSP). There are many DSP applications with noisy inputs for which simplifying approximations of the squaring function implementation have a minor impact on the output quality, while permitting significant reductions in the hardware cost. This article proposes a Low-Error Squaring Function (LESF) and its low-power hardware implementation. Unlike the existing logarithmic squaring functions, LESF benefits from a double-sided error distribution and, consequently, error cancellation in larger calculations. LESF approximates a base-2 logarithmic function with a linear polynomial, i.e., &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\mathrm{log_2}\;f(x) \approx ax+b$&lt;/tex-math&gt;&lt;/inline-formula&gt; . Since input &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$b$&lt;/tex-math&gt;&lt;/inline-formula&gt; in this sum is a constant, LESF replaces the conventional full-adder with a compact specialized adder for hardware efficiency. Our simulation results show that the 16-bit LESF is 23.23 percent more accurate (in the mean relative error distance) than the baseline Mitchell approximate logarithmic squaring function while being 1.8× faster and 39 percent more energy-efficient. LESF and other logarithmic squaring functions are evaluated for the square-law detector application. LESF is shown to be more than 3× more accurate in this application (with respect to the Euclidean distance) than the next most accurate design in the literature, which uses an iterative error compensation technique.},
  archive      = {J_TETC},
  author       = {Mohammad Saeed Ansari and Bruce F. Cockburn and Jie Han},
  doi          = {10.1109/TETC.2020.2989699},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {500-506},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Low-power approximate logarithmic squaring circuit design for DSP applications},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automata-based dynamic fault tolerant task scheduling
approach in fog computing. <em>TETC</em>, <em>10</em>(1), 488–499. (<a
href="https://doi.org/10.1109/TETC.2020.3033672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog computing is an extension of cloud computing that offers computing, storage and communication resources near the network edge, which makes it an ideal platform for processing latency-sensitive and compute-intensive tasks. However, efficient task execution in a fog platform is a challenging problem since fog nodes are loosely interconnected, highly dynamic, heterogeneous, and prone to failures. Therefore, a task scheduling algorithm that ensures reliable execution of the tasks while optimising response time is paramount. To address this challenge, we propose a new Dynamic Fault Tolerant Learning Automata (DFTLA) task scheduling approach. DFTLA determines an efficient assignment of the tasks to the fog nodes based on variable-structure learning automata. We evaluated the proposed DFTLA scheduler and compared its performance with three baseline methods. The results of the experiments show that the proposed algorithm ensures reliable execution of the tasks while optimising response time and energy consumption. Moreover, the proposed approach outperforms the baseline algorithms in all performance evaluation criteria.},
  archive      = {J_TETC},
  author       = {Sara Ghanavati and Jemal Abawajy and Davood Izadi},
  doi          = {10.1109/TETC.2020.3033672},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {488-499},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Automata-based dynamic fault tolerant task scheduling approach in fog computing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PROWL: A cache replacement policy for consistency aware
renewable powered devices. <em>TETC</em>, <em>10</em>(1), 476–487. (<a
href="https://doi.org/10.1109/TETC.2020.3031114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy harvesting systems powered by renewable energy sources employ hybrid volatile-nonvolatile memory to enhance energy efficiency and forward progress. These systems have unreliable power sources and energy buffers with limited capacity, so they complete long-running applications across multiple power outages. However, a power outage might cause data inconsistency, because the data in nonvolatile memories are persistent, while the data in volatile memories are unsteady. State of the art studies proposed various memory architectures and compiler-based techniques to tackle the data inconsistency in these systems. These approaches impose too many unnecessary check-points on the system to avoid data inconsistency. These studies did not consider the effect of cache memory to mask and postpone the imposed check-points on the system for the sake of consistency. In this article, we utilize the cache memory and propose PROWL, a consistency aware cache replacement policy to avoid data inconsistency with fewer check-points. The results show that PROWL has by up to 85 percent fewer check-points compare to the state of the art approaches, and PROWL improves the average response time of the system by up to 65 percent.},
  archive      = {J_TETC},
  author       = {Ali Hoseinghorban and Mohammad Abbasinia and Alireza Ejlali},
  doi          = {10.1109/TETC.2020.3031114},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {476-487},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {PROWL: A cache replacement policy for consistency aware renewable powered devices},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On adaptive influence maximization under general feedback
models. <em>TETC</em>, <em>10</em>(1), 463–475. (<a
href="https://doi.org/10.1109/TETC.2020.3031057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classic influence maximization problem explores the strategies for deploying cascades such that the total influence is maximized, and it assumes that the seed nodes that initiate the cascades are computed prior to the diffusion process. In its adaptive version, the seed nodes are allowed to be launched in an adaptive manner after observing certain diffusion results. In this article, we provide a systematic study on the adaptive influence maximization problem, focusing on the algorithmic analysis of the general feedback models. We introduce the concept of regret ratio to characterize the key trade-off in designing adaptive seeding strategies, based on which we present the approximation analysis for the well-known greedy policy. In addition, we provide analysis concerning improving the efficiencies and bounding the regret ratio. Finally, we propose several future research directions.},
  archive      = {J_TETC},
  author       = {Guangmo Tong and Ruiqi Wang},
  doi          = {10.1109/TETC.2020.3031057},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {463-475},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {On adaptive influence maximization under general feedback models},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DeepEcho: Echoacoustic recognition of materials using
returning echoes with deep neural networks. <em>TETC</em>,
<em>10</em>(1), 450–462. (<a
href="https://doi.org/10.1109/TETC.2020.3029044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we present a novel method for learning and recognizing the properties of materials by using the returning echoes of self-emitted acoustic signals. This is achieved by mimicking the acoustic recognition mechanisms that are present in several animals like bats and dolphins. To implement this mechanism on a computer, we propose an end-to-end machine learning approach that uses convolutional neural networks (CNNs), which can translate spectral cues that are delivered by reflected echoes into meaningful information for the target object. To validate the proposed approach, we conducted two different experiments. In the first experiment, we attempted to learn a set of predefined everyday surfaces using that reflected acoustic signals that are emitted from a distance. As an extension of the first experiment, the second experiment investigated whether echo-acoustic signals can be used for the classification of an acoustically smooth surface (e.g., liquid solutions). The experimental results showed that our system can recognize texture and, density information. In comparison to the conventional machine learning approach that relies on feature-engineering processes, our approach that is based on CNNs demonstrated high accuracies ( &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$&amp;gt;90\%$&lt;/tex-math&gt;&lt;/inline-formula&gt; ) in the test datasets in a robust manner. We conclude this article by discussing the limitations of the proposed approach and exploring the potential avenues to incorporate acoustic recognition functionality into computers.},
  archive      = {J_TETC},
  author       = {Geon Kang and Seung-Chan Kim},
  doi          = {10.1109/TETC.2020.3029044},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {450-462},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {DeepEcho: Echoacoustic recognition of materials using returning echoes with deep neural networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A graph learning based approach for identity inference in
DApp platform blockchain. <em>TETC</em>, <em>10</em>(1), 438–449. (<a
href="https://doi.org/10.1109/TETC.2020.3027309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current cryptocurrencies, such as Bitcoin and Ethereum, enable anonymity by using public keys to represent user accounts. On the other hand, inferring blockchain account types (i.e., miners, smart contracts or exchanges), which are also referred to as blockchain identities, is significant in many scenarios, such as risk assessment and trade regulation. Existing work on blockchain deanonymization mainly focuses on Bitcoin that supports simple transactions of cryptocurrencies. As the popularity of decentralized application (DApp) platform blockchains with Turing-complete smart contracts, represented by Ethereum, identity inference in blockchain faces new challenges because of user diversity and complexity of activities enabled by smart contracts. In this paper, we propose I &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$^2$&lt;/tex-math&gt;&lt;/inline-formula&gt; GL, an identify inference approach based on big graph analytics and learning to address these challenges. Specifically, I &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$^2$&lt;/tex-math&gt;&lt;/inline-formula&gt; GL constructs a transaction graph and aims to infer the identity of nodes using the graph learning technique based on Graph Convolutional Networks. Furthermore, a series of enhancement has been proposed by exploiting unique features of blockchain transaction graph. The experimental results on Ethereum transaction records show that I &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$^2$&lt;/tex-math&gt;&lt;/inline-formula&gt; GL significantly outperforms other state-of-the-art methods.},
  archive      = {J_TETC},
  author       = {Xiao Liu and Zaiyang Tang and Peng Li and Song Guo and Xuepeng Fan and Jinbo Zhang},
  doi          = {10.1109/TETC.2020.3027309},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {438-449},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A graph learning based approach for identity inference in DApp platform blockchain},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonvolatile associative memory design based on spintronic
synapses and CNTFET neurons. <em>TETC</em>, <em>10</em>(1), 428–437. (<a
href="https://doi.org/10.1109/TETC.2020.3026179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many advances have been made in developing bio-inspired intelligent systems for performing important tasks like pattern association, which cannot be performed effectively by the conventional computer architectures. This paper presents a novel nonvolatile associative memory based on spintronic synapses utilizing magnetic tunnel junction (MTJ) and carbon nanotube field-effect transistors (CNTFET)-based neurons. MTJs introduce fascinating features like reconfigurability, nonvolatility, and high endurance to the design, while CNTFETs compensate for the flaws of the conventional transistors in deep nanoscale nodes. The comprehensive simulations validate the functionality of the proposed synaptic cell, and neuron, even in the presence of major process variations. The application of the proposed synaptic cell and neuron in the hardware realization of multi-associative memories is also investigated. The simulation results indicate that the proposed hardware-based multi-associative memory performs similar to its ideal software-based counterpart with a less than 3 percent difference in the successful recall rate. It also noteworthy that since the input and output of the proposed design are logical ‘0’ and ‘1’, the proposed design does not need any extra component for voltage conversion.},
  archive      = {J_TETC},
  author       = {Abdolah Amirany and Mohammad Hossein Moaiyeri and Kian Jafari},
  doi          = {10.1109/TETC.2020.3026179},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {428-437},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Nonvolatile associative memory design based on spintronic synapses and CNTFET neurons},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AppDNA: Profiling app behavior via deep-learning function
call graphs. <em>TETC</em>, <em>10</em>(1), 414–427. (<a
href="https://doi.org/10.1109/TETC.2020.3026335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing number and diversity of applications make malware detection and app recommendation for users more challenging. In this work, we design a framework AppDNA to automatically generate a compact representation for each app to comprehensively profile its behaviors. The versatile representation can be generated once for each app, and then be used for a wide variety of objectives, including malware detection, app categorization and app version detection, etc . We propose to conduct a function-call-graph-based app profiling scheme based on a comprehensive and deep understanding of an app’s behaviors. We design a graph-encoding method to convert a large function call graph to a 64-dimensional fixed length vector to achieve robust app profiling. Our extensive evaluations on 86,332 apps demonstrate that our approach performs app profiling with high accuracy and low computation cost: it takes about 46.5 seconds for one app to extract its function call graph; 0.68 seconds to encode a function call graph; it classifies all 4,024 (benign/malware) apps in around 5.06 seconds with accuracy about 93.07 percent; it classifies all 570 malicious apps’ family (21 families in total) in around 0.83 seconds with accuracy 82.3 percent; it classifies 9,730 apps’ functionality into 2 categories with accuracy 88.1 percent or into 7 categories with accuracy 33.3 percent.},
  archive      = {J_TETC},
  author       = {Anran Li and Shuangshuang Xue and Xiang-Yang Li and Lan Zhang and Jianwei Qian},
  doi          = {10.1109/TETC.2020.3026335},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {414-427},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {AppDNA: Profiling app behavior via deep-learning function call graphs},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Novel quadruple-node-upset-tolerant latch designs with
optimized overhead for reliable computing in harsh radiation
environments. <em>TETC</em>, <em>10</em>(1), 404–413. (<a
href="https://doi.org/10.1109/TETC.2020.3025584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of CMOS technologies, nano-scale CMOS latches have become increasingly sensitive to multiple-node upset (MNU) errors caused by radiations. First, this paper proposes a novel latch design, namely QNUTL that can completely tolerate MNUs such as double-node upsets, triple-node upsets (TNUs), and even quadruple-node upsets (QNUs). The latch is mainly constructed from three dual-interlocked-storage-cells (DICEs) and a triple-level soft-error interceptive module (SIM) that consists of six 2-input C-elements. Due to the single-node-upset self-recoverability of DICEs and the soft-error interception of the SIM, the latch can completely tolerate any QNU. Next, by replacing the DICEs in the QNUTL latch by clock-gating (CG) based ones, a QNUTL-CG latch is proposed to significantly reduce power consumption. Simulation results demonstrate the MNU-tolerance of the proposed latches. Moreover, owing to the use of a high-speed transmission path, clock-gating, and a few transistors, the proposed QNUTL-CG latch has low overhead in terms of area, D-Q delay, CLK-Q delay, and setup time, compared with the state-of-the-art TNU-tolerant latch (TNUTL) which is not QNU-tolerant.},
  archive      = {J_TETC},
  author       = {Aibin Yan and Zhelong Xu and Xiangfeng Feng and Jie Cui and Zhili Chen and Tianming Ni and Zhengfeng Huang and Patrick Girard and Xiaoqing Wen},
  doi          = {10.1109/TETC.2020.3025584},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {404-413},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Novel quadruple-node-upset-tolerant latch designs with optimized overhead for reliable computing in harsh radiation environments},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards provably-secure analog and mixed-signal locking
against overproduction. <em>TETC</em>, <em>10</em>(1), 386–403. (<a
href="https://doi.org/10.1109/TETC.2020.3025561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similar to digital circuits, analog and mixed-signal (AMS) circuits are also susceptible to supply-chain attacks, such as piracy, overproduction, and Trojan insertion. However, unlike digital circuits, the supply-chain security of AMS circuits is less explored. In this work, we propose to perform “logic-locking” on the digital section of the AMS circuits. The idea is to make the analog design intentionally suffer from the effects of process variations, which impede the operation of the circuit. Only on applying the correct key, the effect of process variations are mitigated, and the analog circuit performs as desired. To this end, we render certain components in the analog circuit configurable. We propose an analysis to dictate which components need to be configurable to maximize the effect of an incorrect key. We conduct our analysis on the bandpass filter (BPF), low-noise amplifier (LNA), and low-dropout voltage regulator (LDO) for both correct and incorrect keys to the locked optimizer. We also show experimental results for our technique on a BPF. We also analyze the effect of aging on our locking technique to ensure the reliability of the circuit with the correct key.},
  archive      = {J_TETC},
  author       = {N. G. Jayasankaran and A. Sanabria BÓrbon and E. SÁnchez-Sinencio and J. Hu and J. Rajendran},
  doi          = {10.1109/TETC.2020.3025561},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {386-403},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Towards provably-secure analog and mixed-signal locking against overproduction},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A dynamic-key based secure scan architecture for
manufacturing and in-field IC testing. <em>TETC</em>, <em>10</em>(1),
373–385. (<a href="https://doi.org/10.1109/TETC.2020.3021820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design for testability (DFT) technology based on scan chains is widely used in industry to increase the testability of circuits. However, it also leads to a potential security problem that attackers can use scan chains as a backdoor to attack a system. Common methods to defend such attacks include disabling the scan chain after manufacturing test or employing some secret keys to encrypt/decrypt scan data or to verify the identities of users. The former would make in-field testing impossible and the latter would require storing keys in memory which might also undergo high risk of memory attacks. In this paper we propose a dynamic-key based secure scan architecture that works together with an intrinsic Physical Unclonable Function (PUF) of chips to defend both scan-based and memory attacks while facilitating both manufacturing and in-field testing. A system equipped with this secure architecture will shift out true circuit responses only when legal test patterns are shifted into the scan chains. Moreover, no test key will be stored in memory, hence no memory attacks are possible. We also leverage the PUF to distinct the legal test patterns for different manufactured chips so as to further protect chips. Analysis results show that our protection scheme can achieve a very high security level without sacrificing system performance, testability and diagnosability.},
  archive      = {J_TETC},
  author       = {Kuen-Jong Lee and Ching-An Liu and Chia-Chi Wu},
  doi          = {10.1109/TETC.2020.3021820},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {373-385},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A dynamic-key based secure scan architecture for manufacturing and in-field IC testing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Advancement of textual answer triggering: Cognitive
boosting. <em>TETC</em>, <em>10</em>(1), 361–372. (<a
href="https://doi.org/10.1109/TETC.2020.3022731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Answer Triggering is still perceived as a challenging task in Question Answering (QA) despite the recent successes chalked up by deep learning models. Its demand for near-human sentence comprehension and answer selection has made previous works on the task seemingly incapable of solving the task. This article introduces an Answer Triggering dataset, CogQA, that contains cognitive features of sentences to enhance the performance of answer triggering systems. It also presents the first deep hierarchical end-to-end neural model that leverages the cognitive elements of CogQA to establish neural correlations to its corresponding answer(s). Our results demonstrate the utility of the dataset and its capability of enabling better triggering of answers in QA systems. Furthermore, our hierarchical model&#39;s performance transcends previous works on the WIKIQA benchmark by an appreciable extent.},
  archive      = {J_TETC},
  author       = {Kingsley Nketia Acheampong and Wenhong Tian},
  doi          = {10.1109/TETC.2020.3022731},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {361-372},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Advancement of textual answer triggering: Cognitive boosting},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technology development and modeling of switching lattices
using square and h shaped four-terminal switches. <em>TETC</em>,
<em>10</em>(1), 351–360. (<a
href="https://doi.org/10.1109/TETC.2020.3022426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Switching lattices formed by four-terminal switches are introduced as dense rectangular structures to implement Boolean logic functions. It is clearly shown in literature by a variety of logic synthesis algorithms including the exact one, realizing logic functions on lattices with the fewest number of four-terminal switches, as well as the heuristic ones, that switching lattices offer a significant area advantage in terms of the number of switches over the conventional CMOS implementations. Although the computing potential of switching lattices has been well justified, the same thing cannot be said for their physical implementation. There have been conceptual ideas for the technology development of switching lattices, but no concrete and directly applicable technology has been proposed yet. In this study, we show that switching lattices can be implemented using the CMOS technology. For this purpose, we propose two different four-terminal switch structures with square and H shaped gates. We construct these two structures in three dimensional technology computer-aided design (TCAD) environment satisfying the design rules of the TSMC 65 nm CMOS process and perform simulations. Then, we develop Level 3 DC and AC models of these four-terminal switches in LTspice environment using the TCAD simulation data. As an experiment, we realize logic functions with the developed models using static and dynamic logic solutions. Experimental results show that the realization of logic functions using switching lattices occupy much less layout area and have competitive delay and power consumption values when compared to the conventional CMOS implementations.},
  archive      = {J_TETC},
  author       = {Nihat Akkan and Şerzat SAFALTIN and Levent Aksoy and ISMAIL Çevik and Herman Sedef and Csaba A. Moritz and Mustafa Altun},
  doi          = {10.1109/TETC.2020.3022426},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {351-360},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Technology development and modeling of switching lattices using square and h shaped four-terminal switches},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design of unsigned approximate hybrid dividers based on
restoring array and logarithmic dividers. <em>TETC</em>, <em>10</em>(1),
339–350. (<a href="https://doi.org/10.1109/TETC.2020.3022290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate computer arithmetic has been extensively studied due to its advantages to further reduce power consumption and increase performance at reduced accuracy. Although a number of approximate adders and multipliers have been studied, only a few approximate dividers have been proposed. A logarithmic divider (LD) has low complexity and accuracy, while an exact array divider (EXD) has a high complexity. Therefore, in this article, an approximate hybrid divider (AXHD) is proposed. It takes advantage of both LD and EXD to achieve a tradeoff between hardware performance and accuracy. Exact restoring divider cells are used to generate the most significant bits (MSBs) of the quotient for attaining a high accuracy while the other quotient digits are generated by using a LD as an approximate scheme to improve figures of merit such as power consumption, area and delay. To further save hardware resources, a so-called eliminated approximate hybrid divider (E-AXHD) based on AXHD is also proposed. In this improved design, a reduced width divider is used to replace the EXD in AXHD. Specifically, for a 16-by-8 design, &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$n/(n+1)$&lt;/tex-math&gt;&lt;/inline-formula&gt; array division is used to replace the &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$n/8$&lt;/tex-math&gt;&lt;/inline-formula&gt; array division &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$(n&amp;lt;8)$&lt;/tex-math&gt;&lt;/inline-formula&gt; . The proposed AXHD and E-AXHD are evaluated and analyzed using error and hardware metrics. The proposed designs are also compared with EXD, LD and previous approximate dividers. The results show that the proposed designs outperform previous approximate dividers by considering both energy and error. The proposed hybrid dividers are of particular interest for error tolerant applications such as image processing and machine learning.},
  archive      = {J_TETC},
  author       = {Weiqiang Liu and Tao Xu and Jing Li and Chenghua Wang and Paolo Montuschi and Fabrizio Lombardi},
  doi          = {10.1109/TETC.2020.3022290},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {339-350},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Design of unsigned approximate hybrid dividers based on restoring array and logarithmic dividers},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An asymmetric, one-to-many traffic-aware mm-wave wireless
interconnection architecture for multichip systems. <em>TETC</em>,
<em>10</em>(1), 324–338. (<a
href="https://doi.org/10.1109/TETC.2020.3020615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Platform based computing modules such as embedded systems and micro-servers are multichip systems with in-package memory and processing chips. Such systems consist of both one-to-one (unicast) and one-to-many (broadcast/ multicast) traffic patterns. State-of-the-art wired interconnection architectures such as Network-on-Chip (NoC) are specially designed to handle on-chip unicast traffic and cannot mask the high off-chip communication latency caused by the chip-to-chip I/Os in multichip systems. Moreover, with the increase in memory-intensive applications and hence one-to-many traffic in a multichip system, this scenario gets even worse as conventionally one-to-many traffic is handled as multiple unicast traffic in a multihop NoC infrastructure. Consequently, a small proportion of such one-to-many traffic increases energy consumption and message latency significantly for chip-to-chip communication. Therefore, to support such one-to-many traffic, these multichip systems with in-package memory need one-to-many traffic-aware interconnection infrastructure. To address these issues, we propose the design of one-to-many traffic-aware Wireless Network-in-Package (WiNiP) architecture by using a novel asymmetric WiNiP topology and a traffic-aware Medium Access Control (MAC) mechanism. With cycle-accurate simulations, we demonstrate that the proposed WiNiP architecture reduces the energy consumption and latency up to 46.96 and 47.08 percent respectively for multichip data transfer compared to state-of-the-art wired NiPs for application-specific traffic.},
  archive      = {J_TETC},
  author       = {M Meraj Ahmed and Naseef Mansoor and Amlan Ganguly},
  doi          = {10.1109/TETC.2020.3020615},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {324-338},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {An asymmetric, one-to-many traffic-aware mm-wave wireless interconnection architecture for multichip systems},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ReMap: Reliability management of peak-power-aware real-time
embedded systems through task replication. <em>TETC</em>,
<em>10</em>(1), 312–323. (<a
href="https://doi.org/10.1109/TETC.2020.3018902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing power densities in future technology nodes is a crucial issue in multicore platforms. As the number of cores increases in them, power budget constraints may prevent powering all cores simultaneously at full performance level. Therefore, chip manufacturers introduce a power budget constraint as Thermal Design Power (TDP) for chips. Meanwhile, multicore platforms are suitable for the implementation of fault-tolerance techniques to achieve high reliability. Task Replication is a well-known technique to tolerate transient faults. However, careless task replication may lead to significant peak power consumption. In this article, we consider the problem of achieving a given reliability target while keeping the total power consumption under the chip TDP for a set of periodic soft real-time tasks. For this purpose, we propose a method for mapping and scheduling periodic soft real-time tasks in multicore embedded systems. The proposed method consists of three parts: ( i ) Reliability-Aware Lowest Utilization Mapping, ( ii ) Maximum-Power-Aware EDF Scheduling, and ( iii ) Reliability-and-Peak-Power-Aware Dynamic-Voltage-Frequency-Scaling. Our experiments show that our proposed method provides up to 38.4 percent (on average by 25 percent) peak power reduction compared to state-of-the-art methods.},
  archive      = {J_TETC},
  author       = {Amir Yeganeh-Khaksar and Mohsen Ansari and Alireza Ejlali},
  doi          = {10.1109/TETC.2020.3018902},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {312-323},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {ReMap: Reliability management of peak-power-aware real-time embedded systems through task replication},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient crowdsourced pareto-optimal queries over partial
orders with quality guarantee. <em>TETC</em>, <em>10</em>(1), 297–311.
(<a href="https://doi.org/10.1109/TETC.2020.3017198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of crowdsourcing marketplaces has leveraged the power of human intelligence into tackling computationally challenging problems. Pareto-optimal queries become more and more popular in subtle and comprehensive decision support due to the increasingly complex comparison criteria, e.g., partial orders. However, none of previous work focused on efficient crowdsourced Pareto-optimal queries over partial orders with minimum monetary cost and quality guarantee with a confidence level. In this article, we propose a cost-efficient framework to find Pareto-optimal objects with minimum monetary cost and quality guarantee with a confidence level. We first propose a dynamic-status judgment model based on Student’s &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$t$&lt;/tex-math&gt;&lt;/inline-formula&gt; -distribution, which gives a confidence interval of the judgment to ensure the quality of pairwise comparisons while minimizing the number of crowdsourcers required for each pairwise comparison. We then propose a filtering-verification scheme that takes full advantage of transitivity to avoid unnecessary crowdsourcing comparisons, which significantly reduces the number of crowdsourcing pairwise comparisons. The results of our extensive experiments demonstrate that the dynamic-status judgment model requires a small number of crowdsourcers for a pairwise comparison while maintaining the accuracy, and the filtering-verification scheme can reduce the number of pairwise comparisons by 40 percent in average.},
  archive      = {J_TETC},
  author       = {Bo Yin and Xuetao Wei},
  doi          = {10.1109/TETC.2020.3017198},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {297-311},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Efficient crowdsourced pareto-optimal queries over partial orders with quality guarantee},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IoTRec: The IoT recommender for smart parking system.
<em>TETC</em>, <em>10</em>(1), 280–296. (<a
href="https://doi.org/10.1109/TETC.2020.3014722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a General Data Protection Regulation (GDPR)-compliant Internet of Things (IoT) Recommender (IoTRec) system, developed in the framework of H2020 EU-KR WISE-IoT (Worldwide Interoperability for Semantic IoT) project, which provides the recommendations of parking spots and routes while protecting users’ privacy. It provides recommendations by exploiting the IoT technology (parking and traffic sensors). The IoTRec provides four-fold functions. First, it helps the user to find a free parking spot based on different metrics (such as the nearest or nearest trusted parking spot). Second, it recommends a route (the least crowded or the shortest route) leading to the recommended parking spot from the user’s current location. Third, it provides the real-time provision of expected availability of parking areas (comprised of parking spots organized into groups) in a user-friendly manner. Finally, it provides a GDPR-compliant implementation for operating in a privacy-aware environment. The IoTRec is integrated into the smart parking use case of the WISE-IoT project and is evaluated by the citizens of Santander, Spain through a prototype, but it can be applied to any IoT-enabled locality. The evaluation results show the citizen’s satisfaction with the quality, functionalities, ease of use and reliability of the recommendations/services offered by the IoTRec.},
  archive      = {J_TETC},
  author       = {Yasir Saleem and Pablo Sotres and Samuel Fricker and Carmen LÓpez de la Torre and Noel Crespi and Gyu Myoung Lee and Roberto Minerva and Luis SÁnchez},
  doi          = {10.1109/TETC.2020.3014722},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {280-296},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {IoTRec: The IoT recommender for smart parking system},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and analysis of a novel integral design scheme for
finding finite-time solution of time-varying matrix inequalities.
<em>TETC</em>, <em>10</em>(1), 267–279. (<a
href="https://doi.org/10.1109/TETC.2020.3013692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving linear inequalities is widely used in various fields, and it plays a more and more crucial role in practical engineering applications. However, the existing recurrent neural network models to solve this problem only achieve global convergence without any noise. To overcome this disadvantage, in this article, we propose a novel integral design scheme for finding the robust solution of time-varying matrix inequalities. The core idea of this model is to add an integral term in the construction of the error function to make the model have error memory, so as to eliminate static difference. Meanwhile, appropriate activation functions (AFs) are used in the integral term noise-tolerance zeroing neural network (ITNTZNN) model, which can make error function accomplish finite-time convergence. The noise tolerance property of the ITNTZNN model is proved by theoretical analysis, and the upper limit of convergence time is obtained. Numerical simulative results ulteriorly verify the finite-time and noise-tolerant properties of the ITNTZNN model.},
  archive      = {J_TETC},
  author       = {Yuejie Zeng and Kenli Li and Lin Xiao and Qing Liao},
  doi          = {10.1109/TETC.2020.3013692},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {267-279},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Design and analysis of a novel integral design scheme for finding finite-time solution of time-varying matrix inequalities},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The self-aware information processing factory paradigm for
mixed-critical multiprocessing. <em>TETC</em>, <em>10</em>(1), 250–266.
(<a href="https://doi.org/10.1109/TETC.2020.3011663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to provide performance increases despite the end of Moore’s law and Dennard scaling, architectures aggressively exploit data- and thread-level parallelism using billions of transistors on a single chip, enabled by extreme geometry miniaturization. A resulting challenge is the control, optimization, and reliable operation of such complex multiprocessing architectures. Modern and future systems will be required to operate under multi-dimensional variability: from varying workload, quality-of-service (QoS) goals, and non-functional requirements to varying environmental and operating conditions. A trend has recently emerged to abstract such complex multiprocessing architectures as self-aware factories whose resources are monitored, configured and their use is planned during runtime. In this article, we present the Information Processing Factory (IPF) paradigm for mixed-criticality. We introduce its 5-layer hierarchical organization and a system configuration framework that ensures that the strict requirements of the safety-critical functions are always met while dynamically managing and optimizing the mixed-critical system at runtime. We illustrate the application of IPF in heterogeneous domains with two representative use-cases (healthcare and automotive), investigate the use of IPF to achieve long-term dependability, and highlight the open challenges. Experimental results report the reliability levels achievable with the proposed paradigm.},
  archive      = {J_TETC},
  author       = {Eberle A. Rambo and Bryan Donyanavard and Minjun Seo and Florian Maurer and Thawra Kadeed and Caio B. de Melo and Biswadip Maity and Anmol Surhonne and Andreas Herkersdorf and Fadi Kurdahi and Nikil Dutt and Rolf Ernst},
  doi          = {10.1109/TETC.2020.3011663},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {250-266},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {The self-aware information processing factory paradigm for mixed-critical multiprocessing},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SPRING: A sparsity-aware reduced-precision monolithic 3D CNN
accelerator architecture for training and inference. <em>TETC</em>,
<em>10</em>(1), 237–249. (<a
href="https://doi.org/10.1109/TETC.2020.3003328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) outperform traditional machine learning algorithms across a wide range of applications, such as object recognition, image segmentation, and autonomous driving. However, their ever-growing computational complexity makes it necessary to design efficient hardware accelerators. Most CNN accelerators focus on exploring various dataflow styles and designs that exploit computational parallelism. However, potential performance improvement (speedup) from sparsity has not been adequately addressed. The computation and memory footprint of CNNs can be significantly reduced if sparsity is exploited in network evaluations. To further improve performance and energy efficiency, some accelerators evaluate CNNs with limited precision. However, this is limited to the inference phase since reduced precision sacrifices network accuracy if used in training. In addition, CNN evaluation is usually memory-intensive, especially during training. The performance bottleneck arises from the fact that the memory cannot feed the computational units enough data, resulting in idling of these computational units and thus low utilization ratios. In this article, we propose SPRING, a SParsity-aware Reduced-precision Monolithic 3D CNN accelerator for trainING and inference. SPRING supports both CNN training and inference. It uses a binary mask scheme to encode sparsities in activations and weights. It uses the stochastic rounding algorithm to train CNNs with reduced precision without accuracy loss. To alleviate the memory bottleneck in CNN evaluation, especially during training, SPRING uses an efficient monolithic 3D nonvolatile memory interface to increase memory bandwidth. Compared to Nvidia GeForce GTX 1080 Ti, SPRING achieves 15.6×, 4.2×, and 66.0× improvements in performance, power reduction, and energy efficiency, respectively, for CNN training, and 15.5×, 4.5×, and 69.1× improvements in performance, power reduction, and energy efficiency, respectively, for inference.},
  archive      = {J_TETC},
  author       = {Ye Yu and Niraj K. Jha},
  doi          = {10.1109/TETC.2020.3003328},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {237-249},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {SPRING: A sparsity-aware reduced-precision monolithic 3D CNN accelerator architecture for training and inference},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Agile: A learning-enabled power and performance-efficient
network-on-chip design. <em>TETC</em>, <em>10</em>(1), 223–236. (<a
href="https://doi.org/10.1109/TETC.2020.3003496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A number of techniques to achieve power-efficient Network-on-Chips (NoCs) have been proposed, two of which are power-gating and dynamic voltage and frequency scaling (DVFS). Power-gating reduces static power, and DVFS reduces dynamic power. With the goal of reducing both static and dynamic power, it is intuitive to simultaneously deploy both techniques. However, we observe that the straightforward combination of power-gating and DVFS can result in reduced power benefits and degraded performance. In this article, we uniquely combine power-gating and DVFS with the aim of maximizing the NoC power savings and improving performance. The proposed NoC design, called Agile, consists of several architectural designs and a reinforcement learning (RL) based control policy to mitigate the negative effects induced by the combined power-gating and DVFS. Specifically, a simple bypass switch is deployed to maintain network connectivity, avoiding frequently waking up the powered-off router. An optimized pipeline can simplify pipeline stages of the bypass switch to reduce network latency. Reversible link channel buffers can be dynamically allocated to where they are needed to improve throughput. In addition, the RL control policy predicts NoC traffic and decides optimal power-gating decisions, voltage/frequency levels and NoC architecture configurations at runtime. Furthermore, we explore the use of an artificial neural network (ANN) to efficiently reduce the area overhead of implementing RL. We evaluate our design using the PARSEC benchmarks suite. The full system simulation results show that the proposed design improves the overall power savings by up to 58 percent while improving the performance up to 11 percent as compared to state-of-the-art designs. The ANN-based RL implementation and bypass switch incur nominal area overhead of 5 percent, as compared to a conventional router.},
  archive      = {J_TETC},
  author       = {Hao Zheng and Ahmed Louri},
  doi          = {10.1109/TETC.2020.3003496},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {223-236},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Agile: A learning-enabled power and performance-efficient network-on-chip design},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A quantum mechanics-based framework for EEG signal feature
extraction and classification. <em>TETC</em>, <em>10</em>(1), 211–222.
(<a href="https://doi.org/10.1109/TETC.2020.3000734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum machine learning (QML) is an emerging research field, which is devoted to devising and implementing quantum algorithms that could enable machine learning faster than that of classical computers. In this article, a hierarchic quantum mechanics-based framework is investigated to implement both the feature extraction and classification in the electroencephalogram (EEG) signal. First, the classical EEG signal dataset is prepared as a quantum state while the sign of the data point is preserved. The prepared quantum state is then evolved with the quantum wavelet packet transformation (QWPT) and the wavelet packet energy entropy (WPEE) feature is extracted as the input of the subsequent quantum classifier. We finally propose the improved quantum support vector machine with the arbitrary nonlinear kernel, which is employed to predict the label of the EEG signal. The complexity analysis indicates that the proposed framework provides exponential speedup over the same structured classical counterpart. Besides, the quantitative experimental results verify the feasibility and validity.},
  archive      = {J_TETC},
  author       = {YaoChong Li and Ri-Gui Zhou and RuiQing Xu and Jia Luo and She-Xiang Jiang},
  doi          = {10.1109/TETC.2020.3000734},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {211-222},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A quantum mechanics-based framework for EEG signal feature extraction and classification},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic access control for privacy management of personal
sensing in smart cities. <em>TETC</em>, <em>10</em>(1), 199–210. (<a
href="https://doi.org/10.1109/TETC.2020.2996974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personal and home sensors generate valuable information that could be used in Smart Cities. Unfortunately, typically, this data is locked out and used only by application/system developers. While vendors are partially to blame, one should consider also the “binary nature” of data access. Specifically, either the owner has full control over her data (e.g., in a “closed system”), or she completely looses control, when the data is “opened”. In this context, we propose, a semantic technologies-based, authorization and privacy control framework that enables user to maintain flexible, yet manageable data access control policies. The proposed approach is described in detail, including implementation and testing.},
  archive      = {J_TETC},
  author       = {MichaŁ Drozdowicz and Maria Ganzha and Marcin Paprzycki},
  doi          = {10.1109/TETC.2020.2996974},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {199-210},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Semantic access control for privacy management of personal sensing in smart cities},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design space exploration based methodology for residue
number system digital filters implementation. <em>TETC</em>,
<em>10</em>(1), 186–198. (<a
href="https://doi.org/10.1109/TETC.2020.2997067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decades, the Residue Number System (RNS) has been adopted in DSP as an alternative to the traditional two’s complement number system (TCS) because of the savings in area and power dissipation. In this work, we first perform a comprehensive Design Space Exploration (DSE) to analyze the impact of state-of-the-art design tools and libraries on the implementation of the basic operations (i.e., addition and multiplication) used in DSP. From this DSE, we extract the characteristics of the stand-alone RNS and TCS operators in the different design corners, independently of the specific context of the application. Then, we propose a design methodology, based on the DSE, to fully automate the design of digital filters, hiding the detail of the RNS to the designer, and providing optimal power efficient implementations. Our methodology can enable the efficiency in computation (speed and power) in DSP and in emerging applications, such as Machine Learning and Internet-of-Things.},
  archive      = {J_TETC},
  author       = {Gian Carlo Cardarilli and Luca Di Nunzio and Rocco Fazzolari and Alberto Nannarelli and Massimo Petricca and Marco Re},
  doi          = {10.1109/TETC.2020.2997067},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {186-198},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Design space exploration based methodology for residue number system digital filters implementation},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scratch-DKG: A framework for constructing scratch domain
knowledge graph. <em>TETC</em>, <em>10</em>(1), 170–185. (<a
href="https://doi.org/10.1109/TETC.2020.2996710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of programming platforms, how to utilize the tremendous amount of data produced by the platforms, such as Scatch, has been a big challenge to researchers. The growing data is not only huge, but also heterogeneous and diverse, leading that the existing tools cannot effectively extract valuable information. In this article, considering particular features of Scratch data, we propose an effective framework about constructing a Scratch Domain Knowledge Graph (Scratch-DKG). Our framework includes four modules which are designed to process the semi-structured data, users profile data, projects data and programming knowledge points, respectively. For webpages, we design a template-based wrapper method to extract triples from the semi-structured data. As for users profile data, we improve DeepDive, which is a useful tool to extract information but with the problem of wrong labeling, to extract knowledge triples by the proposed Secondary Labeling Algorithm. For projects data, we propose an advanced keywords extraction method (S-TextRank) to extract keywords triples. For programming knowledge points, we develop a frequently contiguous block combinations mining algorithm to extract the potential domain information of Scratch. Finally, extensive experiments are carried out to evaluate the performance of our proposed methods. The experimental results show that, compared to other competing methods, our proposal can extract more correct and comprehensive Scratch triples.},
  archive      = {J_TETC},
  author       = {Peng Qi and Yan Sun and Hong Luo and Mohsen Guizani},
  doi          = {10.1109/TETC.2020.2996710},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {170-185},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Scratch-DKG: A framework for constructing scratch domain knowledge graph},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting activities of daily living and routine behaviours
in dementia patients living alone using smart meter load disaggregation.
<em>TETC</em>, <em>10</em>(1), 157–169. (<a
href="https://doi.org/10.1109/TETC.2020.2993177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of an ageing population is a significant public health concern. This has led to an increase in the number of people living with progressive neurodegenerative disorders. The strain this places on services means providing 24-hour monitoring is not sustainable. No solution exists to non-intrusively monitor the wellbeing of patients with dementia, resulting in delayed intervention. Using machine learning and signal processing, domestic energy supplies can be disaggregated to detect appliance usage. This enables Activities of Daily Living (ADLs) to be assessed. The aim is to facilitate early intervention and enable patients to stay in their homes for longer. A Support Vector Machine (SVM) and Random Decision Forest classifier are modelled using data from three test homes. The trained models are then used to monitor two patients with dementia during a six-month clinical trial undertaken in partnership with Mersey Care NHS Foundation Trust. In the case of load disaggregation, the SVM achieved (AUC = 0.86074, Sen = 0.756 and Spec = 0.92838). While the Decision Forest achieved (AUC = 0.9429, Sen = 0.9634 and Spec = 0.9634). ADLs are also analysed to identify the behavioural patterns of the occupant while detecting alterations in routine. The approach is sensitive in identifying behavioural routines and detecting anomalies in patient behaviour.},
  archive      = {J_TETC},
  author       = {C. Chalmers and P. Fergus and C. Aday Curbelo Montanez and S. Sikdar and F. Ball and B. Kendall},
  doi          = {10.1109/TETC.2020.2993177},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {157-169},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Detecting activities of daily living and routine behaviours in dementia patients living alone using smart meter load disaggregation},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Opening the doors to dynamic camouflaging: Harnessing the
power of polymorphic devices. <em>TETC</em>, <em>10</em>(1), 137–156.
(<a href="https://doi.org/10.1109/TETC.2020.2991134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The era of widespread globalization has led to the emergence of hardware-centric security threats throughout the IC supply chain. Prior defenses like logic locking, layout camouflaging, and split manufacturing have been researched extensively to protect against intellectual property (IP) piracy at different stages. In this work, we present dynamic camouflaging as a new technique to thwart IP reverse engineering at all stages in the supply chain, viz., the foundry, the test facility, and the end-user. Toward this end, we exploit the multi-functionality, post-fabrication reconfigurability, and run-time polymorphism of spin-based devices, specifically the magneto-electric spin-orbit (MESO) device. Leveraging these unique properties, dynamic camouflaging is shown to be resilient against state-of-the-art analytical SAT-based attacks and test-data mining attacks. Such dynamic reconfigurability is not afforded in CMOS owing to fundamental differences in operation. For such MESO-based camouflaging, we also anticipate massive savings in power, performance, and area over other spin-based camouflaging schemes, due to the energy-efficient electric-field driven reversal of the MESO device. Based on thorough experimentation, we outline the promises of dynamic camouflaging in securing the supply chain end-to-end along with a case study, demonstrating the efficacy of dynamic camouflaging in securing error-tolerant image processing IP.},
  archive      = {J_TETC},
  author       = {Nikhil Rangarajan and Satwik Patnaik and Johann Knechtel and Ramesh Karri and Ozgur Sinanoglu and Shaloo Rakheja},
  doi          = {10.1109/TETC.2020.2991134},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {137-156},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Opening the doors to dynamic camouflaging: Harnessing the power of polymorphic devices},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incremental checkpointing for fault-tolerant stream
processing systems: A data structure approach. <em>TETC</em>,
<em>10</em>(1), 124–136. (<a
href="https://doi.org/10.1109/TETC.2020.2986487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the demand of high-speed stream processing grows, in-memory databases are widely used to analyze streaming data. It is challenging for in-memory systems to meet the requirements of high throughput and data persistence at the same time since data are not stored in disks. ARIES logging and command logging are two popular logging methods. In current applications, both ARIES logging and command logging are necessary. However, no checkpointing mechanism includes both the functions of ARIES logging method and command logging method. Besides, adopting ARIES logging method in an in-memory database creates high overhead. Command logging records redundant commands and has high storage cost. To address the above issues, we utilize order-irrelevant characteristics of data structure and incremental checkpointing concepts to devise a data structure based incremental checkpointing (DSIC) mechanism. DSIC mechanism is a very low overhead checkpointing approach while retaining the features of ARIES logging and command logging. DSIC mechanism reduces more than 70 percent logging time of the existing logging scheme and saves 40 percent storage costs of the existing logging scheme.},
  archive      = {J_TETC},
  author       = {Chia-Yu Lin and Li-Chun Wang and Shu-Ping Chang},
  doi          = {10.1109/TETC.2020.2986487},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {124-136},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Incremental checkpointing for fault-tolerant stream processing systems: A data structure approach},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HCP: Heterogeneous computing platform for federated learning
based collaborative content caching towards 6G networks. <em>TETC</em>,
<em>10</em>(1), 112–123. (<a
href="https://doi.org/10.1109/TETC.2020.2986238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A heterogeneous computing architecture is essential to facilitate intelligent network traffic control for a joint computation, communication, and collaborative caching optimization in 6G networks to provide stringent Quality of Experience (QoE) guarantees. In this paper, we consider a 6G integrated aerial-terrestrial network model where Unmanned Aerial Vehicles (UAVs) and terrestrial Remote Radio Heads (RRHs) jointly serve as heterogeneous Base Stations (hgNBs) of a Cloud Radio Access Network (HCRAN) serving different mobile user (UE) types. We propose a distributed heterogeneous computing platform (HCP) across the UAVs and terrestrial Base Stations (BSs) by utilizing their caching and cooperative communication capabilities. In order to preserve the privacy of the content of the UEs, we propose a 2-stage federated learning algorithm among the UEs, UAVs/BSs, and HCP to collaboratively predict the content caching placement by jointly considering traffic distribution, UE mobility and localized content popularity. An asynchronous weight updating method is adopted to avoid redundant learning transfer in the federated learning. Once the global model is learnt by the HCP, it transfers the learned model to the UEs to facilitate the much desired edge intelligence in the considered 6G tiny cell. The effectiveness of the proposal is evaluated by extensive numerical analysis.},
  archive      = {J_TETC},
  author       = {Zubair Md. Fadlullah and Nei Kato},
  doi          = {10.1109/TETC.2020.2986238},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {112-123},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {HCP: Heterogeneous computing platform for federated learning based collaborative content caching towards 6G networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HMCKRAutoEncoder: An interpretable deep learning framework
for time series analysis. <em>TETC</em>, <em>10</em>(1), 99–111. (<a
href="https://doi.org/10.1109/TETC.2022.3143154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of time series data has long been a problem of great interest in a wide range of fields, such as medical surveillance, gene expression analysis, and economic forecasting. Recently, there has been a renewed interest in time series analysis with deep learning, since deep learning models can achieve state-of-the-art results on various tasks. However, deep learning models such as DNNs have a huge parametric space, which makes DNNs be viewed as complex “black-box” models. We propose a novel framework, HMCKRAutoEncoder, which adopts a two-task learning method to construct a human-machine collaborative knowledge representation (HMCKR) on a hidden layer of an AutoEncoder, to address the “black-box” problem in deep learning based time series analysis. In our framework, the AutoEncoder model is cross-trained by two learning tasks, aiming to generate HMCKR on a hidden layer of the AutoEncoder. We propose a pipeline for HMCKR-based time series analysis for various tasks. Moreover, a human-in-the-loop (HIL) mechanism is introduced to provide humans with the ability to intervene with the decision-making of deep models. Experimental results on three datasets demonstrate that our method is consistently comparable with several state-of-the-art methods while providing interpretability, and outperforms these methods when the HIL mechanism is applied.},
  archive      = {J_TETC},
  author       = {Jilong Wang and Rui Li and Renfa Li and Bin Fu and Danny Z. Chen},
  doi          = {10.1109/TETC.2022.3143154},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {99-111},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {HMCKRAutoEncoder: An interpretable deep learning framework for time series analysis},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Artificial intelligence for mass spectrometry and nuclear
magnetic resonance spectroscopy using a novel data augmentation method.
<em>TETC</em>, <em>10</em>(1), 87–98. (<a
href="https://doi.org/10.1109/TETC.2021.3131371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mass Spectrometry (MS) and Nuclear Magnetic Resonance Spectroscopy (NMR) are valuable analytical and quality control methods for most industrial chemical processes as they provide information on the concentrations of individual compounds and by-products. These processes are traditionally carried out manually and by a specialist, which takes a substantial amount of time and prevents their utilization for real-time closed-loop process control. This article presents recent advances from two projects that use Artificial Neural Networks (ANNs) to address the challenges of automation and performance-efficient realizations of MS and NMR. In the first part, a complete toolchain has been realized to develop simulated spectra and train ANNs to identify compounds in MS. In the second part, a limited number of experimental NMR spectra have been augmented by simulated spectra, to train an ANN with better prediction performance and speed than state-of-the-art analysis. These results suggest that, in the context of the digital transformation of the process industry, we are now on the threshold of a strongly simplified use of MS and MRS and the accompanying data evaluation by machine-supported procedures, and can utilize both methods much wider for reaction and process monitoring or quality control.},
  archive      = {J_TETC},
  author       = {Florian Fricke and Marcelo Brandalero and Sascha Liehr and Simon Kern and Klas Meyer and Stefan Kowarik and Robin Hierzegger and Stephan Westerdick and Michael Maiwald and Michael Hübner},
  doi          = {10.1109/TETC.2021.3131371},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {87-98},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Artificial intelligence for mass spectrometry and nuclear magnetic resonance spectroscopy using a novel data augmentation method},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). In-situ defect detection of metal additive manufacturing: An
integrated framework. <em>TETC</em>, <em>10</em>(1), 74–86. (<a
href="https://doi.org/10.1109/TETC.2021.3108844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metal Additive Manufacturing (AM) is a pillar of the Industry 4.0, with many attractive advantages compared to traditional subtractive fabrication technologies. However, there are many quality issues that can be an obstacle for mass production. The in-situ camera-based monitoring and detection of defects, taking advantage of the layer-by-layer nature of the build, can be an effective solution to this problem. In this context, the use of Computer Vision and Machine Learning algorithms have a very important role. Nonetheless, they are up to this date limited by the scarcity of data for the training, as well as by the difficulty of accessing and integrating the AM process data throughout the fabrication. To tackle this problem, this article proposes a system for in-situ monitoring that analyses images from an off-axis camera mounted on top of the machine to detect the arising defects in real-time, with automated generation of synthetic images based on Generative Adversarial Network (GAN) for dataset augmentation purposes. The computing functionalities are embedded into a holistic distributed AM platform allowing the collection, integration and storage of data at all stages of the AM pipeline.},
  archive      = {J_TETC},
  author       = {Davide Cannizzaro and Antonio Giuseppe Varrella and Stefano Paradiso and Roberta Sampieri and Yukai Chen and Alberto Macii and Edoardo Patti and Santa Di Cataldo},
  doi          = {10.1109/TETC.2021.3108844},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {74-86},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {In-situ defect detection of metal additive manufacturing: An integrated framework},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Boosting extra-functional code reusability in cyber-physical
production systems: The error handling case study. <em>TETC</em>,
<em>10</em>(1), 60–73. (<a
href="https://doi.org/10.1109/TETC.2022.3142816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber-Physical Production Systems (CPPS) are long-living and mechatronic systems, which include mechanics, electrics/electronics and software. The interdisciplinary nature combined with challenges and trends in the context of Industry 4.0 such as a high degree of customization, small lot sizes and evolution cause a high amount of variability. Mastering the variability of functional control software, e.g., different control variants of an actuator type, is itself a challenge in developing and reusing CPPS software. This task becomes even more complex when considering extra-functional software such as operating modes, diagnosis and error handling. These software parts have high interdependencies with functional software, often involving the human-machine interface (HMI) to enable the intervention of operators. This paper illustrates the challenges in documenting the dependencies of these software parts including their variability using family models. A procedural and an object-oriented concept for implementing error handling, which represents an extra-functional task with high dependencies to functional software and the HMI, are proposed. The suitability of both concepts to increase the software&#39;s reusability and, thus, its flexibility in the context of Industry 4.0 is discussed. Their comparison confirms the high potential of the object-oriented extension of IEC 61131-3 to handle planned reuse of extra-functional CPPS software successfully.},
  archive      = {J_TETC},
  author       = {Birgit Vogel-Heuser and Juliane Fischer and Dieter Hess and Eva-Maria Neumann and Marcus Würr},
  doi          = {10.1109/TETC.2022.3142816},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {60-73},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Boosting extra-functional code reusability in cyber-physical production systems: The error handling case study},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Digital transformation of a production line: Network design,
online data collection and energy monitoring. <em>TETC</em>,
<em>10</em>(1), 46–59. (<a
href="https://doi.org/10.1109/TETC.2021.3132432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of Industry 4.0 originates from the will to introduce the benefits of digital computation into new and existing industrial plants to save time, materials and energy. The digital transformation requires that all machinery of the production line are connected together and with the enterprise applications, to capture and analyze data across all manufacturing stages. Then, such collected data can be exploited to take strategic decision on the production and to monitor it, reacting to unexpected behaviors and thus reducing downtime and maintenance costs. This article aims at supporting production engineers approaching digital transformation by exemplifying its key elements on a real life scenario, the Industrial Computer Engineering laboratory of the University of Verona. First of all, the article discusses network design, as communication is an enabler of the other technologies. Network is realized through automatic network synthesis from requirements and characteristics of the production line data flow. Then, the paper discusses data collection and the construction of a digital twin monitoring power consumption of the production line, with the goal of detecting any discrepancy between real time data and digital twin data. This allows to trigger an early intervention on the line, to guarantee an effective maintenance.},
  archive      = {J_TETC},
  author       = {Nicola Dall&#39;Ora and Khaled Alamin and Enrico Fraccaroli and Massimo Poncino and Davide Quaglia and Sara Vinco},
  doi          = {10.1109/TETC.2021.3132432},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {46-59},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Digital transformation of a production line: Network design, online data collection and energy monitoring},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph learning for cognitive digital twins in manufacturing
systems. <em>TETC</em>, <em>10</em>(1), 34–45. (<a
href="https://doi.org/10.1109/TETC.2021.3132251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Future manufacturing requires complex systems that connect simulation platforms and virtualization with physical data from industrial processes. Digital twins incorporate a physical twin, a digital twin, and the connection between the two. Benefits of using digital twins, especially in manufacturing, are abundant as they can increase efficiency across an entire manufacturing life-cycle. The digital twin concept has become increasingly sophisticated and capable over time, enabled by rises in many technologies. In this article, we detail the cognitive digital twin as the next stage of advancement of a digital twin that will help realize the vision of Industry 4.0. Cognitive digital twins will allow enterprises to creatively, effectively, and efficiently exploit implicit knowledge drawn from the experience of existing manufacturing systems. They also enable more autonomous decisions and control, while improving the performance across the enterprise (at scale). This article presents graph learning as one potential pathway towards enabling cognitive functionalities in manufacturing digital twins. A novel approach to realize cognitive digital twins in the product design stage of manufacturing that utilizes graph learning is presented.},
  archive      = {J_TETC},
  author       = {Trier Mortlock and Deepan Muthirayan and Shih-Yuan Yu and Pramod P. Khargonekar and Mohammad Abdullah Al Faruque},
  doi          = {10.1109/TETC.2021.3132251},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {34-45},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Graph learning for cognitive digital twins in manufacturing systems},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Key-components for digital twin modeling with granularity:
Use case car-as-a-service. <em>TETC</em>, <em>10</em>(1), 23–33. (<a
href="https://doi.org/10.1109/TETC.2021.3131532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital technologies are changing the way people interact with the world. The Digital Twin (DT) is one of the key enablers of Industry 4.0. It provides a virtual representation of an observable element of the real world. These elements can be both physical objects such as devices or non-physical such as interactions and processes. Digitalization of the real world enables new business models, transforming traditional products into services, as for instance, the Car-as-a-Service (CaaS). To integrate all components that will be part of systems like CaaS or Smart Cities, it is necessary to have well-defined standards for modeling and defining an architecture especially taking into consideration the granularity level of the system. This paper proposes the main components needed for building DT-based systems with different levels of granularity. These components have been arranged in layers to specify the concerns of each part of the system. A case study has been developed to demonstrate the modeling and the deployment of the Digital Twin, highlighting how this concept can be one of the key enablers for CaaS.},
  archive      = {J_TETC},
  author       = {Charles Steinmetz and Greyce N. Schroeder and Ricardo N. Rodrigues and Achim Rettberg and Carlos E. Pereira},
  doi          = {10.1109/TETC.2021.3131532},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {23-33},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Key-components for digital twin modeling with granularity: Use case car-as-a-service},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine-learning-driven digital twin for lifecycle
management of complex equipment. <em>TETC</em>, <em>10</em>(1), 9–22.
(<a href="https://doi.org/10.1109/TETC.2022.3143346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The full life cycle management of complex equipment is considered fundamental to the intelligent transformation and upgrading of the modern manufacturing industry. Digital twin technology and machine learning have been emerging technologies in recent years. The application of these two technologies in the full life cycle management of complex equipment can make each stage of the life cycle more responsive, predictable, and adaptable. This paper first proposes a technical system that embeds machine learning modules into digital twins. Next, on this basis, a full life cycle digital twin for complex equipment is constructed, and joint application of sub-models and machine learning is explored. Then, the application of a combination of the digital twin in maintenance with machine learning in predictive maintenance of diesel locomotives is presented. The effectiveness of the proposed management method is verified by experiments. The abnormal axle temperature can be alarmed about one week in advance. Lastly, possible application advantages of the combination of digital twin and machine learning in addressing future research direction in this field are introduced.},
  archive      = {J_TETC},
  author       = {Zijie Ren and Jiafu Wan and Pan Deng},
  doi          = {10.1109/TETC.2022.3143346},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {9-22},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Machine-learning-driven digital twin for lifecycle management of complex equipment},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guest editorial: Thematic section on applications of
emerging computing technologies in smart manufacturing and industry 4.0.
<em>TETC</em>, <em>10</em>(1), 6–8. (<a
href="https://doi.org/10.1109/TETC.2022.3146784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special section focus on applications of emerging computing technologies in smart manufacturing and Industry 4.0. Both of these paradigms are transforming factories into highly complex IT systems, generating massive amounts of data. The modeling and optimization of smart industrial processes, to support decision making, has consequently become a new application domain, with its own unique and peculiar issues, for some of the most prominent emerging technologies across the entire computing stack, from hardware and systems design to applicationlevel software. The massive data collection which forms the foundation of digital industries is enabled by increasingly complex and heterogeneous cyber physical systems (CPSs). Owing to that, the design of software for CPSs is receiving renewed interest from research and industry in relation to challenges such as managing variability and dependencies with an unprecedented degree of customization.},
  archive      = {J_TETC},
  author       = {Daniele Jahier Pagliari and Frank Schirrmeister and Nader Bagherzadeh and Enrico Macii},
  doi          = {10.1109/TETC.2022.3146784},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {6-8},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Guest editorial: Thematic section on applications of emerging computing technologies in smart manufacturing and industry 4.0},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TETC’s january 2022 editor-in-chief’s report on the state of
the journal. <em>TETC</em>, <em>10</em>(1), 4–5. (<a
href="https://doi.org/10.1109/TETC.2022.3145254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the state of the journal as reported by the Editor-in-Chief.},
  archive      = {J_TETC},
  author       = {Paolo Montuschi},
  doi          = {10.1109/TETC.2022.3145254},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {4-5},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {TETC&#39;s january 2022 editor-in-chief’s report on the state of the journal},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
