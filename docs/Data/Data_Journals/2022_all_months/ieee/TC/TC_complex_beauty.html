<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tc---260">TC - 260</h2>
<ul>
<li><details>
<summary>
(2022). Convolutional neural networks with discrete cosine transform
features. <em>TC</em>, <em>71</em>(12), 3389–3395. (<a
href="https://doi.org/10.1109/TC.2022.3150574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Discrete Cosine Transform (DCT) exposes features of an image that are not evident in the original image&#39;s spatial domain. This brief contribution proposes a Convolutional Neural Network architecture that combines features from the spatial domain and the DCT domain to improve image classification performance with negligible overhead.},
  archive      = {J_TC},
  author       = {Sanghyeon Ju and Youngjoo Lee and Sunggu Lee},
  doi          = {10.1109/TC.2022.3150574},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3389-3395},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Convolutional neural networks with discrete cosine transform features},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). WBMatrix: An optimized matrix library for white-box block
cipher implementations. <em>TC</em>, <em>71</em>(12), 3375–3388. (<a
href="https://doi.org/10.1109/TC.2022.3152449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {White-box block cipher (WBC) has been proposed by Chow et al. to prevent the secret key to be extracted from its implementation in an untrusted context. A pivotal technique behind WBC is to convert the iterated round functions into a series of look-up tables (LUTs) with encodings. The construction of encoded LUTs consists of matrix operations, such as multiplication and inversion. The widely-used matrix libraries in applications, such as open-source NTL and M4RI, are primarily designed for large dimensional matrix operations. Therefore, they might not be suitable for WBC implementations which are mainly based on small-scale matrices and vectors. In this paper, we propose a new matrix library named WBMatrix for the optimization of WBC implementations. WBMatrix reduces the operating steps of multiplication and simultaneously generates pairwise invertible matrices as encodings. The performance comparison supports that WBMatrix improves the table construction and encryption phases on Intel x86 and ARMv8 platforms. Moreover, WBMatrix also boosts the initialization and encryption phases of LowMC/LowMC-M block ciphers and enhances the performance for the generation of key-dependent Sbox.},
  archive      = {J_TC},
  author       = {Yufeng Tang and Zheng Gong and Tao Sun and Jinhai Chen and Zhe Liu},
  doi          = {10.1109/TC.2022.3152449},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3375-3388},
  shortjournal = {IEEE Trans. Comput.},
  title        = {WBMatrix: An optimized matrix library for white-box block cipher implementations},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Probabilistically guaranteeing end-to-end latencies in
autonomous vehicle computing systems. <em>TC</em>, <em>71</em>(12),
3361–3374. (<a href="https://doi.org/10.1109/TC.2022.3152105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Good responsiveness of autonomous vehicle computing systems is crucial to safety and performance of the vehicles. For example, an autonomous vehicle (AV) may cause an accident if the end-to-end latency from sensing a pedestrian to emergency stop is too high. However, the AV software stacks are too complex to probabilistically analye the end-to-end latency on a multi-core system. They consist of a graph of tasks with different periods, and have a large variability in the task execution times, which may lead to the maximum core utilization $U^{\max }$ greater than 1.0 on some cores. This paper proposes a novel stochastic analysis of the end-to-end latency over the AV stacks that allows $U^{\max }$ to exceed 1.0 on each core. The proposed analysis models the entire stack as a graph of task graphs under a multi-core partitioned scheduling and provides a probabilistic guarantee that the analyzed latency distribution upper-bounds the one observed from a real system under the assumption of independent task execution times. Using the Autoware stack with inter-task dependent execution times, it is shown that our analysis, combined with a task grouping to mitigate the inter-task correlations, can give a latency distribution for each task path that almost upper-bounds the observed one.},
  archive      = {J_TC},
  author       = {Hyoeun Lee and Youngjoon Choi and Taeho Han and Kanghee Kim},
  doi          = {10.1109/TC.2022.3152105},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3361-3374},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Probabilistically guaranteeing end-to-end latencies in autonomous vehicle computing systems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VTrust: Remotely executing mobile apps transparently with
local untrusted OS. <em>TC</em>, <em>71</em>(12), 3349–3360. (<a
href="https://doi.org/10.1109/TC.2022.3152074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasingly, many security and privacy-sensitive applications are running on mobile platforms. However, as mobile operating systems are becoming increasingly sophisticated, they are vulnerable to various attacks. In addressing the need of running high assurance mobile apps in a secure environment even though the operating systems are untrusted, this paper presents vTrust , a new mobile app trusted execution environment, which offloads the general execution and storage of a mobile app to a trusted remote server (e.g., a VM running in a cloud) and secures the I/O between the server and the mobile device with the aid of a trusted hypervisor on the mobile device. Specifically, vTrust establishes an encrypted I/O channel between the local hypervisor and the remote server. In this way, any sensitive data flowing through the mobile OS, which the hypervisor hosts, is encrypted from the perspective of the local mobile OS. To enhance the performance of vTrust , we have also designed multiple optimizations, such as output data compression and selective sensor data transmission. We have implemented vTrust , and our evaluation shows that it has limited impact on both user experience and the application performance.},
  archive      = {J_TC},
  author       = {Yutao Tang and Zhengrui Qin and Zhiqiang Lin and Yue Li and Shanhe Yi and Fengyuan Xu and Qun Li},
  doi          = {10.1109/TC.2022.3152074},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3349-3360},
  shortjournal = {IEEE Trans. Comput.},
  title        = {VTrust: Remotely executing mobile apps transparently with local untrusted OS},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Brain-inspired computing for circuit reliability
characterization. <em>TC</em>, <em>71</em>(12), 3336–3348. (<a
href="https://doi.org/10.1109/TC.2022.3151857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transistor scaling steadily approaches fundamental limits. Sustaining circuit reliability becomes an overwhelming challenge for foundries and their manufacturing processes. Therefore, early and rapid characterization of degradation effects impacting the circuits’ transistors becomes essential. Such degradation effects are caused by design-time variation due to manufacturing variability and/or run-time variation due to transistor aging. In this work, we are the first to employ brain-inspired HDC for circuit reliability. HDC is quickly emerging as an attractive light-weight machine-learning solution. Nowadays, it is mainly applied to bio-signal processing or language recognition. We bring the research of HDC to the next level by demonstrating how it can be applied to address the challenges in circuit reliability. This has far-reaching consequences due to the large savings achieved by 1) reducing the amount of training data and hence the development cycle, 2) removing the need to send the data to the cloud for model training, and 3) speeding up significantly the characterization and classification tasks due to the fast edge-inference. We demonstrate the viability of HDC using SRAM and other circuits as examples. HDC outperforms traditional machine learning methods, such as support vector machine or random forest, in accuracy and requires up to 20x fewer training samples. For a given budget of samples, HDC achieves a 4x smaller error. Our implementation and analysis are based on industrial $14 \;\mathrm{n}\mathrm{m}$ FinFET fully calibrated with Intel measurements with respect to both transistor electrical characteristics as well as manufacturing variability. Open Source: Our framework including the algorithm implementation is available for the community to explore other algorithms and circuits at https://github.com/ML-CAD/HDC-Circuit-Reliability .},
  archive      = {J_TC},
  author       = {Paul R. Genssler and Hussam Amrouch},
  doi          = {10.1109/TC.2022.3151857},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3336-3348},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Brain-inspired computing for circuit reliability characterization},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blockchain-cloud transparent data marketing: Consortium
management and fairness. <em>TC</em>, <em>71</em>(12), 3322–3335. (<a
href="https://doi.org/10.1109/TC.2022.3150724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data are generated by Internet of Things (IoT) devices and centralized at a cloud server, that can later be traded with third parties, i.e., data marketing, to enable various data-intensive applications. However, the centralized approach is recently under debate due to the lack of (1) transparent and distributed marketplace management, and (2) marketing fairness for both IoT users (data sellers) and third parties (data buyers). In this paper, we propose a Blockchain-Cloud Transparent Data Marketing ( Block-DM ) with consortium management and executable fairness. First, we introduce a hybrid data-marketing architecture, where the cloud acts as an efficient data management unit and a consortium blockchain serves as a transparent marketing controller. Under the architecture, consent-based secure data trading and identity privacy for data owners are achieved with the distributed credential issuance and threshold credential openings. Second, with a consortium committee, we design a fair on/off-chain data marketing protocol. By financial incentives and succinct ‘commitments’ of marketing operations, the protocol can achieve the marketing fairness and effective detection of unfair marketing operations. We demonstrate the security of Block-DM with thorough analysis. We conduct extensive experiments with a consortium blockchain network on Hyperledger Fabric to show the feasibility and practicality of Block-DM .},
  archive      = {J_TC},
  author       = {Dongxiao Liu and Cheng Huang and Jianbing Ni and Xiaodong Lin and Xuemin Sherman Shen},
  doi          = {10.1109/TC.2022.3150724},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3322-3335},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Blockchain-cloud transparent data marketing: Consortium management and fairness},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hardware-assisted malware detection and localization using
explainable machine learning. <em>TC</em>, <em>71</em>(12), 3308–3321.
(<a href="https://doi.org/10.1109/TC.2022.3150573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malicious software, popularly known as malware, is widely acknowledged as a serious threat to modern computing systems. Software-based solutions, such as anti-virus software (AVS), are not effective since they rely on matching patterns that can be easily fooled by carefully crafted malware with obfuscation or other deviation capabilities. While recent malware detection methods provide promising results through an effective utilization of hardware features, the detection results cannot be interpreted in a meaningful way. In this paper, we propose a hardware-assisted malware detection framework using explainable machine learning. This paper makes three important contributions. First, we theoretically establish that our proposed method can provide an interpretable explanation of classification results to address the challenge of transparency. Next, we show that the explainable outcome through effective utilization of hardware performance counters and embedded trace buffer can lead to accurate localization of malicious behavior. Finally, we have performed efficiency versus accuracy trade-off analysis using decision tree and recurrent neural networks. Extensive evaluation using a wide variety of real-world malware dataset demonstrates that our framework can produce accurate and human-understandable malware detection results with provable guarantees.},
  archive      = {J_TC},
  author       = {Zhixin Pan and Jennifer Sheldon and Prabhat Mishra},
  doi          = {10.1109/TC.2022.3150573},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3308-3321},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hardware-assisted malware detection and localization using explainable machine learning},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient and scalable FPGA design of GF(<span
class="math inline">2<sup><em>m</em></sup></span><!-- -->2m) inversion
for post-quantum cryptosystems. <em>TC</em>, <em>71</em>(12), 3295–3307.
(<a href="https://doi.org/10.1109/TC.2022.3149422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-quantum cryptosystems based on QC-MDPC codes are designed to mitigate the security threat posed by quantum computers to traditional public-key cryptography. The polynomial inversion is the core operation of key generation in such cryptosystems and the adoption of ephemeral keys imposes the execution of key generation for each session. To this end, there is a need for efficient and scalable hardware implementations of the binary polynomial inversion operation to support the key generation primitive across a wide range of computational platforms. This manuscript proposes an efficient and scalable architecture implementing the binary polynomial inversion at the hardware level. Our solution can deliver a performance-optimized implementation for the large polynomials used in post-quantum code-based cryptosystems and for each FPGA of the mid-range Xilinx Artix-7 family. The effectiveness of the proposed solution was validated by means of the BIKE and LEDAcrypt post-quantum QC-MDPC cryptosystems as representative use cases. Compared to the C11- and the optimized AVX2-based software implementations of LEDAcrypt, instances of the proposed architecture targeting the Artix-7 200 FPGA show an average performance improvement of 31.7 and 2.2 times, respectively. Moreover, the proposed architecture delivers a performance improvement up to 18.1 and 21.5 times for AES-128 and AES-192 security levels, respectively, compared to the BIKE hardware implementation.},
  archive      = {J_TC},
  author       = {Andrea Galimberti and Gabriele Montanaro and Davide Zoni},
  doi          = {10.1109/TC.2022.3149422},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3295-3307},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient and scalable FPGA design of GF($2^m$2m) inversion for post-quantum cryptosystems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MemUnison: A racetrack-ReRAM-combined pipeline architecture
for energy-efficient in-memory CNNs. <em>TC</em>, <em>71</em>(12),
3281–3294. (<a href="https://doi.org/10.1109/TC.2022.3148858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though ReRAM has been greatly successful in reducing energy consumption of various neural networks, it still suffers write amplification in energy, which impedes ReRAM to provide efficient storage for the ubiquitous streaming data in CNNs, such as feature-maps. Racetrack memory, an emerging magnetic memory technique, is a proper candidate to hold streaming data since it enjoys fast sequential-access with ultra-low operating energy in read and write. In this work, we propose a hybrid processing-in-memory architecture, called MemUnison, that coordinates ReRAM and racetrack to overcome the expenditure storage of streaming data in ReRAM. By placing feature-maps in racetrack and leaving weights in ReRAM, a datapath is constructed between the two sides to form a fetch-process-writeback pipeline. As the invalid-shifts of the racetrack memory incurs a large amount of pipeline bubble, we propose a row-based access that can read and write a feature-map without any invalid-shifts. For the row-based operation, a cohesive controlling method is proposed to coordinate racetrack and ReRAM. In runtime, convolution kernels are scheduled in ReRAM banks for cross-channel calculations of one row, by which computing complexity of a convolutional layer can be reduced by 4 orders of magnitude, excessing the 2 order of reduction by traditional ReRAM.},
  archive      = {J_TC},
  author       = {Jihe Wang and Jun Liu and Danghui Wang and Shengbing Zhang and Xiaoya Fan},
  doi          = {10.1109/TC.2022.3148858},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3281-3294},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MemUnison: A racetrack-ReRAM-combined pipeline architecture for energy-efficient in-memory CNNs},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extending on-chain trust to off-chain – trustworthy
blockchain data collection using trusted execution environment (TEE).
<em>TC</em>, <em>71</em>(12), 3268–3280. (<a
href="https://doi.org/10.1109/TC.2022.3148379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain creates a secure environment on top of strict cryptographic assumptions and rigorous security proofs. It permits on-chain interactions to achieve trustworthy properties such as traceability, transparency, and accountability. However, current blockchain trustworthiness is only confined to on-chain, creating a “trust gap” to the physical, off-chain environment. This is due to the lack of a scheme that can truthfully reflect the physical world in a real-time and consistent manner. Such an absence hinders further blockchain applications in the physical world, especially for the security-sensitive ones. In this paper, we propose a framework to extend blockchain trust from on-chain to off-chain, and take trustworthy vaccine tracing as an example scheme. Our scheme consists of 1) a Trusted Execution Environment (TEE)-enabled trusted environment monitoring system built with the Arm Cortex-M33 microcontroller that continuously senses the inside of a vaccine box through trusted sensors and generates anti-forgery data; and 2) a consistency protocol to upload the environment status data from the TEE system to blockchain in a truthful, real-time consistent, continuous and fault-tolerant fashion. Our security analysis indicates that no adversary can tamper with the vaccine in any way without being captured. We carry out an experiment to record the internal status of a vaccine shipping box during transportation, and the results indicate that the proposed system incurs an average latency of 84 ms in local sensing and processing followed by an average latency of 130 ms to have the sensed data transmitted to and been available in the blockchain.},
  archive      = {J_TC},
  author       = {Chunchi Liu and Hechuan Guo and Minghui Xu and Shengling Wang and Dongxiao Yu and Jiguo Yu and Xiuzhen Cheng},
  doi          = {10.1109/TC.2022.3148379},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3268-3280},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Extending on-chain trust to off-chain – trustworthy blockchain data collection using trusted execution environment (TEE)},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A systematic view of model leakage risks in deep neural
network systems. <em>TC</em>, <em>71</em>(12), 3254–3267. (<a
href="https://doi.org/10.1109/TC.2022.3148235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As deep neural networks (DNNs) continue to find applications in ever more domains, the exact nature of the neural network architecture becomes an increasingly sensitive subject, due to either intellectual property protection or risks of adversarial attacks. While prior work has explored aspects of the risk associated with model leakage, exactly which parts of the model are most sensitive and how one infers the full architecture of the DNN when nothing is known about the structure a priori are problems that have been left unexplored. In this paper we address this gap, first by presenting a schema for reasoning about model leakage holistically, and then by proposing and quantitatively evaluating DeepSniffer, a novel learning-based model extraction framework that uses no prior knowledge of the victim model. DeepSniffer is robust to architectural and system noises introduced by the complex memory hierarchy and diverse run-time system optimizations. Taking GPU platforms as a showcase, DeepSniffer performs model extraction by learning both the architecture-level execution features of kernels and the inter-layer temporal association information introduced by the common practice of DNN design. We demonstrate that DeepSniffer works experimentally in the context of an off-the-shelf Nvidia GPU platform running a variety of DNN models and that the extracted models significantly improve attempts at crafting adversarial inputs. The DeepSniffer project has been released in https://github.com/xinghu7788/DeepSniffer .},
  archive      = {J_TC},
  author       = {Xing Hu and Ling Liang and Xiaobing Chen and Lei Deng and Yu Ji and Yufei Ding and Zidong Du and Qi Guo and Timothy Sherwood and Yuan Xie},
  doi          = {10.1109/TC.2022.3148235},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3254-3267},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A systematic view of model leakage risks in deep neural network systems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CloudChain: A cloud blockchain using shared memory consensus
and RDMA. <em>TC</em>, <em>71</em>(12), 3242–3253. (<a
href="https://doi.org/10.1109/TC.2022.3147960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain technologies can enable secure computing environments among mistrusting parties. Permissioned blockchains are particularly enlightened by companies, enterprises, and government agencies due to their efficiency, customizability, and governance-friendly features. Obviously, seamlessly fusing blockchain and cloud computing can significantly benefit permissioned blockchains; nevertheless, most blockchains implemented on clouds are originally designed for loosely-coupled networks where nodes communicate asynchronously, failing to take advantages of the closely-coupled nature of cloud servers. In this paper, we propose an innovative cloud-oriented blockchain – CloudChain, which is a modularized three-layer system composed of the network layer, consensus layer, and blockchain layer. CloudChain is based on a shared-memory model where nodes communicate synchronously by direct memory accesses. We realize the shared-memory model with the Remote Direct Memory Access technology, based on which we propose a shared-memory consensus algorithm to ensure presistence and liveness, the two crucial blockchain security properties countering Byzantine nodes. We also implement a CloudChain prototype based on a RoCEv2-based testbed to experimentally validate our design, and the results verify the feasibility and efficiency of CloudChain.},
  archive      = {J_TC},
  author       = {Minghui Xu and Shuo Liu and Dongxiao Yu and Xiuzhen Cheng and Shaoyong Guo and Jiguo Yu},
  doi          = {10.1109/TC.2022.3147960},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3242-3253},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CloudChain: A cloud blockchain using shared memory consensus and RDMA},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predictive execution of parallel simulations in hard
real-time systems. <em>TC</em>, <em>71</em>(12), 3227–3241. (<a
href="https://doi.org/10.1109/TC.2022.3147416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware-in-the-loop systems are a type of Hard real-time system that require a faster than real-time simulation. A method of running hard real-time systems with slower than real-time simulations is devised. Instead of waiting for the real-time subsystem output to a simulation, multiple simulation input predictions are generated and used to run several parallel simulation paths independent of the real-time subsystem. The simulation path results are stored until needed by the real-time subsystem at which time one is selected and returned to it to continue the execution of the system. A test case using a small modular reactor nuclear power plant hardware-in-the-loop system was used to demonstrate the effects of slower than real-time simulations on a hard real-time system and to show that this method allows such a simulation to be used.},
  archive      = {J_TC},
  author       = {Michael Pietrykowski and Carol Smidts},
  doi          = {10.1109/TC.2022.3147416},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3227-3241},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Predictive execution of parallel simulations in hard real-time systems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FLiMS: A fast lightweight 2-way merger for sorting.
<em>TC</em>, <em>71</em>(12), 3215–3226. (<a
href="https://doi.org/10.1109/TC.2022.3146509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present FLiMS, a highly-efficient and simple parallel algorithm for merging two sorted lists residing in banked and/or wide memory. On FPGAs, its implementation uses fewer hardware resources than the state-of-the-art alternatives, due to the reduced number of comparators and elimination of redundant logic found on prior attempts. In combination with the distributed nature of the selector stage, a higher performance is achieved for the same amount of parallelism or higher. This is useful in many applications such as in parallel merge trees to achieve high-throughput sorting, where the resource utilisation of the merger is critical for building large trees and internalising the workload for fast computation. Also presented are efficient variations of FLiMS for optimizing throughput for skewed datasets, achieving stable sorting or using fewer dequeue signals. Additionally, FLiMS is shown to perform well as conventional software on modern CPUs supporting single-instruction multiple-data (SIMD) instructions, surpassing the performance of some standard libraries for sorting.},
  archive      = {J_TC},
  author       = {Philippos Papaphilippou and Wayne Luk and Chris Brooks},
  doi          = {10.1109/TC.2022.3146509},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3215-3226},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FLiMS: A fast lightweight 2-way merger for sorting},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). JBNN: A hardware design for binarized neural networks using
single-flux-quantum circuits. <em>TC</em>, <em>71</em>(12), 3203–3214.
(<a href="https://doi.org/10.1109/TC.2022.3215085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a high-performance application of low-temperature superconductivity, superconducting single-flux-quantum (SFQ) circuits have high speed and low-power consumption characteristics, which have recently received extensive attention, especially in the field of neural network inference accelerations. Despite these promising advantages, they are still limited by storage capacity and manufacture reliability, making them unfriendly for feedback loops and very large-scale circuits. The Binarized Neural Network (BNN), with minimal memory requirements and no reliance on multiplication, is undoubtedly an attractive candidate for implementing inference hardware using SFQ circuits. This work presents the first SFQ-based Binarized Neural Network inference accelerator, namely JBNN, with a new representation to binarize weights and activation variables. Every SFQ gate is essentially a pipeline stage, making conventional design methods of the accumulator unsuitable for SFQ circuits. So an SFQ-based accumulative parallel counter using SFQ logic cells including T1, OR, and AND is designed to realize the accumulation, where the data size is reduced to a quarter after passing the XNOR column and the AU layer, largely declining the hardware cost. Our evaluation shows that the proposed design outperforms a cryogenic CMOS-based BNN accelerator design running at 77K by 70.92 times while maintaining 97.89\% accuracy on the MNIST benchmark dataset. Without the cooling cost, the power efficiency increases up to 929.18 times.},
  archive      = {J_TC},
  author       = {Rongliang Fu and Junying Huang and Haibin Wu and Xiaochun Ye and Dongrui Fan and Tsung-Yi Ho},
  doi          = {10.1109/TC.2022.3215085},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3203-3214},
  shortjournal = {IEEE Trans. Comput.},
  title        = {JBNN: A hardware design for binarized neural networks using single-flux-quantum circuits},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CAMDNN: Content-aware mapping of a network of deep neural
networks on edge MPSoCs. <em>TC</em>, <em>71</em>(12), 3191–3202. (<a
href="https://doi.org/10.1109/TC.2022.3207137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) workloads are increasingly deployed at the edge. Enabling efficient inference execution while considering model and system heterogeneity remains challenging, especially for ML tasks built with a network of deep neural networks (DNNs). The challenge is to maximize the utilization of all available resources on the multiprocessor system on a chip (MPSoC) at the same time. This becomes even more complicated because the optimal mapping for the network of DNNs can vary with input batch sizes and scene complexity. In this paper, a holistic hierarchical scheduling framework is presented to optimize the execution time for a network of DNN models on an edge MPSoC at runtime, considering varying input characteristics. The framework consists of a local and a global scheduler. The local scheduler maps individual DNNs in the inference pipeline to the best-performing hardware unit while the global scheduler customizes an Integer Linear Programming (ILP) solution to instantiate DNN remapping. To minimize scheduler runtime overhead, an imitation learning (IL) based scheduler is used that approximates the ILP solutions. The proposed scheduling framework (CAMDNN) was implemented on a Qualcomm Robotic RB5 platform. CAMDNN resulted in lower execution time of up to 32\% than heterogeneous earliest finish time, and by factors of 6.67X, 5.6X and 2.17X than the CPU-only, GPU-only and Central Queue schedulers.},
  archive      = {J_TC},
  author       = {Soroush Heidari and Mehdi Ghasemi and Young Geun Kim and Carole-Jean Wu and Sarma Vrudhula},
  doi          = {10.1109/TC.2022.3207137},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3191-3202},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CAMDNN: Content-aware mapping of a network of deep neural networks on edge MPSoCs},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LNS-madam: Low-precision training in logarithmic number
system using multiplicative weight update. <em>TC</em>, <em>71</em>(12),
3179–3190. (<a href="https://doi.org/10.1109/TC.2022.3202747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representing deep neural networks (DNNs) in low-precision is a promising approach to enable efficient acceleration and memory reduction. Previous methods that train DNNs in low-precision typically keep a copy of weights in high-precision during the weight updates. Directly training with low-precision weights leads to accuracy degradation due to complex interactions between the low-precision number systems and the learning algorithms. To address this issue, we develop a co-designed low-precision training framework, termed LNS-Madam, in which we jointly design a logarithmic number system (LNS) and a multiplicative weight update algorithm (Madam). We prove that LNS-Madam results in low quantization error during weight updates, leading to stable performance even if the precision is limited. We further propose a hardware design of LNS-Madam that resolves practical challenges in implementing an efficient datapath for LNS computations. Our implementation effectively reduces energy overhead incurred by LNS-to-integer conversion and partial sum accumulation. Experimental results show that LNS-Madam achieves comparable accuracy to full-precision counterparts with only 8 bits on popular computer vision and natural language tasks. Compared to FP32 and FP8, LNS-Madam reduces the energy consumption by over 90\% and 55\%, respectively.},
  archive      = {J_TC},
  author       = {Jiawei Zhao and Steve Dai and Rangharajan Venkatesan and Brian Zimmer and Mustafa Ali and Ming-Yu Liu and Brucek Khailany and William J. Dally and Anima Anandkumar},
  doi          = {10.1109/TC.2022.3202747},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3179-3190},
  shortjournal = {IEEE Trans. Comput.},
  title        = {LNS-madam: Low-precision training in logarithmic number system using multiplicative weight update},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic sparse attention for scalable transformer
acceleration. <em>TC</em>, <em>71</em>(12), 3165–3178. (<a
href="https://doi.org/10.1109/TC.2022.3208206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers are the mainstream of NLP applications and are becoming increasingly popular in other domains such as Computer Vision. Despite the improvements in model quality, the enormous computation costs make Transformers difficult at deployment, especially when the sequence length is large in emerging applications. Processing attention mechanism as the essential component of Transformer is the bottleneck of execution due to the quadratic complexity. Prior art explores sparse patterns in attention to support long sequence modeling, but those pieces of work are on static or fixed patterns. We demonstrate that the sparse patterns are dynamic, depending on input sequences. Thus, we propose the Dynamic Sparse Attention (DSA) that can efficiently exploit dynamic sparse patterns in attention. Compared with other methods, our approach can achieve better trade-offs between accuracy and model complexity. Moving forward, we identify challenges and provide solutions to implement DSA on existing hardware (GPUs) and specialized hardware in order to achieve practical speedup and efficiency improvements for Transformer execution.},
  archive      = {J_TC},
  author       = {Liu Liu and Zheng Qu and Zhaodong Chen and Fengbin Tu and Yufei Ding and Yuan Xie},
  doi          = {10.1109/TC.2022.3208206},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3165-3178},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Dynamic sparse attention for scalable transformer acceleration},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Architecting a flash-based storage system for low-cost
inference of extreme-scale DNNs. <em>TC</em>, <em>71</em>(12),
3153–3164. (<a href="https://doi.org/10.1109/TC.2022.3209920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The size of deep neural network (DNN) models has been exploding rapidly, demanding a colossal amount of memory capacity. For example, Google has recently scaled its Switch Transformer to have a parameter size of up to 6.4 TB. However, today&#39;s HBM DRAM-based memory system for GPUs and DNN accelerators is suboptimal for these extreme-scale DNNs as it fails to provide enough capacity while its massive bandwidth is poorly utilized. Thus, we propose Leviathan , a DNN inference accelerator, which integrates a cost-effective flash-based storage system, instead. We carefully architect the storage system to provide enough memory bandwidth while preventing performance drop caused by read disturbance errors. Our evaluation of Leviathan demonstrates an 8.3× throughput gain compared to the iso-FLOPS DNN accelerator with conventional SSDs and up to 19.5× higher memory cost-efficiency than the HBM-based DNN accelerator.},
  archive      = {J_TC},
  author       = {Yunho Jin and Shine Kim and Tae Jun Ham and Jae W. Lee},
  doi          = {10.1109/TC.2022.3209920},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3153-3164},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Architecting a flash-based storage system for low-cost inference of extreme-scale DNNs},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-node acceleration for large-scale GCNs. <em>TC</em>,
<em>71</em>(12), 3140–3152. (<a
href="https://doi.org/10.1109/TC.2022.3207127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limited by the memory capacity and computation power, singe-node graph convolutional neural network (GCN) accelerators cannot complete the execution of GCNs within a reasonable amount of time, due to the explosive size of graphs nowadays. Thus, large-scale GCNs call for a multi-node acceleration system (MultiAccSys) like tensor processing unit (TPU) Pod for large-scale neural network. In this work, we aim to scale up single-node GCN accelerator to accelerate GCNs on large-scale graphs. We first identify the communication pattern and challenges of multi-node acceleration for GCNs on large-scale graphs. We observe that (1) irregular coarse-grained communication patterns exist in the execution of GCNs in MultiAccSys, which introduces massive amount of redundant network transmissions and off-chip memory accesses; (2) the acceleration of GCNs in MultiAccSys is mainly bounded by network bandwidth but tolerates network latency. Guided by the above observations, we then propose MultiGCN, an efficient MultiAccSys for large-scale GCNs that trades network latency for network bandwidth. Specifically, by leveraging the network latency tolerance, we first propose a topology-aware multicast mechanism with a one put per multicast message-passing model to reduce transmissions and alleviate network bandwidth requirements. Second , we introduce a scatter-based round execution mechanism which cooperates with the multicast mechanism and reduces redundant off-chip memory accesses. Compared to the baseline MultiAccSys, MultiGCN achieves 4∼ 12× speedup using only 28\% $\sim$ 68\% energy, while reducing 32\% transmissions and 73\% off-chip memory accesses on average. Besides, MultiGCN not only achieves 2.5 ∼ 8× speedup over the state-of-the-art multi-GPU solution, but also scales to large-scale graph as opposed to single-node GCN accelerators.},
  archive      = {J_TC},
  author       = {Gongjian Sun and Mingyu Yan and Duo Wang and Han Li and Wenming Li and Xiaochun Ye and Dongrui Fan and Yuan Xie},
  doi          = {10.1109/TC.2022.3207127},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3140-3152},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Multi-node acceleration for large-scale GCNs},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EGCN: An efficient GCN accelerator for minimizing off-chip
memory access. <em>TC</em>, <em>71</em>(12), 3127–3139. (<a
href="https://doi.org/10.1109/TC.2022.3211413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Graph Convolutional Networks (GCNs) have emerged as a promising solution for graph representation learning, designing specialized GCN accelerators has become an important challenge. An analysis of GCN workloads shows that the main bottleneck of GCN processing is not computation but the memory latency of intensive off-chip data transfer. Therefore, minimizing off-chip data transfer is the primary challenge for designing an efficient GCN accelerator. To address this challenge, optimization is initialized by considering GCNs as tiled matrix multiplication. In this paper, we optimize off-chip memory access from both the in- and out-of-tile perspectives. From the out-of-tile perspective, we find optimal tile configurations of given datasets and on-chip buffer capacity, then observe the dataflow across phases and layers. Inter-layer phase fusion dataflow with optimal tile configuration reduces data transfer of intermediate outputs. From the in-tile perspective, due to the sparsity of tiles, tiles have redundant data which does not participate in computation. Redundant data load is eliminated with hardware support. Finally, we introduce an efficient GCN inference accelerator, EGCN, specialized for minimizing off-chip memory access. EGCN achieves 41.9\% off-chip DRAM access reduction, 1.49× speedup, and 1.95× energy efficiency improvement on average over the state-of-the-art accelerators.},
  archive      = {J_TC},
  author       = {Yunki Han and Kangkyu Park and Youngbeom Jung and Lee-Sup Kim},
  doi          = {10.1109/TC.2022.3211413},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3127-3139},
  shortjournal = {IEEE Trans. Comput.},
  title        = {EGCN: An efficient GCN accelerator for minimizing off-chip memory access},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Future scaling of memory hierarchy for tensor cores and
eliminating redundant shared memory traffic using inter-warp
multicasting. <em>TC</em>, <em>71</em>(12), 3115–3126. (<a
href="https://doi.org/10.1109/TC.2022.3207134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The CUDA core of NVIDIA GPUs had been one of the most efficient computation units for parallel computing. However, recent rapid developments in deep neural networks demand an even higher level of computational performance. To meet this requirement, NVIDIA has introduced the Tensor core in recent generations. However, their impressive enhancements in computational performance have newly brought high pressure on the memory hierarchy. In this paper, first we identify the required memory bandwidth in the memory hierarchy as the computational performance increases in actual GPU hardware. Through a comparison of the CUDA core and the Tensor core in V100, we find that the tremendous performance increase of the Tensor core requires much higher memory bandwidth than that in the CUDA core. Moreover, we thoroughly investigate memory bandwidth requirement over Tensor core generations of V100, RTX TITAN, and A100. Lastly, we analyze a hypothetical next-generation Tensor core introduced by NVIDIA through a GPU simulation, through which we propose an inter-warp multicasting microarchitecture that reduces redundant shared memory (SMEM) traffic during the GEMM process. Our evaluation shows that inter-warp multicasting reduces the SMEM bandwidth pressure by 33\% and improves the performance by 19\% on average in all layers of ResNet-152 and BERT-Large.},
  archive      = {J_TC},
  author       = {Sunjung Lee and Seunghwan Hwang and Michael Jaemin Kim and Jaewan Choi and Jung Ho Ahn},
  doi          = {10.1109/TC.2022.3207134},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3115-3126},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Future scaling of memory hierarchy for tensor cores and eliminating redundant shared memory traffic using inter-warp multicasting},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Algorithm and hardware co-design of energy-efficient LSTM
networks for video recognition with hierarchical tucker tensor
decomposition. <em>TC</em>, <em>71</em>(12), 3101–3114. (<a
href="https://doi.org/10.1109/TC.2022.3212642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long short-term memory (LSTM) is a type of powerful deep neural network that has been widely used in many sequence analysis and modeling applications. However, the large model size problem of LSTM networks make their practical deployment still very challenging, especially for the video recognition tasks that require high-dimensional input data. Aiming to overcome this limitation and fully unlock the potentials of LSTM models, in this paper we propose to perform algorithm and hardware co-design towards high-performance energy-efficient LSTM networks. At algorithm level, we propose to develop fully decomposed hierarchical Tucker (FDHT) structure-based LSTM, namely FDHT-LSTM, which enjoys ultra-low model complexity while still achieving high accuracy. In order to fully reap such attractive algorithmic benefit, we further develop the corresponding customized hardware architecture to support the efficient execution of the proposed FDHT-LSTM model. With the delicate design of memory access scheme, the complicated matrix transformation can be efficiently supported by the underlying hardware without any access conflict in an on-the-fly way. Our evaluation results show that both the proposed ultra-compact FDHT-LSTM models and the corresponding hardware accelerator achieve very high performance. Compared with the state-of-the-art compressed LSTM models, FDHT-LSTM enjoys both order-of-magnitude reduction (more than $1000 \times$ ) in model size and significant accuracy improvement (0.6\% to 12.7\%) across different video recognition datasets. Meanwhile, compared with the state-of-the-art tensor decomposed model-oriented hardware TIE, our proposed FDHT-LSTM architecture achieve $2.5\times$ , $1.46\times$ and $2.41\times$ increase in throughput, area efficiency and energy efficiency, respectively on LSTM-Youtube workload. For LSTM-UCF workload, our proposed design also outperforms TIE with $1.9\times$ higher throughput, $1.83\times$ higher energy efficiency and comparable area efficiency.},
  archive      = {J_TC},
  author       = {Yu Gong and Miao Yin and Lingyi Huang and Chunhua Deng and Bo Yuan},
  doi          = {10.1109/TC.2022.3212642},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3101-3114},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Algorithm and hardware co-design of energy-efficient LSTM networks for video recognition with hierarchical tucker tensor decomposition},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ReAAP: A reconfigurable and algorithm-oriented array
processor with compiler-architecture co-design. <em>TC</em>,
<em>71</em>(12), 3088–3100. (<a
href="https://doi.org/10.1109/TC.2022.3213177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallelism and data reuse are the most critical issues for the design of hardware acceleration in a deep learning processor. Besides, abundant on-chip memories and precise data management are intrinsic design requirements because most of deep learning algorithms are data-driven and memory-bound. In this paper, we propose a compiler-architecture co-design scheme targeting a reconfigurable and algorithm-oriented array processor, named ReAAP. Given specific deep neural networks, the proposed co-design scheme is effective to perform parallelism and data reuse optimization on compute-intensive layers for guiding reconfigurable computing in hardware. Especially, the systemic optimization is performed in our proposed domain-specific compiler to deal with the intrinsic tensions between parallelism and data locality, for the purpose of automatically mapping diverse layer-level workloads onto our proposed reconfigurable array architecture. In this architecture, abundant on-chip memories are software-controlled and its massive data access is precisely handled by compiler-generated instructions. In our experiments, the ReAAP is implemented on an embedded FPGA platform. Experimental results demonstrate that our proposed co-design scheme is effective to integrate software flexibility with hardware parallelism for accelerating diverse deep learning workloads. As a whole system, ReAAP achieves a consistently high utilization of hardware resource for accelerating all the diverse compute-intensive layers in ResNet, MobileNet, and BERT.},
  archive      = {J_TC},
  author       = {Jianwei Zheng and Yu Liu and Xuejiao Liu and Luhong Liang and Deming Chen and Kwang-Ting Cheng},
  doi          = {10.1109/TC.2022.3213177},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3088-3100},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ReAAP: A reconfigurable and algorithm-oriented array processor with compiler-architecture co-design},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). End-to-end synthesis of dynamically controlled machine
learning accelerators. <em>TC</em>, <em>71</em>(12), 3074–3087. (<a
href="https://doi.org/10.1109/TC.2022.3211430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge systems are required to autonomously make real-time decisions based on large quantities of input data under strict power, performance, area, and other constraints. Meeting these constraints is only possible by specializing systems through hardware accelerators purposefully built for machine learning and data analysis algorithms. However, data science evolves at a quick pace, and manual design of custom accelerators has high non-recurrent engineering costs: general solutions are needed to automatically and rapidly transition from the formulation of a new algorithm to the deployment of a dedicated hardware implementation. Our solution is the SOftware Defined Architectures (SODA) Synthesizer, an end-to-end, multi-level, modular, extensible compiler toolchain providing a direct path from machine learning tools to hardware. The SODA Synthesizer frontend is based on the multilevel intermediate representation (MLIR) framework; it ingests pre-trained machine learning models, identifies kernels suited for acceleration, performs high-level optimizations, and prepares them for hardware synthesis. In the backend, SODA leverages state-of-the-art high-level synthesis techniques to generate highly efficient accelerators, targeting both field programmable devices (FPGAs) and application-specific circuits (ASICs). In this paper, we describe how the SODA Synthesizer can also assemble the generated accelerators (based on the finite state machine with datapath model) in a custom system driven by a distributed controller, building a coarse-grained dataflow architecture that does not require a host processor to orchestrate parallel execution of multiple accelerators. We show the effectiveness of our approach by automatically generating ASIC accelerators for layers of popular deep neural networks (DNNs). Our high-level optimizations result in up to 74x speedup on isolated accelerators for individual DNN layers, and our dynamically scheduled architecture yields an additional 3x performance improvement when combining accelerators to handle streaming inputs.},
  archive      = {J_TC},
  author       = {Serena Curzel and Nicolas Bohm Agostini and Vito Giovanni Castellana and Marco Minutoli and Ankur Limaye and Joseph Manzano and Jeff Zhang and David Brooks and Gu-Yeon Wei and Fabrizio Ferrandi and Antonino Tumeo},
  doi          = {10.1109/TC.2022.3211430},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3074-3087},
  shortjournal = {IEEE Trans. Comput.},
  title        = {End-to-end synthesis of dynamically controlled machine learning accelerators},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guest editorial: IEEE TC special issue: Hardware
acceleration of machine learning. <em>TC</em>, <em>71</em>(12),
3072–3073. (<a href="https://doi.org/10.1109/TC.2022.3216428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special section explore research on the latest topics related to the hardware acceleration of machine learning.},
  archive      = {J_TC},
  author       = {Michael Ferdman and Jorge Albericio and Tushar Krishna and Peter Milder},
  doi          = {10.1109/TC.2022.3216428},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3072-3073},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Guest editorial: IEEE TC special issue: hardware acceleration of machine learning},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trojan detection in embedded systems with FinFET technology.
<em>TC</em>, <em>71</em>(11), 3061–3071. (<a
href="https://doi.org/10.1109/TC.2022.3146217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study considers detecting Trojans in circuits using FinFET technology non-destructively, when a golden Integrated Circuit (IC) is unavailable. The method employs short-term aging effects in FinFET transistors and circuit overclocking to induce bit errors at the circuit outputs in conjunction with Machine Learning (ML) tools learning Trojan-free behavior. Short-term aging causes delays along multiple paths in the IC to vary dynamically, causing bit errors at circuit outputs. Overclocking enhances this in FinFET but is not necessary for bulk CMOS technology. We use bit error patterns at the output of the circuit to detect Trojans using an ML classifier trained on simulations of the Trojan-free circuit. The study shows efficacy of the method by using dynamic short-term aging-aware standard cell libraries with FinFET technology that are modeled by considering the dynamic short-term aging of each cell. Trojan detection is robust to chip-to-chip variations. We apply the technique on fourteen Trust-Hub Trojans. Our method detects Trojans with $&amp;gt;$ 95\% accuracy. Trojan detection in FinFET technology is more challenging than in bulk CMOS because the voltage range for switching from a high to low value is smaller. Therefore we use overclocking.},
  archive      = {J_TC},
  author       = {Virinchi Roy Surabhi and Prashanth Krishnamurthy and Hussam Amrouch and Jörg Henkel and Ramesh Karri and Farshad Khorrami},
  doi          = {10.1109/TC.2022.3146217},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3061-3071},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Trojan detection in embedded systems with FinFET technology},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerating address translation for virtualization by
leveraging hardware mode. <em>TC</em>, <em>71</em>(11), 3047–3060. (<a
href="https://doi.org/10.1109/TC.2022.3145671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The overhead of memory virtualization remains nontrivial. The traditional shadow paging (TSP) resorts to a shadow page table (SPT) to achieve the native page walk speed, but page table updates require hypervisor interventions. Alternatively, nested paging enables low-overhead page table updates, but utilizes the hardware MMU to perform a long-latency two-dimensional page walk. This paper proposes new memory virtualization solutions based on hardware (machine) mode—the highest CPU privilege level in some architectures like Sunway and RISC-V. A programming interface, running in hardware mode, enables software-implementation of hardware support functions. We first propose Software-based Nested Paging (SNP) , which extends the software MMU to perform a two-dimensional page walk in hardware mode. Second, we present Swift Shadow Paging (SSP) , which accomplishes page table synchronization by intercepting TLB flushing in hardware mode. Finally we propose Accelerated Shadow Paging (ASP) combining SSP and SNP. ASP handles the last-level SPT page faults by walking two-dimensional page tables in hardware mode, which eliminates most hypervisor interventions. This paper systematically compares multiple memory virtualization models by analyzing their designs and evaluating their performance both on a real system and a simulator. The experiments show that the virtualization overhead of ASP is less than 4.5\% for all workloads.},
  archive      = {J_TC},
  author       = {Sai Sha and Yi Zhang and Yingwei Luo and Xiaolin Wang and Zhenlin Wang},
  doi          = {10.1109/TC.2022.3145671},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3047-3060},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Accelerating address translation for virtualization by leveraging hardware mode},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ParaX: Bandwidth-efficient instance assignment for DL on
multi-NUMA many-core CPUs. <em>TC</em>, <em>71</em>(11), 3032–3046. (<a
href="https://doi.org/10.1109/TC.2022.3145164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commercial clouds now heavily use CPUs in DL (deep learning) because there are large numbers of CPUs which would otherwise sit idle during off-peak periods. Following the trend, CPU vendors have not only released high-performance many-core CPUs but also developed efficient math kernel libraries. However, current DL platforms cannot scale well to a large number of CPU cores, making many-core CPUs inefficient in DL computation. We analyze the memory access patterns of various layers and identify the root cause of the low scalability, i.e., the per-layer barriers that are implicitly imposed by current platforms which assign one single instance (i.e., one batch of input data) to a CPU. The barriers cause severe memory bandwidth contention and CPU starvation in the access-intensive layers (like activation and BN). This paper presents a novel approach called ParaX, which boosts the performance of DL on multi-NUMA (non-uniform memory access) many-core CPUs by effectively alleviating bandwidth contention and CPU starvation. Our key idea is to assign one instance to each CPU core instead of to the entire CPU, so as to remove the per-layer barriers on the executions of the many cores. ParaX designs an ultralight scheduling policy which sufficiently overlaps the access-intensive layers with the compute-intensive ones to avoid contention, and proposes a NUMA-aware gradient server mechanism for training which leverages shared memory to substantially reduce the overhead of per-iteration parameter synchronization. We have implemented ParaX on MXNet. Extensive evaluation on a two-NUMA Intel 8280 CPU shows that ParaX significantly improves the training/inference throughput for all tested models (for image recognition and natural language processing) by $1.73\times \sim 2.93{\times}$ .},
  archive      = {J_TC},
  author       = {Yiming Zhang and Lujia Yin and Dongsheng Li and Yuxing Peng and Kai Lu},
  doi          = {10.1109/TC.2022.3145164},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3032-3046},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ParaX: Bandwidth-efficient instance assignment for DL on multi-NUMA many-core CPUs},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GANDAFL: Dataflow acceleration for short read alignment on
NGS data. <em>TC</em>, <em>71</em>(11), 3018–3031. (<a
href="https://doi.org/10.1109/TC.2022.3144115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DNA read alignment is an integral part of genome study, which has been revolutionised thanks to the growth of Next Generation Sequencing (NGS) technologies. The inherent computational intensity of string matching algorithms such as Smith-Waterman (SmW) and the vast amount of NGS input data, create a bottleneck in the workflows. Accelerated reconfigurable computing has been extensively leveraged to alleviate this bottleneck, focusing on high-performance albeit standalone implementations. In existing accelerated solutions effective co-design of NGS short-read alignment still remains an open issue, mainly due to narrow view on real integration aspects, such as system wide communication and accelerator call overheads. In this paper, we first propose GANDAFL , a novel G enome A ligNment DA ta- FL ow architecture for SmW Matrix-fill and Traceback stages to perform high throughput short-read alignment on NGS data. We then propose a radical software restructuring to widely-used Bowtie2 aligner that allows read alignment by batches to expose acceleration capabilities. Batch alignment minimizes calling overhead of the accelerators whereas moving both Matrix-fill and Traceback on chip extinguishes the communication data overheads. The standalone solution delivers up to ×116 and ×2 speedup over state-of-the-art software and hardware accelerators respectively and GANDAFL-enhanced Bowtie2 aligner delivers a ×1.9 speedup.},
  archive      = {J_TC},
  author       = {Konstantina Koliogeorgi and Sotirios Xydis and Georgi Gaydadjiev and Dimitrios Soudris},
  doi          = {10.1109/TC.2022.3144115},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3018-3031},
  shortjournal = {IEEE Trans. Comput.},
  title        = {GANDAFL: Dataflow acceleration for short read alignment on NGS data},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A low complexity and long period digital random sequence
generator based on residue number system and permutation polynomial.
<em>TC</em>, <em>71</em>(11), 3008–3017. (<a
href="https://doi.org/10.1109/TC.2022.3143702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long period digital random sequence plays an important role in reliable communications and high security scenarios. This paper improved the method of generating long period digital random sequences based on the Residue Number System (RNS) and the Chinese Remainder Theorem (CRT), and a sequence mapping method after CRT extension. This paper proves that the period of sequence after mapping will not degenerate if the modulus used in the mapping stage is coprime with the period of the original sequence. By using the parallelism of RNS, the proposed method can generate sequences at high speed with fewer hardware resources. The NIST test results show that the pass rate of each test item is above 98.40\%, which meets the NIST test confidence requirements, confirming the randomness of the generated sequences. An image encryption test is given as one of the example applications of the generated sequences. On the theoretical basis, by jointly optimizing the sequence mapping and iteration procedure, a hardware implementation architecture is also presented in this paper. The implementation is based on Xilinx XC7Z020CLG484-3 FPGA and compared with the implementations of classical chaotic maps. The results show that the proposed architecture has longer sequence period with less hardware resource consumption and higher generation speed and is more general. Meanwhile, the proposed architecture has fast phase switching ability, which is about 10 clock periods. This is one of the key attributes when the sequence is used in communication systems.},
  archive      = {J_TC},
  author       = {Shilin Chen and Shang Ma and Zhuo Qin and Bixin Zhu and Ziqian Xiao and Meiqing Liu},
  doi          = {10.1109/TC.2022.3143702},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3008-3017},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A low complexity and long period digital random sequence generator based on residue number system and permutation polynomial},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time-memory trade-offs for saber+ on memory-constrained
RISC-v platform. <em>TC</em>, <em>71</em>(11), 2996–3007. (<a
href="https://doi.org/10.1109/TC.2022.3143441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Saber is a module-lattice-based key encapsulation scheme that has been selected as a finalist in the NIST Post-Quantum Cryptography standardization project. As Saber computes on considerably large matrices and vectors of polynomials, its efficient implementation on memory-constrained IoT devices is very challenging. In this paper, we present an implementation of Saber with a minor tweak to the original Saber protocol for achieving reduced memory consumption and better performance. We call this tweaked implementation ‘Saber+’, and the difference compared to Saber is that we use different generation methods of public matrix $\boldsymbol{A}$ and secret vector $\boldsymbol{s}$ for memory optimization. Our highly optimized software implementation of Saber+ on a memory-constrained RISC-V platform achieves 48\% performance improvement compared with the best state-of-the-art memory-optimized implementation of original Saber. Specifically, we present various memory and performance optimizations for Saber+ on a memory-constrained RISC-V microcontroller, with merely 16KB of memory available. We utilize the Number Theoretic Transform (NTT) to speed up the polynomial multiplication in Saber+. For optimizing cycle counts and memory consumption during NTT, we carefully compare the efficiency of the complete and incomplete-NTTs, with platform-specific optimization. We implement 4-layers merging in the complete-NTT and 3-layers merging in the 6-layer incomplete-NTT. An improved on-the-fly generation strategy of the public matrix and secret vector in Saber+ results in low memory footprint. Furthermore, by combining different optimization strategies, various time-memory trade-offs are explored. Our software implementation for Saber+ on selected RISC-V core takes just 3,809K, 3,594K, and 3,193K clock cycles for key generation, encapsulation, and decapsulation, respectively, while consuming only 4.8KB of stack at most.},
  archive      = {J_TC},
  author       = {Jipeng Zhang and Junhao Huang and Zhe Liu and Sujoy Sinha Roy},
  doi          = {10.1109/TC.2022.3143441},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2996-3007},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Time-memory trade-offs for saber+ on memory-constrained RISC-V platform},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight, effective detection and characterization of
mobile malware families. <em>TC</em>, <em>71</em>(11), 2982–2995. (<a
href="https://doi.org/10.1109/TC.2022.3143439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Android malware is an ongoing threat to billions of smart devices’ security, ranging from mobile phones to car infotainment systems. Despite numerous approaches and previous studies to develop solutions for detecting and preventing Android malware, the rapid continuous development of new malware variants requires a careful reconsideration and the development of effective methods to identify malware families given a meager number of malware instances. In this paper, we present DroidMalVet, a novel Android malware family classification and detection approach that does not require to perform complex program analyses or utilize large feature sets. DroidMalVet is the first to use a promising, diverse, and small set of software metrics as features in a supervised learning platform to classify and detect various Android malware families. Our extensive empirical evaluations on two large public malware datasets show that DroidMalVet accurately detects both small and large malware families with F-Score accuracy of 94.4\% and 96\%, and AUC equal to 99.5\% and 99.7\% on the malware families in Drebin and AMD datasets, respectively. Moreover, our results demonstrate the superior performance of DroidMalVet in detecting small families (i.e., families with few samples). DroidMalVet complements existing approaches and presents an early warning tool for detecting known and emerging malware families.},
  archive      = {J_TC},
  author       = {Karim O. Elish and Mahmoud O. Elish and Hussain M. J. Almohri},
  doi          = {10.1109/TC.2022.3143439},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2982-2995},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Lightweight, effective detection and characterization of mobile malware families},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Better adaptive malicious users detection algorithm in human
contact networks. <em>TC</em>, <em>71</em>(11), 2968–2981. (<a
href="https://doi.org/10.1109/TC.2022.3142626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A human contact network (HCN) consists of individuals moving around and interacting with each other. In HCN, it is essential to detect malicious users who break the data delivery through terminating the data delivery or tampering with the data. Since malicious users will pay more but gain less when breaking the data delivery of opportunistic contacts, we focus on the non-opportunistic contacts that occur more frequently and stably. It is observed that people contact with each other more frequently if they have more social features in common. In this paper, we build up topology structure for HCN based on social features, and propose a graph theoretical comparison detection model to perform malicious users detection. Then we present an adaptive detection scheme based on Hamiltonian cycle decomposition. Also, we define comparison-0-string and comparison-1-string to improve the detection efficiency. Moreover, we perform scenario simulations on real data to realize the detected process of malicious users. Experiments show that, when the number of malicious users is bounded by the dimension of HCN, our scheme has a detection rate of 100\% with both false positive rate and false negative rate being 0\%, and the running cost is also very low when compared to baseline approaches. When the number of malicious users exceeds the bound, the detection rate of our scheme decreases slowly, while the false positive rate and false negative rate increase slowly, but they are still better than the baseline approaches.},
  archive      = {J_TC},
  author       = {Limei Lin and Yanze Huang and Li Xu and Sun-Yuan Hsieh},
  doi          = {10.1109/TC.2022.3142626},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2968-2981},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Better adaptive malicious users detection algorithm in human contact networks},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards a tractable exact test for global multiprocessor
fixed priority scheduling. <em>TC</em>, <em>71</em>(11), 2955–2967. (<a
href="https://doi.org/10.1109/TC.2022.3142540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scheduling algorithms are called “global” if they can migrate tasks between cores. Global scheduling algorithms are the de-facto standard practice for general purpose Operating Systems, to balance the workload between cores. However, the exact schedulability analysis of real-time applications for these algorithms is proven to be weakly NP-hard. Despite such a hardness, the research community keeps investigating the methods for an exact schedulability analysis for its relevance and to tightly estimate the execution requirements of real-time systems. Due to the NP-hardness, the available exact tests are very time and memory demanding even for sets of a few tasks. On another hand, the available sufficient tests are very pessimistic, despite consuming less resources. Motivated by these observations, we propose an exact schedulability test for constrained-deadline sporadic tasks under global multiprocessor fixed-priority scheduling scheduler, which is significantly faster and consumes less memory, compared to any other available exact test. To derive a faster test, we exploit the idea of a state-space pruning, aiming at reducing the number of feasible system states to be examined by the test. The resulted test is multiple orders of magnitude faster with respect to other state-of-the-art exact tests. Our C++ implementation is publicly available.},
  archive      = {J_TC},
  author       = {Artem Burmyakov and Enrico Bini and Chang-Gun Lee},
  doi          = {10.1109/TC.2022.3142540},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2955-2967},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Towards a tractable exact test for global multiprocessor fixed priority scheduling},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stereo: Assignment and scheduling in MPSoC under process
variation by combining stochastic and decomposition approaches.
<em>TC</em>, <em>71</em>(11), 2940–2954. (<a
href="https://doi.org/10.1109/TC.2022.3141841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aggressive scaling in integrated circuits creates new challenges such as an increase in power density, temperature, and especially process variation in designing Multiprocessor Systems-on-Chip (MPSoC). While most of the previous works attempt to mitigate the process variation effects at the system level, the eventual design still suffers from the variability of frequency and leakage power. In this paper, we propose a method called Stereo that combines s tochas t ic and d e composition to solve task assignment and scheduling under p r oc e ss variati o n in MPSoCs. In our previous work, we formulated a Mixed Integer Linear Programming (MILP) problem for variation-aware task assignment and scheduling to optimize energy consumption while meeting the real-time constraints. To capture the stochastic behavior of process variation, we employed a chance-constrained programming technique to turn the problem into a corresponding stochastic optimization that can be solved by typical ILP solvers. However, it had a scalability problem. To address this issue, in this work, we leverage a Logic-based Benders Decomposition (LBD) approach to improve the running time for finding an optimal solution of assignments and schedulings under process variation phenomenon). We carried out extensive experiments using Embedded System Synthesis Benchmarks Suite (E3S). The experimental results of the Stereo method evince considerable improvements compared to the baseline method in terms of performance-yield and run-time. The Stereo-based MILP method ameliorates performance-yield up to 2× and run-time by 532×. Moreover, for manifold applications, the Stereo-based LBD method archives 3.47×-91.49× run-time improvement compared to the Stereo-based MILP approach and is capable of assigning and scheduling of more than 50 tasks on 9 processors.},
  archive      = {J_TC},
  author       = {Behnam Khodabandeloo and Ahmad Khonsari and Payman Behnam and Alireza Majidi and Mohammad Hassan Hajiesmaili},
  doi          = {10.1109/TC.2022.3141841},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2940-2954},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Stereo: Assignment and scheduling in MPSoC under process variation by combining stochastic and decomposition approaches},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiplicative complexity of XOR based regular functions.
<em>TC</em>, <em>71</em>(11), 2927–2939. (<a
href="https://doi.org/10.1109/TC.2022.3141249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {XOR-AND Graphs (XAGs) are an enrichment of the classical AND-Inverter Graphs (AIGs) with XOR nodes. In particular, XAGs are networks composed by ANDs, XORs, and inverters. Besides several emerging technologies applications, XAGs are often exploited in cryptography-related applications based on the multiplicative complexity of a Boolean function. The multiplicative complexity of a function is the minimum number of AND gates (i.e., multiplications) that are sufficient to represent the function over the basis {AND, XOR, NOT}. In fact, the minimization of the number of AND gates is important for high-level cryptography protocols such as secure multiparty computation, where processing AND gates is more expensive than processing XOR gates. Moreover, it is an indicator of the degree of vulnerability of the circuit, as a small number of AND gates corresponds to a high vulnerability to algebraic attacks. In this paper we study the multiplicative complexity of Boolean functions characterized by two particular regularities, called autosymmetry and D-reducibility. Moreover, we exploit these regularities for decreasing the number of AND nodes in XAGs. The experimental results validate the proposed approaches.},
  archive      = {J_TC},
  author       = {Anna Bernasconi and Stelvio Cimato and Valentina Ciriani and Maria Chiara Molteni},
  doi          = {10.1109/TC.2022.3141249},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2927-2939},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Multiplicative complexity of XOR based regular functions},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A decentralized mechanism based on differential privacy for
privacy-preserving computation in smart grid. <em>TC</em>,
<em>71</em>(11), 2915–2926. (<a
href="https://doi.org/10.1109/TC.2021.3130402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most successful industrial realizations of Internet of Things, a smart grid is a smart IoT system that deploys widespread smart meters to capture fine-grained data on residential power usage. Unfortunately, it always suffers diverse privacy attacks, which seriously increases the risk of violating the privacy of customers. Although some solutions have been proposed to address this privacy issue, most of them mainly rely on a trusted party and focus on the sanitization of metering masurements. Moreover, these solutions are vulnerable to advanced attacks. In this paper, we propose a decentralized mechanism for privacy-preserving computation in smart grid called DDP, which leaverages the differential privacy and extends the data sanitization from the value domain to the time domain. Specifically, we inject Laplace noise to the measurements at the end of each customer in a distributed manner, and then use a random permutation algorithm to shuffle the power measurement sequence, thereby enforcing differential privacy after aggregation and preventing the sensitive power usage mode informaton of the customers from being inferred by other parties. Extensive experiments demonstrate that DDP shows an outstanding performance in terms of privacy from the non-intrusive load monitoring (NILM) attacks and utility by using two different error analysis.},
  archive      = {J_TC},
  author       = {Zhigao Zheng and Tao Wang and Ali Kashif Bashir and Mamoun Alazab and Shahid Mumtaz and Xiaoyan Wang},
  doi          = {10.1109/TC.2021.3130402},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2915-2926},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A decentralized mechanism based on differential privacy for privacy-preserving computation in smart grid},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting hardware-based data-parallel and multithreading
models for smart edge computing in reconfigurable FPGAs. <em>TC</em>,
<em>71</em>(11), 2903–2914. (<a
href="https://doi.org/10.1109/TC.2021.3107196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current edge computing systems are deployed in highly complex application scenarios with dynamically changing requirements. In order to provide the expected performance and energy efficiency values in these situations, the use of heterogeneous hardware/software platforms at the edge has become widespread. However, these computing platforms still suffer from the lack of unified software-driven programming models to efficiently deploy multi-purpose hardware-accelerated solutions. In parallel, edge computing systems also face another huge challenge: operating under multiple conditions that were not taken into account during any of the design stages. Moreover, these conditions may change over time, forcing self-adaptation mechanisms to become a must. This paper presents an integrated architecture to exploit hardware-accelerated data-parallel models and transparent hardware/software multithreading. In particular, the proposed architecture leverages the ARTICo 3 framework and ReconOS to allow developers to select the most suitable programming model to deploy their edge computing applications onto run-time reconfigurable hardware devices. An evolvable hardware system is used as an additional architectural component during validation, providing support for continuous lifelong learning in smart edge computing scenarios. In particular, the proposed setup exhibits online learning capabilities that include learning by imitation from software-based reference algorithms. Experimental results show the benefits of the proposed approach, exposing different run-time tradeoffs (e.g., computing performance versus functional correctness of the evolved solutions), and highlighting the benefits of using scalable data-parallel models to perform circuit evolution under dynamically changing application scenarios.},
  archive      = {J_TC},
  author       = {Alfonso Rodríguez and Andrés Otero and Marco Platzner and Eduardo de la Torre},
  doi          = {10.1109/TC.2021.3107196},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2903-2914},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Exploiting hardware-based data-parallel and multithreading models for smart edge computing in reconfigurable FPGAs},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid memory buffer microarchitecture for high-radix
routers. <em>TC</em>, <em>71</em>(11), 2888–2902. (<a
href="https://doi.org/10.1109/TC.2021.3076431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical high-radix router microarchitecture consisting of small SRAM-based intermediate buffers has been used in large-scale supercomputers interconnection networks. While hierarchical organization enables efficient scaling to higher switch port count, it requires intermediate buffers which can cause performance bottleneck. Shallow intermediate buffers can cause head-of-line blocking to create backpressure towards input buffers and reduce overall performance. Increasing intermediate buffer size overcomes this problem but becomes infeasible due to the large overhead. In this work, we propose to organise decentralized intermediate buffers as a centralized buffer and leverage alternate memory technology to increase its capacity. In particular, we exploit the high-density nature of Spin-Torque Transfer Magnetic RAM (STT-MRAM) to increase intermediate buffer depth while also providing near-zero leakage power. STT-MRAM has disadvantages such as higher write latency and higher write energy. To overcome these disadvantages, we propose DeepHiR, a novel deep hybrid buffer organization (STT-MRAM and SRAM) combined with a centralized buffer organization to provide high performance with minimal cost. Although the deep intermediate buffer provided by DeepHiR can effectively improve router performance, a large amount of input buffer will still cause a lot of hardware overhead. At the same time, deeper intermediate buffers also makes it take longer for the backpressure to propagate to the source node, thereby reducing the performance of DeepHiR. Therefore, we further propose ElasHiR, which leverages elastic input buffer design in the centralized row buffer to allow a part of the centralized row buffer to act as input buffer. ElasHiR adopts reduced input buffers and automatically determines the length of input buffer in the centralized row buffer. This design minimizes the buffer resource while achieving excellent efficiency. Evaluation results show that DeepHiR can achieve 56.7 percent performance improvement in packet latency under synthetic traffic, and the cost of energy and area is moderate. ElasHiR can reduce the input buffer by 93.8 percent with performance comparable to DeepHiR.},
  archive      = {J_TC},
  author       = {Cunlu Li and Dezun Dong and Xiangke Liao and John Kim},
  doi          = {10.1109/TC.2021.3076431},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2888-2902},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hybrid memory buffer microarchitecture for high-radix routers},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient hardware malware detectors that are resilient to
adversarial evasion. <em>TC</em>, <em>71</em>(11), 2872–2887. (<a
href="https://doi.org/10.1109/TC.2021.3068873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware Malware Detectors (HMDs) have recently been proposed to make systems more malware-resistant. HMDs use hardware features to detect malware as a computational anomaly. Several aspects of the detector construction have been explored, leading to detectors with high accuracy. In this article, we explore whether malware developers can modify malware to avoid HMDs detection. We show that existing HMDs can be effectively reverse-engineered and subsequently evaded. Next, we explore whether retraining using evasive malware would help and show that retraining is limited. To address these limitations, we propose a new type of Resilient HMDs (RHMDs) that stochastically switch between different detectors. These detectors can be shown to be provably more difficult to reverse engineer based on recent results in probably approximately correct (PAC) learnability theory. We show that indeed such detectors are resilient to both reverse engineering and evasion, and that the resilience increases with the number and diversity of the individual detectors. Furthermore, we show that an optimal switching strategy between the RHMDs base detectors not only reduces misclassification on evasive malware but also maintains high classification accuracy on non-evasive malware. Our results demonstrate that these HMDs offer effective defense against evasive malware at low additional complexity.},
  archive      = {J_TC},
  author       = {Md Shohidul Islam and Khaled N. Khasawneh and Nael Abu-Ghazaleh and Dmitry Ponomarev and Lei Yu},
  doi          = {10.1109/TC.2021.3068873},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2872-2887},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient hardware malware detectors that are resilient to adversarial evasion},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Practical temporal prefetching with compressed on-chip
metadata. <em>TC</em>, <em>71</em>(11), 2858–2871. (<a
href="https://doi.org/10.1109/TC.2021.3065909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal prefetchers are powerful because they can prefetch irregular sequences of memory accesses, but temporal prefetchers are commercially infeasible because they store large amounts of metadata in DRAM. This article presents Triage, the first temporal data prefetcher that does not require off-chip metadata. Triage builds on two insights: (1) Metadata are not equally useful, so the less useful metadata need not be saved, and (2) for irregular workloads, it is more profitable to use portions of the LLC to store metadata than data. We also introduce novel schemes to identify useful metadata, to compress metadata, and to determine the fraction of the LLC to dedicate for metadata. Using an industrial-strength simulator running irregular workloads on a single-core system, we show that at a prefetch degree of 4, Triage improves performance by 41.1 percent compared to a baseline with no prefetching, whereas BO, a state-of-the-art prefetcher that uses only on-chip metadata, sees only 10.9 percent improvement. Compared with MISB, a temporal prefetcher that uses off-chip metadata, Triage provides a design alternative that reduces memory traffic by an order of magnitude (260.8 percent extra traffic for MISB at degree 1 versus 56.9 percent for Triage), while reducing coverage by 20 percent.},
  archive      = {J_TC},
  author       = {Hao Wu and Krishnendra Nathella and Matthew Pabst and Dam Sunwoo and Akanksha Jain and Calvin Lin},
  doi          = {10.1109/TC.2021.3065909},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2858-2871},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Practical temporal prefetching with compressed on-chip metadata},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Architectural supports for block ciphers in a RISC CPU core
by instruction overloading. <em>TC</em>, <em>71</em>(11), 2844–2857. (<a
href="https://doi.org/10.1109/TC.2021.3050515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel computer architectural concept of instruction overloading to support block ciphers. Instead of adding new instructions, we extend only the execution of some existing instructions. The proposed method allows a central processing unit core to execute different operations for the same instructions, depending on the address of the data, similar to operator overloading in object-oriented languages. We first present an extension for the AES algorithm, then we demonstrate its enhanced applicability with two further extensions supporting multiple block ciphers and hardware masking. The first extension for AES is also applicable to add/AND-rotate-XOR-based block ciphers such as SIMON. The AES and SIMON encryption speed, on this extended core, is at least doubled and is significantly less affected by memory latency. In addition, the AES encryption code requires only 18\% of the memory of the previous software implementation. The second extension can further support various block ciphers defined over GF(2 8 ), and the SM4 encryption speed is increased by at least 182\%. The third extension provides correlation power analysis (CPA) resistance with a 66.6\% area overhead but almost no speed overhead, whereas a typical software anti-CPA AES implementation requires at least hundreds of times the execution time.},
  archive      = {J_TC},
  author       = {P. Choi and W. Kong and J.-H. Kim and M.-K. Lee and Dong Kyue Kim},
  doi          = {10.1109/TC.2021.3050515},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2844-2857},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Architectural supports for block ciphers in a RISC CPU core by instruction overloading},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An extensive study of flexible design methods for the number
theoretic transform. <em>TC</em>, <em>71</em>(11), 2829–2843. (<a
href="https://doi.org/10.1109/TC.2020.3017930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient lattice-based cryptosystems operate with polynomial rings with the Number Theoretic Transform (NTT) to reduce the computational complexity of polynomial multiplication. NTT has therefore become a major arithmetic component (thus computational bottleneck) in various cryptographic constructions like hash functions, key-encapsulation mechanisms, digital signatures, and homomorphic encryption. Although there exist several hardware designs in prior work for NTT, they all are isolated design instances fixed for specific NTT parameters or parallelization level. This article provides an extensive study of flexible design methods for NTT implementation. To that end, we evaluate three cases: (1) parametric hardware design, (2) high-level synthesis (HLS) design approach, and (3) design for software implementation compiled on soft-core processors, where all are targeted on reconfigurable hardware devices. We evaluate the designs that implement multiple NTT parameters and/or processing elements, demonstrate the design details for each case, and provide a fair comparison with each other and prior work. On a Xilinx Virtex-7 FPGA, compared to HLS and processor-based methods, the results show that the parametric hardware design is on average $4.4\times$ and $73.9\times$ smaller and $22.5\times$ and $19.3\times$ faster, respectively. Surprisingly, HLS tools can yield less efficient solutions than processor-based approaches in some cases.},
  archive      = {J_TC},
  author       = {Ahmet Can Mert and Emre Karabulut and Erdinç Öztürk and Erkay Savaş and Aydin Aysu},
  doi          = {10.1109/TC.2020.3017930},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2829-2843},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An extensive study of flexible design methods for the number theoretic transform},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SOT-MRAM digital PIM architecture with extended parallelism
in matrix multiplication. <em>TC</em>, <em>71</em>(11), 2816–2828. (<a
href="https://doi.org/10.1109/TC.2022.3155277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging device-based digital processing-in-memory (PIM) architectures have been actively studied due to their energy and area efficiency derived from analog to digital converter (ADC)-less PIM hardware. However, digital PIM architectures generally need large extra memories to copy parameters, and they also suffer from low computation per memory-cycle efficiencies. In this paper, we present a novel spin-orbit torque magnetic random access memory (SOT-MRAM) based digital PIM architecture to alleviate the extra memory size burden and computation cycle issues. First, we propose the spintronics-assisted logic-in-memory (SLIM) cells to support efficient digital logic operations inside memories, where the voltage-controlled magnetic anisotropy (VCMA) is exploited to enhance the computation per memory-cycle efficiencies. In addition, crossed input source PIM (CRISP) architecture is proposed to extend the merits of SLIM cells by eliminating the extra memories for parameter copying while significantly improving the degree of parallel processing. An intra-memory pipelining scheme is also considered to further increase the throughput of CRISP. The proposed CRISP architecture has been implemented using 28 nm CMOS process, and it presents 1.10 TOPS/W and 0.95 TOPS/mm 2 , showing considerable improvements of energy efficiency and throughput per area, compared to the state-of-the-art digital PIM architecture. Finally, to evaluate the impact of computation errors induced from the SOT devices and circuits in CRISP architecture, classification accuracy simulations have been performed while applying computation errors.},
  archive      = {J_TC},
  author       = {Taehwan Kim and Yunho Jang and Min-Gu Kang and Byong-Guk Park and Kyung-Jin Lee and Jongsun Park},
  doi          = {10.1109/TC.2022.3155277},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2816-2828},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SOT-MRAM digital PIM architecture with extended parallelism in matrix multiplication},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The butterfly effect in primary visual cortex. <em>TC</em>,
<em>71</em>(11), 2803–2815. (<a
href="https://doi.org/10.1109/TC.2022.3173080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring and establishing artificial neural networks with electrophysiological characteristics and high computational efficiency is a popular topic that has been explored for many years in the fields of pattern recognition and computer vision. Inspired by the working mechanism of the primary visual cortex, pulse-coupled neural networks (PCNNs) can exhibit the characteristics of synchronous oscillation, refractory period, and exponential decay. These characteristics empower the PCNN model to group pixels with similar spatiality and gray values and to process digital images without training. However, electrophysiological evidence shows that the neurons exhibit highly complex nonlinear dynamics when stimulated by external periodic signals. This chaos phenomenon, also known as the ‘butterfly effect,” cannot be explained by all PCNN models. In this work, we analyze the main obstacle preventing PCNN models from imitating a real primary visual cortex. We consider neuronal excitation as a stochastic process. We then propose a novel neural network of the primary visual cortex, called a continuous-coupled neural network (CCNN). Theoretical analysis indicates that the dynamic behavior of the CCNN is distinct from the PCNN. Numerical results show that the CCNN model exhibits periodic behavior under a DC stimulus, and exhibits chaotic behavior under an AC stimulus, which is consistent with the testing results of primary visual cortex neurons. Furthermore, the image and video processing mechanisms of the CCNN model are analyzed. For image processing tasks, this model encodes the pixel intensity as the frequency of output signals so that it can group pixels with similar gray values. This image processing method can reduce the local gray level difference of the image, and compensate for small local discontinuities in the image. For video processing tasks, the CCNN encodes changing pixels as non-periodic chaotic signals, and it encodes static pixels as periodic signals. It thus achieves the purpose of moving target object recognition by distinguishing the dynamic states corresponding to different neuron clusters in the video. Experimental results on image segmentation indicate that the CCNN model has better performance than the state-of-the-art of visual cortex neural network models.},
  archive      = {J_TC},
  author       = {Jizhao Liu and Jing Lian and Julien Clinton Sprott and Qidong Liu and Yide Ma},
  doi          = {10.1109/TC.2022.3173080},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2803-2815},
  shortjournal = {IEEE Trans. Comput.},
  title        = {The butterfly effect in primary visual cortex},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time-coded spiking fourier transform in neuromorphic
hardware. <em>TC</em>, <em>71</em>(11), 2792–2802. (<a
href="https://doi.org/10.1109/TC.2022.3162708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {After several decades of continuously optimizing computing systems, the Moore&#39;s law is reaching its end. However, there is an increasing demand for fast and efficient processing systems that can handle large streams of data while decreasing system footprints. Neuromorphic computing answers this need by creating decentralized architectures that communicate with binary events over time. Despite its rapid growth in the last few years, novel algorithms are needed that can leverage the potential of this emerging computing paradigm and can stimulate the design of advanced neuromorphic chips. In this work, we propose a time-based spiking neural network that is mathematically equivalent to the Fourier transform. We implemented the network in the neuromorphic chip Loihi and conducted experiments on five different real scenarios with an automotive frequency modulated continuous wave radar. Experimental results validate the algorithm, and we hope they prompt the design of ad hoc neuromorphic chips that can improve the efficiency of state-of-the-art digital signal processors and encourage research on neuromorphic computing for signal processing.},
  archive      = {J_TC},
  author       = {Javier López-Randulfe and Nico Reeb and Negin Karimi and Chen Liu and Hector A. Gonzalez and Robin Dietrich and Bernhard Vogginger and Christian Mayr and Alois Knoll},
  doi          = {10.1109/TC.2022.3162708},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2792-2802},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Time-coded spiking fourier transform in neuromorphic hardware},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spiking generative adversarial networks with a neural
network discriminator: Local training, bayesian models, and continual
meta-learning. <em>TC</em>, <em>71</em>(11), 2778–2791. (<a
href="https://doi.org/10.1109/TC.2022.3191738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromorphic data carries information in spatio-temporal patterns encoded by spikes. Accordingly, a central problem in neuromorphic computing is training spiking neural networks (SNNs) to reproduce spatio-temporal spiking patterns in response to given spiking stimuli. Most existing approaches model the input-output behavior of an SNN in a deterministic fashion by assigning each input to a specific desired output spiking sequence. In contrast, in order to fully leverage the time-encoding capacity of spikes, this work proposes to train SNNs so as to match distributions of spiking signals rather than individual spiking signals. To this end, the paper introduces a novel hybrid architecture comprising a conditional generator, implemented via an SNN, and a discriminator, implemented by a conventional artificial neural network (ANN). The role of the ANN is to provide feedback during training to the SNN within an adversarial iterative learning strategy that follows the principle of generative adversarial network (GANs). In order to better capture multi-modal spatio-temporal distribution, the proposed approach – termed SpikeGAN – is further extended to support Bayesian learning of the generator&#39;s weight. Finally, settings with time-varying statistics are addressed by proposing an online meta-learning variant of SpikeGAN. Experiments bring insights into the merits of the proposed approach as compared to existing solutions based on (static) belief networks and maximum likelihood (or empirical risk minimization). In our experiments, handwritten digit images generated by SpikeGAN are observed to train an ANN classifier with $20\%$ higher accuracy than a comparable belief network. Our experiments also demonstrate the use of SpikeGAN to generate neuromorphic data sets from handwritten digits. It is shown that these data can be used to train an SNN classifier that achieves an accuracy level approaching the baseline accuracy of an SNN classifier trained on rate-encoded real data.},
  archive      = {J_TC},
  author       = {Bleema Rosenfeld and Osvaldo Simeone and Bipin Rajendran},
  doi          = {10.1109/TC.2022.3191738},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2778-2791},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Spiking generative adversarial networks with a neural network discriminator: Local training, bayesian models, and continual meta-learning},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive memory-enhanced time delay reservoir and its
memristive implementation. <em>TC</em>, <em>71</em>(11), 2766–2777. (<a
href="https://doi.org/10.1109/TC.2022.3173151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time Delay Reservoir (TDR) is a hardware-friendly machine learning approach from two perspectives. First, it can prevent the connection overhead of neural networks with increasing neurons. Second, through its dynamic system representation, TDR can also be implemented in hardware by different systems. However, it performs poorly on tasks that involve long-term dependency. In this work, we first introduce a higher-order delay unit, which is capable of accumulating and transferring the long history states in an adaptive manner to further enhance the reservoir memory. Particle Swarm Optimisation is applied to optimize the enhanced degree of memory adaptivity. Our experiments demonstrate its superiority both for short- and long-term memory datasets over seven existing approaches. In light of the hardware-friendly feature of TDR, we further propose a memristive implementation of our adaptive memory-enhanced TDR, where a dynamic memristor and the memristor-based delay element are applied to construct the reservoir. Through circuit simulation, the feasibility of our proposed memristive implementation is verified. The comparisons with different hardware reservoirs show that our proposed memristive implementation is effective both for short- and long-term memory datasets, while exhibiting benefits in terms of smaller circuit area and lower power consumption compared with traditional hardware reservoirs.},
  archive      = {J_TC},
  author       = {Xinming Shi and Leandro L. Minku and Xin Yao},
  doi          = {10.1109/TC.2022.3173151},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2766-2777},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Adaptive memory-enhanced time delay reservoir and its memristive implementation},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OpenHD: A GPU-powered framework for hyperdimensional
computing. <em>TC</em>, <em>71</em>(11), 2753–2765. (<a
href="https://doi.org/10.1109/TC.2022.3179226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperdimensional computing (HDC) has emerged as an alternative lightweight learning solution to deep neural networks. A key characteristic of HDC is the great extent of parallelism that can facilitate hardware acceleration. However, previous hardware implementations of HDC seldom focus on GPU designs, which were also inefficient partly due to the complexity of accelerating HDC on GPUs. In this paper, we present OpenHD, a flexible and high-performance GPU-powered framework for automating the mapping of general HDC applications including classification and clustering to GPUs. OpenHD takes advantage of memory optimization strategies specialized for HDC, minimizing the access time to different memory subsystems, and removing redundant operations. We also propose a novel training method to enable data parallelism in HDC training. Our evaluation result shows that the proposed training rapidly achieves the target accuracy, reducing the required training epochs by 4×. With OpenHD, users can deploy GPU-accelerated HDC applications without domain expert knowledge. Compared to the state-of-the-art GPU-powered HDC implementation, our evaluation on NVIDIA Jetson TX2 shows that OpenHD is up to 10.5× and 314× faster for HDC-based classification and clustering, respectively. Compared with non-HDC classification and clustering on GPUs, OpenHD-based HDC is 11.7× and 53× faster at comparable accuracy. OpenHD is available at: https://github.com/UCSD-SEELab/openhd .},
  archive      = {J_TC},
  author       = {Jaeyoung Kang and Behnam Khaleghi and Tajana Rosing and Yeseong Kim},
  doi          = {10.1109/TC.2022.3179226},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2753-2765},
  shortjournal = {IEEE Trans. Comput.},
  title        = {OpenHD: A GPU-powered framework for hyperdimensional computing},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring model stability of deep neural networks for
reliable RRAM-based in-memory acceleration. <em>TC</em>,
<em>71</em>(11), 2740–2752. (<a
href="https://doi.org/10.1109/TC.2022.3174585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RRAM-based in-memory computing (IMC) effectively accelerates deep neural networks (DNNs). Furthermore, model compression techniques, such as quantization and pruning, are necessary to improve algorithm mapping and hardware performance. However, in the presence of RRAM device variations, low-precision and sparse DNNs suffer from severe post-mapping accuracy loss. To address this, in this work, we investigate a new metric, model stability , from the loss landscape to help shed light on accuracy loss under variations and model compression, which guides an algorithmic solution to maximize model stability and mitigate accuracy loss. Based on statistical data from a CMOS/RRAM 1T1R test chip at 65nm, we characterize wafer-level RRAM variations and develop a cross-layer benchmark tool that incorporates quantization, pruning, device variations, model stability, and IMC architecture parameters to assess post-mapping accuracy and hardware performance. Leveraging this tool, we show that a loss-landscape-based DNN model selection for stability effectively tolerates device variations and achieves a post-mapping accuracy higher than that with 50\% lower RRAM variations. Moreover, we quantitatively interpret why model pruning increases the sensitivity to variations, while a lower-precision model has better tolerance to variations. Finally, we propose a novel variation-aware training method to improve model stability, in which there exists the most stable model for the best post-mapping accuracy of compressed DNNs. Experimental evaluation of the method shows up to 19\%, 21\%, and 11\% post-mapping accuracy improvement for our 65nm RRAM device, across various precision and sparsity, on CIFAR-10, CIFAR-100, and SVHN datasets, respectively.},
  archive      = {J_TC},
  author       = {Gokul Krishnan and Li Yang and Jingbo Sun and Jubin Hazra and Xiaocong Du and Maximilian Liehr and Zheng Li and Karsten Beckmann and Rajiv V. Joshi and Nathaniel C. Cady and Deliang Fan and Yu Cao},
  doi          = {10.1109/TC.2022.3174585},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2740-2752},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Exploring model stability of deep neural networks for reliable RRAM-based in-memory acceleration},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised spiking instance segmentation on event data
using STDP features. <em>TC</em>, <em>71</em>(11), 2728–2739. (<a
href="https://doi.org/10.1109/TC.2022.3191968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking Neural Networks (SNN) and the field of Neuromorphic Engineering has brought about a paradigm shift in how to approach Machine Learning (ML) and Computer Vision (CV) problem. This paradigm shift comes from the adaption of event-based sensing and processing. An event-based vision sensor allows for sparse and asynchronous events to be produced that are dynamically related to the scene. Allowing not only the spatial information but a high-fidelity of temporal information to be captured. Meanwhile avoiding the extra overhead and redundancy of conventional high frame rate approaches. However, with this change in paradigm, many techniques from traditional CV and ML are not applicable to these event-based spatial-temporal visual streams. As such a limited number of recognition, detection and segmentation approaches exist. In this paper, we present a novel approach that can perform instance segmentation using just the weights of a Spike Time Dependent Plasticity trained Spiking Convolutional Neural Network that was trained for object recognition. This exploits the spatial and temporal aspects of the SpikeSEG network&#39;s internal feature representations adding this new discriminative capability. We highlight the new capability by successfully transforming a single class unsupervised network for face detection into a multi-person face recognition and instance segmentation network.},
  archive      = {J_TC},
  author       = {Paul Kirkland and Davide Manna and Alex Vicente and Gaetano Di Caterina},
  doi          = {10.1109/TC.2022.3191968},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2728-2739},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Unsupervised spiking instance segmentation on event data using STDP features},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VisualNet: An end-to-end human visual system inspired
framework to reduce inference latency of deep neural networks.
<em>TC</em>, <em>71</em>(11), 2717–2727. (<a
href="https://doi.org/10.1109/TC.2022.3188211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acceleration of deep neural network (DNN) inference has gained increasing attention recently with the wide adoption of DNNs for practical applications. For computer vision tasks where inputs are images, existing works mostly focus on improving the throughput of inference for multiple images. However, in many real-time applications, it is critical to reduce the latency of a single image inference, which is more complicated than improving the throughput because of the inherent data dependencies. On the other hand, from human brain&#39;s perspective, the complexity in our visual surroundings is first encoded as a pattern of light on a two dimensional array of photoreceptors, with little direct resemblance to the original input or the ultimate percept. Within just a few hundred microns of retinal thickness, this initial signal encoded by our photoreceptors must be transformed into an adequate representation of the entire visual scene. Inspired by how the retina helps human brain incept new information efficiently, we present an end-to-end structured framework built using any existing convolutional neural network (CNN) as the backbone. The proposed framework, called VisualNet, can create task parallelism for the backbone during the inference of a single image. Experiments using a number of neural networks for the ImageNet classification task and the CIFAR-10 classification task on GPUs and CPUs show that the proposed VisualNet reduces the latency of the regular network it builds on by up to 80.6\% when both are fully parallelized with state-of-the-art acceleration libraries. At the same time, VisualNet can achieve similar or slightly higher accuracy.},
  archive      = {J_TC},
  author       = {Tianchen Wang and Jiawei Zhang and Jinjun Xiong and Song Bian and Zheyu Yan and Meiping Huang and Jian Zhuang and Takashi Sato and Xiaowei Xu and Yiyu Shi},
  doi          = {10.1109/TC.2022.3188211},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2717-2727},
  shortjournal = {IEEE Trans. Comput.},
  title        = {VisualNet: An end-to-end human visual system inspired framework to reduce inference latency of deep neural networks},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SpikeBASE: Spiking neural learning algorithm with backward
adaptation of synaptic efflux. <em>TC</em>, <em>71</em>(11), 2707–2716.
(<a href="https://doi.org/10.1109/TC.2022.3197089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-inspired Spiking Neural Network (SNN) is opening new possibilities towards human-level intelligence, by leveraging its nature of spatiotemporal information encoding and processing that bring both learning effectiveness and energy efficiency. Although substantial advances in SNN studies have been made, highly effective SNN learning algorithms are still urged, driven by the challenges of coordinating spiking spatiotemporal dynamics. We therefore propose a novel algorithm, SpikeBASE, denoting Spiking learning with Backward Adaption of Synaptic Efflux, to globally, supervisedly, and comprehensively coordinate the synaptic dynamics including both synaptic strength and responses. SpikeBASE can learn synaptic strength by backpropagating the error through the predefined synaptic responses. More importantly, SpikeBASE enables synaptic response adaptation through backpropagation, to mimic the complex dynamics of neural transmissions. Further, SpikeBASE enables multi-scale temporal memory formation by supporting multi-synaptic response adaptation. We have evaluated the algorithm on a challenging scarce data learning task and shown highly promising performance. The proposed SpikeBASE algorithm, through comprehensively coordinating the learning of synaptic strength, synaptic responses, and multi-scale temporal memory formation, has demonstrated its effectiveness on end-to-end SNN training. This study is expected to greatly advance the learning effectiveness of SNN and thus broadly benefit smart and efficient big data applications.},
  archive      = {J_TC},
  author       = {Jake Stauffer and Qingxue Zhang},
  doi          = {10.1109/TC.2022.3197089},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2707-2716},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SpikeBASE: Spiking neural learning algorithm with backward adaptation of synaptic efflux},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guest editorial: IEEE TC special issue on software, hardware
and applications for neuromorphic computing. <em>TC</em>,
<em>71</em>(11), 2705–2706. (<a
href="https://doi.org/10.1109/TC.2022.3208389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special section focus on software, hardware, and computer applications for neuromorphic computing. Inspired by biological neural systems, neuromorphic computing has drawn much attention for its great potential of achieving machine intelligence at extremely low energy dissipation. Bio-inspired computing models have been investigated for information encoding, sparse representation, event driven communication/computation, and online learning. This new computing paradigm triggered a recent wave of innovations in software and hardware architecture and emerging device technology, which consequently enabled many novel applications. This is an exemplar of research area where the application, computing model, architecture and circuit level design are tightly coupled to deliver unprecedented functionality and energy efficiency.},
  archive      = {J_TC},
  author       = {Yiran Chen and Qinru Qiu},
  doi          = {10.1109/TC.2022.3208389},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2705-2706},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Guest editorial: IEEE TC special issue on software, hardware and applications for neuromorphic computing},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tapping into NFV environment for opportunistic serverless
edge function deployment. <em>TC</em>, <em>71</em>(10), 2698–2704. (<a
href="https://doi.org/10.1109/TC.2021.3132776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Even with Network Function Virtualization (NFV), many commodity network servers have spare cycles. Despite that they are small and irregularly occur, spare cycles are fit for deploying short-lived serverless computing functions at the network edge. In this work, we perform detailed analyses of the benefits and limitations of co-locating serverless functions on NFV-ready servers. We propose NEMO , a novel platform that enables efficient serverless edge function deployment in the NFV environment. NEMO can intelligently harvest spare cycles of network functions to warm up the serverless functions and speed up the function invocation in an agile manner. Besides, NEMO can judiciously manage the thread conflict in a resource-limited environment. We build a prototype of NEMO. Our thorough evaluations show that NEMO can harvest up to 41\% spare cycles and achieve about 12.5 $\sim$ 25X performance improvement compared with straightforward co-location.},
  archive      = {J_TC},
  author       = {Lu Zhang and Weiqi Feng and Chao Li and Xiaofeng Hou and Pengyu Wang and Jing Wang and Minyi Guo},
  doi          = {10.1109/TC.2021.3132776},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2698-2704},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Tapping into NFV environment for opportunistic serverless edge function deployment},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Thermal-aware design for approximate DNN accelerators.
<em>TC</em>, <em>71</em>(10), 2687–2697. (<a
href="https://doi.org/10.1109/TC.2022.3141054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent breakthroughs in Neural Networks (NNs) have made DNN accelerators ubiquitous and led to an ever-increasing quest on adopting them from Cloud to edge computing. However, state-of-the-art DNN accelerators pack immense computational power in a relatively confined area, inducing significant on-chip power densities that lead to intolerable thermal bottlenecks. Existing state of the art focuses on using approximate multipliers only to trade-off efficiency with inference accuracy. In this work, we present a thermal-aware approximate DNN accelerator design in which we additionally trade-off approximation with temperature effects towards designing DNN accelerators that satisfy tight temperature constraints. Using commercial multi-physics tool flows for heat simulations, we demonstrate how our thermal-aware approximate design reduces the temperature from 139 $^{\circ }$ C, in an accurate circuit, down to 79 $^{\circ }$ C. This enables DNN accelerators to fulfill tight thermal constraints, while still maximizing the performance and reducing the energy by around 75\% with a negligible accuracy loss of merely 0.44\% on average for a wide range of NN models. Furthermore, using physics-based transistor aging models, we demonstrate how reductions in voltage and temperature obtained by our approximate design considerably improve the circuit’s reliability. Our approximate design exhibits around 40\% less aging-induced degradation compared to the baseline design.},
  archive      = {J_TC},
  author       = {Georgios Zervakis and Iraklis Anagnostopoulos and Sami Salamin and Ourania Spantidi and Isai Roman-Ballesteros and Jörg Henkel and Hussam Amrouch},
  doi          = {10.1109/TC.2022.3141054},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2687-2697},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Thermal-aware design for approximate DNN accelerators},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptively reduced DRAM caching for energy-efficient high
bandwidth memory. <em>TC</em>, <em>71</em>(10), 2675–2686. (<a
href="https://doi.org/10.1109/TC.2022.3140897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-package DRAM cache provides a higher bandwidth than conventional memory systems. Adapting the cache management to the run-time characteristics of each application seems a promising approach improving bandwidth efficiency and performance. Regrettably, fine-grained cache block monitoring and adaptation often becomes impractical due to its significant bandwidth, performance and hardware overheads. This paper proposes a novel mechanism for monitoring cache blocks using two parameters that are adjustable at run time. We propose two low-cost counter-based mechanisms to realize the block monitors in DRAM. Moreover, we propose a novel scheduling mechanism that opportunistically transfers the counter information to the DRAM stack when the data movement overhead reaches its minimum. Our simulation results on a set of data intensive parallel applications indicate that the proposed mechanisms achieve averages of 31\%, 24\% performance improvements over the state-of-the-art DRAM cache architectures. System energy savings over the same baselines are 29\%, 18\% on average.},
  archive      = {J_TC},
  author       = {Payman Behnam and Mahdi Nazm Bojnordi},
  doi          = {10.1109/TC.2022.3140897},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2675-2686},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Adaptively reduced DRAM caching for energy-efficient high bandwidth memory},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial-keyword skyline publish/subscribe query processing
over distributed sliding window streaming data. <em>TC</em>,
<em>71</em>(10), 2659–2674. (<a
href="https://doi.org/10.1109/TC.2022.3140884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current spatial-keyword publish/subscribe systems need to handle spatial-keyword skyline queries over geo-textual streams to continuously obtain good results. The skyline queries in such systems face two main problems: (1) query problems, because the powerful query capability is required for the strict limit of the response time and the large number of items concerned by the users, and (2) scalability issue, because millions of active users are maintained simultaneously with many network-connected machines. Unfortunately, the current approach is towards static data. Thus, this paper first proposes a distributed skyline query processing framework. Then, we optimize the skyline computing by introducing MF-R $^t$ -tree, which is an update-efficient and space-saving indexing structure and a fast approach for processing a continuous spatial-keyword skyline query called $eager^*$ . Finally, a spatial and textual signature-based communication optimization method is proposed to support scalability. The experimental results indicate that (1) MF-R $^t$ -tree can significantly reduce update costs, while maintaining a low storage cost, and a query performance comparable to IL-Quadtree, (2) $eager^*$ can averagely accelerate 79.72 × faster than the method based on BNL, (3) the communication optimization method significantly reduces the communication cost, and (4) the distributed framework can efficiently support large-scale skyline queries.},
  archive      = {J_TC},
  author       = {Ze Deng and Yue Wang and Tao Liu and Schahram Dustdar and Rajiv Ranjan and Albert Zomaya and Yizhi Liu and Lizhe Wang},
  doi          = {10.1109/TC.2022.3140884},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2659-2674},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Spatial-keyword skyline Publish/Subscribe query processing over distributed sliding window streaming data},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DeepP: Deep learning multi-program prefetch configuration
for the IBM POWER 8. <em>TC</em>, <em>71</em>(10), 2646–2658. (<a
href="https://doi.org/10.1109/TC.2021.3139997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current multi-core processors implement sophisticated hardware prefetchers, that can be configured by application (PID), to improve the system performance. When running multiple applications, each application can present different prefetch requirements, hence different configurations can be used. Setting the optimal prefetch configuration for each application is a complex task since it does not only depend on the application characteristics but also on the interference at the shared memory resources (e.g., memory bandwidth). In his paper, we propose DeepP , a deep learning approach for the IBM POWER8 that identifies at run-time the best prefetch configuration for each application in a workload. To this end, the neural network predicts the performance of each application under the studied prefetch configurations by using a set of performance events. The prediction accuracy of the network is improved thanks to a dynamic training methodology that allows learning the impact of dynamic changes of the prefetch configuration on performance. At run-time, the devised network infers the best prefetch configuration for each application and adjusts it dynamically. Experimental results show that the proposed approach improves performance, on average, by 5.8\%, 6.7\%, and 15.8\% compared to the default prefetch configuration across different 6-, 8-, and 10-application workloads, respectively.},
  archive      = {J_TC},
  author       = {Manel Lurbe and Josué Feliu and Salvador Petit and Maria E. Gómez and Julio Sahuquillo},
  doi          = {10.1109/TC.2021.3139997},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2646-2658},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DeepP: Deep learning multi-program prefetch configuration for the IBM POWER 8},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effective runtime management of tasks and priorities in GNU
OpenMP applications. <em>TC</em>, <em>71</em>(10), 2632–2645. (<a
href="https://doi.org/10.1109/TC.2021.3139463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {OpenMP has become a reference standard for the design of parallel applications. This standard is evolving quickly, thus offering new opportunities to the application programmers. However, OpenMP runtime environments are often not fully aligned with the actual requirements imposed by the evolution of such a standard. Among the main lacks, we find: (a) a limited capability to effectively cope with task priorities, and (b) the inadequacy in guaranteeing core properties while processing tasks such as the so-called work-conservativeness —the ability of the OpenMP runtime environment to fully exploit the underlying multi-processor/multi-core machine through the avoidance of thread-blocking phases. In this article, we present the design of extensions to the GNU OpenMP ( GOMP ) implementation, integrated into gcc , which allow the effective management of tasks and their priorities. Our proposal is based on a user-space library—modularly combined with the one already offered by GOMP —and an external kernel-level Linux module—offering the opportunity to exploit raising hardware facilities for task/priority management. We also provide experimental results showing the effectiveness of our proposal, achieved by running either OpenMP common benchmarks or a new benchmark application ( Hashtag-Text ) that we explicitly devised to stress the runtime environment in relation to the above-mentioned task/priority management aspects.},
  archive      = {J_TC},
  author       = {Emiliano Silvestri and Alessandro Pellegrini and Pierangelo Di Sanzo and Francesco Quaglia},
  doi          = {10.1109/TC.2021.3139463},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2632-2645},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Effective runtime management of tasks and priorities in GNU OpenMP applications},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tremors: Privacy-breaching inference of computing tasks
using vibration-based condition monitors. <em>TC</em>, <em>71</em>(10),
2620–2631. (<a href="https://doi.org/10.1109/TC.2021.3139181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the adaptation of vibration-based condition monitoring systems and techniques, popularly used in industrial condition-based maintenance, for identifying the possibility of compromising the privacy of personal computing systems. This work exploits the automated fan-based heat dissipation features and read/write operations of disk-based storage, commonly present in personal computers, to read computing task-specific vibration signatures on the computer’s cabinet/case. These vibration signatures are then used to identify the broad classes of tasks being executed on a separate computer without ever needing to log into the monitored machine. This work builds upon the premise that heterogeneous tasks have distinct computing requirements, which translates to variations in the amount of heat generated by the computer’s processor, eventually leading to variations in the computer’s heat control fan speed. The variations in the fan’s speed and the frequency of read/write operations to disk-based storage create unique vibration signatures, which maps uniquely to the computer’s processing operations, leading to a breach of privacy of the computer. Our work’s preliminary results suggest that computer-based tasks can be mapped from their vibration signatures with an accuracy of at least $70\%$ . We additionally study the task identification granularity of such an approach.},
  archive      = {J_TC},
  author       = {Anandarup Mukherjee and Pallav Kumar Deb and Sudip Misra},
  doi          = {10.1109/TC.2021.3139181},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2620-2631},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Tremors: Privacy-breaching inference of computing tasks using vibration-based condition monitors},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid annealing method based on subQUBO model extraction
with multiple solution instances. <em>TC</em>, <em>71</em>(10),
2606–2619. (<a href="https://doi.org/10.1109/TC.2021.3138629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ising machines are expected to solve combinatorial optimization problems efficiently by representing them as Ising models or equivalent quadratic unconstrained binary optimization (QUBO) models . However, upper bound exists on the computable problem size due to the hardware limitations of Ising machines. This paper propose a new hybrid annealing method based on partial QUBO extraction, called subQUBO model extraction, with multiple solution instances. For a given QUBO model, the proposed method obtains $N_I$ quasi-optimal solutions (quasi-ground-state solutions) in some way using a classical computer. The solutions giving these quasi-optimal solutions are called solution instances . We extract a size-limited subQUBO model as follows based on a strong theoretical background: we randomly select $N_S$ $(N_S&amp;lt;N_I)$ solution instances among them and focus on a particular binary variable $x_i$ in the $N_S$ solution instances. If $x_i$ value is much varied over $N_S$ solution instances, it is included in the subQUBO model; otherwise, it is not. We find a (quasi-)ground-state solution of the extracted subQUBO model using an Ising machine and add it as a new solution instance. By repeating this process, we can finally obtain a (quasi-)ground-state solution of the original QUBO model. Experimental evaluations confirm that the proposed method can obtain better quasi-ground-state solution than existing methods for large-sized QUBO models.},
  archive      = {J_TC},
  author       = {Yuta Atobe and Masashi Tawada and Nozomu Togawa},
  doi          = {10.1109/TC.2021.3138629},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2606-2619},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hybrid annealing method based on subQUBO model extraction with multiple solution instances},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regularity-based virtualization under the ARINC 653 standard
for embedded systems. <em>TC</em>, <em>71</em>(10), 2592–2605. (<a
href="https://doi.org/10.1109/TC.2021.3138019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In embedded real-time virtualized systems (ERTVS), the ARINC 653 standard specifies a cyclic scheduling policy to guarantee the real-time performance of tasks in multiple Virtual Machines (VMs) residing on shared hardware. Based on this policy, the Regularity-based Resource Partitioning (RRP) model defines an efficient interface specification to hierarchically partition and assign resource slices among VMs. Although this model has received plenty of attention recently, three major pieces remain missing for applying this model in ERTVS. (1) Embedded systems are more sensitive to resource utilization efficiency since this may drastically affect their deployment cost for including additional cores. Therefore, this paper proposes an optimal and an approximate RRP resource scheduler for multi-core platforms. (2) A resource reconfiguration is required when an embedded system has to switch between operating modes, resulting in the current cyclic schedule being replaced by another pre-configured and verified cyclic schedule. This paper formalizes a new One-Hop Reconfiguration (OHR) problem tailored for mode-switch-capable embedded systems and introduces a corresponding optimal solution. (3) No RRP-based toolset is currently available for embedded systems. This paper thus presents an optimized RRP toolset tailored for embedded systems. Numerous experiments are conducted to evaluate the efficacy of this toolset.},
  archive      = {J_TC},
  author       = {Guangli Dai and Pavan Kumar Paluri and Albert Mo Kim Cheng and Bozheng Liu},
  doi          = {10.1109/TC.2021.3138019},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2592-2605},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Regularity-based virtualization under the ARINC 653 standard for embedded systems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DML: Dynamic partial reconfiguration with scalable task
scheduling for multi-applications on FPGAs. <em>TC</em>,
<em>71</em>(10), 2577–2591. (<a
href="https://doi.org/10.1109/TC.2021.3137785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For several new applications, FPGA-based computation has shown better latency and energy efficiency compared to CPU or GPU-based solutions. We note two clear trends in FPGA-based computing. On the edge, the complexity of applications is increasing, requiring more resources than possible on today&#39;s edge FPGAs. In contrast, in the data center, FPGA sizes have increased to the point where multiple applications must be mapped to fully utilize the programmable fabric. While these limitations affect two separate domains, they both can be dealt with by using dynamic partial reconfiguration (DPR). Thus, there is a renewed interest to deploy DPR for FPGA-based hardware. In this work, we present Doing More with Less (DML) – a methodology for scheduling heterogeneous tasks across an FPGA&#39;s resources in a resource efficient manner while effectively hiding the latency of DPR. With the help of an integer linear programming (ILP) based scheduler, we demonstrate the mapping of diverse computational workloads in both cloud and edge-like scenarios. Our novel contributions include: enabling IP-level pipelining and parallelization to exploit the parallelism available within batches of work in our scheduler, and strategies to map and run multiple applications simultaneously. We consider the application of our methodology on real world benchmarks on both small (a Zedboard) and large (a ZCU106) FPGAs, across different workload batching and multiple-application scenarios. Our evaluation proves the real world efficacy of our solution, and we demonstrate an average speedup of 5X and up to 7.65X on a ZCU106 over a bulk-batching baseline via our scheduling strategies. We also demonstrate the scalablity of our scheduler by simultaneously mapping multiple applications to a single FPGA, and explore different approaches to sharing FPGA resources between applications.},
  archive      = {J_TC},
  author       = {Ashutosh Dhar and Edward Richter and Mang Yu and Wei Zuo and Xiaohao Wang and Nam Sung Kim and Deming Chen},
  doi          = {10.1109/TC.2021.3137785},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2577-2591},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DML: Dynamic partial reconfiguration with scalable task scheduling for multi-applications on FPGAs},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FeFET multi-bit content-addressable memories for in-memory
nearest neighbor search. <em>TC</em>, <em>71</em>(10), 2565–2576. (<a
href="https://doi.org/10.1109/TC.2021.3136576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nearest neighbor (NN) search computations are at the core of many applications such as few-shot learning, classification, and hyperdimensional computing. As such, efficient hardware support for NN search is highly desired. In-memory computing using emerging devices offers attractive solutions for NN search. Solutions based on ternary content-addressable memories (TCAMs) offer high energy and latency improvements for NN search at the expense of accuracy. In this work, we propose a novel distance function that can be natively evaluated with multi-bit content-addressable memories (MCAMs) based on ferroelectric FETs (FeFETs) to perform a single-step, in-memory NN search. We evaluate the efficacy of FeFET MCAMs in the context of few-shot learning applications with different datasets. As an example, we achieve a 78.54\% accuracy for a 5-way, 5-shot classification task for the mini-ImageNet dataset (only 1.5\% lower than software-based implementations) when using a 3-bit MCAM for NN search. We consider the effects of FeFET threshold voltage variations on the application accuracy and analyze the area and search energy requirements of FeFET MCAMs for accurate operations. Our results indicate that MCAMs require 2× lower area and search energy than TCAMs to achieve the same accuracy. Furthermore, we experimentally demonstrate a 2-bit implementation of FeFET MCAM using AND arrays from GLOBALFOUNDRIES to further validate the design concept.},
  archive      = {J_TC},
  author       = {Arman Kazemi and Mohammad Mehdi Sharifi and Ann Franchesca Laguna and Franz Müller and Xunzhao Yin and Thomas Kämpfe and Michael Niemier and X. Sharon Hu},
  doi          = {10.1109/TC.2021.3136576},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2565-2576},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FeFET multi-bit content-addressable memories for in-memory nearest neighbor search},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Alternative tower field construction for quantum
implementation of the AES s-box. <em>TC</em>, <em>71</em>(10),
2553–2564. (<a href="https://doi.org/10.1109/TC.2021.3135759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grover’s search algorithm allows a quantum adversary to find a $k$ -bit secret key of a block cipher by making O( $2^{k/2}$ ) block cipher queries. Resistance of a block cipher to such an attack is evaluated by quantum resources required to implement Grover’s oracle for the target cipher. The quantum resources are typically estimated by the $\textit {T}$ -depth of its circuit implementation and the number of qubits used by the circuit (width). Since the AES S-box is the only component which requires $\textit {T}$ -gates in a quantum implementation of AES, recent research has put its focus on efficient implementation of the AES S-box. However, any efficient implementation with low $\textit {T}$ -depth will not be practical in the real world without considering qubit consumption of the implementation. In this work, we propose three methods of trade-off between time and space for the quantum implementation of the AES S-box. In particular, one of our methods turns out to use the smallest number of qubits among the existing methods, significantly reducing its $\textit {T}$ -depth.},
  archive      = {J_TC},
  author       = {Doyoung Chung and Seungkwang Lee and Dooho Choi and Jooyoung Lee},
  doi          = {10.1109/TC.2021.3135759},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2553-2564},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Alternative tower field construction for quantum implementation of the AES S-box},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluation and optimization of distributed machine learning
techniques for internet of things. <em>TC</em>, <em>71</em>(10),
2538–2552. (<a href="https://doi.org/10.1109/TC.2021.3135752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) and split learning (SL) are state-of-the-art distributed machine learning techniques to enable machine learning training without accessing raw data on clients or end devices. However, their comparative training performance under real-world resource-restricted Internet of Things (IoT) device settings remains barely studied. This work provides empirical comparisons of FL and SL in real-world IoT settings regarding (i) learning performance with heterogeneous data distributions and (ii) on-device execution overhead. Our analyses in this work demonstrate that the learning performance of SL is better than FL under an imbalanced data distribution but worse than FL under an extreme non-IID data distribution. Recently, FL and SL are combined to form splitfed learning (SFL) to leverage each of their benefits (e.g., parallel training of FL and lightweight on-device computation requirement of SL). Our work considers FL, SL, and SFL, and mounts them on Raspberry Pi devices to evaluate their performance, including training time, communication overhead, power consumption, and memory usage with resource-restricted IoT devices. Besides evaluations, we apply two optimizations. First, we generalize SFL by carefully examining the possibility of a hybrid type of model training at the server-side. The generalized SFL merges sequential (dependent) and parallel (independent) processes of model training and thus is beneficial to a system with a large scale of IoT devices, specifically at the server-side operations. Second, we propose pragmatic techniques to substantially reduce the communication overhead by up to four times for the SL and (generalized) SFL.},
  archive      = {J_TC},
  author       = {Yansong Gao and Minki Kim and Chandra Thapa and Alsharif Abuadbba and Zhi Zhang and Seyit Camtepe and Hyoungshick Kim and Surya Nepal},
  doi          = {10.1109/TC.2021.3135752},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2538-2552},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Evaluation and optimization of distributed machine learning techniques for internet of things},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new approach for side channel analysis on stream ciphers
and related constructions. <em>TC</em>, <em>71</em>(10), 2527–2537. (<a
href="https://doi.org/10.1109/TC.2021.3135191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Side Channel Analysis (SCA) is among the newly emerged threats to small scale devices performing a cryptographic operation. While such analysis is well studied against the block ciphers, we observe that the stream cipher counterpart is not that much explored. We propose novel modelling that can work with a number of stream ciphers and related constructions. We show practical state/key recovery attacks on the lightweight ciphers, LIZARD, PLANTLET and GRAIN-128-AEAD. We consider the software platform (where the Hamming weight leakage is available) as well as the hardware platform (where the Hamming distance leakage is available). Through the modelling of Satisfiability Modulo Theory (SMT), we show that the solution can be obtained in a matter of seconds in most cases. In a handful of cases, however, the entire state/key recovery is not feasible in a practical amount of time. For those cases, we show full recovery is possible when a small number of bits are guessed. We also study the effect of increasing/decreasing the number of keystream bits on the solution time. Following a number of literature, we initially assume the traces that are obtained are noiseless. Later, we show how an extension of our model can deal with the noisy traces (which is a more general assumption).},
  archive      = {J_TC},
  author       = {Anubhab Baksi and Satyam Kumar and Santanu Sarkar},
  doi          = {10.1109/TC.2021.3135191},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2527-2537},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A new approach for side channel analysis on stream ciphers and related constructions},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stop and look: A novel checkpointing and debugging flow for
FPGAs. <em>TC</em>, <em>71</em>(10), 2513–2526. (<a
href="https://doi.org/10.1109/TC.2021.3133828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware checkpointing enables live migration, fault recovery, and context switching, but has been difficult to achieve for FPGA applications. We detail techniques to checkpoint complex FPGA designs and develop StateMover, a new checkpoint-based debugging flow for FPGAs that combines the speed of hardware execution with the full observability and controllability of simulation. StateMover can safely stop a running design and seamlessly move its state back and forth between an FPGA and a simulator. StateMover can create complete design checkpoints even for designs that have multi-cycle I/O interfaces, contain buried state that is not accessible by FPGA readback, and use external memories. StateMover and its associated IPs allow a designer to quickly make a design checkpointable, with a small area overhead. Moving the state from/to an FPGA to/from a simulator can be performed in a few seconds for large Xilinx UltraScale FPGAs.},
  archive      = {J_TC},
  author       = {Sameh Attia and Vaughn Betz},
  doi          = {10.1109/TC.2021.3133828},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2513-2526},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Stop and look: A novel checkpointing and debugging flow for FPGAs},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RvDfi: A RISC-v architecture with security enforcement by
high performance complete data-flow integrity. <em>TC</em>,
<em>71</em>(10), 2499–2512. (<a
href="https://doi.org/10.1109/TC.2021.3133701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid revolution of open-source hardware, RISC-V architecture has been prevalent in both academic research and industrial developments. Due to the increasing threats of information leakage, it is imperative to provide a secure RISC-V ecosystem to defend against malicious software exploits. Toward this goal, data-flow integrity (DFI) is employed as a strict security policy for enforcing the legitimacy of each data access, thereby filtering out most of the attack exploits. However, due to the intensive computations needed by DFI, there are only limited proposals successfully implementing partial DFI with low performance overhead. Moreover, all the previous studies failed to enforce the complete DFI policy in a real hardware platform, while trading off security strength for performance efficiency. To provide RISC-V architecture with high security enforcement and low performance overhead, we leverage the open-source Rocket Chip and propose RvDfi , the first complete DFI implementation based on RISC-V architecture with only 17.8\% performance overhead on average and 3.9\% in minimum, incurring much less performance loss compared to the 166.3\% overhead caused by previous complete DFI implementation.},
  archive      = {J_TC},
  author       = {Lang Feng and Jiayi Huang and Luyi Li and Haochen Zhang and Zhongfeng Wang},
  doi          = {10.1109/TC.2021.3133701},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2499-2512},
  shortjournal = {IEEE Trans. Comput.},
  title        = {RvDfi: A RISC-V architecture with security enforcement by high performance complete data-flow integrity},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dandelion: Boosting DNN usability under dataset scarcity.
<em>TC</em>, <em>71</em>(10), 2487–2498. (<a
href="https://doi.org/10.1109/TC.2021.3132170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of deep neural network (DNN) has provided transformative impacts on many fields, including computer vision and video recognition. However, the impact is limited by the need for large, labeled datasets to enable effective training. To address this fundamental problem, we propose a novel inter-network system (Dandelion), providing architecture support (Dandelion-architecture) for data augmentation that trains DNNs with rare images generated by the generative adversarial network (GAN) with orthogonal attributes modified (Dandelion-function. The approach can account for the latency requirement and resource limitation of target applications by exploiting data and computation reuses between the two networks; this amortizes the impact of bottleneck brought by GAN and facilitates design of inter-network accelerator. Moreover, we show how to implement two-network design on 3D architecture to further enhance the accelerator. Our results show that with the generated images, DNN yields 13.6\% - 37.5\% improvement on accuracy, depending on the data scarcity level. Our architecture achieves at least 30\% speedup compared with the baseline while 40\% of the overhead brought by the incorporation of GAN is reduced in our design compared with ScaleDeep, and 26.3\% of performance improvement over TETRIS.},
  archive      = {J_TC},
  author       = {Xiangru Chen and Jiaqi Zhang and Sandip Ray},
  doi          = {10.1109/TC.2021.3132170},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2487-2498},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Dandelion: Boosting DNN usability under dataset scarcity},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PAM: A piecewise-linearly-approximated floating-point
multiplier with unbiasedness and configurability. <em>TC</em>,
<em>71</em>(10), 2473–2486. (<a
href="https://doi.org/10.1109/TC.2021.3131850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate computing is a promising alternative to improve energy efficiency for IoT devices on the edge. This work proposes a piecewise-linearly-approximated and unbiased floating-point approximate multiplier with run-time configurability. We provide a theoretically sound formulation that turns multiplication approximation to an optimization problem. With the formulation and findings, a multi-level architecture is proposed to easily incorporate run-time configurability and module execution parallelism. Finally, the proposed multiplier is further optimized to reduce the circuit implementation complexity, making the multiplier linearly dependent on the precision requirement, instead of quadratically or exponentially as in prior work. When compared to the prior state-of-the-art approximate floating-point multiplier, ApproxLP M. Imani et al , “ApproxLP: Approximate multiplication with linearization and iterative error control,” in Proc. ACM/IEEE Des. Autom. Conf. , 2019, pp. 1–6., the proposed multiplier outperforms in all the aspects including accuracy, area, and delay. By replacing a full-precision floating-point multiplier in GPU, the proposed design can improve the energy efficiency for various edge computing tasks. Even with Level 1 approximation, the proposed multiplier improves energy efficiency up to 20× for machine learning on CIFAR-10, with almost negligible accuracy loss.},
  archive      = {J_TC},
  author       = {Chuangtao Chen and Weikang Qian and Mohsen Imani and Xunzhao Yin and Cheng Zhuo},
  doi          = {10.1109/TC.2021.3131850},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2473-2486},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PAM: A piecewise-linearly-approximated floating-point multiplier with unbiasedness and configurability},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Triangle counting accelerations: From algorithm to in-memory
computing architecture. <em>TC</em>, <em>71</em>(10), 2462–2472. (<a
href="https://doi.org/10.1109/TC.2021.3131049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triangles are the basic substructure of networks and triangle counting (TC) has been a fundamental graph computing problem in numerous fields such as social network analysis. Nevertheless, like other graph computing problems, due to the high memory-computation ratio and random memory access pattern, TC involves a large amount of data transfers thus suffers from the bandwidth bottleneck in the traditional Von-Neumann architecture. To overcome this challenge, in this paper, we propose to accelerate TC with the emerging processing-in-memory (PIM) architecture through an algorithm-architecture co-optimization manner. To enable the efficient in-memory implementations, we come up to reformulate TC with bitwise logic operations (such as AND ), and develop customized graph compression and mapping techniques for efficient data flow management. With the emerging computational Spin-Transfer Torque Magnetic RAM (STT-MRAM) array, which is one of the most promising PIM enabling techniques, the device-to-architecture co-simulation results demonstrate that the proposed TC in-memory accelerator outperforms the state-of-the-art GPU and FPGA accelerations by $12.2\times$ and $31.8\times$ , respectively, and achieves a $34\times$ energy efficiency improvement over the FPGA accelerator.},
  archive      = {J_TC},
  author       = {Xueyan Wang and Jianlei Yang and Yinglin Zhao and Xiaotao Jia and Rong Yin and Xuhang Chen and Gang Qu and Weisheng Zhao},
  doi          = {10.1109/TC.2021.3131049},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2462-2472},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Triangle counting accelerations: From algorithm to in-memory computing architecture},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dependent task offloading for edge computing based on deep
reinforcement learning. <em>TC</em>, <em>71</em>(10), 2449–2461. (<a
href="https://doi.org/10.1109/TC.2021.3131040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing is an emerging promising computing paradigm that brings computation and storage resources to the network edge, hence significantly reducing the service latency and network traffic. In edge computing, many applications are composed of dependent tasks where the outputs of some are the inputs of others. How to offload these tasks to the network edge is a vital and challenging problem which aims to determine the placement of each running task in order to maximize the Quality-of-Service (QoS). Most of the existing studies either design heuristic algorithms that lack strong adaptivity or learning-based methods but without considering the intrinsic task dependency. Different from the existing work, we propose an intelligent task offloading scheme leveraging off-policy reinforcement learning empowered by a Sequence-to-Sequence (S2S) neural network, where the dependent tasks are represented by a Directed Acyclic Graph (DAG). To improve the training efficiency, we combine a specific off-policy policy gradient algorithm with a clipped surrogate objective. We then conduct extensive simulation experiments using heterogeneous applications modelled by synthetic DAGs. The results demonstrate that: 1) our method converges fast and steadily in training; 2) it outperforms the existing methods and approximates the optimal solution in latency and energy consumption under various scenarios.},
  archive      = {J_TC},
  author       = {Jin Wang and Jia Hu and Geyong Min and Wenhan Zhan and Albert Y. Zomaya and Nektarios Georgalas},
  doi          = {10.1109/TC.2021.3131040},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2449-2461},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Dependent task offloading for edge computing based on deep reinforcement learning},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). General and fast inter-process communication via bypassing
privileged software. <em>TC</em>, <em>71</em>(10), 2435–2448. (<a
href="https://doi.org/10.1109/TC.2021.3130751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IPC (Inter-Process Communication) is a widely used operating system (OS) technique that allows one process to invoke the services of other processes. The IPC participants may share the same OS ( internal IPC ) or use a separate OS ( external IPC ). Even though a long line of researches has optimized the performance of IPC, it is still a major factor of the run-time overhead of IPC-intensive applications. Furthermore, there is no one-size-fits-all solution for both internal and external IPC. This paper presents SkyBridge, a general communication technique designed and optimized for both types of IPC. SkyBridge requires no involvement of the privileged software (the kernel or the hypervisor) and enables a process to directly switch to the virtual address space of the target process, regardless of whether they are running on the same OS or not. We have implemented SkyBridge on two microkernels (seL4 and Google Zircon) as well as an open-source serverless hypervisor (Firecracker). The evaluation results show that SkyBridge improves the latency of internal IPC and external IPC by up to 19.6x and 1265.7x, respectively.},
  archive      = {J_TC},
  author       = {Zeyu Mi and Haoqi Zhuang and Binyu Zang and Haibo Chen},
  doi          = {10.1109/TC.2021.3130751},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2435-2448},
  shortjournal = {IEEE Trans. Comput.},
  title        = {General and fast inter-process communication via bypassing privileged software},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). A reputation-based mechanism for transaction processing in
blockchain systems. <em>TC</em>, <em>71</em>(10), 2423–2434. (<a
href="https://doi.org/10.1109/TC.2021.3129934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain protocols require nodes to verify all received transactions before forwarding them. However, massive spam transactions cause the participants in blockchain systems to consume many resources in verifying and propagating transactions. This paper proposes a reputation-based mechanism to increase the efficiency of processing transactions by considering the reputations of the sending nodes. Reputations are in turn adjusted based on the quality of transaction processing. Our proposed reputation-based mechanism offers three main contributions. First, we modify the verification strategy so that nodes set a probability of verifying a received transaction considering the likelihood of it being spam: transactions from a node with a low reputation have a high probability of being verified. Second, we optimize the transaction forwarding protocol to reduce propagation delay by prioritizing forwarding transactions to reputable receivers. Third, we design a data request protocol that provides alternative data exchange methods for nodes with different reputations. A series of simulations demonstrate the performance of our reputation-based mechanism.},
  archive      = {J_TC},
  author       = {Jiarui Zhang and Yukun Cheng and Xiaotie Deng and Bo Wang and Jan Xie and Yuanyuan Yang and Mengqian Zhang},
  doi          = {10.1109/TC.2021.3129934},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2423-2434},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A reputation-based mechanism for transaction processing in blockchain systems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Edge-centric programming for IoT applications with automatic
code partitioning. <em>TC</em>, <em>71</em>(10), 2408–2422. (<a
href="https://doi.org/10.1109/TC.2021.3129367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IoT application development usually involves separate programming at the device side and server side. While separate programming style is sufficient for many simple applications, it is not suitable for many complex applications that involve complex interactions and intensive data processing. We propose EdgeProg , an edge-centric programming approach to simplify IoT application programming, motivated by the increasing popularity of edge computing. With EdgeProg, users could write application logic in a centralized manner with an augmented If-This-Then-That (IFTTT) syntax and virtual sensor mechanism. The program can be processed at the edge server, which can automatically generate the actual application code and intelligently partition the code into device code and server code, for achieving the optimal latency. EdgeProg employs dynamic linking and loading to deploy the device code on a variety of IoT devices, which do not run any application-specific codes at the start. Results show that EdgeProg achieves an average reduction of 20.96\%, 27.8\% and 79.41\% in terms of execution latency, energy consumption, and lines of code compared with state-of-the-art approaches.},
  archive      = {J_TC},
  author       = {Borui Li and Wei Dong},
  doi          = {10.1109/TC.2021.3129367},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2408-2422},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Edge-centric programming for IoT applications with automatic code partitioning},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weight-aware cache for application-level proportional i/o
sharing. <em>TC</em>, <em>71</em>(10), 2395–2407. (<a
href="https://doi.org/10.1109/TC.2021.3129366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtualization technology has enabled server consolidation where multiple servers are co-located on a single physical machine to improve resource utilization. In such systems, proportional I/O sharing is critical to meet the SLO (Service-Level Objectives) of the applications running in each virtual instance. However, previous studies focus on block-level I/O proportionality without considering the upper-layer I/O caches, which handle I/O requests on behalf of the underlying storage devices, thereby failing to achieve application-level proportional I/O sharing. To overcome this limitation, we propose a new cache management scheme, Weight-aware Cache (WaC), which reflects the I/O weights on cache allocation and reclamation. Specifically, WaC prioritizes higher-weighted applications in the lock acquisition process of cache allocation by re-ordering the lock waiting queue based on I/O weight. Additionally, WaC keeps the number of cache entries of each application proportional to its I/O weight, through weight-aware cache reclamation. To verify the efficacy of our scheme, we implement and evaluate WaC on both the page cache and bcache. The experimental results demonstrate that our scheme improves I/O proportionality with negligible overhead in various cases.},
  archive      = {J_TC},
  author       = {Jonggyu Park and Young Ik Eom},
  doi          = {10.1109/TC.2021.3129366},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2395-2407},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Weight-aware cache for application-level proportional I/O sharing},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FAS-DQN: Freshness-aware scheduling via reinforcement
learning for latency-sensitive applications. <em>TC</em>,
<em>71</em>(10), 2381–2394. (<a
href="https://doi.org/10.1109/TC.2021.3129342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for real-time data processing has become increasingly attractive in Cyber-Physical Systems(CPSs), especially for data-intensive embedded real-time applications. In order to timely perceive and respond to environmental changes, the basic design requirement in such systems is to provide data service with high freshness. As modern CPSs become more complex, there are a broad set of system mode switch behaviors, some unforeseen, in a dynamic computational environment. However, conventional control algorithms can hardly handle such new scenarios, since most of them assume that the operational behavior is fixed. In this paper, we study the problem of how to maximize the freshness of data in multi-modal systems. We first use a recently proposed new conception, namely Age of Information (AoI) to quantify the freshness of data by combining the AoI metric with real-time constraints. Then, we propose, to our knowledge, the first freshness-aware scheduling solution to settle the problem via deep reinforcement learning(RL). To be specific, we develop an RL framework that can continuously update its scheduling strategies and maximize the freshness of data in the long term. Extensive simulation experiments are conducted and the results demonstrate that the proposed FAS-DQN outperforms other traditional state-of-the-art methods in terms of data freshness.},
  archive      = {J_TC},
  author       = {Chunyang Zhou and Guohui Li and Jianjun Li and Quan Zhou and Bing Guo},
  doi          = {10.1109/TC.2021.3129342},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2381-2394},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FAS-DQN: Freshness-aware scheduling via reinforcement learning for latency-sensitive applications},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Clustering driven iterated hybrid search for vertex
bisection minimization. <em>TC</em>, <em>71</em>(10), 2370–2380. (<a
href="https://doi.org/10.1109/TC.2021.3128504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Vertex Bisection Minimization Problem (VBMP) is a relevant graph partitioning model with a variety of practical applications. This work introduces a clustering driven iterated hybrid search algorithm (CLUHS), which is the first approach that applies clustering to reinforce iterated local search for solving VBMP. The proposed CLUHS uses hierarchical clustering to build an initial solution, guide local search process and perform search diversification. Experimental studies on 137 benchmark instances show the high competitiveness of the proposed approach compared to the state-of-the-art methods. In particular, CLUHS finds new record-breaking solutions for 18 instances.},
  archive      = {J_TC},
  author       = {Yan Jin and Bowen Xiong and Kun He and Jin-Kao Hao and Chu-Min Li and Zhang-Hua Fu},
  doi          = {10.1109/TC.2021.3128504},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2370-2380},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Clustering driven iterated hybrid search for vertex bisection minimization},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Soft error effects on arm microprocessors: Early estimations
versus chip measurements. <em>TC</em>, <em>71</em>(10), 2358–2369. (<a
href="https://doi.org/10.1109/TC.2021.3128501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extensive research efforts are being carried out to evaluate and improve the reliability of computing devices either through beam experiments or simulation-based fault injection. Unfortunately, it is still largely unclear to which extend fault injection can provide an accurate error rate estimation at early stages and if beam experiments can be used to identify the weakest resources in a device. The importance and challenges associated with a timely, but yet realistic reliability evaluation grow with the increase of complexity in both the hardware domain, with the integration of different types of cores in an SoC (System-on-Chip), and the software domain, with the OS (operating system) required to take full advantage of the available resources. In this paper, we combine and analyze data gathered with extensive beam experiments (on the final physical CPU hardware) and microarchitectural fault injections (on early microarchitectural CPU models). We target a standalone Arm Cortex-A5 CPU and an Arm Cortex-A9 CPU integrated into an SoC and evaluate their reliability in bare-metal and Linux-based configurations. Combining experimental data that covers more than 18 million years of device time with the result of more than 176,000 injections we find that both the SoC integration and the presence of the OS increase the system DUEs (Detected Unrecoverable Errors) rate (for different reasons) but do not significantly impact the SDCs (Silent Data Corruptions) rate which is solely attributed to the CPU core. Our reliability analysis demonstrates that even considering SoC integration and OS inclusion, early, pre-silicon microarchitecture-level fault injection delivers accurate SDC rates estimations and lower bounds for the DUE rates.},
  archive      = {J_TC},
  author       = {Pablo R. Bodmann and George Papadimitriou and Rubens L. Rech Junior and Dimitris Gizopoulos and Paolo Rech},
  doi          = {10.1109/TC.2021.3128501},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2358-2369},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Soft error effects on arm microprocessors: Early estimations versus chip measurements},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Memory-aware denial-of-service attacks on shared cache in
multicore real-time systems. <em>TC</em>, <em>71</em>(9), 2351–2357. (<a
href="https://doi.org/10.1109/TC.2021.3108044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we identify that memory performance plays a crucial role in the feasibility and effectiveness for performing denial-of-service attacks on shared cache. Based on this insight, we introduce new cache DoS attacks, which can be mounted from the user-space and can cause extreme worst-case execution time (WCET) impacts to cross-core victims—even if the shared cache is partitioned—by taking advantage of the platform’s memory address mapping information and HugePage support. We deploy these enhanced attacks on two popular embedded out-of-order multicore platforms using both synthetic and real-world benchmarks. The proposed DoS attacks achieve up to 111X WCET increases on the tested platforms.},
  archive      = {J_TC},
  author       = {Michael Bechtel and Heechul Yun},
  doi          = {10.1109/TC.2021.3108044},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2351-2357},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Memory-aware denial-of-service attacks on shared cache in multicore real-time systems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhanced side-channel analysis on ECDSA employing fixed-base
comb method. <em>TC</em>, <em>71</em>(9), 2341–2350. (<a
href="https://doi.org/10.1109/TC.2022.3191736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Table-based scalar multiplication provides practical security for ECDSA signature generation. However, a novel key recovery attack against this form of ECDSA signature generation that exploits the collisions between entries was recently proposed at CHES 2021. This attack is possible even if table entries are unknown, such as with random permutated entry ordering. In this paper, we enhance the efficiency of the key recovery attack against secure ECDSA signature generation based on fixed-base comb scalar multiplication. We significantly reduce the required number of traces by compressing collision information using the mathematical relationship between table entry collisions. We verify this is a practical threat by performing an experiment on fixed-base comb method with window width $w=4$ . Using our method, up to 27 traces are needed, much fewer than 1,019 traces required in the CHES publication. We cluster real traces measured using 32-bit STM32F4 microcontroller. In the experiment, we provide a selection method of points of interest using variance traces and unsupervised clustering-based leakage detection. With the selection method, we succeed in clustering leakages into 16 classes with a 100\% success rate with 32-bit MCU. This represents the first experiment to cluster the more leakage classes with a 32-bit MCU than in literature.},
  archive      = {J_TC},
  author       = {Sunghyun Jin and Sung Min Cho and HeeSeok Kim and Seokhie Hong},
  doi          = {10.1109/TC.2022.3191736},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2341-2350},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enhanced side-channel analysis on ECDSA employing fixed-base comb method},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ELOFS: An extensible low-overhead flash file system for
resource-scarce embedded devices. <em>TC</em>, <em>71</em>(9),
2327–2340. (<a href="https://doi.org/10.1109/TC.2022.3152079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging applications like machine learning in embedded devices (e.g., satellites and vehicles) require huge storage space, which recently stimulates the widespread deployment of large-scale flash memory in IoT devices. However, existing embedded file systems fall short in managing large-capacity storage efficiently for two reasons. First, prior arts store data structures of file systems either in flash or in main memory, which severely magnifies the scarcity of computing and memory resources. Moreover, the fine-grained metadata management in the existing embedded file systems induces significant energy consumption for large-capacity storage. In this paper, we propose a novel embedded file system, ELOFS, to tackle the above issues and manage large-capacity NAND flash on resource-scarce devices. ELOFS is made efficient through three novel techniques. First, we redefine the space management granularity and streamline the metadata to speed up the mounting performance. In addition, we design hybrid file structures to adapt dissimilar access patterns of embedded devices. Furthermore, ELOFS provides opportunities for in-depth cooperation with application-specific systems. We implement ELOFS with Memory Technology Device (MTD) interfaces, and the experimental results show that ELOFS outperforms YAFFS and UBIFS in terms of write, read, and deletions with orders of magnitude reductions on memory footprint and mounting time.},
  archive      = {J_TC},
  author       = {Runyu Zhang and Duo Liu and Xianzhang Chen and Xiongxiong She and Chaoshu Yang and Yujuan Tan and Zhaoyan Shen and Zili Shao and Lei Qiao},
  doi          = {10.1109/TC.2022.3152079},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2327-2340},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ELOFS: An extensible low-overhead flash file system for resource-scarce embedded devices},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enabling one-size-fits-all compilation optimization for
inference across machine learning computers. <em>TC</em>,
<em>71</em>(9), 2313–2326. (<a
href="https://doi.org/10.1109/TC.2021.3128266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning Computers (MLCs) with tensor functional units (e.g., NVIDIA’s Tensor Core, Google’s TPU and Habana’s Tensor Processor Core) have emerged significantly over recent years. The broad diversity of MLCs makes it hard to deploy machine learning workloads with optimized performance. Though deep learning compilers (e.g., TVM) are effective to produce optimized code for different hardware back-ends, when deploying to a new MLC, it is tedious to implement platform-specific compilation optimizations by thoroughly understanding system/architectural details. To address this problem, we propose a holistic approach to achieve one-size-fits-all compilation optimization for inference across different MLCs. The key observation is that diverse MLCs share multiple key architectural characteristics (e.g., tensor primitives and on-chip scratchpad memory) for tensor processing, which can be generalized for conducting cross-platform compilation optimizations. Concretely, we propose the Tensor Abstract Machine (TAM), which features such common architectural characteristics, as the abstraction of a broad range of MLCs. To leverage architectural characteristics of the TAM, we propose the Tensor Scheduling Language (TSL) consisting of tensor computation description and tensor scheduling primitives for implementing operations with portable optimization. By implementing tensor operations with TSL, the related optimized code for different MLCs can be automatically generated. To validate our proposal, we conduct experiments on 3 commodity MLCs including GPU with Tensor Cores, VTA (on FPGA), and Cloud TPU. Experimental results demonstrate that the code generated from the same optimization schedule achieves 1.05x to 2.05x better performance than hand-tuned libraries and deep learning compilers across different platforms.},
  archive      = {J_TC},
  author       = {Yuanbo Wen and Qi Guo and Zidong Du and Jianxing Xu and Zhenxing Zhang and Xing Hu and Wei Li and Rui Zhang and Chao Wang and Xuehai Zhou and Tianshi Chen},
  doi          = {10.1109/TC.2021.3128266},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2313-2326},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling one-size-fits-all compilation optimization for inference across machine learning computers},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GCONV chain: Optimizing the whole-life cost in end-to-end
CNN acceleration. <em>TC</em>, <em>71</em>(9), 2300–2312. (<a
href="https://doi.org/10.1109/TC.2021.3128159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The acceleration of CNNs has gained increasing attention since their success in computer vision. Since the heterogeneous layers cannot be processed by accelerators proposed for convolution layers only, modern end-to-end CNN acceleration solutions either transform diverse computation into matrix/vector arithmetic, which loses data reuse opportunities in convolution, or introduce dedicated functional unit to each kind of layer, which results in underutilization and high update expenses. To enhance the whole-life cost efficiency, we need a solution that is efficient in processing CNN layers and has the generality to apply to all kinds of existing and emerging layers. To this end, we propose GCONV Chain, a method to convert the entire CNN computation into a chain of standard general convolutions (GCONV) that can be efficiently processed by existing CNN accelerators with low-overhead hardware support. This paper comprehensively analyzes the GCONV Chain model and proposes a full-stack implementation to support GCONV Chain. Our results on various CNNs demonstrate that GCONV Chain improves the performance and energy efficiency of existing CNN accelerators by an average of 3.4x and 3.2x respectively. Furthermore, we show that GCONV Chain provides low whole-life costs for CNN acceleration, including both developer efforts and total cost of ownership.},
  archive      = {J_TC},
  author       = {Jiaqi Zhang and Xiangru Chen and Sandip Ray},
  doi          = {10.1109/TC.2021.3128159},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2300-2312},
  shortjournal = {IEEE Trans. Comput.},
  title        = {GCONV chain: Optimizing the whole-life cost in end-to-end CNN acceleration},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid-SIMD: A modular and reconfigurable approach to beyond
von neumann computing. <em>TC</em>, <em>71</em>(9), 2287–2299. (<a
href="https://doi.org/10.1109/TC.2021.3127354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing complexity of real-life applications demands constant improvements of microprocessor systems. One of the most frequently adopted microprocessor design scheme is the von Neumann architecture. Central Processing Unit (CPU) performs computations and communicates with memory in a constant exchange of information. This unceasing motion of data between these two components became a significant performance bottleneck. A lot of power, energy, and computational time are wasted in this communication. With Beyond von Neumann Computing (BvNC) paradigms, calculations are performed inside or very close to a memory array. BvNC approaches are proposed in the literature, mainly based on modifications of existing memories, enabling simple computations. Others exploit emerging technologies to both store and compute data, using analog operations. In this work we follow a different approach, where computational units are placed close to memory cells, improving versatility and performance. We propose a Hybrid-SIMD architecture made of memory and computing elements in an interleaved structure. Hybrid-SIMD can be used both as a low density memory and as SIMD accelerator. We insert our design in a classical von Neumann system based on a RISC-V processor, and we estimate its impact, demonstrating its capability to improve speed reducing at the same time energy consumption.},
  archive      = {J_TC},
  author       = {Andrea Coluccio and Umberto Casale and Angela Guastamacchia and Giovanna Turvani and Marco Vacca and Massimo Ruo Roch and Maurizio Zamboni and Mariagrazia Graziano},
  doi          = {10.1109/TC.2021.3127354},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2287-2299},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hybrid-SIMD: A modular and reconfigurable approach to beyond von neumann computing},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FIFA: A fully invertible FPGA architecture to reduce
BTI-induced aging effects. <em>TC</em>, <em>71</em>(9), 2277–2286. (<a
href="https://doi.org/10.1109/TC.2021.3127082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {FPGAs are increasingly becoming sensitive to aging effects mainly through BTI phenomena in the latest technology nodes. This phenomenon can be modeled as a shift in the threshold voltage of transistors which leads to performance degradation and reduction in SNM of SRAM cells. To reduce the aging effects on FPGA building blocks, we propose FIFA, a fully invertible FPGA architecture. In this architecture, two small modules are introduced to make the bitstream of logic and routing resources of the FPGA tiles fully invertible. Accordingly, to insert recovery cycles in the lifetime of the transistors, the bitstream stored in the configuration SRAM cells can be inverted occasionally without any changes in the functionality of the design. Using the proposed architecture, it is neither required to repeat the placement and routing procedures to generate multiple configuration bitstreams, nor extra memory is needed to save alternative bitstreams for changing the SRAM cells’ contents. Our experimental results over a set of industrial benchmarks show that by choosing an appropriate switch block arrangement and an optimized flipping frequency, the proposed architecture can improve the aging induced performance degradation by up to 62.5\% with acceptable power and area overheads on logic and routing resources.},
  archive      = {J_TC},
  author       = {Mohammad Hadi Mottaghi and Mehdi Sedighi and Morteza Saheb Zamani},
  doi          = {10.1109/TC.2021.3127082},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2277-2286},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FIFA: A fully invertible FPGA architecture to reduce BTI-induced aging effects},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SWEL-COFAE: Wear leveling and adaptive encoding assisted
compression of frequent words in non-volatile main memories.
<em>TC</em>, <em>71</em>(9), 2263–2276. (<a
href="https://doi.org/10.1109/TC.2021.3126156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging Non-Volatile memories such as Phase Change Memory (PCM) and Resistive RAM are projected as potential replacements of the traditional DRAM-based main memories. However, limited write endurance and high write energy limit their chances of adoption as a mainstream main memory standard. Therefore, developing solutions that enhance the lifetime of these memories while offering a decent system performance has a great impact in building future large capacity and energy-efficient main memories. In this paper, we propose a word-level compression scheme called COMF to reduce bitflips in PCMs by removing the most repeated words from the cache blocks before writing into memory. COMF is augmented with an adaptive granularity-based encoding technique to form COFAE, which reduces the bitflips to a further extent. We also propose SWEL-COFAE, an intra-line stride-based wear leveling technique to improve lifetime by balancing the bitflip pressure within the cells of the memory lines. Experimental results show that the proposed technique improves lifetime by 101\% and reduces bitflips and energy by 59\% and 61\% respectively over baseline.},
  archive      = {J_TC},
  author       = {Arijit Nath and Hemangee K. Kapoor},
  doi          = {10.1109/TC.2021.3126156},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2263-2276},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SWEL-COFAE: Wear leveling and adaptive encoding assisted compression of frequent words in non-volatile main memories},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluation of cache attacks on arm processors and secure
caches. <em>TC</em>, <em>71</em>(9), 2248–2262. (<a
href="https://doi.org/10.1109/TC.2021.3126150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Timing-based side and covert channels in processor caches continue to be a threat to modern computers. This work shows for the first time, a systematic, large-scale analysis of Arm devices and the detailed results of attacks the processors are vulnerable to. Compared to x86, Arm uses different architectures, microarchitectural implementations, cache replacement policies, etc., which affects how attacks can be launched, and how security testing for the vulnerabilities should be done. To evaluate security, this paper presents security benchmarks specifically developed for testing Arm processors and their caches. The benchmarks are evaluated with sensitivity tests, which examine how sensitive the benchmarks are to having a correct configuration in the testing phase. Further, to evaluate a large number of devices, this work leverages a novel approach of using a cloud-based Arm device testbed for architectural and security research on timing channels and runs the benchmarks on 34 different physical devices. In parallel, there has been much interest in secure caches to defend the various attacks. Consequently, this paper also investigates secure cache architectures using proposed benchmarks. Especially, this paper implements and evaluates secure PL and RF caches, showing the security of PL and RF caches, but also uncovers new weaknesses.},
  archive      = {J_TC},
  author       = {Shuwen Deng and Nikolay Matyunin and Wenjie Xiong and Stefan Katzenbeisser and Jakub Szefer},
  doi          = {10.1109/TC.2021.3126150},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2248-2262},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Evaluation of cache attacks on arm processors and secure caches},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PySchedCL: Leveraging concurrency in heterogeneous
data-parallel systems. <em>TC</em>, <em>71</em>(9), 2234–2247. (<a
href="https://doi.org/10.1109/TC.2021.3125792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decade, high performance compute capabilities exhibited by heterogeneous GPGPU platforms have led to the popularity of data parallel programming languages such as CUDA and OpenCL. Developing high performance parallel programming solutions using such languages involve a steep learning curve due to the complexity of the underlying heterogeneous compute devices and their impact on performance. This has led to the emergence of several High Performance Computing frameworks which provide high-level abstractions for easing the development of data-parallel applications on heterogeneous platforms. However, the scheduling decisions undertaken by such frameworks only exploit coarse-grained concurrency in data parallel applications. In this paper, we propose PySchedCL , a framework which explores fine-grained concurrency aware scheduling decisions that harness the power of heterogeneous CPU/GPU architectures efficiently. We showcase the efficacy of such scheduling mechanisms over existing coarse-grained dynamic scheduling schemes by conducting extensive experimental evaluations for a diverse set of popular Deep Learning benchmarks.},
  archive      = {J_TC},
  author       = {Anirban Ghose and Siddharth Singh and Vivek Kulaharia and Lokesh Dokara and Srijeeta Maity and Soumyajit Dey},
  doi          = {10.1109/TC.2021.3125792},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2234-2247},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PySchedCL: Leveraging concurrency in heterogeneous data-parallel systems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Characterization, modeling, and test of intermediate state
defects in STT-MRAMs. <em>TC</em>, <em>71</em>(9), 2219–2233. (<a
href="https://doi.org/10.1109/TC.2021.3125228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The manufacturing process of STT-MRAM requires unique steps to fabricate and integrate magnetic tunnel junction (MTJ) devices which are data-storing elements. Thus, understanding the defects in MTJs and their faulty behaviors are paramount for developing high-quality test solutions. This article applies the advanced device-aware test to intermediate (IM) state defects in MTJ devices based on silicon measurements and circuit simulations. An IM state manifests itself as an abnormal third resistive state, which differs from the two bi-stable states of MTJ. We performed silicon measurements on MTJ devices with diameter ranging from 60nm to 120nm; the results show that the occurrence probability of IM state strongly depends on the switching direction, device size, and bias voltage. We demonstrate that the conventional resistor-based fault modeling and test approach fails to appropriately model and test such a defect. Therefore, device-aware test is applied. We first physically model the defect and incorporate it into a Verilog-A MTJ compact model and calibrate it with silicon data. Thereafter, this model is used for a systematic fault analysis based on circuit simulations to obtain accurate and realistic faults in a pre-defined fault space. Our simulation results show that an IM state defect leads to intermittent write transition faults. Finally, we propose and implement a device-aware test solution to detect the IM state defect.},
  archive      = {J_TC},
  author       = {Lizhou Wu and Siddharth Rao and Mottaqiallah Taouil and Erik Jan Marinissen and Gouri Sankar Kar and Said Hamdioui},
  doi          = {10.1109/TC.2021.3125228},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2219-2233},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Characterization, modeling, and test of intermediate state defects in STT-MRAMs},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BlueVisor: Time-predictable hardware hypervisor for
many-core embedded systems. <em>TC</em>, <em>71</em>(9), 2205–2218. (<a
href="https://doi.org/10.1109/TC.2021.3125226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whilst virtualization was once restricted to large-scale computing platforms, it is now widely deployed on modern embedded computing systems. This has been driven by the availability of hardware support which alleviates the performance penalties incurred by traditional software virtualization technologies. In the domain of hard real-time systems, specialist virtualization technology which respects restricted timing requirements and constraints can be deployed to allow sharing of processors. However, other aspects of the embedded system (I/O, memory, and communication) are harder to analyze. In this paper, we argue that in order to support real-time virtualization on modern embedded systems, additional system-wide hardware support is required. We propose BlueVisor , an analyzable and scalable hardware hypervisor for many-core embedded systems, which enables time-predictable CPU, memory, and I/O virtualization, as well as supporting a fast interrupt handler, and inter-VM communication. We describe the design and implementation of the real-time hypervisor, and demonstrate how a BlueVisor -based virtualization system can be leveraged to meet real-time requirements with significant improvement in system performance, and with a low-performance cost when executing different types of software.},
  archive      = {J_TC},
  author       = {Zhe Jiang and Ran Wei and Pan Dong and Yan Zhuang and Neil C. Audsley and Ian Gray},
  doi          = {10.1109/TC.2021.3125226},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2205-2218},
  shortjournal = {IEEE Trans. Comput.},
  title        = {BlueVisor: Time-predictable hardware hypervisor for many-core embedded systems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bypassing multicore memory bugs with coarse-grained
reconfigurable logic. <em>TC</em>, <em>71</em>(9), 2191–2204. (<a
href="https://doi.org/10.1109/TC.2021.3125188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multicore systems deploy sophisticated memory hierarchies to improve memory operations’ throughput and latency by exploiting multiple levels of cache hierarchy and several complex memory-access instructions. As a result, the functional verification of the memory subsystem is one of the most challenging tasks in the overall system design effort, leading to many bugs in the released product. In this work, we propose MemPatch, a novel reconfigurable hardware solution to bypass such escaped bugs. To design MemPatch, we first analyzed publicly available errata documents and classified memory-related bugs by root cause and symptoms. We then leveraged that learning to design a specialized, reconfigurable detection fabric, implementing finite state machines that can model the bug-triggering events at the microarchitectural level. Finally, we complemented this detection logic with hardware offering multiple bug-bypassing options. Our evaluation of MemPatch mapped a multicore RISC-V out-of-order processor, augmented with our logic, to a Xilinx ZCU102 FPGA board. When configured to detect up to 32 distinct bugs, MemPatch entails 7.6\% area and 7.3\% power overheads. An estimate on a commercial ARM Cortex-A57 processor target indicates that the area overhead would be much lower, 1.0\%. The performance impact was found to be no more than 1\% in all cases.},
  archive      = {J_TC},
  author       = {Doowon Lee and Valeria Bertacco},
  doi          = {10.1109/TC.2021.3125188},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2191-2204},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Bypassing multicore memory bugs with coarse-grained reconfigurable logic},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A first look at RISC-v virtualization from an embedded
systems perspective. <em>TC</em>, <em>71</em>(9), 2177–2190. (<a
href="https://doi.org/10.1109/TC.2021.3124320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article describes the first public implementation and evaluation of the latest version of the RISC-V hypervisor extension (H-extension v0.6.1) specification in a Rocket chip core. To perform a meaningful evaluation for modern multi-core embedded and mixed-criticality systems, we have ported Bao, an open-source static partitioning hypervisor, to RISC-V. We have also extended the RISC-V platform-level interrupt controller (PLIC) to enable direct guest interrupt injection with low and deterministic latency and we have enhanced the timer infrastructure to avoid trap and emulation overheads. Experiments were carried out in FireSim, a cycle-accurate, FPGA-accelerated simulator, and the system was also successfully deployed and tested in a Zynq UltraScale+ MPSoC ZCU104. Our hardware implementation was open-sourced and is currently in use by the RISC-V community towards the ratification of the H-extension specification.},
  archive      = {J_TC},
  author       = {Bruno Sá and José Martins and Sandro Pinto},
  doi          = {10.1109/TC.2021.3124320},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2177-2190},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A first look at RISC-V virtualization from an embedded systems perspective},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Magnifying side-channel leakage of lattice-based
cryptosystems with chosen ciphertexts: The case study of kyber.
<em>TC</em>, <em>71</em>(9), 2163–2176. (<a
href="https://doi.org/10.1109/TC.2021.3122997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lattice-based cryptography, as an active branch of post-quantum cryptography (PQC), has drawn great attention from side-channel analysis researchers in recent years. Despite the various side-channel targets examined in previous studies, detail on revealing the secret-dependent information efficiently is less studied. In this paper, we propose adaptive EM side-channel attacks with carefully constructed ciphertexts on Kyber, which is a finalist of NIST PQC standardization project. We demonstrate that specially chosen ciphertexts allow an adversary to modulate the leakage of a target device and enable full key extraction with a small number of traces through simple power analysis. Compared to prior research, our techniques require fewer traces and avoid building complex templates. We practically evaluate our methods using both a reference implementation and the ARM-specific implementation in pqm4 library. For the reference implementation, we target the leakage of the output of the inverse NTT computation and recover the full key with only four traces. For the pqm4 implementation, we develop a message-recovery attack that leads to extraction of the full secret key with between eight and 960 traces, depending on the compiler optimization level. We discuss the relevance of our findings to other lattice-based schemes and explore potential countermeasures.},
  archive      = {J_TC},
  author       = {Zhuang Xu and Owen Pemberton and Sujoy Sinha Roy and David Oswald and Wang Yao and Zhiming Zheng},
  doi          = {10.1109/TC.2021.3122997},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2163-2176},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Magnifying side-channel leakage of lattice-based cryptosystems with chosen ciphertexts: The case study of kyber},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Protecting synchronization mechanisms of parallel big data
kernels via logging. <em>TC</em>, <em>71</em>(9), 2156–2162. (<a
href="https://doi.org/10.1109/TC.2021.3122993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing effort to reduce power consumption in machines, fault tolerance becomes more of a concern. This holds particularly for large-scale computing, where execution failures due to soft faults waste excessive time and resources. These large-scale applications are normally parallel in nature and rely on control structures tailored specifically for parallel computing, such as locks and barriers. While there are many studies on resilient software, to our knowledge none of them focus on protecting these parallel control structures. In this work, we present a method of ensuring the correct operation of both locks and barriers in parallel applications. Our method tracks the memory locations used within parallel sections and detects a violation of the control structures. Upon detecting any violation, the violating thread is rolled back to the beginning of the structure and reattempts it, similar to rollback mechanisms in transactional memory systems. We test the method on representative samples of the BigDataBench kernels and find it exhibits a mean error reduction of 93.6\% for basic mutex locks and barriers with a mean 6.55\% execution time overhead at 64 threads. Additionally, we provide a comparison to transactional memory methods and demonstrate up to a mean 57.5\% execution time overhead reduction.},
  archive      = {J_TC},
  author       = {Travis LeCompte and Lu Peng and Xu Yuan and Nian-Feng Tzeng},
  doi          = {10.1109/TC.2021.3122993},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2156-2162},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Protecting synchronization mechanisms of parallel big data kernels via logging},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural-PIM: Efficient processing-in-memory with neural
approximation of peripherals. <em>TC</em>, <em>71</em>(9), 2142–2155.
(<a href="https://doi.org/10.1109/TC.2021.3122905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing-in-memory (PIM) architectures have demonstrated great potential in accelerating numerous deep learning tasks. Particularly, resistive random-access memory (RRAM) devices provide a promising hardware substrate to build PIM accelerators due to their abilities to realize efficient in-situ vector-matrix multiplications (VMMs). However, existing PIM accelerators suffer from frequent and energy-intensive analog-to-digital (A/D) conversions, severely limiting their performance. This paper presents a new PIM architecture to efficiently accelerate deep learning tasks by minimizing the required A/D conversions with analog accumulation and neural approximated peripheral circuits. We first characterize the different dataflows employed by existing PIM accelerators, based on which a new dataflow is proposed to remarkably reduce the required A/D conversions for VMMs by extending shift and add (S+A) operations into the analog domain before the final quantizations. We then leverage a neural approximation method to design both analog accumulation circuits (S+A) and quantization circuits (ADCs) with RRAM crossbar arrays in a highly-efficient manner. Finally, we apply them to build a RRAM-based PIM accelerator (i.e., Neural-PIM ) upon the proposed analog dataflow and evaluate its system-level performance. Evaluations on different benchmarks demonstrate that Neural-PIM can improve energy efficiency by $5.36\times$ ( $1.73\times$ ) and speed up throughput by $3.43\times$ ( $1.59\times$ ) without losing accuracy, compared to the state-of-the-art RRAM-based PIM accelerators, i.e., ISAAC [1] (CASCADE [2]).},
  archive      = {J_TC},
  author       = {Weidong Cao and Yilong Zhao and Adith Boloor and Yinhe Han and Xuan Zhang and Li Jiang},
  doi          = {10.1109/TC.2021.3122905},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2142-2155},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Neural-PIM: Efficient processing-in-memory with neural approximation of peripherals},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CFHider: Protecting control flow confidentiality with intel
SGX. <em>TC</em>, <em>71</em>(9), 2128–2141. (<a
href="https://doi.org/10.1109/TC.2021.3122903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Program control flow reflects the algorithm of that program and may reveal implementation vulnerabilities. Thus its confidentiality needs to be protected, especially in a cloud setting. However, most existing control flow obfuscation methods are software-based, which cannot offer high confidentiality while maintaining low performance overhead. In this paper, we propose CFHider, a hardware-assisted solution. By performing program transformation and leveraging Trusted Execution Environments (Intel SGX), CFHider moves branch statement conditions to an opaque and trusted memory space during the program execution. We proved that by generating Obfuscation Invariants, CFHider is able to provide provable control flow confidentiality protection. Based on the design of CFHider, we also developed a prototype system for Java applications. Our security analysis and experimental results indicate that CFHider is effective in protecting control flow confidentiality and incurs a much reduced performance overhead than existing software-based solutions (by a factor of 18.1).},
  archive      = {J_TC},
  author       = {Yongzhi Wang and Yu Zou and Yulong Shen and Yao Liu},
  doi          = {10.1109/TC.2021.3122903},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2128-2141},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CFHider: Protecting control flow confidentiality with intel SGX},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A syscall-level binary-compatible unikernel. <em>TC</em>,
<em>71</em>(9), 2116–2127. (<a
href="https://doi.org/10.1109/TC.2021.3122896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unikernels are minimal single-purpose virtual machines. They are highly popular in the research domain due to the benefits they provide. A barrier to their widespread adoption is the difficulty/impossibility to port existing applications to current unikernels. HermiTux is the first unikernel providing system call-level binary compatibility with Linux applications. It is composed of a hypervisor and a lightweight kernel layer emulating the load- and runtime Linux ABI. HermiTux relieves application developers from the burden of porting software, while providing unikernel benefits such as security through hardware-assisted virtualized isolation, swift boot time, and low disk/memory footprint. Fast system calls and kernel modularity are enabled through binary rewriting and analysis techniques, as well as shared library substitution. HermiTux&#39;s design principles are architecture-independent and we present a prototype on both the x86-64 and ARM aarch64 ISAs, targeting various cloud as well as edge/embedded deployments. We demonstrate HermiTux&#39;s compatibility over a range of native C/C++/Fortran/Python Linux applications. We also show that it offers a similar degree of lightweightness compared to other unikernels, and that it performs similarly to Linux in many cases: its performance overhead averages 3\% in memory- and compute-bound scenarios, and its I/O performance is acceptable.},
  archive      = {J_TC},
  author       = {Pierre Olivier and Hugo Lefeuvre and Daniel Chiba and Stefan Lankes and Changwoo Min and Binoy Ravindran},
  doi          = {10.1109/TC.2021.3122896},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2116-2127},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A syscall-level binary-compatible unikernel},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incentive assignment in hybrid consensus blockchain systems
in pervasive edge environments. <em>TC</em>, <em>71</em>(9), 2102–2115.
(<a href="https://doi.org/10.1109/TC.2021.3122891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing is becoming pervasive in our daily lives with emerging smart devices and the development of communication technology. Smart devices with various resources make data transactions prevalent over edge environments. To ensure such transactions are unmodifiable and undeniable, blockchain technology is introduced into edge environments. In this paper, we propose a hybrid blockchain system to enhance the security for transactions and determine the incentive for miners in edge computing environments. We propose a Proof of Work (PoW) and Proof of Stake (PoS) hybrid consensus blockchain system utilizing the heterogeneity of devices to adapt to the characteristic of edge environments. We raise the incentive assignment problem for a fair incentive to PoW miners. We formulate the problem and propose an iterative and another heuristic algorithm to determine the incentive that the miner will receive for a new block. We further prove that the iterative algorithm can obtain global optimal results. Simulation and experiment results show that our proposed algorithm can give a reasonable incentive to miners under different system parameters in edge blockchain systems.},
  archive      = {J_TC},
  author       = {Yaodong Huang and Yiming Zeng and Fan Ye and Yuanyuan Yang},
  doi          = {10.1109/TC.2021.3122891},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2102-2115},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Incentive assignment in hybrid consensus blockchain systems in pervasive edge environments},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A ferroelectric-based volatile/non-volatile dual-mode buffer
memory for deep neural network accelerators. <em>TC</em>,
<em>71</em>(9), 2088–2101. (<a
href="https://doi.org/10.1109/TC.2021.3122872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network (DNN) inference and training produce a large amount of intermediate data. To achieve high energy efficiency, sufficient on-chip buffer is preferred to reduce the energy and time consuming off-chip DRAM access. However, SRAM buffer suffers from large area cost and high standby power due to its large cell size and high leakage current. Although embedded DRAM (eDRAM) offers higher memory density, its energy consumption is high due to frequent refresh operation, which is induced by the short refresh interval (40∼100μs). In this paper, a dual-mode buffer memory based on the CMOS compatible HfZrO 2 ferroelectric material is proposed for DNN accelerators. It can operate in both volatile eDRAM mode and non-volatile ferroelectric RAM (FeRAM) mode. The functionality of the proposed dual-mode memory bit-cell design is verified using SPICE simulation with the multi-domain Preisach physical model. A data-lifetime-aware memory mode configuration protocol is proposed to optimize the buffer access energy for both DNN inference and training. Detailed circuitry and architectural support for the dual-mode memory are presented. For DNN training with ferroelectric-field-effect-transistor (FeFET) and SRAM-based compute-in-memory (CIM) accelerator, the proposed dual-mode buffer design improves the overall energy efficiency by 92.2\%∼98.7\%, 44.1\%∼47.6\%, 12.6\%∼13.0\% compared to baseline designs using SRAM buffer with the same buffer area, eDRAM and FeRAM with the same buffer capacity, respectively. For DNN inference with tensor-processing-unit (TPU)-like systolic array, the energy efficiency during computing is improved by 40.7\%∼45.6\%, 18.4\%∼29.6\% compared to the designs with eDRAM and FeRAM buffer, respectively. By storing the persistent data using the non-volatile mode, the energy efficiency of systolic array is improved by 2.3×∼5.5× over SRAM-based design when standby is frequent. The chip area overhead of the dual-mode buffer design is 5.2\%, 4.1\% and 7.2\% for FeFET-based-CIM, SRAM-based-CIM and systolic-array-based accelerators using eDRAM buffer, respectively.},
  archive      = {J_TC},
  author       = {Yandong Luo and Yuan-Chun Luo and Shimeng Yu},
  doi          = {10.1109/TC.2021.3122872},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2088-2101},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A ferroelectric-based Volatile/Non-volatile dual-mode buffer memory for deep neural network accelerators},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deadlock avoidance algorithms for recursion-tree modeled
requests in parallel executions. <em>TC</em>, <em>71</em>(9), 2073–2087.
(<a href="https://doi.org/10.1109/TC.2021.3122843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an extension of the banker&#39;s algorithm to resolve deadlock for programs whose resource-request graph can be modeled as a recursion tree for parallel execution. Our algorithm implements the banker&#39;s logic, with the key difference being that some properties of the tree are fully exploited to improve the resource utilization and safety check in deadlock avoidance. For an $n$ -node tree modeled program making requests to $m$ types of resources, our recursion-tree based algorithm can obtain a time complexity of $O(mn\log \log n)$ on average in safety check while reducing the conservativeness in resource utilization. We reap these benefits by proposing a concept of the resource critical tree and leverage it to localize the maximum claim associated with each node in the tree. To tackle the case when the tree model is not statically known, we relax the definition of a local maximum claim by sacrificing some resource utilization. With this trade-off, the algorithm can resolve the deadlock and achieve more efficient safety checks within time of $O(m\log \log n)$ . Our empirical studies on a two-dimensional integration problem on sparse grids show that the proposed algorithms can reduce resource utilization conservativeness and improve avoidance performance by minimizing the number of safety checks.},
  archive      = {J_TC},
  author       = {Yang Wang and Min Li and Hao Dai and Kenneth B. Kent and Kejiang Ye and Chengzhong Xu},
  doi          = {10.1109/TC.2021.3122843},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2073-2087},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Deadlock avoidance algorithms for recursion-tree modeled requests in parallel executions},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Leaking secrets through modern branch predictors in the
speculative world. <em>TC</em>, <em>71</em>(9), 2059–2072. (<a
href="https://doi.org/10.1109/TC.2021.3122830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transient execution attacks that exploit speculation have raised significant concerns in computer systems. Typically, branch predictors are leveraged to trigger mis-speculation in transient execution attacks. In this work, we demonstrate a new class of speculation-based attacks that targets the branch prediction unit (BPU). We find that speculative resolution of conditional branches (i.e., in nested speculation) alter the states of pattern history table (PHT) in modern processors, which are not restored after the corresponding branches are later squashed. Such characteristic allows attackers to exploit the BPU as the secret transmitting medium in transient execution attacks. To evaluate the discovered vulnerability, we build a novel attack framework, BranchSpectre , that enables exfiltration of unintended secrets through observing speculative PHT updates (in the form of covert and side channels). We further investigate the PHT collision mechanism in the history-based predictor and the branch prediction mode transitions in Intel processors. Built upon such knowledge, we implement an ultra-high speed covert channel ( BranchSpectre-cc ) as well as two side channels (i.e., BranchSpectre-v1 and BranchSpectre-v2 ) that merely rely on BPU for mis-speculation trigger and secret inference in the speculative domain. Notably, BranchSpectre side channels can take advantage of much simpler code patterns than those used in Spectre attacks. We present an extensive BranchSpectre code gadget analysis on a set of popular real-world application code bases followed by a demonstration of side channel attack on OpenSSL. The evaluation results show substantially wider existence and higher exploitability of BranchSpectre code patterns in real-world software. Finally, we discuss several secure branch prediction mechanisms that can mitigate transient execution attacks exploiting modern branch predictors.},
  archive      = {J_TC},
  author       = {Md Hafizul Islam Chowdhuryy and Fan Yao},
  doi          = {10.1109/TC.2021.3122830},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2059-2072},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Leaking secrets through modern branch predictors in the speculative world},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymmetric crosstalk harness signaling for common eigenmode
elimination. <em>TC</em>, <em>71</em>(9), 2048–2058. (<a
href="https://doi.org/10.1109/TC.2021.3120242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel scheme based on the Crosstalk Harnessed Signaling (CHS) technique for parallel high-speed interfaces. The proposed scheme, called Asymmetric Crosstalk Harnessed Signaling (ACHS), provides a robust and consistent eye opening for all the transmitted bits in the interface. The Hadamard matrix employed for CHS encoding has been modified to a non-square format in order to either eliminate the common eigenmode or turn it differential. This in turn results in a signaling scheme that converts a binary data array to a slightly larger signal/interconnect array. The resulting signaling and routing overhead translates into a comparable eye opening across all the bits inside the encoded bus, overcoming the crosstalk sensitivity associated to the common mode present in the original CHS scheme. Both CHS and the proposed ACHS are implemented on a 16-bit source-synchronous bus wired through 3-dimensional interconnect arrangements in a multi-stripline stackup, in order to compare performances in very aggressive crosstalk environments. Simulation results show a consistent eye opening across all data bits when ACHS is applied, rendering a fully functional bus with only 11\% routing overhead, against a practically inoperable bus in the CHS case, due to the eye collapse for the common-mode encoded bit.},
  archive      = {J_TC},
  author       = {Daniel Iparraguirre and José G. Delgado-Frias and Howard Heck},
  doi          = {10.1109/TC.2021.3120242},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2048-2058},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Asymmetric crosstalk harness signaling for common eigenmode elimination},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Near-precise parameter approximation for multiple
multiplications on a single DSP block. <em>TC</em>, <em>71</em>(9),
2036–2047. (<a href="https://doi.org/10.1109/TC.2021.3119187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DSP blocks are one of the efficient solutions to implement multiply-accumulate (MAC) operations on FPGA’s. However, since the DSP blocks have wide multiplier and adder blocks, MAC operations using low bit-length parameters lead to an underutilization. Hence, an efficient approximation technique is introduced. The technique includes manipulation and approximation of the low bit-length parameters based upon a Single DSP - Multiple Multiplication (SDMM) execution. The accuracy of the developed optimization technique was evaluated for different CNN weight bit precisions using the Alexnet and VGG-16 networks and the ImageNet ILSVRC-2012 dataset. The optimization can be implemented without loss of accuracy in almost all cases, while it causes slight accuracy losses in a few cases. Through these optimizations, multiple parameter multiplications are performed in a single DSP block at the cost of a small hardware overhead. As a result of our optimizations, the parameters are represented in a different format on off-chip memory, providing up to 33\% compression without any hardware cost. A prototype systolic array architecture was implemented employing our optimizations on a Xilinx Zynq FPGA. It reduced the number of DSP blocks by 66.6\%, 75\%, and 83.3\% for 8, 6, and 4-bit input variables, respectively.},
  archive      = {J_TC},
  author       = {Ercan Kalali and Rene van Leuken},
  doi          = {10.1109/TC.2021.3119187},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2036-2047},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Near-precise parameter approximation for multiple multiplications on a single DSP block},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stochastic SOT device based SNN architecture for on-chip
unsupervised STDP learning. <em>TC</em>, <em>71</em>(9), 2022–2035. (<a
href="https://doi.org/10.1109/TC.2021.3119180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging device based spiking neural network (SNN) hardware design has been actively studied. Especially, energy and area efficient synapse crossbar has been of particular interest, but processing units for weight summations in synapse crossbar are still a main bottleneck for energy and area efficient hardware design. In this paper, we propose an efficient SNN architecture with stochastic spin-orbit torque (SOT) device based multi-bit synapses. First, we present SOT device based synapse array using modified gray code. The modified gray code based synapse needs only N devices to represent 2 N levels of synapse weights. Accumulative spike technique is also adopted in the proposed synapse array, to improve ADC utilization and reduce the number of neuron updates. In addition, we propose hardware friendly algorithmic techniques to improve classification accuracies as well as energy efficiencies. Non-spike depression based stochastic spike-timing-dependent plasticity is used to reduce the overlapping input representation and classification error. Early read termination is also employed to reduce energy consumption by turning off less associated neurons. The proposed SNN processor has been implemented using 65nm CMOS process, and it shows 90\% classification accuracy in MNIST dataset consuming 0.78μJ/image (training) and 0.23μJ/image (inference) of energy with an area of 1.12mm 2 .},
  archive      = {J_TC},
  author       = {Yunho Jang and Gyuseong Kang and Taehwan Kim and Yeongkyo Seo and Kyung-Jin Lee and Byong-Guk Park and Jongsun Park},
  doi          = {10.1109/TC.2021.3119180},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2022-2035},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Stochastic SOT device based SNN architecture for on-chip unsupervised STDP learning},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy-efficient instruction delivery in embedded systems
with domain wall memory. <em>TC</em>, <em>71</em>(9), 2010–2021. (<a
href="https://doi.org/10.1109/TC.2021.3117439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As performance and energy-efficiency improvements from technology scaling are slowing down, new technologies are being researched in hopes of disrupting results. Domain wall memory (DWM) is an emerging non-volatile technology that promises extreme data density, fast access times and low power consumption. However, DWM access time depends on the memory location distance from access ports, requiring expensive shifting. This causes overheads on performance and energy consumption. In this article, we implement our previously proposed shift-reducing instruction memory placement (SHRIMP) on a RISC-V core in RTL, provide the first thorough evaluation of the control logic required for DWM and SHRIMP and evaluate the effects on system energy and energy-efficiency. SHRIMP reduces the number of shifts by 36\% on average compared to a linear placement in CHStone and Coremark benchmark suites when evaluated on the RISC-V processor system. The reduced shift amount leads to an average reduction of 14\% in cycle counts compared to the linear placement. When compared to an SRAM-based system, although increasing memory usage by 26\%, DWM with SHRIMP allows a 73\% reduction in memory energy and 42\% relative energy delay product. We estimate overall energy reductions of 14\%, 15\% and 19\% in three example embedded systems.},
  archive      = {J_TC},
  author       = {Joonas Multanen and Kari Hepola and Asif Ali Khan and Jeronimo Castrillon and Pekka Jääskeläinen},
  doi          = {10.1109/TC.2021.3117439},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2010-2021},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Energy-efficient instruction delivery in embedded systems with domain wall memory},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MM-FSM<span class="math inline">:</span>: A high-efficiency
general nonlinear function generator for stochastic computation.
<em>TC</em>, <em>71</em>(9), 1998–2009. (<a
href="https://doi.org/10.1109/TC.2021.3117127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear function calculation is widely used in numerous science and technology fields. Stochastic computation is a novel high-efficiency value representation and calculation scheme, which is helpful to reduce hardware cost. However, the main challenges of stochastic computation based nonlinear function implementation lie on poor generalization, low accuracy and high latency. In this paper, we propose a multiple driving and multiple dimension finite state machine (MM-FSM) to realize major single variable nonlinear functions used in information and signal processing areas on common platforms with low complexity and latency. We provide corresponding synthesis method of the activation parameters and conditional parameters of MM-FSM. In order to improve the calculation accuracy, we further propose an adaptive scaling algorithm for MM-FSM. The most salient feature of MM-FSM is that we can configure different types of nonlinear functions with the same MM-FSM structure. Thus, MM-FSM can be used in a wide range of stochastic based applications. Compared with the traditional stochastic scheme and Coordinate Rotation Digital Computer (CORDIC) algorithm, simulation results show that the proposed MM-FSM nonlinear function generator has significantly lower complexity while guaranteeing the calculation accuracy.},
  archive      = {J_TC},
  author       = {Xincheng Feng and Ke Hu and Kaining Han},
  doi          = {10.1109/TC.2021.3117127},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1998-2009},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MM-FSM$:$: a high-efficiency general nonlinear function generator for stochastic computation},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enabling the duo-phase data management to realize longevity
bit-alterable flash memory. <em>TC</em>, <em>71</em>(8), 1982–1997. (<a
href="https://doi.org/10.1109/TC.2021.3116862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bit-alterable flash memory is a cutting-edge technology that enables a novel operation, called page-level erase operation, to erase a flash page within a block arbitrarily. Though the page-level erase operation can ease the overhead of the live-page copying during a garbage collection process, it also introduces a new wear-leveling problem. In such the problem, flash pages within the same block will receive different program/erase (P/E) cycles during runtime. Some specific pages storing hot data will be worn out soon; therefore, the lifespan of the bit-alterable flash memory will be short due to the uneven worn out of flash pages. Consequently, it becomes a critical issue with the bit-alterable flash memory, and the state-of-art wear-leveling designs cannot resolve this problem. For tackling the problem, this study presents a duo-phase data management scheme considering the page-level wear-leveling issue. The duo-phase mechanism simultaneously cares about page- and block-level wear-leveling issues. On the page-level wear-leveling problem, the duo-phase scheme identifies hot flash pages via the few bits and softly restricts the hot flash page to store hot data without sacrificing storage capacity. On the other hand, our proposed mechanism also figures out the solution to minimize the number of copied live pages for the block-level wear-leveling issue. According to the experimental results, our duo-phase scheme can prolong the lifespan of the bit-alterable flash memory by 1.5 to 1.9 times, compared with the state-of-art garbage collection mechanisms.},
  archive      = {J_TC},
  author       = {Tseng-Yi Chen and Shao-Hung Chi and Ming-Chang Yang and Ting-Ying Chien},
  doi          = {10.1109/TC.2021.3116862},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1982-1997},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling the duo-phase data management to realize longevity bit-alterable flash memory},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Input-aware sparse tensor storage format selection for
optimizing MTTKRP. <em>TC</em>, <em>71</em>(8), 1968–1981. (<a
href="https://doi.org/10.1109/TC.2021.3113028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Canonical polyadic decomposition (CPD) is one of the most common tensor computations adopted in many scientific applications. The major bottleneck of CPD is matricized tensor times Khatri-Rao product (MTTKRP). To optimize the performance of MTTKRP, various sparse tensor formats have been proposed such as CSF and HiCOO. However, due to the spatial complexity of the tensors, no single format fits all tensors. To address this problem, we propose SpTFS , a framework that automatically predicts the optimal storage format for an input sparse tensor. Specifically, SpTFS leverages a set of sampling methods to lower the sparse tensor to fixed-sized matrices and sparsity features. In addition, SpTFS adopts both supervised learning based and unsupervised learning based methods to predict the optimal sparse tensor storage formats. For supervised learning, we propose TnsNet that combines convolution neural network (CNN) and the feature layer, which effectively captures the sparsity patterns of the input tensors. Once trained, the TnsNet can be used with either density or histogram representation of the input tensor for optimal format prediction. Whereas for unsupervised learning, we propose TnsClustering that consists of a feature encoder using convolutional layers and fully connected layers, and a K-means++ model to cluster sparse tensors for optimal tensor format prediction, without massively profiling on the hardware platform. SpTFS can use the above two models to predict the optimal tensor storage format for accelerating MTTKRP accurately. The experimental results show that both TnsNet and TnsClustering can achieve higher prediction accuracy and performance speedup compared to the state-of-the-art works.},
  archive      = {J_TC},
  author       = {Qingxiao Sun and Yi Liu and Hailong Yang and Ming Dun and Zhongzhi Luan and Lin Gan and Guangwen Yang and Depei Qian},
  doi          = {10.1109/TC.2021.3113028},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1968-1981},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Input-aware sparse tensor storage format selection for optimizing MTTKRP},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LFOC+: A fair OS-level cache-clustering policy for commodity
multicore systems. <em>TC</em>, <em>71</em>(8), 1952–1967. (<a
href="https://doi.org/10.1109/TC.2021.3112970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commodity multicore systems are increasingly adopting hardware support that enables the system software to partition the last-level cache (LLC). This support makes it possible for the operating system (OS) or the Virtual Machine Monitor (VMM) to mitigate shared-resource contention effects on multicores by assigning different co-running applications to various cache partitions. Recently cache-clustering (or partition-sharing) strategies have emerged as a way to improve system throughput and fairness on new platforms with cache-partitioning support. As opposed to strict cache-partitioning, which allocates separate cache partitions to each application, cache-clustering allows partitions to be shared by a group of applications. In this article we propose LFOC+, a fairness-aware OS-level cache-clustering policy for commodity multicore systems. LFOC+ tries to mimic the behavior of the optimal cache-clustering solution for fairness, which we could obtain for different workload scenarios by using a simulation tool. Our dynamic cache-clustering strategy continuously gathers data from performance monitoring counters to classify applications at runtime based on the degree of cache sensitivity and contentiousness, and effectively separates cache-sensitive applications from aggressor programs to improve fairness, while providing acceptable system throughput. We implemented LFOC+ in the Linux kernel and evaluated it on a real system featuring an Intel Skylake processor, where we compare its effectiveness to that of four previously proposed cache-clustering policies. Our experimental analysis reveals that LFOC+ constitutes a lightweight OS-level policy and improves fairness relative to two other state-of-the-art fairness-aware strategies –Dunn and LFOC–, by up to 22\% and up to 20.6\%, respectively, and by 9\% and 4.9\% on average.},
  archive      = {J_TC},
  author       = {Juan Carlos Saez and Fernando Castro and Graziano Fanizzi and Manuel Prieto-Matias},
  doi          = {10.1109/TC.2021.3112970},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1952-1967},
  shortjournal = {IEEE Trans. Comput.},
  title        = {LFOC+: A fair OS-level cache-clustering policy for commodity multicore systems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Olympus: Reaching memory-optimality on DNN processors.
<em>TC</em>, <em>71</em>(8), 1939–1951. (<a
href="https://doi.org/10.1109/TC.2021.3112262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In DNN processors, main memory consumes much more energy than arithmetic operations. Therefore, many memory-oriented network scheduling (MONS) techniques are introduced to exploit on-chip data reuse opportunities and reduce accesses to memory. However, to derive the theoretical lower bound of memory overhead for DNNs is still a significant challenge, which also sheds light on how to reach memory-level optimality by means of network scheduling. Prior work on MONS mainly focused on disparate optimization techniques or missed some of the data reusing opportunities in diverse network models, thus their results are likely to deviate from the true memory-optimality that can be achieved in processors. This paper introduces Olympus, which comprehensively considers the entire memory-level DNN scheduling space, formally analyzes the true memory-optimality and also how to reach the memory-optimal schedules for an arbitrary DNN running on a DNN processor. The key idea behind Olympus is to derive a true memory lower-bound regarding both the intra-layer and inter-layer reuse opportunities, which has not been simultaneously explored by prior works. Evaluation on SOTA DNN processors of different architectures shows that Olympus can guarantee the minimum off-chip memory access, and it reduces 12.3-85.6\% DRAM access and saves 7.4-70.3\% energy on the latest network models.},
  archive      = {J_TC},
  author       = {Xuyi Cai and Ying Wang and Kaijie Tu and Chengsi Gao and Lei Zhang},
  doi          = {10.1109/TC.2021.3112262},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1939-1951},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Olympus: Reaching memory-optimality on DNN processors},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learned FBF: Learning-based functional bloom filter for
key–value storage. <em>TC</em>, <em>71</em>(8), 1928–1938. (<a
href="https://doi.org/10.1109/TC.2021.3112079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a challenging attempt to replace a traditional data structure with a learned model, this paper proposes a learned functional Bloom filter (L-FBF) for a key–value storage. The learned model in the proposed L-FBF learns the characteristics and the distribution of given data and classifies each input. It is shown through theoretical analysis that the L-FBF provides a lower search failure rate than a single FBF in the same memory size, while providing the same semantic guarantees. For model training, character-level neural networks are used with pretrained embeddings. In experiments, four types of different character-level neural networks are trained: a single gated recurrent unit (GRU), two GRUs, a single long short-term memory (LSTM), and a single one-dimensional convolutional neural network (1D-CNN). Experimental results prove the validity of theoretical results, and show that the L-FBF reduces the search failures by 82.8\% to 83.9\% when compared with a single FBF under the same amount of memory used.},
  archive      = {J_TC},
  author       = {Hayoung Byun and Hyesook Lim},
  doi          = {10.1109/TC.2021.3112079},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1928-1938},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Learned FBF: Learning-based functional bloom filter for Key–Value storage},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lattice-based cryptosystems on FPGA: Parallelization and
comparison using HLS. <em>TC</em>, <em>71</em>(8), 1916–1927. (<a
href="https://doi.org/10.1109/TC.2021.3112052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with hardware implementations for lattice-based cryptography. Various CPA and CCA secure algorithms for LWE, RLWE and MLWE problems have been studied, parallelized, implemented and compared on FPGA using high-level synthesis. The impact of PRNG choices on the implementations performances and costs is also evaluated. HLS allows us to compare various sets of algorithms, architectures and parameters with a reduced design effort. Our results are often similar to state-of-the-art for various speed and cost trade-offs. Sometimes we obtain better results thanks to the exploration of numerous architecture and algorithm optimizations.},
  archive      = {J_TC},
  author       = {Timo Zijlstra and Karim Bigou and Arnaud Tisserand},
  doi          = {10.1109/TC.2021.3112052},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1916-1927},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Lattice-based cryptosystems on FPGA: Parallelization and comparison using HLS},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comprehensive methodology to optimize FPGA designs via the
roofline model. <em>TC</em>, <em>71</em>(8), 1903–1915. (<a
href="https://doi.org/10.1109/TC.2021.3111761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With reconfigurable fabrics delivering increasing performance over the years, Field-Programmable Gate Arrays (FPGAs) are becoming an appealing solution for next-generation High-Performance Computing (HPC) systems. However, in order to gain traction among traditional von Neumann architectures, the optimization process of Field-Programmable Gate Array (FPGA) designs should be further abstracted to a higher level. In fact, while High-Level Synthesis (HLS) already provides a handy way to write FPGA code with common high-level languages, substantial effort and expertise are still required to optimize the resulting FPGA design for the underlying hardware. To overcome this problem, we propose a semi-automated performance optimization methodology based on a Hierarchical Roofline model for FPGAs. System-wide and applications-specific optimizations such as off-chip memory transfer and data locality optimizations are guided by the FPGA Roofline model whereas FPGA-specific optimizations are automatically searched by a Design Space Exploration (DSE) engine. We demonstrate the way this methodology allows to easily analyze and optimize to peak system performance a wide set of applications ranging from particle methods, wavefront algorithms, and sparse arithmetic computations. In addition, we prove that the integrated Design Space Exploration (DSE) engine achieves a 14.36x maximum speedup if compared to previous automated solutions in the literature.},
  archive      = {J_TC},
  author       = {Marco Siracusa and Emanuele Del Sozzo and Marco Rabozzi and Lorenzo Di Tucci and Samuel Williams and Donatella Sciuto and Marco Domenico Santambrogio},
  doi          = {10.1109/TC.2021.3111761},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1903-1915},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A comprehensive methodology to optimize FPGA designs via the roofline model},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). <span class="math inline">MC − FLEX</span>MC-FLEX: Flexible
mixed-criticality real-time scheduling by task-level mode switch.
<em>TC</em>, <em>71</em>(8), 1889–1902. (<a
href="https://doi.org/10.1109/TC.2021.3111743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed-criticality (MC) scheduling becomes popular in real-time systems as it supports different criticality levels in a resource-efficient manner. Although it has been well established (i) how to guarantee MC schedulability offline , existing studies have paid less attention to achieve (ii) how to minimize deadline misses of low-criticality tasks at runtime ; in addition, it has not matured yet how to address (ii) without compromising (i). In this paper, we propose $\mathsf{MC{-}FLEX}$ , which employs a task-level mode transition mechanism (as opposed to system-level one). $\mathsf{MC{-}FLEX}$ not only determines time instants at which each high-criticality task enters and exits the critical mode in a task level, but also selects time instants and target low-criticality task(s) to be dropped and resumed for each task-level mode switch of individual high-criticality tasks, yielding the achievement of both (i) and (ii). Via simulation results, we demonstrate that the proposed framework reduces the job deadline miss ratio of low-criticality tasks at runtime (by over 54.8\% compared to the existing work), without compromising offline MC schedulability.},
  archive      = {J_TC},
  author       = {Jaewoo Lee and Jinkyu Lee},
  doi          = {10.1109/TC.2021.3111743},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1889-1902},
  shortjournal = {IEEE Trans. Comput.},
  title        = {$\mathsf{MC{-}FLEX}$MC-FLEX: Flexible mixed-criticality real-time scheduling by task-level mode switch},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contention minimization in emerging SMART NoC via direct and
indirect routes. <em>TC</em>, <em>71</em>(8), 1874–1888. (<a
href="https://doi.org/10.1109/TC.2021.3111517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SMART (Single-cycle Multi-hop Asynchronous Repeated Traversal) Network-on-Chip (NoC), a recently proposed dynamically reconfigurable NoC, enables single-cycle long-distance communication by building single-bypass paths directly between distant communication pairs. However, such a single-cycle single-bypass path will be readily broken when contention occurs. Thus, packets will be buffered at intermediate routers with blocking latency from other contending packets, and extra router-stage latency to rebuild the remaining path when available, reducing the bypassing benefits that SMART NoC offers. In this article, we for the first time propose an effective contention-minimized routing algorithm to achieve maximal bypassing in SMART NoCs. Specifically, we identify two potential routes for packets: direct route , with which packets can reach the destination in a single bypass; and indirect route , with which packets can reach the destination in multiple bypasses via a (multiple) intermediate router(s). The novel feature of the proposed routing strategy is that, contrary to an intuitive approach, not the routes with minimal distance but the indirect routes via the arbitrary intermediate routers (even if they may be non-minimal) that avoid contentions yield the minimized end-to-end latency. Our new routing strategy can greatly enrich the path diversity, effectively minimize the conflicts between communication pairs, greatly balance the workloads and fully utilize bypass paths. Evaluation on realistic benchmarks demonstrates the effectiveness of the proposed routing strategy, which achieves average performance improvement by 35.48 percent in communication latency, 28.31 percent in application schedule length, and 37.59 percent in network throughput, compared with the current routing in SMART NoCs.},
  archive      = {J_TC},
  author       = {Peng Chen and Hui Chen and Jun Zhou and Mengquan Li and Weichen Liu and Chunhua Xiao and Yiyuan Xie and Nan Guan},
  doi          = {10.1109/TC.2021.3111517},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1874-1888},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Contention minimization in emerging SMART NoC via direct and indirect routes},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fluid scheduling algorithm for DAG tasks with constrained
or arbitrary deadlines. <em>TC</em>, <em>71</em>(8), 1860–1873. (<a
href="https://doi.org/10.1109/TC.2021.3111512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A number of scheduling algorithms have been proposed for real-time parallel tasks modeled as Directed Acyclic Graphs (DAGs). Many of them focus on scheduling DAG tasks with implicit deadlines. Fewer studies have considered DAG tasks with constrained deadlines or arbitrary deadlines. In this study, we propose a scheduling strategy based on fluid scheduling theory and we target DAG tasks with constrained or arbitrary deadlines. We prove that the proposed algorithm has a capacity augmentation bound of $\frac{1}{2}(1+\beta +\sqrt{(1+\beta)^2-\frac{4}{m}})$ when scheduling multiple DAG tasks with constrained deadlines, in which $m$ is the number of processors and $\beta$ is the maximum ratio of task period to deadline. This value is lower than the current best result $\beta +2\sqrt{(\beta +1-\frac{1}{m})(1-\frac{1}{m})}$ . We also prove that a capacity augmentation bound of $\frac{1}{2}(1+\sqrt{2}+\sqrt{(1+\sqrt{2})^2-\frac{4\sqrt{2}}{m}})$ is guaranteed by our algorithm in the case of scheduling multiple DAG tasks with deadlines greater than periods. To the best of our knowledge, this is the first capacity augmentation bound that has been proven for scheduling multiple DAG tasks with deadlines greater than periods. Our experiments show that our algorithm outperforms the state of the art scheduling algorithms in the percentage of schedulable task sets.},
  archive      = {J_TC},
  author       = {Fei Guan and Long Peng and Jiaqing Qiao},
  doi          = {10.1109/TC.2021.3111512},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1860-1873},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A fluid scheduling algorithm for DAG tasks with constrained or arbitrary deadlines},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OctCNN: A high throughput FPGA accelerator for CNNs using
octave convolution algorithm. <em>TC</em>, <em>71</em>(8), 1847–1859.
(<a href="https://doi.org/10.1109/TC.2021.3110413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development and continuous evolution of convolutional neural networks (CNNs), FPGAs have become one of the most attractive candidates for deploying CNNs due to their re-programmability, low power consumption, and fast time-to-market characteristics. However, as the network structure of models deepens, previous FPGA solutions based on the traditional convolution are still limited by computational power, making it challenging to meet feedforward performance requirements. In this article, we introduce the state-of-the-art octave convolution (OctConv) into the CNN accelerator design for the first time to improve the hardware acceleration efficiency and design a dedicated OctPU for mapping OctConv to FPGAs efficiently, which employs a parallel dataflow pattern to exploit the parallelism of OctConv sufficiently. Based on this, we present a novel and scalable architecture that dynamically combines the inter-layer pipelined structure and multi-layer reuse structure, achieving a compromise between specificity and scalability with limited resources. Meanwhile, to obtain the optimized solution from the complex design space search, we build a multidimensional performance and resource analysis model and a two-stage search algorithm based on greedy and heuristic algorithms. We evaluate our proposal by implementing VGG16 and ResNet50 on the Xilinx VU9P FPGA. Experimental results show that our prototype accelerators can achieve an average of 3321 GOP/s for the convolutional layers for VGG16 and 2873 GOP/s for the overall ResNet50 using OctConv. Compared to previous works based on the traditional convolution, our prototypes own a 1.72× to 2.33× speedup in throughput and a 2.01× to 5.18× improvement in computational density. Our design also presents an excellent compromise performance and generalization compared to previous hardware and software co-optimization works.},
  archive      = {J_TC},
  author       = {Wenqi Lou and Lei Gong and Chao Wang and Zidong Du and Xuehai Zhou},
  doi          = {10.1109/TC.2021.3110413},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1847-1859},
  shortjournal = {IEEE Trans. Comput.},
  title        = {OctCNN: A high throughput FPGA accelerator for CNNs using octave convolution algorithm},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Revisiting the optimization of cauchy reed-solomon coding
matrix for fault-tolerant data storage. <em>TC</em>, <em>71</em>(8),
1839–1846. (<a href="https://doi.org/10.1109/TC.2021.3110131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cauchy Reed-Solomon (CRS) codes are a class of erasure-resilient codes which are widely applicable in modern data storage systems. Several existing works have considered making the coding process of the extended CRS codes more efficient. It has been found that this efficiency is highly dependent on the underlying Cauchy coding matrices, particularly, the density of the associated bitmatrices. In this work, we revisit the problem of optimizing the coding bitmatrices, and propose three approaches aiming to find the bitmatrices with the lowest density in this context, namely a mixed integer linear programming approach, a local optimal algorithm with heuristic perturbation, and a branch-and-bound algorithm. Experimental results show that the proposed approaches are able to find bitmatrices with significantly lower density than those found using existing techniques. Moreover, the local optimal algorithm with heuristic perturbation is surprisingly efficient in finding good solutions under constrained computation time.},
  archive      = {J_TC},
  author       = {Mykyta Makovenko and Min Cheng and Chao Tian},
  doi          = {10.1109/TC.2021.3110131},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1839-1846},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Revisiting the optimization of cauchy reed-solomon coding matrix for fault-tolerant data storage},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PRIMER: Profiling interrupts using electromagnetic
side-channel for embedded devices. <em>TC</em>, <em>71</em>(8),
1824–1838. (<a href="https://doi.org/10.1109/TC.2021.3109457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent proliferation of CPS and IoT devices has led to an increasing demand for analyzing performance and timing of event-driven computational activity, especially interrupts and exceptions. However, these devices typically lack hardware resources, power, and system-software infrastructure for profiling/monitoring such events. Even when feasible, the profiling/monitoring activity itself can perturb the performance and timing of the timing-sensitive activity to be analyzed, therefore producing misleading results. Thus, we present Primer , a novel approach for profiling interrupts. Primer leverages existing unintentional (side-channel) electromagnetic emanations of the profiled/monitored device to identify its asynchronous execution (e.g., interrupt handlers). Primer leaves the monitored system (and its behavior) completely unchanged, requires no system resources or support, and introduces neither overheads nor perturbation in the monitored system. We validate Primer by analyzing signals that correspond to five different types of interrupts on an IoT device (ARM Cortex-M), achieving 99.5\% accuracy (with no false positives), and on an MSP430 microcontroller-based device with even better accuracy. We also demonstrate the effectiveness of Primer in analyzing page faults and network interrupts when executing real-world applications on a more sophisticated embedded device (ARM Cortex-A8), and show that the results provided by Primer can provide useful insights about an application&#39;s interaction with the system&#39;s virtual memory and network-oriented services.},
  archive      = {J_TC},
  author       = {Moumita Dey and Baki Berkay Yilmaz and Milos Prvulovic and Alenka Zajić},
  doi          = {10.1109/TC.2021.3109457},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1824-1838},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PRIMER: Profiling interrupts using electromagnetic side-channel for embedded devices},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient, flexible, and constant-time gaussian sampling
hardware for lattice cryptography. <em>TC</em>, <em>71</em>(8),
1810–1823. (<a href="https://doi.org/10.1109/TC.2021.3107729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a discrete Gaussian sampling hardware design that can flexibly support different sampling parameters, that is more efficient (in area-delay product) compared to the majority of earlier proposals, and that has constant execution time. The proposed design implements a Cumulative Distribution Table (CDT) approach, reduces the table size with Gaussian convolutions, and adopts an innovative fusion tree search algorithm to achieve a compact and fast sampling technique—to our best knowledge, this is the first hardware implementation of fusion tree search algorithm. The proposed hardware can support all the discrete Gaussian distributions used in post-quantum digital signatures and key encapsulation algorithms (FALCON, qTESLA, and FrodoKEM), the homomorphic encryption library of SEAL, and other algorithms such BLISS digital signature and LP public-key encryption. Our proposed hardware can be configured at design-time to optimize a single configuration or at run-time to support multiple Gaussian distribution parameters. Our design, furthermore, has constant-time behavior by design, eliminating timing side-channel attacks—this is achieved by reading all table contents at the same time to also reduce the latency. The results on a Xilinx Virtex-7 FPGA show that our solution can outperform all prior proposals in area-delay product by 1.67–235.88×, only falling short to those designed for the LP encryption scheme.},
  archive      = {J_TC},
  author       = {Emre Karabulut and Erdem Alkim and Aydin Aysu},
  doi          = {10.1109/TC.2021.3107729},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1810-1823},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient, flexible, and constant-time gaussian sampling hardware for lattice cryptography},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An open-source platform for high-performance non-coherent
on-chip communication. <em>TC</em>, <em>71</em>(8), 1794–1809. (<a
href="https://doi.org/10.1109/TC.2021.3107726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On-chip communication infrastructure is a central component of modern systems-on-chip (SoCs), and it continues to gain importance as the number of cores, the heterogeneity of components, and the on-chip and off-chip bandwidth continue to grow. Decades of research on on-chip networks enabled cache-coherent shared-memory multiprocessors. However, communication fabrics that meet the needs of heterogeneous many-cores and accelerator-rich SoCs, which are not, or only partially, coherent, are a much less mature research area. In this work, we present a modular, topology-agnostic, high-performance on-chip communication platform. The platform includes components to build and link subnetworks with customizable bandwidth and concurrency properties and adheres to a state-of-the-art, industry-standard protocol. We discuss microarchitectural trade-offs and timing/area characteristics of our modules and show that they can be composed to build high-bandwidth (e.g., 2.5 GHz and 1024 bit data width) end-to-end on-chip communication fabrics (not only network switches but also DMA engines and memory controllers) with high degrees of concurrency. We design and implement a state-of-the-art ML training accelerator, where our communication fabric scales to 1024 cores on a die, providing 32 TB/s cross-sectional bandwidth at only 24 ns round-trip latency between any two cores.},
  archive      = {J_TC},
  author       = {Andreas Kurth and Wolfgang Rönninger and Thomas Benz and Matheus Cavalcante and Fabian Schuiki and Florian Zaruba and Luca Benini},
  doi          = {10.1109/TC.2021.3107726},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1794-1809},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An open-source platform for high-performance non-coherent on-chip communication},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A data layout with good data locality for single-machine
based graph engines. <em>TC</em>, <em>71</em>(8), 1784–1793. (<a
href="https://doi.org/10.1109/TC.2021.3107725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph engines have been used in many applications to handle big graphs efficiently. The majority of the research to improve their performance has focused primarily on the design of efficient graph processing. This paper claims, however, the focus should be given also to graph storage design. This is because good storage design can improve both CPU performance and I/O performance of graph engines. In this paper, we propose an efficient data layout for single-machine based graph engines. We identify the common node access pattern of the graph algorithms running on single-machine based graph engines. Based on this finding, we propose the breadth-first (BF) data layout which places the nodes processed together in the same or adjacent storage space so that they can be accessed together as much as possible. The experimental results show that the BF data layout improves both CPU and I/O performances significantly in all single-machine based graph engines.},
  archive      = {J_TC},
  author       = {Yong-Yeon Jo and Myung-Hwan Jang and Sang-Wook Kim and Sunju Park},
  doi          = {10.1109/TC.2021.3107725},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1784-1793},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A data layout with good data locality for single-machine based graph engines},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time task scheduling for machine perception in
intelligent cyber-physical systems. <em>TC</em>, <em>71</em>(8),
1770–1783. (<a href="https://doi.org/10.1109/TC.2021.3106496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores criticality-based real-time scheduling of neural-network-based machine inference pipelines in cyber-physical systems (CPS) to mitigate the effect of algorithmic priority inversion. We specifically focus on the perception subsystem, an important subsystem feeding other components (e.g., planning and control). In general, priority inversion occurs in real-time systems when computations that are of lower priority are performed together with or ahead of those that are of higher priority. In current machine perception software, significant priority inversion occurs because resource allocation to the underlying neural network models does not differentiate between critical and less critical data within a scene. To remedy this problem, in recent work, we proposed an architecture to partition the input data into regions of different criticality, then formulated a utility-based optimization problem to batch and schedule their processing in a manner that maximizes confidence in perception results, subject to criticality-based time constraints. This journal extension matures the work in several directions: (i) We extend confidence maximization to a generalized utility optimization formulation that accounts for criticality in the utility function itself, offering finer-grained control over resource allocation within the perception pipeline; (ii) we further instantiate and compare two different criticality metrics (distance-based and relative velocity-based) to understand their relative advantages; and (iii) we explore the limitations of the approach, specifically how inaccuracies in criticality-based attention cueing affect performance. All experiments are conducted on the NVIDIA Jetson AGX Xavier platform with a real-world driving dataset.},
  archive      = {J_TC},
  author       = {Shengzhong Liu and Shuochao Yao and Xinzhe Fu and Huajie Shao and Rohan Tabish and Simon Yu and Ayoosh Bansal and Heechul Yun and Lui Sha and Tarek Abdelzaher},
  doi          = {10.1109/TC.2021.3106496},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1770-1783},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Real-time task scheduling for machine perception in intelligent cyber-physical systems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DVREI: Dynamic verifiable retrieval over encrypted images.
<em>TC</em>, <em>71</em>(8), 1755–1769. (<a
href="https://doi.org/10.1109/TC.2021.3106482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing awareness in privacy has partly contributed to the renewed interest in privacy-preserving encrypted image retrieval, and designing for outsourced images stored on cloud servers, etc. However, there are some limitations in these existing schemes such as low retrieval accuracy, low retrieval efficiency, and less efficient result verification in the dynamic setting. Therefore, in this paper we present a novel Dynamic Verifiable Retrieval over Encrypted Images (DVREI) scheme. First, a pre-trained Convolutional Neural Network (CNN) model is utilized to extract image features to improve retrieval accuracy. Then, an encrypted index based on the K-means clustering algorithm is designed to improve retrieval efficiency. Finally, a dynamic verification tree based on the chameleon hash is used to verify the correctness of the retrieval results and support dynamic updates. We theoretically and experimentally evaluate the security and performance of DVREI to demonstrate its practicability.},
  archive      = {J_TC},
  author       = {Yingying Li and Jianfeng Ma and Yinbin Miao and Huizhong Li and Qiang Yan and Yue Wang and Ximeng Liu and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TC.2021.3106482},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1755-1769},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DVREI: Dynamic verifiable retrieval over encrypted images},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A data layout and fast failure recovery scheme for
distributed storage systems with mixed erasure codes. <em>TC</em>,
<em>71</em>(8), 1740–1754. (<a
href="https://doi.org/10.1109/TC.2021.3105882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Erasure coding becomes increasingly popular in distributed storage systems (DSSes) for providing high reliability with low storage overhead. However, traditional random data placement induces massive cross-rack traffic and severely imbalanced load during failure recovery, which degrades the recovery performance significantly. In addition, various erasure codes coexisting in a DSS exacerbates the above problems. In this paper, we propose PDL, a PBD-based Data Layout, to optimize failure recovery performance in DSSes. PDL is constructed based on Pairwise Balanced Design, a combinatorial design scheme with uniform mathematical properties, and thus presents a uniform data layout for mixed erasure codes. Then we propose rPDL, a failure recovery scheme based on PDL. rPDL reduces cross-rack traffic effectively and provides nearly balanced cross-rack traffic distribution by uniformly choosing replacement nodes and retrieving determined available blocks to recover the lost blocks. We implemented PDL and rPDL in Hadoop 3.1.1. Compared with the existing data layout and recovery scheme in HDFS, experimental results show that rPDL achieves much higher recovery throughput, $6.27\times$ on average for single-node failures, $5.14\times$ for multi-node failures and $1.48\times$ for single-rack failures, respectively. It also reduces degraded read latency by an average of 62.83 percent, and provides better support to front-end applications in case of component failures.},
  archive      = {J_TC},
  author       = {Liangliang Xu and Min Lyu and Zhipeng Li and Cheng Li and Yinlong Xu},
  doi          = {10.1109/TC.2021.3105882},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1740-1754},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A data layout and fast failure recovery scheme for distributed storage systems with mixed erasure codes},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). K-level truthful incentivizing mechanism and generalized
k-MAB problem. <em>TC</em>, <em>71</em>(7), 1724–1739. (<a
href="https://doi.org/10.1109/TC.2021.3105831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-armed bandits problem has been widely utilized in economy-related areas. Incentives are explored in the sharing economy to inspire users for better resource allocation. Previous works build a budget-feasible incentive mechanism to learn users’ cost distribution. However, they only consider a special case that all tasks are considered as the same. The general problem asks for finding a solution when the cost for different tasks varies. In this paper, we investigate this problem by considering a system with $k$ levels of difficulty. We present two incentivizing strategies for offline and online implementation, and formally derive the ratio of utility between them in different scenarios. We propose a regret-minimizing mechanism to decide incentives by dynamically adjusting budget assignment and learning from users’ cost distributions. We further extend the problem to a more generalized k-MAB problem by removing the contextual information of difficulties. CUE-UCB algorithm is proposed to address the online advertisement problem for multi-platforms. Our experiment demonstrates utility improvement about 7 times and time saving of 54\% to meet a utility objective compared to the previous works in sharing economy, and up to 175\% increment of utility for online advertising.},
  archive      = {J_TC},
  author       = {Pengzhan Zhou and Xin Wei and Cong Wang and Yuanyuan Yang},
  doi          = {10.1109/TC.2021.3105831},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1724-1739},
  shortjournal = {IEEE Trans. Comput.},
  title        = {K-level truthful incentivizing mechanism and generalized k-MAB problem},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy-efficient stream compaction through filtering and
coalescing accesses in GPGPU memory partitions. <em>TC</em>,
<em>71</em>(7), 1711–1723. (<a
href="https://doi.org/10.1109/TC.2021.3104749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based applications are essential in emerging domains such as data analytics or machine learning. Data gathering in a knowledge-based society requires great data processing efficiency. High-throughput GPGPU architectures are key to enable efficient graph processing. Nonetheless, irregular and sparse memory access patterns present in graph-based applications induce high memory divergence and contention, which result in poor GPGPU efficiency for graph processing. Recent work has pointed out the importance of stream compaction operations, and has proposed a Stream Compaction Unit (SCU) to offload them to a specialized hardware. On the other hand, memory contention caused by high divergence has been tackled with the Irregular accesses Reorder Unit (IRU), delivering improved memory coalescing. In this paper, we propose a new unit, the IRU-enhanced SCU (ISCU), that leverages the strengths of both approaches. The ISCU employs the efficient mechanisms of the IRU to improve SCU stream compaction efficiency and throughput limitations, achieving a synergistic effect for graph processing. We evaluate the ISCU for a wide variety of state-of-the-art graph-based algorithms and applications. Results show that the ISCU achieves a performance speedup of 2.2x and 90 percent energy savings derived from a high reduction of 78 percent memory accesses, while incurring in 8.5 percent area overhead.},
  archive      = {J_TC},
  author       = {Albert Segura and Jose-Maria Arnau and Antonio González},
  doi          = {10.1109/TC.2021.3104749},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1711-1723},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Energy-efficient stream compaction through filtering and coalescing accesses in GPGPU memory partitions},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Resilient scheduling of moldable parallel jobs to cope with
silent errors. <em>TC</em>, <em>71</em>(7), 1696–1710. (<a
href="https://doi.org/10.1109/TC.2021.3104747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the resilient scheduling of moldable parallel jobs on high-performance computing (HPC) platforms. Moldable jobs allow for choosing a processor allocation before execution, and their execution time obeys various speedup models. The objective is to minimize the overall completion time or the makespan, when jobs can fail due to silent errors and hence may need to be re-executed after each failure until successful completion. Our work generalizes the classical scheduling framework for failure-free jobs. To cope with silent errors, we introduce two resilient scheduling algorithms, Lpa-List and Batch-List , both of which use the List strategy to schedule the jobs. Without knowing a priori how many times each job will fail, Lpa-List relies on a local strategy to allocate processors to the jobs, while Batch-List schedules the jobs in batches and allows only a restricted number of failures per job in each batch. We prove approximation ratios for the two algorithms under several prominent speedup models (e.g., roofline, communication, Amdahl, power, monotonic, and a mix model). An extensive set of simulations is conducted to evaluate different variants of the two algorithms, and the results show that they consistently outperform some baseline heuristics. Overall, our best algorithm is within a factor of 1.6 of a lower bound on average over the entire set of experiments, and within a factor of 4.2 in the worst case.},
  archive      = {J_TC},
  author       = {Anne Benoit and Valentin Le Fèvre and Lucas Perotin and Padma Raghavan and Yves Robert and Hongyang Sun},
  doi          = {10.1109/TC.2021.3104747},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1696-1710},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Resilient scheduling of moldable parallel jobs to cope with silent errors},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FeFET-based binarized neural networks under
temperature-dependent bit errors. <em>TC</em>, <em>71</em>(7),
1681–1695. (<a href="https://doi.org/10.1109/TC.2021.3104736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ferroelectric FET (FeFET) is a highly promising emerging non-volatile memory (NVM) technology, especially for binarized neural network (BNN) inference on the low-power edge. The reliability of such devices, however, inherently depends on temperature. Hence, changes in temperature during run time manifest themselves as changes in bit error rates. In this work, we reveal the temperature-dependent bit error model of FeFET memories, evaluate its effect on BNN accuracy, and propose countermeasures. We begin on the transistor level and accurately model the impact of temperature on bit error rates of FeFET. This analysis reveals temperature-dependent asymmetric bit error rates. Afterwards, on the application level, we evaluate the impact of the temperature-dependent bit errors on the accuracy of BNNs. Under such bit errors, the BNN accuracy drops to unacceptable levels when no countermeasures are employed. We propose two countermeasures: (1) Training BNNs for bit error tolerance by injecting bit flips into the BNN data, and (2) applying a bit error rate assignment algorithm (BERA) which operates in a layer-wise manner and does not inject bit flips during training. In experiments, the BNNs, to which the countermeasures are applied to, effectively tolerate temperature-dependent bit errors for the entire range of operating temperature.},
  archive      = {J_TC},
  author       = {Mikail Yayla and Sebastian Buschjäger and Aniket Gupta and Jian-Jia Chen and Jörg Henkel and Katharina Morik and Kuan-Hsun Chen and Hussam Amrouch},
  doi          = {10.1109/TC.2021.3104736},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1681-1695},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FeFET-based binarized neural networks under temperature-dependent bit errors},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High fidelity simulation of hybrid systems using higher
order hybrid automata. <em>TC</em>, <em>71</em>(7), 1668–1680. (<a
href="https://doi.org/10.1109/TC.2021.3100746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid systems are a subset of Cyber-Physical System (CPS), where a physical process (the plant) is controlled by a discrete controller. The controller induces mode switches, which are modelled as guard conditions leading to sudden discontinuities. Correctly capturing sudden discontinuities during simulation is the primary challenge to maintain fidelity. De-facto industry standard tools, such as Simulink and Modelica, have been known to produce incorrect outputs when simulating systems involving complicated guards. For example, transcendental guards leading to the well-known even number of level crossing detection problem or guards leading the system state into the complex plane have been shown to produce invalid results. To tackle this problem we propose Higher Order Hybrid Automata and its compositional execution semantics. Using this semantics a novel numerical simulation approach for hybrid systems is developed. The key idea is to approximate the guard and Ordinary Differential Equations with Taylor polynomials so as to accurately detect zero-crossings induced by the guards. Simulation results show that systems with transcendental guards can be simulated using our approach efficiently, while maintaining high simulation fidelity.},
  archive      = {J_TC},
  author       = {Jin Woo Ro and Avinash Malik and Partha Roop},
  doi          = {10.1109/TC.2021.3100746},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1668-1680},
  shortjournal = {IEEE Trans. Comput.},
  title        = {High fidelity simulation of hybrid systems using higher order hybrid automata},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive federated learning on non-IID data with resource
constraint. <em>TC</em>, <em>71</em>(7), 1655–1667. (<a
href="https://doi.org/10.1109/TC.2021.3099723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has been widely recognized as a promising approach by enabling individual end-devices to cooperatively train a global model without exposing their own data. One of the key challenges in FL is the non-independent and identically distributed (Non-IID) data across the clients, which decreases the efficiency of stochastic gradient descent (SGD) based training process. Moreover, clients with different data distributions may cause bias to the global model update, resulting in a degraded model accuracy. To tackle the Non-IID problem in FL, we aim to optimize the local training process and global aggregation simultaneously. For local training, we analyze the effect of hyperparameters (e.g., the batch size, the number of local updates) on the training performance of FL. Guided by the toy example and theoretical analysis, we are motivated to mitigate the negative impacts incurred by Non-IID data via selecting a subset of participants and adaptively adjust their batch size. A deep reinforcement learning based approach has been proposed to adaptively control the training of local models and the phase of global aggregation. Extensive experiments on different datasets show that our method can improve the model accuracy by up to 30 percent, as compared to the state-of-the-art approaches.},
  archive      = {J_TC},
  author       = {Jie Zhang and Song Guo and Zhihao Qu and Deze Zeng and Yufeng Zhan and Qifeng Liu and Rajendra Akerkar},
  doi          = {10.1109/TC.2021.3099723},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1655-1667},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Adaptive federated learning on non-IID data with resource constraint},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Remap-based inter-partition copy for arrayed solid-state
drives. <em>TC</em>, <em>71</em>(7), 1640–1654. (<a
href="https://doi.org/10.1109/TC.2021.3099694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The internal copy (IC) is a simple, yet powerful in-storage processing function, which changes the locations of data blocks without invoking any data transfer between the host and storage. Owing to the out-of-place update constraint of flash memory, solid-state disks (SSDs) employ a flash translation layer (FTL) to manage the logical-to-physical address translation. By leveraging the address indirection feature of SSDs, the IC can be processed only by remapping flash pages to new logical addresses without flash read/write operations. In the existing studies on remap-based IC, SSDs were assumed to have only a single FTL instance. However, recent large-capacity SSDs adopt an arrayed architecture including multiple FTL controllers, where each controller runs an FTL instance to manage its own partitioned address space. For the arrayed SSDs, inter-partition copy requests cannot be handled by address remapping because each partition is managed by a different FTL instance. In this study, we propose an inter-partition remap technique for IC-enabled arrayed SSDs. Additionally, we present the block allocation technique to minimize the number of inter-partition copy requests. Our proposed IC techniques were implemented on an actual arrayed SSD, and showed significant performance improvements compared to the previous remap techniques in several use cases.},
  archive      = {J_TC},
  author       = {Kyuhwa Han and Dongkun Shin},
  doi          = {10.1109/TC.2021.3099694},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1640-1654},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Remap-based inter-partition copy for arrayed solid-state drives},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CAP: Communication-aware automated parallelization for deep
learning inference on CMP architectures. <em>TC</em>, <em>71</em>(7),
1626–1639. (<a href="https://doi.org/10.1109/TC.2021.3099688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time inference of deep learning models on embedded and energy-efficient devices becomes increasingly desirable with the rapid growth of artificial intelligence on edge. Specifically, to achieve superb energy-efficiency and scalability, efficient parallelization of single-pass deep neural network (DNN) inference on chip multiprocessor (CMP) architectures is urgently required by many time-sensitive applications. However, as the number of processing cores scales up and the performance of cores has grown much fast, the on-chip inter-core data movement is prone to be a performance bottleneck for computation. To remedy this problem and further improve the performance of network inference, in this work, we introduce a communication-aware DNN parallelization technique called CAP, by exploiting the elasticity and noise-tolerance of deep learning algorithms on CMP. Moreover, in the hope that the conducted studies can provide new design values for real-time neural network inference on embedded chips, we also have evaluated the proposed approach on both multi-core Neural Network Accelerators (NNA) chips and general-purpose chip-multiprocessors. Our experimental results show that the proposed CAP can achieve 1.12×-1.65× system speedups and 1.14×-2.70× energy efficiency for different neural networks while maintaining the inference accuracy, compared to baseline approaches.},
  archive      = {J_TC},
  author       = {Kaiwei Zou and Ying Wang and Long Cheng and Songyun Qu and Huawei Li and Xiaowei Li},
  doi          = {10.1109/TC.2021.3099688},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1626-1639},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CAP: Communication-aware automated parallelization for deep learning inference on CMP architectures},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A structure-aware storage optimization for out-of-core
concurrent graph processing. <em>TC</em>, <em>71</em>(7), 1612–1625. (<a
href="https://doi.org/10.1109/TC.2021.3098976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the huge demand for graph analytics in many real-world applications, massive iterative graph processing jobs are concurrently performed on the same graphs and suffer from significant high data access cost. To lower the data access cost toward high performance, several out-of-core concurrent graph processing solutions are recently designed to handle concurrent jobs by enabling these jobs to share the accesses of the same graph data. However, the set of active vertices in each partition are usually different for various concurrent jobs and also evolve with time, where some high-degree ones (or called hub-vertices ) of these active vertices require more iterations to converge due to the power-law property of real-world graphs. In consequence, existing solutions still suffer from much unnecessary I/O traffic, because they have to entirely load each partition into the memory for concurrent jobs even if most vertices in this partition are inactive and may be shared by a few jobs. In this paper, we propose an efficient structure-aware storage system, called GraphSO, for higher throughput of the execution of concurrent graph processing jobs. It can be integrated into existing out-of-core graph processing systems to promote the execution efficiency of concurrent jobs with lower I/O overhead. The key design of GraphSO is a fine-grained storage management scheme. Specifically, it logically divides the partitions of existing graph processing systems into a series of small same-sized chunks. At runtime, these small chunks with active vertices are judiciously loaded by GraphSO to construct new logical partitions (i.e., each logical partition is a subset of active chunks) for existing graph processing systems to handle, where the most-frequently-used chunks are preferentially loaded to construct the logical partitions and the other ones are delayed to wait to be required by more jobs. In this way, it can effectively spare the cost of loading the graph data associated with the inactive vertices with low repartitioning overhead and can also enable the loaded graph data to be fully shared by concurrent jobs. Moreover, GraphSO also designs a buffering strategy to efficiently cache the most-frequently-used chunks in the main memory to further minimize the I/O traffic by avoiding repeated load of them. Experimental results show that GraphSO improves the throughput of GridGraph, GraphChi, X-Stream, DynamicShards, LUMOS, Graphene, and Wonderland by 1.4-3.5 times, 2.1-4.3 times, 1.9-4.1 times, 1.9-2.9 times, 1.5-3.1 times, 1.3-1.5 times, and 1.3-2.7 times after integrating with them, respectively.},
  archive      = {J_TC},
  author       = {Xiaofei Liao and Jin Zhao and Yu Zhang and Bingsheng He and Ligang He and Hai Jin and Lin Gu},
  doi          = {10.1109/TC.2021.3098976},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1612-1625},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A structure-aware storage optimization for out-of-core concurrent graph processing},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). KVSTL: An application support to LSM-tree based key-value
store via shingled translation layer data management. <em>TC</em>,
<em>71</em>(7), 1598–1611. (<a
href="https://doi.org/10.1109/TC.2021.3098961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LSM-tree based Key-value (KV) stores greatly fit the needs of write-intensive applications with its efficient data store and retrieval operations to datasets. To accommodate ever-growing datasets, shingled magnetic recording (SMR) drives have become a popular option to provide large storage capacity for KV stores at low cost. SMR drives achieve high storage density via overlapping tracks on the disk surface. However, the overlapped track layout induces the sequential-write constraint and prevents KV stores from storing and rearranging KV pairs efficiently. In this paper, we present KVSTL, a KV store aware Shingled Translation Layer (STL), to preserve the merits of existing KV stores, while exploiting the high storage density of SMR drives. KVSTL is proposed as an application support to hide the management complexity of SMR drives and facilitates the management of SMR drives via passing only the “level” and “invalidation” information of LSM-tree based KV stores onto SMR drives. The proposed KVSTL achieves its performance enhancement via managing key-value pairs with level awareness and enabling efficient storage space management with the invalidation information. The results show that KVSTL can reduce the written data amount for 69.45 percent on average and the latency for up to 62.72 percent when compared with SMR-based LevelDB.},
  archive      = {J_TC},
  author       = {Shuo-Han Chen and Yuhong Liang and Ming-Chang Yang},
  doi          = {10.1109/TC.2021.3098961},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1598-1611},
  shortjournal = {IEEE Trans. Comput.},
  title        = {KVSTL: An application support to LSM-tree based key-value store via shingled translation layer data management},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On a consistency testing model and strategy for revealing
RISC processor’s dark instructions and vulnerabilities. <em>TC</em>,
<em>71</em>(7), 1586–1597. (<a
href="https://doi.org/10.1109/TC.2021.3097174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One major security vulnerability of a microprocessor can be attributed to its underlying instruction set architecture (ISA). Generally, it is required that no secret instructions be included in the ISA or implemented in the processor micro-architecture. Such a requirement is particularly important for the reduced instruction set computing (RISC) processors that are widely used nowadays, and applying the proposed consistency testing approach is poised to ensure this requirement is met. Capable of revealing any possible dark instructions (i.e., executable instructions but without clear definitions of their behavior) in RISC processors, a consistency test comes in three phases. During the generation phase, based on the instruction set encoding rules, all the undefined instructions are generated. Even with a smaller test space, this step guarantees the test coverage needed to reveal all the dark instructions that may exist. In the next phase, all the undefined instructions obtained from the previous phase are executed on the processor under test, following a set of persistence strategies; any instruction exhibiting unusual execution result will be deemed suspicious and recorded so. During the last analysis phase, each of those recorded suspicious instructions will be checked and analyzed to decide whether it truly constitutes a dark instruction. We have applied the proposed testing model and strategy to several RISC processors and found that all of them have a few dark instructions previously unknown. The potential vulnerabilities of these processors introduced by their respective dark instructions have thus been evaluated and exposed.},
  archive      = {J_TC},
  author       = {Yuze Wang and Peng Liu and Weidong Wang and Xiaohang Wang and Yingtao Jiang},
  doi          = {10.1109/TC.2021.3097174},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1586-1597},
  shortjournal = {IEEE Trans. Comput.},
  title        = {On a consistency testing model and strategy for revealing RISC processor’s dark instructions and vulnerabilities},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast generation of RSA keys using smooth integers.
<em>TC</em>, <em>71</em>(7), 1575–1585. (<a
href="https://doi.org/10.1109/TC.2021.3095669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Primality generation is the cornerstone of several essential cryptographic systems. The problem has been a subject of deep investigations, but there is still a substantial room for improvements. Typically, the algorithms used have two parts – trial divisions aimed at eliminating numbers with small prime factors and primality tests based on an easy-to-compute statement that is valid for primes and invalid for composites. In this paper, we will showcase a technique that will eliminate the first phase of the primality testing algorithms. The computational simulations show a reduction of the primality generation time by about 30 percent in the case of 1024-bit RSA key pairs. This can be particularly beneficial in the case of decentralized environments for shared RSA keys as the initial trial division part of the key generation algorithms can be avoided at no cost. This also significantly reduces the communication complexity. Another essential contribution of the paper is the introduction of a new one-way function that is computationally simpler than the existing ones used in public-key cryptography. This function can be used to create new random number generators, and it also could be potentially used for designing entirely new public-key encryption systems.},
  archive      = {J_TC},
  author       = {Vassil Dimitrov and Luigi Vigneri and Vidal Attias},
  doi          = {10.1109/TC.2021.3095669},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1575-1585},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Fast generation of RSA keys using smooth integers},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). STT-MRAM-based reliable weak PUF. <em>TC</em>,
<em>71</em>(7), 1564–1574. (<a
href="https://doi.org/10.1109/TC.2021.3095657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, micro-nano device characteristics like ferroelectrics and resistive switching are being used to build important security primitives such as Physical Unclonable Function (PUF). The micro-nano device-based hardware security primitives, although with higher security, energy efficiency, and integration density, suffer from serious reliability issues caused by process scaling. To mitigate this issue, this paper introduces a reconfigurable weak PUF based on spin-transfer torque magnetoresistive random-access memory (STT-MRAM), which adopts the crossing switches implemented with simple demultiplexes (DEMUXs) to improve the flexibility and reliability. Moreover, two algorithms, neighboring bit lines and top-$n$n , are proposed to enlarge the gap between two parallel reading currents, thus further enhancing the reliability of PUF responses. Experimental results demonstrate that the proposed PUF scheme achieves good uniqueness (50.64 percent), uniformity (50.02 percent), and bit-aliasing ( $\approx$ 49.80\%). Particularly, the proposed method significantly improves the PUF reliability, achieving low bit error rate (BER $\leq$ 2.13\%) within the range of -20 $^\circ$ C to 90 $^\circ$ C.},
  archive      = {J_TC},
  author       = {Yupeng Hu and Linjun Wu and Zhuojun Chen and Yun Huang and Xiaolin Xu and Keqin Li and Jiliang Zhang},
  doi          = {10.1109/TC.2021.3095657},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1564-1574},
  shortjournal = {IEEE Trans. Comput.},
  title        = {STT-MRAM-based reliable weak PUF},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Precise dynamic symbolic execution for nonuniform data
access in smart contracts. <em>TC</em>, <em>71</em>(7), 1551–1563. (<a
href="https://doi.org/10.1109/TC.2021.3092639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic symbolic execution (DSE) has been successfully adopted for vulnerability detection in desktop and mobile platforms. Unfortunately, we cannot simply extrapolate those techniques to smart contracts. The major challenge is that smart contracts exhibit a nonuniform data access mode. Other than accessing the data via uniform addresses, smart contracts compromise multiple addressing modes, including flat address mode and key-value mode. More seriously, accessing a key-value table usually involves additional hash operations to obtain the keys. In this paper, we propose a DSE framework to resolve the nonuniform data access in smart contracts. More specifically, we exactly track the symbolic variables with concrete addresses and compute the actual/hash keys for table-like accesses. We also take the symbolic keys into account to distinguish data accesses incidentally with the same concrete keys resulting from artificially generated values. We describe the DSE framework in operational semantics. On top of the framework, we implement an integer overflow detector Nova and a multi-transactional vulnerability detector Mtvd . The experiments show that Nova outperforms state-of-the-art analysis tools in detecting the integer overflows with much higher precision and recall, 94.2 and 93.0 percent, respectively. Mtvd successfully reports three ether leaking vulnerabilities and one suicidal issue from real-world smart contracts.},
  archive      = {J_TC},
  author       = {Jianjun Huang and Jiasheng Jiang and Wei You and Bin Liang},
  doi          = {10.1109/TC.2021.3092639},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1551-1563},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Precise dynamic symbolic execution for nonuniform data access in smart contracts},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ComPreEND: Computation pruning through predictive early
negative detection for ReLU in a deep neural network accelerator.
<em>TC</em>, <em>71</em>(7), 1537–1550. (<a
href="https://doi.org/10.1109/TC.2021.3092205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A vast amount of activation values of DNNs are zeros due to ReLU (Rectified Linear Unit), which is one of the most common activation functions used in modern neural networks. Since ReLU outputs zero for all negative inputs, the inputs to ReLU do not need to be determined exactly as long as they are negative. However, many accelerators usually do not consider such aspects of DNNs, losing a huge amount of opportunities for speedups and energy savings. To exploit such opportunities, we propose early negative detection (END), a computation pruning technique that detects the negative results at an early stage. The key to the early negative detection is the adoption of inverted two&#39;s complement representation for filter parameters. This ensures that as soon as the intermediate results become negative, the final results are guaranteed to be negative. Upon detection, the remaining computation can be skipped and the following ReLU output can be simply set to zero. We also propose a DNN accelerator architecture (ComPreEND) that takes advantage of such skipping. ComPreEND with END significantly improves both the energy efficiency and the performance according to the evaluation. Compared to the baseline, we obtain 20.5 and 29.3 percent speedup with accurate mode and predictive mode, and energy savings by 28.4 and 41.4 percent, respectively.},
  archive      = {J_TC},
  author       = {Namhyung Kim and Hanmin Park and Dongwoo Lee and Sungbum Kang and Jinho Lee and Kiyoung Choi},
  doi          = {10.1109/TC.2021.3092205},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1537-1550},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ComPreEND: Computation pruning through predictive early negative detection for ReLU in a deep neural network accelerator},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MGARD+: Optimizing multilevel methods for error-bounded
scientific data reduction. <em>TC</em>, <em>71</em>(7), 1522–1536. (<a
href="https://doi.org/10.1109/TC.2021.3092201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, data reduction is becoming increasingly important in dealing with the large amounts of scientific data. Existing multilevel compression algorithms offer a promising way to manage scientific data at scale, but may suffer from relatively low performance and reduction quality. In this paper, we propose MGARD+, a multilevel data reduction and refactoring framework drawing on previous multilevel methods, to achieve high-performance data decomposition and high-quality error-bounded lossy compression. Our contributions are four-fold: 1) We propose to leverage a level-wise coefficient quantization method, which uses different error tolerances to quantize the multilevel coefficients. 2) We propose an adaptive decomposition method which treats the multilevel decomposition as a preconditioner and terminates the decomposition process at an appropriate level. 3) We leverage a set of algorithmic optimization strategies to significantly improve the performance of multilevel decomposition/recomposition. 4) We evaluate our proposed method using four real-world scientific datasets and compare with several state-of-the-art lossy compressors. Experiments demonstrate that our optimizations improve the decomposition/recomposition performance of the existing multilevel method by up to $70 \times$ , and the proposed compression method can improve compression ratio by up to $2 \times$ compared with other state-of-the-art error-bounded lossy compressors under the same level of data distortion.},
  archive      = {J_TC},
  author       = {Xin Liang and Ben Whitney and Jieyang Chen and Lipeng Wan and Qing Liu and Dingwen Tao and James Kress and David Pugmire and Matthew Wolf and Norbert Podhorszki and Scott Klasky},
  doi          = {10.1109/TC.2021.3092201},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1522-1536},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MGARD+: Optimizing multilevel methods for error-bounded scientific data reduction},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High throughput hardware/software heterogeneous system for
RRPN-based scene text detection. <em>TC</em>, <em>71</em>(7), 1507–1521.
(<a href="https://doi.org/10.1109/TC.2021.3092195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rotation Region Proposal Networks (RRPN) are used to generate rotated proposals with the information of text angle for arbitrary oriented scene text detection (STD). However, the computational complexity of RRPN inference is relatively high compared with other methods, which makes it difficult for massive deployment. In this paper, the first full-stack FPGA-CPU heterogeneous system design of RRPN-based STD algorithm is proposed. A hardware/software partition method is presented to analyze and split the tasks to enhance the computation efficiency of hardware. The fast 2D Winograd algorithm and block floating point are utilized to reduce computation complexity while maintaining a relatively high precision. The implementation results show that the peak performance of MAC arrays in the proposed architecture reaches 655.4 GOPS and the energy efficiency achieves 64.9 GOPS/W. By fully exploiting the parallel and pipelined merits in the algorithms, the first hardware architectures for skew non-maximum suppression (S-NMS) layer and rotation region-of-interest (RRoI) polling layer are proposed. The throughput of the proposed hardware/software heterogeneous system achieves 40 times and 1.4 times improvements compared with CPU and GPU, respectively. Moreover, the comprehensive operating expense ratio of pure CPU, GPU, and the proposed system is 80.7:2.5:1, which indicates that it is suitable for massive deployment.},
  archive      = {J_TC},
  author       = {Yao Xin and Donglong Chen and Chongyang Zeng and Weichen Zhang and Yi Wang and Ray C. C. Cheung},
  doi          = {10.1109/TC.2021.3092195},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1507-1521},
  shortjournal = {IEEE Trans. Comput.},
  title        = {High throughput Hardware/Software heterogeneous system for RRPN-based scene text detection},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving interference analysis for real-time DAG tasks
under partitioned scheduling. <em>TC</em>, <em>71</em>(7), 1495–1506.
(<a href="https://doi.org/10.1109/TC.2021.3092181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time systems with strict timing constraints have been widely applied in many fields. The Directed acyclic graph (DAG) task model has been widely studied and applied to model real-time systems with partial parallelism and precedence constraints in each task. Our paper focuses on the worst-case response time (WCRT) analysis of DAG tasks under partitioned scheduling on multiprocessors. We investigate a parallel structure named $Str$ , which helps obtain more accurate analysis results, and propose a new offline scheduling analysis algorithm named reducing repetitive calculation (RRC). Experiments with synthetic workload are conducted to compare the results calculated by RRC and the state-of-the-art, as well as the observed average response time on a real embedded system. Results show that RRC has better performance in terms of analysis accuracy.},
  archive      = {J_TC},
  author       = {Yulong Wu and Weizhe Zhang and Nan Guan and Yue Tang},
  doi          = {10.1109/TC.2021.3092181},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1495-1506},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Improving interference analysis for real-time DAG tasks under partitioned scheduling},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enabling efficient random data insertion/deletion on
block-based file systems. <em>TC</em>, <em>71</em>(6), 1479–1494. (<a
href="https://doi.org/10.1109/TC.2021.3092178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The file model based on character streams is widely used by most file systems today. However, such a model is originally designed for the storage media that are friendly for sequential accesses, such as mechanical hard disk. Although data can be updated (/overwritten) in the midway of files or appended to the end of files, they cannot be efficiently inserted or deleted. In particular, the random insertion/deletion of data in the midway of a file in a block-based file system often results in the updating of all subsequent chunks of the file, which seriously amplifies the write traffic and degrades the performance of the file system. In some application scenarios such as virtualized computing platforms with potentially very large files, the performance overheads due to the write amplification of a random insert/delete request might be ridiculously high. The observation therefore motivates this work in proposing the data compaction strategies to manage the data and storage space of each file, so as to enhance the performance of random insert/delete requests and the space utilization of file systems. The proposed data compaction strategies effectively relax the limitation of the space utilization guarantees provided by existing data/space management schemes, such as the buddy memory allocator. Besides, we propose a chunk anonymization scheme which can be applied to existing search trees to efficiently manage the indexing information of the chunks of a file. When working on a self-balancing search tree such as red–black tree, the chunk anonymization scheme can efficiently locate the chunk that contains the data of a given logical offset within the file in logarithmic time with respect to the file size. Moreover, a search tree with the chunk anonymization scheme can be efficiently maintained in logarithmic time when serving a random insert or delete request. Evaluation results show that the proposed data compaction strategies and chunk anonymization scheme can balance space utilization guarantees and random insert/delete performance.},
  archive      = {J_TC},
  author       = {Yi-Han Lien and Yi-Hua Chen and Po-Chun Huang},
  doi          = {10.1109/TC.2021.3092178},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1479-1494},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling efficient random data Insertion/Deletion on block-based file systems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel CONV acceleration strategy based on logical PE set
segmentation for row stationary dataflow. <em>TC</em>, <em>71</em>(6),
1466–1478. (<a href="https://doi.org/10.1109/TC.2021.3089366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (DCNNs) have been proposed as enhanced developments of neural networks (NNs) in the field of artificial intelligence (AI) and successfully applied in deep learning (DL) scenarios. With the advancement of technology, the number of network layers has continuously increased, resulting in a huge number of calculations and memory accesses required in the training and inference process of DCNNs and thereby hindering their further deployment and application. Using a specific dataflow formed by reusable DCNN data in the network-on-chip (NoC), reducing the memory access pressure and improving DCNN processing efficiency has become a promising acceleration scheme for the current DCNN. In this article, a novel convolution layer (CONV) acceleration strategy based on logical PE set segmentation for row stationary (RS) dataflow is proposed to solve the problems of low flexibility and inefficient processing array utilization faced by the conventional folding mapping strategy. The simulation results show that the new mapping strategy based on PE set segmentation can achieve better processing element utilization and CONV acceleration improvement at the expense of little increase in the data movement energy consumption compared with the conventional strategy.},
  archive      = {J_TC},
  author       = {Bowen Zhang and Huaxi Gu and Kun Wang and Yintang Yang},
  doi          = {10.1109/TC.2021.3089366},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1466-1478},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A novel CONV acceleration strategy based on logical PE set segmentation for row stationary dataflow},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalability in computing and robotics. <em>TC</em>,
<em>71</em>(6), 1453–1465. (<a
href="https://doi.org/10.1109/TC.2021.3089044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient engineered systems require scalability. A scalable system has increasing performance with increasing system size. In an ideal situation, the increase in performance (e.g., speedup) corresponds to the number of units (e.g., processors, robots, users) that are added to the system (e.g., three times the number of processors in a computer would lead to three times faster computations). However, if multiple units work on the same task, then coordination among these units is required. This coordination can introduce overheads with an impact on system performance. The coordination costs can lead to sublinear improvement or even diminishing performance with increasing system size. However, there are also systems that implement efficient coordination and exploit collaboration of units to attain superlinear improvement. Modeling the scalability dynamics is key to understanding and engineering efficient systems. Known laws of scalability, such as Amdahl’s law, Gustafson’s law, and Gunther’s Universal Scalability Law, are minimalistic phenomenological models that explain a rich variety of system behaviors through concise equations. While useful to gain general insights, the phenomenological nature of these models may limit the understanding of the underlying dynamics, as they are detached from first principles that could explain coordination overheads or synergies among units. Through a decentralized system approach, we propose a general model based on generic interactions between units that is able to describe, as specific cases, any general pattern of scalability included by previously reported laws. The proposed general model of scalability has the advantage of being built on first principles, or at least on a microscopic description of interaction between units, and therefore has the potential to contribute to a better understanding of system behavior and scalability. We show that this generic model can be applied to a diverse set of systems, such as parallel supercomputers, robot swarms, or wireless sensor networks, therefore creating a unified view on interdisciplinary design for scalability.},
  archive      = {J_TC},
  author       = {Heiko Hamann and Andreagiovanni Reina},
  doi          = {10.1109/TC.2021.3089044},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1453-1465},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Scalability in computing and robotics},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). S2 engine: A novel systolic architecture for sparse
convolutional neural networks. <em>TC</em>, <em>71</em>(6), 1440–1452.
(<a href="https://doi.org/10.1109/TC.2021.3087946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have achieved great success in performing cognitive tasks. However, execution of CNNs requires a large amount of computing resources and generates heavy memory traffic, which imposes a severe challenge on computing system design. Through optimizing parallel executions and data reuse in convolution, systolic architecture demonstrates great advantages in accelerating CNN computations. However, regular internal data transmission path in traditional systolic architecture prevents the systolic architecture from completely leveraging the benefits introduced by neural network sparsity. Deployment of fine-grained sparsity on the existing systolic architectures is greatly hindered by the incurred computational overheads. In this work, we propose ${\mathsf {S}}^{2}$ Engine – a novel systolic architecture that can fully exploit the sparsity in CNNs with maximized data reuse. ${\mathsf {S}}^{2}$ Engine transmits compressed data internally and allows each processing element to dynamically select an aligned data from the compressed dataflow in convolution. Compared to the naïve systolic array, ${\mathsf {S}}^{2}$ Engine achieves about $3.2\times$ and about $3.0\times$ improvements on speed and energy efficiency, respectively.},
  archive      = {J_TC},
  author       = {Jianlei Yang and Wenzhi Fu and Xingzhou Cheng and Xucheng Ye and Pengcheng Dai and Weisheng Zhao},
  doi          = {10.1109/TC.2021.3087946},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1440-1452},
  shortjournal = {IEEE Trans. Comput.},
  title        = {S2 engine: A novel systolic architecture for sparse convolutional neural networks},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-accuracy multiply-accumulate (MAC) technique for unary
stochastic computing. <em>TC</em>, <em>71</em>(6), 1425–1439. (<a
href="https://doi.org/10.1109/TC.2021.3087027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiply-accumulate (MAC) operations are common in data processing and machine learning but costly in terms of hardware usage. Stochastic Computing (SC) is a promising approach for low-cost hardware design of complex arithmetic operations such as multiplication. Computing with deterministic unary bit-streams (defined as bit-streams with all 1s grouped at the beginning or end of a bit-stream) has been recently suggested to improve the accuracy of SC. Conventionally, SC designs use multiplexer (mux) units or OR gates to accumulate data in the stochastic domain. MUX-based addition suffers from scaling of data and OR -based addition from inaccuracy. This work proposes a novel technique for MAC operation on unary bit-streams that allows exact, non-scaled addition of multiplication results. By introducing a relative delay between the products, we control correlation between bit-streams and eliminate OR -based addition error. We evaluate the accuracy of the proposed technique compared to the state-of-the-art MAC designs. After quantization, the proposed technique demonstrates at least 37 percent and up to 100 percent decrease of the mean absolute error for uniformly distributed random input values, compared to traditional OR -based MAC designs. Further, we demonstrate that the proposed technique is practical and evaluate area, power and energy of three possible implementations.},
  archive      = {J_TC},
  author       = {Peter Schober and M. Hassan Najafi and Nima TaheriNejad},
  doi          = {10.1109/TC.2021.3087027},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1425-1439},
  shortjournal = {IEEE Trans. Comput.},
  title        = {High-accuracy multiply-accumulate (MAC) technique for unary stochastic computing},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time full-chip thermal tracking: A post-silicon,
machine learning perspective. <em>TC</em>, <em>71</em>(6), 1411–1424.
(<a href="https://doi.org/10.1109/TC.2021.3086112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a novel approach to real-time tracking of full-chip heatmaps for commercial off-the-shelf microprocessors based on machine-learning. The proposed post-silicon approach, named RealMaps , only uses the existing embedded temperature sensors and workload-independent utilization information, which are available in real-time. Moreover, RealMaps does not require any knowledge of the proprietary design details or manufacturing process-specific information of the chip. Consequently, the methods presented in this work can be implemented by either the original chip manufacturer or a third party alike, and is aimed at supplementing, rather than substituting, the temperature data sensed from the existing embedded sensors. The new approach starts with offline acquisition of accurate spatial and temporal heatmaps using an infrared thermal imaging setup while nominal working conditions are maintained on the chip. To build the dynamic thermal model, a temporal-aware long-short-term-memory (LSTM) neutral network is trained with system-level features such as chip frequency, instruction counts, and other high-level performance metrics as inputs. Instead of a pixel-wise heatmap estimation, we perform 2D spatial discrete cosine transformation (DCT) on the heatmaps so that they can be expressed with just a few dominant DCT coefficients. This allows for the model to be built to estimate just the dominant spatial features of the 2D heatmaps, rather than the entire heatmap images, making it significantly more efficient. Experimental results from two commercial chips show that RealMaps can estimate the full-chip heatmaps with 0.9 $^{\circ }$ C and 1.2 $^{\circ }$ C root-mean-square-error respectively and take only 0.4ms for each inference which suits well for real-time use. Compared to the state of the art pre-silicon approach, RealMaps shows similar accuracy, but with much less computational cost.},
  archive      = {J_TC},
  author       = {Sheriff Sadiqbatcha and Jinwei Zhang and Hussam Amrouch and Sheldon X.-D. Tan},
  doi          = {10.1109/TC.2021.3086112},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1411-1424},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Real-time full-chip thermal tracking: A post-silicon, machine learning perspective},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Up to <span
class="math inline">8<em>k</em></span><!-- -->8k-bit modular montgomery
multiplication in residue number systems with fast 16-bit residue
channels. <em>TC</em>, <em>71</em>(6), 1399–1410. (<a
href="https://doi.org/10.1109/TC.2021.3086071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware realization of public-key cryptosystems often entails Montgomery modular multiplication (MMM), which is more efficient in residue number systems (RNS). A large pool of co-prime moduli allows for higher number of dynamically changeable moduli-set pairs for the required base extension, leading to ultra-wide key-lengths to accommodate the indispensable resistance to differential power-analysis (DPA) attacks. The moduli are often of the form ${2^r} - {{\delta }}$ , where $r$ denotes the width of residue channels. In a previous relevant RNS MMM design, with $r\ = \ 64$ , probability of a successful DPA attack is less than ${2^{ - 66}}$ , where efficient arithmetic is obtained only for a limited set of moduli that are insufficient for key-lengths over 1024 bits. Here we propose a free- ${{\delta }}$ RNS MMM scheme, for up-to 8192-bit key-lengths and fast 16-bit residue channels, based on the proposed ${{\delta }}$ -independent modulo-( ${2^r} - {{\delta }}$ ) adders and multipliers. Moreover, we propose an especial method for moduli selection that is required for base extension, leading to the same aforementioned DPA-resistance measure and much lower measures for key-lengths over 1024. The implementation results show $82,69,44\ percent$ less RSA delay, for key-lengths $512,1024,2048$ , respectively of the home designs versus the 512-bit main reference design, and more than $5,100\ percent$ for $4096,8192$ key-lengths, respectively, all per 512-bit encrypted messages.},
  archive      = {J_TC},
  author       = {Zabihollah Ahmadpour and Ghassem Jaberipur},
  doi          = {10.1109/TC.2021.3086071},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1399-1410},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Up to $8k$8k-bit modular montgomery multiplication in residue number systems with fast 16-bit residue channels},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VMT: Virtualized multi-threading for accelerating graph
workloads on commodity processors. <em>TC</em>, <em>71</em>(6),
1386–1398. (<a href="https://doi.org/10.1109/TC.2021.3086069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern-day graph workloads operate on huge graphs through pointer chasing which leads to high last-level cache (LLC) miss rates and limited memory-level parallelism (MLP). Simultaneous Multi-Threading (SMT) effectively hides the memory access latencies for multi-threaded graph workloads provided that sufficient threads are supported in hardware. Unfortunately, providing a sufficiently large number of physical threads incurs an unjustifiably high hardware cost for commodity SMT processors which typically implement only two physical hardware threads. Ideally, we would like to achieve aggressive-SMT performance when running graph workloads on modest commodity processors. In this paper, we propose Virtualized Multi-Threading (VMT), a low-overhead multi-threading paradigm for accelerating graph workloads on commodity processors. Unlike prior multi-threading paradigms, VMT virtualizes both the physical hardware threads and the architecture state: VMT maps a large number of logical software threads to a small number of physical hardware threads, while maintaining the architecture state of the logical threads in the processor&#39;s cache hierarchy. Implemented on top of a quad-core 2-way SMT processor, VMT achieves an average speedup of 1.74× for a set of representative graph workloads, while incurring minimal hardware cost (195 bytes per core to support up to 32 logical threads). VMT&#39;s low hardware cost paves the way for implementation in commodity processors.},
  archive      = {J_TC},
  author       = {Josué Feliu and Ajeya Naithani and Julio Sahuquillo and Salvador Petit and Moinuddin Qureshi and Lieven Eeckhout},
  doi          = {10.1109/TC.2021.3086069},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1386-1398},
  shortjournal = {IEEE Trans. Comput.},
  title        = {VMT: Virtualized multi-threading for accelerating graph workloads on commodity processors},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Some conditional cube testers for grain-128a of reduced
rounds. <em>TC</em>, <em>71</em>(6), 1374–1385. (<a
href="https://doi.org/10.1109/TC.2021.3085144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a new strategy, maximum last $\alpha$α round , is proposed to select cubes for cube attacks. This strategy considers the cubes in a particular round where the probability of its superpoly to be 1 is at most $\alpha$ , where $\alpha$ is a very small number. A heuristic method to find a number of suitable cubes using this strategy and the previously used strategies (i.e., maximum initial zero, maximum last zero) are proposed. To get a bias at the higher rounds, the heuristic, too, imposes conditions on some state bits of the cipher to make the non-constant superpoly of a cube as zero for the first few rounds. Some cube testers are formed by using those suitable cubes to implement a distinguishing attack on Grain-128a of reduced KSA (or initialization) rounds. We present a distinguisher for Grain-128a of 191 (out of 256) KSA round in the single key setup and 201 (out of 256) KSA round in the weak key setup by using the cubes of dimension 5. The number of rounds is the highest till today, and the cube dimension is smaller than the previous results. Further, we tested our algorithm on Grain-128 and achieved good results by using small cubes.},
  archive      = {J_TC},
  author       = {Deepak Kumar Dalai and Santu Pal and Santanu Sarkar},
  doi          = {10.1109/TC.2021.3085144},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1374-1385},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Some conditional cube testers for grain-128a of reduced rounds},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Polynomial computation using unipolar stochastic logic and
correlation technique. <em>TC</em>, <em>71</em>(6), 1358–1373. (<a
href="https://doi.org/10.1109/TC.2021.3085120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses polynomial computation with unipolar stochastic logic by exploiting correlation between the bit-streams. The AND-OR, double-NAND, OR-AND and double-NOR circuits are presented for polynomials with all positive coefficients whose sum is less than or equal to one by mathematically analyzing the joint probability distribution of coefficient bit-streams. The NAND-AND expansion is also developed for polynomials with alternatively positive and negative coefficients whose absolute values are decreasing by applying the same idea. Unlike the original methods with multiple uncorrelated random number sources (RNSs) for coefficient bit-stream generation, the presented methods only require a single RNS. Since the RNSs take up huge hardware resource in stochastic circuits, the proposed RNS-sharing techniques for polynomial computation result in a significant reduction of hardware complexity. For the factorization technique in the general polynomials, this paper enhances the original stochastic designs for the second-order polynomial and further presents the simple correlation-dependent circuits. Results show that the proposed architectures are superior to the previous ones by reducing the total number of RNSs.},
  archive      = {J_TC},
  author       = {Shao-I Chu and Chi-Long Wu and Tu N. Nguyen and Bing-Hong Liu},
  doi          = {10.1109/TC.2021.3085120},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1358-1373},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Polynomial computation using unipolar stochastic logic and correlation technique},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AEML: An acceleration engine for multi-GPU load-balancing in
distributed heterogeneous environment. <em>TC</em>, <em>71</em>(6),
1344–1357. (<a href="https://doi.org/10.1109/TC.2021.3084407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the rapid growth computation requirements in big data and artificial intelligence area, CPU-GPU heterogeneous clusters can provide more powerful computing capacity compared to CPU clusters. The high parallel computing capabilities of GPUs greatly accelerate computation-intensive applications. And the number of GPUs on single computing node is scalable, which greatly improves the computing capacity of the cluster under the condition of limited cluster size. However, there is a lack of the effective load-balancing scheduling model in multi-GPU hardware environment. This article proposes AEML, an acceleration engine for multi-GPU load-balancing in distributed heterogeneous environment. AEML can effectively integrate GPUs into distributed processing framework and achieve great load-balance among multiple heterogeneous GPUs. We propose a heterogeneous task execution model based on multiple GPUs and multiple streams (MGMS), which can effectively balance the workload of multiple GPUs. MGMS model utilizes four core techniques: a fine-grained task mapping mechanism, a device resource unified management scheme, a novel resource-aware GPU task scheduling strategy, and a feedback-based streams adjustment scheme. The implementation of AEML system is based on Spark 3.0.0 and NVIDIA CUDA 10.0. We comprehensively evaluate the performance of AEML with multiple typical benchmarks. Experimental results show that AEML can fully exploit the computing power of GPUs and achieve great load-balance among multiple heterogeneous GPUs.},
  archive      = {J_TC},
  author       = {Zhuo Tang and Lifan Du and Xuedong Zhang and Li Yang and Kenli Li},
  doi          = {10.1109/TC.2021.3084407},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1344-1357},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AEML: An acceleration engine for multi-GPU load-balancing in distributed heterogeneous environment},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DedupHR: Exploiting content locality to alleviate read/write
interference in deduplication-based flash storage. <em>TC</em>,
<em>71</em>(6), 1332–1343. (<a
href="https://doi.org/10.1109/TC.2021.3084116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data deduplication has been widely employed in flash-based storage as an effective technique for enhancing reliability and cost efficiency. However, the read/write interference problem of flash storage, where on-going write requests prevent read requests from accessing available data and significantly increase read latency, remains a critical concern with the rapid evolutions of flash chips from SLC through MLC to TLC and on to QLC, with PLC on the horizon, especially under workloads with a mixture of read and write requests. To significantly improve the read performance in face of read/write interference, which is often more critical than the write performance, we propose an enhanced Hot Read Data Replication scheme for deduplication-based flash storage, called DedupHR, to alleviate the read/write interference problem. DedupHR exploits the content locality in the deduplication-based flash storage and outsources the data blocks with high reference count to a surrogate space such as a dedicated spare flash chip or an over-provisioned space in an SSD. By servicing some conflicted read requests on the surrogate flash space, DedupHR can significantly alleviate, if not entirely eliminate, the contention between the read requests and the on-going write requests. The evaluation results show that DedupHR significantly improves the state-of-the-art schemes in terms of the system performance and cost efficiency. Consequently, the tail-latency of the deduplication-based flash storage is also reduced.},
  archive      = {J_TC},
  author       = {Suzhen Wu and Chunfeng Du and Weiwei Zhang and Bo Mao and Hong Jiang},
  doi          = {10.1109/TC.2021.3084116},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1332-1343},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DedupHR: Exploiting content locality to alleviate Read/Write interference in deduplication-based flash storage},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting spectre attacks using hardware performance
counters. <em>TC</em>, <em>71</em>(6), 1320–1331. (<a
href="https://doi.org/10.1109/TC.2021.3082471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectre attacks can be catastrophic and widespread because they exploit common design flaws caused by the speculative capabilities in modern processors to leak sensitive data through side channels. Completely fixing the problem would require a redesign of the architecture for transient execution or the implementation of a new design on re-configurable hardware. However, such fixes cannot be backported to old machines with fixed hardware design. Completely replacing those machines will take a long time. Moreover, existing software patches may cause significant performance overhead. This paper proposes to detect Spectre by monitoring deviations in microarchitectural events using hardware performance counters with promising accuracy above 90 percent under a variety of workload conditions. However, the attacker may attempt to evade detection by slowing down the attack or mimicking benign programs. This paper thus compares different evasion strategies quantitatively and demonstrates that it is possible for the attacker to avoid detection when operating the attacks at a lower speed while maintaining a reasonable attack success rate. Then, we show that, in order to resist evasion, the original detector must be enhanced by randomly switching between a set of detectors using different features and sampling periods so we can keep the detection accuracy above 80 percent.},
  archive      = {J_TC},
  author       = {Congmiao Li and Jean-Luc Gaudiot},
  doi          = {10.1109/TC.2021.3082471},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1320-1331},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Detecting spectre attacks using hardware performance counters},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3RSeT: Read disturbance rate reduction in STT-MRAM caches by
selective tag comparison. <em>TC</em>, <em>71</em>(6), 1305–1319. (<a
href="https://doi.org/10.1109/TC.2021.3082004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent development in memory technologies has introduced Spin-Transfer Torque Magnetic RAM (STT-MRAM) as the most promising replacement for SRAMs in on-chip cache memories. Besides its lower leakage power, higher density, immunity to radiation-induced particles, and non-volatility, an unintentional bit flip during read operation, referred to as read disturbance error, is a severe reliability challenge in STT-MRAM caches. One major source of read disturbance error in STT-MRAM caches is simultaneous accesses to all tags for parallel comparison operation in a cache set, which has not been addressed in previous work. This article first demonstrates that high read accesses to tag array extremely increase the read disturbance rate and then proposes a low-cost scheme, so-called Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison (3RSeT), to reduce the error rate by eliminating a significant portion of tag reads. 3RSeT proactively disables the tags that have no chance for hit, using low significant bits of the tags on each access request. Our evaluations using gem5 full-system cycle-accurate simulator show that 3RSeT reduces the read disturbance rate in the tag array by 71.8 percent, which results in 3.6x improvement in $Mean~Time~To~Failure$ (MTTF). In addition, the energy consumption is reduced by 62.1 percent without compromising performance and with less than 0.4 percent area overhead.},
  archive      = {J_TC},
  author       = {Elham Cheshmikhani and Hamed Farbeh and Hossein Asadi},
  doi          = {10.1109/TC.2021.3082004},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1305-1319},
  shortjournal = {IEEE Trans. Comput.},
  title        = {3RSeT: Read disturbance rate reduction in STT-MRAM caches by selective tag comparison},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). S-FLASH: A NAND flash-based deep neural network accelerator
exploiting bit-level sparsity. <em>TC</em>, <em>71</em>(6), 1291–1304.
(<a href="https://doi.org/10.1109/TC.2021.3082003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The processing in-memory (PIM) approach that combines memory and processor appears to solve the memory wall problem. NAND flash memory, which is widely adopted in edge devices, is one of the promising platforms for PIM with its high-density property and the intrinsic ability for analog vector-matrix multiplication. Despite its potential, the domain conversion process, which converts an analog current to a digital value, accounts for most energy consumption on the NAND flash-based accelerator. It restricts the NAND flash memory usage for PIM compared to the other platforms. In this article, we propose a NAND flash-based DNN accelerator to achieve both large memory density and energy efficiency among various platforms. As the NAND flash memory already shows higher memory density than other memory platforms, we aim to enhance energy efficiency by reducing the domain conversion process burden. First, we optimize the bit width of partial multiplication by considering the analog-to-digital converter (ADC) resource. For further optimization, we propose a methodology to exploit many zero partial multiplication results for enhancing both energy efficiency and throughput. The proposed work successfully exploits the bit-level sparsity of DNN, which results in achieving up to 8.6×/8.2× larger energy efficiency/throughput over the provisioned baseline.},
  archive      = {J_TC},
  author       = {Myeonggu Kang and Hyeonuk Kim and Hyein Shin and Jaehyeong Sim and Kyeonghan Kim and Lee-Sup Kim},
  doi          = {10.1109/TC.2021.3082003},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1291-1304},
  shortjournal = {IEEE Trans. Comput.},
  title        = {S-FLASH: A NAND flash-based deep neural network accelerator exploiting bit-level sparsity},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Malware analysis by combining multiple detectors and
observation windows. <em>TC</em>, <em>71</em>(6), 1276–1290. (<a
href="https://doi.org/10.1109/TC.2021.3082002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malware developers continually attempt to modify the execution pattern of malicious code hiding it inside apparent normal applications, which makes its detection and classification challenging. This article proposes an ensemble detector, which exploits the capabilities of the main analysis algorithms proposed in the literature designed to offer greater resilience to specific evasion techniques. In particular, the article presents different methods to optimally combine both generic and specialized detectors during the analysis process, which can be used to increase the unpredictability of the detection strategy, as well as improve the detection rate in presence of unknown malware families and provide better detection performance in the absence of a constant re-training of detector needed to cope with the evolution of malware. The paper also presents an alpha-count mechanism that explores how the length of the observation time window can affect the detection accuracy and speed of different combinations of detectors during the malware analysis. An extended experimental campaign has been conducted on both an open-source sandbox and an Android smartphone with different malware datasets. A trade-off among performance, training time, and mean-time-to-detect is presented. Finally, a comparison with other ensemble detectors is also presented.},
  archive      = {J_TC},
  author       = {Massimo Ficco},
  doi          = {10.1109/TC.2021.3082002},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1276-1290},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Malware analysis by combining multiple detectors and observation windows},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Memory efficient hash-based longest prefix matching
architecture with zero false +ve and nearly zero false −ve rate for IP
processing. <em>TC</em>, <em>71</em>(6), 1261–1275. (<a
href="https://doi.org/10.1109/TC.2021.3080184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel hash-based hardware architecture for longest prefix match (LPM) scheme has been presented for IP processing. The main idea is to have zero false positive (+ve) rate with negligible false negative (−ve) rate. This has been achieved by implementing hardware-based simple hash functions for faster generation of routing table addresses. Prefix table has been designed using multiple read-port memory modules to deploy concurrent access of multiple memory words. Moreover, in this architecture, memory requirement has been reduced by maintaining next-hop address pointers instead of keeping actual next-hop address using global next-hop address memory. Prefix search time is fixed and restricts an LPM search operation within a single clock cycle irrespective of IPv4 and IPv6 address suits. This architecture can accommodate increased prefix growth trend of 40–100 percent with unchanged memory capacity. After rigorous testing, false -ve rate is found mostly zero and reasonably negligible ( $\approx$ 0.00512 percent) in worst-cases, however, false +ve rate is always zero. Lower bounds in memory requirements for four schemes of the proposed LPM architecture with their probable failure-rates are analysed. System failure-rates are observed against incremental prefix growth with variable number of hash functions ranging from 6 to 8 to ensure future sustainability of the scheme.},
  archive      = {J_TC},
  author       = {Sanchita Saha Ray and Surajeet Ghosh and Bhaskar Sardar},
  doi          = {10.1109/TC.2021.3080184},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1261-1275},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Memory efficient hash-based longest prefix matching architecture with zero false +ve and nearly zero false −ve rate for IP processing},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parallel strategies for SIDH: Toward computing SIDH twice as
fast. <em>TC</em>, <em>71</em>(6), 1249–1260. (<a
href="https://doi.org/10.1109/TC.2021.3080139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present novel strategies and concrete algorithms for the parallel computation of the Supersingular Isogeny-based Diffie-Hellman key exchange (SIDH) protocol when executed on multi-core platforms. The most relevant design idea exploited by our approach is that of concurrently computing scalar multiplication operations along with a parallelized version of the strategies required for constructing and evaluating large smooth degree isogenies. We report experimental results showing that a three-core implementation of our parallel approach achieves an acceleration factor of 1.45 compared against a sequential implementation of the Supersingular Isogeny Key Encapsulation (SIKE) protocol instantiated with the prime $p_{751}$ .},
  archive      = {J_TC},
  author       = {Daniel Cervantes-Vázquez and Eduardo Ochoa-Jiménez and Francisco Rodríguez-Henríquez},
  doi          = {10.1109/TC.2021.3080139},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1249-1260},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Parallel strategies for SIDH: Toward computing SIDH twice as fast},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-performance FPGA accelerator for SIKE. <em>TC</em>,
<em>71</em>(6), 1237–1248. (<a
href="https://doi.org/10.1109/TC.2021.3078691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we provide improvements for the architecture of Supersingular Isogeny Key Encapsulation (SIKE), a post-quantum cryptography candidate. We develop a new highly optimized Montgomery multiplication algorithm and architecture for prime fields. The multiplier occupies less area and provide better timing results than the state-of-the-art. We also provide improvements to the scheduling of SIKE in our program ROM. We implement SIKE for all Round 3 NIST security levels (SIKEp434 for NIST security level 1, SIKEp503 for NIST security level 2, SIKEp610 for NIST security level 3, and SIKEp751 for NIST security level 5) on Xilinx Artix 7 and Xilinx Virtex 7 FPGAs. Our best implementation (NIST security level 1) runs 38 percent faster and occupies 30 percent less hardware resources in comparison to the leading counterpart available in the literature and implementations for other security levels achieved similar improvement.},
  archive      = {J_TC},
  author       = {Rami El Khatib and Reza Azarderakhsh and Mehran Mozaffari-Kermani},
  doi          = {10.1109/TC.2021.3078691},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1237-1248},
  shortjournal = {IEEE Trans. Comput.},
  title        = {High-performance FPGA accelerator for SIKE},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving blockchains with client-assistance. <em>TC</em>,
<em>71</em>(5), 1230–1236. (<a
href="https://doi.org/10.1109/TC.2021.3072129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain is a distributed database shared among disparate parties. It promises to enable new applications and solutions to wide-ranging domains. However, today&#39;s blockchains suffer from low throughput and high latency. This impedes widespread adoption of more complex blockchain-based applications. We propose a new direction for the future development of blockchains: pushing utmost work to client-side to make the blockchain-core as simple as possible. To show the feasibility and practicability of this idea, we construct both client-assisted consensus and client-assisted smart contracts. The client-assisted consensus only requires a single round-trip between clients and blockchain nodes; it is leaderless and parallelizable. Our experimental results show that it can process thousands of transactions per second when the number of replicas is 400. The client-assisted smart contract pushes the expensive execution to client-side and ensures the correctness by verifiable computation. It avoids duplicated execution, allows parallel execution and reduces transaction/blockchain sizes.},
  archive      = {J_TC},
  author       = {Jian Liu and Kui Ren},
  doi          = {10.1109/TC.2021.3072129},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1230-1236},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Improving blockchains with client-assistance},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep neural network training architecture with
inference-aware heterogeneous data-type. <em>TC</em>, <em>71</em>(5),
1216–1229. (<a href="https://doi.org/10.1109/TC.2021.3078316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As deep learning applications often encounter accuracy degradation due to the distorted inputs from a variety of environmental conditions, training with personal data has become essential for the edge devices. Hence, &amp;#x2018;training on edge&amp;#x2019; by supporting a trainable deep learning accelerator has been actively studied. Nevertheless, previous research does not consider the fundamental datapath for training and the importance of retaining the high performance for inference tasks. In this work, we propose NeuroFlix, a deep neural network training accelerator supporting heterogeneous data-type of floating- and fixed-point for input operands. From two perspectives: 1) separate precision decision for each input data, 2) maintenance of high performance on inference, we configure the data with low-bit fixed-point of activation/weight and floating-point based error gradient securing up to half-precision. A novel MAC architecture is designed to compute low- and high-precision modes for the different input combinations. By substituting a high-cost floating-point based addition to brick-level separate accumulations, we realize both area-efficient architecture and high throughput for low-precision computation. Consequently, NeuroFlix outperforms the previous architectures of state-of-the-art configurations proving its high efficiency in both training and inference. By also comparing with the off-the-shelf bfloat16-based accelerator, it achieves 1.2 &amp;#x00D7;/2.0 &amp;#x00D7; of speedup/energy-efficiency at training and further enhancement of 3.6 &amp;#x00D7;/4.5 &amp;#x00D7; at inference.},
  archive      = {J_TC},
  author       = {Seungkyu Choi and Jaekang Shin and Lee-Sup Kim},
  doi          = {10.1109/TC.2021.3078316},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1216-1229},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A deep neural network training architecture with inference-aware heterogeneous data-type},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Folding BIKE: Scalable hardware implementation for
reconfigurable devices. <em>TC</em>, <em>71</em>(5), 1204–1215. (<a
href="https://doi.org/10.1109/TC.2021.3078294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary digital infrastructures and systems use and trust Public-Key Cryptography to exchange keys over insecure communication channels. With the development and progress in the research field of quantum computers, well established schemes like RSA and ECC are more and more threatened. The urgent demand to find and standardize new schemes &amp;#x2013; which are secure in a post-quantum world &amp;#x2013; was also realized by the National Institute of Standards and Technology which announced a Post-Quantum Cryptography Standardization Project in 2017. Recently, the round three candidates were announced and one of the alternate candidates is the Key Encapsulation Mechanism scheme BIKE. In this article, we investigate different strategies to efficiently implement the BIKE algorithm on Field-Programmable Gate Arrays (FPGAs). To this extend, we improve already existing polynomial multipliers, propose efficient strategies to realize polynomial inversions, and implement the Black-Gray-Flip decoder for the first time. Additionally, our implementation is designed to be scalable and generic with the BIKE specific parameters. All together, the fastest designs achieve latencies of 2.69 ms for the key generation, 0.1 ms for the encapsulation, and 1.89 ms for the decapsulation considering the lowest security level.},
  archive      = {J_TC},
  author       = {Jan Richter-Brockmann and Johannes Mono and Tim G&amp;#x00FC;neysu},
  doi          = {10.1109/TC.2021.3078294},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1204-1215},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Folding BIKE: Scalable hardware implementation for reconfigurable devices},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constructing completely independent spanning trees in a
family of line-graph-based data center networks. <em>TC</em>,
<em>71</em>(5), 1194–1203. (<a
href="https://doi.org/10.1109/TC.2021.3077587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past decade has seen growing importance being attached to the Completely Independent Spanning Trees (CISTs). The CISTs can facilitate many network functionalities, and the existence and construction schemes of CISTs in various networks can be an indicator of the network&#39;s robustness. In this paper, we establish the number of CISTs that can be constructed in the line graph of the complete graph $K_n$ (denoted $L(K_n)$ , for $n\geq 4$ ), and present an algorithm to construct the optimal (i.e., maximal) number of CISTs in $L(K_n)$ . The $L(K_n)$ is a special class of SWCube [13], an architectural model proposed for data center networks. Our construction algorithm is also implemented to verify its validity.},
  archive      = {J_TC},
  author       = {Yifeng Wang and Baolei Cheng and Yu Qian and Dajin Wang},
  doi          = {10.1109/TC.2021.3077587},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1194-1203},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Constructing completely independent spanning trees in a family of line-graph-based data center networks},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-bank on-chip memory management techniques for CNN
accelerators. <em>TC</em>, <em>71</em>(5), 1181–1193. (<a
href="https://doi.org/10.1109/TC.2021.3076987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since off-chip DRAM access affects both performance and power consumption significantly, convolutional neural network (CNN) accelerators commonly aim to maximize data reuse in on-chip memory. By organizing the on-chip memory to multiple banks, we may hide off-chip DRAM access delay by prefetching data to unused banks during computation. When and where to prefetch data and how to reuse the feature map data between layers define the multi-bank on-chip memory management (MOMM) problem. In this paper, we propose compiler techniques to solve the MOMM problem with two different objectives: one is to minimize the off-chip memory access volume, and the other is to minimize the processing delay caused by unhidden DRAM accesses. By running CNN benchmarks on a cycle-level NPU simulator, we demonstrate the trade-off relation between two objectives. Compared with the baseline approach that does not reuse the feature map between layers, we could reduce the DRAM access volume and the processing delay up to 55.0 and 79.4 percent, respectively. Moreover, we extend the proposed techniques to consider layer fusion that aims to reuse feature maps between layers. Experiment results confirm the superiority of the proposed hybrid fusion technique to the per-layer processing technique and the pure fusion technique.},
  archive      = {J_TC},
  author       = {Duseok Kang and Donghyun Kang and Soonhoi Ha},
  doi          = {10.1109/TC.2021.3076987},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1181-1193},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Multi-bank on-chip memory management techniques for CNN accelerators},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient ancilla-free reversible and quantum circuits for
the hidden weighted bit function. <em>TC</em>, <em>71</em>(5),
1170–1180. (<a href="https://doi.org/10.1109/TC.2021.3076435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Hidden Weighted Bit function plays an important role in the study of classical models of computation. A common belief is that this function is exponentially hard to implement using reversible ancilla-free circuits, even though introducing a small number of ancillae allows a very efficient implementation. In this paper, we refute the exponential hardness conjecture by developing a polynomial-size reversible ancilla-free circuit computing the Hidden Weighted Bit function. Our circuit has size $O(n^{6.42})$O(n6.42), where $n$n is the number of input bits. We also show that the Hidden Weighted Bit function can be computed by a quantum ancilla-free circuit of size $O(n^2)$O(n2). The technical tools employed come from a combination of Theoretical Computer Science (Barrington&amp;#x0027;s theorem) and Physics (simulation of fermionic Hamiltonians) techniques.},
  archive      = {J_TC},
  author       = {Sergey Bravyi and Theodore J. Yoder and Dmitri Maslov},
  doi          = {10.1109/TC.2021.3076435},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1170-1180},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient ancilla-free reversible and quantum circuits for the hidden weighted bit function},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PITEM: Permutations-based instruction tracking via
electromagnetic side-channel signal analysis. <em>TC</em>,
<em>71</em>(5), 1156–1169. (<a
href="https://doi.org/10.1109/TC.2021.3076354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of cyber-physical systems (CPS) and internet of things (IoT) devices impose significant security and privacy concerns that necessitate robust monitoring and malware detection systems. This paper proposes PITEM, a framework for instruction-level monitoring and malware detection using electromagnetic (EM) side-channels. PITEM identifies instruction types with similar EM emanations using hierarchical clustering. To track all combinations of these instruction types, we generate EM signatures for all permutations of them. In testing, we predict the permutation class of testing traces by a matched-filter-like predictor. We test the performance on two devices (FPGA-based and ARM-based) with 50 MHz and 1 GHz clock frequencies. We achieve 95.67 and 87.35 percent accuracies for these devices for single execution of permutations. We note that the accuracy increases to 100 percent when permutation blocks are repeated. Furthermore, we test the limits of the system by tracking permutations of instructions of the same type. With sufficient bandwidth and number of repetitions, individual instructions can be resolved with 87.5 and 95.78 percent accuracies for these devices. The performance is evaluated for different relative signal-to-noise ratio (SNR) levels and performance is stable for relative SNR values $&amp;#x003E;15$&amp;#x003E;15 dB. Finally, we demonstrate PITEM&amp;#x0027;s ability to detect fine-grained malware with 99.89 percent accuracy.},
  archive      = {J_TC},
  author       = {Elvan Mert Ugurlu and Baki Berkay Yilmaz and Alenka Zaji&amp;#x0107; and Milos Prvulovic},
  doi          = {10.1109/TC.2021.3076354},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1156-1169},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PITEM: Permutations-based instruction tracking via electromagnetic side-channel signal analysis},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enabling homomorphically encrypted inference for large DNN
models. <em>TC</em>, <em>71</em>(5), 1145–1155. (<a
href="https://doi.org/10.1109/TC.2021.3076123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of machine learning services in the last few years has raised data privacy concerns. Homomorphic encryption (HE) enables inference using encrypted data but it incurs 100x&amp;#x2013;10,000x memory and runtime overheads. Secure deep neural network (DNN) inference using HE is currently limited by computing and memory resources, with frameworks requiring hundreds of gigabytes of DRAM to evaluate small models. To overcome these limitations, in this paper we explore the feasibility of leveraging hybrid memory systems comprised of DRAM and persistent memory. In particular, we explore the recently-released Intel&amp;#x00AE; Optane&amp;#x2122; PMem technology and the Intel&amp;#x00AE; HE-Transformer nGraph&amp;#x00AE; to run large neural networks such as MobileNetV2 (in its largest variant) and ResNet-50 for the first time in the literature. We present an in-depth analysis of the efficiency of the executions with different hardware and software configurations. Our results conclude that DNN inference using HE incurs on friendly access patterns for this memory configuration, yielding efficient executions.},
  archive      = {J_TC},
  author       = {Guillermo Lloret-Talavera and Marc Jorda and Harald Servat and Fabian Boemer and Chetan Chauhan and Shigeki Tomishima and Nilesh N. Shah and Antonio J. Pe&amp;#x00F1;a},
  doi          = {10.1109/TC.2021.3076123},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1145-1155},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling homomorphically encrypted inference for large DNN models},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shuhai: A tool for benchmarking high bandwidth memory on
FPGAs. <em>TC</em>, <em>71</em>(5), 1133–1144. (<a
href="https://doi.org/10.1109/TC.2021.3075765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {FPGAs are starting to incorporate High Bandwidth Memory (HBM) to both reduce the memory bandwidth bottleneck encountered in some applications and to provide more capacity to store application state. However, the overall performance characteristics of HBMs are still not well understood, especially in the context of FPGAs, making it difficult to optimize designs relying on HBM. In this article, we bridge the gap between nominal specifications and actual performance by characterizing HBM on a state-of-the-art FPGA, i.e., a Xilinx Alveo U280 featuring a two-stack HBM subsystem. To this end, we have developed Shuhai, a benchmarking tool that throws light on all the subtle details of the performance and usage of HBMs on an FPGA. FPGA-based benchmarking should also provide a more accurate picture of HBM than measuring performance on CPUs/GPUs, since CPUs/GPUs are noisier systems due to their complex control logic and cache hierarchy. Since the memory itself is complex, leveraging custom hardware logic to benchmark it directly from an FPGA provides more details as well as more accurate and deterministic measurements. We observe that 1) HBM is able to provide up to 425 GB/s memory bandwidth, and 2) how HBM is used has a significant impact on the achievable throughput, which in turn demonstrates the importance of unveiling the performance characteristics of HBM so as to use HBM in the right manner. To demonstrate the generality of Shuhai, we also show results for other types of memory, e.g., DDR4, and DDR3, and quantitatively compare the performance characteristics of HBM with those of DDR4 and DDR3.},
  archive      = {J_TC},
  author       = {Hongjing Huang and Zeke Wang and Jie Zhang and Zhenhao He and Chao Wu and Jun Xiao and Gustavo Alonso},
  doi          = {10.1109/TC.2021.3075765},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1133-1144},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Shuhai: A tool for benchmarking high bandwidth memory on FPGAs},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SLA-based scheduling of spark jobs in hybrid cloud computing
environments. <em>TC</em>, <em>71</em>(5), 1117–1132. (<a
href="https://doi.org/10.1109/TC.2021.3075625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data frameworks such as Apache Spark is becoming prominent to perform large-scale data analytics jobs in various domains. However, due to limited resource availability, the local or on-premise computing resources are often not sufficient to run these jobs. Therefore, public cloud resources can be hired on a pay-per-use basis from the cloud service providers to deploy a Spark cluster entirely on the cloud. Nevertheless, using only cloud resources can be costly. Hence, both local and cloud resources nowadays are used together to deploy a hybrid cloud computing cluster. However, scheduling jobs in a cluster deployed on hybrid clouds is challenging in the presence of various Service-Level Agreement (SLA) demands such as cost minimization and job deadline guarantee. Most of the existing works either consider a public or a locally deployed cluster and mainly focus on improving job performance in the cluster. In this article, we propose efficient scheduling algorithms that leverage from different VM instance pricing in a hybrid cloud deployed cluster to optimize the Virtual Machine (VM) usage cost for both local and cloud resources and maximize the job deadline met percentage. We have conducted extensive simulation-based experiments to compare our proposed algorithms with the baseline approaches. In addition, we have developed a prototype system on top of Apache Mesos cluster manager and performed real experiments to evaluate the applicability of our proposed approaches in a real platform with benchmark applications. The results show that our proposed algorithms are highly scalable and reduce the cost of VM usage of a hybrid cluster for up to 20 percent.},
  archive      = {J_TC},
  author       = {Muhammed Tawfiqul Islam and Huaming Wu and Shanika Karunasekera and Rajkumar Buyya},
  doi          = {10.1109/TC.2021.3075625},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1117-1132},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SLA-based scheduling of spark jobs in hybrid cloud computing environments},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Beyond binary search: Parallel in-place construction of
implicit search tree layouts. <em>TC</em>, <em>71</em>(5), 1104–1116.
(<a href="https://doi.org/10.1109/TC.2021.3075392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present parallel algorithms to efficiently permute a sorted array into the level-order binary search tree (BST), level-order B-tree (B-tree), and van Emde Boas (vEB) layouts in-place. We analytically determine the complexity of our algorithms and empirically measure their performance. When considering the total time to permute the data in-place and to perform a series of search queries, the vEB layout provides the best performance on the CPU. Given an input of $N$N=537 million 64-bit integers, the benefits of query performance (compared to binary search) outweigh the cost of in-place permutation when performing as few as 0.37\% of $N$N queries. On the GPU, results depend on the particular architecture, with the B-tree and vEB layouts performing the best. The number of queries necessary to reach the break-even point with binary search ranges from 1.3\% to 8.9\% of $N$N=1,074 million 32-bit integers.},
  archive      = {J_TC},
  author       = {Kyle Berney and Henri Casanova and Ben Karsin and Nodari Sitchinava},
  doi          = {10.1109/TC.2021.3075392},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1104-1116},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Beyond binary search: Parallel in-place construction of implicit search tree layouts},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BAFL: A blockchain-based asynchronous federated learning
framework. <em>TC</em>, <em>71</em>(5), 1092–1103. (<a
href="https://doi.org/10.1109/TC.2021.3072033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an emerging distributed machine learning (ML) method, federated learning (FL) can protect data privacy through collaborative learning of artificial intelligence (AI) models across a large number of devices. However, inefficiency and vulnerability to poisoning attacks have slowed FL performance. Therefore, a blockchain-based asynchronous federated learning (BAFL) framework is proposed to ensure the security and efficiency required by FL. The blockchain ensures that the model data cannot be tampered with while asynchronous learning speeds up global aggregation. A novel entropy weight method is used to evaluate the participating rank and proportion of the local model trained in BAFL of the devices. The energy consumption and local model update efficiency are balanced by adjusting the local training and communication delay and optimizing the block generation rate. The extensive evaluation results show that the proposed BAFL framework has higher efficiency and higher performance for preventing poisoning attacks than other distributed ML methods.},
  archive      = {J_TC},
  author       = {Lei Feng and Yiqi Zhao and Shaoyong Guo and Xuesong Qiu and Wenjing Li and Peng Yu},
  doi          = {10.1109/TC.2021.3072033},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1092-1103},
  shortjournal = {IEEE Trans. Comput.},
  title        = {BAFL: A blockchain-based asynchronous federated learning framework},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal parallelization of single/multi-segment real-time
tasks for global EDF. <em>TC</em>, <em>71</em>(5), 1077–1091. (<a
href="https://doi.org/10.1109/TC.2021.3071730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Targeting global EDF scheduling, this article proposes an optimal algorithm for parallelizing tasks with parallelization freedom. For this, we extend the interference-based sufficient schedulability analysis and derive monotonic increasing properties of both tolerance and interference for the schedulability. Leveraging those properties, we propose a one-way search&amp;#x2013;based optimal algorithm with polynomial time complexity. We present a formal proof of the optimality of the proposed algorithm. We first address the single-segment task model and then extend to the multi-segment task model. Our extensive experiments through both simulation and actual implementation show that our proposed approach can significantly improve the schedulability.},
  archive      = {J_TC},
  author       = {Youngeun Cho and Do Hyung Kim and Daechul Park and Seung Su Lee and Chang-Gun Lee},
  doi          = {10.1109/TC.2021.3071730},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1077-1091},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimal parallelization of Single/Multi-segment real-time tasks for global EDF},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BM-RCGL: Benchmarking approach for localization of
reliability-critical gates in combinational logic blocks. <em>TC</em>,
<em>71</em>(5), 1063–1076. (<a
href="https://doi.org/10.1109/TC.2021.3071253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and effective localization of reliability-critical gates (RCGs) is one of the important prerequisites for low-cost circuit fault tolerance in the early stages of circuit design. This article introduces an accurate and effective approach for localizing RCGs in combinational logic blocks through a benchmarking technique. In the proposed approach, uniform non-Bernoulli sequences are used to produce a set of input vectors for driving circuits. A full-period linear congruential algorithm is employed to generate a sequence that provides the sampled order for the RCGs to be analyzed. This ensures that each gate in the circuit is treated as fairly as possible. To accelerate the localization process, an input-vector-based pruning technique combined with a counting method is also introduced to identify the specified number of RCGs. Then, the criticality of gate reliability for each RCG is measured through benchmarking. A clustering algorithm carries out the convergence checking for the proposed approach. The performance of the proposed approach was evaluated in terms of accuracy, stability, and time-space overhead by various simulations on 74-series circuits and ISCAS-85 benchmark circuits. The results show that its accuracy is close to that of the Monte Carlo model and its stability is better than that of other approximate methods. Moreover, compared with approximate methods, the time overhead of our approach is advantageous in the presence of similar memory overheads.},
  archive      = {J_TC},
  author       = {Jie Xiao and Zhanhui Shi and Xuhua Yang and Jungang Lou},
  doi          = {10.1109/TC.2021.3071253},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1063-1076},
  shortjournal = {IEEE Trans. Comput.},
  title        = {BM-RCGL: Benchmarking approach for localization of reliability-critical gates in combinational logic blocks},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reinforcement learning-based resource partitioning for
improving responsiveness in cloud gaming. <em>TC</em>, <em>71</em>(5),
1049–1062. (<a href="https://doi.org/10.1109/TC.2021.3070879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud gaming has been very popular in recent years, but issues relating to maintaining low interaction delay to guarantee satisfactory user experience are still prevalent. We observe that the server-side processing delay in cloud gaming system could be heavily influenced by how the resources are partitioned among processes. However, finding the optimal partitioning policy that minimizes the response delay faces several critical challenges. First, fine-grained resource partitioning is non-trivial due to the limitations of hardwre-based resource isolation techniques. Second, game wokload is highly dynamic and unpredictable, making the design of efficient resource partitioning policy more challenging. In this article, we propose an online resource partitioning framework for reducing response delay in cloud gaming, which has several promising properties. First, we divide the processes into disjoint groups and partition resources among process groups, which greatly simplifies the resource partitioning problem while ensuring high partitioning effectiveness. Second, to tackle dynamic workload changes, we classify game workloads into several clusters and maintain separate process grouping plan for each cluster. Third, we leverage reinforcement learning to adaptively choose the best actions for minimizing response delay in real time. We evaluate the proposed framework in a real cloud gaming environment using several real games. The experimental results show that our approach can reduce the response delay by 22 to 41 percent compared to a system without resource partitioning, and outperforms other resource partitioning policies significantly.},
  archive      = {J_TC},
  author       = {Yusen Li and Xiwei Wang and Haoyuan Liu and Lingjun Pu and Shanjiang Tang and Gang Wang and Xiaoguang Liu},
  doi          = {10.1109/TC.2021.3070879},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1049-1062},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Reinforcement learning-based resource partitioning for improving responsiveness in cloud gaming},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AutoDiagn: An automated real-time diagnosis framework for
big data systems. <em>TC</em>, <em>71</em>(5), 1035–1048. (<a
href="https://doi.org/10.1109/TC.2021.3070639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data processing systems, such as Hadoop and Spark, usually work in large-scale, highly-concurrent, and multi-tenant environments that can easily cause hardware and software malfunctions or failures, thereby leading to performance degradation. Several systems and methods exist to detect big data processing systems&amp;#x2019; performance degradation, perform root-cause analysis, and even overcome the issues causing such degradation. However, these solutions focus on specific problems such as stragglers and inefficient resource utilization. There is a lack of a generic and extensible framework to support the real-time diagnosis of big data systems. In this article, we propose, develop and validate AutoDiagn. This generic and flexible framework provides holistic monitoring of a big data system while detecting performance degradation and enabling root-cause analysis. We present an implementation and evaluation of AutoDiagn that interacts with a Hadoop cluster deployed on a public cloud and tested with real-world benchmark applications. Experimental results show that AutoDiagn can offer a high accuracy root-cause analysis framework, at the same time as offering a small resource footprint, high throughput, and low latency.},
  archive      = {J_TC},
  author       = {Umit Demirbaga and Zhenyu Wen and Ayman Noor and Karan Mitra and Khaled Alwasel and Saurabh Garg and Albert Y. Zomaya and Rajiv Ranjan},
  doi          = {10.1109/TC.2021.3070639},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1035-1048},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AutoDiagn: An automated real-time diagnosis framework for big data systems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An end-to-end learning-based metadata management approach
for distributed file systems. <em>TC</em>, <em>71</em>(5), 1021–1034.
(<a href="https://doi.org/10.1109/TC.2021.3070471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current distributed file systems are designed to support PB-scale even EB-scale data storage. Metadata service, which manages file attribute information and the global namespace tree, is crucial to system performance. Distributed metadata management, using multiple metadata servers (MDS&amp;#x0027;s) to store metadata, provides effective approaches to alleviate the workload of a single server. However, maintaining good metadata locality and keeping load balancing among MDS&amp;#x0027;s at the same time is a nontrivial problem. To better take advantage of the current distribution of the metadata, in this article, we present the first machine learning based model called DeepHash, which leverages the neural network to learn a locality preserving hashing (LPH) mapping scheme. DeepHash first converts the metadata nodes to feature vectors by the network embedding technology. Due to the absence of training labels, i.e., the hash values of metadata nodes, we design a pair loss function with distinctive characters to train DeepHash, and introduce the sampling strategy to improve the training efficiency. Besides, we propose an efficient algorithm to dynamically balance the workload and adopt the cache model to improve query efficiency. The experiments on the Amazon EC2 platform demonstrate that the DeepHash can preserve the metadata locality meanwhile maintaining a high load balancing, which denotes the effectiveness and efficiency of DeepHash compared with traditional and state-of-the-art schemes.},
  archive      = {J_TC},
  author       = {Yuanning Gao and Xiaofeng Gao and Ruisi Zhang and Guihai Chen},
  doi          = {10.1109/TC.2021.3070471},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1021-1034},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An end-to-end learning-based metadata management approach for distributed file systems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rad-hard designs by automated latching-delay assignment and
time-borrowable d-flip-flop. <em>TC</em>, <em>71</em>(5), 1008–1020. (<a
href="https://doi.org/10.1109/TC.2021.3070213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the safety-critical applications (e.g., automotive and medical electronics) emerge, various techniques of radiation hardening by design (RHBD) are proposed to deal with soft errors. Among all RHBD techniques, Built-In Soft-Error Resilience (BISER) is the first one to apply the delayed latching to separate input signals on all flip-flops for error detection. However, the delay values induced by BISER extend the setup time of all flip-flops, and may fail to meet the timing specification of the design. For minimizing such delay impact on the setup time of each flip-flop, we propose the Automated Latching-Delay Assignment (ALDA) to transfer partial values to the CK-Q delay. Later, Time-Borrowable D-Flip-Flop (TBD-FF) as well as a modified design flow is also proposed to realize the delay assignment by ALDA and to complete the design hardening. Experiments show that ALDA together with TBD-FF effectively protects five benchmark circuits against soft errors, and optimally avoids the timing violations caused by the prior delayed-latching solutions.},
  archive      = {J_TC},
  author       = {Dave Y.-W. Lin and Charles H.-P. Wen},
  doi          = {10.1109/TC.2021.3070213},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1008-1020},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Rad-hard designs by automated latching-delay assignment and time-borrowable D-flip-flop},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CEnT: An efficient architecture to eliminate intra-array
write disturbance in PCM. <em>TC</em>, <em>71</em>(5), 992–1007. (<a
href="https://doi.org/10.1109/TC.2021.3068577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase Change Memory (PCM), with its better scaling potential compared to DRAM, is seen as a promising candidate to replace or complement DRAM. The heat generated from a RESET programming pulse to a PCM cell can disturb the neighboring cells which are not being programmed. Write disturbance (WD) poses a critical reliability challenge in high-density PCM memory with scaling below 20nm process technology node. Increasing the intra-cell space can eliminate the WD, however, it reduces the storage density which counteracts the benefits of scalability in PCM. At architectural level, a verify and correct (VnC) technique can be used to address this problem. However, this leads to an increased number of write operations, thus degrading performance, energy efficiency and memory lifetime. Due to its dependence on the type of programming operation and the state of the neighboring cell, WD is a data-dependent problem. Exploiting this property, encoding techniques have been proposed to reduce the frequency of WD-vulnerable data patterns. These techniques, however, do not eliminate the WD in an array and ultimately rely on the VnC method to ensure reliable memory operation. This article introduces a novel architecture, based on encoding and multi-level programming characteristics of PCM, to eliminate the intra-array WD in PCM. By eliminating WD and hence the need for a VnC operation, the proposed architecture improves performance, energy efficiency and memory lifetime. Our evaluation of the proposed architecture shows an average reduction of 57 percent in the number of writes (to service one write request) over the existing state-of-the-art intra-array WD-mitigation technique. Depending on the PCM write bandwidth, the proposed architecture can reduce the write service time by up to 27 percent, on average, compared to the existing best-performing technique. This leads to an average improvement of 15 percent in IPC. Additionally, by eliminating the overhead of a verify operation, the write energy efficiency is also improved by 8 percent over the previous art. Finally, with an average reduction of 26 percent in bit flips, the proposed method also improves the memory lifetime. The proposed method is also proven to be effective when considering WD both within the word-lines and across the bit-lines.},
  archive      = {J_TC},
  author       = {Muhammad Imran and Taehyun Kwon and Nur A. Touba and Joon-Sung Yang},
  doi          = {10.1109/TC.2021.3068577},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {992-1007},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CEnT: An efficient architecture to eliminate intra-array write disturbance in PCM},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Elastic bloom filter: Deletable and expandable filter using
elastic fingerprints. <em>TC</em>, <em>71</em>(4), 984–991. (<a
href="https://doi.org/10.1109/TC.2021.3067713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bloom filter, answering whether an item is in a set, has achieved great success in various fields, including networking, databases, and bioinformatics. However, the Bloom filter has two main shortcomings: no support of item deletion and no support of expansion. Existing solutions either support deletion at the cost of using additional memory, or support expansion at the cost of increasing the false positive rate and decreasing the query speed. Unlike existing solutions, we propose the Elastic Bloom filter (EBF) to address the two shortcomings simultaneously. Importantly, when EBF expands, the false positives decrease. Our key technique is Elastic Fingerprints , which dynamically absorb and release bits during compression and expansion. To support deletion, EBF can first delete the corresponding fingerprint and then update the corresponding bit in the Bloom filter. To support expansion, Elastic Fingerprints release bits and insert them to the Bloom filter. Our experimental results show that the Elastic Bloom filter significantly outperforms existing works.},
  archive      = {J_TC},
  author       = {Yuhan Wu and Jintao He and Shen Yan and Jianyu Wu and Tong Yang and Olivier Ruas and Gong Zhang and Bin Cui},
  doi          = {10.1109/TC.2021.3067713},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {984-991},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Elastic bloom filter: Deletable and expandable filter using elastic fingerprints},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). L4L: Experience-driven computational resource control in
federated learning. <em>TC</em>, <em>71</em>(4), 971–983. (<a
href="https://doi.org/10.1109/TC.2021.3068219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the large-scale deployment of machine learning applications, there is much research attention on exploiting a vast amount of data stored on mobile clients. To preserve data privacy, federated learning has been proposed to enable large-scale machine learning by massive clients without exposing raw data. Existing works of federated learning struggle for accelerating the learning process, but ignore the energy efficiency that is critical for resource-constrained clients. In this article, we propose to improve the energy efficiency of federated learning by lowering CPU cycle frequencies of clients who are faster in the training group. Based on this idea, we formulate an optimization problem aiming to minimize the total system cost defined as a weighted sum of learning time and energy consumption. Due to the hardness of the formulated optimization problem and unpredictability of network quality, we propose L4L (Learning for Learning), an experience-driven computational resource control approach based on the deep reinforcement learning, which can derive the near-optimal solution with only the clients’ bandwidth information in the previous training rounds. We conduct the experiments using both real-world traces and synthetic traces to evaluate the proposed L4L approach. The results demonstrate the superiority of L4L as compared with the state-of-the-art solutions.},
  archive      = {J_TC},
  author       = {Yufeng Zhan and Peng Li and Leijie Wu and Song Guo},
  doi          = {10.1109/TC.2021.3068219},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {971-983},
  shortjournal = {IEEE Trans. Comput.},
  title        = {L4L: Experience-driven computational resource control in federated learning},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enabling secure and space-efficient metadata management in
encrypted deduplication. <em>TC</em>, <em>71</em>(4), 959–970. (<a
href="https://doi.org/10.1109/TC.2021.3067326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Encrypted deduplication combines encryption and deduplication in a seamless way to provide confidentiality guarantees for the physical data in deduplicated storage, yet it incurs substantial metadata storage overhead due to the additional storage of keys. We present a new encrypted deduplication storage system called ${\sf Metadedup}$ , which suppresses metadata storage by also applying deduplication to metadata. Its idea builds on indirection, which adds another level of metadata chunks that record metadata information. We find that metadata chunks are highly redundant in real-world workloads and hence can be effectively deduplicated. We further extend ${\sf Metadedup}$ to incorporate multiple servers via a distributed key management approach, so as to provide both fault-tolerant storage and security guarantees. We extensively evaluate ${\sf Metadedup}$ from performance and storage efficiency perspectives. We show that ${\sf Metadedup}$ achieves high throughput in writing and restoring files, and saves the metadata storage by up to 93.94 percent for real-world backup workloads.},
  archive      = {J_TC},
  author       = {Jingwei Li and Suyu Huang and Yanjing Ren and Zuoru Yang and Patrick P. C. Lee and Xiaosong Zhang and Yao Hao},
  doi          = {10.1109/TC.2021.3067326},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {959-970},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling secure and space-efficient metadata management in encrypted deduplication},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the reliability of FeFET on-chip memory. <em>TC</em>,
<em>71</em>(4), 947–958. (<a
href="https://doi.org/10.1109/TC.2021.3066899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ferroelectric Field-Effect Transistor (FeFET) is a promising future technology for non-volatile on-chip memories. It is rapidly attracting an ever-increasing attention from industry. The key advantage of FeFETs is full compatibility with the existing CMOS fabrication process beside their very low power consumption. To enable ultra-dense memories, 1-FeFET AND Arrays were proposed in which a memory cell is formed from merely a single FeFET. All access transistors, which are traditionally needed to operate memory cells, are removed. However, this imposes a new challenge of indirect write disturbances . Neighboring memory cells are indirectly degraded whenever a direct write operation occurs to a particular FeFET cell. Only recently the impact of such indirect disturbances on the FeFET reliability was experimentally investigated at device (i.e., transistor) level. However, to explore and properly judge the feasibility of 1-FeFET AND Arrays for on-chip memories, investigating only the reliability of individual cells is indeed insufficient. Bridging the gap between the device level and system (i.e., chip) level is inevitable. In the presence of indirect disturbances, the position of a write access within the array plays a key role, which is governed by the running workloads. In addition, whether the write operation flips the previously stored value or not also plays an important role with regards to reliability. Hence, running workloads, which determine not only the position of the memory cells to be written but also the values written to them, plays an essential role in determining 1-FeFET AND Array reliability over time. Therefore, studying the reliability of FeFETs only at the device level (as done in state of the art) is insufficient. In this work, we investigate, for the first time, the reliability of FeFET memories from device to system level. To achieve that, we develop a unified model capturing the impact of both indirect disturbances and direct writes on the reliability of FeFET cells. Our study at system level then employs the unified model in the context of application workloads. We investigate different array sizes, write voltages, write methods and a wide range of workloads using the example of CPU caches as an example of on-chip memory. We demonstrate that indirect write disturbances are the dominate effect degrading the reliability of FeFET memories. For most cells, it contributes over 90 percent to the overall induced degradation. This provides guidelines for researchers at both device and circuit level to optimize the FeFET reliability further while considering the hidden impact of indirect write disturbances.},
  archive      = {J_TC},
  author       = {Paul R. Genssler and Victor M. van Santen and Jörg Henkel and Hussam Amrouch},
  doi          = {10.1109/TC.2021.3066899},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {947-958},
  shortjournal = {IEEE Trans. Comput.},
  title        = {On the reliability of FeFET on-chip memory},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient memory arbitration in high-level synthesis from
multi-threaded code. <em>TC</em>, <em>71</em>(4), 933–946. (<a
href="https://doi.org/10.1109/TC.2021.3066466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-level synthesis (HLS) is an increasingly popular method for generating hardware from a description written in a software language like C/C++. Traditionally, HLS tools have operated on sequential code, however in recent years there has been a drive to synthesise multi-threaded code. In this context, a major challenge facing HLS tools is how to automatically partition memory among parallel threads to fully exploit the bandwidth available on an FPGA device and minimise memory contention. Existing partitioning approaches require inefficient arbitration circuitry to serialise accesses to each bank because they make conservative assumptions about which threads might access which memory banks. In this article, we design a static analysis that can prove certain memory banks are only accessed by certain threads, and use this analysis to simplify or even remove the arbiters while preserving correctness. We show how this analysis can be implemented using the Microsoft Boogie verifier on top of satisfiability modulo theories (SMT) solver, and propose a tool named EASY using automatic formal verification. Our work supports arbitrary input code with any irregular memory access patterns and indirect array addressing forms. We implement our approach in LLVM and integrate it into the LegUp HLS tool. For a set of typical application benchmarks our results have shown that EASY can achieve 0.13× (avg. 0.43×) of area and 1.64× (avg. 1.28×) of performance compared to the baseline, with little additional compilation time relative to the long time in hardware synthesis.},
  archive      = {J_TC},
  author       = {Jianyi Cheng and Shane T. Fleming and Yu Ting Chen and Jason Anderson and John Wickerson and George A. Constantinides},
  doi          = {10.1109/TC.2021.3066466},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {933-946},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient memory arbitration in high-level synthesis from multi-threaded code},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed detection of minimum cuts in wireless multi-hop
networks. <em>TC</em>, <em>71</em>(4), 919–932. (<a
href="https://doi.org/10.1109/TC.2021.3065527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communicating over multi-hop connections simplifies the establishment of wireless multi-hop networks but brings new challenges such as limited reliability, bottlenecks, and weak connections. The minimum cut of a graph is the smallest subset of edges whose removal disconnects some nodes from the others. Finding minimum cuts of a wireless multi-hop network may reveal useful information such as bottlenecks and critical areas. This article introduces a distributed algorithm for detecting minimum cuts of a given witless multi-hop network by finding available edge-disjoint paths. Initially, the paths between two arbitrary neighbors are detected and these nodes are grouped as visited nodes. Then, the other nodes are added to the visited group one by one by finding at most $O(n)$ paths in total where $n$ is the number of nodes. The comprehensive simulation results showed that the proposed asynchronous algorithm detects minimum cuts with up to 37.1 and 55.8 percent lower sent bytes than the existing synchronous and central algorithms, respectively.},
  archive      = {J_TC},
  author       = {Vahid Khalilpour Akram},
  doi          = {10.1109/TC.2021.3065527},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {919-932},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Distributed detection of minimum cuts in wireless multi-hop networks},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Impact of NCFET technology on eliminating the cooling cost
and boosting the efficiency of google TPU. <em>TC</em>, <em>71</em>(4),
906–918. (<a href="https://doi.org/10.1109/TC.2021.3065454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent breakthroughs in Neural Networks (NNs) led to significant accuracy improvements of several machine learning applications such as image classification and voice recognition. However, this accuracy improvement comes at the cost of an immense increase in computation demands. NNs became one of the most common and computationally intensive workloads in today&#39;s datacenters. To address these computational demands, Google announced in 2016 the Tensor Processing Unit (TPU), an advanced custom ASIC accelerator for NN inference. Two new TPU versions (v2 and v3) followed in 2017 and 2018 that support also training. Google TPUv3 packs an immense processing power ( $\mathrm{90TFLOPS}$ per chip) in a tiny and condensed area, leading to very high on-chip power densities and thus excessive temperature. In this article, superlattice thermoelectric cooling, which is one of the emerging on-chip cooling, is considered as an advanced cooling example for Google TPU and we investigate the impact of Negative Capacitance FET (NCFET), which is one of the recent emerging technologies, on the cooling and efficiency of TPU. Through full-chip design, of the computational core of the TPU, based on $14\mathrm{nm}$ Intel FinFET technology and multiphysics temperature simulations, we demonstrate that NCFET can significantly minimize the required cooling-cost. More than 4000 NCFET configurations are evaluated in order to traverse the entire design space defined by the thickness of the ferroelectric layer of NCFET, the operating voltage, cooling, and the operating frequency, in addition to all possible FinFET&#39;s configurations. Moreover, our experimental evaluation shows that by eliminating the cooling cost, NCFET delivers 2.8x higher efficiency compared to the conventional FinFET baseline.},
  archive      = {J_TC},
  author       = {Sami Salamin and Georgios Zervakis and Florian Klemme and Hammam Kattan and Yogesh Chauhan and Jörg Henkel and Hussam Amrouch},
  doi          = {10.1109/TC.2021.3065454},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {906-918},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Impact of NCFET technology on eliminating the cooling cost and boosting the efficiency of google TPU},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting outlier machine instances through gaussian mixture
variational autoencoder with one dimensional CNN. <em>TC</em>,
<em>71</em>(4), 892–905. (<a
href="https://doi.org/10.1109/TC.2021.3065073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today&#39;s large datacenters house a massive number of machines, each of which is being closely monitored with multivariate time series (e.g., CPU idle, memory utilization) to ensure service quality. Detecting outlier machine instances with multivariate time series is crucial for service management. However, it is a challenging task due to the multiple classes and various shapes, high dimensionality, and lack of labels of multivariate time series. In this article, we propose DOMI, a novel unsupervised model that combines Gaussian mixture VAE with 1D-CNN, to d etect o utlier m achine i nstances. Its core idea is to capture the normal patterns of machine instances by learning their latent representations that consider the shape characteristics, reconstruct input data by the learned representations, and apply reconstruction probabilities to determine outliers. Moreover, DOMI interprets the detected outlier instance based on the reconstruction probability changes of univariate time series. Extensive experiments have been conducted on the dataset collected from 1821 machines with a 1.5-month-period, which are deployed in ByteDance, a top global content service provider. DOMI achieves the best F1-Score of 0.94 and AUC score of 0.99, significantly outperforming the best performing baseline method by 0.08 and 0.03, respectively. Moreover, its interpretation accuracy is up to 0.93.},
  archive      = {J_TC},
  author       = {Ya Su and Youjian Zhao and Ming Sun and Shenglin Zhang and Xidao Wen and Yongsu Zhang and Xian Liu and Xiaozhou Liu and Junliang Tang and Wenfei Wu and Dan Pei},
  doi          = {10.1109/TC.2021.3065073},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {892-905},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Detecting outlier machine instances through gaussian mixture variational autoencoder with one dimensional CNN},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). General reuse-centric CNN accelerator. <em>TC</em>,
<em>71</em>(4), 880–891. (<a
href="https://doi.org/10.1109/TC.2021.3064608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces the first general reuse-centric accelerator for CNN inferences. Unlike prior work that exploits similarities only across consecutive video frames, general reuse-centric accelerator is able to discover similarities among arbitrary patches within an image or across independent images, and translate them into computation time and energy savings. Experiments show that the accelerator complements both prior software-based CNN and various CNN hardware accelerators, producing up to 14.96X speedups for similarity discovery, up to 2.70X speedups for overall inference.},
  archive      = {J_TC},
  author       = {Nihat Mert Cicek and Lin Ning and Ozcan Ozturk and Xipeng Shen},
  doi          = {10.1109/TC.2021.3064608},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {880-891},
  shortjournal = {IEEE Trans. Comput.},
  title        = {General reuse-centric CNN accelerator},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward QoS-awareness and improved utilization of spatial
multitasking GPUs. <em>TC</em>, <em>71</em>(4), 866–879. (<a
href="https://doi.org/10.1109/TC.2021.3064352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datacenters use GPUs to provide the significant computing throughput required by emerging user-facing services. The diurnal user access pattern of user-facing services provides a strong incentive to co-located applications for better GPU utilization, and prior work has focused on enabling co-location on multicore processors and traditional non-preemptive GPUs. However, current GPUs are evolving towards spatial multitasking and introduce a new set of challenges to eliminate QoS violations. To address this open problem, we explore the underlying causes of QoS violation on spatial multitasking GPUs. In response to these causes, we propose C-Laius, a runtime system that carefully allocates the computation resource to co-located applications for maximizing the throughput of batch applications while guaranteeing the required QoS of user-facing services. C-Laius not only allows co-locating one user-facing application with multiple batch applications, but also supports the co-location of multiple user-facing applications with batch applications. In the case of a single co-located user-facing application, our evaluation on an Nvidia RTX 2080Ti GPU shows that C-Laius improves the utilization of spatial multitasking GPUs by 20.8 percent, while achieving the 99\%-ile latency target for user-facing services. As to the case of multiple co-located user-facing applications, C-Laius ensures no violation of QoS while improving the accelerator utilization by 35.9 percent on average.},
  archive      = {J_TC},
  author       = {Wei Zhang and Quan Chen and Ningxin Zheng and Weihao Cui and Kaihua Fu and Minyi Guo},
  doi          = {10.1109/TC.2021.3064352},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {866-879},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Toward QoS-awareness and improved utilization of spatial multitasking GPUs},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sim-d: A SIMD accelerator for hard real-time systems.
<em>TC</em>, <em>71</em>(4), 851–865. (<a
href="https://doi.org/10.1109/TC.2021.3064290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging safety-critical systems require high-performance data-parallel architectures and, problematically, ones that can guarantee tight and safe worst-case execution times. Given the complexity of existing architectures like GPUs, it is unlikely that sufficiently accurate models and algorithms for timing analysis will emerge in the foreseeable future. This motivates our work on Sim-D, a clean-slate approach to designing a real-time data-parallel architecture. Sim-D enforces a predictable execution model by isolating compute- and access resources in hardware. The DRAM controller uninterruptedly transfers tiles of data, requested by entire work-groups. This permits work-groups to be executed as a sequence of deterministic access- and compute phases, scheduling phases from up to two work-groups in parallel. Evaluation using a cycle-accurate timing model shows that Sim-D can achieve performance on par with an embedded-grade NVIDIA TK1 GPU under two conditions: applications refrain from using indirect DRAM transfers into large buffers, and Sim-D&#39;s scratchpads provide sufficient bandwidth. Sim-D&#39;s design facilitates derivation of safe WCET bounds that are tight within 12.7 percent on average, at an additional average performance penalty of $\sim$ 9.2 percent caused by scheduling restrictions on phases.},
  archive      = {J_TC},
  author       = {Roy Spliet and Robert D. Mullins},
  doi          = {10.1109/TC.2021.3064290},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {851-865},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Sim-D: A SIMD accelerator for hard real-time systems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PyQUBO: Python library for mapping combinatorial
optimization problems to QUBO form. <em>TC</em>, <em>71</em>(4),
838–850. (<a href="https://doi.org/10.1109/TC.2021.3063618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present PyQUBO, an open-source Python library for constructing quadratic unconstrained binary optimizations (QUBOs) from the objective functions and the constraints of optimization problems. PyQUBO enables users to prepare QUBOs or Ising models for various combinatorial optimization problems with ease thanks to the abstraction of expressions and the extensibility of the program. QUBOs and Ising models formulated using PyQUBO are solvable by Ising machines, including quantum annealing machines. We introduce the features of PyQUBO with applications in the number partitioning problem, knapsack problem, graph coloring problem, and integer factorization using a binary multiplier. Moreover, we demonstrate how PyQUBO can be applied to production-scale problems through integration with quantum annealing machines. Through its flexibility and ease of use, PyQUBO has the potential to make quantum annealing a more practical tool among researchers.},
  archive      = {J_TC},
  author       = {Mashiyat Zaman and Kotaro Tanahashi and Shu Tanaka},
  doi          = {10.1109/TC.2021.3063618},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {838-850},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PyQUBO: Python library for mapping combinatorial optimization problems to QUBO form},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enabling large-scale simulation of CAM on the sunway
TaihuLight supercomputer. <em>TC</em>, <em>71</em>(4), 824–837. (<a
href="https://doi.org/10.1109/TC.2021.3063422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Community Atmosphere Model (CAM) has been ported, redesigned, and scaled to the full system of the Sunway TaihuLight, and provides peta-scale climate modeling performance. Based on a novel domain decomposition method, we have fully optimized the complete model code by using both OpenACC refactoring and more aggressive and finer-grained Athread approaches. The Athread approach enables us to achieve exceptional memory control and usage, efficient vectorization, and sophisticated utilization of the thread-level communication mechanism. We have also further refined the load-balance behaviors towards ultra-large-scale numerical simulation. By combining all these novelties, we achieved a simulation speed of 7.2 and 25.6 simulation-year-per-day (SYPD) for global 25-km and 100-km resolution, respectively (1.2- to 2.2-fold improvements over previous efforts), and a sustainable double-precision performance of 3.3 PFlops for a 750-m global simulation when using 10075000 cores.},
  archive      = {J_TC},
  author       = {Yuxuan Li and Xiaohui Duan and Lin Gan and Wubing Wan and Yuhu Chen and Kai Xu and Jinzhe Yang and Weiguo Liu and Wei Xue and Haohuan Fu and Guangwen Yang},
  doi          = {10.1109/TC.2021.3063422},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {824-837},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling large-scale simulation of CAM on the sunway TaihuLight supercomputer},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OSC: An online self-configuring big data framework for
optimization of QoS. <em>TC</em>, <em>71</em>(4), 809–823. (<a
href="https://doi.org/10.1109/TC.2021.3063278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big-data frameworks such as MapReduce/Hadoop or Spark have many performance-critical configuration parameters which may interact with each other in a complex way. Their optimal values for an application on a given cluster are affected by not only the application itself but also its input data. This makes offline auto-configuration approaches hard to be used in practice because the input data of an application may change at each run. To address this issue, we propose an Online Self-Configuring (OSC) approach that automatically determines the optimal parameter values for a given application. OSC synergistically integrates three key techniques. First, OSC leverages ensemble learning to build a precise performance model for a given application. Second, it quantifies the importance of the parameters and interaction intensity between them to accelerate the genetic algorithm for searching optimal configuration parameters. Third, OSC supports an incremental modeling approach to achieve low overhead of the models for online needs. These techniques allow OSC to effectively learn the characteristics of an application and optimize its performance by automatically adjusting the configurations at runtime. Our implementation of OSC atop MapReduce/Hadoop 2.6 improves performance by 60 percent on average and up to 120 percent compared with the state-of-the-art approach. Lastly, the performance benefit of an application running on OSC generally increases along with its input data size.},
  archive      = {J_TC},
  author       = {Zhendong Bei and Nam Sung Kim and Kai HWang and Zhibin Yu},
  doi          = {10.1109/TC.2021.3063278},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {809-823},
  shortjournal = {IEEE Trans. Comput.},
  title        = {OSC: An online self-configuring big data framework for optimization of QoS},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rare computing: Removing redundant multiplications from
sparse and repetitive data in deep neural networks. <em>TC</em>,
<em>71</em>(4), 795–808. (<a
href="https://doi.org/10.1109/TC.2021.3063269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research shows that 4-bit data precision is sufficient for Deep Neural Network (DNN) inference without accuracy degradation. Due to the low bit-width, a large amount of data is repeated. In this article, we propose a hardware architecture, named Rare Computing Architecture (RCA), that skips redundant computations due to repetitive data in the networks. By exploiting redundancy, RCA is not significantly affected by data-sparsity and maintains great improvements in performance and energy efficiency, while the improvements of existing DNN accelerators are vulnerable to variations in sparsity. In the RCA, repeated data in a window for censoring repetition are detected by a Redundancy Censoring Unit (RCU) and processed at a time, achieving high effective throughput. Additionally, we present a dataflow that exploits abundant data-reusability in DNNs, which enables the high-throughput computations to be ceaselessly performed without an increase of bandwidth for data-read. The proposed architecture is evaluated in two ways of exploiting weight- and activation-repetition. In the evaluation, RCA is compared to a value-agnostic computation and UCNN that is the state-of-the-art accelerator exploiting weight-repetition. Additionally, RCA is compared to Bit-pragmatic that exploits bit-level sparsity. Both evaluations demonstrate that the RCA shows steadily high improvements in performance and energy-efficiency.},
  archive      = {J_TC},
  author       = {Kangkyu Park and Seungkyu Choi and Yeongjae Choi and Lee-Sup Kim},
  doi          = {10.1109/TC.2021.3063269},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {795-808},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Rare computing: Removing redundant multiplications from sparse and repetitive data in deep neural networks},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heterogeneity-aware gradient coding for tolerating and
leveraging stragglers. <em>TC</em>, <em>71</em>(4), 779–794. (<a
href="https://doi.org/10.1109/TC.2021.3063180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed gradient descent has been widely adopted in the machine learning field because considerable computing resources are available when facing the huge volume of data. Specifically, the gradient over the whole data is cooperatively computed by multiple workers. However, its performance can be severely affected by slow workers, namely stragglers. Recently, coding-based approaches have been introduced to mitigate the straggler problem, but they could hardly deal with the heterogeneity among workers. Besides, they always discard the results of stragglers causing huge resource waste. In this article, we first investigate how to tolerate stragglers by discarding their results and then seek to leverage the stragglers. For tolerating stragglers, we propose a heterogeneity-aware coding scheme that encodes gradients adaptive to the computing capability of workers. Theoretically, this scheme is optimal for stragglers tolerance. Relying on the scheme, we further propose an algorithm called DHeter-aware to exploit the gradients of stragglers which we called delayed gradients. Moreover, theoretical results characterized for DHeter-aware exhibits the same convergence rate as the gradient descent without delayed gradients. Experiments on various tasks and clusters demonstrate that our coding scheme outperforms all the state-of-the-art methods and the DHeter-aware further accelerates the coding scheme by achieving 25 percent time savings.},
  archive      = {J_TC},
  author       = {Haozhao Wang and Song Guo and Bin Tang and Ruixuan Li and Yutong Yang and Zhihao Qu and Yi Wang},
  doi          = {10.1109/TC.2021.3063180},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {779-794},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Heterogeneity-aware gradient coding for tolerating and leveraging stragglers},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A RISC-v ISA extension for ultra-low power IoT wireless
signal processing. <em>TC</em>, <em>71</em>(4), 766–778. (<a
href="https://doi.org/10.1109/TC.2021.3063027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an instruction-set extension to the open-source RISC-V ISA (RV32IM) dedicated to ultra-low power (ULP) software-defined wireless IoT transceivers. The custom instructions are tailored to the needs of 8/16/32-bit integer complex arithmetic typically required by quadrature modulations. The proposed extension occupies only two major opcodes and most instructions are designed to come at a near-zero energy cost. Both an instruction accurate (IA) and a cycle accurate (CA) model of the new architecture are used to evaluate six IoT baseband processing test benches including FSK demodulation and LoRa preamble detection. Simulation results show cycle count improvements from 19 to 68 percent. Post synthesis simulations for a target 22nm FD-SOI technology show less than 1 percent power and 28 percent area overheads, respectively, relative to a baseline RV32IM design. Power simulations show a peak power consumption of 380 µW for Bluetooth LE demodulation and 225 µW for LoRa preamble detection (BW = 500 kHz, SF = 11).},
  archive      = {J_TC},
  author       = {Hela Belhadj Amor and Carolynn Bernier and Zdeněk Přikryl},
  doi          = {10.1109/TC.2021.3063027},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {766-778},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A RISC-V ISA extension for ultra-low power IoT wireless signal processing},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Building and checking suffix array simultaneously by induced
sorting method. <em>TC</em>, <em>71</em>(4), 756–765. (<a
href="https://doi.org/10.1109/TC.2021.3061709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many efficient open-source suffix sorters using the induced sorting (IS) method to build the fundamental data structure suffix array (SA) for compressing and indexing data have been proposed. To avoid potential faults caused by possible implementation bugs, checking the output SA from any IS sorter without engineering warranty for correctness is a de-facto process. The existing SA checkers commonly perform checking after an SA is built completely, with significant time and space complexities compared with that of builders. This article proposes an efficient solution for building and checking SA simultaneously by enhancing the original IS method with a checking scheme using hash computations to on-the-fly verify the results produced by the last induction phase of IS method. Given an input of constant alphabet, this checking scheme requires linear time and constant RAM space when running on external memory, and its time and space overheads are negligible compared with that for building SA. In our experiments on real-world data, the proposed methods take advantages over the counterparts of existing SA checkers by running faster with less space. This work can help provide a value-added bonus feature for open-source IS sorters to guarantee the correctness of a built SA, and such a feature should be desirable for applications using these sorters.},
  archive      = {J_TC},
  author       = {Bin Lao and Yi Wu and Ge Nong and Wai Hong Chan},
  doi          = {10.1109/TC.2021.3061709},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {756-765},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Building and checking suffix array simultaneously by induced sorting method},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Concurrent application bias scheduling for energy efficiency
of heterogeneous multi-core platforms. <em>TC</em>, <em>71</em>(4),
743–755. (<a href="https://doi.org/10.1109/TC.2021.3061558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimizing energy consumption of concurrent applications on heterogeneous multi-core platforms is challenging given the diversity in energy-performance profiles of both the applications and hardware. Adaptive learning techniques made the exhaustive Pareto-optimal space exploration practically feasible to identify an energy efficient configuration. Existing approaches consider a single application&#39;s characteristic for optimizing energy consumption. However, an optimal configuration for a given application in isolation may not be optimal when other applications are run concurrently. Approaches that consider concurrent application scenarios overlook the weight of total energy consumption per application, restricting them from prioritizing among applications. We address this limitation by considering the mutual effect of concurrent applications on system wide energy consumption to adapt resource configuration at run-time. We characterize each application&#39;s power-performance profile as a weighted bias through off-line profiling. We infer this model combined with an on-line predictive strategy to make resource allocation decisions for minimizing energy consumption while honoring performance requirements. The proposed strategy is implemented as a user-space process and evaluated on a heterogeneous hardware platform of Odroid XU3 over the Rodinia benchmark suite. Experimental results show up to 61 percent of energy saving compared to the standard baseline of Linux governors and up to 27 percent of energy gain compared to state-of-the-art adaptive learning-based resource management techniques.},
  archive      = {J_TC},
  author       = {Elham Shamsa and Anil Kanduri and Pasi Liljeberg and Amir M. Rahmani},
  doi          = {10.1109/TC.2021.3061558},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {743-755},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Concurrent application bias scheduling for energy efficiency of heterogeneous multi-core platforms},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient CRT-based bit-parallel multiplier for special
pentanomials. <em>TC</em>, <em>71</em>(3), 736–742. (<a
href="https://doi.org/10.1109/TC.2021.3058346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Chinese remainder theorem (CRT)-based multiplier is a new type of hybrid bit-parallel multiplier, which can achieve nearly the same time complexity compared with the fastest multiplier known to date with reduced space complexity. However, the current CRT-based multipliers are only applicable to trinomials. In this article, we propose an efficient CRT-based bit-parallel multiplier for a special type of pentanomial $x^m+x^{m-k}+x^{m-2k}+x^{m-3k}+1, 5k+1&amp;lt;m\leq 11k$ . Through transforming the non-constant part $x^m+x^{m-k}+x^{m-2k}+x^{m-3k}$ into a binomial, we can obtain relatively simpler quotient and remainder computations, which lead to faster implementation with reduced space complexity compared with classic quadratic multipliers for the same pentanomials. Moreover, for some $m$ , our proposal can match the fastest multipliers for irreducible Type I, Type II, and Type C.1 pentanomials of the same degree, but space complexities are roughly reduced by 8 percent.},
  archive      = {J_TC},
  author       = {Yin Li and Xinyuan Cui and Yu Zhang},
  doi          = {10.1109/TC.2021.3058346},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {736-742},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An efficient CRT-based bit-parallel multiplier for special pentanomials},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast en/decoding of reed-solomon codes for failure recovery.
<em>TC</em>, <em>71</em>(3), 724–735. (<a
href="https://doi.org/10.1109/TC.2021.3060701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reed-Solomon (RS) codes are used in many storage systems for failure recovery. In popular software implementations, RS codes are defined by using a parity check matrix that is either a Cauchy matrix padded with an identity or a Vandermonde matrix. The encoding complexity can be reduced by searching for a Cauchy matrix that has a smaller number of ‘1&#39;s in its bit matrices or exploiting Reed-Muller (RM) transform in the Vandermonde matrix multiplication. This article proposes two new approaches that improve upon the previous schemes. In our first approach, different constructions of finite fields are explored to further reduce the number of ‘1&#39;s in the bit matrices of the Cauchy matrix and a new searching method is developed to find the matrices with minimum number of ‘1&#39;s. Our second approach defines RS codes using a parity check matrix in the format of a Vandermonde matrix concatenated with an identity matrix so that the multiplication with the inverse erasure columns in the encoding is eliminated and the decoding can be carried out using simplified formulas. The Vandermonde matrix in such an unconventional RS code definition needs to be constructed using finite field elements in non-consecutive order. A modification is also developed in this article to enable the application of the RM transform in this case to reduce the matrix multiplication complexity. For 4-erasure-correcting RS codes over $GF(2^8)$ , the two proposed approaches increase the encoding throughput by 40 and 15 percent on average over the prior works based on Cauchy matrix and Vandermonde matrix with RM transform, respectively, for a range of codeword length. Moreover, the decoding throughput is also significantly improved.},
  archive      = {J_TC},
  author       = {Yok Jye Tang and Xinmiao Zhang},
  doi          = {10.1109/TC.2021.3060701},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {724-735},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Fast En/Decoding of reed-solomon codes for failure recovery},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient 4-way vectorizations of the montgomery ladder.
<em>TC</em>, <em>71</em>(3), 712–723. (<a
href="https://doi.org/10.1109/TC.2021.3060505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose two new algorithms for 4-way vectorization of the well known Montgomery ladder over elliptic curves of Montgomery form. The first algorithm is suitable for variable base scalar multiplication. In comparison to the previous work by Hisil et al. [17] , it eliminates a number of non-multiplication operations at the cost of a single multiplication by a curve constant. Implementation results show this trade-off to be advantageous. The second algorithm is suitable for fixed base scalar multiplication and provides clear speed improvement over a previous vectorization strategy due to Costigan and Schwabe (2009). The well known Montgomery curves Curve25519 and Curve448 are part of the TLS protocol, version 1.3. For these two curves, we provide constant time assembly implementations of the new algorithms. Additionally, for the algorithm of Hisil et al. [17] , we provide improved implementations for Curve25519 and new implementation for Curve448. Timings results on the Haswell and Skylake processors indicate that in practice the new algorithms are to be preferred over previous methods for scalar multiplication on these curves.},
  archive      = {J_TC},
  author       = {Kaushik Nath and Palash Sarkar},
  doi          = {10.1109/TC.2021.3060505},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {712-723},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient 4-way vectorizations of the montgomery ladder},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Homing sequence derivation with quantified boolean
satisfiability. <em>TC</em>, <em>71</em>(3), 696–711. (<a
href="https://doi.org/10.1109/TC.2021.3058302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Homing sequence derivation for nondeterministic finite state machines (NFSMs) has important applications in software/hardware system testing and verification. Unlike prior methods based on explicit tree-based search, in this article we formulate the derivation of a preset/adaptive homing sequence in terms of quantified Boolean formula (QBF) solving. This formulation exploits compact circuit representation of NFSMs and QBF encoding of the existence condition of homing sequence for effective computation. The implicit circuit representation effectively avoids explicit state enumeration, and can be more scalable. Different encoding schemes and QBF solvers are evaluated for their suitability for the homing sequence derivation. Experiments on various computation methods and benchmarks show the generality and feasibility of a proposed approach.},
  archive      = {J_TC},
  author       = {Kuan-Hua Tu and Hung-En Wang and Jie-Hong R. Jiang and Natalia Kushik and Nina Yevtushenko},
  doi          = {10.1109/TC.2021.3058302},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {696-711},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Homing sequence derivation with quantified boolean satisfiability},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Selective neuron re-computation (SNRC) for error-tolerant
neural networks. <em>TC</em>, <em>71</em>(3), 684–695. (<a
href="https://doi.org/10.1109/TC.2021.3056992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Neural networks (ANNs) are widely used to solve classification problems for many machine learning applications. When errors occur in the computational units of an ANN implementation due to for example radiation effects, the result of an arithmetic operation can be changed, and therefore, the predicted classification class may be erroneously affected. This is not acceptable when ANNs are used in many safety-critical applications, because the incorrect classification may result in a system failure. Existing error-tolerant techniques usually rely on physically replicating parts of the ANN implementation or incurring in a significant computation overhead. Therefore, efficient protection schemes are needed for ANNs that are run on a processor and used in resource-limited platforms. A technique referred to as Selective Neuron Re-Computation (SNRC), is proposed in this paper. As per the ANN structure and algorithmic properties, SNRC can identify the cases in which the errors have no impact on the outcome; therefore, errors only need to be handled by re-computation when the classification result is detected as unreliable. Compared with existing temporal redundancy-based protection schemes, SNRC saves more than 60 percent of the re-computation (more than 90 percent in many cases) overhead to achieve complete error protection as assessed over a wide range of datasets. Different activation functions are also evaluated.},
  archive      = {J_TC},
  author       = {Shanshan Liu and Pedro Reviriego and Fabrizio Lombardi},
  doi          = {10.1109/TC.2021.3056992},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {684-695},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Selective neuron re-computation (SNRC) for error-tolerant neural networks},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient software implementation of the SIKE protocol using
a new data representation. <em>TC</em>, <em>71</em>(3), 670–683. (<a
href="https://doi.org/10.1109/TC.2021.3057331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to relatively small public and secret keys, the Supersingular Isogeny Key Encapsulation (SIKE) protocol made it into the third evaluation round of the post-quantum standardization project of the National Institute of Standards and Technology (NIST). Even though a large body of research has been devoted to the efficient implementation of SIKE, its latency is still undesirably long for many real-world applications. Most existing implementations of the SIKE protocol use the Montgomery representation for the underlying field arithmetic since the corresponding reduction algorithm is considered the fastest method for performing multiple-precision modular reduction. In this paper, we propose a new data representation for supersingular isogeny-based Elliptic-Curve Cryptography (ECC), of which SIKE is a sub-class. This new representation enables significantly faster implementations of modular reduction than the Montgomery reduction, and also other finite-field arithmetic operations used in ECC can benefit from our data representation. We implemented all arithmetic operations in C using the proposed representation such that they have constant execution time and integrated them to the latest version of the SIKE software library. Using four different parameters sets, we benchmarked our design and the optimized generic implementation on a 2.6 GHz Intel Xeon E5-2690 processor. Our results show that, for the prime of SIKEp751, the proposed reduction algorithm is approximately 2.61 times faster than the currently best implementation of Montgomery reduction, and our representation also enables significantly better timings for other finite-field operations. Due to these improvements, we were able to achieve a speed-up by a factor of about 1.65, 2.03, 1.61, and 1.48 for SIKEp751, SIKEp610, SIKEp503, and SIKEp434, respectively, compared to state-of-the-art generic implementations.},
  archive      = {J_TC},
  author       = {Jing Tian and Piaoyang Wang and Zhe Liu and Jun Lin and Zhongfeng Wang and Johann Großschädl},
  doi          = {10.1109/TC.2021.3057331},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {670-683},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient software implementation of the SIKE protocol using a new data representation},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonlinear code-based low-overhead fine-grained control flow
checking. <em>TC</em>, <em>71</em>(3), 658–669. (<a
href="https://doi.org/10.1109/TC.2021.3057132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A hardware-based control flow monitoring technique enables the detection of errors in both the control flow and the instruction stream executed on a processor. However, as shown in recent publications, these techniques fail to detect malicious carefully-tuned manipulations of the instruction stream in a basic block. This article presents a non-linear encoder and checker that can cope with this weakness. It is a MAC based control flow checker that has the advantage of working with basic blocks of variable length, can detect every error, and performs the computation in real-time. The architecture can easily be modified to support different signature size and error masking probabilities.},
  archive      = {J_TC},
  author       = {Gilad Dar and Giorgio Di Natale and Osnat Keren},
  doi          = {10.1109/TC.2021.3057132},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {658-669},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Nonlinear code-based low-overhead fine-grained control flow checking},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MAGIC: Making IMR-based HDD perform like CMR-based HDD.
<em>TC</em>, <em>71</em>(3), 643–657. (<a
href="https://doi.org/10.1109/TC.2021.3059770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past decades have witnessed the tremendous success of Conventional Magnetic Recording (CMR)-based Hard Disk Drives (HDDs) in data storage. To eliminate the bottleneck of CMR-based HDDs in providing higher areal density, an emerging Interlaced Magnetic Recording (IMR) is capable of achieving higher areal density with limited changes to disk makeup. Nevertheless, existing approaches for IMR-based HDDs may suffer serious read and write performance degradation as compared with CMR-based HDDs. Thus, this article presents a device-level solution, namely MAGIC translation layer, which aims at MA kin G I MR-based HDDs perform like C MR-based HDDs in terms of comparable access performance. Specifically, not merely trying to improve the performance of raw IMR-based HDDs, this work, for the first time, moves one step forward to minimize the performance gap between IMR and CMR-based HDDs. Technically, by 1) fully utilizing two special CMR-like potentials of IMR and 2) gracefully trading the sequential access performance as space usage increases, MAGIC minimizes track rewriting overheads to achieve CMR-like performance. Our results reveal that MAGIC not only improves the write performance compared with existing designs, but also has potential to approach read and write performance of CMR-based HDD.},
  archive      = {J_TC},
  author       = {Yuhong Liang and Ming-Chang Yang and Shuo-Han Chen},
  doi          = {10.1109/TC.2021.3059770},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {643-657},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MAGIC: Making IMR-based HDD perform like CMR-based HDD},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lime: Low-cost and incremental learning for dynamic
heterogeneous information networks. <em>TC</em>, <em>71</em>(3),
628–642. (<a href="https://doi.org/10.1109/TC.2021.3057082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the interconnected relationships of large-scale information networks like social, scholar and Internet of Things networks is vital for tasks like recommendation and fraud detection. The vast majority of the real-world networks are inherently heterogeneous and dynamic, containing many different types of nodes and edges and can change drastically over time. The dynamicity and heterogeneity make it extremely challenging to reason about the network structure. Unfortunately, existing approaches are inadequate in modeling real-life dynamical networks as they either have strong assumption of a given stochastic process or fail to capture the heterogeneity of network structure, and they all require extensive computational resources. We introduce Lime , a better approach for modeling dynamic and heterogeneous information networks. Lime is designed to extract high-quality network representation with significantly lower memory resources and computational time over the state-of-the-arts. Unlike prior work that uses a vector to encode each network node, we exploit the semantic relationships among network nodes to encode multiple nodes with similar semantics in shared vectors. By using many fewer node vectors, our approach significantly reduces the required memory space for encoding large-scale networks. To effectively trade information sharing for reduced memory footprint, we employ the recursive neural network (RsNN) with carefully designed optimization strategies to explore the node semantics in a novel cuboid space. We then go further by showing, for the first time, how an effective incremental learning approach can be developed – with the help of RsNN, our cuboid structure, and a set of novel optimization techniques – to allow a learning framework to quickly and efficiently adapt to a constantly evolving network. We evaluate Lime by applying it to three representative network-based tasks, node classification, node clustering and anomaly detection, performing on three large-scale datasets. We compare Lime against eleven prior state-of-the-art approaches for learning network representation. Our extensive experiments demonstrate that Lime not only reduces the memory footprint by over 80 percent and the processing time over 2x when learning network representation but also delivers comparable performance for downstream processing tasks. We show that our incremental learning method can boost the learning time by up to 20x without compromising the quality of the learned network representation.},
  archive      = {J_TC},
  author       = {Hao Peng and Renyu Yang and Zheng Wang and Jianxin Li and Lifang He and Philip S. Yu and Albert Y. Zomaya and Rajiv Ranjan},
  doi          = {10.1109/TC.2021.3057082},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {628-642},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Lime: Low-cost and incremental learning for dynamic heterogeneous information networks},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable energy-efficient microarchitectures with
computational error tolerance via redundant residue number systems.
<em>TC</em>, <em>71</em>(3), 613–627. (<a
href="https://doi.org/10.1109/TC.2021.3055754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to high leakage current and threshold voltage, Dennard scaling has reached its limit on conventional semiconductor technology. Energy reduction at the transistor level by simply lowering supply voltage has proven to be infeasible for these devices (e.g., MOSFETs). Some recently proposed millivolt switch techniques aim to mitigate these issues, by maintaining a high on/off ratio of drain currents with a much lower supply voltage. However, $V_{dd}$ reduction is constrained by high intermittent error probabilities in millivolt switches. Energy-efficient microarchitectures that are computationally error-tolerant are therefore urgently needed. This article systematically leverages the error correction and checkpointing properties of Redundant Residue Number Systems (RRNS) by varying the number of non-redundant ( $n$ ) and redundant ( $r$ ) residues. The state-of-the-art of RRNS microarchitecture is confined to a fixed configuration point within such a ($n$n, $r$r)-RRNS design plane, as it supports single error correction alone. Being able to efficiently handle resilience in this ($n$n, $r$r)-RRNS plane significantly improves reliability, allowing further ${V_{dd}}$ reduction to save energy. To this end, first, we propose a scalable RRNS microarchitecture that simultaneously supports both, error-correction, as well as checkpointing with restart capabilities upon detecting uncorrectable errors. Second, we design a novel RRNS-based adaptive checkpointing&amp;restart mechanisms that automatically guarantees reliability while minimizing the energy-delay product (EDP). To the best of our knowledge, these are the first set of checkpointing mechanisms targeting the RRNS infrastructure. Moreover, these mechanisms optimize the usage efficiency of memory capacity. Third, we systematically explore the RRNS design space to find the best ( $n$ , $r$ ) configuration point. For similar reliability when compared to a conventional binary core without computationally error-tolerant (runs at high $V_{dd}$ ), the proposed RRNS scalable microarchitecture reduces EDP by 53 percent on average for memory-intensive workloads and by 67 percent on average for non-memory-intensive workloads.},
  archive      = {J_TC},
  author       = {Bobin Deng and Sriseshan Srikanth and Anirudh Jain and Thomas M. Conte and Erik DeBenedictis and Jeanine Cook},
  doi          = {10.1109/TC.2021.3055754},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {613-627},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Scalable energy-efficient microarchitectures with computational error tolerance via redundant residue number systems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NBBS: A non-blocking buddy system for multi-core machines.
<em>TC</em>, <em>71</em>(3), 599–612. (<a
href="https://doi.org/10.1109/TC.2021.3060393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Common implementations of core memory allocation components handle concurrent allocation/release requests by synchronizing threads via spin-locks. This approach is not prone to scale, a problem that has been addressed in the literature by introducing layered allocation services or replicating the core allocators—the bottom-most ones within the layered architecture. Both these solutions tend to reduce the pressure of actual concurrent accesses to each individual core allocator. In this article, we explore an alternative approach to scalability of memory allocation/release, which can be still combined with those literature proposals. We present a fully non-blocking buddy system, where threads performing concurrent allocations/releases do not undergo any spin-lock based synchronization. Our solution allows threads to proceed in parallel, and commit their allocations/releases unless a conflict is materialized while handling the allocator metadata—memory fragmentation and coalescing are also carried out in a fully non-blocking manner. Conflict detection relies in our solution on atomic Read-Modify-Write (RMW) machine instructions, guaranteed to execute atomically by the processor firmware. We also provide a proof of the correctness of our non-blocking buddy system and show the results of an experimental study that outlines the effectiveness of our solution.},
  archive      = {J_TC},
  author       = {Romolo Marotta and Mauro Ianni and Alessandro Pellegrini and Francesco Quaglia},
  doi          = {10.1109/TC.2021.3060393},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {599-612},
  shortjournal = {IEEE Trans. Comput.},
  title        = {NBBS: A non-blocking buddy system for multi-core machines},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A machine learning pipeline stage for adaptive frequency
adjustment. <em>TC</em>, <em>71</em>(3), 587–598. (<a
href="https://doi.org/10.1109/TC.2021.3057764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A machine learning (ML) design framework is proposed for adaptively adjusting clock frequency based on propagation delay of individual instructions. A random forest model is trained to classify propagation delays in real time, utilizing current operation type, current operands, and computation history as ML features. The trained model is implemented in Verilog as an additional pipeline stage within TigerMIPS processor. The modified system is experimentally tested at the gate level in 45 nm CMOS technology, exhibiting simultaneously a speedup of 70 percent and an energy reduction of 30 percent with coarse-grained ML classification as compared with the baseline TigerMIPS. A speedup of 89 percent is demonstrated with finer granularities with a simultaneous 15.5 percent reduction in energy consumption.},
  archive      = {J_TC},
  author       = {Arash Fouman Ajirlou and Inna Partin-Vaisband},
  doi          = {10.1109/TC.2021.3057764},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {587-598},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A machine learning pipeline stage for adaptive frequency adjustment},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reduced precision DWC: An efficient hardening strategy for
mixed-precision architectures. <em>TC</em>, <em>71</em>(3), 573–586. (<a
href="https://doi.org/10.1109/TC.2021.3058872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Duplication with Comparison (DWC) is an effective software-level solution to improve the reliability of computing devices. However, it introduces performance and energy consumption overheads that could be unsuitable for high-performance computing or real-time safety-critical applications. In this article, we present Reduced-Precision Duplication with Comparison (RP-DWC) as a means to lower the overhead of DWC by executing the redundant copy in reduced precision. RP-DWC is particularly suitable for modern mixed-precision architectures, such as NVIDIA GPUs, that feature dedicated functional units for computing with programmable accuracy. We discuss the benefits and challenges associated with RP-DWC and show that the intrinsic difference between the mixed-precision copies allows for detecting most, but not all, errors. However, as the undetected faults are the ones that fall into the difference between precisions, they are the ones that produce a much smaller impact on the application output and, thus, might be tolerated. We investigate RP-DWC impact into fault detection, performance, and energy consumption on Volta GPUs. Through fault injection and beam experiment, using three microbenchmarks and four real applications, we show that RP-DWC achieves an excellent coverage (up to 86 percent) with minimal overheads (as low as 0.1 percent time and 24 percent energy consumption overhead).},
  archive      = {J_TC},
  author       = {Fernando F. dos Santos and Marcelo Brandalero and Michael B. Sullivan and Pedro M. Basso and Michael Hübner and Luigi Carro and Paolo Rech},
  doi          = {10.1109/TC.2021.3058872},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {573-586},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Reduced precision DWC: An efficient hardening strategy for mixed-precision architectures},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Observing the invisible: Live cache inspection for
high-performance embedded systems. <em>TC</em>, <em>71</em>(3), 559–572.
(<a href="https://doi.org/10.1109/TC.2021.3060650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vast majority of high-performance embedded systems implement multi-level CPU cache hierarchies. But the exact behavior of these CPU caches has historically been opaque to system designers. Absent expensive hardware debuggers, an understanding of cache makeup remains tenuous at best. This enduring opacity further obscures the complex interplay among applications and OS-level components, particularly as they compete for the allocation of cache resources. Notwithstanding the relegation of cache comprehension to proxies such as static cache analysis, performance counter-based profiling, and cache hierarchy simulations, the underpinnings of cache structure and evolution continue to elude software-centric solutions. In this article, we explore a novel method of studying cache contents and their evolution via snapshotting. Our method complements extant approaches for cache profiling to better formulate, validate, and refine hypotheses on the behavior of modern caches. We leverage cache introspection interfaces provided by vendors to perform live cache inspections without the need for external hardware. We present CacheFlow, a proof-of-concept Linux kernel module which snapshots cache contents on an NVIDIA Tegra TX1 system on chip and a Hardkernel Odroid XU4.},
  archive      = {J_TC},
  author       = {Dharmesh Tarapore and Shahin Roozkhosh and Steven Brzozowski and Renato Mancuso},
  doi          = {10.1109/TC.2021.3060650},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {559-572},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Observing the invisible: Live cache inspection for high-performance embedded systems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enabling pulse-level programming, compilation, and execution
in XACC. <em>TC</em>, <em>71</em>(3), 547–558. (<a
href="https://doi.org/10.1109/TC.2021.3057166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noisy gate-model quantum processing units (QPUs) are currently available from vendors over the cloud, and digital quantum programming approaches exist to run low-depth circuits on physical hardware. These digital representations are ultimately lowered to pulse-level instructions by vendor quantum control systems to affect unitary evolution representative of the submitted digital circuit. Vendors are beginning to open this pulse-level control system to the public via specified interfaces. Robust programming methodologies, software frameworks, and backend simulation technologies for this analog model of quantum computation will prove critical to advancing pulse-level control research and development. Prototypical use cases for this include error mitigation, optimal pulse control, and physics-inspired pulse construction. Here we present an extension to the XACC quantum-classical software framework that enables pulse-level programming for superconducting, gate-model quantum computers, and a novel, general, and extensible pulse-level simulation backend for XACC that scales on classical compute clusters via MPI. Our work enables custom backend Hamiltonian definitions and gate-level compilation to available pulses with a focus on performance and scalability. We end with a demonstration of this capability, and show how to use XACC for pertinent pulse-level programming tasks.},
  archive      = {J_TC},
  author       = {Thien Nguyen and Alexander McCaskey},
  doi          = {10.1109/TC.2021.3057166},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {547-558},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling pulse-level programming, compilation, and execution in XACC},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Polymorphic accelerators for deep neural networks.
<em>TC</em>, <em>71</em>(3), 534–546. (<a
href="https://doi.org/10.1109/TC.2020.3048624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) come with many forms, such as convolutional neural networks, multilayer perceptron, and recurrent neural networks, to meet diverse needs of machine learning applications. However, existing DNN accelerator designs, when used to execute multiple neural networks, suffer from underutilization of processing elements, heavy feature map traffic, and large area overhead. In this article, we propose a novel approach, Polymorphic Accelerators , to address the flexibility issue fundamentally. We introduce the abstraction of logical accelerators to decouple the fixed mapping with physical resources. Three procedures are proposed that work collaboratively to reconfigure the accelerator for the current network that is being executed and to enable cross-layer data reuse among logical accelerators. Evaluation results show that the proposed approach achieves significant improvement in data reuse, inference latency and performance, e.g., 1.52x and 1.63x increase in throughput compared with state-of-the-art flexible dataflow approach and resource partitioning approach, respectively. This demonstrates the effectiveness and promise of polymorphic accelerator architecture.},
  archive      = {J_TC},
  author       = {Arash Azizimazreah and Lizhong Chen},
  doi          = {10.1109/TC.2020.3048624},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {534-546},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Polymorphic accelerators for deep neural networks},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Svelto: High-level synthesis of multi-threaded accelerators
for graph analytics. <em>TC</em>, <em>71</em>(3), 520–533. (<a
href="https://doi.org/10.1109/TC.2021.3057860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph analytics are an emerging class of irregular applications. Operating on very large datasets, they present unique behaviors, such as fine-grained, unpredictable memory accesses, and highly unbalanced task level parallelism, that make existing high-performance general-purpose processors or accelerators (e.g., GPUs) suboptimal. To address these issues, research and industry are developing a variety of custom accelerator designs for this application area, including solutions based on reconfigurable devices (Field Programmable Gate Arrays). These new approaches often employ High-Level Synthesis (HLS) to Speed up the development of the accelerators. In this paper, we propose a novel architecture template for the automatic generation of accelerators for graph analytics and irregular applications. The architecture template includes a dynamic task scheduling mechanism, a parallel array of accelerators that enables supporting task-level parallelism with context switching, and a related multi-channel memory interface that decouples communication from computation and provides support for fine-grained atomic memory operations. We discuss the integration of the architectural template in an HLS flow, presenting the necessary modifications to enable automatic generation of the custom architectures starting from OpenMP annotated code. We evaluate our approach first by synthesizing and exploring triangle counting, a common graph algorithm, and then by synthesizing custom designs for a set of graph database benchmark queries, representing series of graph pattern matching routines. We compare the synthesized accelerators with previous state-of-the-art methodologies for the synthesis of parallel architectures, showing that the proposed approach allows reducing resource usage by optimizing the number of accelerators replicas without any performance penalty.},
  archive      = {J_TC},
  author       = {Marco Minutoli and Vito Giovanni Castellana and Nicola Saporetti and Stefano Devecchi and Marco Lattuada and Pietro Fezzardi and Antonino Tumeo and Fabrizio Ferrandi},
  doi          = {10.1109/TC.2021.3057860},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {520-533},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Svelto: High-level synthesis of multi-threaded accelerators for graph analytics},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FlexiPair: An automated programmable framework for pairing
cryptosystems. <em>TC</em>, <em>71</em>(3), 506–519. (<a
href="https://doi.org/10.1109/TC.2021.3058345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pairing cryptosystems are extremely powerful mathematical tools for developing cryptographic protocols that can provide end-to-end security for applications like Internet-of-Things (IoT), cloud services and cyber-physical systems (CPS). However, these applications require the implementations to be light-weight but still real-time, with the additional feature of being flexible. The flexibility can come from different choices of underlying algorithms along with suitable parameter choices. A software implementation offers better flexibility but lacks in timing performance, whereas custom hardware delivers better performance but has poor flexibility. Furthermore, the designs over small characteristic curves are now insecure against recent attacks. Existing designs do not address the drawback of less flexibility and huge resource consumption collectively. In this article, we present a micro-program controlled hardware design which has the least resource consumption among the similar existing designs on FPGA that offer such programmability and flexibility. This redundant number arithmetic-based architecture consumes only 2506 slices on Xilinx Virtex-7 FPGA. It can be migrated to other device families or updated for different algorithms without data-path or control-path modification. To enhance the flexibility, we developed a custom assembly-like finite state machine (FSM) description, called Prism, and necessary tool to generate the micro-program states. To illustrate the functionality of Prism, we present designs for Tate and Optimal-Ate pairing with the micro-program states generated using this tool.},
  archive      = {J_TC},
  author       = {Arnab Bag and Debapriya Basu Roy and Sikhar Patranabis and Debdeep Mukhopadhyay},
  doi          = {10.1109/TC.2021.3058345},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {506-519},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FlexiPair: An automated programmable framework for pairing cryptosystems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online machine learning for energy-aware multicore real-time
embedded systems. <em>TC</em>, <em>71</em>(2), 493–505. (<a
href="https://doi.org/10.1109/TC.2021.3056070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present an Online Learning Artificial Neural Network (ANN) model that is able to predict the performance of tasks in lower frequency levels and safely optimize real-time embedded systems&amp;#x2019; power saving operations. The proposed ANN model is supported by feature selection, which provides the most relevant variables to describe shared resource contention in the selected multicore architecture. The variables are used at runtime to produce a performance trace that encompasses sufficient information for the ANN model to predict the impact of a frequency change on the performance of tasks. A migration heuristic encompassing a weighted activity vector is combined with the ANN model to dynamically adjust frequencies and also to trigger task migrations among cores, enabling further optimization by solving resource contentions and balancing the load among cores. The proposed solution achieved energy-savings of 24.97 percent on average when compared to the run-to-end approach, and it did it without compromising the criticality of any single task. The overhead incurred in terms of execution time was 0.1791 percent on average. Each prediction added 15.3585$\mu s$&amp;#x03BC;s on average and each retraining cycle triggered at frequency adjustments was never larger than 100$\mu s$&amp;#x03BC;s.},
  archive      = {J_TC},
  author       = {Jos&amp;#x00E9; Luis Conradi Hoffmann and Ant&amp;#x00F4;nio Augusto Fr&amp;#x00F6;hlich},
  doi          = {10.1109/TC.2021.3056070},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {493-505},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Online machine learning for energy-aware multicore real-time embedded systems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Colony: A privileged trusted execution environment with
extensibility. <em>TC</em>, <em>71</em>(2), 479–492. (<a
href="https://doi.org/10.1109/TC.2021.3055293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The code base of system software is growing fast, which results in a large number of vulnerabilities: for example, 296 CVEs have been found in Xen hypervisor and 2195 CVEs in Linux kernel. To reduce the reliance on the trust of system software, many researchers try to provide trusted execution environments (TEEs), which can be categorized into two types: non-privileged TEEs and privileged TEEs. Non-privileged TEEs (e.g., Intel SGX) are extensible, but cannot protect security services like virtual machine introspection (VMI) due to the lack of system-level semantics. On the contrary, privileged TEEs (e.g., the secure world of ARM TrustZone) have system-level semantics, but any additional service implemented in the privileged TEE directly increases the TCB of the entire system. In this article, we propose a new design of TEE to support system-level security services and achieve better extensibility with a small TCB. Each TEE instance of the proposed design is named a Colony. Specifically, we introduce a secure monitor for isolation and capability management. Each Colony is assigned capabilities to access only necessary system-level semantics. We use the new TEE to build four security services, including secure device accessing, VMI tools, a system call tracer, and a much more complex service to virtualize ARM TrustZone with multiple Colonies. We have implemented the system on ARMv7 and ARMv8 platforms, in Xen hypervisor and Linux kernel, and perform a detailed evaluation to show its efficiency.11.This paper is an extended version of the conference paper published in USENIX Security&amp;#x2019;17: vTZ: Virtualizing ARM TrustZone [29] . A brief summary of differences is in Section8 .},
  archive      = {J_TC},
  author       = {Yubin Xia and Zhichao Hua and Yang Yu and Jinyu Gu and Haibo Chen and Binyu Zang and Haibing Guan},
  doi          = {10.1109/TC.2021.3055293},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {479-492},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Colony: A privileged trusted execution environment with extensibility},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Flow-based microfluidic biochips with distributed channel
storage: Synthesis, physical design, and wash optimization. <em>TC</em>,
<em>71</em>(2), 464–478. (<a
href="https://doi.org/10.1109/TC.2021.3054689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {System-architecture design optimization of flow-based microfluidic biochips has been extensively investigated over the past decade. Most of the prior work, however, is still based on chip architectures with dedicated storage units and this, not only limits the performance of biochips, but also increases their fabrication cost. To overcome this limitation, a distributed channel-storage architecture can be implemented, where fluid samples can be cached temporarily in flow channels instead of using a dedicated storage. This new concept of fluid storage, however, requires a careful arrangement of fluid samples to enable the channels to fulfill the dual functions of transportation and caching. Moreover, to avoid cross-contamination between different fluidic flows, wash operations are necessary to remove the residue left in flow channels. In this article, we formulate the first practical system level design and wash optimization problem for microfluidic biochips with distributed channel storage architecture, considering high-level synthesis, physical design, and wash optimization simultaneously, and present a top-down design flow to solve this problem systematically. Given the protocol of a biochemical application and the corresponding design requirements, our goal is to generate a chip architecture with low fabrication cost. Meanwhile the biochemical application can be executed efficiently with an optimized wash scheme. Experimental results on multiple benchmarks confirm that our approach leads to short completion time of biochemical applications, low chip cost, as well as high wash efficiency.},
  archive      = {J_TC},
  author       = {Xing Huang and Wenzhong Guo and Zhisheng Chen and Bing Li and Tsung-Yi Ho and Ulf Schlichtmann},
  doi          = {10.1109/TC.2021.3054689},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {464-478},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Flow-based microfluidic biochips with distributed channel storage: Synthesis, physical design, and wash optimization},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). λDNN: Achieving predictable distributed DNN training with
serverless architectures. <em>TC</em>, <em>71</em>(2), 450–463. (<a
href="https://doi.org/10.1109/TC.2021.3054656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless computing is becoming a promising paradigm for Distributed Deep Neural Network (DDNN) training in the cloud, as it allows users to decompose complex model training into a number of functions without managing virtual machines or servers. Though provided with a simpler resource interface (i.e., function number and memory size), inadequate function resource provisioning (either under-provisioning or over-provisioning) easily leads to unpredictable DDNN training performance in serverless platforms. Our empirical studies on AWS Lambda indicate that, such unpredictable performance of serverless DDNN training is mainly caused by the resource bottleneck of Parameter Servers (PS) and small local batch size. In this article, we design and implement $\lambda$λDNN , a cost-efficient function resource provisioning framework to provide predictable performance for serverless DDNN training workloads, while saving the budget of provisioned functions. Leveraging the PS network bandwidth and function CPU utilization, we build a lightweight analytical DDNN training performance model to enable our design of $\lambda$λDNN resource provisioning strategy, so as to guarantee DDNN training performance with serverless functions. Extensive prototype experiments on AWS Lambda and complementary trace-driven simulations demonstrate that, $\lambda$λDNN can deliver predictable DDNN training performance and save the monetary cost of function resources by up to 66.7 percent, compared with the state-of-the-art resource provisioning strategies, yet with an acceptable runtime overhead.},
  archive      = {J_TC},
  author       = {Fei Xu and Yiling Qin and Li Chen and Zhi Zhou and Fangming Liu},
  doi          = {10.1109/TC.2021.3054656},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {450-463},
  shortjournal = {IEEE Trans. Comput.},
  title        = {λDNN: Achieving predictable distributed DNN training with serverless architectures},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). High-radix design of a scalable montgomery modular
multiplier with low latency. <em>TC</em>, <em>71</em>(2), 436–449. (<a
href="https://doi.org/10.1109/TC.2021.3052999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proposed herein is a scalable high-radix (i.e., $2^m$2m) Montgomery Modular (MM) Multiplication circuit replacing the integer multiplications in each iteration of the Montgomery MM algorithm (related to the product of $m$m bits of the multiplier and the multiplicand) with carry-save compressions and completely eliminating costly multiplications. Furthermore, the proposed Montgomery MM decomposes the multiplicand itself using a radix of $2^w$2w with $w\geq 2m$w&amp;#x2265;2m, thereby achieving a scalable design, which can deliver an issue latency of one cycle and a cycle (count) latency of $O(N^2/(wmp))$O(N2/(wmp)) where $p$p denotes the number of available processing elements, each of which is designed to complete the above iteration by computing in part the product of $w$w bits of the multiplicand and $m$m bits of the multiplier. The area complexity of the proposed Montgomery MM is $O(wmp)$O(wmp), and thus, the Area-Latency-Product complexity is $O(N^{2})$O(N2).},
  archive      = {J_TC},
  author       = {Bo Zhang and Zeming Cheng and Massoud Pedram},
  doi          = {10.1109/TC.2021.3052999},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {436-449},
  shortjournal = {IEEE Trans. Comput.},
  title        = {High-radix design of a scalable montgomery modular multiplier with low latency},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PRESTO: A penalty-aware real-time scheduler for task graphs
on heterogeneous platforms. <em>TC</em>, <em>71</em>(2), 421–435. (<a
href="https://doi.org/10.1109/TC.2021.3052389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scheduling real-time applications modelled as directed acyclic graphs on heterogeneous distributed platforms is known to be a challenging as well as a computationally demanding problem. This article deals with the design of an efficient scheduler for executing a real-time task graph on a distributed platform consisting of a set of fully connected heterogeneous processors. The objective of the scheduling strategy is to minimize a generic penalty function which can be amicably adopted toward its deployment in various application domains such as real-time embedded systems, cloud/fog computing, industrial automation and IoTs, smart grids, automotive and avionic systems, etc. We have first encoded the problem as a constraint satisfaction problem and then developed an efficient list-based heuristic scheduling algorithm called Penalty-aware REal-time Scheduler for Task graphs on heterOgeneous platforms ( PRESTO ), to generate a minimal penalty deadline-meeting static schedule. The generic efficacy of PRESTO is exhibited through extensive simulation-based experiments using standard benchmark task graphs. The practical applicability of PRESTO in diverse scenarios have further been exhibited by using the scheme in two different real-world case studies, the first of which relates to automotive embedded systems, while the second is in the domain of fog computing.},
  archive      = {J_TC},
  author       = {Debabrata Senapati and Arnab Sarkar and Chandan Karfa},
  doi          = {10.1109/TC.2021.3052389},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {421-435},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PRESTO: A penalty-aware real-time scheduler for task graphs on heterogeneous platforms},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performance analysis of timing-speculative processors.
<em>TC</em>, <em>71</em>(2), 407–420. (<a
href="https://doi.org/10.1109/TC.2021.3051877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a framework to estimate the number of timing errors experienced by an application as it runs on a timing-speculative processor. It takes a hybrid approach combining an accurate gate-level dynamic timing analysis engine to find timing errors in the processor&amp;#x2019;s control network with a fast architecture-level execution-driven simulator based on a path activation model of the datapath. We develop an instruction-level error model that estimates the likelihood of an instruction experiencing a timing error, capturing the effects of process and data variations as well as inter-instruction correlations caused by the error recovery scheme used by the processor. Finally, we utilize two well-known laws of applied statistics, the law of small numbers and the law of large numbers, to estimate, with bounded inaccuracy, the total number of timing errors experienced by a specific application. Our experiments show that the combination of running application and its input data can change performance by as much as 25 percent, demonstrating that application-specific analysis is necessary for accurate evaluation of timing-speculative processors and should be used to inform design decisions and assess the suitability of applications for timing speculation.},
  archive      = {J_TC},
  author       = {Omid Assare and Rajesh K. Gupta},
  doi          = {10.1109/TC.2021.3051877},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {407-420},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Performance analysis of timing-speculative processors},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Endogenous trusted DRL-based service function chain
orchestration for IoT. <em>TC</em>, <em>71</em>(2), 397–406. (<a
href="https://doi.org/10.1109/TC.2021.3051681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of the Internet of Things, trust has become a limited factor in the integration of heterogeneous IoT networks. In this regard, we use the combination of blockchain technology and SDN/NFV to build a heterogeneous IoT network resource management model based on the consortium chain. In order to solve the efficiency problem caused by the full amount of data on the chain, we deploy light nodes and full nodes for the consortium chain. At the same time, we use the idea of identification to realize the separation of identification and resource information, build the application mode of on-chain identification and off-chain information, and realize resources endogenous trust management. We also propose a practical Byzantine fault-tolerant consensus mechanism based on reputation value to save consensus costs and improve efficiency. Combined with artificial intelligence technology, we introduce deep reinforcement learning for service function chain orchestration, and design a service function chain orchestration algorithm based on Asynchronous Advantage Actor-Critic to optimize orchestration costs. The final simulation results show that the consensus algorithm and service function chain orchestration algorithm we designed have good performance in terms of cost saving and efficiency improvement.},
  archive      = {J_TC},
  author       = {Shaoyong Guo and Yuanyuan Qi and Yi Jin and Wenjing Li and Xuesong Qiu and Luoming Meng},
  doi          = {10.1109/TC.2021.3051681},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {397-406},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Endogenous trusted DRL-based service function chain orchestration for IoT},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis &amp; design of convolution operator for high speed
and high accuracy convolutional neural network-based inference engines.
<em>TC</em>, <em>71</em>(2), 390–396. (<a
href="https://doi.org/10.1109/TC.2021.3051627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inference engines (IEs) based on convolutional neural networks (CNN) are memory intensive and computationally complex. The IEs require optimum data format for representing kernel weights and feature maps (FMs) to reduce the computational complexity of the convolution operator (CO). The proposed CO implements multiplications in floating-point and additions in fixed-point, considering the implementation aspects and loss-of-precision. Here, the optimal data format is decided through MATLAB-based range and precision analysis. To accomplish this image-based models such as AlexNet, VGG-16, and VGG-19 are considered with single-precision floating-point representation (SPFP) as reference representation. Analysis reveals that half-precision floating-point (HPFP) and 16-bit fixed-point (10-bit integer, 6-bit fraction) representations are required for kernel weights and feature maps respectively. A 16-bit Fix/Float 2&amp;#x00D7;1 CO is designed. A trade-off analysis of the CO with proposed data format, 16-bit fixed-point, SPFP, and HPFP is performed. This CO has worst-case accuracy as close as 97 percent with SPFP. The proposed 2&amp;#x00D7;1 CO is implemented with a multiplication operation processing unit (MOPU) in place of the shifter/barrel-shifter unit. ASIC implementation of CO requires a 22 percent lesser area, 17.98 percent lesser power than HPFP with 250MHz clock. Also, the speed of 750MOPS and hardware efficiency of 24.22TOPS/W are achieved.},
  archive      = {J_TC},
  author       = {S. Deepika and V. Arunachalam},
  doi          = {10.1109/TC.2021.3051627},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {390-396},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Analysis &amp;#x0026; design of convolution operator for high speed and high accuracy convolutional neural network-based inference engines},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Classical artificial neural network training using quantum
walks as a search procedure. <em>TC</em>, <em>71</em>(2), 378–389. (<a
href="https://doi.org/10.1109/TC.2021.3051559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a computational procedure that applies a quantum algorithm to train classical artificial neural networks. The goal of the procedure is to apply quantum walk as a search algorithm in a complete graph to find all synaptic weights of a classical artificial neural network. Each vertex of this complete graph represents a possible synaptic weight set in the $w$ -dimensional search space, where $w$ is the number of weights of the neural network. To know the number of iterations required a priori to obtain the solutions is one of the main advantages of the procedure. Another advantage is that the proposed method does not stagnate in local minimums. Thus, it is possible to use the quantum walk search procedure as an alternative to the backpropagation algorithm. The proposed method was employed for a $XOR$ problem to prove the proposed concept. To solve this problem, the proposed method trained a classical artificial neural network with nine weights. However, the procedure can find solutions for any number of dimensions. The results achieved demonstrate the viability of the proposal, contributing to machine learning and quantum computing researches.},
  archive      = {J_TC},
  author       = {Luciano S. de Souza and Jonathan H. A. de Carvalho and Tiago A. E. Ferreira},
  doi          = {10.1109/TC.2021.3051559},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {378-389},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Classical artificial neural network training using quantum walks as a search procedure},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The graph structure of the generalized discrete arnold's cat
map. <em>TC</em>, <em>71</em>(2), 364–377. (<a
href="https://doi.org/10.1109/TC.2021.3051387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chaotic dynamics is an important source for generating pseudorandom binary sequences (PRBS). Much efforts have been devoted to obtaining period distribution of the generalized discrete Arnold&amp;#x0027;s Cat map in various domains using all kinds of theoretical methods, including Hensel&amp;#x0027;s lifting approach. Diagonalizing the transform matrix of the map, this article gives the explicit formulation of any iteration of the generalized Cat map. Then, its real graph (cycle) structure in any binary arithmetic domain is disclosed. The subtle rules on how the cycles (itself and its distribution) change with the arithmetic precision $e$e are elaborately investigated and proved. The regular and beautiful patterns of Cat map demonstrated in a computer adopting fixed-point arithmetics are rigorously proved and experimentally verified. The results can serve as a benchmark for studying the dynamics of the variants of the Cat map in any domain. In addition, the used methodology can be used to evaluate randomness of PRBS generated by iterating any other maps.},
  archive      = {J_TC},
  author       = {Chengqing Li and Kai Tan and Bingbing Feng and Jinhu L&amp;#x00FC;},
  doi          = {10.1109/TC.2021.3051387},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {364-377},
  shortjournal = {IEEE Trans. Comput.},
  title        = {The graph structure of the generalized discrete arnold&amp;#x0027;s cat map},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A framework for crossing temperature-induced timing errors
underlying hardware accelerators to the algorithm and application
layers. <em>TC</em>, <em>71</em>(2), 349–363. (<a
href="https://doi.org/10.1109/TC.2021.3050978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temperature rising is an unavoidable effect on VLSI and has always been a critical issue in any system-on-chip &amp;#x2013; especially when targeting compute-intensive applications. This effect increases the delay in hardware accelerators, resulting in timing errors due to unsustainable clock frequency, whose impact must be carefully evaluated on design time to measure the performance degradation of the hardware accelerator. Further, a hardware operating at a higher temperature accelerates device aging, which incurs in more timing errors. This issue is usually addressed with the inclusion of timing guardbands that compensate for the deleterious effects of temperature, ensuring the hardware accelerator works within a reliable zone, i.e., without any timing errors caused by temperature effects at runtime. However, guardbands directly result in considerable performance and efficiency losses because the circuit will be clocked at a frequency lower than its full potential. Accelerators on edge devices often dismiss such guardbands to explore the full potential of the designed circuits, posing an enormous design challenge as this approach requires a careful evaluation of the impact of timing errors on the quality of the target applications. Many algorithms, such as in multimedia and machine learning applications, are capable of tolerating hardware errors. Yet, these algorithms have a dynamic behavior (i.e., closed-loop) where a timing error can be propagated, affecting subsequent steps. Measuring the degradation-induced errors in these applications is very challenging given that an accurate gate-level simulation to investigate degradation-induced timing errors needs to be coupled dynamically with a system-level simulator to unveil how induced errors in the underlying hardware ultimately impact the algorithm execution in the hardware accelerator. This is the first work to achieve this goal. State-of-the-art works have studied accelerators under timing-errors when removing (or narrowing) guardbands. However, their approach was suitable only for open-loop hardware accelerators which are entirely agnostic of complex interactions of the algorithms. Unlike prior work, this paper investigates temperature- and aging-induced timing-errors in the joint accelerator-algorithm interactions and their runtime impacts. Our framework investigates aging effects across the different layers starting from transistor physics all the way up to the algorithm layer. The hardware accelerator employed as a case study in this work is the sum of absolute differences (SAD), which is the most compute-intensive accelerator on commercial video encoder for mobile applications. Our results demonstrate the runtime behavior impacts of three advanced block-matching algorithms of the video encoder in a joint operation by a SAD accelerator under timing-errors induced by temperature and aging effects considering a 14nm FinFET technology.},
  archive      = {J_TC},
  author       = {Guilherme Paim and Hussam Amrouch and Leandro M. G. Rocha and Brunno Abreu and Eduardo Ant&amp;#x00F4;nio C&amp;#x00E9;sar da Costa and Sergio Bampi and J&amp;#x00F6;rg Henkel},
  doi          = {10.1109/TC.2021.3050978},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {349-363},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A framework for crossing temperature-induced timing errors underlying hardware accelerators to the algorithm and application layers},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inverse queuing model-based feedback control for elastic
container provisioning of web systems in kubernetes. <em>TC</em>,
<em>71</em>(2), 337–348. (<a
href="https://doi.org/10.1109/TC.2021.3049598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Container orchestration platforms such as Kubernetes and Kubernetes-derived KubeEdge (called Kubernetes-based systems collectively) have been gradually used to conduct unified management of Cloud, Fog, and Edge resources. Container provisioning algorithms are crucial to guaranteeing quality of services (QoS) of such Kubernetes-based systems. However, most existing algorithms focus on placement and migration of fixed number of containers without considering elastic provisioning of containers. Meanwhile, widely used linear-performance-model-based feedback control or fixed-processing-rate-based queuing model on diverse platforms cannot describe the performance of containerized Web systems accurately. Furthermore, a fixed reference point used by existing methods is likely to generate inaccurate output errors incurring great fluctuations encountered with large arrival-rate changes. In this article, a feedback control method is designed based on a combination of varying-processing-rate queuing model and linear-model to provision containers elastically which improves the accuracy of output errors by learning reference models for different arrival rates automatically and mapping output errors from reference models to the queuing model. Our approach is compared with several state-of-art algorithms on a real Kubernetes cluster. Experimental results illustrate that our approach obtains the lowest percentage of service level agreement (SLA) violation (decreasing no less than 8.44 percent) and the second lowest cost.},
  archive      = {J_TC},
  author       = {Zhicheng Cai and Rajkumar Buyya},
  doi          = {10.1109/TC.2021.3049598},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {337-348},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Inverse queuing model-based feedback control for elastic container provisioning of web systems in kubernetes},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Voltage over-scaling-based lightweight authentication for
IoT security. <em>TC</em>, <em>71</em>(2), 323–336. (<a
href="https://doi.org/10.1109/TC.2021.3049543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a challenging task to deploy lightweight security protocols in resource-constrained IoT applications. A hardware-oriented lightweight authentication protocol based on device signature generated during voltage over-scaling (VOS) was recently proposed to address this issue. VOS-based authentication employs the computation unit such as adders to generate the process variation dependent error, which is combined with secret keys to create a two-factor authentication protocol. In this article, machine learning (ML)-based modeling attacks to break such authentication is presented. We also propose a challenge self-obfuscation structure (CSoS) which employs previous challenges combined with keys or random numbers to obfuscate the current challenge for the VOS-based authentication to resist ML attacks. Experimental results show that ANN, RNN, and CMA-ES can clone the challenge-response behavior of VOS-based authentication with up to 99.65 percent prediction accuracy, while the prediction accuracy is less than 51.2 percent after deploying our proposed ML resilient technique. In addition, our proposed CSoS also shows good obfuscation ability for strong PUFs. Experimental results show that the modeling accuracy is below 54 percent when 106 challenge-response pairs (CRPs) are collected to model the CSoS-based Arbiter PUF with ML attacks based on LR, SVM, ANN, RNN, and CMA-ES.},
  archive      = {J_TC},
  author       = {Jiliang Zhang and Chaoqun Shen and Haihan Su and Md Tanvir Arafin and Gang Qu},
  doi          = {10.1109/TC.2021.3049543},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {323-336},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Voltage over-scaling-based lightweight authentication for IoT security},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Auto-tuning parameters for emerging multi-stream flash-based
storage drives through new i/o pattern generations. <em>TC</em>,
<em>71</em>(2), 309–322. (<a
href="https://doi.org/10.1109/TC.2020.3048303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of big data processing, more and more data centers in cloud storage are now replacing traditional HDDs with enterprise SSDs. Both developers and users of these SSDs require thorough benchmarking to evaluate and configure the variable parameters of emerging technologies. [2] and [3] are the recent development of the SSD industry, which assists in placing data on SSDs in a smart way to improve application performance and SSD endurance. The challenging part to use multi-stream SSDs is to assign stream IDs to incoming writes, such that each stream consists of data with a similar lifetime. The benefit of the stream management algorithms varies over different workloads. Thus, first, we propose a new framework, called Pattern I/O generator (PatIO), to capture the enterprise storage behavior that is prevailing across various user workloads, virtualization setup, file systems, and volume managers for the database server applications on flash-based storage. Second, using PatIO, we study what type of applications may be benefited by which stream assignment algorithm. Third, we design the framework to automatically tune the variable parameters of different stream identification algorithms of the multi-stream SSDs. Our evaluation shows 20 to 110 percent of the reward function increase, measuring the cumulative impact on application performance and SSD endurance.},
  archive      = {J_TC},
  author       = {Jaki Bhimani and Adnan Maruf and Ningfang Mi and Rajinikanth Pandurangan and Vijay Balakrishnan},
  doi          = {10.1109/TC.2020.3048303},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {309-322},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Auto-tuning parameters for emerging multi-stream flash-based storage drives through new I/O pattern generations},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Circ-tree: A b+-tree variant with circular design for
persistent memory. <em>TC</em>, <em>71</em>(2), 296–308. (<a
href="https://doi.org/10.1109/TC.2020.3047972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several B+-tree variants have been developed to exploit the byte-addressable non-volatile memory (NVM). We attentively investigate the properties of B+-tree and find that, a conventional B+-tree node is a linear structure in which key-value (KV) pairs are maintained from the zero offset of a node. These KV pairs are shifted in a unidirectional fashion for insertions and deletions. Inserting and deleting one KV pair may inflict a large amount of write amplifications due to shifting existing KV pairs. This badly impairs the performance of in-NVM B+-tree. In this article, we propose a novel circular design for B+-tree. With regard to NVM&#39;s byte-addressability, our Circ-Tree embraces tree nodes in a circular structure without a fixed base address, and bidirectionally shifts KV pairs for insertions and deletions to minimize write amplifications. We have implemented a prototype for Circ-Tree and conducted extensive experiments. Experimental results show that Circ-Tree significantly outperforms two state-of-the-art in-NVM B+-tree variants, i.e., NV-tree and FAST+FAIR, by up to 1.6× and 8.6×, respectively, in terms of write performance. The end-to-end comparison by running YCSB to KV stores built on NV-tree, FAST+FAIR, and Circ-Tree reveals that Circ-Tree yields up to 29.3 and 47.4 percent higher write performance, respectively, than NV-tree and FAST+FAIR.},
  archive      = {J_TC},
  author       = {Chundong Wang and Gunavaran Brihadiswarn and Xingbin Jiang and Sudipta Chattopadhyay},
  doi          = {10.1109/TC.2020.3047972},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {296-308},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Circ-tree: A b+-tree variant with circular design for persistent memory},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fault impact estimation for lightweight fault detection in
image filtering. <em>TC</em>, <em>71</em>(2), 282–295. (<a
href="https://doi.org/10.1109/TC.2020.3047548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical redundancy-based fault detection techniques, such as Duplication with Comparison (DWC), rely on replicating the computation and comparing the replicas&amp;#x2019; output at a bit-wise granularity. In many application environments these costs are prohibitive, especially when applications are characterized by an intrinsic level of tolerance. This article presents a novel fault-detection approach for the specific context of image filtering. Peculiarity of the proposed approach is that it estimates the impact of the fault on the processed output, in order to determine whether the image is usable or should be re-processed. To limit overheads, the proposed solution exploits Approximate Computing (AC), allowing the definition of disciplined AC strategies to trade-off between accuracy and costs. Core of our solution is the successful combination of Image Quality Assessment metrics and Machine Learning models to assess the visual impact of the fault in a lightweight manner. Extensive experimental campaigns demonstrate the effectiveness of the solution, achieving achieving a reduction in terms of execution time up to 44 percent with respect to the classical DWC, with a fault detection precision ranging from 94.58 to 96.70 percent, and recall ranging from 88.2 to 97.8 percent, depending on the adopted level of approximation.},
  archive      = {J_TC},
  author       = {Cristiana Bolchini and Giacomo Boracchi and Luca Cassano and Antonio Miele and Diego Stucchi},
  doi          = {10.1109/TC.2020.3047548},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {282-295},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Fault impact estimation for lightweight fault detection in image filtering},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient processing of sparse tensor decomposition via
unified abstraction and PE-interactive architecture. <em>TC</em>,
<em>71</em>(2), 266–281. (<a
href="https://doi.org/10.1109/TC.2020.3046617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel architecture to efficiently perform sparse tensor decomposition/completion. As the generalization of vectors and matrices, tensors are widely used to process high-dimensional data. Sparse tensor decomposition (SpTD) is not only an emerging tensor analysis technique but also an effective tool to reduce the storage and computation costs of tensors. However, conventional general-purpose processors are inefficient to perform SpTD, mainly due to: i) variable sparsity degree and flexible buffer size requirement; ii) difficulties of fusing multiple execution kernels to pursue better performance. For domain-specific accelerator designers on the other hand, the diversity of decomposition algorithms is also an important problem that must be considered. To solve these challenges, we propose a unified abstraction for SpTD algorithms and design a specialized accelerator. First, we formulate two types of core kernels (SpLrMM and LrSampling) that serve as a standard form to fit a broad range of SpTD algorithms. Second, we design a sparse tensor engine (STE) to efficiently perform SpTD. STE uses a processing element (PE)-interactive architecture where PEs can be flexibly grouped together via Network-on-Chip (NoC) to share the buffer capacity, bandwidth, and compute resources. We evaluate our accelerator with extensive experiments, and it can achieve an average speedup of 45&amp;#x00D7; over CPU and 29&amp;#x00D7; over GPU.},
  archive      = {J_TC},
  author       = {Bangyan Wang and Lei Deng and Zheng Qu and Shuangchen Li and Zheng Zhang and Yuan Xie},
  doi          = {10.1109/TC.2020.3046617},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {266-281},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient processing of sparse tensor decomposition via unified abstraction and PE-interactive architecture},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MDev-NVMe: Mediated pass-through NVMe virtualization
solution with adaptive polling. <em>TC</em>, <em>71</em>(2), 251–265.
(<a href="https://doi.org/10.1109/TC.2020.3045785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fast access to data and high parallel processing in high-performance computing instigates an urgent demand on the improvement of the NVMe storage within modern data centers. However, the former NVMe virtualization&amp;#x2019;s unsatisfactory performance demonstrates that NVMe devices are often underutilized within cloud platforms. An NVMe virtualization mechanism with high performance and device sharing has captured researchers and developers&amp;#x2019; attention. This article introduces MDev-NVMe, a new virtualization solution for NVMe storage device with (1) full NVMe storage virtualization for VMs running native NVMe driver, (2) a mediated pass-through mechanism for NVMe management, and (3) adaptive configuration of active polling optimization to simultaneously achieve high throughput, low latency performance, and substantial device scalability. We practically implement the MDev-NVMe as a Linux kernel module. This article subsequently evaluates MDev-NVMe with Intel OPTANE and P3600 SSD by comparing several mainstream NVMe virtualization mechanisms using application-level I/O benchmarks. MDev-NVMe with active polling can demonstrate a 142 percent improvement over native (interrupt-driven) throughput and over 2.5 &amp;#x00D7; the Virtio throughput with only 70 percent native average latency and 31 percent Virtio average latency. Finally, the advantages of MDev-NVMe and the importance of adaptive polling are discussed, offering evidence that MDev-NVMe is a superior virtualization choice for cloud storage.},
  archive      = {J_TC},
  author       = {Bo Peng and Jianguo Yao and Yaozu Dong and Haibing Guan},
  doi          = {10.1109/TC.2020.3045785},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {251-265},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MDev-NVMe: Mediated pass-through NVMe virtualization solution with adaptive polling},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An automatic-addressing architecture with fully serialized
access in racetrack memory for energy-efficient CNNs. <em>TC</em>,
<em>71</em>(1), 235–250. (<a
href="https://doi.org/10.1109/TC.2020.3045433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Racetrack memory, an emerging low-power magnetic memory, promises a competitive replacement for traditional memory in the accelerators. However, random access in racetrack memory is time and energy expenditure for CNN accelerators because of its large amount of invalid-shifts. In this article, we propose an automatic-addressing architecture that builds a novel data layout to guarantee that the next round of memory access can be always satisfied at the in-situ or rigorously adjacent cells of current round, producing a fully serialized access footprint that can drive instant port-alignment without any invalid-shifts in racetrack memory. By this way, original address-based access degrades to the selections repeated among the three candidates, i.e., one in-situ cell and two neighbor cells. Based on this simplification, a lightweight access management can generate the sequence of one-out-three selections according to the deterministic access behaviors defined by CNN hyper-parameters. The evaluation shows that, when deploying the five popular CNN applications to our architecture, the physical shifts of racetrack is curtailed by 74.64 percent over legacy layout, which achieves 54.2 and 42.1 percent energy reduction on read and write, respectively. A case study of YOLOv2 indicates that our architecture performs 6.503 GOp/J that achieves $18.5 \times$ improvement to server-level GPUs.},
  archive      = {J_TC},
  author       = {Jihe Wang and Jun Liu and Danghui Wang and Jianfeng An and Xiaoya Fan},
  doi          = {10.1109/TC.2020.3045433},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {235-250},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An automatic-addressing architecture with fully serialized access in racetrack memory for energy-efficient CNNs},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How to reduce the bit-width of an ising model by adding
auxiliary spins. <em>TC</em>, <em>71</em>(1), 223–234. (<a
href="https://doi.org/10.1109/TC.2020.3045112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annealing machines have been developed as non-von Neumann computers aimed at solving combinatorial optimization problems efficiently. To use annealing machines for solving combinatorial optimization problems, we have to represent the objective function and constraints by an Ising model, which is a theoretical model in statistical physics. Further, it is necessary to transform the Ising model according to the hardware limitations. In the transformation, the process of effectively reducing the bit-widths of coefficients in the Ising model has hardly been studied so far. Thus, when we consider the Ising model with a large bit-width, a naive method, which means right bit-shift, has to be applied. Since it is expected that obtaining highly accurate solutions is difficult by the naive method, it is necessary to construct a method for efficiently reducing the bit-width. This article proposes methods for reducing the bit-widths of interaction and external magnetic field coefficients in the Ising model and proves that the reduction gives theoretically the same ground state of the original Ising model. The experimental evaluations also demonstrate the effectiveness of our proposed methods.},
  archive      = {J_TC},
  author       = {Daisuke Oku and Masashi Tawada and Shu Tanaka and Nozomu Togawa},
  doi          = {10.1109/TC.2020.3045112},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {223-234},
  shortjournal = {IEEE Trans. Comput.},
  title        = {How to reduce the bit-width of an ising model by adding auxiliary spins},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Breaking the interaction wall: A DLPU-centric deep learning
computing system. <em>TC</em>, <em>71</em>(1), 209–222. (<a
href="https://doi.org/10.1109/TC.2020.3044245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the broad successes of deep learning, many CPU-centric artificial intelligent computing systems employ specialized devices such as GPUs, FPGAs, and ASICs, which can be named as Deep Learning Processing Units (DLPUs), for processing computation-intensive deep learning tasks. The separation between the scalar control operations mapped on CPUs and the vector computation operations mapped on DLPUs causes the frequent and costly interactions between CPUs and DLPUs, leading to the Interaction Wall . Moreover, the increasing algorithm complexity and DLPU computation speed would further aggravate the interaction wall substantially. To break the interaction wall, we propose a novel DLPU-centric deep learning computing system consisting of an exception-oriented programming (EOP) model and the architectural support of CPULESS DLPU . The EOP model processes scalar control operations of a deep learning task as exception handlers to maximally avoid stalling the crucial and dominated vector computation operations. Together with the CPULESS DLPU which integrates a scalar processing unit (SPU) for scalar control operations and the parallel processing unit (PPU) for vector computation operations into a fused pipeline, the proposed DLPU-centric system can cost-effectively leverage the EOP model to execute the two kinds of operations simultaneously without disturbing each other. Compared with a state-of-the-art commodity CPU-centric system with discrete V100 GPU via PCIe bus, experimental results show that our DLPU-centric system achieves 10.30× better performance and 92.99 percent energy savings, respectively. Moreover, compared with a CPU-centric version of DLPU system where the SPU serves as the host with integrated PPU, the proposed DLPU-centric system still achieves 15.60 percent better performance from avoided interactions.},
  archive      = {J_TC},
  author       = {Zidong Du and Qi Guo and Yongwei Zhao and Xi Zeng and Ling Li and Limin Cheng and Zhiwei Xu and Ninghui Sun and Yunji Chen},
  doi          = {10.1109/TC.2020.3044245},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {209-222},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Breaking the interaction wall: A DLPU-centric deep learning computing system},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Speculative barriers with transactional memory. <em>TC</em>,
<em>71</em>(1), 197–208. (<a
href="https://doi.org/10.1109/TC.2020.3044234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transactional Memory (TM) is a synchronization model for parallel programming which provides optimistic concurrency control. Transactions can run in parallel and are only serialized in case of conflict. In this article we use hardware TM (HTM) to implement an optimistic speculative barrier (SB) to replace the lock-based solution. SBs leverage HTM support to elide barriers speculatively. When a thread reaches an SB, a new SB transaction is started, keeping the updates private to the thread, and letting the HTM system detect potential conflicts. Once the last thread reaches the corresponding SB, the speculative threads can commit their changes. The main contributions of this work are: an API for SBs implemented with HTM extensions; a procedure to check the speculation state in between barriers to enable SBs with non-transactional codes; a HTM SB-aware conflict resolution enhancement where SB transactions stall on a conflict with a standard transaction; and a set of SB use guidelines derived from our experience on using SBs in a variety of applications. We evaluated our proposals in two different architectures with a full-system simulator and an IBM Power8 server. Results show an overall performance improvement of SBs over traditional barriers.},
  archive      = {J_TC},
  author       = {Manuel Pedrero and Ricardo Quislant and Eladio Gutierrez and Emilio L. Zapata and Oscar Plata},
  doi          = {10.1109/TC.2020.3044234},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {197-208},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Speculative barriers with transactional memory},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A proximal iteratively reweighted approach for efficient
network sparsification. <em>TC</em>, <em>71</em>(1), 185–196. (<a
href="https://doi.org/10.1109/TC.2020.3044142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The huge size of deep neural networks makes it difficult to deploy on the embedded platforms with limited computation resources directly. In this article, we propose a novel trimming approach to determine the redundant parameters of the trained deep neural network in a layer-wise manner to produce a compact neural network. This is achieved by minimizing a nonconvex sparsity-inducing term of the network parameters while maintaining the response close to the original one. We present a proximal iteratively reweighted method to resolve the resulting nonconvex model, which approximates the nonconvex objective by a weighted l1 norm of the network parameters. Moreover, to alleviate the computational burden, we develop a novel termination criterion during the subproblem solution, significantly reducing the total pruning time. Global convergence analysis and a worst-case O(1/k) ergodic convergence rate for our proposed algorithm is established. Numerical experiments demonstrate the proposed approach is efficient and reliable.},
  archive      = {J_TC},
  author       = {Hao Wang and Xiangyu Yang and Yuanming Shi and Jun Lin},
  doi          = {10.1109/TC.2020.3044142},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {185-196},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A proximal iteratively reweighted approach for efficient network sparsification},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient CP-ABE scheme with shared decryption in cloud
storage. <em>TC</em>, <em>71</em>(1), 175–184. (<a
href="https://doi.org/10.1109/TC.2020.3043950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute-based encryption (ABE) is a preferred technology used to access control the data stored in the cloud servers. However, in many cases, the authorized decryption user may be unable to decrypt the ciphertext in time for some reason. To be on the safe side, several alternate users are delegated to cooperate to decrypt the ciphertext, instead of one user doing that. We provide a ciphertext-policy ABE scheme with shared decryption in this article. An authorized user can recover the messages independently. At the same time, these alternate users (semi-authorized users) can work together to get the messages. We also improve the basic scheme to ensure that the semi-authorized users perform the decryption tasks honestly. An integrated access tree is used to improve the efficiency for our scheme. The new scheme is proved CPA-secure in the standard model. The experimental result shows that our scheme is very efficient on both computational overhead and storage cost.},
  archive      = {J_TC},
  author       = {Ningyu Chen and Jiguo Li and Yichen Zhang and Yuyan Guo},
  doi          = {10.1109/TC.2020.3043950},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {175-184},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient CP-ABE scheme with shared decryption in cloud storage},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A high-level synthesis methodology for energy and
reliability-oriented designs. <em>TC</em>, <em>71</em>(1), 161–174. (<a
href="https://doi.org/10.1109/TC.2020.3043885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shrinking technology sizes of the CMOS circuits makes it possible to place more transistors on a single chip at each technology generation. On the other hand, circuits become more vulnerable to radiation effects due to lower supply and threshold voltage levels; thus, the number of transient faults in circuits tends to increase. Moreover, energy reduction techniques also negatively affect the reliability of circuits. Traditional high-level synthesis (HLS) methods usually consider only area and latency along with either energy or reliability. Especially the effect of using different voltage levels on reliability is completely ignored by previous studies. In this article, we present two new HLS methods for application-specific integrated circuit (ASIC) design under area and timing constraints with the objectives of low energy consumption and high reliability. For the mapping and scheduling steps of HLS, we propose integer linear programming (ILP) and genetic algorithm (GA)-based optimization methods. While ILP provides the optimum results, the CPU time increases exponentially with the number of application nodes. On the other hand, GA-based metaheuristic is faster and determines optimum or near-optimum results in shorter times than ILP. Additionally, we use a selective duplication method to further improve the overall reliability.},
  archive      = {J_TC},
  author       = {Selma Dilek and Rawan Smri and Suleyman Tosun and Deniz Dal},
  doi          = {10.1109/TC.2020.3043885},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {161-174},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A high-level synthesis methodology for energy and reliability-oriented designs},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scheduling of real-time tasks with multiple critical
sections in multiprocessor systems. <em>TC</em>, <em>71</em>(1),
146–160. (<a href="https://doi.org/10.1109/TC.2020.3043742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of multiprocessor synchronization and locking protocols is a key factor to utilize the computation power of multiprocessor systems under real-time constraints. While multiple protocols have been developed in the past decades, their performance highly depends on the task partition and prioritization. The recently proposed Dependency Graph Approach showed its advantages and attracted a lot of interest. It is, however, restricted to task sets where each task has at most one critical section. In this article, we remove this restriction and demonstrate how to utilize algorithms for the classical job shop scheduling problem to construct a dependency graph for tasks with multiple critical sections. To show the applicability, we discuss the implementation in $\text{LITMUS}^{\text{RT}}$ and report the overheads. Moreover, we provide extensive numerical evaluations under different configurations, which in many situations show significant improvement compared to the state-of-the-art.},
  archive      = {J_TC},
  author       = {Jian-Jia Chen and Junjie Shi and Georg von der Brüggen and Niklas Ueter},
  doi          = {10.1109/TC.2020.3043742},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {146-160},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Scheduling of real-time tasks with multiple critical sections in multiprocessor systems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-triangular self-synchronizing stream ciphers.
<em>TC</em>, <em>71</em>(1), 134–145. (<a
href="https://doi.org/10.1109/TC.2020.3043714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose an instantiation, called ${\sf Stanislas}$ , of a dedicated Self-Synchronizing Stream Cipher (SSSC) involving an automaton with finite input memory using non-triangular state transition functions. Previous existing SSSC are based on automata with shifts or triangular functions ( $T$ –functions) as state transition functions. Our algorithm ${\sf Stanislas}$ admits a matrix representation deduced from a general and systematic methodology called Linear Parameter Varying (LPV). This particular representation comes from the automatic theory and from a special property of dynamical systems called flatness. Hardware implementations and comparisons with some state-of-the-art stream ciphers on Xilinx FPGAs are presented. It turns out that ${\sf Stanislas}$ provides bigger throughput than the considered stream ciphers (synchronous and self-synchronizing) when straightforward implementations are considered. Moreover, its synchronization delay is much smaller than the SSSC Moustique (40 clock cycles instead of 105) and the standard approach CFB1-AES128 (40 clock cycles instead of 128).},
  archive      = {J_TC},
  author       = {Julien Francq and Loïc Besson and Paul Huynh and Philippe Guillot and Gilles Millerioux and Marine Minier},
  doi          = {10.1109/TC.2020.3043714},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {134-145},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Non-triangular self-synchronizing stream ciphers},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting symmetrization and d-reducibility for approximate
logic synthesis. <em>TC</em>, <em>71</em>(1), 121–133. (<a
href="https://doi.org/10.1109/TC.2020.3043476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate synthesis is a recent trend in logic synthesis where one changes some outputs of a logic specification, within the error tolerance of a given application, to reduce the complexity of the final implementation. We attack the problem by exploiting the allowed flexibility in order to maximize the regularity of the specified Boolean functions. Specifically, we consider two types of regularity: symmetry and D-reducibility , and contribute two algorithms to find, respectively, a symmetric and a D-reducible approximation of a given target function $f$ , within the given error rate threshold if possible. When targeting symmetry, we characterize and compute polynomially the closest symmetric approximation, i.e., the symmetric function obtained by injecting the minimum number of errors in the original incompletely specified Boolean function, with an unbounded number of errors; then, we discuss strategies to achieve partial symmetrization of the original specification while satisfying given error bounds. Finally, we present a polynomial heuristic algorithm to compute a D-reducible approximation of an incompletely specified target function, under a bit error metric. Experimental results on classical and new benchmarks confirm the effectiveness of the proposed approaches.},
  archive      = {J_TC},
  author       = {Anna Bernasconi and Valentina Ciriani and Tiziano Villa},
  doi          = {10.1109/TC.2020.3043476},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {121-133},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Exploiting symmetrization and D-reducibility for approximate logic synthesis},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-latency ASIC algorithms of modular squaring of large
integers for VDF evaluation. <em>TC</em>, <em>71</em>(1), 107–120. (<a
href="https://doi.org/10.1109/TC.2020.3043400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is an attempt in quest of the fastest hardware algorithms for the computation of the evaluation component of verifiable delay functions (VDFs), $a^{2^T} \bmod N$ , proposed for use in various distributed protocols, in which no party is assumed to compute it significantly faster than other participants. To this end, we propose a class of modular squaring algorithms suitable for low-latency ASIC implementations. The proposed algorithms aim to achieve highest levels of parallelization that have not been explored in previous works in the literature, which usually pursue more balanced optimization of speed and area. For this, we utilize redundant representations of integers and introduce three modular squaring algorithms that work with integers in redundant forms: i) Montgomery algorithm, ii) memory-based algorithm and iii) direct reduction algorithm for fixed moduli. All algorithms enable $O(\log k)$ depth circuit implementations, where $k$ is the bit-size of the modulus $N$ in the VDF function. We analyze and compare gate level-circuits of the proposed algorithms and provide estimates for their critical path delay and gate count.},
  archive      = {J_TC},
  author       = {Ahmet Can Mert and Erdinç Öztürk and Erkay Savaş},
  doi          = {10.1109/TC.2020.3043400},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {107-120},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Low-latency ASIC algorithms of modular squaring of large integers for VDF evaluation},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performance optimization of many-core systems by exploiting
task migration and dark core allocation. <em>TC</em>, <em>71</em>(1),
92–106. (<a href="https://doi.org/10.1109/TC.2020.3042663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an effective scheme often adopted for performance tuning in many-core processors, task migration provides an opportunity for “hot” tasks to be migrated to run on a “cool” core that has a lower temperature. When a task needs to migrate from one processor core to another, the migration can embark on numerous modes defined by the migration paths undertaken and/or the destinations of the migration. Selecting the right migration mode that a task shall follow has always been difficult, and it can be more challenging with the existence of dark cores that can be called back to service (reactivated), which ushers in additional task migration modes. Previous works have demonstrated that dark cores can be placed near the active cores to reduce power density so that the active cores can run at higher voltage/frequency levels for higher performance. However, the existing task migration schemes neither consider the impact of dark cores on each application&#39;s performance, nor exploit performance trade-off under different migration modes. Unlike the existing task migration schemes, in this article, a runtime task migration algorithm that simultaneously takes both migration modes and dark cores into consideration is proposed, and it essentially has two major steps. In the first step, for a specific migration mode that is tied to an application whose tasks need to be migrated, the number of dark cores is determined so that the overall performance is maximized. The second step is to find an appropriate core region and its location for each application to optimize the communication latency and computation performance; during this step, focus is placed on reducing the fragmentation of the free core regions resulting from the task migration. Experimental results have confirmed that our approach achieves over 50 percent reduction in total response time when compared to recently proposed thermal-aware runtime task migration approachess.},
  archive      = {J_TC},
  author       = {Shengyan Wen and Xiaohang Wang and Amit Kumar Singh and Yingtao Jiang and Mei Yang},
  doi          = {10.1109/TC.2020.3042663},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {92-106},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Performance optimization of many-core systems by exploiting task migration and dark core allocation},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-resource VNF deployment in a heterogeneous cloud.
<em>TC</em>, <em>71</em>(1), 81–91. (<a
href="https://doi.org/10.1109/TC.2020.3042247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging paradigm of Network Function Virtualization (NFV) promises to shorten the renewal cycles of network functions and reduce the capital expenses by flexibly deploying virtualized network functions (VNFs) implementation on commodity servers. However, the required resource of each type (CPU, memory, etc.) for the running VNF should be provisioned to guarantee the performance when processing packets. This comes with different deployment cost, especially in a heterogeneous cloud consisting of a large number of network function platforms from various vendors. To optimally operate VNFs, it is necessary for the network operator to dynamically deploy VNFs in the expensive cloud infrastructures. In this article, we initiate the study of minimizing the deployment cost under multi-resource constraints in a heterogeneous cloud. We formulate multi-resource VNF deployment problem (MVDP) as an optimization program and prove its hardness. We propose an offline $(1, d+1)$ -bicriteria approximation algorithm and an $(\mathcal {O}(1), \mathcal {O}(n \cdot \log n))$ -competitive online algorithm to deploy VNFs in a scalable manner, where $d$ is the number of resource types and $n$ is the number of required VNFs. Large-scale simulations and DPDK-based OpenNetVM implementation show that our algorithms can reduce the overall cost by 34\% and improve the performance in terms of multi-resource allocation.},
  archive      = {J_TC},
  author       = {Jiaqi Zheng and Zixuan Zhang and Qiufang Ma and Xiaofeng Gao and Chen Tian and Guihai Chen},
  doi          = {10.1109/TC.2020.3042247},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {81-91},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Multi-resource VNF deployment in a heterogeneous cloud},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning for HDD health assessment: An application
based on LSTM. <em>TC</em>, <em>71</em>(1), 69–80. (<a
href="https://doi.org/10.1109/TC.2020.3042053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hard disk drive failures are one of the most common causes of service downtime in data centers. Predictive maintenance techniques have been adopted to extend the Remaining Useful Life (RUL) of these drives, and minimize service shortage and data loss. Several approaches based on machine and deep learning techniques have been proposed to address these issues, mostly exploiting models based on Self-Monitoring analysis and Reporting Technology (SMART) attributes. While these models have proven to be reliable, their performance is affected by the lack of information about the proximity of disk failure in time. Moreover, many of these techniques are sensitive to the highly unbalanced nature of existing data-sets, in terms of good to failed hard disk ratio. In this article, we propose a LSTM (Long Short Term Memory)-based model combining SMART attributes and temporal analysis for estimating a hard drive health status according to its time to failure. Our approach outperforms state-of-the-art methods when evaluated on two data-sets, one containing hourly samples from 23395 disks and the other reporting daily samples from 29878‬ disks. Experimental results showed that our approach is well suited to data-sets with different sampling periods, being able to predict hard drive health status up to 45 days before failure.},
  archive      = {J_TC},
  author       = {Aniello De Santo and Antonio Galli and Michela Gravina and Vincenzo Moscato and Giancarlo Sperlì},
  doi          = {10.1109/TC.2020.3042053},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {69-80},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Deep learning for HDD health assessment: An application based on LSTM},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive page migration policy with huge pages in tiered
memory systems. <em>TC</em>, <em>71</em>(1), 53–68. (<a
href="https://doi.org/10.1109/TC.2020.3036686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To accommodate the growing demand for memory capacity in a cost-effective way, multiple types of memory are incorporated in a single system. In such tiered memory systems consisting of small fast and large slow memory components, accurately identifying the performance importance of pages is critical to properly migrate hot pages to fast memory. Meanwhile, growing address translation cost due to the increasing memory footprints, helped adopting huge pages in common systems. Although such page hotness identification problems have existed for a long time, this article revisits the problem in the new context of tiered memory systems and huge pages. This article first investigates the memory locality behaviors of applications with three potential migration polices, least-recently-used (LRU), least-frequently-used (LFU), and random with huge pages. The evaluation shows that none of the three migration policies excel the others, as the effectiveness of each policy depends on application behaviors. In addition, the results show huge pages can be effective even with page migration, if a proper migration policy is used. Based on the observation, this paper proposes a novel dynamic policy selection mechanism, which identifies the best migration policy for a given workload. It allows multiple concurrently running workloads to adopt different policies. To find the optimal one for each workload, this study first identifies key features that must be inferred from limited approximate memory access information collected using accessed bits in page tables. In addition, it proposes a parallel emulation of alternative policies to assess the benefit of possible alternatives. The proposed dynamic policy selection can achieve 23.8percent performance improvement compared to a prior approximate mechanism based on LRU lists in Linux systems.},
  archive      = {J_TC},
  author       = {Taekyung Heo and Yang Wang and Wei Cui and Jaehyuk Huh and Lintao Zhang},
  doi          = {10.1109/TC.2020.3036686},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {53-68},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Adaptive page migration policy with huge pages in tiered memory systems},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FadingBF: A bloom filter with consistent guarantees for
online applications. <em>TC</em>, <em>71</em>(1), 40–52. (<a
href="https://doi.org/10.1109/TC.2020.3036424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bloom filter (BF), when used by an online application, experiences monotonically increasing false-positive errors. The decay of stale elements can control false-positives. Existing mechanisms for decay require unreasonable storage and computation. Inexpensive methods reset the BF periodically, resulting in inconsistent guarantees and performance issues in the underlying computing system. In this article, we propose Fading Bloom filter (FadingBF), which can provide inexpensive yet safe decay of elements. FadingBF neither requires additional storage nor computation to achieve this but instead exploits the underlying storage medium’s intrinsic properties, i.e., DRAM capacitor characteristics. We realize FadingBF by implementing the BF on a DRAM memory module with its periodic refresh disabled . Consequently, the capacitors holding the data elements that are not accessed frequently will predictably lose charge and naturally decay. The retention time of capacitors guarantees against premature deletion. However, some capacitors may store information longer than required due to the FadingBF’s software and hardware variables. Using an analytical model of the FadingBF, we show that carefully tuning its parameters can minimize such cases. For a surveillance application, we demonstrate that FadingBF achieves better guarantees through graceful decay, consumes 57 percent lesser energy, and has a system load that is lesser than the standard BF.},
  archive      = {J_TC},
  author       = {Prasanna Karthik Vairam and Pratyush Kumar and Chester Rebeiro and V. Kamakoti},
  doi          = {10.1109/TC.2020.3036424},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {40-52},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FadingBF: A bloom filter with consistent guarantees for online applications},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online service function chain placement for
cost-effectiveness and network congestion control. <em>TC</em>,
<em>71</em>(1), 27–39. (<a
href="https://doi.org/10.1109/TC.2020.3035991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging network function virtualization is migrating traditional middleboxes, e.g., firewalls, load balancers, proxies, from dedicated hardware to virtual network functions (VNFs) running on commercial servers defined as network points of presence (N-PoPs). VNFs further chain up for more complex network services called service function chains (SFCs). SFCs introduce new flexibility and scalability which greatly reduce expenses and rolling out time of network services. However, chasing the lowest cost may lead to congestion on popular N-PoPs and links, thus resulting in performance degradation or violation of service-level agreements. To address this problem, we propose a novel scheme that reduces the operating cost and controls network congestion at the same time. It does so by placing VNFs and routing flows among them jointly. Given the problem is NP-hard, we design an approximation algorithm named candidate path selection (CPS) with a theoretical performance guarantee. We then consider cases when SFC demands fluctuate frequently. We propose an online candidate path selection (OCPS) algorithm to handle such cases considering the VNF migration cost. OCPS is designed to preserve good performance under various migration costs and prediction errors. Extensive simulation results highlight that CPS and OCPS algorithms perform better than baselines and comparably to the optimal solution.},
  archive      = {J_TC},
  author       = {Xiaojun Shang and Zhenhua Liu and Yuanyuan Yang},
  doi          = {10.1109/TC.2020.3035991},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {27-39},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Online service function chain placement for cost-effectiveness and network congestion control},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A lightweight and efficient GPU for NDP utilizing data
access pattern of image processing. <em>TC</em>, <em>71</em>(1), 13–26.
(<a href="https://doi.org/10.1109/TC.2020.3035826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the demand for image applications with high resolution increases, the importance of the system for image processing is growing. Graphics processing units (GPUs) can increase computational capacity with massive parallelism, but are still subject to limited memory bandwidth. Near-data-processing (NDP) is expected to mitigate the performance and energy overhead caused as a result of data transfer by performing computations on the logic die of 3D-stacked memory. Although prior studies have demonstrated the advantages of NDP, a NDP solution focused on image processing has not yet been developed. This article proposes a GPU-based NDP architecture and well-matched optimization strategies considering both the characteristics of image applications and NDP constraints. First, data allocation to the processing unit is addressed to maintain the data locality and data access pattern. Second, a lightweight yet efficient NDP GPU architecture is proposed. By applying a prefetcher that leverages the pattern-aware data allocation, the number of active warps and the on-chip SRAM size of the NDP are significantly reduced. This enables the NDP constraints to be satisfied and a greater number of processing units to be integrated on a logic die. The evaluation results show that the proposed NDP GPU improves the performance by 1.85× and consumes 82.7 percent energy compared to the baseline NDP GPU.},
  archive      = {J_TC},
  author       = {Jungwoo Choi and Boyeal Kim and Ji-Ye Jeon and Hyuk-Jae Lee and Euicheol Lim and Chae Eun Rhee},
  doi          = {10.1109/TC.2020.3035826},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {13-26},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A lightweight and efficient GPU for NDP utilizing data access pattern of image processing},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantum dimension reduction for pattern recognition in
high-resolution spatio-spectral data. <em>TC</em>, <em>71</em>(1), 1–12.
(<a href="https://doi.org/10.1109/TC.2020.3034883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The promises of advanced quantum computing technology have driven research in the simulation of quantum computers on classical hardware, where the feasibility of quantum algorithms for real-world problems can be investigated. In domains such as High Energy Physics (HEP) and Remote Sensing Hyperspectral Imagery, classical computing systems are held back by enormous readouts of high-resolution data. Due to the multi-dimensionality of the readout data, processing and performing pattern recognition operations for this enormous data are both computationally intensive and time-consuming. In this article, we propose a methodology that utilizes Quantum Haar Transform (QHT) and a modified Grover&#39;s search algorithm for time-efficient dimension reduction and dynamic pattern recognition in data sets that are characterized by high spatial resolution and high dimensionality. QHT is performed on the data to reduce its dimensionality at preserved spatial locality, while the modified Grover&#39;s search algorithm is used to search for dynamically changing multiple patterns in the reduced data set. By performing search operations on the reduced data set, processing overheads are minimized. Moreover, quantum techniques produce results in less time than classical dimension reduction and search methods. The feasibility of the proposed methodology is verified by emulating the quantum algorithms on classical hardware based on field programmable gate arrays (FPGAs). We present designs of the quantum circuits for multi-dimensional QHT and multi-pattern Grover&#39;s search. We also present two emulation techniques and the corresponding hardware architectures for this methodology. A high performance reconfigurable computer (HPRC) was used for the experimental evaluation, and high-resolution images were used as the input data set. Analysis of the methods and implications of the experimental results are discussed.},
  archive      = {J_TC},
  author       = {Naveed Mahmud and Bennett Haase-Divine and Andrew MacGillivray and Esam El-Araby},
  doi          = {10.1109/TC.2020.3034883},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Quantum dimension reduction for pattern recognition in high-resolution spatio-spectral data},
  volume       = {71},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
