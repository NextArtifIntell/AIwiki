<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TVCG_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tvcg---427">TVCG - 427</h2>
<ul>
<li><details>
<summary>
(2022). Identification and classification of off-vertex critical
points for contour tree construction on unstructured meshes of
hexahedra. <em>TVCG</em>, <em>28</em>(12), 5178–5180. (<a
href="https://doi.org/10.1109/TVCG.2021.3074438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topology of isosurfaces changes at isovalues of critical points, making such points an important feature when building contour trees or Morse-Smale complexes. Hexahedral elements with linear interpolants can contain additional off-vertex critical points in element bodies and on element faces. Moreover, a point on the face of a hexahedron which is critical in the element-local context is not necessarily critical in the global context. Weber et al. (2002) introduce a method to determine whether critical points on faces are also critical in the global context, based on the gradient of the asymptotic decider (G. M. Nielson and B. Hamann) (1991) in each element that shares the face. However, as defined, the method of Weber et al. contains an error, and can lead to incorrect results. In this work we correct the error.},
  archive      = {J_TVCG},
  author       = {Marius K. Koch and Paul H. J. Kelly and Peter E. Vincent},
  doi          = {10.1109/TVCG.2021.3074438},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5178-5180},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Identification and classification of off-vertex critical points for contour tree construction on unstructured meshes of hexahedra},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deterministic linear time constrained triangulation using
simplified earcut. <em>TVCG</em>, <em>28</em>(12), 5172–5177. (<a
href="https://doi.org/10.1109/TVCG.2021.3070046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triangulation algorithms that conform to a set of non-intersecting input segments typically proceed in an incremental fashion, by inserting points first, and then segments. Inserting a segment amounts to: (1) deleting all the triangles it intersects; (2) filling the so generated hole with two polygons that have the wanted segment as shared edge; (3) triangulate each polygon separately. In this article we prove that these polygons are such that all their convex vertices but two can be used to form triangles in an earcut fashion, without the need to check whether other polygon points are located within each ear. The fact that any simple polygon contains at least three convex vertices guarantees the existence of a valid ear to cut, ensuring convergence. Not only this translates to an optimal deterministic linear time triangulation algorithm, but such algorithm is also trivial to implement. We formally prove the correctness of our approach, also validating it in practical applications and comparing it with prior art.},
  archive      = {J_TVCG},
  author       = {Marco Livesu and Gianmarco Cherchi and Riccardo Scateni and Marco Attene},
  doi          = {10.1109/TVCG.2021.3070046},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5172-5177},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Deterministic linear time constrained triangulation using simplified earcut},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on affective and cognitive VR. <em>TVCG</em>,
<em>28</em>(12), 5154–5171. (<a
href="https://doi.org/10.1109/TVCG.2021.3110459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Virtual Reality (VR), users can be immersed in emotionally intense and cognitively engaging experiences. Yet, despite strong interest from scholars and a large amount of work associating VR and Affective and Cognitive States (ACS), there is a clear lack of structured and systematic form in which this research can be classified. We define “Affective and Cognitive VR” to relate to works which (1) induce ACS, (2) recognize ACS, or (3) exploit ACS by adapting virtual environments based on ACS measures. This survey clarifies the different models of ACS, presents the methods for measuring them with their respective advantages and drawbacks in VR, and showcases Affective and Cognitive VR studies done in an Immersive Virtual Environment (IVE) in a non-clinical context. Our article covers the main research lines in Affective and Cognitive VR. We provide a comprehensive list of references with the analysis of 63 research articles and summarize future works directions.},
  archive      = {J_TVCG},
  author       = {Tiffany Luong and Anatole Lecuyer and Nicolas Martin and Ferran Argelaguet},
  doi          = {10.1109/TVCG.2021.3110459},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5154-5171},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A survey on affective and cognitive VR},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on ML4VIS: Applying machine learning advances to
data visualization. <em>TVCG</em>, <em>28</em>(12), 5134–5153. (<a
href="https://doi.org/10.1109/TVCG.2021.3106142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the great success of machine learning (ML), researchers have applied ML techniques to visualizations to achieve a better design, development, and evaluation of visualizations. This branch of studies, known as ML4VIS, is gaining increasing research attention in recent years. To successfully adapt ML techniques for visualizations, a structured understanding of the integration of ML4VIS is needed. In this article, we systematically survey 88 ML4VIS studies, aiming to answer two motivating questions: “what visualization processes can be assisted by ML?” and “how ML techniques can be used to solve visualization problems? ” This survey reveals seven main processes where the employment of ML techniques can benefit visualizations: Data Processing4VIS, Data-VIS Mapping, Insight Communication, Style Imitation, VIS Interaction, VIS Reading, and User Profiling . The seven processes are related to existing visualization theoretical models in an ML4VIS pipeline, aiming to illuminate the role of ML-assisted visualization in general visualizations. Meanwhile, the seven processes are mapped into main learning tasks in ML to align the capabilities of ML with the needs in visualization. Current practices and future opportunities of ML4VIS are discussed in the context of the ML4VIS pipeline and the ML-VIS mapping. While more studies are still needed in the area of ML4VIS, we hope this article can provide a stepping-stone for future exploration. A web-based interactive browser of this survey is available at https://ml4vis.github.io .},
  archive      = {J_TVCG},
  author       = {Qianwen Wang and Zhutian Chen and Yong Wang and Huamin Qu},
  doi          = {10.1109/TVCG.2021.3106142},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5134-5153},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A survey on ML4VIS: Applying machine learning advances to data visualization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A conceptual model and taxonomy for collaborative augmented
reality. <em>TVCG</em>, <em>28</em>(12), 5113–5133. (<a
href="https://doi.org/10.1109/TVCG.2021.3101545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To support the nuances of collaborative work, many researchers have been exploring the field of Augmented Reality (AR), aiming to assist in co-located or remote scenarios. Solutions using AR allow taking advantage from seamless integration of virtual objects and real-world objects, thus providing collaborators with a shared understanding or common ground environment. However, most of the research efforts, so far, have been devoted to experiment with technology and mature methods to support its design and development. Therefore, it is now time to understand where the field stands and how well can it address collaborative work with AR, to better characterize and evaluate the collaboration process. In this article, we perform an analysis of the different dimensions that should be taken into account when analysing the contributions of AR to the collaborative work effort. Then, we bring these dimensions forward into a conceptual framework and propose an extended human-centered taxonomy for the categorization of the main features of Collaborative AR. Our goal is to foster harmonization of perspectives for the field, which may help create a common ground for systematization and discussion. We hope to influence and improve how research in this field is reported by providing a structured list of the defining characteristics. Finally, some examples of the use of the taxonomy are presented to show how it can serve to gather information for characterizing AR-supported collaborative work, and illustrate its potential as the grounds to elicit further studies.},
  archive      = {J_TVCG},
  author       = {Bernardo Marques and Samuel Silva and João Alves and Tiago Araújo and Paulo Dias and Beatriz Sousa Santos},
  doi          = {10.1109/TVCG.2021.3101545},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5113-5133},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A conceptual model and taxonomy for collaborative augmented reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Survey on visual analysis of event sequence data.
<em>TVCG</em>, <em>28</em>(12), 5091–5112. (<a
href="https://doi.org/10.1109/TVCG.2021.3100413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event sequence data record series of discrete events in the time order of occurrence. They are commonly observed in a variety of applications ranging from electronic health records to network logs, with the characteristics of large-scale, high-dimensional and heterogeneous. This high complexity of event sequence data makes it difficult for analysts to manually explore and find patterns, resulting in ever-increasing needs for computational and perceptual aids from visual analytics techniques to extract and communicate insights from event sequence datasets. In this paper, we review the state-of-the-art visual analytics approaches, characterize them with our proposed design space, and categorize them based on analytical tasks and applications. From our review of relevant literature, we have also identified several remaining research challenges and future research opportunities.},
  archive      = {J_TVCG},
  author       = {Yi Guo and Shunan Guo and Zhuochen Jin and Smiti Kaul and David Gotz and Nan Cao},
  doi          = {10.1109/TVCG.2021.3100413},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5091-5112},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Survey on visual analysis of event sequence data},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Being an avatar “for real”: A survey on virtual embodiment
in augmented reality. <em>TVCG</em>, <em>28</em>(12), 5071–5090. (<a
href="https://doi.org/10.1109/TVCG.2021.3099290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual self-avatars have been increasingly used in Augmented Reality (AR) where one can see virtual content embedded into physical space. However, little is known about the perception of self-avatars in such a context. The possibility that their embodiment could be achieved in a similar way as in Virtual Reality opens the door to numerous applications in education, communication, entertainment, or the medical field. This article aims to review the literature covering the embodiment of virtual self-avatars in AR. Our goal is (i) to guide readers through the different options and challenges linked to the implementation of AR embodiment systems, (ii) to provide a better understanding of AR embodiment perception by classifying the existing knowledge, and (iii) to offer insight on future research topics and trends for AR and avatar research. To do so, we introduce a taxonomy of virtual embodiment experiences by defining a “body avatarization” continuum. The presented knowledge suggests that the sense of embodiment evolves in the same way in AR as in other settings, but this possibility has yet to be fully investigated. We suggest that, whilst it is yet to be well understood, the embodiment of avatars has a promising future in AR and conclude by discussing possible directions for research.},
  archive      = {J_TVCG},
  author       = {Adélaïde Genay and Anatole Lécuyer and Martin Hachet},
  doi          = {10.1109/TVCG.2021.3099290},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5071-5090},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Being an avatar “for real”: A survey on virtual embodiment in augmented reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AI4VIS: Survey on artificial intelligence approaches for
data visualization. <em>TVCG</em>, <em>28</em>(12), 5049–5070. (<a
href="https://doi.org/10.1109/TVCG.2021.3099002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualizations themselves have become a data format. Akin to other data formats such as text and images, visualizations are increasingly created, stored, shared, and (re-)used with artificial intelligence (AI) techniques. In this survey, we probe the underlying vision of formalizing visualizations as an emerging data format and review the recent advance in applying AI techniques to visualization data (AI4VIS). We define visualization data as the digital representations of visualizations in computers and focus on data visualization (e.g., charts and infographics). We build our survey upon a corpus spanning ten different fields in computer science with an eye toward identifying important common interests. Our resulting taxonomy is organized around WHAT is visualization data and its representation, WHY and HOW to apply AI to visualization data. We highlight a set of common tasks that researchers apply to the visualization data and present a detailed discussion of AI approaches developed to accomplish those tasks. Drawing upon our literature review, we discuss several important research questions surrounding the management and exploitation of visualization data, as well as the role of AI in support of those processes. We make the list of surveyed papers and related material available online at.},
  archive      = {J_TVCG},
  author       = {Aoyu Wu and Yun Wang and Xinhuan Shu and Dominik Moritz and Weiwei Cui and Haidong Zhang and Dongmei Zhang and Huamin Qu},
  doi          = {10.1109/TVCG.2021.3099002},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5049-5070},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {AI4VIS: Survey on artificial intelligence approaches for data visualization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey of perception-based visualization studies by task.
<em>TVCG</em>, <em>28</em>(12), 5026–5048. (<a
href="https://doi.org/10.1109/TVCG.2021.3098240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge of human perception has long been incorporated into visualizations to enhance their quality and effectiveness. The last decade, in particular, has shown an increase in perception-based visualization research studies. With all of this recent progress, the visualization community lacks a comprehensive guide to contextualize their results. In this report, we provide a systematic and comprehensive review of research studies on perception related to visualization. This survey reviews perception-focused visualization studies since 1980 and summarizes their research developments focusing on low-level tasks, further breaking techniques down by visual encoding and visualization type. In particular, we focus on how perception is used to evaluate the effectiveness of visualizations, to help readers understand and apply the principles of perception of their visualization designs through a task-optimized approach. We concluded our report with a summary of the weaknesses and open research questions in the area.},
  archive      = {J_TVCG},
  author       = {Ghulam Jilani Quadri and Paul Rosen},
  doi          = {10.1109/TVCG.2021.3098240},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5026-5048},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A survey of perception-based visualization studies by task},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Three-dimensional mesh steganography and steganalysis: A
review. <em>TVCG</em>, <em>28</em>(12), 5006–5025. (<a
href="https://doi.org/10.1109/TVCG.2021.3075136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional (3-D) meshes are commonly used to represent virtual surfaces and volumes. Over the past decade, 3-D meshes have emerged in industrial, medical, and entertainment applications, being of large practical significance for 3-D mesh steganography and steganalysis. In this article, we provide a systematic survey of the literature on 3-D mesh steganography and steganalysis. Compared with an earlier survey (Girdhar et al. , 2017), we propose a new taxonomy of steganographic algorithms with four categories: 1) two-state domain, 2) LSB domain, 3) permutation domain, and 4) transform domain. Regarding steganalysis algorithms, we divide them into two categories: 1) universal steganalysis and 2) specific steganalysis. For each category, the history of technical developments and the current technological level are introduced and discussed. Finally, we highlight some promising future research directions and challenges in improving the performance of 3-D mesh steganography and steganalysis.},
  archive      = {J_TVCG},
  author       = {Hang Zhou and Weiming Zhang and Kejiang Chen and Weixiang Li and Nenghai Yu},
  doi          = {10.1109/TVCG.2021.3075136},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5006-5025},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Three-dimensional mesh steganography and steganalysis: A review},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VisRecall: Quantifying information visualisation
recallability via question answering. <em>TVCG</em>, <em>28</em>(12),
4995–5005. (<a href="https://doi.org/10.1109/TVCG.2022.3198163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite its importance for assessing the effectiveness of communicating information visually, fine-grained recallability of information visualisations has not been studied quantitatively so far. In this work, we propose a question-answering paradigm to study visualisation recallability and present VisRecall — a novel dataset consisting of 200 visualisations that are annotated with crowd-sourced human (N = 305) recallability scores obtained from 1,000 questions of five question types. Furthermore, we present the first computational method to predict recallability of different visualisation elements, such as the title or specific data values. We report detailed analyses of our method on VisRecall and demonstrate that it outperforms several baselines in overall recallability and FE-, F-, RV-, and U-question recallability. Our work makes fundamental contributions towards a new generation of methods to assist designers in optimising visualisations.},
  archive      = {J_TVCG},
  author       = {Yao Wang and Chuhan Jiao and Mihai Bâce and Andreas Bulling},
  doi          = {10.1109/TVCG.2022.3198163},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4995-5005},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VisRecall: Quantifying information visualisation recallability via question answering},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A unified understanding of deep NLP models for text
classification. <em>TVCG</em>, <em>28</em>(12), 4980–4994. (<a
href="https://doi.org/10.1109/TVCG.2022.3184186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of deep natural language processing (NLP) models for text classification has led to an urgent need for a unified understanding of these models proposed individually. Existing methods cannot meet the need for understanding different models in one framework due to the lack of a unified measure for explaining both low-level (e.g., words) and high-level (e.g., phrases) features. We have developed a visual analysis tool, DeepNLPVis, to enable a unified understanding of NLP models for text classification. The key idea is a mutual information-based measure, which provides quantitative explanations on how each layer of a model maintains the information of input words in a sample. We model the intra- and inter-word information at each layer measuring the importance of a word to the final prediction as well as the relationships between words, such as the formation of phrases. A multi-level visualization, which consists of a corpus-level, a sample-level, and a word-level visualization, supports the analysis from the overall training set to individual samples. Two case studies on classification tasks and comparison between models demonstrate that DeepNLPVis can help users effectively identify potential problems caused by samples and model architectures and then make informed improvements.},
  archive      = {J_TVCG},
  author       = {Zhen Li and Xiting Wang and Weikai Yang and Jing Wu and Zhengyan Zhang and Zhiyuan Liu and Maosong Sun and Hui Zhang and Shixia Liu},
  doi          = {10.1109/TVCG.2022.3184186},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4980-4994},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A unified understanding of deep NLP models for text classification},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Persistence cycles for visual exploration of persistent
homology. <em>TVCG</em>, <em>28</em>(12), 4966–4979. (<a
href="https://doi.org/10.1109/TVCG.2021.3110663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persistent homology is a fundamental tool in topological data analysis used for the most diverse applications. Information captured by persistent homology is commonly visualized using scatter plots representations. Despite being widely adopted, such a visualization technique limits user understanding and is prone to misinterpretation. This article proposes a new approach for the efficient computation of persistence cycles, a geometric representation of the features captured by persistent homology. We illustrate the importance of rendering persistence cycles when analyzing scalar fields, and we discuss the advantages that our approach provides compared to other techniques in topology-based visualization. We provide an efficient implementation of our approach based on discrete Morse theory, as a new module for the Topology Toolkit. We show that our implementation has comparable performance with respect to state-of-the-art toolboxes while providing a better framework for visually analyzing persistent homology information.},
  archive      = {J_TVCG},
  author       = {Federico Iuricich},
  doi          = {10.1109/TVCG.2021.3110663},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4966-4979},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Persistence cycles for visual exploration of persistent homology},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NeuroConstruct: 3D reconstruction and visualization of
neurites in optical microscopy brain images. <em>TVCG</em>,
<em>28</em>(12), 4951–4965. (<a
href="https://doi.org/10.1109/TVCG.2021.3109460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce NeuroConstruct, a novel end-to-end application for the segmentation, registration, and visualization of brain volumes imaged using wide-field microscopy. NeuroConstruct offers a Segmentation Toolbox with various annotation helper functions that aid experts to effectively and precisely annotate micrometer resolution neurites. It also offers an automatic neurites segmentation using convolutional neuronal networks (CNN) trained by the Toolbox annotations and somas segmentation using thresholding. To visualize neurites in a given volume, NeuroConstruct offers a hybrid rendering by combining iso-surface rendering of high-confidence classified neurites, along with real-time rendering of raw volume using a 2D transfer function for voxel classification score versus voxel intensity value. For a complete reconstruction of the 3D neurites, we introduce a Registration Toolbox that provides automatic coarse-to-fine alignment of serially sectioned samples. The quantitative and qualitative analysis show that NeuroConstruct outperforms the state-of-the-art in all design aspects. NeuroConstruct was developed as a collaboration between computer scientists and neuroscientists, with an application to the study of cholinergic neurons, which are severely affected in Alzheimer&#39;s disease.},
  archive      = {J_TVCG},
  author       = {Parmida Ghahremani and Saeed Boorboor and Pooya Mirhosseini and Chetan Gudisagar and Mala Ananth and David Talmage and Lorna W. Role and Arie E. Kaufman},
  doi          = {10.1109/TVCG.2021.3109460},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4951-4965},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NeuroConstruct: 3D reconstruction and visualization of neurites in optical microscopy brain images},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relationship-based point cloud completion. <em>TVCG</em>,
<em>28</em>(12), 4940–4950. (<a
href="https://doi.org/10.1109/TVCG.2021.3109392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a partial point cloud completion approach for scenes that are composed of multiple objects. We focus on pairwise scenes where two objects are in close proximity and are contextually related to each other, such as a chair tucked in a desk, a fruit in a basket, a hat on a hook and a flower in a vase. Different from existing point cloud completion methods, which mainly focus on single objects, we design a network that encodes not only the geometry of the individual shapes, but also the spatial relations between different objects. More specifically, we complete missing parts of the objects in a conditional manner, where the partial or completed point cloud of the other object is used as an additional input to help predict missing parts. Based on the idea of conditional completion, we further propose a two-path network, which is guided by a consistency loss between different sequences of completion. Our method can handle difficult cases where the objects heavily occlude each other. Also, it only requires a small set of training data to reconstruct the interaction area compared to existing completion approaches. We evaluate our method qualitatively and quantitatively via ablation studies and in comparison to the state-of-the-art point cloud completion methods.},
  archive      = {J_TVCG},
  author       = {Xi Zhao and Bowen Zhang and Jinji Wu and Ruizhen Hu and Taku Komura},
  doi          = {10.1109/TVCG.2021.3109392},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4940-4950},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Relationship-based point cloud completion},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GeodesicEmbedding (GE): A high-dimensional embedding
approach for fast geodesic distance queries. <em>TVCG</em>,
<em>28</em>(12), 4930–4939. (<a
href="https://doi.org/10.1109/TVCG.2021.3109975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop a novel method for fast geodesic distance queries. The key idea is to embed the mesh into a high-dimensional space, such that the euclidean distance in the high-dimensional space can induce the geodesic distance in the original manifold surface. However, directly solving the high-dimensional embedding problem is not feasible due to the large number of variables and the fact that the embedding problem is highly nonlinear. We overcome the challenges with two novel ideas. First, instead of taking all vertices as variables, we embed only the saddle vertices, which greatly reduces the problem complexity. We then compute a local embedding for each non-saddle vertex. Second, to reduce the large approximation error resulting from the purely euclidean embedding, we propose a cascaded optimization approach that repeatedly introduces additional embedding coordinates with a non-euclidean function to reduce the approximation residual. Using the precomputation data, our approach can determine the geodesic distance between any two vertices in near-constant time. Computational testing results show that our method is more desirable than previous geodesic distance queries methods.},
  archive      = {J_TVCG},
  author       = {Qianwei Xia and Juyong Zhang and Zheng Fang and Jin Li and Mingyue Zhang and Bailin Deng and Ying He},
  doi          = {10.1109/TVCG.2021.3109975},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4930-4939},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GeodesicEmbedding (GE): A high-dimensional embedding approach for fast geodesic distance queries},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiscale visualization: A structured literature analysis.
<em>TVCG</em>, <em>28</em>(12), 4918–4929. (<a
href="https://doi.org/10.1109/TVCG.2021.3109387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiscale visualizations are typically used to analyze multiscale processes and data in various application domains, such as the visual exploration of hierarchical genome structures in molecular biology. However, creating such multiscale visualizations remains challenging due to the plethora of existing work and the expression ambiguity in visualization research. Up to today, there has been little work to compare and categorize multiscale visualizations to understand their design practices. In this article, we present a structured literature analysis to provide an overview of common design practices in multiscale visualization research. We systematically reviewed and categorized 122 published journal or conference articles between 1995 and 2020. We organized the reviewed articles in a taxonomy that reveals common design factors. Researchers and practitioners can use our taxonomy to explore existing work to create new multiscale navigation and visualization techniques. Based on the reviewed articles, we examine research trends and highlight open research challenges.},
  archive      = {J_TVCG},
  author       = {Eren Cakmak and Dominik Jäckle and Tobias Schreck and Daniel A. Keim and Johannes Fuchs},
  doi          = {10.1109/TVCG.2021.3109387},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4918-4929},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multiscale visualization: A structured literature analysis},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven colormap adjustment for exploring spatial
variations in scalar fields. <em>TVCG</em>, <em>28</em>(12), 4902–4917.
(<a href="https://doi.org/10.1109/TVCG.2021.3109014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colormapping is an effective and popular visualization technique for analyzing patterns in scalar fields. Scientists usually adjust a default colormap to show hidden patterns by shifting the colors in a trial-and-error process. To improve efficiency, efforts have been made to automate the colormap adjustment process based on data properties (e.g., statistical data value or histogram distribution). However, as the data properties have no direct correlation to the spatial variations, previous methods may be insufficient to reveal the dynamic range of spatial variations hidden in the data. To address the above issues, we conduct a pilot analysis with domain experts and summarize three requirements for the colormap adjustment process. Based on the requirements, we formulate colormap adjustment as an objective function, composed of a boundary term and a fidelity term, which is flexible enough to support interactive functionalities. We compare our approach with alternative methods under a quantitative measure and a qualitative user study (25 participants), based on a set of data with broad distribution diversity. We further evaluate our approach via three case studies with six domain experts. Our method is not necessarily more optimal than alternative methods of revealing patterns, but rather is an additional color adjustment option for exploring data with a dynamic range of spatial variations.},
  archive      = {J_TVCG},
  author       = {Qiong Zeng and Yongwei Zhao and Yinqiao Wang and Jian Zhang and Yi Cao and Changhe Tu and Ivan Viola and Yunhai Wang},
  doi          = {10.1109/TVCG.2021.3109014},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4902-4917},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Data-driven colormap adjustment for exploring spatial variations in scalar fields},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geodesic tracks: Computing discrete geodesics with
track-based steiner point propagation. <em>TVCG</em>, <em>28</em>(12),
4887–4901. (<a href="https://doi.org/10.1109/TVCG.2021.3109042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a simple yet effective method for computing geodesic distances on triangle meshes. Unlike the popular window propagation methods that partition mesh edges into intervals of varying lengths, our method places evenly-spaced, source-independent Steiner points on edges. Given a source vertex, our method constructs a Steiner-point graph that partitions the surface into mutually exclusive tracks, called geodesic tracks. Inside each triangle, the tracks form sub-regions in which the change of distance field is approximately linear. Our method does not require any pre-computation, and can effectively balance speed and accuracy. Experimental results show that with 5 Steiner points on each edge, the mean relative error is less than 0.3 $\%$ for common 3D models used in the graphics community. We propose a set of effective filtering rules to eliminate a large amount of useless broadcast events. For a 1000K-face model, our method runs 10 times faster than the conventional Steiner point method that examines a complete graph of Steiner points in each triangle. We also observe that using more Steiner points increases the accuracy at only a small extra computational cost. Our method works well for meshes with poor triangulation and non-manifold configuration, which often poses challenges to the existing PDE methods. We show that geodesic tracks, as a new data structure that encodes rich information of discrete geodesics, support accurate geodesic path and isoline tracing, and efficient distance query. Our method can be easily extended to meshes with non-constant density functions and/or anisotropic metrics.},
  archive      = {J_TVCG},
  author       = {Wenlong Meng and Shiqing Xin and Changhe Tu and Shuangmin Chen and Ying He and Wenping Wang},
  doi          = {10.1109/TVCG.2021.3109042},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4887-4901},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Geodesic tracks: Computing discrete geodesics with track-based steiner point propagation},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geometry-guided dense perspective network for speech-driven
facial animation. <em>TVCG</em>, <em>28</em>(12), 4873–4886. (<a
href="https://doi.org/10.1109/TVCG.2021.3107669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Realistic speech-driven 3D facial animation is a challenging problem due to the complex relationship between speech and face. In this paper, we propose a deep architecture, called Geometry-guided Dense Perspective Network (GDPnet) , to achieve speaker-independent realistic 3D facial animation. The encoder is designed with dense connections to strengthen feature propagation and encourage the re-use of audio features, and the decoder is integrated with an attention mechanism to adaptively recalibrate point-wise feature responses by explicitly modeling interdependencies between different neuron units. We also introduce a non-linear face reconstruction representation as a guidance of latent space to obtain more accurate deformation, which helps solve the geometry-related deformation and is good for generalization across subjects. Huber and HSIC (Hilbert-Schmidt Independence Criterion) constraints are adopted to promote the robustness of our model and to better exploit the non-linear and high-order correlations. Experimental results on the public dataset and real scanned dataset validate the superiority of our proposed GDPnet compared with state-of-the-art model. The code is available for research purposes at http://cic.tju.edu.cn/faculty/likun/projects/GDPnet .},
  archive      = {J_TVCG},
  author       = {Jingying Liu and Binyuan Hui and Kun Li and Yunke Liu and Yu-Kun Lai and Yuxiang Zhang and Yebin Liu and Jingyu Yang},
  doi          = {10.1109/TVCG.2021.3107669},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4873-4886},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Geometry-guided dense perspective network for speech-driven facial animation},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GEViTRec: Data reconnaissance through recommendation using a
domain-specific visualization prevalence design space. <em>TVCG</em>,
<em>28</em>(12), 4855–4872. (<a
href="https://doi.org/10.1109/TVCG.2021.3107749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genomic Epidemiology (genEpi) is a branch of public health that uses many different data types including tabular, network, genomic, and geographic, to identify and contain outbreaks of deadly diseases. Due to the volume and variety of data, it is challenging for genEpi domain experts to conduct data reconnaissance; that is, have an overview of the data they have and make assessments toward its quality, completeness, and suitability. We present an algorithm for data reconnaissance through automatic visualization recommendation, GEViTRec. Our approach handles a broad variety of dataset types and automatically generates visually coherent combinations of charts, in contrast to existing systems that primarily focus on singleton visual encodings of tabular datasets. We automatically detect linkages across multiple input datasets by analyzing non-numeric attribute fields, creating a data source graph within which we analyze and rank paths. For each high-ranking path, we specify chart combinations with positional and color alignments between shared fields, using a gradual binding approach to transform initial partial specifications of singleton charts to complete specifications that are aligned and oriented consistently. A novel aspect of our approach is its combination of domain-agnostic elements with domain-specific information that is captured through a domain-specific visualization prevalence design space. Our implementation is applied to both synthetic data and real Ebola outbreak data. We compare GEViTRec&#39;s output to what previous visualization recommendation systems would generate, and to manually crafted visualizations used by practitioners. We conducted formative evaluations with ten genEpi experts to assess the relevance and interpretability of our results. Code, Data, and Study Materials Availability: https://github.com/amcrisan/GEVitRec .},
  archive      = {J_TVCG},
  author       = {Anamaria Crisan and Shannah E. Fisher and Jennifer L. Gardy and Tamara Munzner},
  doi          = {10.1109/TVCG.2021.3107749},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4855-4872},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GEViTRec: Data reconnaissance through recommendation using a domain-specific visualization prevalence design space},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluating effects of background stories on graph
perception. <em>TVCG</em>, <em>28</em>(12), 4839–4854. (<a
href="https://doi.org/10.1109/TVCG.2021.3107297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A graph is an abstract model that represents relations among entities, for example, the interactions between characters in a novel. A background story endows entities and relations with real-world meanings and describes the semantics and context of the abstract model, for example, the actual story that the novel presents. Considering practical experience and prior research, human viewers who are familiar with the background story of a graph and those who do not know the background story may perceive the same graph differently. However, no previous research has adequately addressed this problem. This research article thus presents an evaluation that investigated the effects of background stories on graph perception. Three hypotheses that focused on the role of visual focus areas, graph structure identification, and mental model formation on graph perception were formulated and guided three controlled experiments that evaluated the hypotheses using real-world graphs with background stories. An analysis of the resulting experimental data, which compared the performance of participants who read and did not read the background stories, obtained a set of instructive findings. First, having knowledge about a graph&#39;s background story influences participants’ focus areas during interactive graph explorations. Second, such knowledge significantly affects one&#39;s ability to identify community structures but not high degree and bridge structures. Third, this knowledge influences graph recognition under blurred visual conditions. These findings can bring new considerations to the design of storytelling visualizations and interactive graph explorations.},
  archive      = {J_TVCG},
  author       = {Ying Zhao and Jingcheng Shi and Jiawei Liu and Jian Zhao and Fangfang Zhou and Wenzhi Zhang and Kangyi Chen and Xin Zhao and Chunyao Zhu and Wei Chen},
  doi          = {10.1109/TVCG.2021.3107297},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4839-4854},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evaluating effects of background stories on graph perception},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vivern–a virtual environment for multiscale visualization
and modeling of DNA nanostructures. <em>TVCG</em>, <em>28</em>(12),
4825–4838. (<a href="https://doi.org/10.1109/TVCG.2021.3106328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DNA nanostructures offer promising applications, particularly in the biomedical domain, as they can be used for targeted drug delivery, construction of nanorobots, or as a basis for molecular motors. One of the most prominent techniques for assembling these structures is DNA origami. Nowadays, desktop applications are used for the in silico design of such structures. However, as such structures are often spatially complex, their assembly and analysis are complicated. Since virtual reality (VR) was proven to be advantageous for such spatial-related tasks and there are no existing VR solutions focused on this domain, we propose Vivern, a VR application that allows domain experts to design and visually examine DNA origami nanostructures. Our approach presents different abstracted visual representations of the nanostructures, various color schemes, and an ability to place several DNA nanostructures and proteins in one environment, thus allowing for the detailed analysis of complex assemblies. We also present two novel examination tools, the Magic Scale Lens and the DNA Untwister, that allow the experts to visually embed different representations into local regions to preserve the context and support detailed investigation. To showcase the capabilities of our solution, prototypes of novel nanodevices conceptualized by our collaborating experts, such as DNA-protein hybrid structures and DNA origami superstructures, are presented. Finally, the results of two rounds of evaluations are summarized. They demonstrate the advantages of our solution, especially for scenarios where current desktop tools are very limited, while also presenting possible future research directions.},
  archive      = {J_TVCG},
  author       = {David Kuťák and Matias Nicolás Selzer and Jan Byška and María Luján Ganuza and Ivan Barišić and Barbora Kozlíková and Haichao Miao},
  doi          = {10.1109/TVCG.2021.3106328},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4825-4838},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Vivern–A virtual environment for multiscale visualization and modeling of DNA nanostructures},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simulating fractures with bonded discrete element method.
<em>TVCG</em>, <em>28</em>(12), 4810–4824. (<a
href="https://doi.org/10.1109/TVCG.2021.3106738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Along with motion and deformation, fracture is a fundamental behaviour for solid materials, playing a critical role in physically-based animation. Many simulation methods including both continuum and discrete approaches have been used by the graphics community to animate fractures for various materials. However, compared with motion and deformation, fracture remains a challenging task for simulation, because the material&#39;s geometry, topology and mechanical states all undergo continuous (and sometimes chaotic) changes as fragmentation develops. Recognizing the discontinuous nature of fragmentation, we propose a discrete approach, namely the Bonded Discrete Element Method (BDEM), for fracture simulation. The research of BDEM in engineering has been growing rapidly in recent years, while its potential in graphics has not been explored. We also introduce several novel changes to BDEM to make it more suitable for animation design. Compared with other fracture simulation methods, the BDEM has some attractive benefits, e.g., efficient handling of multiple fractures, simple formulation and implementation, and good scaling consistency. But it also has some critical weaknesses, e.g., high computational cost, which demand further research. A number of examples are presented to demonstrate the pros and cons, which are then highlighted in the conclusion and discussion.},
  archive      = {J_TVCG},
  author       = {Jia-Ming Lu and Chen-Feng Li and Geng-Chen Cao and Shi-Min Hu},
  doi          = {10.1109/TVCG.2021.3106738},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4810-4824},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Simulating fractures with bonded discrete element method},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Explaining semi-supervised text alignment through
visualization. <em>TVCG</em>, <em>28</em>(12), 4797–4809. (<a
href="https://doi.org/10.1109/TVCG.2021.3105899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of variance in complex text traditions is an arduous task when carried out manually. Text alignment algorithms provide domain experts with a robust alternative to such repetitive tasks. Existing white-box approaches allow the digital humanities to establish syntax-based metrics taking into account the spelling, morphology and order of words. However, they produce limited results, as semantic meanings are typically not taken into account. Our interdisciplinary collaboration between visualization and digital humanities combined a semi-supervised text alignment approach based on word embeddings that take not only syntactic but also semantic text features into account, thereby improving the overall quality of the alignment. In our collaboration, we developed different visual interfaces that communicate the word distribution in high-dimensional vector space generated by the underlying neural network for increased transparency, assessment of the tool’s reliability and overall improved hypothesis generation. We further offer visual means to enable the expert reader to feed domain knowledge into the system at multiple levels with the aim of improving both the product and the process of text alignment. This ultimately illustrates how visualization can engage with and augment complex modes of reading in the humanities.},
  archive      = {J_TVCG},
  author       = {Christofer Meinecke and David Joseph Wrisley and Stefan Jänicke},
  doi          = {10.1109/TVCG.2021.3105899},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4797-4809},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Explaining semi-supervised text alignment through visualization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Side-by-side comparison of human perception and performance
using augmented, hybrid, and virtual reality. <em>TVCG</em>,
<em>28</em>(12), 4787–4796. (<a
href="https://doi.org/10.1109/TVCG.2021.3105606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alternative reality (XR) technologies, including physical, augmented, hybrid, and virtual reality, offer ways for engineered spaces to be evaluated. Traditionally, practitioners (such as those designing spacecraft habitats) have relied on physical mockups to perform such design evaluations, but digital XR technologies present several streamlining advantages over their physical counterparts. These digital environments vary in their level of virtuality, and consequently have different effects on human perception and performance, with respect to a completely physical mockup environment. To date, very little has been done to characterize and quantify such differences in human perception and performance across XR environments of equal fidelity for the same end application. Here, we show that perception and performance in the virtual reality environment most closely mirror those in the physical reality environment, as measured through volumetric assessment and functional task experiments. These experiments required subjects to judge the dimensions of 3D objects and perform operational tasks presented via checklists. Our results highlight the potential for virtual reality systems to accelerate the iterative design of engineered spaces relative to the use of physical mockups, while preserving the human perception and performance characteristics of a completely physical environment. These findings also elucidate specific advantages and disadvantages to specific digital XR technologies with respect to one another and the physical reality baseline. Practitioners may inform their selection of an XR modality for their specific end application based on this comparative analysis, as it contextualizes the niche for each technology in the realm of iterative design for engineered spaces.},
  archive      = {J_TVCG},
  author       = {Neil T. Banerjee and Alex J. Baughman and Shu-Yu Lin and Zoë A. Witte and David M. Klaus and Allison P. Anderson},
  doi          = {10.1109/TVCG.2021.3105606},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4787-4796},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Side-by-side comparison of human perception and performance using augmented, hybrid, and virtual reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effectiveness error: Measuring and improving RadViz visual
effectiveness. <em>TVCG</em>, <em>28</em>(12), 4770–4786. (<a
href="https://doi.org/10.1109/TVCG.2021.3104879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RadViz contributes to multidimensional analysis by using 2D points for encoding data elements and interpreting them along the original data dimensions. For these characteristics it is used in different application domains, like clustering, anomaly detection, and software visualization. However, it is likely that using the dimension arrangement that comes with the data will produce a plot that leads users to make inaccurate conclusions about points values and data distribution. This article attacks this problem without altering the original RadViz design: It defines, for both a single point and a set of points, the metric of effectiveness error , and uses it to define the objective function of a dimension arrangement strategy, arguing that minimizing it increases the overall RadViz visual quality . This article investigated the intuition that reducing the effectiveness error is beneficial for other well-known RadViz problems, like points clumping toward the center, many-to-one plotting of non-proportional points, and cluster separation. It presents an algorithm that reduces to zero the effectiveness error for a single point and a heuristic that approximates the dimension arrangement minimizing the effectiveness error for an arbitrary set of points. A set of experiments based on 21 real datasets has been performed, with the goals of analyzing the advantages of reducing the effectiveness error, comparing the proposed dimension arrangement strategy with other related proposals, and investigating the heuristic accuracy. The Effectiveness Error metric, the algorithm, and the heuristic presented in this article have been made available in a d3.js plugin at https://aware-diag-sapienza.github.io/d3-radviz .},
  archive      = {J_TVCG},
  author       = {Marco Angelini and Graziano Blasilli and Simone Lenti and Alessia Palleschi and Giuseppe Santucci},
  doi          = {10.1109/TVCG.2021.3104879},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4770-4786},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effectiveness error: Measuring and improving RadViz visual effectiveness},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VisInReport: Complementing visual discourse analytics
through personalized insight reports. <em>TVCG</em>, <em>28</em>(12),
4757–4769. (<a href="https://doi.org/10.1109/TVCG.2021.3104026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present VisInReport, a visual analytics tool that supports the manual analysis of discourse transcripts and generates reports based on user interaction. As an integral part of scholarly work in the social sciences and humanities, discourse analysis involves an aggregation of characteristics identified in the text, which, in turn, involves a prior identification of regions of particular interest. Manual data evaluation requires extensive effort, which can be a barrier to effective analysis. Our system addresses this challenge by augmenting the users’ analysis with a set of automatically generated visualization layers. These layers enable the detection and exploration of relevant parts of the discussion supporting several tasks, such as topic modeling or question categorization. The system summarizes the extracted events visually and verbally, generating a content-rich insight into the data and the analysis process. During each analysis session, VisInReport builds a shareable report containing a curated selection of interactions and annotations generated by the analyst. We evaluate our approach on real-world datasets through a qualitative study with domain experts from political science, computer science, and linguistics. The results highlight the benefit of integrating the analysis and reporting processes through a visual analytics system, which supports the communication of results among collaborating researchers.},
  archive      = {J_TVCG},
  author       = {Rita Sevastjanova and Mennatallah El-Assady and Adam Bradley and Christopher Collins and Miriam Butt and Daniel Keim},
  doi          = {10.1109/TVCG.2021.3104026},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4757-4769},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VisInReport: Complementing visual discourse analytics through personalized insight reports},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards systematic design considerations for visualizing
cross-view data relationships. <em>TVCG</em>, <em>28</em>(12),
4741–4756. (<a href="https://doi.org/10.1109/TVCG.2021.3102966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the scale of data and the complexity of analysis tasks, insight discovery often requires coordinating multiple visualizations (views), with each view displaying different parts of data or the same data from different perspectives. For example, to analyze car sales records, a marketing analyst uses a line chart to visualize the trend of car sales, a scatterplot to inspect the price and horsepower of different cars, and a matrix to compare the transaction amounts in types of deals. To explore related information across multiple views, current visual analysis tools heavily rely on brushing and linking techniques, which may require a significant amount of user effort (e.g., many trial-and-error attempts). There may be other efficient and effective ways of displaying cross-view data relationships to support data analysis with multiple views, but currently there are no guidelines to address this design challenge. In this article, we present systematic design considerations for visualizing cross-view data relationships, which leverages descriptive aspects of relationships and usable visual context of multi-view visualizations. We discuss pros and cons of different designs for showing cross-view data relationships, and provide a set of recommendations for helping practitioners make design decisions.},
  archive      = {J_TVCG},
  author       = {Maoyuan Sun and Akhil Namburi and David Koop and Jian Zhao and Tianyi Li and Haeyong Chung},
  doi          = {10.1109/TVCG.2021.3102966},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4741-4756},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards systematic design considerations for visualizing cross-view data relationships},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Outcome-explorer: A causality guided interactive visual
interface for interpretable algorithmic decision making. <em>TVCG</em>,
<em>28</em>(12), 4728–4740. (<a
href="https://doi.org/10.1109/TVCG.2021.3102051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread adoption of algorithmic decision-making systems has brought about the necessity to interpret the reasoning behind these decisions. The majority of these systems are complex black box models, and auxiliary models are often used to approximate and then explain their behavior. However, recent research suggests that such explanations are not overly accessible to lay users with no specific expertise in machine learning and this can lead to an incorrect interpretation of the underlying model. In this article, we show that a predictive and interactive model based on causality is inherently interpretable, does not require any auxiliary model, and allows both expert and non-expert users to understand the model comprehensively. To demonstrate our method we developed Outcome Explorer, a causality guided interactive interface, and evaluated it by conducting think-aloud sessions with three expert users and a user study with 18 non-expert users. All three expert users found our tool to be comprehensive in supporting their explanation needs while the non-expert users were able to understand the inner workings of a model easily.},
  archive      = {J_TVCG},
  author       = {Md Naimul Hoque and Klaus Mueller},
  doi          = {10.1109/TVCG.2021.3102051},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4728-4740},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Outcome-explorer: A causality guided interactive visual interface for interpretable algorithmic decision making},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). S4: Self-supervised learning of spatiotemporal similarity.
<em>TVCG</em>, <em>28</em>(12), 4713–4727. (<a
href="https://doi.org/10.1109/TVCG.2021.3101418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an ML-driven approach that enables interactive example-based queries for similar behavior in ensembles of spatiotemporal scientific data. This addresses an important use case in the visual exploration of simulation and experimental data, where data is often large, unlabeled and has no meaningful similarity measures available. We exploit the fact that nearby locations often exhibit similar behavior and train a Siamese Neural Network in a self-supervised fashion, learning an expressive latent space for spatiotemporal behavior. This space can be used to find similar behavior with just a few user-provided examples. We evaluate this approach on several ensemble datasets and compare with multiple existing methods, showing both qualitative and quantitative results.},
  archive      = {J_TVCG},
  author       = {Gleb Tkachev and Steffen Frey and Thomas Ertl},
  doi          = {10.1109/TVCG.2021.3101418},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4713-4727},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {S4: Self-supervised learning of spatiotemporal similarity},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning task-agnostic action spaces for movement
optimization. <em>TVCG</em>, <em>28</em>(12), 4700–4712. (<a
href="https://doi.org/10.1109/TVCG.2021.3100095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method for exploring the dynamics of physically based animated characters, and learning a task-agnostic action space that makes movement optimization easier. Like several previous article, we parameterize actions as target states, and learn a short-horizon goal-conditioned low-level control policy that drives the agent&#39;s state towards the targets. Our novel contribution is that with our exploration data, we are able to learn the low-level policy in a generic manner and without any reference movement data. Trained once for each agent or simulation environment, the policy improves the efficiency of optimizing both trajectories and high-level policies across multiple tasks and optimization algorithms. We also contribute novel visualizations that show how using target states as actions makes optimized trajectories more robust to disturbances; this manifests as wider optima that are easy to find. Due to its simplicity and generality, our proposed approach should provide a building block that can improve a large variety of movement optimization methods and applications.},
  archive      = {J_TVCG},
  author       = {Amin Babadi and Michiel van de Panne and C. Karen Liu and Perttu Hämäläinen},
  doi          = {10.1109/TVCG.2021.3100095},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4700-4712},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Learning task-agnostic action spaces for movement optimization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). UrbanRama: Navigating cities in virtual reality.
<em>TVCG</em>, <em>28</em>(12), 4685–4699. (<a
href="https://doi.org/10.1109/TVCG.2021.3099012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring large virtual environments, such as cities, is a central task in several domains, such as gaming and urban planning. VR systems can greatly help this task by providing an immersive experience; however, a common issue with viewing and navigating a city in the traditional sense is that users can either obtain a local or a global view, but not both at the same time, requiring them to continuously switch between perspectives, losing context and distracting them from their analysis. In this article, our goal is to allow users to navigate to points of interest without changing perspectives. To accomplish this, we design an intuitive navigation interface that takes advantage of the strong sense of spatial presence provided by VR. We supplement this interface with a perspective that warps the environment, called UrbanRama, based on a cylindrical projection, providing a mix of local and global views. The design of this interface was performed as an iterative process in collaboration with architects and urban planners. We conducted a qualitative and a quantitative pilot user study to evaluate UrbanRama and the results indicate the effectiveness of our system in reducing perspective changes, while ensuring that the warping doesn&#39;t affect distance and orientation perception.},
  archive      = {J_TVCG},
  author       = {Shaoyu Chen and Fabio Miranda and Nivan Ferreira and Marcos Lage and Harish Doraiswamy and Corinne Brenner and Connor Defanti and Michael Koutsoubis and Luc Wilson and Ken Perlin and Claudio Silva},
  doi          = {10.1109/TVCG.2021.3099012},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4685-4699},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {UrbanRama: Navigating cities in virtual reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PlaneFusion: Real-time indoor scene reconstruction with
planar prior. <em>TVCG</em>, <em>28</em>(12), 4671–4684. (<a
href="https://doi.org/10.1109/TVCG.2021.3099480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time dense SLAM techniques aim to reconstruct the dense three-dimensional geometry of a scene in real time with an RGB or RGB-D sensor. An indoor scene is an important type of working environment for these techniques. The planar prior can be used in this scenario to improve the reconstruction quality, especially for large low-texture regions that commonly occur in an indoor scene. This article fully explores the planar prior in a dense SLAM pipeline. First, we propose a novel plane detection and segmentation method that runs at 200 Hz on a modern graphics processing unit. Our algorithm for constructing global plane constraints is very efficient; hence, we use it in the process of each input frame for the camera pose estimation while maintaining the real-time performance. Second, we propose herein a plane-based map representation that greatly reduces the memory footprint of plane regions while keeping the geometric details on planes. The experiments reveal that our system yields superior reconstruction results with planar information running at more than 30 fps. Aside from speed and storage improvements, our technique also handles the low-texture problem in plane regions.},
  archive      = {J_TVCG},
  author       = {Bingjian Gong and Zunjie Zhu and Chenggang Yan and Zhiguo Shi and Feng Xu},
  doi          = {10.1109/TVCG.2021.3099480},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4671-4684},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PlaneFusion: Real-time indoor scene reconstruction with planar prior},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A wheelchair locomotion interface in a VR disability
simulation reduces implicit bias. <em>TVCG</em>, <em>28</em>(12),
4658–4670. (<a href="https://doi.org/10.1109/TVCG.2021.3099115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research investigates how experiencing virtual embodiment in a wheelchair affects implicit bias towards people who use wheelchairs. We also investigate how receiving information from a virtual instructor who uses a wheelchair affects implicit bias towards people who use wheelchairs. Implicit biases are actions or judgments of people towards various concepts or stereotypes (e.g., races). We hypothesized that experiencing a Disability Simulation (DS) through an avatar in a wheelchair and receiving information from an instructor with a disability will have a significant effect on participants’ ability to recall disability-related information and will reduce implicit biases towards people who use wheelchairs. To investigate this hypothesis, a 2x2 between-subjects user study was conducted where participants experienced an immersive VR DS that presents information about Multiple Sclerosis (MS) with factors of instructor (i.e., instructor with a disability versus instructor without a disability) and locomotion interface (i.e., without a disability – locomotion through in-place-walking, with a disability – locomotion in a wheelchair). Participants took a disability-focused Implicit Association Test two times, once before and once after experiencing the DS. They also took a test of knowledge retention about MS. The primary result is: experiencing the DS through locomotion in a wheelchair was better for both the disability-related information recall task and reducing implicit bias towards people who use wheelchairs.},
  archive      = {J_TVCG},
  author       = {Tanvir Irfan Chowdhury and John Quarles},
  doi          = {10.1109/TVCG.2021.3099115},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4658-4670},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A wheelchair locomotion interface in a VR disability simulation reduces implicit bias},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MVNet: Multi-variate multi-view brain network comparison
over uncertain data. <em>TVCG</em>, <em>28</em>(12), 4640–4657. (<a
href="https://doi.org/10.1109/TVCG.2021.3098123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visually identifying effective bio-markers from human brain networks poses non-trivial challenges to the field of data visualization and analysis. Existing methods in the literature and neuroscience practice are generally limited to the study of individual connectivity features in the brain (e.g., the strength of neural connection among brain regions). Pairwise comparisons between contrasting subject groups (e.g., the diseased and the healthy controls) are normally performed. The underlying neuroimaging and brain network construction process is assumed to have 100 percent fidelity. Yet, real-world user requirements on brain network visual comparison lean against these assumptions. In this work, we present MV $^{2}$ Net, a visual analytics system that tightly integrates multi-variate multi-view visualization for brain network comparison with an interactive wrangling mechanism to deal with data uncertainty. On the analysis side, the system integrates multiple extraction methods on diffusion and geometric connectivity features of brain networks, an anomaly detection algorithm for data quality assessment, single- and multi-connection feature selection methods for bio-marker detection. On the visualization side, novel designs are introduced which optimize network comparisons among contrasting subject groups and related connectivity features. Our design provides level-of-detail comparisons, from juxtaposed and explicit-coding views for subject group comparisons, to high-order composite view for correlation of network comparisons, and to fiber tract detail view for voxel-level comparisons. The proposed techniques are inspired and evaluated in expert studies, as well as through case analyses on diffusion and geometric bio-markers of certain neurology diseases. Results in these experiments demonstrate the effectiveness and superiority of MV $^{2}$ Net over state-of-the-art approaches.},
  archive      = {J_TVCG},
  author       = {Lei Shi and Junnan Hu and Zhihao Tan and Jun Tao and Jiayan Ding and Yan Jin and Yanjun Wu and Paul M. Thompson},
  doi          = {10.1109/TVCG.2021.3098123},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4640-4657},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MVNet: Multi-variate multi-view brain network comparison over uncertain data},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shedding light on cast shadows: An investigation of
perceived ground contact in AR and VR. <em>TVCG</em>, <em>28</em>(12),
4624–4639. (<a href="https://doi.org/10.1109/TVCG.2021.3097978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual objects in augmented reality (AR) often appear to float atop real world surfaces, which makes it difficult to determine where they are positioned in space. This is problematic as many applications for AR require accurate spatial perception. In the current study, we examine how the way we render cast shadows–which act as an important monocular depth cue for creating a sense of contact between an object and the surface beneath it–impacts spatial perception. Over two experiments, we evaluate people&#39;s sense of surface contact given both traditional and non-traditional shadow shading methods in optical see-through augmented reality (OST AR), video see-through augmented reality (VST AR), and virtual reality (VR) head-mounted displays. Our results provide evidence that nontraditional shading techniques for rendering shadows in AR displays may enhance the accuracy of one&#39;s perception of surface contact. This finding implies a possible tradeoff between photorealism and accuracy of depth perception, especially in OST AR displays. However, it also supports the use of more stylized graphics like non-traditional cast shadows to improve perception and interaction in AR applications.},
  archive      = {J_TVCG},
  author       = {Haley Adams and Jeanine Stefanucci and Sarah Creem-Regehr and Grant Pointon and William Thompson and Bobby Bodenheimer},
  doi          = {10.1109/TVCG.2021.3097978},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4624-4639},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Shedding light on cast shadows: An investigation of perceived ground contact in AR and VR},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DeHumor: Visual analytics for decomposing humor.
<em>TVCG</em>, <em>28</em>(12), 4609–4623. (<a
href="https://doi.org/10.1109/TVCG.2021.3097709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite being a critical communication skill, grasping humor is challenging—a successful use of humor requires a mixture of both engaging content build-up and an appropriate vocal delivery (e.g., pause). Prior studies on computational humor emphasize the textual and audio features immediately next to the punchline, yet overlooking longer-term context setup. Moreover, the theories are usually too abstract for understanding each concrete humor snippet. To fill in the gap, we develop DeHumor , a visual analytical system for analyzing humorous behaviors in public speaking. To intuitively reveal the building blocks of each concrete example, DeHumor decomposes each humorous video into multimodal features and provides inline annotations of them on the video script. In particular, to better capture the build-ups, we introduce content repetition as a complement to features introduced in theories of computational humor and visualize them in a context linking graph. To help users locate the punchlines that have the desired features to learn, we summarize the content (with keywords) and humor feature statistics on an augmented time matrix. With case studies on stand-up comedy shows and TED talks, we show that DeHumor is able to highlight various building blocks of humor examples. In addition, expert interviews with communication coaches and humor researchers demonstrate the effectiveness of DeHumor for multimodal humor analysis of speech content and vocal delivery.},
  archive      = {J_TVCG},
  author       = {Xingbo Wang and Yao Ming and Tongshuang Wu and Haipeng Zeng and Yong Wang and Huamin Qu},
  doi          = {10.1109/TVCG.2021.3097709},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4609-4623},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DeHumor: Visual analytics for decomposing humor},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Virtual replicas of real places: Experimental
investigations. <em>TVCG</em>, <em>28</em>(12), 4594–4608. (<a
href="https://doi.org/10.1109/TVCG.2021.3096494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As virtual reality (VR) technology becomes cheaper, higher-quality, and more widely available, it is seeing increasing use in a variety of applications including cultural heritage, real estate, and architecture. A common goal for all these applications is a compelling virtual recreation of a real place. Despite this, there has been very little research into how users perceive and experience such replicated spaces. This article reports the results from a series of three user studies investigating this topic. Results include that the scale of the room and large objects in it are most important for users to perceive the room as real and that non-physical behaviors such as objects floating in air are readily noticeable and have a negative effect even when the errors are small in scale.},
  archive      = {J_TVCG},
  author       = {Richard Skarbez and Joseph L. Gabbard and Doug A. Bowman and J. Todd Ogle and Thomas Tucker},
  doi          = {10.1109/TVCG.2021.3096494},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4594-4608},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Virtual replicas of real places: Experimental investigations},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FCoSE: A fast compound graph layout algorithm with
constraint support. <em>TVCG</em>, <em>28</em>(12), 4582–4593. (<a
href="https://doi.org/10.1109/TVCG.2021.3095303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual analysis of relational information is vital in most real-life analytics applications. Automatic layout is a key requirement for effective visual display of such information. This article introduces a new layout algorithm named fCoSE for compound graphs showing varying levels of groupings or abstractions with support for user-specified placement constraints. fCoSE builds on a previous compound spring embedder layout algorithm and makes use of the spectral graph drawing technique for producing a quick draft layout, followed by phases where constraints are enforced and compound structures are properly shown while polishing the layout with respect to commonly accepted graph layout criteria. Experimental evaluation verifies that fCoSE produces quality layouts and is fast enough for interactive applications with small to medium-sized graphs by combining the speed of spectral graph drawing technique with the quality of force-directed layout algorithms while satisfying specified constraints and properly displaying compound structures. An implementation of fCoSE along with documentation and a demo page is freely available on GitHub at https://github.com/iVis-at-Bilkent/cytoscape.js-fcose .},
  archive      = {J_TVCG},
  author       = {Hasan Balci and Ugur Dogrusoz},
  doi          = {10.1109/TVCG.2021.3095303},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4582-4593},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FCoSE: A fast compound graph layout algorithm with constraint support},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Show me your face: Towards an automated method to provide
timely guidance in visual analytics. <em>TVCG</em>, <em>28</em>(12),
4570–4581. (<a href="https://doi.org/10.1109/TVCG.2021.3094870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Providing guidance during a Visual Analytics session can support analysts in pursuing their goals more efficiently. However, the effectiveness of guidance depends on many factors: Determining the right timing to provide it is one of them. Although in complex analysis scenarios choosing the right timing could make the difference between a dependable and a superfluous guidance, an analysis of the literature suggests that this problem did not receive enough attention. In this paper, we describe a methodology to determine moments in which guidance is needed. Our assumption is that the need of guidance would influence the user state-of-mind, as in distress situations during the analytical process, and we hypothesize that such moments could be identified by analyzing the user&#39;s facial expressions. We propose a framework composed by a facial recognition software and a machine learning model trained to detect when to provide guidance according to changes of the user facial expressions. We trained the model by interviewing eight analysts during their work and ranked multiple facial features based on their relative importance in determining the need of guidance. Finally, we show that by applying only minor modifications to its architecture, our prototype was able to detect a need of guidance on the fly and made our methodology well suited also for real-time analysis sessions. The results of our evaluations show that our methodology is indeed effective in determining when a need of guidance is present, which constitutes a prerequisite to providing timely and effective guidance in VA.},
  archive      = {J_TVCG},
  author       = {Davide Ceneda and Alessio Arleo and Theresia Gschwandtner and Silvia Miksch},
  doi          = {10.1109/TVCG.2021.3094870},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4570-4581},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Show me your face: Towards an automated method to provide timely guidance in visual analytics},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human bas-relief generation from a single photograph.
<em>TVCG</em>, <em>28</em>(12), 4558–4569. (<a
href="https://doi.org/10.1109/TVCG.2021.3092877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a semi-automatic method for producing human bas-relief from a single photograph. Given an input photo of one or multiple persons, our method first estimates a 3D skeleton for each person in the image. SMPL models are then fitted to the 3D skeletons to generate a 3D guide model. To align the 3D guide model with the image, we compute a 2D warping field to non-rigidly register the projected contours of the guide model with the body contours in the image. Then the normal map of the 3D guide model is warped by the 2D deformation field to reconstruct an overall base shape. Finally, the base shape is integrated with a fine-scale normal map to produce the final bas-relief. To tackle the complex intra- and inter-body interactions, we design an occlusion relationship resolution method that operates at the level of 3D skeletons with minimal user inputs. To tightly register the model contours to the image contours, we propose a non-rigid point matching algorithm harnessing user-specified sparse correspondences. Experiments demonstrate that our human bas-relief generation method is capable of producing perceptually realistic results on various single-person and multi-person images, on which the state-of-the-art depth and pose estimation methods often fail.},
  archive      = {J_TVCG},
  author       = {Zhenjie Yang and Beijia Chen and Youyi Zheng and Xiang Chen and Kun Zhou},
  doi          = {10.1109/TVCG.2021.3092877},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4558-4569},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Human bas-relief generation from a single photograph},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Particle merging-and-splitting. <em>TVCG</em>,
<em>28</em>(12), 4546–4557. (<a
href="https://doi.org/10.1109/TVCG.2021.3093776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robustly handling collisions between individual particles in a large particle-based simulation has been a challenging problem. We introduce particle merging-and-splitting , a simple scheme for robustly handling collisions between particles that prevents inter-penetrations of separate objects without introducing numerical instabilities. This scheme merges colliding particles at the beginning of the time-step and then splits them at the end of the time-step. Thus, collisions last for the duration of a time-step, allowing neighboring particles of the colliding particles to influence each other. We show that our merging-and-splitting method is effective in robustly handling collisions and avoiding penetrations in particle-based simulations. We also show how our merging-and-splitting approach can be used for coupling different simulation systems using different and otherwise incompatible integrators. We present simulation tests involving complex solid-fluid interactions, including solid fractures generated by fluid interactions.},
  archive      = {J_TVCG},
  author       = {Nghia Truong and Cem Yuksel and Chakrit Watcharopas and Joshua A. Levine and Robert M. Kirby},
  doi          = {10.1109/TVCG.2021.3093776},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4546-4557},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Particle merging-and-splitting},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interpretable anomaly detection in event sequences via
sequence matching and visual comparison. <em>TVCG</em>, <em>28</em>(12),
4531–4545. (<a href="https://doi.org/10.1109/TVCG.2021.3093585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is a common analytical task that aims to identify rare cases that differ from the typical cases that make up the majority of a dataset. When analyzing event sequence data, the task of anomaly detection can be complex because the sequential and temporal nature of such data results in diverse definitions and flexible forms of anomalies. This, in turn, increases the difficulty in interpreting detected anomalies. In this article, we propose a visual analytic approach for detecting anomalous sequences in an event sequence dataset via an unsupervised anomaly detection algorithm based on Variational AutoEncoders. We further compare the anomalous sequences with their reconstructions and with the normal sequences through a sequence matching algorithm to identify event anomalies. A visual analytics system is developed to support interactive exploration and interpretations of anomalies through novel visualization designs that facilitate the comparison between anomalous sequences and normal sequences. Finally, we quantitatively evaluate the performance of our anomaly detection algorithm, demonstrate the effectiveness of our system through case studies, and report feedback collected from study participants.},
  archive      = {J_TVCG},
  author       = {Shunan Guo and Zhuochen Jin and Qing Chen and David Gotz and Hongyuan Zha and Nan Cao},
  doi          = {10.1109/TVCG.2021.3093585},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4531-4545},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interpretable anomaly detection in event sequences via sequence matching and visual comparison},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Designing with pictographs: Envision topics without
sacrificing understanding. <em>TVCG</em>, <em>28</em>(12), 4515–4530.
(<a href="https://doi.org/10.1109/TVCG.2021.3092680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Past studies have shown that when a visualization uses pictographs to encode data, they have a positive effect on memory, engagement, and assessment of risk. However, little is known about how pictographs affect one’s ability to understand a visualization, beyond memory for values and trends. We conducted two crowdsourced experiments to compare the effectiveness of using pictographs when showing part-to-whole relationships. In Experiment 1, we compared pictograph arrays to more traditional bar and pie charts. We tested participants’ ability to generate high-level insights following Bloom’s taxonomy of educational objectives via 6 free-response questions. We found that accuracy for extracting information and generating insights did not differ overall between the two versions. To explore the motivating differences between the designs, we conducted a second experiment where participants compared charts containing pictograph arrays to more traditional charts on 5 metrics and explained their reasoning. We found that some participants preferred the way that pictographs allowed them to envision the topic more easily, while others preferred traditional bar and pie charts because they seem less cluttered and faster to read. These results suggest that, at least in simple visualizations depicting part-to-whole relationships, the choice of using pictographs has little influence on sensemaking and insight extraction. When deciding whether to use pictograph arrays, designers should consider visual appeal, perceived comprehension time, ease of envisioning the topic, and clutteredness.},
  archive      = {J_TVCG},
  author       = {Alyxander Burns and Cindy Xiong and Steven Franconeri and Alberto Cairo and Narges Mahyar},
  doi          = {10.1109/TVCG.2021.3092680},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4515-4530},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Designing with pictographs: Envision topics without sacrificing understanding},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A rotation-invariant framework for deep point cloud
analysis. <em>TVCG</em>, <em>28</em>(12), 4503–4514. (<a
href="https://doi.org/10.1109/TVCG.2021.3092570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many deep neural networks were designed to process 3D point clouds, but a common drawback is that rotation invariance is not ensured, leading to poor generalization to arbitrary orientations. In this article, we introduce a new low-level purely rotation-invariant representation to replace common 3D Cartesian coordinates as the network inputs. Also, we present a network architecture to embed these representations into features, encoding local relations between points and their neighbors, and the global shape structure. To alleviate inevitable global information loss caused by the rotation-invariant representations, we further introduce a region relation convolution to encode local and non-local information. We evaluate our method on multiple point cloud analysis tasks, including (i) shape classification, (ii) part segmentation, and (iii) shape retrieval. Extensive experimental results show that our method achieves consistent, and also the best performance, on inputs at arbitrary orientations, compared with all the state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Xianzhi Li and Ruihui Li and Guangyong Chen and Chi-Wing Fu and Daniel Cohen-Or and Pheng-Ann Heng},
  doi          = {10.1109/TVCG.2021.3092570},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4503-4514},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A rotation-invariant framework for deep point cloud analysis},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Color contrast enhanced rendering for optical see-through
head-mounted displays. <em>TVCG</em>, <em>28</em>(12), 4490–4502. (<a
href="https://doi.org/10.1109/TVCG.2021.3091686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most commercially available optical see-through head-mounted displays (OST-HMDs) utilize optical combiners to simultaneously visualize the physical background and virtual objects. The displayed images perceived by users are a blend of rendered pixels and background colors. Enabling high fidelity color perception in mixed reality (MR) scenarios using OST-HMDs is an important but challenging task. We propose a real-time rendering scheme to enhance the color contrast between virtual objects and the surrounding background for OST-HMDs. Inspired by the discovery of color perception in psychophysics, we first formulate the color contrast enhancement as a constrained optimization problem. We then design an end-to-end algorithm to search the optimal complementary shift in both chromaticity and luminance of the displayed color. This aims at enhancing the contrast between virtual objects and the real background as well as keeping the consistency with the original displayed color. We assess the performance of our approach using a simulated OST-HMD environment and an off-the-shelf OST-HMD. Experimental results from objective evaluations and subjective user studies demonstrate that the proposed approach makes rendered virtual objects more distinguishable from the surrounding background, thereby bringing a better visual experience.},
  archive      = {J_TVCG},
  author       = {Yunjin Zhang and Rui Wang and Yifan Peng and Wei Hua and Hujun Bao},
  doi          = {10.1109/TVCG.2021.3091686},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4490-4502},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Color contrast enhanced rendering for optical see-through head-mounted displays},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collaborative sense-making in genomic research: The role of
visualisation. <em>TVCG</em>, <em>28</em>(12), 4477–4489. (<a
href="https://doi.org/10.1109/TVCG.2021.3090746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genomic research emerges from collaborative work within and across different scientific disciplines. A diverse range of visualisation techniques has been employed to aid this research, yet relatively little is known as to how these techniques facilitate collaboration. We conducted a case study of collaborative research within a biomedical institute to learn more about the role visualisation plays in genomic mapping. Interviews were conducted with molecular biologists (N = 5) and bioinformaticians (N = 6). We found that genomic research comprises a variety of distinct disciplines engaged in complex analytic tasks that each resist simplification, and their complexity influences how visualisations were used. Visualisation use was impacted by group-specific interactions and temporal work patterns. Visualisations were also crucial to the scientific workflow, used for both question formation and confirmation of hypotheses, and acted as an anchor for the communication of ideas and discussion. In the latter case, two approaches were taken: providing collaborators with either interactive or static imagery representing a viewpoint. The use of generic software for simplified visualisations, and quick production and curation was also noted. We discuss these findings with reference to group-specific interactions and present recommendations for improving collaborative practices through visual analytics.},
  archive      = {J_TVCG},
  author       = {Markus Rittenbruch and Kellie Vella and Margot Brereton and James M. Hogan and Daniel Johnson and Julian Heinrich and Sean O’Donoghue},
  doi          = {10.1109/TVCG.2021.3090746},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4477-4489},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Collaborative sense-making in genomic research: The role of visualisation},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A support-free infill structure based on layer construction
for 3D printing. <em>TVCG</em>, <em>28</em>(12), 4462–4476. (<a
href="https://doi.org/10.1109/TVCG.2021.3091509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of the light-weight infill structure is a hot research topic in additive manufacturing. In recent years, various infill structures have been proposed to reduce the amount of printing material. However, 3D models filled with them may have very different structural performances under different loading conditions. In addition, most of them are not self-supporting. To mitigate these issues, a novel light-weight infill structure based on the layer construction is proposed in this article. The layers of the proposed infill structure continuously and periodically transform between triangles and hexagons. The geometries of two adjacent layers are controlled to be self-supporting for different 3D printing technologies. The machine code (Gcode) of the filled 3D model is generated in the construction of the infill structure for 3D printers. That means 3D models filled with the proposed infill structure do not need an extra slicing process before printing, which is time consuming in some cases. Structural simulations and physical experiments demonstrate that our infill structure has comparable structural performance under different loading conditions. Furthermore, the relationship between the structural stiffness and the parameters of the infill structure is investigated, which will be helpful for non-professional users.},
  archive      = {J_TVCG},
  author       = {Wenpeng Xu and Yi Liu and Menglin Yu and Dongxiao Wang and Shouming Hou and Bo Li and Weiming Wang and Ligang Liu},
  doi          = {10.1109/TVCG.2021.3091509},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4462-4476},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A support-free infill structure based on layer construction for 3D printing},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluation of a low-cost virtual reality surround-screen
projection system. <em>TVCG</em>, <em>28</em>(12), 4452–4461. (<a
href="https://doi.org/10.1109/TVCG.2021.3091485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two of the most popular mediums for virtual reality are head-mounted displays and surround-screen projection systems, such as CAVE Automatic Virtual Environments. In recent years, HMDs suffered a significant reduction in cost and have become widespread consumer products. In contrast, CAVEs are still expensive and remain accessible to a limited number of researchers. This study aims to evaluate both objective and subjective characteristics of a CAVE-like monoscopic low-cost virtual reality surround-screen projection system compared to advanced setups and HMDs. For objective results, we measured the head position estimation accuracy and precision of a low-cost active infrared (IR) based tracking system, used in the proposed low-cost CAVE, relatively to an infrared marker-based tracking system, used in a laboratory-grade CAVE system. For subjective characteristics, we investigated the sense of presence and cybersickness elicited in users during a visual search task outside personal space, beyond arms reach, where the importance of stereo vision is diminished. Thirty participants rated their sense of presence and cybersickness after performing the VR search task with our CAVE-like system and a modern HMD. The tracking showed an accuracy error of 1.66 cm and .4 mm of precision jitter. The system was reported to elicit presence but at a lower level than the HMD, while causing significant lower cybersickness. Our results were compared to a previous study performed with a laboratory-grade CAVE and support that a VR system implemented with low-cost devices could be a viable alternative to laboratory-grade CAVEs for visual search tasks outside the user&#39;s personal space.},
  archive      = {J_TVCG},
  author       = {Afonso Gonçalves and Adrián Borrego and Jorge Latorre and Roberto Llorens and Sergi Bermúdez i Badia},
  doi          = {10.1109/TVCG.2021.3091485},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4452-4461},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evaluation of a low-cost virtual reality surround-screen projection system},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). C.DOT - convolutional deep object tracker for augmented
reality based purely on synthetic data. <em>TVCG</em>, <em>28</em>(12),
4434–4451. (<a href="https://doi.org/10.1109/TVCG.2021.3089096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented reality applications use object tracking to estimate the pose of a camera and to superimpose virtual content onto the observed object. Today, a number of tracking systems are available, ready to be used in industrial applications. However, such systems are hard to handle for a service maintenance engineer, due to obscure configuration procedures. In this article, we investigate options towards replacing the manual configuration process with a machine learning approach based on automatically synthesized data. We present an automated process of creating object tracker facilities exclusively from synthetic data. The data is highly enhanced to train a convolutional neural network, while still being able to receive reliable and robust results during real world applications only from simple RGB cameras. Comparison against related work using the LINEMOD dataset showed that we are able to outperform similar approaches. For our intended industrial applications with high accuracy demands, its performance is still lower than common object tracking methods with manual configuration. Yet, it can greatly support those as an add-on during initialization, due to its higher reliability.},
  archive      = {J_TVCG},
  author       = {Kevin Kennard Thiel and Florian Naumann and Eduard Jundt and Stephan Günnemann and Gudrun Klinker},
  doi          = {10.1109/TVCG.2021.3089096},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4434-4451},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {C.DOT - convolutional deep object tracker for augmented reality based purely on synthetic data},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mesh total generalized variation for denoising.
<em>TVCG</em>, <em>28</em>(12), 4418–4433. (<a
href="https://doi.org/10.1109/TVCG.2021.3088118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that the Total Generalized Variation (TGV) is highly effective in preserving sharp features as well as smooth transition variations for image processing tasks. However, currently there is no existing work that is suitable for applying TGV to 3D data, in particular, triangular meshes. In this article, we develop a novel framework for discretizing second-order TGV on triangular meshes. Further, we propose a TGV-based variational method for the denoising of face normal fields on triangular meshes. The TGV regularizer in our method is composed of a first-order term and a second-order term, which are automatically balanced. The first-order term allows our TGV regularizer to locate and preserve sharp features, while the second-order term allows our regularizer to recognize and recover smoothly curved regions. To solve the optimization problem, we introduce an efficient iterative algorithm based on variable-splitting and augmented Lagrangian method. Extensive results and comparisons on synthetic and real scanning data validate that the proposed method outperforms the state-of-the-art visually and numerically.},
  archive      = {J_TVCG},
  author       = {Zheng Liu and Yanlei Li and Weina Wang and Ligang Liu and Renjie Chen},
  doi          = {10.1109/TVCG.2021.3088118},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4418-4433},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mesh total generalized variation for denoising},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). JPEG robust invertible grayscale. <em>TVCG</em>,
<em>28</em>(12), 4403–4417. (<a
href="https://doi.org/10.1109/TVCG.2021.3088531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Invertible grayscale is a special kind of grayscale from which the original color can be recovered. Given an input color image, this seminal work tries to hide the color information into its grayscale counterpart while making it hard to recognize any anomalies. This powerful functionality is enabled by training a hiding sub-network and restoring sub-network in an end-to-end way. Despite its expressive results, two key limitations exist: 1) The restored color image often suffers from some noticeable visual artifacts in the smooth regions. 2) It is very sensitive to JPEG compression, i.e., the original color information cannot be well recovered once the intermediate grayscale image is compressed by JPEG. To overcome these two limitations, this article introduces adversarial training and JPEG simulator respectively. Specifically, two auxiliary adversarial networks are incorporated to make the intermediate grayscale images and final restored color images indistinguishable from normal grayscale and color images. And the JPEG simulator is utilized to simulate real JPEG compression during the online training so that the hiding and restoring sub-networks can automatically learn to be JPEG robust. Extensive experiments demonstrate that the proposed method is superior to the original invertible grayscale work both qualitatively and quantitatively while ensuring the JPEG robustness. We further show that the proposed framework can be applied under different types of grayscale constraints and achieve excellent results.},
  archive      = {J_TVCG},
  author       = {Kunlin Liu and Dongdong Chen and Jing Liao and Weiming Zhang and Hang Zhou and Jie Zhang and Wenbo Zhou and Nenghai Yu},
  doi          = {10.1109/TVCG.2021.3088531},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4403-4417},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {JPEG robust invertible grayscale},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Implicit error, uncertainty and confidence in visualization:
An archaeological case study. <em>TVCG</em>, <em>28</em>(12), 4389–4402.
(<a href="https://doi.org/10.1109/TVCG.2021.3088339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While we know that the visualization of quantifiable uncertainty impacts the confidence in insights, little is known about whether the same is true for uncertainty that originates from aspects so inherent to the data that they can only be accounted for qualitatively. Being embedded within an archaeological project, we realized how assessing such qualitative uncertainty is crucial in gaining a holistic and accurate understanding of regional spatio-temporal patterns of human settlements over millennia. We therefore investigated the impact of visualizing qualitative implicit errors on the sense-making process via a probe that deliberately represented three distinct implicit errors, i.e., differing collection methods, subjectivity of data interpretations and assumptions on temporal continuity. By analyzing the interactions of 14 archaeologists with different levels of domain expertise, we discovered that novices became more actively aware of typically overlooked data issues and domain experts became more confident of the visualization itself. We observed how participants quoted social factors to alleviate some uncertainty, while in order to minimize it they requested additional contextual breadth or depth of the data. While our visualization did not alleviate all uncertainty, we recognized how it sparked reflective meta-insights regarding methodological directions of the data. We believe our findings inform future visualizations on how to handle the complexity of implicit errors for a range of user typologies and for highly data-critical application domains such as the digital humanities.},
  archive      = {J_TVCG},
  author       = {Georgia Panagiotidou and Ralf Vandam and Jeroen Poblome and Andrew Vande Moere},
  doi          = {10.1109/TVCG.2021.3088339},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4389-4402},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Implicit error, uncertainty and confidence in visualization: An archaeological case study},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Omega-test: A predictive early-z culling to improve the
graphics pipeline energy-efficiency. <em>TVCG</em>, <em>28</em>(12),
4375–4388. (<a href="https://doi.org/10.1109/TVCG.2021.3087863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most common task of GPUs is to render images in real time. When rendering a 3D scene, a key step is to determine which parts of every object are visible in the final image. There are different approaches to solve the visibility problem, the Z-Test being the most common. A main factor that significantly penalizes the energy efficiency of a GPU, especially in the mobile arena, is the so-called overdraw , which happens when a portion of an object is shaded and rendered but finally occluded by another object. This useless work results in a waste of energy; however, a conventional Z-Test only avoids a fraction of it. In this article we present a novel microarchitectural technique, the Omega-Test, to drastically reduce the overdraw on a Tile-Based Rendering (TBR) architecture. Graphics applications have a great degree of inter-frame coherence, which makes the output of a frame very similar to the previous one. The proposed approach leverages the frame-to-frame coherence by using the resulting information of the Z-Test for a tile (a buffer containing all the calculated pixel depths for a tile), which is discarded by nowadays GPUs, to predict the visibility of the same tile in the next frame. As a result, the Omega-Test early identifies occluded parts of the scene and avoids the rendering of non-visible surfaces eliminating costly computations and off-chip memory accesses. Our experimental evaluation shows average EDP savings in the overall GPU/Memory system of 26.4 percent and an average speedup of 16.3 percent for the evaluated benchmarks.},
  archive      = {J_TVCG},
  author       = {David Corbalán-Navarro and Juan L. Aragón and Martí Anglada and Enrique de Lucas and Joan-Manuel Parcerisa and Antonio González},
  doi          = {10.1109/TVCG.2021.3087863},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4375-4388},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Omega-test: A predictive early-Z culling to improve the graphics pipeline energy-efficiency},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A topological similarity measure between multi-resolution
reeb spaces. <em>TVCG</em>, <em>28</em>(12), 4360–4374. (<a
href="https://doi.org/10.1109/TVCG.2021.3087273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searching similarity between a pair of shapes or data is an important problem in data analysis and visualization. The problem of computing similarity measures using scalar topology has been studied extensively and proven useful in the shape and data matching. Even though multi-field or multivariate (consists of multiple scalar fields) topology reveals richer topological features, research on building tools for computing similarity measures using multi-field topology is still in its infancy. In the current article, we propose a novel similarity measure between two piecewise-linear multi-fields based on their multi-resolution Reeb spaces - a newly developed data-structure that captures the topology of a multi-field. Overall, our method consists of two steps: (i) building a multi-resolution Reeb space corresponding to each of the multi-fields and (ii) proposing a similarity measure between two multi-resolution Reeb spaces by computing a list of topologically consistent matching pairs (of nodes) and the similarity between them. We demonstrate the effectiveness of the proposed similarity measure in detecting topological features from real time-varying multi-field data in two application domains - one from computational physics and one from computational chemistry.},
  archive      = {J_TVCG},
  author       = {Yashwanth Ramamurthi and Tripti Agarwal and Amit Chattopadhyay},
  doi          = {10.1109/TVCG.2021.3087273},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4360-4374},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A topological similarity measure between multi-resolution reeb spaces},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HisVA: A visual analytics system for studying history.
<em>TVCG</em>, <em>28</em>(12), 4344–4359. (<a
href="https://doi.org/10.1109/TVCG.2021.3086414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studying history involves many difficult tasks. Examples include searching for proper data in a large event space, understanding stories of historical events by time and space, and finding relationships among events that may not be apparent. Instructors who extensively use well-organized and well-argued materials (e.g., textbooks and online resources) can lead students to a narrow perspective in understanding history and prevent spontaneous investigation of historical events, with the students asking their own questions. In this article, we proposed HisVA, a visual analytics system that allows the efficient exploration of historical events from Wikipedia using three views: event, map, and resource. HisVA provides an effective event exploration space, where users can investigate relationships among historical events by reviewing and linking them in terms of space and time. To evaluate our system, we present two usage scenarios, a user study with a qualitative analysis of user exploration strategies, and in-class deployment results.},
  archive      = {J_TVCG},
  author       = {Dongyun Han and Gorakh Parsad and Hwiyeon Kim and Jaekyom Shim and Oh-Sang Kwon and Kyung A. Son and Jooyoung Lee and Isaac Cho and Sungahn Ko},
  doi          = {10.1109/TVCG.2021.3086414},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4344-4359},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HisVA: A visual analytics system for studying history},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performance improvement and skill transfer in table tennis
through training in virtual reality. <em>TVCG</em>, <em>28</em>(12),
4332–4343. (<a href="https://doi.org/10.1109/TVCG.2021.3086403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sports professionals have been increasingly using Virtual Reality (VR) for training and assessment of skill-based sports. Yet fundamental questions about the virtue of VR training for skill-based sports remain unanswered: Can the complex motor skills in these sports be learned in VR? If so, do these skills transfer to the real world? We have developed a VR table tennis system that incorporates customized physics with realistic audio-visual stimuli, haptics, and motion capture to enhance VR immersion and collect information about the player’s posture and technique. We have assessed skill acquisition and training transfer by comparing real table tennis performance between a control group (n=7) that received no training and an experimental group (n=8) trained for five sessions in VR. Results show a significant improvement in technique but no significant changes in the number of the returned balls in the experimental group in the real-life retention session. However, no significant differences are found in the control group. Our findings support the notion that complex skills can be learned in VR and that obtained skills can transfer to the real world. This work offers an inexpensive VR table tennis training platform, enabling effective training via real-time motor and ball returning technique feedback.},
  archive      = {J_TVCG},
  author       = {Hawkar Oagaz and Breawn Schoun and Min-Hyung Choi},
  doi          = {10.1109/TVCG.2021.3086403},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4332-4343},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Performance improvement and skill transfer in table tennis through training in virtual reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pixel-wise weighted region-based 3D object tracking using
contour constraints. <em>TVCG</em>, <em>28</em>(12), 4319–4331. (<a
href="https://doi.org/10.1109/TVCG.2021.3085197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Region-based methods are currently achieving state-of-the-art performance for monocular 3D object tracking. However, they are still prone to fail in cases of partial occlusions and ambiguous colors. We propose a novel region-based method to tackle these problems. The key idea is to derive a pixel-wise weighted region-based cost function using contour constraints. First, we propose a novel region-based cost function using search lines around the object contour, which is more efficient than previous region-based cost functions using signed distance transform, and in the meantime can deal with partial occlusions and ambiguous colors more effectively. Second, we propose an optimal searching strategy to search the object contour points in cluttered scenes, and then use the object contour points to detect partial occlusions and ambiguous colors. Third, we propose a pixel-wise weight function based on color and distance constraints of the object contour points, and integrate it into the proposed region-based cost function to reduce the negative impact of partial occlusions and ambiguous colors. We verify the effectiveness and efficiency of our method on challenging public datasets. Experiments demonstrate that our method outperforms the recent state-of-the-art region-based methods in complex scenarios, especially in the presence of partial occlusions and ambiguous colors.},
  archive      = {J_TVCG},
  author       = {Hong Huang and Fan Zhong and Xueying Qin},
  doi          = {10.1109/TVCG.2021.3085197},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4319-4331},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Pixel-wise weighted region-based 3D object tracking using contour constraints},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consistent two-flow network for tele-registration of point
clouds. <em>TVCG</em>, <em>28</em>(12), 4304–4318. (<a
href="https://doi.org/10.1109/TVCG.2021.3086113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rigid registration of partial observations is a fundamental problem in various applied fields. In computer graphics, special attention has been given to the registration between two partial point clouds generated by scanning devices. State-of-the-art registration techniques still struggle when the overlap region between the two point clouds is small, and completely fail if there is no overlap between the scan pairs. In this article, we present a learning-based technique that alleviates this problem, and allows registration between point clouds, presented in arbitrary poses, and having little or even no overlap, a setting that has been referred to as tele-registration . Our technique is based on a novel neural network design that learns a prior of a class of shapes and can complete a partial shape. The key idea is combining the registration and completion tasks in a way that reinforces each other. In particular, we simultaneously train the registration network and completion network using two coupled flows, one that register-and-complete , and one that complete-and-register , and encourage the two flows to produce a consistent result. We show that, compared with each separate flow, this two-flow training leads to robust and reliable tele-registration, and hence to a better point cloud prediction that completes the registered scans. It is also worth mentioning that each of the components in our neural network outperforms state-of-the-art methods in both completion and registration. We further analyze our network with several ablation studies and demonstrate its performance on a large number of partial point clouds, both synthetic and real-world, that have only small or no overlap.},
  archive      = {J_TVCG},
  author       = {Zihao Yan and Zimu Yi and Ruizhen Hu and Niloy J. Mitra and Daniel Cohen-Or and Hui Huang},
  doi          = {10.1109/TVCG.2021.3086113},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4304-4318},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Consistent two-flow network for tele-registration of point clouds},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating homogeneous data-driven BRDF parameters from a
reflectance map under known natural lighting. <em>TVCG</em>,
<em>28</em>(12), 4289–4303. (<a
href="https://doi.org/10.1109/TVCG.2021.3085560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we demonstrate robust estimation of the model parameters of a fully-linear data-driven BRDF model from a reflectance map under known natural lighting. To regularize the estimation of the model parameters, we leverage the reflectance similarities within a material class. We approximate the space of homogeneous BRDFs using a Gaussian mixture model, and assign a material class to each Gaussian in the mixture model. We formulate the estimation of the model parameters as a non-linear maximum a-posteriori optimization, and introduce a linear approximation that estimates a solution per material class from which the best solution is selected. We demonstrate the efficacy and robustness of our method using the MERL BRDF database under a variety of natural lighting conditions, and we provide a proof-of-concept real-world experiment.},
  archive      = {J_TVCG},
  author       = {Victoria L. Cooper and James C. Bieron and Pieter Peers},
  doi          = {10.1109/TVCG.2021.3085560},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4289-4303},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Estimating homogeneous data-driven BRDF parameters from a reflectance map under known natural lighting},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rationalizing architectural surfaces based on clustering of
joints. <em>TVCG</em>, <em>28</em>(12), 4274–4288. (<a
href="https://doi.org/10.1109/TVCG.2021.3085685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the problem of clustering the set of vertices in a given 3D mesh. The problem is motivated by the need for value engineering in architectural projects. We first derive a max-norm based metric to estimate the geometric disparity between a given pair of vertices, and characterize the problem in terms of this measure. We show that this distance can be computed by using Sequential Quadratic Programming (SQP). Next we introduce two different algorithms for clustering the set of vertices on a given mesh, respectively based on two disparity measurements: max-norm and L2-norm based metric. An equivalence is established between mesh vertices and physical joints in an architectural mesh. By replacing individual joints by their equivalent cluster representative, the number of unique joints in the facade mesh, and therefore the fabrication cost, is dramatically reduced. Finally, we present an algorithm for remeshing a given surface in order to further reduce the number of joint clusters. The framework is tested for a set of real-world architectural surfaces to illustrate the effectiveness and utility of our approach. Overall, this approach tackles the important problem reducing fabrication cost of joints without modifying the underlying connectivity that was specified by the architect.},
  archive      = {J_TVCG},
  author       = {Weidan Xiong and Chong Mo Cheung and Pedro V. Sander and Ajay Joneja},
  doi          = {10.1109/TVCG.2021.3085685},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4274-4288},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Rationalizing architectural surfaces based on clustering of joints},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Penetration depth between two convex polyhedra: An efficient
stochastic global optimization approach. <em>TVCG</em>, <em>28</em>(12),
4267–4273. (<a href="https://doi.org/10.1109/TVCG.2021.3085703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the detailed design phase of an aerospace program, one of the most important consistency checks is to ensure that no two distinct objects occupy the same physical space. Since exact geometrical modeling is usually intractable, geometry models are discretized, which often introduces small interferences not present in the fully detailed model. In this paper, we focus on computing the depth of the interference, so that these false positive interferences can be removed, and attention can be properly focused on the actual design. Specifically, we focus on efficiently computing the penetration depth between two polyhedra, which is a well-studied problem in the computer graphics community. We formulate the problem as a constrained five-variable global optimization problem, and then derive an equivalent unconstrained, two-variable nonsmooth problem. To solve the optimization problem, we apply a popular stochastic multistart optimization algorithm in a novel way, which exploits the advantages of each problem formulation simultaneously. Numerical results for the algorithm, applied to 14 randomly generated pairs of penetrating polytopes, illustrate both the effectiveness and efficiency of the method.},
  archive      = {J_TVCG},
  author       = {Mark A. Abramson and Griffin D. Kent and Gavin W. Smith},
  doi          = {10.1109/TVCG.2021.3085703},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4267-4273},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Penetration depth between two convex polyhedra: An efficient stochastic global optimization approach},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). InfoColorizer: Interactive recommendation of color palettes
for infographics. <em>TVCG</em>, <em>28</em>(12), 4252–4266. (<a
href="https://doi.org/10.1109/TVCG.2021.3085327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements’ spatial arrangement. We propose a data-driven method that provides flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.},
  archive      = {J_TVCG},
  author       = {Lin-Ping Yuan and Ziqi Zhou and Jian Zhao and Yiqiu Guo and Fan Du and Huamin Qu},
  doi          = {10.1109/TVCG.2021.3085327},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4252-4266},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {InfoColorizer: Interactive recommendation of color palettes for infographics},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). UNOC: Understanding occlusion for embodied presence in
virtual reality. <em>TVCG</em>, <em>28</em>(12), 4240–4251. (<a
href="https://doi.org/10.1109/TVCG.2021.3085407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking body and hand motions in 3D space is essential for social and self-presence in augmented and virtual environments. Unlike the popular 3D pose estimation setting, the problem is often formulated as egocentric tracking based on embodied perception (e.g., egocentric cameras, handheld sensors). In this article, we propose a new data-driven framework for egocentric body tracking, targeting challenges of omnipresent occlusions in optimization-based methods (e.g., inverse kinematics solvers). We first collect a large-scale motion capture dataset with both body and finger motions using optical markers and inertial sensors. This dataset focuses on social scenarios and captures ground truth poses under self-occlusions and body-hand interactions. We then simulate the occlusion patterns in head-mounted camera views on the captured ground truth using a ray casting algorithm and learn a deep neural network to infer the occluded body parts. Our experiments show that our method is able to generate high-fidelity embodied poses by applying the proposed method to the task of real-time egocentric body tracking, finger motion synthesis, and 3-point inverse kinematics.},
  archive      = {J_TVCG},
  author       = {Mathias Parger and Chengcheng Tang and Yuanlu Xu and Christopher D. Twigg and Lingling Tao and Yijing Li and Robert Wang and Markus Steinberger},
  doi          = {10.1109/TVCG.2021.3085407},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4240-4251},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {UNOC: Understanding occlusion for embodied presence in virtual reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deconstructing categorization in visualization
recommendation: A taxonomy and comparative study. <em>TVCG</em>,
<em>28</em>(12), 4225–4239. (<a
href="https://doi.org/10.1109/TVCG.2021.3085751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualization recommendation (VisRec) systems provide users with suggestions for potentially interesting and useful next steps during exploratory data analysis. These recommendations are typically organized into categories based on their analytical actions, i.e., operations employed to transition from the current exploration state to a recommended visualization. However, despite the emergence of a plethora of VisRec systems in recent work, the utility of the categories employed by these systems in analytical workflows has not been systematically investigated. Our article explores the efficacy of recommendation categories by formalizing a taxonomy of common categories and developing a system, Frontier , that implements these categories. Using Frontier , we evaluate workflow strategies adopted by users and how categories influence those strategies. Participants found recommendations that add attributes to enhance the current visualization and recommendations that filter to sub-populations to be comparatively most useful during data exploration. Our findings pave the way for next-generation VisRec systems that are adaptive and personalized via carefully chosen, effective recommendation categories.},
  archive      = {J_TVCG},
  author       = {Doris Jung-Lin Lee and Vidya Setlur and Melanie Tory and Karrie Karahalios and Aditya Parameswaran},
  doi          = {10.1109/TVCG.2021.3085751},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4225-4239},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Deconstructing categorization in visualization recommendation: A taxonomy and comparative study},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ClipGen: A deep generative model for clipart vectorization
and synthesis. <em>TVCG</em>, <em>28</em>(12), 4211–4224. (<a
href="https://doi.org/10.1109/TVCG.2021.3084944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel deep learning-based approach for automatically vectorizing and synthesizing the clipart of man-made objects. Given a raster clipart image and its corresponding object category (e.g., airplanes), the proposed method sequentially generates new layers, each of which is composed of a new closed path filled with a single color. The final result is obtained by compositing all layers together into a vector clipart image that falls into the target category. The proposed approach is based on an iterative generative model that (i) decides whether to continue synthesizing a new layer and (ii) determines the geometry and appearance of the new layer. We formulated a joint loss function for training our generative model, including the shape similarity, symmetry, and local curve smoothness losses, as well as vector graphics rendering accuracy loss for synthesizing clipart recognizable by humans. We also introduced a collection of man-made object clipart, ClipNet , which is composed of closed-path layers, and two designed preprocessing tasks to clean up and enrich the original raw clipart. To validate the proposed approach, we conducted several experiments and demonstrated its ability to vectorize and synthesize various clipart categories. We envision that our generative model can facilitate efficient and intuitive clipart designs for novice users and graphic designers.},
  archive      = {J_TVCG},
  author       = {I-Chao Shen and Bing-Yu Chen},
  doi          = {10.1109/TVCG.2021.3084944},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4211-4224},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ClipGen: A deep generative model for clipart vectorization and synthesis},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Did i hit the door? Effects of self-avatars and calibration
in a person-plus-virtual-object system on perceived frontal passability
in VR. <em>TVCG</em>, <em>28</em>(12), 4198–4210. (<a
href="https://doi.org/10.1109/TVCG.2021.3083423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The availability of new and improved display, tracking and input devices for Virtual Reality experiences has facilitated the use of partial and full body self-avatars in interaction with virtual objects in the environment. However, scaling the avatar to match the user&#39;s body dimensions remains to be a cumbersome process. Moreover, the effect of body-scaled self-avatars on size perception of virtual handheld objects and related action capabilities has been relatively unexplored. To this end, we present an empirical evaluation investigating the effect of the presence or absence of body-scaled self-avatars and visuo-motor calibration on frontal passability affordance judgments when interacting with virtual handheld objects. The self-avatar&#39;s dimensions were scaled to match the participant&#39;s eyeheight, arms length, shoulder width and body depth along the mid section. The results indicate that the presence of body-scaled self-avatars produce more realistic judgments of passability and aid the calibration process when interacting with virtual objects. Also, participants rely on the visual size of virtual objects to make judgments even though the kinesthetic and proprioceptive feedback of the object is missing or mismatched.},
  archive      = {J_TVCG},
  author       = {Ayush Bhargava and Roshan Venkatakrishnan and Rohith Venkatakrishnan and Hannah Solini and Kathryn Lucaites and Andrew C. Robb and Christopher C. Pagano and Sabarish V. Babu},
  doi          = {10.1109/TVCG.2021.3083423},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4198-4210},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Did i hit the door? effects of self-avatars and calibration in a person-plus-virtual-object system on perceived frontal passability in VR},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust optical based hand interaction for virtual reality.
<em>TVCG</em>, <em>28</em>(12), 4186–4197. (<a
href="https://doi.org/10.1109/TVCG.2021.3083411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using optical sensors to track hand gestures in virtual reality (VR) simulations requires issues such as occlusion, field-of-view, accuracy and stability of sensors to be addressed or mitigated. We introduce an optical hand-based interaction system that comprises two Leap Motion sensors mounted onto a VR headset at different orientations. Our system collects sensor data from the leap motions, combines and processes it to produce optimal hand tracking data, that minimises the effect of sensor occlusion and noise. This contrasts with previous systems that do not use multiple head-mounted sensors or incorporate hand-data aggregation. We also present a study that compares the proposed system with glove-based and traditional motion controller-based interaction. We investigate hand interactions effect on the feeling of naturalness and immersion. The results show that the use of two head-mounted sensors and the data aggregation system increased the number of valid hands presented to the user and can be successfully applied to VR. The user study shows that there is a strong preference for the proposed system due to the natural feeling and freeing interaction. The absence of an indirect interface such as gloves or controllers was found to aid in creating a more natural and immersive experience.},
  archive      = {J_TVCG},
  author       = {Adam Grant Worrallo and Thomas Hartley},
  doi          = {10.1109/TVCG.2021.3083411},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4186-4197},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Robust optical based hand interaction for virtual reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Slicing-tracking-detection: Simultaneous multi-cylinder
detection from large-scale and complex point clouds. <em>TVCG</em>,
<em>28</em>(12), 4172–4185. (<a
href="https://doi.org/10.1109/TVCG.2021.3082572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple cylinders detection from large-scale and complex point clouds is a historical but challenging problem, considering the efficiency and accuracy. We propose a novel framework, named slicing-tracking-detection (STD), that detects multiple cylinders accurately and simultaneously from point clouds of large-scale and complex process plants. In this framework, the 3D cylinder detection problem is reformulated as a cylinder ingredients tracking task based on multi-object tracking (MOT). First, we generate slices from the input point cloud, and render them to slice sequence. Then, the cycle of a cylinder is modeled with a Markov Decision Process (MDP), where the ingredient is tracked with a template and the miss tracking is associated with ingredient proposals through reinforcement learning. Finally, by applying MDP for each cylinder, multiple cylinders can be detected simultaneously and accurately. Extensive experiments show that the proposed STD framework can significantly outperform the state-of-the-art approaches in efficiency, accuracy, and robustness. The source code is available at http://zhiyongsu.github.io .},
  archive      = {J_TVCG},
  author       = {Zhuheng Lu and Weiwei Mao and Yuewei Dai and Weiqing Li and Zhiyong Su},
  doi          = {10.1109/TVCG.2021.3082572},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4172-4185},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Slicing-tracking-detection: Simultaneous multi-cylinder detection from large-scale and complex point clouds},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The impact of focus and context visualization techniques on
depth perception in optical see-through head-mounted displays.
<em>TVCG</em>, <em>28</em>(12), 4156–4171. (<a
href="https://doi.org/10.1109/TVCG.2021.3079849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the depth of virtual content has proven to be a challenging task in Augmented Reality (AR) applications. Existing studies have shown that the visual system makes use of multiple depth cues to infer the distance of objects, occlusion being one of the most important ones. The ability to generate appropriate occlusions becomes particularly important for AR applications that require the visualization of augmented objects placed below a real surface. Examples of these applications are medical scenarios in which the visualization of anatomical information needs to be observed within the patient&#39;s body. In this regard, existing works have proposed several focus and context ( F+C ) approaches to aid users in visualizing this content using Video See-Through (VST) Head-Mounted Displays (HMDs). However, the implementation of these approaches in Optical See-Through (OST) HMDs remains an open question due to the additive characteristics of the display technology. In this article, we, for the first time, design and conduct a user study that compares depth estimation between VST and OST HMDs using existing in-situ visualization methods. Our results show that these visualizations cannot be directly transferred to OST displays without increasing error in depth perception tasks. To tackle this gap, we perform a structured decomposition of the visual properties of AR F+C methods to find best-performing combinations. We propose the use of chromatic shadows and hatching approaches transferred from computer graphics. In a second study, we perform a factorized analysis of these combinations, showing that varying the shading type and using colored shadows can lead to better depth estimation when using OST HMDs.},
  archive      = {J_TVCG},
  author       = {Alejandro Martin-Gomez and Jakob Weiss and Andreas Keller and Ulrich Eck and Daniel Roth and Nassir Navab},
  doi          = {10.1109/TVCG.2021.3079849},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4156-4171},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The impact of focus and context visualization techniques on depth perception in optical see-through head-mounted displays},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual analytics for RNN-based deep reinforcement learning.
<em>TVCG</em>, <em>28</em>(12), 4141–4155. (<a
href="https://doi.org/10.1109/TVCG.2021.3076749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) targets to train an autonomous agent to interact with a pre-defined environment and strives to achieve specific goals through deep neural networks (DNN). Recurrent neural network (RNN) based DRL has demonstrated superior performance, as RNNs can effectively capture the temporal evolution of the environment and respond with proper agent actions. However, apart from the outstanding performance, little is known about how RNNs understand the environment internally and what has been memorized over time. Revealing these details is extremely important for deep learning experts to understand and improve DRLs, which in contrast, is also challenging due to the complicated data transformations inside these models. In this article, we propose Deep Reinforcement Learning Interactive Visual Explorer ( DRL IVE), a visual analytics system to effectively explore, interpret, and diagnose RNN-based DRLs. Having focused on DRL agents trained for different Atari games, DRL IVE accomplishes three tasks: game episode exploration, RNN hidden/cell state examination, and interactive model perturbation. Using the system, one can flexibly explore a DRL agent through interactive visualizations, discover interpretable RNN cells by prioritizing RNN hidden/cell states with a set of metrics, and further diagnose the DRL model by interactively perturbing its inputs. Through concrete studies with multiple deep learning experts, we validated the efficacy of DRL IVE.},
  archive      = {J_TVCG},
  author       = {Junpeng Wang and Wei Zhang and Hao Yang and Chin-Chia Michael Yeh and Liang Wang},
  doi          = {10.1109/TVCG.2021.3076749},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4141-4155},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual analytics for RNN-based deep reinforcement learning},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nebula: A coordinating grammar of graphics. <em>TVCG</em>,
<em>28</em>(12), 4127–4140. (<a
href="https://doi.org/10.1109/TVCG.2021.3076222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multiple coordinated views (MCVs), visualizations across views update their content in response to users’ interactions in other views. Interactive systems provide direct manipulation to create coordination between views, but are restricted to limited types of predefined templates. By contrast, textual specification languages enable flexible coordination but expose technical burden. To bridge the gap, we contribute Nebula, a grammar based on natural language for coordinating visualizations in MCVs. The grammar design is informed by a novel framework based on a systematic review of 176 coordinations from existing theories and applications, which describes coordination by demonstration, i.e., how coordination is performed by users. With the framework, Nebula specification formalizes coordination as a composition of user- and coordination-triggered interactions in origin and destination views, respectively, along with potential data transformation between the interactions. We evaluate Nebula by demonstrating its expressiveness with a gallery of diverse examples and analyzing its usability on cognitive dimensions.},
  archive      = {J_TVCG},
  author       = {Ran Chen and Xinhuan Shu and Jiahui Chen and Di Weng and Junxiu Tang and Siwei Fu and Yingcai Wu},
  doi          = {10.1109/TVCG.2021.3076222},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4127-4140},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Nebula: A coordinating grammar of graphics},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design of a pupil-matched occlusion-capable optical
see-through wearable display. <em>TVCG</em>, <em>28</em>(12), 4113–4126.
(<a href="https://doi.org/10.1109/TVCG.2021.3076069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art optical see-through head-mounted displays (OST-HMD) for augmented reality applications lack the ability to correctly render light blocking behavior between digital and physical objects, known as mutual occlusion capability. In this article, we present a novel optical architecture for enabling a high performance, occlusion-capable optical see-through head-mounted display (OCOST-HMD). The design utilizes a single-layer, double-pass architecture, creating a compact OCOST-HMD that is capable of rendering per-pixel mutual occlusion, correctly pupil-matched viewing perspective between virtual and real scenes, and a wide see-through field of view (FOV). Based on this architecture, we present a design embodiment and a compact prototype implementation. The prototype demonstrates a virtual display with an FOV of 34° by 22°, an angular resolution of 1.06 arc minutes per pixel, and an average image contrast greater than 40 percent at the Nyquist frequency of 53 cycles/mm. Furthermore, the device achieves a see-through FOV of 90° by 50°, within which about 40° diagonally is occlusion-enabled, and has an angular resolution of 1.0 arc minutes (comparable to a 20/20 vision) and a dynamic range greater than 100:1. We conclude the paper with a quantitative comparison of the key optical performance such as modulation transfer function, image contrast, and color rendering accuracy of our OCOST-HMD system with and without occlusion enabled for various lighting environments.},
  archive      = {J_TVCG},
  author       = {Austin Wilson and Hong Hua},
  doi          = {10.1109/TVCG.2021.3076069},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4113-4126},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Design of a pupil-matched occlusion-capable optical see-through wearable display},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The unmet data visualization needs of decision makers within
organizations. <em>TVCG</em>, <em>28</em>(12), 4101–4112. (<a
href="https://doi.org/10.1109/TVCG.2021.3074023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When an organization chooses one course of action over alternatives, this task typically falls on a decision maker with relevant knowledge, experience, and understanding of context. Decision makers rely on data analysis, which is either delegated to analysts, or done on their own. Often the decision maker combines data, likely uncertain or incomplete, with non-formalized knowledge within a multi-objective problem space, weighing the recommendations of analysts within broader contexts and goals. As most past research in visual analytics has focused on understanding the needs and challenges of data analysts, less is known about the tasks and challenges of organizational decision makers, and how visualization support tools might help. Here we characterize the decision maker as a domain expert, review relevant literature in management theories, and report the results of an empirical survey and interviews with people who make organizational decisions. We identify challenges and opportunities for novel visualization tools, including trade-off overviews, scenario-based analysis, interrogation tools, flexible data input and collaboration support. Our findings stress the need to expand visualization design beyond data analysis into tools for information management.},
  archive      = {J_TVCG},
  author       = {Evanthia Dimara and Harry Zhang and Melanie Tory and Steven Franconeri},
  doi          = {10.1109/TVCG.2021.3074023},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4101-4112},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The unmet data visualization needs of decision makers within organizations},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inspecting the running process of horizontal federated
learning via visual analytics. <em>TVCG</em>, <em>28</em>(12),
4085–4100. (<a href="https://doi.org/10.1109/TVCG.2021.3074010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a decentralized training approach, horizontal federated learning (HFL) enables distributed clients to collaboratively learn a machine learning model while keeping personal/private information on local devices. Despite the enhanced performance and efficiency of HFL over local training, clues for inspecting the behaviors of the participating clients and the federated model are usually lacking due to the privacy-preserving nature of HFL. Consequently, the users can only conduct a shallow-level analysis of potential abnormal behaviors and have limited means to assess the contributions of individual clients and implement the necessary intervention. Visualization techniques have been introduced to facilitate the HFL process inspection, usually by providing model metrics and evaluation results as a dashboard representation. Although the existing visualization methods allow a simple examination of the HFL model performance, they cannot support the intensive exploration of the HFL process. In this article, strictly following the HFL privacy-preserving protocol, we design an exploratory visual analytics system for the HFL process termed HFLens , which supports comparative visual interpretation at the overview, communication round, and client instance levels. Specifically, the proposed system facilitates the investigation of the overall process involving all clients, the correlation analysis of clients’ information in one or different communication round(s), the identification of potential anomalies, and the contribution assessment of each HFL client. Two case studies confirm the efficacy of our system. Experts’ feedback suggests that our approach indeed helps in understanding and diagnosing the HFL process better.},
  archive      = {J_TVCG},
  author       = {Quan Li and Xiguang Wei and Huanbin Lin and Yang Liu and Tianjian Chen and Xiaojuan Ma},
  doi          = {10.1109/TVCG.2021.3074010},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4085-4100},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Inspecting the running process of horizontal federated learning via visual analytics},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient point-in-polygon tests by grids without the
trouble of tuning the grid resolutions. <em>TVCG</em>, <em>28</em>(12),
4073–4084. (<a href="https://doi.org/10.1109/TVCG.2021.3073919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The grid-based approach is popular for point-in-polygon tests. However, there is a trade-off between the preprocessing and the inclusion test, which always requires the grid resolutions to be tuned. In this article, we address this challenge by enhancing the grid structure using $y$ -axis-aligned stripes, which are formed by the $y$ -axis-aligned lines passing through the endpoints of the edge segments in the cell, thereby managing the edge segments in each grid cell. Moreover, we precompute the inclusion properties of the $x$ -axis-aligned top borders of the stripes during preprocessing. Therefore, to answer a query point with the ray crossing method, we can emit a ray from the point to propagate upwards until the ray arrives at the top border of a stripe. We thoroughly consider singular cases to guarantee each query point can be answered in the stripe that contains the point. In our method, the computational load can be decreased, as one coordinate of the intersection point between the ray and an edge is known in advance, and parallel computing can be well exploited because the branching operations for determining whether an edge intersects with the ray are saved. Experimental results show that the efficiency of our method does not vary much with respect to the grid resolutions, so the trouble of tuning grid resolutions can be avoided. Ultimately, our method with a low grid resolution can reduce the preprocessing time and still achieve a higher inclusion test efficiency than the existing methods with a high grid resolution, especially on GPUs.},
  archive      = {J_TVCG},
  author       = {Wencheng Wang and Shengchun Wang},
  doi          = {10.1109/TVCG.2021.3073919},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4073-4084},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Efficient point-in-polygon tests by grids without the trouble of tuning the grid resolutions},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). My virtual self: The role of movement in children’s sense of
embodiment. <em>TVCG</em>, <em>28</em>(12), 4061–4072. (<a
href="https://doi.org/10.1109/TVCG.2021.3073906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are vast potential applications for children&#39;s entertainment and education with modern virtual reality (VR) experiences, yet we know very little about how the movement or form of such a virtual body can influence children&#39;s feelings of control (agency) or the sensation that they own the virtual body (ownership). In two experiments, we gave a total of 197 children aged 4-14 years a virtual hand which moved synchronously or asynchronously with their own movements and had them interact with a VR environment. We found that movement synchrony influenced feelings of control and ownership at all ages. In Experiment 1 only, participants additionally felt haptic feedback either congruently, delayed or not at all – this did not influence feelings of control or ownership. In Experiment 2 only, participants used either a virtual hand or non-human virtual block. Participants embodied both forms to some degree, provided visuomotor signals were synchronous (as indicated by ownership, agency, and location ratings). Yet, only the hand in the synchronous movement condition was described as feeling like part of the body, rather than like a tool (e.g., a mouse or controller). Collectively, these findings highlight the overall dominance of visuomotor synchrony for children&#39;s own-body representation; that children can embody non-human forms to some degree; and that embodiment is also somewhat constrained by prior expectations of body form.},
  archive      = {J_TVCG},
  author       = {Hayley Dewe and Janna M. Gottwald and Laura-Ashleigh Bird and Harry Brenton and Marco Gillies and Dorothy Cowie},
  doi          = {10.1109/TVCG.2021.3073906},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4061-4072},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {My virtual self: The role of movement in children&#39;s sense of embodiment},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep colormap extraction from visualizations. <em>TVCG</em>,
<em>28</em>(12), 4048–4060. (<a
href="https://doi.org/10.1109/TVCG.2021.3070876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of $\sim$ 64K visualizations that cover a wide variety of data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram. Quantitative comparisons to existing methods show the superior performance of our approach on both synthetic and real-world visualizations. We further demonstrate the utility of our method with two use cases, i.e., color transfer and color remapping.},
  archive      = {J_TVCG},
  author       = {Lin-Ping Yuan and Wei Zeng and Siwei Fu and Zhiliang Zeng and Haotian Li and Chi-Wing Fu and Huamin Qu},
  doi          = {10.1109/TVCG.2021.3070876},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4048-4060},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Deep colormap extraction from visualizations},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ERGOBOSS: Onomic ptimization of dy-upporting urfaces.
<em>TVCG</em>, <em>28</em>(12), 4032–4047. (<a
href="https://doi.org/10.1109/TVCG.2021.3112127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans routinely sit or lean against supporting surfaces and it is important to shape these surfaces to be comfortable and ergonomic. We give a method to design the geometric shape of rigid supporting surfaces to maximize the ergonomics of physically based contact between the surface and a deformable human. We model the soft deformable human using a layer of FEM deformable tissue surrounding a rigid core, with measured realistic elastic material properties, and large-deformation nonlinear analysis. We define a novel cost function to measure the ergonomics of contact between the human and the supporting surface. We give a stable and computationally efficient contact model that is differentiable with respect to the supporting surface shape. This makes it possible to optimize our ergonomic cost function using gradient-based optimizers. Our optimizer produces supporting surfaces superior to prior work on ergonomic shape design. Our examples include furniture, apparel and tools. We also validate our results by scanning a real human subject&#39;s foot and optimizing a shoe sole shape to maximize foot contact ergonomics. We 3D-print the optimized shoe sole, measure contact pressure using pressure sensors, and demonstrate that the real unoptimized and optimized pressure distributions qualitatively match those predicted by our simulation.},
  archive      = {J_TVCG},
  author       = {Danyong Zhao and Yijing Li and Siddhartha Chaudhuri and Timothy Langlois and Jernej Barbič},
  doi          = {10.1109/TVCG.2021.3112127},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4032-4047},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ERGOBOSS: Onomic ptimization of dy-upporting urfaces},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic projection mapping for robust sphere posture
tracking using uniform/biased circumferential markers. <em>TVCG</em>,
<em>28</em>(12), 4016–4031. (<a
href="https://doi.org/10.1109/TVCG.2021.3111085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In spatial augmented reality, a widely dynamic projection mapping system has been developed as a novel approach to graphics presentation for widely moving objects in dynamic situations. However, this method necessitates a novel tracking marker design that is resistant to random and complex occlusion and out-of-focus blurring, which conventional markers have not achieved. This article presents a uniform circumferential marker that becomes an ellipse in perspective projection and expresses geometric information. It can track the relative posture of a dynamically moving sphere with high speed, high accuracy, and robustness owing to continuous contour lines, thereby supporting both wide-range movement in the depth direction and human interaction. Moreover, a biased circumferential marker is proposed to embed unique coding, where the absolute posture is decoded with a novel recognition algorithm. Moreover, rough initialization using the geometry of multiple ellipses is proposed for both markers to start the automatic and precise tracking. Real-time rotation visualization onto the surface of a moving sphere is made possible with the high-speed, widely dynamic projection mapping system. The tracking performance is demonstrated to exhibit sufficient basic tracking performance as well as robustness against blurring and occlusion compared to conventional dot-based markers.},
  archive      = {J_TVCG},
  author       = {Yuri Mikawa and Tomohiro Sueishi and Yoshihiro Watanabe and Masatoshi Ishikawa},
  doi          = {10.1109/TVCG.2021.3111085},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4016-4031},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dynamic projection mapping for robust sphere posture tracking using Uniform/Biased circumferential markers},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CriPAV: Street-level crime patterns analysis and
visualization. <em>TVCG</em>, <em>28</em>(12), 4000–4015. (<a
href="https://doi.org/10.1109/TVCG.2021.3111146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting and analyzing crime patterns in big cities is a challenging spatiotemporal problem. The hardness of the problem is linked to two main factors, the sparse nature of the crime activity and its spread in large spatial areas. Sparseness hampers most time series (crime time series) comparison methods from working properly, while the handling of large urban areas tends to render the computational costs of such methods impractical. Visualizing different patterns hidden in crime time series data is another issue in this context, mainly due to the number of patterns that can show up in the time series analysis. In this article, we present a new methodology to deal with the issues above, enabling the analysis of spatiotemporal crime patterns in a street-level of detail. Our approach is made up of two main components designed to handle the spatial sparsity and spreading of crimes in large areas of the city. The first component relies on a stochastic mechanism from which one can visually analyze probable×intensive crime hotspots. Such analysis reveals important patterns that can not be observed in the typical intensity-based hotspot visualization. The second component builds upon a deep learning mechanism to embed crime time series in Cartesian space. From the embedding, one can identify spatial locations where the crime time series have similar behavior. The two components have been integrated into a web-based analytical tool called CriPAV (Crime Pattern Analysis and Visualization), which enables global as well as a street-level view of crime patterns. Developed in close collaboration with domain experts, CriPAV has been validated through a set of case studies with real crime data in São Paulo - Brazil. The provided experiments and case studies reveal the effectiveness of CriPAV in identifying patterns such as locations where crimes are not intense but highly probable to occur as well as locations that are far apart from each other but bear similar crime patterns.},
  archive      = {J_TVCG},
  author       = {Germain García-Zanabria and Marcos M. Raimundo and Jorge Poco and Marcelo Batista Nery and Cláudio T. Silva and Sergio Adorno and Luis Gustavo Nonato},
  doi          = {10.1109/TVCG.2021.3111146},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4000-4015},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CriPAV: Street-level crime patterns analysis and visualization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context-consistent generation of indoor virtual environments
based on geometry constraints. <em>TVCG</em>, <em>28</em>(12),
3986–3999. (<a href="https://doi.org/10.1109/TVCG.2021.3111729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a system that can automatically generate immersive and interactive virtual reality (VR) scenes by taking real-world geometric constraints into account. Our system can not only help users avoid real-world obstacles in virtual reality experiences, but also provide context-consistent contents to preserve their sense of presence. To do so, our system first identifies the positions and bounding boxes of scene objects as well as a set of interactive planes from 3D scans. Then context-consistent virtual objects that have similar geometric properties to the real ones can be automatically selected and placed into the virtual scene, based on learned object association relations and layout patterns from large amounts of indoor scene configurations. We regard virtual object replacement as a combinatorial optimization problem, considering both geometric and contextual consistency constraints. Quantitative and qualitative results show that our system can generate plausible interactive virtual scenes that highly resemble real environments, and have the ability to keep the sense of presence for users in their VR experiences.},
  archive      = {J_TVCG},
  author       = {Yu He and Ying-Tian Liu and Yi-Han Jin and Song-Hai Zhang and Yu-Kun Lai and Shi-Min Hu},
  doi          = {10.1109/TVCG.2021.3111729},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3986-3999},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Context-consistent generation of indoor virtual environments based on geometry constraints},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Haptic ankle platform for interactive walking in virtual
reality. <em>TVCG</em>, <em>28</em>(12), 3974–3985. (<a
href="https://doi.org/10.1109/TVCG.2021.3111675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an impedance type ankle haptic interface for providing users with an immersive navigation experience in virtual reality (VR). The ankle platform, actuated by an electric motor with feedback control, enables the use of foot-tapping gestures to create a walking experience like a real one and to haptically render different types of walking terrains. Experimental studies demonstrated that the interface can be easily used to generate virtual walking and is capable of rendering terrains, such as hard and soft surfaces, and multi-layer complex dynamic terrains. The designed system is a seated-type VR locomotion interface, therefore allowing its user to maintain a stable seated posture to comfortably navigate a virtual scene.},
  archive      = {J_TVCG},
  author       = {Ata Otaran and Ildar Farkhatdinov},
  doi          = {10.1109/TVCG.2021.3111675},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3974-3985},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Haptic ankle platform for interactive walking in virtual reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GDR-net: A geometric detail recovering network for 3D
scanned objects. <em>TVCG</em>, <em>28</em>(12), 3959–3973. (<a
href="https://doi.org/10.1109/TVCG.2021.3110658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the problem of mesh super-resolution such that the geometry details which are not well represented in the low-resolution models can be recovered and well represented in the generated high-quality models. The main challenges of this problem are the nonregularity of 3D mesh representation and the high complexity of 3D shapes. We propose a deep neural network called GDR-Net to solve this ill-posed problem, which resolves the two challenges simultaneously. First, to overcome the nonregularity, we regress a displacement in radial basis function parameter space instead of the vertex-wise coordinates in the euclidean space. Second, to overcome the high complexity, we apply the detail recovery process to small surface patches extracted from the input surface and obtain the overall high-quality mesh by fusing the refined surface patches. To train the network, we constructed a dataset composed of both real-world and synthetic scanned models, including high/low-quality pairs. Our experimental results demonstrate that GDR-Net works well for general models and outperforms previous methods for recovering geometric details.},
  archive      = {J_TVCG},
  author       = {Wanquan Feng and Juyong Zhang and Yuanfeng Zhou and Shiqing Xin},
  doi          = {10.1109/TVCG.2021.3110658},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3959-3973},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GDR-net: A geometric detail recovering network for 3D scanned objects},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Message from the editor-in-chief and from the associate
editor-in-chief. <em>TVCG</em>, <em>28</em>(11), v. (<a
href="https://doi.org/10.1109/TVCG.2022.3203810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Welcome to the November 2022 issue of the IEEE Transactions on Visualization and Computer Graphics (TVCG). This issue contains selected papers accepted at the IEEE International Symposium on Mixed and Augmented Reality (ISMAR). The conference took place in Singapore from October 17-22,2022 in hybrid mode.},
  archive      = {J_TVCG},
  author       = {Klaus Mueller and Doug Bowman},
  doi          = {10.1109/TVCG.2022.3203810},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {v},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Message from the editor-in-chief and from the associate editor-in-chief},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). : From real infrared eye-images to synthetic sequences of
gaze behavior. <em>TVCG</em>, <em>28</em>(11), 3948–3958. (<a
href="https://doi.org/10.1109/TVCG.2022.3203100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current methods for segmenting eye imagery into skin, sclera, pupil, and iris cannot leverage information about eye motion. This is because the datasets on which models are trained are limited to temporally non-contiguous frames. We present Temporal RIT-Eyes, a Blender pipeline that draws data from real eye videos for the rendering of synthetic imagery depicting natural gaze dynamics. These sequences are accompanied by ground-truth segmentation maps that may be used for training image-segmentation networks. Temporal RIT-Eyes relies on a novel method for the extraction of 3D eyelid pose (top and bottom apex of eyelids/eyeball boundary) from raw eye images for the rendering of gaze-dependent eyelid pose and blink behavior. The pipeline is parameterized to vary in appearance, eye/head/camera/illuminant geometry, and environment settings (indoor/outdoor). We present two open-source datasets of synthetic eye imagery: sGiW is a set of synthetic-image sequences whose dynamics are modeled on those of the Gaze in Wild dataset, and sOpenEDS2 is a series of temporally non-contiguous eye images that approximate the OpenEDS-2019 dataset. We also analyze and demonstrate the quality of the rendered dataset qualitatively and show significant overlap between latent-space representations of the source and the rendered datasets.},
  archive      = {J_TVCG},
  author       = {Aayush K. Chaudhary and Nitinraj Nair and Reynold J. Bailey and Jeff B. Pelz and Sachin S. Talathi and Gabriel J. Diaz},
  doi          = {10.1109/TVCG.2022.3203100},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3948-3958},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {: From real infrared eye-images to synthetic sequences of gaze behavior},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effective throughput analysis of different task execution
strategies for mid-air fitts’ tasks in virtual reality. <em>TVCG</em>,
<em>28</em>(11), 3939–3947. (<a
href="https://doi.org/10.1109/TVCG.2022.3203105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fitts&#39; law and throughput based on effective measures are two mathematical models frequently used to analyze human motor performance in a standardized pointing task, e.g., to compare the performance of input and output devices. Even though pointing has been deeply studied in 2D, it is not well understood how different task execution strategies affect throughput in pointing in 3D virtual environments. In this work, we examine the effective throughput measure, claimed to be invariant to task execution strategies, in Virtual Reality (VR) systems with three such strategies, “as fast, as precise, and as fast and as precise as possible” for ray casting and virtual hand interaction, by re-analyzing data from a 3D pointing ISO 9241-411 study. Results show that effective throughput is not invariant for different task execution strategies in VR, which also matches a more recent 2D result. Normalized speed vs. accuracy curves also did not fit the data. We thus suggest that practitioners, developers, and researchers who use MacKenzie&#39;s effective throughput formulation should consider our findings when analyzing 3D user pointing performance in VR systems.},
  archive      = {J_TVCG},
  author       = {Anil Ufuk Batmaz and Wolfgang Stuerzlinger},
  doi          = {10.1109/TVCG.2022.3203105},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3939-3947},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effective throughput analysis of different task execution strategies for mid-air fitts&#39; tasks in virtual reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TapeTouch: A handheld shape-changing device for haptic
display of soft objects. <em>TVCG</em>, <em>28</em>(11), 3928–3938. (<a
href="https://doi.org/10.1109/TVCG.2022.3203087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haptic feedback is widely used to enhance realism in virtual reality (VR). Shape and softness are two common factors perceived by the users in the haptic rendering of soft objects. To integrate these factors, we propose a new handheld shape-changing device, TapeTouch, to provide various shapes and softness in real time. TapeTouch is based on a controllable shape-changing tape, which is mainly composed of four motors and a section of brass tape. We design a structure of the components to fit a portable controller and allow to flexibly adjust the shape of the brass tape. After decoding desired shapes into the signals to control the motor, we automatically reproduce varying shapes and levels of softness to the finger or palm touching the shape-changing tape. We conducted two user studies to understand the capability of TapeTouch to render shape and softness, and the results showed that TapeTouch could provide a variety of distinguishable shapes as well as multiple levels of softness. Based on the results, we performed two VR experience studies to verify that the haptic feedback from TapeTouch enhances VR realism.},
  archive      = {J_TVCG},
  author       = {Lifeng Zhu and Xudong Jiang and Jiangwei Shen and Heng Zhang and Yiting Mo and Aiguo Song},
  doi          = {10.1109/TVCG.2022.3203087},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3928-3938},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TapeTouch: A handheld shape-changing device for haptic display of soft objects},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Characterizing physiological responses to fear, frustration,
and insight in virtual reality. <em>TVCG</em>, <em>28</em>(11),
3917–3927. (<a href="https://doi.org/10.1109/TVCG.2022.3203113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physiological sensing often complements studies of human behavior in virtual reality (VR) to detect users&#39; affective and cognitive states. Some psychological states, such as fear and frustration, can be particularly hard to differentiate from a physiological perspective as they are close in the arousal and valence emotional space. Moreover, it is largely unclear how users&#39; physiological reactions are expressed in response to transient psychological states such as fear, frustration, and insight—especially since these are rich indicators for characterizing users&#39; responses to dynamic systems but are hard to capture in highly interactive settings. We conducted a study ($N=24$) to analyze participants&#39; pulmonary, electrodermal, cardiac, and pupillary responses to moments of fear, frustration, and insight in immersive settings. Participants interacted in five VR environments, throughout which we measured their physiological reactions and analyzed the patterns we observed. We also measured subjective fear and frustration using questionnaires. We found differences between fear and frustration pupillary, respiratory, and electrodermal responses, as well as between the pupillary changes that followed fear in a horror game and those that followed fear in a vertigo experiment. We present the relationships between fear levels, frustration levels, and their physiological responses. To detect these affective events and states, we introduce user-independent binary classification models that achieved an average micro $F_{1}$ score of 71\% for detecting fear in a horror game, 75\% for fear of vertigo, 76\% for frustration, and 75\% for insight, showing the promise for detecting these states from passive and objective signals.},
  archive      = {J_TVCG},
  author       = {Tiffany Luong and Christian Holz},
  doi          = {10.1109/TVCG.2022.3203113},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3917-3927},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Characterizing physiological responses to fear, frustration, and insight in virtual reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Objects may be farther than they appear: Depth compression
diminishes over time with repeated calibration in virtual reality.
<em>TVCG</em>, <em>28</em>(11), 3907–3916. (<a
href="https://doi.org/10.1109/TVCG.2022.3203112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior research in depth perception and perceptuo-motor calibration have primarily focused on participants completing experiments in single sessions and therefore do not empirically evaluate changes over time. Further, these studies do not typically take into account the amount of experience that the participants have in virtual reality (VR) prior to participation, the role of experience during participation, or calibration that may occur throughout the experiment session. In this contribution, we conducted a novel empirical evaluation of how calibration affects perception-action coordination over time. We recruited novice VR users and they completed eight sessions of a depth perception reaching experiment over the course of 12 weeks. During these experiments, we examined how participants&#39; ability to estimate depth in a virtual environment changed as they gradually gained experience. While previous literature has shown that participants tend to underestimate distances, we found that this underestimation diminished over time as they gained experience in the virtual environment. Our study highlights the need for carrying out VR studies over time and the influence that longitudinal calibration can have on spatial perception in long-term VR experiences.},
  archive      = {J_TVCG},
  author       = {Kristopher Kohm and Sabarish V. Babu and Christopher Pagano and Andrew Robb},
  doi          = {10.1109/TVCG.2022.3203112},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3907-3916},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Objects may be farther than they appear: Depth compression diminishes over time with repeated calibration in virtual reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluation of pointing ray techniques for distant object
referencing in model-free outdoor collaborative augmented reality.
<em>TVCG</em>, <em>28</em>(11), 3896–3906. (<a
href="https://doi.org/10.1109/TVCG.2022.3203094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referencing objects of interest is a common requirement in many collaborative tasks. Nonetheless, accurate object referencing at a distance can be challenging due to the reduced visibility of the objects or the collaborator and limited communication medium. Augmented Reality (AR) may help address the issues by providing virtual pointing rays to the target of common interest. However, such pointing ray techniques can face critical limitations in large outdoor spaces, especially when the environment model is unavailable. In this work, we evaluated two pointing ray techniques for distant object referencing in model-free AR from the literature: the Double Ray technique enhancing visual matching between rays and targets, and the Parallel Bars technique providing artificial orientation cues. Our experiment in outdoor AR involving participants as pointers and observers partially replicated results from a previous study that only evaluated observers in simulated AR. We found that while the effectiveness of the Double Ray technique is reduced with the additional workload for the pointer and human pointing errors, it is still beneficial for distant object referencing.},
  archive      = {J_TVCG},
  author       = {Yuan Li and Ibrahim A. Tahmid and Feiyu Lu and Doug A. Bowman},
  doi          = {10.1109/TVCG.2022.3203094},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3896-3906},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evaluation of pointing ray techniques for distant object referencing in model-free outdoor collaborative augmented reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The impacts of referent display on gesture and speech
elicitation. <em>TVCG</em>, <em>28</em>(11), 3885–3895. (<a
href="https://doi.org/10.1109/TVCG.2022.3203090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elicitation studies have become a popular method of participatory design. While traditionally used to examine unimodal gesture interactions, elicitation has started being used with other novel interaction modalities. Unfortunately, there has been no work that examines the impact of referent display on elicited interaction proposals. To address that concern this work provides a detailed comparison between two elicitation studies that were similar in design apart from the way that participants were prompted for interaction proposals (i.e., the referents). Based on this comparison the impact of referent display on speech and gesture interaction proposals are each discussed. The interaction proposals between these elicitation studies were not identical. Gesture proposals were the least impacted by referent display, showing high proposal similarity between the two works. Speech proposals were highly biased by text referents with proposals directly mirroring text-based referents an average of 69.36\% of the time. In short, the way that referents are presented during elicitation studies can impact the resulting interaction proposals; however, the level of impact found is dependent on the modality of input elicited.},
  archive      = {J_TVCG},
  author       = {Adam S. Williams and Francisco R. Ortega},
  doi          = {10.1109/TVCG.2022.3203090},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3885-3895},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The impacts of referent display on gesture and speech elicitation},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Traces in virtual environments: A framework and exploration
to conceptualize the design of social virtual environments.
<em>TVCG</em>, <em>28</em>(11), 3874–3884. (<a
href="https://doi.org/10.1109/TVCG.2022.3203092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating social Virtual Environments (VEs) is an ongoing challenge. Traces of prior human interactions, or traces of use, are used in Physical Environments (PEs) to create more meaningful relationships with the PE and the people within it. In this paper, we explore how the concept of traces of use can be transferred from PEs to VEs to increase known success factors for social VEs, such as increased social presence. First, we introduce a conceptualization and discussion ($N=4$ expert interviews) of a “Traces in VEs” framework. Second, we evaluate the framework in two lab studies ($N=46$ in total), exploring the effect of traces in (i) VE vs. PE, and (ii) on social presence. Our findings confirm that traces increase the feeling of social presence. However, their meaning may differ depending on the environment. Our framework offers a structured overview of relevant components and relationships that need to be considered when designing meaningful user experiences in VE using traces. Thus, our work is valuable for practitioners and researchers who systematically want to create social VEs.},
  archive      = {J_TVCG},
  author       = {Linda Hirsch and Ceenu George and Andreas Butz},
  doi          = {10.1109/TVCG.2022.3203092},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3874-3884},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Traces in virtual environments: A framework and exploration to conceptualize the design of social virtual environments},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incorporating situation awareness cues in virtual reality
for users in dynamic in-vehicle environments. <em>TVCG</em>,
<em>28</em>(11), 3865–3873. (<a
href="https://doi.org/10.1109/TVCG.2022.3203086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing ubiquity and mobility of virtual reality (VR) devices has introduced novel use cases, one of which is using VR in vehicles, both human-driven and fully automated. However, the effects of the adoption of VR-in-the-car on user task performance, safety, trust, and perceived risk are still largely unknown or not fully understood. Blocking out the physical world and substituting it with a virtual environment has many potential benefits including fewer distractions and greater productivity. However, one shortcoming of this seclusion is losing situation awareness which becomes critical in dynamic, in-vehicle environments, even when the user is not in the driver&#39;s seat. Hence, this study aims to understand the effects of providing VR users with situation awareness cues about the real world, when riding in a human-driven or a fully automated car. The results of this driving simulator experiment provide valuable insights into passengers&#39; experience and their information needs while immersed in VR environments. Identifying passengers&#39; unique challenges and needs, as well as developing solutions for them, is expected to improve users&#39; travel experience towards a wider adoption of VR devices.},
  archive      = {J_TVCG},
  author       = {Nadia Fereydooni and Einat Tenenboim and Bruce N. Walker and Srinivas Peeta},
  doi          = {10.1109/TVCG.2022.3203086},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3865-3873},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Incorporating situation awareness cues in virtual reality for users in dynamic in-vehicle environments},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FoV-NeRF: Foveated neural radiance fields for virtual
reality. <em>TVCG</em>, <em>28</em>(11), 3854–3864. (<a
href="https://doi.org/10.1109/TVCG.2022.3203102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) is becoming ubiquitous with the rise of consumer displays and commercial VR platforms. Such displays require low latency and high quality rendering of synthetic imagery with reduced compute overheads. Recent advances in neural rendering showed promise of unlocking new possibilities in 3D computer graphics via image-based representations of virtual or physical environments. Specifically, the neural radiance fields (NeRF) demonstrated that photo-realistic quality and continuous view changes of 3D scenes can be achieved without loss of view-dependent effects. While NeRF can significantly benefit rendering for VR applications, it faces unique challenges posed by high field-of-view, high resolution, and stereoscopic/egocentric viewing, typically causing low quality and high latency of the rendered images. In VR, this not only harms the interaction experience but may also cause sickness. To tackle these problems toward six-degrees-of-freedom, egocentric, and stereo NeRF in VR, we present the first gaze-contingent 3D neural representation and view synthesis method. We incorporate the human psychophysics of visual- and stereo-acuity into an egocentric neural representation of 3D scenery. We then jointly optimize the latency/performance and visual quality while mutually bridging human perception and neural scene synthesis to achieve perceptually high-quality immersive interaction. We conducted both objective analysis and subjective studies to evaluate the effectiveness of our approach. We find that our method significantly reduces latency (up to 99\% time reduction compared with NeRF) without loss of high-fidelity rendering (perceptually identical to full-resolution ground truth). The presented approach may serve as the first step toward future VR/AR systems that capture, teleport, and visualize remote environments in real-time.},
  archive      = {J_TVCG},
  author       = {Nianchen Deng and Zhenyi He and Jiannan Ye and Budmonde Duinkharjav and Praneeth Chakravarthula and Xubo Yang and Qi Sun},
  doi          = {10.1109/TVCG.2022.3203102},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3854-3864},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FoV-NeRF: Foveated neural radiance fields for virtual reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gaze-vergence-controlled see-through vision in augmented
reality. <em>TVCG</em>, <em>28</em>(11), 3843–3853. (<a
href="https://doi.org/10.1109/TVCG.2022.3203110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented Reality (AR) see-through vision is an interesting research topic since it enables users to see through a wall and see the occluded objects. Most existing research focuses on the visual effects of see-through vision, while the interaction method is less studied. However, we argue that using common interaction modalities, e.g., midair click and speech, may not be the optimal way to control see-through vision. This is because when we want to see through something, it is physically related to our gaze depth/vergence and thus should be naturally controlled by the eyes. Following this idea, this paper proposes a novel gaze-vergence-controlled (GVC) see-through vision technique in AR. Since gaze depth is needed, we build a gaze tracking module with two infrared cameras and the corresponding algorithm and assemble it into the Microsoft HoloLens 2 to achieve gaze depth estimation. We then propose two different GVC modes for see-through vision to fit different scenarios. Extensive experimental results demonstrate that our gaze depth estimation is efficient and accurate. By comparing with conventional interaction modalities, our GVC techniques are also shown to be superior in terms of efficiency and more preferred by users. Finally, we present four example applications of gaze-vergence-controlled see-through vision.},
  archive      = {J_TVCG},
  author       = {Zhimin Wang and Yuxin Zhao and Feng Lu},
  doi          = {10.1109/TVCG.2022.3203110},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3843-3853},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Gaze-vergence-controlled see-through vision in augmented reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neurophysiological and subjective analysis of VR emotion
induction paradigm. <em>TVCG</em>, <em>28</em>(11), 3832–3842. (<a
href="https://doi.org/10.1109/TVCG.2022.3203099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ecological validity of emotion-inducing scenarios is essential for emotion research. In contrast to the classical passive induction paradigm, immersive VR fully engages the psychological and physiological components of the subject, which is considered an ecologically valid paradigm for studying emotion. Several studies investigate the emotional responses to different VR tasks or games using subjective scales. However, little research regards VR as an eliciting material, especially when systematically analyzing emotional processes in VR from a neurophysiological perspective. To fill this gap and scientifically evaluate VR&#39;s ability to be used as an active method for emotion elicitation, we investigate the dynamic relationship between explicit information (subjective evaluations) and implicit information (objective neurophysiological data). A total of 28 participants are enlisted to watch eight VR videos while their SAM/IPQ scores and EEG data are recorded simultaneously. In ecologically valid scenarios, the subjective results demonstrate that VR has significant advantages for evoking emotion in arousal-valence. This conclusion is backed by our examination of objective neurophysiological evidence that VR videos effectively induce high-arousal emotions. In addition, we obtain features of critical channels and frequency oscillations associated with emotional valence, thereby validating previous research in more lifelike circumstances. In particular, we discover hemispheric asymmetry in the occipital region under high and low emotional arousal, which adds to our understanding of neural features and the dynamics of emotional arousal. As a result, we successfully integrate EEG and VR to demonstrate that VR is more pragmatic for evoking natural feelings and is beneficial for emotional research. Our research has set a precedent for new methodologies of using VR induction paradigms to acquire a more reliable explanation of affective computing.},
  archive      = {J_TVCG},
  author       = {Ming Li and Junjun Pan and Yang Gao and Yang Shen and Fang Luo and Ju Dai and Aimin Hao and Hong Qin},
  doi          = {10.1109/TVCG.2022.3203099},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3832-3842},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Neurophysiological and subjective analysis of VR emotion induction paradigm},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-free authoring by demonstration of assembly
instructions in augmented reality. <em>TVCG</em>, <em>28</em>(11),
3821–3831. (<a href="https://doi.org/10.1109/TVCG.2022.3203104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the most compelling applications of Augmented Reality are spatially registered tutorials. The effort of creating such instructions remains one of the obstacles precluding a wider use. We propose a system that is capable of extracting 3D instructions in a completely model-free manner from demonstrations, based on volumetric changes. The instructions are visualised later in an interactive Augmented Reality guidance application, on a mobile head-mounted display. We enable a technology that can be used by anyone in an ad-hoc tabletop setup for assemblies with rigid components.},
  archive      = {J_TVCG},
  author       = {Ana Stanescu and Peter Mohr and Dieter Schmalstieg and Denis Kalkofen},
  doi          = {10.1109/TVCG.2022.3203104},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3821-3831},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Model-free authoring by demonstration of assembly instructions in augmented reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantifying the effects of working in VR for one week.
<em>TVCG</em>, <em>28</em>(11), 3810–3820. (<a
href="https://doi.org/10.1109/TVCG.2022.3203103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) provides new possibilities for modern knowledge work. However, the potential advantages of virtual work environments can only be used if it is feasible to work in them for an extended period of time. Until now, there are limited studies of long-term effects when working in VR. This paper addresses the need for understanding such long-term effects. Specifically, we report on a comparative study $i$, in which participants were working in VR for an entire week—for five days, eight hours each day—as well as in a baseline physical desktop environment. This study aims to quantify the effects of exchanging a desktop-based work environment with a VR-based environment. Hence, during this study, we do not present the participants with the best possible VR system but rather a setup delivering a comparable experience to working in the physical desktop environment. The study reveals that, as expected, VR results in significantly worse ratings across most measures. Among other results, we found concerning levels of simulator sickness, below average usability ratings and two participants dropped out on the first day using VR, due to migraine, nausea and anxiety. Nevertheless, there is some indication that participants gradually overcame negative first impressions and initial discomfort. Overall, this study helps lay the groundwork for subsequent research, by clearly highlighting current shortcomings and identifying opportunities for improving the experience of working in VR.},
  archive      = {J_TVCG},
  author       = {Verena Biener and Snehanjali Kalamkar and Negar Nouri and Eyal Ofek and Michel Pahud and John J. Dudley and Jinghui Hu and Per Ola Kristensson and Maheshya Weerasinghe and Klen Čopič Pucihar and Matjaž Kljun and Stephan Streuber and Jens Grubert},
  doi          = {10.1109/TVCG.2022.3203103},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3810-3820},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Quantifying the effects of working in VR for one week},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Precueing object placement and orientation for manual tasks
in augmented reality. <em>TVCG</em>, <em>28</em>(11), 3799–3809. (<a
href="https://doi.org/10.1109/TVCG.2022.3203111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a user is performing a manual task, AR or VR can provide information about the current subtask (cueing) and upcoming subtasks (precueing) that makes them easier and faster to complete. Previous research on cueing and precueing in AR and VR has focused on path-following tasks requiring simple actions at each of a series of locations, such as pushing a button or just visiting. We consider a more complex task, whose subtasks involve moving to and picking up an item, moving that item to a designated place while rotating it to a specific angle, and depositing it. We conducted two user studies to examine how people accomplish this task while wearing an AR headset, guided by different visualizations that cue and precue movement and rotation. Participants performed best when given movement information for two successive subtasks and rotation information for a single subtask. In addition, participants performed best when the rotation visualization was split across the manipulated object and its destination.},
  archive      = {J_TVCG},
  author       = {Jen-Shuo Liu and Barbara Tversky and Steven Feiner},
  doi          = {10.1109/TVCG.2022.3203111},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3799-3809},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Precueing object placement and orientation for manual tasks in augmented reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Investigating search among physical and virtual objects
under different lighting conditions. <em>TVCG</em>, <em>28</em>(11),
3788–3798. (<a href="https://doi.org/10.1109/TVCG.2022.3203093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By situating computer-generated content in the physical world, mobile augmented reality (AR) can support many tasks that involve effective search and inspection of physical environments. Currently, there is limited information regarding the viability of using AR in realistic wide-area outdoor environments and how AR experiences affect human behavior in these environments. Here, we conducted a wide-area outdoor AR user study ($n=48$) using a commercially available AR headset (Microsoft Hololens 2) to compare (1) user interactions with physical and virtual objects in the environment (2) the effects of different lighting conditions on user behavior and AR experience and (3) the impact of varying cognitive load on AR task performance. Participants engaged in a treasure hunt task where they searched for and classified virtual target items (green “gems”) in an augmented outdoor courtyard scene populated with physical and virtual objects. Cognitive load was manipulated so that in half the search trials users were required to monitor an audio stream and respond to specific target sounds. Walking paths, head orientation and eye gaze information were measured, and users were queried about their memory of encountered objects and provided feedback on the experience. Key findings included (1) Participants self-reported significantly lower comfort in the ambient natural light condition, with virtual objects more visible and participants more likely to walk into physical objects at night; (2) recall for physical objects was worse than for virtual objects, (3) participants discovered more gems hidden behind virtual objects than physical objects, implying higher attention on virtual objects and (4) dual-tasking modified search behavior. These results suggest there are important technical, perceptual and cognitive factors that must be considered if the full potential of “anywhere and anytime mobile AR” is to be realized.},
  archive      = {J_TVCG},
  author       = {You-Jin Kim and Radha Kumaran and Ehsan Sayyad and Anne Milner and Tom Bullock and Barry Giesbrecht and Tobias Höllerer},
  doi          = {10.1109/TVCG.2022.3203093},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3788-3798},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Investigating search among physical and virtual objects under different lighting conditions},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Making resets away from targets: POI aware redirected
walking. <em>TVCG</em>, <em>28</em>(11), 3778–3787. (<a
href="https://doi.org/10.1109/TVCG.2022.3203095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapidly developing Redirected Walking (ROW) technologies have enabled VR applications to immerse users in large virtual environments (VE) while actually walking in relatively small physical environments (PE). When an unavoidable collision emerges in a PE, the ROW controller suspends the user&#39;s immersive experience and resets the user to a new direction in PE. Existing ROW methods mainly aim to reduce the number of resets. However, from the perspective of the user experience, when users are about to reach a point of interest (POI) in a VE, reset interruptions are more likely to have an impact on user experience. In this paper, we propose a new ROW method, aiming to keep resets occurring at a longer distance from the virtual target, as well as to reduce the number of resets. Simulation experiments and real user studies demonstrate that our method outperforms state-of-the-art ROW methods in the number of resets and dramatically increases the distance between the reset locations and the virtual targets.},
  archive      = {J_TVCG},
  author       = {Sen-Zhe Xu and Tian-Qi Liu and Jia-Hong Liu and Stefanie Zollmann and Song-Hai Zhang},
  doi          = {10.1109/TVCG.2022.3203095},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3778-3787},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Making resets away from targets: POI aware redirected walking},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effects of small talk with a crowd of virtual humans on
users’ emotional and behavioral responses. <em>TVCG</em>,
<em>28</em>(11), 3767–3777. (<a
href="https://doi.org/10.1109/TVCG.2022.3203107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this contribution, we empirically investigated the effect of small talk on the users&#39; non-verbal behaviors and emotions when users interacted with a crowd of virtual humans (VHs) with positive behavioral dispositions. Users were tasked with collecting items in a virtual marketplace via natural speech-based dialogue with a crowd of virtual pedestrians and vendors. The users were able to engage in natural speech-based conversation in a predefined corpus of small talk content that covered various commonplace small talk topics such as conversations about the weather, general concerns, and entertainment based on similar real-life situations. For instance, the VHs with the small talk ability would ask the users some simple questions to make small talk or remind the users of their belongings. We conducted a between-subjects empirical evaluation to investigate whether the user behaviors and emotions were different between a small talk condition and a non-small talk condition, and examined gender effects of the participants. We collected objective and subjective measures of the users to analyze users&#39; emotions and social interaction behaviors, when in conversation with VHs that either possessed small-talk capability or not, besides task or goal oriented dialogue capabilities. Our result revealed that the VHs with small talk capability could alter the emotions and non-verbal behaviors of the users. Furthermore, the non-verbal behaviors between female and male participants differed greatly in the presence or absence of small talk.},
  archive      = {J_TVCG},
  author       = {Xing-Da Jhan and Sai-Keung Wong and Elham Ebrahimi and Yuwen Lai and Wei-Chia Huang and Sabarish V. Babu},
  doi          = {10.1109/TVCG.2022.3203107},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3767-3777},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effects of small talk with a crowd of virtual humans on users&#39; emotional and behavioral responses},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The impacts of lens and stereo camera separation on
perceived slant in virtual reality head-mounted displays. <em>TVCG</em>,
<em>28</em>(11), 3759–3766. (<a
href="https://doi.org/10.1109/TVCG.2022.3203098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereoscopic AR and VR headsets have displays and lenses that are either fixed or adjustable to match a limited range of user inter-pupillary distances (IPDs). Projective geometry predicts a misperception of depth when either the displays or virtual cameras used to render images are misaligned with the eyes. However, misalignment between the eyes and lenses might also affect binocular convergence, which could further distort perceived depth. This possibility has been largely ignored in previous studies. Here, we evaluated this phenomenon in a VR headset in which the inter-lens and inter-axial camera separations are coupled and adjustable. In a baseline condition, both were matched to observers&#39; IPDs. In two other conditions, the inter-lens and inter-axial camera separations were set to the maximum and minimum allowed by the headset. In each condition, observers were instructed to adjust a fold created by two intersecting, textured surfaces until it appeared to have an angle of 90°. The task was performed at three randomly interleaved viewing distances, monocularly and binocularly. In the monocular condition, observers underestimated the fold angle and there was no effect of viewing distance on their settings. In the binocular conditions, we found that when the lens and camera separation were less than the viewer&#39;s IPD, they exhibited compression of perceived slant relative to baseline. The reverse pattern was seen when the lens and camera separation were larger than the viewer&#39;s IPD. These results were well explained by a geometric model that considers shifts in convergence due to lens and display misalignment with the eyes, as well as the relative contribution of monocular cues.},
  archive      = {J_TVCG},
  author       = {Jonathan Tong and Laurie M Wilcox and Robert S Allison},
  doi          = {10.1109/TVCG.2022.3203098},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3759-3766},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The impacts of lens and stereo camera separation on perceived slant in virtual reality head-mounted displays},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VocabulARy: Learning vocabulary in AR supported by keyword
visualisations. <em>TVCG</em>, <em>28</em>(11), 3748–3758. (<a
href="https://doi.org/10.1109/TVCG.2022.3203116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning vocabulary in a primary or secondary language is enhanced when we encounter words in context. This context can be afforded by the place or activity we are engaged with. Existing learning environments include formal learning, mnemonics, flashcards, use of a dictionary or thesaurus, all leading to practice with new words in context. In this work, we propose an enhancement to the language learning process by providing the user with words and learning tools in context, with VocabulARy. VocabulARy visually annotates objects in AR, in the user&#39;s surroundings, with the corresponding English (first language) and Japanese (second language) words to enhance the language learning process. In addition to the written and audio description of each word, we also present the user with a keyword and its visualisation to enhance memory retention. We evaluate our prototype by comparing it to an alternate AR system that does not show an additional visualisation of the keyword, and, also, we compare it to two non-AR systems on a tablet, one with and one without visualising the keyword. Our results indicate that AR outperforms the tablet system regarding immediate recall, mental effort and task-completion time. Additionally, the visualisation approach scored significantly higher than showing only the written keyword with respect to immediate and delayed recall and learning efficiency, mental effort and task-completion time.},
  archive      = {J_TVCG},
  author       = {Maheshya Weerasinghe and Verena Biener and Jens Grubert and Aaron Quigley and Alice Toniolo and Klen Čopič Pucihar and Matjaž Kljun},
  doi          = {10.1109/TVCG.2022.3203116},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3748-3758},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VocabulARy: Learning vocabulary in AR supported by keyword visualisations},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Arigatō: Effects of adaptive guidance on engagement and
performance in augmented reality learning environments. <em>TVCG</em>,
<em>28</em>(11), 3737–3747. (<a
href="https://doi.org/10.1109/TVCG.2022.3203088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experiential learning (ExL) is the process of learning through experience or more specifically “learning through reflection on doing”. In this paper, we propose a simulation of these experiences, in Augmented Reality (AR), addressing the problem of language learning. Such systems provide an excellent setting to support “adaptive guidance”, in a digital form, within a real environment. Adaptive guidance allows the instructions and learning content to be customised for the individual learner, thus creating a unique learning experience. We developed an adaptive guidance AR system for language learning, we call Arigatō (Augmented Reality Instructional Guidance &amp; Tailored Omniverse), which offers immediate assistance, resources specific to the learner&#39;s needs, manipulation of these resources, and relevant feedback. Considering guidance, we employ this prototype to investigate the effect of the amount of guidance (fixed vs. adaptive-amount) and the type of guidance (fixed vs. adaptive-associations) on the engagement and consequently the learning outcomes of language learning in an AR environment. The results for the amount of guidance show that compared to the adaptive-amount, the fixed-amount of guidance group scored better in the immediate and delayed (after 7 days) recall tests. However, this group also invested a significantly higher mental effort to complete the task. The results for the type of guidance show that the adaptive-associations group outperforms the fixed-associations group in the immediate, delayed (after 7 days) recall tests, and learning efficiency. The adaptive-associations group also showed significantly lower mental effort and spent less time to complete the task.},
  archive      = {J_TVCG},
  author       = {Maheshya Weerasinghe and Aaron Quigley and Klen Čopič Pucihar and Alice Toniolo and Angela Miguel and Matjaž Kljun},
  doi          = {10.1109/TVCG.2022.3203088},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3737-3747},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Arigatō: Effects of adaptive guidance on engagement and performance in augmented reality learning environments},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CoLi-BA: Compact linearization based solver for bundle
adjustment. <em>TVCG</em>, <em>28</em>(11), 3727–3736. (<a
href="https://doi.org/10.1109/TVCG.2022.3203119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bundle adjustment (BA) is widely used in SLAM and SfM, which are key technologies in Augmented Reality. For real-time SLAM and large-scale SfM, the efficiency of BA is of great importance. This paper proposes CoLi-BA, a novel and efficient BA solver that significantly improves the optimization speed by compact linearization and reordering. Specifically, for each reprojection function, the redundant matrix representation of Jacobian is replaced with a tiny 3D vector, by which the computational complexity, memory storage, and cache missing for Hessian matrix construction and Schur complement are significantly reduced. Besides, we also propose a novel reordering strategy to improve the cache efficiency for Schur complement. Experiments on diverse datasets show that the speed of the proposed CoLi-BA is five times that of Ceres and two times that of g2o without sacrificing accuracy. We further verify the effectiveness by porting CoLi-BA to the open-source SLAM and SfM systems. Even when running the proposed solver in a single thread, the local BA of SLAM only takes about 20ms on a desktop PC, and the reconstruction of SfM with seven thousand photos only takes half an hour. The source code is available on the webpage: https://github.com/zju3dv/CoLi-BA.},
  archive      = {J_TVCG},
  author       = {Zhichao Ye and Guanglin Li and Haomin Liu and Zhaopeng Cui and Hujun Bao and Guofeng Zhang},
  doi          = {10.1109/TVCG.2022.3203119},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3727-3736},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CoLi-BA: Compact linearization based solver for bundle adjustment},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Action-specific perception &amp; performance on a fitts’s
law task in virtual reality: The role of haptic feedback. <em>TVCG</em>,
<em>28</em>(11), 3715–3726. (<a
href="https://doi.org/10.1109/TVCG.2022.3203003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While user&#39;s perception and performance are predominantly examined independently in virtual reality, the Action-Specific Perception (ASP) theory postulates that the performance of an individual on a task modulates this individual&#39;s spatial and time perception pertinent to the task&#39;s components and procedures. This paper examines the association between performance and perception and the potential effects that tactile feedback modalities could generate. This paper reports a user study (N=24), in which participants performed a standardized Fitts&#39;s law target acquisition task by using three feedback modalities: visual, visuo-electrotactile, and visuo-vibrotactile. The users completed 3 Target Sizes × 2 Distances × 3 feedback modalities = 18 trials. The size perception, distance perception, and (movement) time perception were assessed at the end of each trial. Performance-wise, the results showed that electrotactile feedback facilitates a significantly better accuracy compared to vibrotactile and visual feedback, while vibrotactile provided the worst accuracy. Electrotactile and visual feedback enabled a comparable reaction time, while the vibrotactile offered a substantially slower reaction time than visual feedback. Although amongst feedback types the pattern of differences in perceptual aspects were comparable to performance differences, none of them was statistically significant. However, performance indeed modulated perception. Significant action-specific effects on spatial and time perception were detected. Changes in accuracy modulate both size perception and time perception, while changes in movement speed modulate distance perception. Also, the index of difficulty was found to modulate all three perceptual aspects. However, individual differences appear to affect the magnitude of action-specific effects. These outcomes highlighted the importance of haptic feedback on performance, and importantly the significance of action-specific effects on spatial and time perception in VR, which should be considered in future VR studies.},
  archive      = {J_TVCG},
  author       = {Panagiotis Kourtesis and Sebastian Vizcay and Maud Marchal and Claudio Pacchierotti and Ferran Argelaguet},
  doi          = {10.1109/TVCG.2022.3203003},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3715-3726},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Action-specific perception &amp; performance on a fitts&#39;s law task in virtual reality: The role of haptic feedback},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predict-and-drive: Avatar motion adaption in room-scale
augmented reality telepresence with heterogeneous spaces. <em>TVCG</em>,
<em>28</em>(11), 3705–3714. (<a
href="https://doi.org/10.1109/TVCG.2022.3203109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Avatar-mediated symmetric Augmented Reality (AR) telepresence has emerged with the ability to empower users located in different remote spaces to interact with each other in 3D through avatars. However, different spaces have heterogeneous structures and features, which bring difficulties in synchronizing avatar motions with real user motions and adapting avatar motions to local scenes. To overcome these issues, existing methods generate mutual movable spaces or retarget the placement of avatars. However, these methods limit the telepresence experience in a small sub-area space, fix the positions of users and avatars, or adjust the beginning/ending positions of avatars without presenting smooth transitions. Moreover, the delay between the avatar retargeting and users&#39; real transitions can break the semantic synchronization between users&#39; verbal conversation and perceived avatar motion. In this paper, we first examine the impact of the aforementioned transition delay and explore the preferred transition style with the existence of such delay through user studies. With the results showing a significant negative effect of avatar transition delay and providing the design choice of the transition style, we propose a Predict-and-Drive controller to diminish the delay and present the smooth transition of the telepresence avatar. We also introduce a grouping component as an upgrade to immediately calculate a coarse virtual target once the user initiates a transition, which could further eliminate the avatar transition delay. Once having the coarse virtual target or an exactly predicted target, we find the corresponding target for the avatar according to the pre-constructed mapping of objects of interest between two spaces. The avatar control component maintains an artificial potential field of the space and drives the avatar towards the target while respecting the obstacles in the physical environment. We further conduct ablation studies to evaluate the effectiveness of our proposed components.},
  archive      = {J_TVCG},
  author       = {Xuanyu Wang and Hui Ye and Christian Sandor and Weizhan Zhang and Hongbo Fu},
  doi          = {10.1109/TVCG.2022.3203109},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3705-3714},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Predict-and-drive: Avatar motion adaption in room-scale augmented reality telepresence with heterogeneous spaces},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Projective bisector mirror (PBM): Concept and rationale.
<em>TVCG</em>, <em>28</em>(11), 3694–3704. (<a
href="https://doi.org/10.1109/TVCG.2022.3203108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our world is full of cameras, whether they are installed in the environment or integrated into mobile devices such as mobile phones or head-mounted displays. Displaying external camera views in our egocentric view with a picture-in-picture approach allows us to understand their view; however, it would not allow us to correlate their viewpoint with our perceived reality. We introduce Projective Bisector Mirrors for visualizing a camera view comprehensibly in the egocentric view of an observer with the metaphor of a virtual mirror. Our concept projects the image of a capturing camera onto the bisecting plane between the capture and the observer camera. We present extensive mathematical descriptions of this novel paradigm for multi-view visualization, discuss the effects of tracking errors and provide concrete implementation for multiple exemplary use-cases.},
  archive      = {J_TVCG},
  author       = {Kevin Yu and Kostantinos Zacharis and Ulrich Eck and Nassir Navab},
  doi          = {10.1109/TVCG.2022.3203108},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3694-3704},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Projective bisector mirror (PBM): Concept and rationale},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Foveated stochastic lightcuts. <em>TVCG</em>,
<em>28</em>(11), 3684–3693. (<a
href="https://doi.org/10.1109/TVCG.2022.3203089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foveated rendering provides an idea for accelerating rendering algorithms without sacrificing the perceived rendering quality in virtual reality applications. In this paper, we propose a foveated stochastic lightcuts method to render high-quality many-lights illumination effects in high perception-sensitive regions. First, we introduce a spatiotemporal-luminance based lightcuts generation method to generate lightcuts with different accuracy for different visual perception-sensitive regions. Then we propose a multi-resolution light samples selection method to select the light sample for each node in the lightcuts more efficiently. Our method supports full-dynamic scenes containing over 250k dynamic light sources and dynamic diffuse/specular/glossy objects. It provides frame rates up to 110fps for high-quality many-lights illumination effects in high perception-sensitive regions of the HVS in VR HMDs. Compared with the state-of-the-art stochastic lightcuts method using the same rendering time, our method achieves smaller mean squared errors in the fovea and periphery. We also conduct user studies to prove that the perceived quality of our method has a high visual similarity with the results of the ground truth rendered by using the stochastic lightcuts with 2048 light samples per pixel.},
  archive      = {J_TVCG},
  author       = {Xuehuai Shi and Lili Wang and Jian Wu and Runze Fan and Aimin Hao},
  doi          = {10.1109/TVCG.2022.3203089},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3684-3693},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Foveated stochastic lightcuts},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Use virtual reality to enhance intercultural sensitivity: A
randomised parallel longitudinal study. <em>TVCG</em>, <em>28</em>(11),
3673–3683. (<a href="https://doi.org/10.1109/TVCG.2022.3203091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior studies suggest that emotional empathy is one of the components of intercultural sensitivity - the affective dimension under the concept of intercultural communication competence. Based on existing theories and findings, this paper reports a randomised parallel longitudinal study investigating the use of virtual reality (VR) exposure to enhance intercultural sensitivity. A total of 80 participants (36 females and 44 males) joined the study and were included in the data analysis. The participants were randomly assigned to the VR group, the video group, and the control group. Their intercultural sensitivity was measured three times: one week before the exposure ($T_{1}$), right after the exposure ($T_{2}$), and three weeks after the exposure ($T_{3}$). The results suggested that (1) the intercultural sensitivity of the VR group was significantly enhanced in both within-subject comparisons and between-subject comparisons, (2) there were no significant differences in intercultural sensitivity between the VR group and the video group at $T_{2}$, but the VR group retained the enhancement better at $T_{3}$, and (3) the sense of presence and emotional empathy well predicted the change in intercultural sensitivity of the VR group. The results, together with the participants&#39; feedback and comments, provide new insights into the practice of using VR for intercultural sensitivity training and encourage future research on exploring the contributing factors of the results.},
  archive      = {J_TVCG},
  author       = {Chen Li and Angel Lo Lo Kon and Horace Ho Shing Ip},
  doi          = {10.1109/TVCG.2022.3203091},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3673-3683},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Use virtual reality to enhance intercultural sensitivity: A randomised parallel longitudinal study},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient flower text entry in virtual reality.
<em>TVCG</em>, <em>28</em>(11), 3662–3672. (<a
href="https://doi.org/10.1109/TVCG.2022.3203101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text entry is a frequently used task in virtual reality (VR) applications, and controller is the most common interactive device in current VR systems. However, in terms of typing speed, there is still a gap between the existing controller-based text entry techniques and using a physical keyboard in reality, so it is important to improve the efficiency of the controller-based text entry. In this paper, we introduce Flower Text Entry, a single-controller text entry method based on a newly designed flower-shaped keyboard using hand 3D translation interaction for letters selection. We conduct user studies to optimize the keyboard design and the mapping between the interaction and selection, so as to evaluate our method. The results show that our method has high typing speed, lower error rate, and is very friendly to novices compared with the state-of-the-art controller-based text entry methods. After a short training, the novice group can type at 17.65 words per minute (WPM), and the potential expert group can type at 22.97 WPM. The highest typing speed is up to 30.80 WPM achieved by a potential expert participant.},
  archive      = {J_TVCG},
  author       = {Jiaye Leng and Lili Wang and Xiaolong Liu and Xuehuai Shi and Miao Wang},
  doi          = {10.1109/TVCG.2022.3203101},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3662-3672},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Efficient flower text entry in virtual reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual comparison of networks in VR. <em>TVCG</em>,
<em>28</em>(11), 3651–3661. (<a
href="https://doi.org/10.1109/TVCG.2022.3203001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Networks are an important means for the representation and analysis of data in a variety of research and application areas. While there are many efficient methods to create layouts for networks to support their visual analysis, approaches for the comparison of networks are still underexplored. Especially when it comes to the comparison of weighted networks, which is an important task in several areas, such as biology and biomedicine, there is a lack of efficient visualization approaches. With the availability of affordable high-quality virtual reality (VR) devices, such as head-mounted displays (HMDs), the research field of immersive analytics emerged and showed great potential for using the new technology for visual data exploration. However, the use of immersive technology for the comparison of networks is still underexplored. With this work, we explore how weighted networks can be visually compared in an immersive VR environment and investigate how visual representations can benefit from the extended 3D design space. For this purpose, we develop different encodings for 3D node-link diagrams supporting the visualization of two networks within a single representation and evaluate them in a pilot user study. We incorporate the results into a more extensive user study comparing node-link representations with matrix representations encoding two networks simultaneously. The data and tasks designed for our experiments are similar to those occurring in real-world scenarios. Our evaluation shows significantly better results for the node-link representations, which is contrary to comparable 2D experiments and indicates a high potential for using VR for the visual comparison of networks.},
  archive      = {J_TVCG},
  author       = {Lucas Joos and Sabrina Jaeger-Honz and Falk Schreiber and Daniel A. Keim and Karsten Klein},
  doi          = {10.1109/TVCG.2022.3203001},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3651-3661},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual comparison of networks in VR},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). From shielding to avoidance: Passenger augmented reality and
the layout of virtual displays for productivity in shared transit.
<em>TVCG</em>, <em>28</em>(11), 3640–3650. (<a
href="https://doi.org/10.1109/TVCG.2022.3203002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Passengers spend considerable periods of time in shared transit spaces, relying on smartphones and laptops for work. However, these displays are limited in size and ergonomics compared to typical multi-monitor setups used in the office, impairing productivity. Augmented Reality (AR) headsets could provide large, flexible virtual workspaces during travel, enabling passengers to work more efficiently. This paper investigates the factors affecting how passengers choose to layout virtual displays in car, train, subway and plane environments, studying the affordances of each mode of transport and the presence of others. Results from our experiment showed: significant usage of the physical environment to align displays; strong social effects meant avoiding placing displays over other passengers or their belongings; and use of displays for shielding oneself from others. Our findings show the unique challenges posed by the mode of transport and presence of others on the use of AR for mobile productivity in the future.},
  archive      = {J_TVCG},
  author       = {Daniel Medeiros and Mark McGill and Alexander Ng and Robert McDermid and Nadia Pantidi and Julie Williamson and Stephen Brewster},
  doi          = {10.1109/TVCG.2022.3203002},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3640-3650},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {From shielding to avoidance: Passenger augmented reality and the layout of virtual displays for productivity in shared transit},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Instant automatic emptying of panoramic indoor scenes.
<em>TVCG</em>, <em>28</em>(11), 3629–3639. (<a
href="https://doi.org/10.1109/TVCG.2022.3202999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays 360° cameras, capable to capture full environments in a single shot, are increasingly being used in a variety of Extended Reality (XR) applications that require specific Diminished Reality (DR) techniques to conceal selected classes of objects. In this work, we present a new data-driven approach that, from an input 360° image of a furnished indoor space automatically returns, with very low latency, an omnidirectional photorealistic view and architecturally plausible depth of the same scene emptied of all clutter. Contrary to recent data-driven inpainting methods that remove single user-defined objects based on their semantics, our approach is holistically applied to the entire scene, and is capable to separate the clutter from the architectural structure in a single step. By exploiting peculiar geometric features of the indoor environment, we shift the major computational load on the training phase and having an extremely lightweight network at prediction time. Our end-to-end approach starts by calculating an attention mask of the clutter in the image based on the geometric difference between full and empty scene. This mask is then propagated through gated convolutions that drive the generation of the output image and its depth. Returning the depth of the resulting structure allows us to exploit, during supervised training, geometric losses of different orders, including robust pixel-wise geometric losses and high-order 3D constraints typical of indoor structures. The experimental results demonstrate that our method provides interactive performance and outperforms current state-of-the-art solutions in prediction accuracy on available commonly used indoor panoramic benchmarks. In addition, our method presents consistent quality results even for scenes captured in the wild and for data for which there is no ground truth to support supervised training.},
  archive      = {J_TVCG},
  author       = {Giovanni Pintore and Marco Agus and Eva Almansa and Enrico Gobbetti},
  doi          = {10.1109/TVCG.2022.3202999},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3629-3639},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Instant automatic emptying of panoramic indoor scenes},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gesture spotter: A rapid prototyping tool for key gesture
spotting in virtual and augmented reality applications. <em>TVCG</em>,
<em>28</em>(11), 3618–3628. (<a
href="https://doi.org/10.1109/TVCG.2022.3203004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we examine the task of key gesture spotting: accurate and timely online recognition of hand gestures. We specifically seek to address two key challenges faced by developers when integrating key gesture spotting functionality into their applications. These are: i) achieving high accuracy and zero or negative activation lag with single-time activation; and ii) avoiding the requirement for deep domain expertise in machine learning. We address the first challenge by proposing a key gesture spotting architecture consisting of a novel gesture classifier model and a novel single-time activation algorithm. This key gesture spotting architecture was evaluated on four separate hand skeleton gesture datasets, and achieved high recognition accuracy with early detection. We address the second challenge by encapsulating different data processing and augmentation strategies, as well as the proposed key gesture spotting architecture, into a graphical user interface and an application programming interface. Two user studies demonstrate that developers are able to efficiently construct custom recognizers using both the graphical user interface and the application programming interface.},
  archive      = {J_TVCG},
  author       = {Junxiao Shen and John Dudley and George Mo and Per Ola Kristensson},
  doi          = {10.1109/TVCG.2022.3203004},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3618-3628},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Gesture spotter: A rapid prototyping tool for key gesture spotting in virtual and augmented reality applications},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Content-aware brightness solving and error mitigation in
large-scale multi-projection mapping. <em>TVCG</em>, <em>28</em>(11),
3607–3617. (<a href="https://doi.org/10.1109/TVCG.2022.3203085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projection mapping with inexpensive hardware often suffers from calibration errors that lead to visually compromised results. In this paper, we classify common errors that lead to typical visual artifacts. Based on this classification, we present the first content-aware brightness solver. It is tailored for high GPU performance, yet efficiently hides the most common calibration artifacts. Moreover, it is specifically designed to handle both single and larger networked projection mapping setups with minimal latency.},
  archive      = {J_TVCG},
  author       = {Philipp Kurth and Markus Leuschner and Marc Stamminger and Frank Bauer},
  doi          = {10.1109/TVCG.2022.3203085},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3607-3617},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Content-aware brightness solving and error mitigation in large-scale multi-projection mapping},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-sensory display of self-avatar’s physiological state:
Virtual breathing and heart beating can increase sensation of effort in
VR. <em>TVCG</em>, <em>28</em>(11), 3596–3606. (<a
href="https://doi.org/10.1109/TVCG.2022.3203120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we explore the multi-sensory display of self-avatars&#39; physiological state in Virtual Reality (VR), as a means to enhance the connection between the users and their avatar. Our approach consists in designing and combining a coherent set of visual, auditory and haptic cues to represent the avatar&#39;s cardiac and respiratory activity. These sensory cues are modulated depending on the avatar&#39;s simulated physical exertion. We notably introduce a novel haptic technique to represent respiratory activity using a compression belt simulating abdominal movements that occur during a breathing cycle. A series of experiments was conducted to evaluate the influence of our multi-sensory rendering techniques on various aspects of the VR user experience, including the sense of virtual embodiment and the sensation of effort during a walking simulation. A first study ($\mathrm{N}=30$) that focused on displaying cardiac activity showed that combining sensory modalities significantly enhances the sensation of effort. A second study ($\mathrm{N}=20$) that focused on respiratory activity showed that combining sensory modalities significantly enhances the sensation of effort as well as two sub-components of the sense of embodiment. Interestingly, the user&#39;s actual breathing tended to synchronize with the simulated breathing, especially with the multi-sensory and haptic displays. A third study ($\mathrm{N}=18$) that focused on the combination of cardiac and respiratory activity showed that combining both rendering techniques significantly enhances the sensation of effort. Taken together, our results promote the use of our novel breathing display technique and multi-sensory rendering of physiological parameters in VR applications where effort sensations are prominent, such as for rehabilitation, sport training, or exergames.},
  archive      = {J_TVCG},
  author       = {Yann Moullec and Justine Saint-Aubert and Julien Manson and Mélanie Cogné and Anatole Lécuyer},
  doi          = {10.1109/TVCG.2022.3203120},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3596-3606},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-sensory display of self-avatar&#39;s physiological state: Virtual breathing and heart beating can increase sensation of effort in VR},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weighted pointer: Error-aware gaze-based interaction through
fallback modalities. <em>TVCG</em>, <em>28</em>(11), 3585–3595. (<a
href="https://doi.org/10.1109/TVCG.2022.3203096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaze-based interaction is a fast and ergonomic type of hands-free interaction that is often used with augmented and virtual reality when pointing at targets. Such interaction, however, can be cumbersome whenever user, tracking, or environmental factors cause eye tracking errors. Recent research has suggested that fallback modalities could be leveraged to ensure stable interaction irrespective of the current level of eye tracking error. This work thus presents Weighted Pointer interaction, a collection of error-aware pointing techniques that determine whether pointing should be performed by gaze, a fallback modality, or a combination of the two, depending on the level of eye tracking error that is present. These techniques enable users to accurately point at targets when eye tracking is accurate and inaccurate. A virtual reality target selection study demonstrated that Weighted Pointer techniques were more performant and preferred over techniques that required the use of manual modality switching.},
  archive      = {J_TVCG},
  author       = {Ludwig Sidenmark and Mark Parent and Chi-Hao Wu and Joannes Chan and Michael Glueck and Daniel Wigdor and Tovi Grossman and Marcello Giordano},
  doi          = {10.1109/TVCG.2022.3203096},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3585-3595},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Weighted pointer: Error-aware gaze-based interaction through fallback modalities},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Message from the ISMAR 2022 science and technology journal
program chairs and TVCG guest editors. <em>TVCG</em>, <em>28</em>(11),
1. (<a href="https://doi.org/10.1109/TVCG.2022.3203811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this special issue of IEEE Transactions on Visualization and Computer Graphics (TVCG), we are pleased to present the journal papers from the 21st IEEE International Symposium on Mixed and Augmented Reality (ISMAR 2022), which will be held as a hybrid conference between October 17 and 21, 2022 in Singapore. ISMAR continues the over twenty year long tradition of IWAR, ISMR, and ISAR, and is the premier conference for Mixed and Augmented Reality in the world.},
  archive      = {J_TVCG},
  author       = {Daisuke Iwai and Joseph L. Gabbard and Guillaume Moreau and Lili Wang},
  doi          = {10.1109/TVCG.2022.3203811},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {1},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Message from the ISMAR 2022 science and technology journal program chairs and TVCG guest editors},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A state-of-the-art survey of tasks for tree design and
evaluation with a curated task dataset. <em>TVCG</em>, <em>28</em>(10),
3563–3584. (<a href="https://doi.org/10.1109/TVCG.2021.3064037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of information visualization, the concept of “tasks” is an essential component of theories and methodologies for how a visualization researcher or a practitioner understands what tasks a user needs to perform and how to approach the creation of a new design. In this article, we focus on the collection of tasks for tree visualizations, a common visual encoding in many domains ranging from biology to computer science to geography. In spite of their commonality, no prior efforts exist to collect and abstractly define tree visualization tasks. We present a literature review of tree visualization articles and generate a curated dataset of over 200 tasks. To enable effective task abstraction for trees, we also contribute a novel extension of the Multi-Level Task Typology to include more specificity to support tree-specific tasks as well as a systematic procedure to conduct task abstractions for tree visualizations. All tasks in the dataset were abstracted with the novel typology extension and analyzed to gain a better understanding of the state of tree visualizations. These abstracted tasks can benefit visualization researchers and practitioners as they design evaluation studies or compare their analytical tasks with ones previously studied in the literature to make informed decisions about their design. We also reflect on our novel methodology and advocate more broadly for the creation of task-based knowledge repositories for different types of visualizations. The Supplemental Material, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org/10.1109/TVCG.2021.3064037 , will be maintained on OSF: https://osf.io/u5ehs/ .},
  archive      = {J_TVCG},
  author       = {Aditeya Pandey and Uzma Haque Syeda and Chaitya Shah and John A. Guerra-Gomez and Michelle A. Borkin},
  doi          = {10.1109/TVCG.2021.3064037},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3563-3584},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A state-of-the-art survey of tasks for tree design and evaluation with a curated task dataset},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visualization in motion: A research agenda and two
evaluations. <em>TVCG</em>, <em>28</em>(10), 3546–3562. (<a
href="https://doi.org/10.1109/TVCG.2022.3184993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We contribute a research agenda for visualization in motion and two experiments to understand how well viewers can read data from moving visualizations. We define visualizations in motion as visual data representations that are used in contexts that exhibit relative motion between a viewer and an entire visualization. Sports analytics, video games, wearable devices, or data physicalizations are example contexts that involve different types of relative motion between a viewer and a visualization. To analyze the opportunities and challenges for designing visualization in motion , we show example scenarios and outline a first research agenda. Motivated primarily by the prevalence of and opportunities for visualizations in sports and video games we started to investigate a small aspect of our research agenda: the impact of two important characteristics of motion—speed and trajectory on a stationary viewer&#39;s ability to read data from moving donut and bar charts. We found that increasing speed and trajectory complexity did negatively affect the accuracy of reading values from the charts and that bar charts were more negatively impacted. In practice, however, this impact was small: both charts were still read fairly accurately.},
  archive      = {J_TVCG},
  author       = {Lijie Yao and Anastasia Bezerianos and Romain Vuillemot and Petra Isenberg},
  doi          = {10.1109/TVCG.2022.3184993},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3546-3562},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualization in motion: A research agenda and two evaluations},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual analysis of multi-parameter distributions across
ensembles of 3D fields. <em>TVCG</em>, <em>28</em>(10), 3530–3545. (<a
href="https://doi.org/10.1109/TVCG.2021.3061925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For an ensemble of 3D multi-parameter fields, we present a visual analytics workflow to analyse whether and which parts of a selected multi-parameter distribution is present in all ensemble members. Supported by a parallel coordinate plot, a multi-parameter brush is applied to all ensemble members to select data points with similar multi-parameter distribution. By a combination of spatial sub-division and a covariance analysis of partitioned sub-sets of data points, a tight partition in multi-parameter space with reduced number of selected data points is obtained. To assess the representativeness of the selected multi-parameter distribution across the ensemble, we propose a novel extension of violin plots that can show multiple parameter distributions simultaneously. We investigate the visual design that effectively conveys (dis-)similarities in multi-parameter distributions, and demonstrate that users can quickly comprehend parameter-specific differences regarding distribution shape and representativeness from a side-by-side view of these plots. In a 3D spatial view, users can analyse and compare the spatial distribution of selected data points in different ensemble members via interval-based isosurface raycasting. In two real-world application cases we show how our approach is used to analyse the multi-parameter distributions across an ensemble of 3D fields.},
  archive      = {J_TVCG},
  author       = {Alexander Kumpf and Josef Stumpfegger and Patrick Fabian Härtl and Rüdiger Westermann},
  doi          = {10.1109/TVCG.2021.3061925},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3530-3545},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual analysis of multi-parameter distributions across ensembles of 3D fields},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). To explore what isn’t there—glyph-based visualization for
analysis of missing values. <em>TVCG</em>, <em>28</em>(10), 3513–3529.
(<a href="https://doi.org/10.1109/TVCG.2021.3065124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article contributes a novel visualization method, Missingness Glyph, for analysis and exploration of missing values in data. Missing values are a common challenge in most data generating domains and may cause a range of analysis issues. Missingness in data may indicate potential problems in data collection and pre-processing, or highlight important data characteristics. While the development and improvement of statistical methods for dealing with missing data is a research area in its own right, mainly focussing on replacing missing values with estimated values, considerably less focus has been put on visualization of missing values. Nonetheless, visualization and explorative analysis has great potential to support understanding of missingness in data, and to enable gaining of novel insights into patterns of missingness in a way that statistical methods are unable to. The Missingness Glyph supports identification of relevant missingness patterns in data, and is evaluated and compared to two other visualization methods in context of the missingness patterns. The results are promising and confirms that the Missingness Glyph in several cases perform better than the alternative visualization methods.},
  archive      = {J_TVCG},
  author       = {Sara Johansson Fernstad and Jimmy Johansson Westberg},
  doi          = {10.1109/TVCG.2021.3065124},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3513-3529},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {To explore what isn&#39;t There—Glyph-based visualization for analysis of missing values},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SG-GAN: Adversarial self-attention GCN for point cloud
topological parts generation. <em>TVCG</em>, <em>28</em>(10), 3499–3512.
(<a href="https://doi.org/10.1109/TVCG.2021.3069195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point clouds are fundamental in the representation of 3D objects. However, they can also be highly unstructured and irregular. This makes it difficult to directly extend 2D generative models to three-dimensional space. In this article, we cast the problem of point cloud generation as a topological representation learning problem. In order to capture the representative features of 3D shapes in the latent space, we propose a hierarchical mixture model that integrates self-attention with an inference tree structure for constructing a point cloud generator. Based on this, we design a novel Generative Adversarial Network (GAN) architecture that is capable of generating recognizable point clouds in an unsupervised manner. The proposed adversarial framework (SG-GAN) relies on self-attention mechanism and Graph Convolution Network (GCN) to hierarchically infer the latent topology of 3D shapes. Embedding and transferring the global topology information in a tree framework allows our model to capture and enhance the structural connectivity. Furthermore, the proposed architecture endows our model with partially generating 3D structures. Finally, we propose two gradient penalty methods to stabilize the training of SG-GAN and overcome the possible mode collapse of GAN networks. To demonstrate the performance of our model, we present both quantitative and qualitative evaluations and show that SG-GAN is more efficient in training and it exceeds the state-of-the-art in 3D point cloud generation.},
  archive      = {J_TVCG},
  author       = {Yushi Li and George Baciu},
  doi          = {10.1109/TVCG.2021.3069195},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3499-3512},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SG-GAN: Adversarial self-attention GCN for point cloud topological parts generation},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PowerNet: Learning-based real-time power-budget rendering.
<em>TVCG</em>, <em>28</em>(10), 3486–3498. (<a
href="https://doi.org/10.1109/TVCG.2021.3064367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the prevalence of embedded GPUs on mobile devices, power-efficient rendering has become a widespread concern for graphics applications. Reducing the power consumption of rendering applications is critical for extending battery life. In this paper, we present a new real-time power-budget rendering system to meet this need by selecting the optimal rendering settings that maximize visual quality for each frame under a given power budget. Our method utilizes two independent neural networks trained entirely by synthesized datasets to predict power consumption and image quality under various workloads. This approach spares time-consuming precomputation or runtime periodic refitting and additional error computation. We evaluate the performance of the proposed framework on different platforms, two desktop PCs and two smartphones. Results show that compared to the previous state of the art, our system has less overhead and better flexibility. Existing rendering engines can integrate our system with negligible costs.},
  archive      = {J_TVCG},
  author       = {Yunjin Zhang and Rui Wang and Yuchi Huo and Wei Hua and Hujun Bao},
  doi          = {10.1109/TVCG.2021.3064367},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3486-3498},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PowerNet: Learning-based real-time power-budget rendering},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimization and augmentation for data parallel contour
trees. <em>TVCG</em>, <em>28</em>(10), 3471–3485. (<a
href="https://doi.org/10.1109/TVCG.2021.3064385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contour trees are used for topological data analysis in scientific visualization. While originally computed with serial algorithms, recent work has introduced a vector-parallel algorithm. However, this algorithm is relatively slow for fully augmented contour trees which are needed for many practical data analysis tasks. We therefore introduce a representation called the hyperstructure that enables efficient searches through the contour tree and use it to construct a fully augmented contour tree in data parallel, with performance on average 6 times faster than the state-of-the-art parallel algorithm in the TTK topological toolkit.},
  archive      = {J_TVCG},
  author       = {Hamish A. Carr and Oliver Rübel and Gunther H. Weber and James P. Ahrens},
  doi          = {10.1109/TVCG.2021.3064385},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3471-3485},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Optimization and augmentation for data parallel contour trees},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiscale unfolding: Illustratively visualizing the whole
genome at a glance. <em>TVCG</em>, <em>28</em>(10), 3456–3470. (<a
href="https://doi.org/10.1109/TVCG.2021.3065443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Multiscale Unfolding, an interactive technique for illustratively visualizing multiple hierarchical scales of DNA in a single view, showing the genome at different scales and demonstrating how one scale spatially folds into the next. The DNA&#39;s extremely long sequential structure—arranged differently on several distinct scale levels—is often lost in traditional 3D depictions, mainly due to its multiple levels of dense spatial packing and the resulting occlusion. Furthermore, interactive exploration of this complex structure is cumbersome, requiring visibility management like cut-aways. In contrast to existing temporally controlled multiscale data exploration, we allow viewers to always see and interact with any of the involved scales. For this purpose we separate the depiction into constant-scale and scale transition zones. Constant-scale zones maintain a single-scale representation, while still linearly unfolding the DNA. Inspired by illustration, scale transition zones connect adjacent constant-scale zones via level unfolding, scaling, and transparency. We thus represent the spatial structure of the whole DNA macro-molecule, maintain its local organizational characteristics, linearize its higher-level organization, and use spatially controlled, understandable interpolation between neighboring scales. We also contribute interaction techniques that provide viewers with a coarse-to-fine control for navigating within our all-scales-in-one-view representations and visual aids to illustrate the size differences. Overall, Multiscale Unfolding allows viewers to grasp the DNA&#39;s structural composition from chromosomes to the atoms, with increasing levels of “unfoldedness,” and can be applied in data-driven illustration and communication.},
  archive      = {J_TVCG},
  author       = {Sarkis Halladjian and David Kouřil and Haichao Miao and M. Eduard Gröller and Ivan Viola and Tobias Isenberg},
  doi          = {10.1109/TVCG.2021.3065443},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3456-3470},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multiscale unfolding: Illustratively visualizing the whole genome at a glance},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interactive visual exploration of longitudinal historical
career mobility data. <em>TVCG</em>, <em>28</em>(10), 3441–3455. (<a
href="https://doi.org/10.1109/TVCG.2021.3067200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increased availability of quantitative historical datasets has provided new research opportunities for multiple disciplines in social science. In this article, we work closely with the constructors of a new dataset, CGED-Q (China Government Employee Database-Qing), that records the career trajectories of over 340,000 government officials in the Qing bureaucracy in China from 1760 to 1912. We use these data to study career mobility from a historical perspective and understand social mobility and inequality. However, existing statistical approaches are inadequate for analyzing career mobility in this historical dataset with its fine-grained attributes and long time span, since they are mostly hypothesis-driven and require substantial effort. We propose CareerLens , an interactive visual analytics system for assisting experts in exploring, understanding, and reasoning from historical career data. With CareerLens , experts examine mobility patterns in three levels-of-detail, namely, the macro-level providing a summary of overall mobility, the meso-level extracting latent group mobility patterns, and the micro-level revealing social relationships of individuals. We demonstrate the effectiveness and usability of CareerLens through two case studies and receive encouraging feedback from follow-up interviews with domain experts.},
  archive      = {J_TVCG},
  author       = {Yifang Wang and Hongye Liang and Xinhuan Shu and Jiachen Wang and Ke Xu and Zikun Deng and Cameron Campbell and Bijia Chen and Yingcai Wu and Huamin Qu},
  doi          = {10.1109/TVCG.2021.3067200},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3441-3455},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive visual exploration of longitudinal historical career mobility data},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Influence maximization with visual analytics. <em>TVCG</em>,
<em>28</em>(10), 3428–3440. (<a
href="https://doi.org/10.1109/TVCG.2022.3190623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In social networks, individuals’ decisions are strongly influenced by recommendations from their friends, acquaintances, and favorite renowned personalities. The popularity of online social networking platforms makes them the prime venues to advertise products and promote opinions. The Influence Maximization (IM) problem entails selecting a seed set of users that maximizes the influence spread, i.e., the expected number of users positively influenced by a stochastic diffusion process triggered by the seeds. Engineering and analyzing IM algorithms remains a difficult and demanding task due to the NP-hardness of the problem and the stochastic nature of the diffusion processes. Despite several heuristics being introduced, they often fail in providing enough information on how the network topology affects the diffusion process, precious insights that could help researchers improve their seed set selection. In this paper, we present VAIM, a visual analytics system that supports users in analyzing, evaluating, and comparing information diffusion processes determined by different IM algorithms. Furthermore, VAIM provides useful insights that the analyst can use to modify the seed set of an IM algorithm, so to improve its influence spread. We assess our system by: $(i)$ a qualitative evaluation based on a guided experiment with two domain experts on two different data sets; $(ii)$ a quantitative estimation of the value of the proposed visualization through the ICE-T methodology by Wall et al. (IEEE TVCG - 2018). The twofold assessment indicates that VAIM effectively supports our target users in the visual analysis of the performance of IM algorithms.},
  archive      = {J_TVCG},
  author       = {Alessio Arleo and Walter Didimo and Giuseppe Liotta and Silvia Miksch and Fabrizio Montecchiani},
  doi          = {10.1109/TVCG.2022.3190623},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3428-3440},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Influence maximization with visual analytics},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incompressibility enforcement for multiple-fluid SPH using
deformation gradient. <em>TVCG</em>, <em>28</em>(10), 3417–3427. (<a
href="https://doi.org/10.1109/TVCG.2021.3062643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To maintain incompressibility in SPH fluid simulations is important for visual plausibility. However, it remains an outstanding challenge to enforce incompressibility in such recent multiple-fluid simulators as the mixture-model SPH framework. To tackle this problem, we propose a novel incompressible SPH solver, where the compressibility of fluid is directly measured by the deformation gradient. By disconnecting the incompressibility of fluid from the conditions of constant density and divergence-free velocity, the new incompressible SPH solver is applicable to both single- and multiple-fluid simulations. The proposed algorithm can be readily integrated into existing incompressible SPH frameworks developed for single-fluid, and is fully parallelizable on GPU. Applied to multiple-fluid simulations, the new incompressible SPH scheme significantly improves the visual effects of the mixture-model simulation, and it also allows exploitation for artistic controlling.},
  archive      = {J_TVCG},
  author       = {Bo Ren and Wei He and Chen-Feng Li and Xu Chen},
  doi          = {10.1109/TVCG.2021.3062643},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3417-3427},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Incompressibility enforcement for multiple-fluid SPH using deformation gradient},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving visualization design for effective multi-objective
decision making. <em>TVCG</em>, <em>28</em>(10), 3405–3416. (<a
href="https://doi.org/10.1109/TVCG.2021.3065126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-makers across many professions are often required to make multi-objective decisions over increasingly larger volumes of data with several competing criteria. Data visualization is a powerful tool for exploring these complex ‘solution spaces’, but there is limited research on its ability to support multi-objective decisions. In this article, we explore the effects of chart complexity and data volume on decision quality in multi-objective scenarios with complex trade-offs. We look at the impact of four common multidimensional chart types (scatter plot matrices, parallel coordinates plots, heat maps, radar charts), the number of options and dimensions and participant chart usage experience on decision time and accuracy when selecting the ‘optimal option’. As objectively evaluating the quality of multi-objective decisions and the trade-offs involved is challenging, we employ rank- and score-based accuracy metrics. While heat maps demonstrate a time advantage, our findings show no strong performance benefit for one chart type over another for accuracy. We find mixed evidence for the impact of chart complexity on performance, with our results suggesting the existence of a ‘ceiling’ in the number of dimensions considered by participants. This points to a potential limit to data complexity that is useful for decision making. Lastly, participants who use charts frequently performed better, suggesting that users can potentially be trained to effectively use complex visualizations in their decision-making.},
  archive      = {J_TVCG},
  author       = {Bianchi Dy and Nazim Ibrahim and Ate Poorthuis and Sam Joyce},
  doi          = {10.1109/TVCG.2021.3065126},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3405-3416},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Improving visualization design for effective multi-objective decision making},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geometrically interlocking space-filling tiling based on
fabric weaves. <em>TVCG</em>, <em>28</em>(10), 3391–3404. (<a
href="https://doi.org/10.1109/TVCG.2021.3065457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce a framework for the geometric design and fabrication of a family of geometrically interlocking space-filling shapes, which we call woven tiles. Our framework is based on a unique combination of (1) Voronoi partitioning of space using curve segments as the Voronoi sites and (2) the design of these curve segments based on weave patterns closed under symmetry operations. The underlying weave geometry provides an interlocking property to the tiles and the closure property under symmetry operations ensure single tile can fill space. In order to demonstrate this general framework, we focus on specific symmetry operations induced by fabric weaving patterns. We specifically showcase the design and fabrication of woven tiles on flat and curved domains by using the most common 2-fold fabrics, namely, plain, twill, and satin weaves. We further evaluate and compare the mechanical behavior of the so created woven tiles through finite element analysis.},
  archive      = {J_TVCG},
  author       = {Vinayak R. Krishnamurthy and Ergun Akleman and Sai Ganesh Subramanian and Matthew Ebert and Jiaqi Cui and Chia-an Fu and Courtney Starrett},
  doi          = {10.1109/TVCG.2021.3065457},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3391-3404},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Geometrically interlocking space-filling tiling based on fabric weaves},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GAN-based multi-style photo cartoonization. <em>TVCG</em>,
<em>28</em>(10), 3376–3390. (<a
href="https://doi.org/10.1109/TVCG.2021.3067201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cartoon is a common form of art in our daily life and automatic generation of cartoon images from photos is highly desirable. However, state-of-the-art single-style methods can only generate one style of cartoon images from photos and existing multi-style image style transfer methods still struggle to produce high-quality cartoon images due to their highly simplified and abstract nature. In this article, we propose a novel multi-style generative adversarial network (GAN) architecture, called MS-CartoonGAN, which can transform photos into multiple cartoon styles. MS-CartoonGAN uses only unpaired photos and cartoon images of multiple styles for training. To achieve this, we propose to use (1) a hierarchical semantic loss with sparse regularization to retain semantic content and recover flat shading in different abstract levels, (2) a new edge-promoting adversarial loss for producing fine edges, and (3) a style loss to enhance the difference between output cartoon styles and make training process more stable. We also develop a multi-domain architecture, where the generator consists of a shared encoder and multiple decoders for different cartoon styles, along with multiple discriminators for individual styles. By observing that cartoon images drawn by different artists have their unique styles while sharing some common characteristics, our shared network architecture exploits the common characteristics of cartoon styles, achieving better cartoonization and being more efficient than single-style cartoonization. We show that our multi-domain architecture can theoretically guarantee to output desired multiple cartoon styles. Through extensive experiments including a user study, we demonstrate the superiority of the proposed method, outperforming state-of-the-art single-style and multi-style image style transfer methods.},
  archive      = {J_TVCG},
  author       = {Yezhi Shu and Ran Yi and Mengfei Xia and Zipeng Ye and Wang Zhao and Yang Chen and Yu-Kun Lai and Yong-Jin Liu},
  doi          = {10.1109/TVCG.2021.3067201},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3376-3390},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GAN-based multi-style photo cartoonization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DTexFusion: Dynamic texture fusion using a consumer RGBD
sensor. <em>TVCG</em>, <em>28</em>(10), 3365–3375. (<a
href="https://doi.org/10.1109/TVCG.2021.3064846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In addition to 3D geometry, accurate representation of texture is important when digitizing real objects in virtual worlds. Based on a single consumer RGBD sensor, accurate texture representation for static objects can be realized by fusing multi-frame information; however, extending the process to dynamic objects, which typically have time-varying textures, is difficult. Thus, to address this problem, we propose a compact keyframe-based representation that decouples a dynamic texture into a basic static texture and a set of multiplicative changing maps. With this representation, the proposed method first aligns textures recorded from multiple keyframes with the reconstructed dynamic geometry of the object. Errors in the alignment and geometry are then compensated in an innovative iterative linear optimization framework. With the reconstructed texture, we then employ a scheme to synthesize the dynamic object from arbitrary viewpoints. By considering temporal and local pose similarities jointly, dynamic textures in all keyframes are fused to guarantee high-quality image generation. Experimental results demonstrate that the proposed method handles various dynamic objects, including faces, bodies, cloth, and toys. In addition, qualitative and quantitative comparisons demonstrate that the proposed method outperforms state-of-the-art solutions.},
  archive      = {J_TVCG},
  author       = {Chengwei Zheng and Feng Xu},
  doi          = {10.1109/TVCG.2021.3064846},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3365-3375},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DTexFusion: Dynamic texture fusion using a consumer RGBD sensor},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Declutter and focus: Empirically evaluating design
guidelines for effective data communication. <em>TVCG</em>,
<em>28</em>(10), 3351–3364. (<a
href="https://doi.org/10.1109/TVCG.2021.3068337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data visualization design has a powerful effect on which patterns we see as salient and how quickly we see them. The visualization practitioner community prescribes two popular guidelines for creating clear and efficient visualizations: declutter and focus. The declutter guidelines suggest removing non-critical gridlines, excessive labeling of data values, and color variability to improve aesthetics and to maximize the emphasis on the data relative to the design itself. The focus guidelines for explanatory communication recommend including a clear headline that describes the relevant data pattern, highlighting a subset of relevant data values with a unique color, and connecting those values to written annotations that contextualize them in a broader argument. We evaluated how these recommendations impact recall of the depicted information across cluttered, decluttered, and decluttered+focused designs of six graph topics. Undergraduate students were asked to redraw previously seen visualizations, to recall their topics and main conclusions, and to rate the varied designs on aesthetics, clarity, professionalism, and trustworthiness. Decluttering designs led to higher ratings on professionalism, and adding focus to the design led to higher ratings on aesthetics and clarity. They also showed better memory for the highlighted pattern in the data, as reflected across redrawings of the original visualization and typed free-response conclusions, though we do not know whether these results would generalize beyond our memory-based tasks. The results largely empirically validate the intuitions of visualization designers and practitioners. The stimuli, data, analysis code, and Supplementary Materials are available at https://osf.io/wes9u/ .},
  archive      = {J_TVCG},
  author       = {Kiran Ajani and Elsie Lee and Cindy Xiong and Cole Nussbaumer Knaflic and William Kemper and Steven Franconeri},
  doi          = {10.1109/TVCG.2021.3068337},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3351-3364},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Declutter and focus: Empirically evaluating design guidelines for effective data communication},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditions of a multi-view 3D display for accurate
reproduction of perceived glossiness. <em>TVCG</em>, <em>28</em>(10),
3336–3350. (<a href="https://doi.org/10.1109/TVCG.2021.3063182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualizing objects as they are perceived in the real world is often critical in our daily experiences. We previously focused on objects&#39; surface glossiness visualized with a 3D display and found that a multi-view 3D display reproduces perceived glossiness more accurately than a 2D display. This improvement of glossiness reproduction can be explained by the fact that a glossy surface visualized by a multi-view 3D display appropriately provides luminance differences between the two eyes and luminance changes accompanying the viewer&#39;s lateral head motion. In the present study, to determine the requirements of a multi-view 3D display for the accurate reproduction of perceived glossiness, we developed a simulator of a multi-view 3D display to independently and simultaneously manipulate the viewpoint interval and the magnitude of the optical inter-view crosstalk. Using the simulator, we conducted a psychophysical experiment and found that glossiness reproduction is most accurate when the viewpoint interval is small and there is just a small (but not too small) amount of crosstalk. We proposed a simple yet perceptually valid model that quantitatively predicts the reproduction accuracy of perceived glossiness.},
  archive      = {J_TVCG},
  author       = {Yuichi Sakano and Hiroshi Ando},
  doi          = {10.1109/TVCG.2021.3063182},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3336-3350},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Conditions of a multi-view 3D display for accurate reproduction of perceived glossiness},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive irradiance sampling for many-light rendering of
subsurface scattering. <em>TVCG</em>, <em>28</em>(10), 3324–3335. (<a
href="https://doi.org/10.1109/TVCG.2021.3066640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rendering a translucent material involves integrating the product of the transmittance-weighted irradiance and the BSSRDF over the surface of it. In previous methods, this spatial integral was computed by creating a dense distribution of discrete points over the surface or by importance-sampling based on the BSSRDF. Both of these approaches necessitate specifying the number of samples, which affects both the quality and the computation time for rendering. An insufficient number of samples leads to noise and artifacts in the rendered image and an excessive number results in a prohibitively long rendering time. In this article, we propose an error estimation method for translucent materials in a many-light rendering framework. Our adaptive sampling can automatically determine the number of samples so that the estimated relative error of each pixel intensity is less than a user-specified threshold. We also propose an efficient method to generate the sampling points that make large contributions to the pixel intensity taking into account the BSSRDF. This enables us to use a simple uniform sampling, instead of costly importance sampling based on the BSSRDF. The experimental results show that our method can accurately estimate the error. In addition, in comparison with the previous methods, our sampling method achieves better estimation accuracy in equal-time.},
  archive      = {J_TVCG},
  author       = {Kosuke Nabata and Kei Iwasaki},
  doi          = {10.1109/TVCG.2021.3066640},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3324-3335},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Adaptive irradiance sampling for many-light rendering of subsurface scattering},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Task-based visual interactive modeling: Decision trees and
rule-based classifiers. <em>TVCG</em>, <em>28</em>(9), 3307–3323. (<a
href="https://doi.org/10.1109/TVCG.2020.3045560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual analytics enables the coupling of machine learning models and humans in a tightly integrated workflow, addressing various analysis tasks. Each task poses distinct demands to analysts and decision-makers. In this survey, we focus on one canonical technique for rule-based classification, namely decision tree classifiers. We provide an overview of available visualizations for decision trees with a focus on how visualizations differ with respect to 16 tasks. Further, we investigate the types of visual designs employed, and the quality measures presented. We find that (i) interactive visual analytics systems for classifier development offer a variety of visual designs, (ii) utilization tasks are sparsely covered, (iii) beyond classifier development, node-link diagrams are omnipresent, (iv) even systems designed for machine learning experts rarely feature visual representations of quality measures other than accuracy. In conclusion, we see a potential for integrating algorithmic techniques, mathematical quality measures, and tailored interactive visualizations to enable human experts to utilize their knowledge more effectively.},
  archive      = {J_TVCG},
  author       = {Dirk Streeb and Yannick Metz and Udo Schlegel and Bruno Schneider and Mennatallah El-Assady and Hansjörg Neth and Min Chen and Daniel A. Keim},
  doi          = {10.1109/TVCG.2020.3045560},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3307-3323},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Task-based visual interactive modeling: Decision trees and rule-based classifiers},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Diagnosing ensemble few-shot classifiers. <em>TVCG</em>,
<em>28</em>(9), 3292–3306. (<a
href="https://doi.org/10.1109/TVCG.2022.3182488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The base learners and labeled samples (shots) in an ensemble few-shot classifier greatly affect the model performance. When the performance is not satisfactory, it is usually difficult to understand the underlying causes and make improvements. To tackle this issue, we propose a visual analysis method, FSLDiagnotor. Given a set of base learners and a collection of samples with a few shots, we consider two problems: 1) finding a subset of base learners that well predict the sample collections; and 2) replacing the low-quality shots with more representative ones to adequately represent the sample collections. We formulate both problems as sparse subset selection and develop two selection algorithms to recommend appropriate learners and shots, respectively. A matrix visualization and a scatterplot are combined to explain the recommended learners and shots in context and facilitate users in adjusting them. Based on the adjustment, the algorithm updates the recommendation results for another round of improvement. Two case studies are conducted to demonstrate that FSLDiagnotor helps build a few-shot classifier efficiently and increases the accuracy by 12\% and 21\%, respectively.},
  archive      = {J_TVCG},
  author       = {Weikai Yang and Xi Ye and Xingxing Zhang and Lanxi Xiao and Jiazhi Xia and Zhongyuan Wang and Jun Zhu and Hanspeter Pfister and Shixia Liu},
  doi          = {10.1109/TVCG.2022.3182488},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3292-3306},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Diagnosing ensemble few-shot classifiers},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Do you believe your (social media) data? A personal story on
location data biases, errors, and plausibility as well as their
visualization. <em>TVCG</em>, <em>28</em>(9), 3277–3291. (<a
href="https://doi.org/10.1109/TVCG.2022.3141605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a case study on a journey about a personal data collection of carnivorous plant species habitats, and the resulting scientific exploration of location data biases, data errors, location hiding, and data plausibility. While initially driven by personal interest, our work led to the analysis and development of various means for visualizing threats to insight from geo-tagged social media data. In the course of this endeavor we analyzed local and global geographic distributions and their inaccuracies. We also contribute Motion Plausibility Profiles—a new means for visualizing how believable a specific contributor’s location data is or if it was likely manipulated. We then compared our own repurposed social media dataset with data from a dedicated citizen science project. Compared to biases and errors in the literature on traditional citizen science data, with our visualizations we could also identify some new types or show new aspects for known ones. Moreover, we demonstrate several types of errors and biases for repurposed social media data. Please note that people with color impairments may consider our alternative paper version.},
  archive      = {J_TVCG},
  author       = {Tobias Isenberg and Zujany Salazar and Rafael Blanco and Catherine Plaisant},
  doi          = {10.1109/TVCG.2022.3141605},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3277-3291},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Do you believe your (Social media) data? a personal story on location data biases, errors, and plausibility as well as their visualization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video vectorization via bipartite diffusion curves
propagation and optimization. <em>TVCG</em>, <em>28</em>(9), 3265–3276.
(<a href="https://doi.org/10.1109/TVCG.2021.3061131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new video vectorization approach for converting videos in the raster format to vector representation with the benefits of resolution independence and compact storage. Through classifying extracted curves in each video frame into salient ones and non-salient ones, we introduce a novel bipartite diffusion curves (BDCs) representation in order to preserve both important image features such as sharp boundaries and regions with smooth color variation. This bipartite representation allows us to propagate non-salient curves across frames such that the propagation, in conjunction with geometry optimization and color optimization of salient curves, ensures the preservation of fine details within each frame and across different frames, and meanwhile, achieves good spatial-temporal coherence. Thorough experiments on a variety of videos show that our method is capable of converting videos to the vector representation with low reconstruction errors, low computational cost, and fine details, demonstrating our superior performance over the state of the art. We also show that, when used for video upsampling, our method produces results comparable to video super-resolution.},
  archive      = {J_TVCG},
  author       = {Yuanqi Li and Chuan Wang and Jing Hong and Jie Zhu and Jie Guo and Jue Wang and Yanwen Guo and Wenping Wang},
  doi          = {10.1109/TVCG.2021.3061131},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3265-3276},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Video vectorization via bipartite diffusion curves propagation and optimization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The effect of exploration mode and frame of reference in
immersive analytics. <em>TVCG</em>, <em>28</em>(9), 3252–3264. (<a
href="https://doi.org/10.1109/TVCG.2021.3060666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design space for user interfaces for Immersive Analytics applications is vast. Designers can combine navigation and manipulation to enable data exploration with ego- or exocentric views, have the user operate at different scales, or use different forms of navigation with varying levels of physical movement. This freedom results in a multitude of different viable approaches. Yet, there is no clear understanding of the advantages and disadvantages of each choice. Our goal is to investigate the affordances of several major design choices, to enable both application designers and users to make better decisions. In this article, we assess two main factors, exploration mode and frame of reference, consequently also varying visualization scale and physical movement demand. To isolate each factor, we implemented nine different conditions in a Space-Time Cube visualization use case and asked 36 participants to perform multiple tasks. We analyzed the results in terms of performance and qualitative measures and correlated them with participants’ spatial abilities. While egocentric room-scale exploration significantly reduced mental workload, exocentric exploration improved performance in some tasks. Combining navigation and manipulation made tasks easier by reducing workload, temporal demand, and physical effort.},
  archive      = {J_TVCG},
  author       = {Jorge Wagner and Wolfgang Stuerzlinger and Luciana Nedel},
  doi          = {10.1109/TVCG.2021.3060666},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3252-3264},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The effect of exploration mode and frame of reference in immersive analytics},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GPU optimization for high-quality kinetic fluid simulation.
<em>TVCG</em>, <em>28</em>(9), 3235–3251. (<a
href="https://doi.org/10.1109/TVCG.2021.3059753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fluid simulations are often performed using the incompressible Navier-Stokes equations (INSE), leading to sparse linear systems which are difficult to solve efficiently in parallel. Recently, kinetic methods based on the adaptive-central-moment multiple-relaxation-time (ACM-MRT) model [1], [2] have demonstrated impressive capabilities to simulate both laminar and turbulent flows, with quality matching or surpassing that of state-of-the-art INSE solvers. Furthermore, due to its local formulation, this method presents the opportunity for highly scalable implementations on parallel systems such as GPUs. However, an efficient ACM-MRT-based kinetic solver needs to overcome a number of computational challenges, especially when dealing with complex solids inside the fluid domain. In this article, we present multiple novel GPU optimization techniques to efficiently implement high-quality ACM-MRT-based kinetic fluid simulations in domains containing complex solids. Our techniques include a new communication-efficient data layout, a load-balanced immersed-boundary method, a multi-kernel launch method using a simplified formulation of ACM-MRT calculations to enable greater parallelism, and the integration of these techniques into a parametric cost model to enable automated prameter search to achieve optimal execution performance. We also extended our method to multi-GPU systems to enable large-scale simulations. To demonstrate the state-of-the-art performance and high visual quality of our solver, we present extensive experimental results and comparisons to other solvers.},
  archive      = {J_TVCG},
  author       = {Yixin Chen and Wei Li and Rui Fan and Xiaopei Liu},
  doi          = {10.1109/TVCG.2021.3059753},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3235-3251},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GPU optimization for high-quality kinetic fluid simulation},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable scalable vector graphics: Automatic translation of
interactive SVGs to a multithread VDOM for fast rendering.
<em>TVCG</em>, <em>28</em>(9), 3219–3234. (<a
href="https://doi.org/10.1109/TVCG.2021.3059294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dominant markup language for Web visualizations—Scalable Vector Graphics (SVG)—is comparatively easy to learn, and is open, accessible, customizable via CSS, and searchable via the DOM, with easy interaction handling and debugging. Because these attributes allow visualization creators to focus on design on implementation details, tools built on top of SVG, such as D3.js, are essential to the visualization community. However, slow SVG rendering can limit designs by effectively capping the number of on-screen data points, and this can force visualization creators to switch to Canvas or WebGL. These are less flexible (e.g., no search or styling via CSS), and harder to learn. We introduce Scalable Scalable Vector Graphics (SSVG) to reduce these limitations and allow complex and smooth visualizations to be created with SVG. SSVG automatically translates interactive SVG visualizations into a dynamic virtual DOM (VDOM) to bypass the browser&#39;s slow ‘to specification’ rendering by intercepting JavaScript function calls. De-coupling the SVG visualization specification from SVG rendering, and obtaining a dynamic VDOM, creates flexibility and opportunity for visualization system research. SSVG uses this flexibility to free up the main thread for more interactivity and renders the visualization with Canvas or WebGL on a web worker. Together, these concepts create a drop-in JavaScript library which can improve rendering performance by 3–9× with only one line of code added . To demonstrate applicability, we describe the use of SSVG on multiple example visualizations including published visualization research. A free copy of this article, collected data, and source code are available as open science at osf.io/ge8wp.},
  archive      = {J_TVCG},
  author       = {Michail Schwab and David Saffo and Nicholas Bond and Shash Sinha and Cody Dunne and Jeff Huang and James Tompkin and Michelle A. Borkin},
  doi          = {10.1109/TVCG.2021.3059294},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3219-3234},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scalable scalable vector graphics: Automatic translation of interactive SVGs to a multithread VDOM for fast rendering},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Meta-PU: An arbitrary-scale upsampling network for point
cloud. <em>TVCG</em>, <em>28</em>(9), 3206–3218. (<a
href="https://doi.org/10.1109/TVCG.2021.3058311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud upsampling is vital for the quality of the mesh in three-dimensional reconstruction. Recent research on point cloud upsampling has achieved great success due to the development of deep learning. However, the existing methods regard point cloud upsampling of different scale factors as independent tasks. Thus, the methods need to train a specific model for each scale factor, which is both inefficient and impractical for storage and computation in real applications. To address this limitation, in this article, we propose a novel method called “Meta-PU” to first support point cloud upsampling of arbitrary scale factors with a single model. In the Meta-PU method, besides the backbone network consisting of residual graph convolution (RGC) blocks, a meta-subnetwork is learned to adjust the weights of the RGC blocks dynamically, and a farthest sampling block is adopted to sample different numbers of points. Together, these two blocks enable our Meta-PU to continuously upsample the point cloud with arbitrary scale factors by using only a single model. In addition, the experiments reveal that training on multiple scales simultaneously is beneficial to each other. Thus, Meta-PU even outperforms the existing methods trained for a specific scale factor only.},
  archive      = {J_TVCG},
  author       = {Shuquan Ye and Dongdong Chen and Songfang Han and Ziyu Wan and Jing Liao},
  doi          = {10.1109/TVCG.2021.3058311},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3206-3218},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Meta-PU: An arbitrary-scale upsampling network for point cloud},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adapting virtual embodiment through reinforcement learning.
<em>TVCG</em>, <em>28</em>(9), 3193–3205. (<a
href="https://doi.org/10.1109/TVCG.2021.3057797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Virtual Reality, having a virtual body opens a wide range of possibilities as the participant’s avatar can appear to be quite different from oneself for the sake of the targeted application (e.g., for perspective-taking). In addition, the system can partially manipulate the displayed avatar movement through some distortion to make the overall experience more enjoyable and effective (e.g., training, exercising, rehabilitation). Despite its potential, an excessive distortion may become noticeable and break the feeling of being embodied into the avatar. Past researches have shown that individuals have a relatively high tolerance to movement distortions and a great variability of individual sensitivities to distortions. In this article, we propose a method taking advantage of Reinforcement Learning (RL) to efficiently identify the magnitude of the maximum distortion that does not get noticed by an individual (further noted the detection threshold). We show through a controlled experiment with subjects that the RL method finds a more robust detection threshold compared to the adaptive staircase method, i.e., it is more able to prevent subjects from detecting the distortion when its amplitude is equal or below the threshold. Finally, the associated majority voting system makes the RL method able to handle more noise within the forced choices input than adaptive staircase. This last feature is essential for future use with physiological signals as these latter are even more susceptible to noise. It would then allow to calibrate embodiment individually to increase the effectiveness of the proposed interactions.},
  archive      = {J_TVCG},
  author       = {Thibault Porssut and Yawen Hou and Olaf Blanke and Bruno Herbelin and Ronan Boulic},
  doi          = {10.1109/TVCG.2021.3057797},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3193-3205},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Adapting virtual embodiment through reinforcement learning},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wearable 3D machine knitting: Automatic generation of shaped
knit sheets to cover real-world objects. <em>TVCG</em>, <em>28</em>(9),
3180–3192. (<a href="https://doi.org/10.1109/TVCG.2021.3056101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knitting can efficiently fabricate stretchable and durable soft surfaces. These surfaces are often designed to be worn on solid objects as covers, garments, and accessories. Given a 3D model, we consider a knit for it wearable if the knit not only reproduces the shape of the 3D model but also can be put on and taken off from the model without deforming the model. This “wearability” places additional constraints on surface design and fabrication, which existing machine knitting approaches do not take into account. We introduce the first practical automatic pipeline to generate knit designs that are both wearable and machine knittable. Our pipeline handles knittability and wearability with two separate modules that run in parallel. Specifically, given a 3D object and its corresponding 3D garment surface, our approach first converts the garment surface into a topological disc by introducing a set of cuts. The resulting cut surface is then fed into a physically-based unclothing simulation module to ensure the garment’s wearability over the object. The unclothing simulation determines which of the previously introduced cuts could be sewn permanently without impacting wearability. Concurrently, the cut surface is converted into an anisotropic stitch mesh. Then, our novel, stochastic, any-time flat-knitting scheduler generates fabrication instructions for an industrial knitting machine. Finally, we fabricate the garment and manually assemble it into one complete covering worn by the target object. We demonstrate our method’s robustness and knitting efficiency by fabricating models with various topological and geometric complexities. Further, we show that our method can be incorporated into a knitting design tool for creating knitted garments with customized patterns.},
  archive      = {J_TVCG},
  author       = {Kui Wu and Marco Tarini and Cem Yuksel and James McCann and Xifeng Gao},
  doi          = {10.1109/TVCG.2021.3056101},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3180-3192},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Wearable 3D machine knitting: Automatic generation of shaped knit sheets to cover real-world objects},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adapted SIMPLE algorithm for incompressible SPH fluids with
a broad range viscosity. <em>TVCG</em>, <em>28</em>(9), 3168–3179. (<a
href="https://doi.org/10.1109/TVCG.2021.3055789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In simulating viscous incompressible SPH fluids, incompressibility and viscosity are typically solved in two separate stages. However, the interference between pressure and shear forces could cause the missing of behaviors that include preservation of sharp surface details and remarkable viscous behaviors such as buckling and rope coiling. To alleviate this problem, we introduce for the first time the semi-implicit method for pressure linked equations (SIMPLE) into SPH to solve incompressible fluids with a broad range viscosity. We propose to link incompressibility and viscosity solvers, and impose incompressibility and viscosity constraints iteratively to gradually remove the interference between pressure and shear forces. We will also discuss how to solve the particle deficiency problem for both incompressibility and viscosity solvers. Our method is stable at simulating incompressible fluids whose viscosity can range from zero to an extremely high value. Compared to state-of-the-art methods, our method not only produces realistic viscous behaviors, but is also better at preserving sharp surface details.},
  archive      = {J_TVCG},
  author       = {Shusen Liu and Xiaowei He and Wencheng Wang and Enhua Wu},
  doi          = {10.1109/TVCG.2021.3055789},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3168-3179},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Adapted SIMPLE algorithm for incompressible SPH fluids with a broad range viscosity},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantifiable fine-grain occlusion removal assistance for
efficient VR exploration. <em>TVCG</em>, <em>28</em>(9), 3154–3167. (<a
href="https://doi.org/10.1109/TVCG.2021.3053287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an occlusion management approach that handles fine-grain occlusions, and that quantifies and localizes occlusions as a user explores a virtual environment (VE). Fine-grain occlusions are handled by finding the VE region where they occur, and by constructing a multiperspective visualization that lets the user explore the region from the current location, with intuitive head motions, without first having to walk to the region. VE geometry close to the user is rendered conventionally, from the user’s viewpoint, to anchor the user, avoiding disorientation and simulator sickness. Given a viewpoint, residual occlusions are quantified and localized as VE voxels that cannot be seen from the given viewpoint but that can be seen from nearby viewpoints. This residual occlusion quantification and localization helps the user ascertain that a VE region has been explored exhaustively. The occlusion management approach was tested in three controlled studies, which confirmed the exploration efficiency benefit of the approach, and in perceptual experiments, which confirmed that exploration efficiency does not come at the cost of reducing spatial awareness and sense of presence, or of increasing simulator sickness.},
  archive      = {J_TVCG},
  author       = {Jian Wu and Lili Wang and Hui Zhang and Voicu Popescu},
  doi          = {10.1109/TVCG.2021.3053287},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3154-3167},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Quantifiable fine-grain occlusion removal assistance for efficient VR exploration},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consensus based networking of distributed virtual
environments. <em>TVCG</em>, <em>28</em>(9), 3138–3153. (<a
href="https://doi.org/10.1109/TVCG.2021.3052580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed virtual environments (DVEs) are challenging to create as the goals of consistency and responsiveness become contradictory under increasing latency. DVEs have been considered as both distributed transactional databases and force-reflection systems. Both are good approaches, but they do have drawbacks. Transactional systems do not support Level 3 (L3) collaboration: manipulating the same degree-of-freedom at the same time. Force-reflection requires a client-server architecture and stabilisation techniques. With Consensus Based Networking (CBN), we suggest DVEs be considered as a distributed data-fusion problem. Many simulations run in parallel and exchange their states, with remote states integrated with continous authority. Over time the exchanges average out local differences, performing a distribued-average of a consistent, shared state. CBN aims to build simulations that are highly responsive, but consistent enough for use cases such as the piano-movers problem. CBN’s support for heterogeneous nodes can transparently couple different input methods, avoid the requirement of determinism, and provide more options for personal control over the shared experience. Our work is early, however we demonstrate many successes, including L3 collaboration in room-scale VR, 1000’s of interacting objects, complex configurations such as stacking, and transparent coupling of haptic devices. These have been shown before, but each with a different technique; CBN supports them all within a single, unified system.},
  archive      = {J_TVCG},
  author       = {Sebastian Friston and Elias Griffith and David Swapp and Simon Julier and Caleb Irondi and Fred Jjunju and Ryan Ward and Alan Marshall and Anthony Steed},
  doi          = {10.1109/TVCG.2021.3052580},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3138-3153},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Consensus based networking of distributed virtual environments},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tracking internal frames of reference for consistent
molecular distribution functions. <em>TVCG</em>, <em>28</em>(9),
3126–3137. (<a href="https://doi.org/10.1109/TVCG.2021.3051632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In molecular analysis, spatial distribution functions (SDF) are fundamental instruments in answering questions related to spatial occurrences and relations of atomic structures over time. Given a molecular trajectory, SDFs can, for example, reveal the occurrence of water in relation to particular structures and hence provide clues of hydrophobic and hydrophilic regions. For the computation of meaningful distribution functions, the definition of molecular reference structures is essential. Therefore we introduce the concept of an internal frame of reference (IFR) for labeled point sets that represent selected molecular structures, and we propose an algorithm for tracking the IFR over time and space using a variant of Kabsch’s algorithm. This approach lets us generate a consistent space for the aggregation of the SDF for molecular trajectories and molecular ensembles. We demonstrate the usefulness of the technique by applying it to temporal molecular trajectories as well as ensemble datasets. The examples include different docking scenarios with DNA, insulin, and aspirin.},
  archive      = {J_TVCG},
  author       = {Robin Skånberg and Martin Falk and Mathieu Linares and Anders Ynnerman and Ingrid Hotz},
  doi          = {10.1109/TVCG.2021.3051632},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3126-3137},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Tracking internal frames of reference for consistent molecular distribution functions},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Plausible 3D face wrinkle generation using variational
autoencoders. <em>TVCG</em>, <em>28</em>(9), 3113–3125. (<a
href="https://doi.org/10.1109/TVCG.2021.3051251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Realistic 3D facial modeling and animation have been increasingly used in many graphics, animation, and virtual reality applications. However, generating realistic fine-scale wrinkles on 3D faces, in particular, on animated 3D faces, is still a challenging problem that is far away from being resolved. In this article we propose an end-to-end system to automatically augment coarse-scale 3D faces with synthesized fine-scale geometric wrinkles. By formulating the wrinkle generation problem as a supervised generation task, we implicitly model the continuous space of face wrinkles via a compact generative model, such that plausible face wrinkles can be generated through effective sampling and interpolation in the space. We also introduce a complete pipeline to transfer the synthesized wrinkles between faces with different shapes and topologies. Through many experiments, we demonstrate our method can robustly synthesize plausible fine-scale wrinkles on a variety of coarse-scale 3D faces with different shapes and expressions.},
  archive      = {J_TVCG},
  author       = {Qixin Deng and Luming Ma and Aobo Jin and Huikun Bi and Binh Huy Le and Zhigang Deng},
  doi          = {10.1109/TVCG.2021.3051251},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3113-3125},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Plausible 3D face wrinkle generation using variational autoencoders},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Impact of cognitive biases on progressive visualization.
<em>TVCG</em>, <em>28</em>(9), 3093–3112. (<a
href="https://doi.org/10.1109/TVCG.2021.3051013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Progressive visualization is fast becoming a technique in the visualization community to help users interact with large amounts of data. With progressive visualization, users can examine intermediate results of complex or long running computations, without waiting for the computation to complete. While this has shown to be beneficial to users, recent research has identified potential risks. For example, users may misjudge the uncertainty in the intermediate results and draw incorrect conclusions or see patterns that are not present in the final results. In this article, we conduct a comprehensive set of studies to quantify the advantages and limitations of progressive visualization. Based on a recent report by Micallef et al. , we examine four types of cognitive biases that can occur with progressive visualization: uncertainty bias , illusion bias , control bias , and anchoring bias . The results of the studies suggest a cautious but promising use of progressive visualization – while there can be significant savings in task completion time, accuracy can be negatively affected in certain conditions. These findings confirm earlier reports of the benefits and drawbacks of progressive visualization and that continued research into mitigating the effects of cognitive biases is necessary.},
  archive      = {J_TVCG},
  author       = {Marianne Procopio and Ab Mosca and Carlos Scheidegger and Eugene Wu and Remco Chang},
  doi          = {10.1109/TVCG.2021.3051013},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3093-3112},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Impact of cognitive biases on progressive visualization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast 3D indoor scene synthesis by learning spatial relation
priors of objects. <em>TVCG</em>, <em>28</em>(9), 3082–3092. (<a
href="https://doi.org/10.1109/TVCG.2021.3050143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a framework for fast synthesizing indoor scenes, given a room geometry and a list of objects with learnt priors. Unlike existing data-driven solutions, which often learn priors by co-occurrence analysis and statistical model fitting, our method measures the strengths of spatial relations by tests for complete spatial randomness (CSR), and learns discrete priors based on samples with the ability to accurately represent exact layout patterns. With the learnt priors, our method achieves both acceleration and plausibility by partitioning the input objects into disjoint groups, followed by layout optimization using position-based dynamics (PBD) based on the Hausdorff metric. Experiments show that our framework is capable of measuring more reasonable relations among objects and simultaneously generating varied arrangements in seconds compared with the state-of-the-art works.},
  archive      = {J_TVCG},
  author       = {Song-Hai Zhang and Shao-Kui Zhang and Wei-Yu Xie and Cheng-Yang Luo and Yong-Liang Yang and Hongbo Fu},
  doi          = {10.1109/TVCG.2021.3050143},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3082-3092},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fast 3D indoor scene synthesis by learning spatial relation priors of objects},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The effect of alignment on people’s ability to judge event
sequence similarity. <em>TVCG</em>, <em>28</em>(9), 3070–3081. (<a
href="https://doi.org/10.1109/TVCG.2021.3050497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event sequences are central to the analysis of data in domains that range from biology and health, to logfile analysis and people&#39;s everyday behavior. Many visualization tools have been created for such data, but people are error-prone when asked to judge the similarity of event sequences with basic presentation methods. This article describes an experiment that investigates whether local and global alignment techniques improve people&#39;s performance when judging sequence similarity. Participants were divided into three groups (basic versus local versus global alignment), and each participant judged the similarity of 180 sets of pseudo-randomly generated sequences. Each set comprised a target, a correct choice and a wrong choice. After training, the global alignment group was more accurate than the local alignment group (98 versus 93 percent correct), with the basic group getting 95 percent correct. Participants’ response times were primarily affected by the number of event types, the similarity of sequences (measured by the Levenshtein distance) and the edit types (nine combinations of deletion, insertion and substitution). In summary, global alignment is superior and people&#39;s performance could be further improved by choosing alignment parameters that explicitly penalize sequence mismatches.},
  archive      = {J_TVCG},
  author       = {Roy A. Ruddle and Jürgen Bernard and Hendrik Lücke-Tieke and Thorsten May and Jörn Kohlhammer},
  doi          = {10.1109/TVCG.2021.3050497},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3070-3081},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The effect of alignment on people&#39;s ability to judge event sequence similarity},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Errata to “parallax free registration for augmented reality
optical see-through displays in the peripersonal space” [1] (DOI:
10.1109/TVCG.2020.3021534). <em>TVCG</em>, <em>28</em>(8), 3069. (<a
href="https://doi.org/10.1109/TVCG.2022.3176417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the original article, there was a mistake in the content of Table 2 page 8 column 1 as published. The values of the mean and standard deviation of the virtual-to-real overlay error in visual angles, which are reported for different checkerboard distances, are to be corrected. Due to a typing error within the data analysis code, we mistakenly considered an erroneous value of the average angular resolution for the eye-replacement camera. This scale factor is used to pass from the original registration errors (expressed in pixel) to the angular registration errors (in arcmin). The value of the average angular resolution is $\approx 2.67$≈2.67 arcmin/pixel. The corrected Table 2 appears below.},
  archive      = {J_TVCG},
  author       = {Vincenzo Ferrari and Nadia Cattari and Umberto Fontana and Fabrizio Cutolo},
  doi          = {10.1109/TVCG.2022.3176417},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3069},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Errata to “Parallax free registration for augmented reality optical see-through displays in the peripersonal space” [1] (DOI: 10.1109/TVCG.2020.3021534)},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual drift detection for event sequence data of business
processes. <em>TVCG</em>, <em>28</em>(8), 3050–3068. (<a
href="https://doi.org/10.1109/TVCG.2021.3050071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event sequence data is increasingly available in various application domains, such as business process management, software engineering, or medical pathways. Processes in these domains are typically represented as process diagrams or flow charts. So far, various techniques have been developed for automatically generating such diagrams from event sequence data. An open challenge is the visual analysis of drift phenomena when processes change over time. In this article, we address this research gap. Our contribution is a system for fine-granular process drift detection and corresponding visualizations for event logs of executed business processes. We evaluated our system both on synthetic and real-world data. On synthetic logs, we achieved an average F-score of 0.96 and outperformed all the state-of-the-art methods. On real-world logs, we identified all types of process drifts in a comprehensive manner. Finally, we conducted a user study highlighting that our visualizations are easy to use and useful as perceived by process mining experts. In this way, our work contributes to research on process mining, event sequence analysis, and visualization of temporal data.},
  archive      = {J_TVCG},
  author       = {Anton Yeshchenko and Claudio Di Ciccio and Jan Mendling and Artem Polyvyanyy},
  doi          = {10.1109/TVCG.2021.3050071},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3050-3068},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual drift detection for event sequence data of business processes},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-illusion: A study on cognition of role-playing in
immersive virtual environments. <em>TVCG</em>, <em>28</em>(8),
3035–3049. (<a href="https://doi.org/10.1109/TVCG.2020.3044563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the design and results of an experiment investigating the occurrence of self-illusion and its contribution to realistic behavior consistent with a virtual role in virtual environments. Self-illusion is a generalized illusion about one&#39;s self in cognition, eliciting a sense of being associated with a role in a virtual world, despite sure knowledge that this role is not the actual self in the real world. We validate and measure self-illusion through an experiment where each participant occupies a non-human perspective and plays a non-human role using this role&#39;s behavior patterns. 77 participants were enrolled for the user study according to the priori power analysis. In the mixed-design experiment with different levels of manipulations, we asked the participants to play a cat (a non-human role) within an immersive VE and captured their different kinds of responses, finding that the participants with higher self-illusion can connect themselves to the virtual role more easily. Based on statistical analysis of questionnaires and behavior data, there is some evidence that self-illusion can be considered a novel psychological component of presence because it is dissociated from sense of embodiment (SoE), plausibility illusion (Psi), and place illusion (PI). Moreover, self-illusion has the potential to be an effective evaluation metric for user experience in a virtual reality system for certain applications.},
  archive      = {J_TVCG},
  author       = {Sheng Li and Xiang Gu and Kangrui Yi and Yanlin Yang and Guoping Wang and Dinesh Manocha},
  doi          = {10.1109/TVCG.2020.3044563},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3035-3049},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Self-illusion: A study on cognition of role-playing in immersive virtual environments},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Perceptual quality assessment of omnidirectional images as
moving camera videos. <em>TVCG</em>, <em>28</em>(8), 3022–3034. (<a
href="https://doi.org/10.1109/TVCG.2021.3050888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Omnidirectional images (also referred to as static 360 $^{\circ }$ panoramas) impose viewing conditions much different from those of regular 2D images. How do humans perceive image distortions in immersive virtual reality (VR) environments is an important problem which receives less attention. We argue that, apart from the distorted panorama itself, two types of VR viewing conditions are crucial in determining the viewing behaviors of users and the perceived quality of the panorama: the starting point and the exploration time. We first carry out a psychophysical experiment to investigate the interplay among the VR viewing conditions, the user viewing behaviors, and the perceived quality of 360 $^{\circ }$ images. Then, we provide a thorough analysis of the collected human data, leading to several interesting findings. Moreover, we propose a computational framework for objective quality assessment of 360 $^{\circ }$ images, embodying viewing conditions and behaviors in a delightful way. Specifically, we first transform an omnidirectional image to several video representations using different user viewing behaviors under different viewing conditions. We then leverage advanced 2D full-reference video quality models to compute the perceived quality. We construct a set of specific quality measures within the proposed framework, and demonstrate their promises on three VR quality databases.},
  archive      = {J_TVCG},
  author       = {Xiangjie Sui and Kede Ma and Yiru Yao and Yuming Fang},
  doi          = {10.1109/TVCG.2021.3050888},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3022-3034},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Perceptual quality assessment of omnidirectional images as moving camera videos},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). New media and space: An empirical study of learning and
enjoyment through museum hybrid space. <em>TVCG</em>, <em>28</em>(8),
3013–3021. (<a href="https://doi.org/10.1109/TVCG.2020.3043324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A museum hybrid space combines physical artifacts co-located with virtual and augmented reality displays. Although the technology exists to provide museums with hybrid space, there are no empirical studies on effectiveness of the museum hybrid space in terms of learning and enjoyment. This article takes an experimental approach and measures the enjoyment and learning (dependent variables) of participants in response to selected environments (independent variables) including a traditional environment (based on photos and labels), a video-enhanced environment (based on projected video clips), and a VR-enhanced environment (based on video game). The main outcome of this article is demonstrating that the use of VR technology and the resulting hybrid space (i.e., VR-enhanced environment) results in novel museum experiences that provide greater impacts on audience in terms of learning and enjoyment.},
  archive      = {J_TVCG},
  author       = {Farzan Baradaran Rahimi and Jeffrey E. Boyd and Richard M. Levy and Jennifer Eiserman},
  doi          = {10.1109/TVCG.2020.3043324},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3013-3021},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {New media and space: An empirical study of learning and enjoyment through museum hybrid space},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mesh denoising with facet graph convolutions. <em>TVCG</em>,
<em>28</em>(8), 2999–3012. (<a
href="https://doi.org/10.1109/TVCG.2020.3045490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the problem of mesh denoising, which consists of removing noise from corrupted 3D meshes while preserving existing geometric features. Most mesh denoising methods require a lot of mesh-specific parameter fine-tuning, to account for specific features and noise types. In recent years, data-driven methods have demonstrated their robustness and effectiveness with respect to noise and feature properties on a wide variety of geometry and image problems. Most existing mesh denoising methods still use hand-crafted features, and locally denoise facets rather than examine the mesh globally. In this work, we propose the use of a fully end-to-end learning strategy based on graph convolutions, where meaningful features are learned directly by our network. It operates on a graph of facets, directly on the existing topology of the mesh, without resampling, and follows a multi-scale design to extract geometric features at different resolution levels. Similar to most recent pipelines, given a noisy mesh, we first denoise face normals with our novel approach, then update vertex positions accordingly. Our method performs significantly better than the current state-of-the-art learning-based methods. Additionally, we show that it can be trained on noisy data, without explicit correspondence between noisy and ground-truth facets. We also propose a multi-scale denoising strategy, better suited to correct noise with a low spatial frequency.},
  archive      = {J_TVCG},
  author       = {Matthieu Armando and Jean-Sébastien Franco and Edmond Boyer},
  doi          = {10.1109/TVCG.2020.3045490},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2999-3012},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mesh denoising with facet graph convolutions},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GridSet: Visualizing individual elements and attributes for
analysis of set-typed data. <em>TVCG</em>, <em>28</em>(8), 2983–2998.
(<a href="https://doi.org/10.1109/TVCG.2020.3047111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present GridSet, a novel set visualization for exploring elements, their attributes, intersections, as well as entire sets. In this set visualization, each set representation is composed of glyphs, which represent individual elements and their attributes utilizing different visual encodings. In each set, elements are organized within a grid treemap layout that can provide space-efficient overviews of the elements structured by set intersections across multiple sets. These intersecting elements can be connected among sets through visual links. These visual representations for the individual set, elements, and intersection in GridSet facilitate novel interaction approaches for undertaking analysis tasks by utilizing both macroscopic views of sets, as well as microscopic views of elements and attribute details. In order to perform multiple set operations, GridSet supports a simple and straightforward process for set operations through dragging and dropping set objects. Our use cases involving two large set-typed datasets demonstrate that GridSet facilitates the exploration and identification of meaningful patterns and distributions of elements with respect to attributes and set intersections for solving complex analysis problems in set-typed data.},
  archive      = {J_TVCG},
  author       = {Haeyong Chung and Santhosh Nandhakumar and Seungwon Yang},
  doi          = {10.1109/TVCG.2020.3047111},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2983-2998},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GridSet: Visualizing individual elements and attributes for analysis of set-typed data},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feasibility study on virtual reality based basketball tactic
training. <em>TVCG</em>, <em>28</em>(8), 2970–2982. (<a
href="https://doi.org/10.1109/TVCG.2020.3046326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a VR-based basketball training system comprising a standalone VR device and a tablet is proposed. The system is intended to improve the ability of players to understand offensive tactics and practice these tactics correctly. We compare the training effectiveness of various degrees of immersion, including a conventional basketball tactic board, a 2D monitor, and virtual reality. A multi-camera-based human tracking system was designed and built around a real-world basketball court to record and analyze the running trajectory of each player during tactical execution. The accuracy of the running path and hesitation time at each tactical step were evaluated for each participant. Furthermore, we assessed several subjective measurements, including simulator sickness, presence, and sport imagery ability, to conduct a more comprehensive exploration of the feasibility of the proposed VR framework for basketball tactics training. The results indicate that the proposed system is useful for learning complex tactics. Furthermore, high VR immersion training improves athletes’ abilities with regards to strategic imagery.},
  archive      = {J_TVCG},
  author       = {Wan-Lun Tsai and Tse-Yu Pan and Min-Chun Hu},
  doi          = {10.1109/TVCG.2020.3046326},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2970-2982},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Feasibility study on virtual reality based basketball tactic training},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EmbComp: Visual interactive comparison of vector embeddings.
<em>TVCG</em>, <em>28</em>(8), 2953–2969. (<a
href="https://doi.org/10.1109/TVCG.2020.3045918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces embComp , a novel approach for comparing two embeddings that capture the similarity between objects, such as word and document embeddings. We survey scenarios where comparing these embedding spaces is useful. From those scenarios, we derive common tasks, introduce visual analysis methods that support these tasks, and combine them into a comprehensive system. One of embComp ’s central features are overview visualizations that are based on metrics for measuring differences in the local structure around objects. Summarizing these local metrics over the embeddings provides global overviews of similarities and differences. Detail views allow comparison of the local structure around selected objects and relating this local information to the global views. Integrating and connecting all of these components, embComp supports a range of analysis workflows that help understand similarities and differences between embedding spaces. We assess our approach by applying it in several use cases, including understanding corpora differences via word vector embeddings, and understanding algorithmic differences in generating embeddings.},
  archive      = {J_TVCG},
  author       = {Florian Heimerl and Christoph Kralj and Torsten Möller and Michael Gleicher},
  doi          = {10.1109/TVCG.2020.3045918},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2953-2969},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EmbComp: Visual interactive comparison of vector embeddings},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep sketch-guided cartoon video inbetweening.
<em>TVCG</em>, <em>28</em>(8), 2938–2952. (<a
href="https://doi.org/10.1109/TVCG.2021.3049419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel framework to produce cartoon videos by fetching the color information from two input keyframes while following the animated motion guided by a user sketch. The key idea of the proposed approach is to estimate the dense cross-domain correspondence between the sketch and cartoon video frames, and employ a blending module with occlusion estimation to synthesize the middle frame guided by the sketch. After that, the input frames and the synthetic frame equipped with established correspondence are fed into an arbitrary-time frame interpolation pipeline to generate and refine additional inbetween frames. Finally, a module to preserve temporal consistency is employed. Compared to common frame interpolation methods, our approach can address frames with relatively large motion and also has the flexibility to enable users to control the generated video sequences by editing the sketch guidance. By explicitly considering the correspondence between frames and the sketch, we can achieve higher quality results than other image synthesis methods. Our results show that our system generalizes well to different movie frames, achieving better results than existing solutions.},
  archive      = {J_TVCG},
  author       = {Xiaoyu Li and Bo Zhang and Jing Liao and Pedro V. Sander},
  doi          = {10.1109/TVCG.2021.3049419},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2938-2952},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Deep sketch-guided cartoon video inbetweening},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep exemplar-based color transfer for 3D model.
<em>TVCG</em>, <em>28</em>(8), 2926–2937. (<a
href="https://doi.org/10.1109/TVCG.2020.3041487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recoloring 3D models is a challenging task that often requires professional knowledge and tedious manual efforts. In this article, we present the first deep-learning framework for exemplar-based 3D model recolor, which can automatically transfer the colors from a reference image to the 3D model texture. Our framework consists of two modules to solve two major challenges in the 3D color transfer. First, we propose a new feed-forward Color Transfer Network to achieve high-quality semantic-level color transfer by finding dense semantic correspondences between images. Second, considering 3D model constraints such as UV mapping, we design a novel 3D Texture Optimization Module which can generate a seamless and coherent texture by combining color transferred results rendered in multiple views. Experiments show that our method performs robustly and generalizes well to various kinds of models.},
  archive      = {J_TVCG},
  author       = {Mohan Zhang and Jing Liao and Jinhui Yu},
  doi          = {10.1109/TVCG.2020.3041487},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2926-2937},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Deep exemplar-based color transfer for 3D model},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CosmoVis: An interactive visual analysis tool for exploring
hydrodynamic cosmological simulations. <em>TVCG</em>, <em>28</em>(8),
2909–2925. (<a href="https://doi.org/10.1109/TVCG.2022.3159630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce CosmoVis , an open source web-based visualization tool for the interactive analysis of massive hydrodynamic cosmological simulation data. CosmoVis was designed in close collaboration with astrophysicists to enable researchers and citizen scientists to share and explore these datasets, and to use them to investigate a range of scientific questions. CosmoVis visualizes many key gas, dark matter, and stellar attributes extracted from the source simulations, which typically consist of complex data structures multiple terabytes in size, often requiring extensive data wrangling. CosmoVis introduces a range of features to facilitate real-time analysis of these simulations, including the use of “virtual skewers,” simulated analogues of absorption line spectroscopy that act as spectral probes piercing the volume of gaseous cosmic medium. We explain how such synthetic spectra can be used to gain insight into the source datasets and to make functional comparisons with observational data. Furthermore, we identify the main analysis tasks that CosmoVis enables and present implementation details of the software interface and the client-server architecture. We conclude by providing details of three contemporary scientific use cases that were conducted by domain experts using the software and by documenting expert feedback from astrophysicists at different career levels.},
  archive      = {J_TVCG},
  author       = {David Abramov and Joseph N. Burchett and Oskar Elek and Cameron Hummels and J. Xavier Prochaska and Angus G. Forbes},
  doi          = {10.1109/TVCG.2022.3159630},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2909-2925},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CosmoVis: An interactive visual analysis tool for exploring hydrodynamic cosmological simulations},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). C3 assignment: Camera cubemap color assignment for creative
interior design. <em>TVCG</em>, <em>28</em>(8), 2895–2908. (<a
href="https://doi.org/10.1109/TVCG.2020.3041728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color design for 3D indoor scenes is a challenging problem due to many factors that need to be balanced. Although learning from images is a commonly adopted strategy, this strategy may be more suitable for natural scenes in which objects tend to have relatively fixed colors. For interior scenes consisting mostly of man-made objects, creative yet reasonable color assignments are expected. We propose $C^{3}$C3 Assignment , a system providing diverse suggestions for interior color design while satisfying general global and local rules including color compatibility, color mood, contrast, and user preference. We extend these constraints from the image domain to $\mathbb {R}^3$ , and formulate 3D interior color design as an optimization problem. The design is accomplished in an omnidirectional manner to ensure a comfortable experience when the inhabitant observes the interior scene from possible positions and directions. We design a surrogate-assisted evolutionary algorithm to efficiently solve the highly nonlinear optimization problem for interactive applications, and investigate the system performance concerning problem complexity, solver convergence, and suggestion diversity. Preliminary user studies have been conducted to validate the rule extension from 2D to 3D and to verify system usability.},
  archive      = {J_TVCG},
  author       = {Juncong Lin and Pintong Xiao and Yinan Fu and Yubin Shi and Hongran Wang and Shihui Guo and Ying He and Tong-Yee Lee},
  doi          = {10.1109/TVCG.2020.3041728},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2895-2908},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {C3 assignment: Camera cubemap color assignment for creative interior design},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blending surface segmentation and editing for 3D models.
<em>TVCG</em>, <em>28</em>(8), 2879–2894. (<a
href="https://doi.org/10.1109/TVCG.2020.3045450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing and fitting shape primitives from underlying 3D models are key components of many computer graphics and computer vision applications. Although a vast number of structural recovery methods are available, they usually fail to identify blending surfaces, which corresponds to small transitional regions among relatively large primary patches. To address this issue, we present a novel approach for automatic segmentation and surface fitting with accurate geometric parameters from 3D models, especially mechanical parts. Overall, we formulate the structural segmentation as a Markov random field (MRF) labeling problem. In contrast to existing techniques, we first propose a new clustering algorithm to build superfacets by incorporating 3D local geometric information. This algorithm extracts the general quadric and rolling-ball blending regions, and improves the robustness of further segmentation. Next, we apply a specially designed MRF framework to efficiently partition the original model into different meaningful patches of known surface types by defining the multilabel energy function on the superfacets. Furthermore, we present an iterative optimization algorithm based on skeleton extraction to fit rolling-ball blending patches by recovering the parameters of the rolling center trajectories and ball radius. Experiments on different complex models demonstrate the effectiveness and robustness of the proposed method, and the superiority of our method is also verified through comparisons with state-of-the-art approaches. We further apply our algorithm in applications such as mesh editing by changing the radius of the rolling balls.},
  archive      = {J_TVCG},
  author       = {Long Zhang and Jianwei Guo and Jun Xiao and Xiaopeng Zhang and Dong-Ming Yan},
  doi          = {10.1109/TVCG.2020.3045450},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2879-2894},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Blending surface segmentation and editing for 3D models},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Affective congruence in visualization design: Influences on
reading categorical maps. <em>TVCG</em>, <em>28</em>(8), 2867–2878. (<a
href="https://doi.org/10.1109/TVCG.2021.3050118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work in data visualization has demonstrated that small, perceptually-distinct color palettes—such as those used in categorical mapping—can connote significant affective qualities. Data that are mapped or otherwise visualized are also often emotive in nature, either inherently (e.g., climate change, disease mortality rates), or by design, such as can be found in visual storytelling. However, little is known about how the affective qualities of color interact with those of data context in visualization design. This article describes the results of a crowdsourced study on the influence of affectively congruent versus incongruent color schemes on categorical map-reading response. We report both objective (pattern detection; area comparison) and subjective (affective quality; appropriateness; preference) measures of map-reader response. Our results suggest that affectively congruent colors amplify perceptions of the affective qualities of maps with emotive topics, affective incongruence may cause confusion, and that affective congruence is particularly influential in maps of positive-leaning data topics. Finally, we offer preliminary design recommendations for balancing color congruence with other design factors, and for synthesizing color and affective context in thematic map design.},
  archive      = {J_TVCG},
  author       = {Cary L. Anderson and Anthony C. Robinson},
  doi          = {10.1109/TVCG.2021.3050118},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2867-2878},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Affective congruence in visualization design: Influences on reading categorical maps},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerating unstructured mesh point location with RT cores.
<em>TVCG</em>, <em>28</em>(8), 2852–2866. (<a
href="https://doi.org/10.1109/TVCG.2020.3042930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a technique that leverages ray tracing hardware available in recent Nvidia RTX GPUs to solve a problem other than classical ray tracing. Specifically, we demonstrate how to use these units to accelerate the point location of general unstructured elements consisting of both planar and bilinear faces. This unstructured mesh point location problem has previously been challenging to accelerate on GPU architectures; yet, the performance of these queries is crucial to many unstructured volume rendering and compute applications. Starting with a CUDA reference method, we describe and evaluate three approaches that reformulate these point queries to incrementally map algorithmic complexity to these new hardware ray tracing units. Each variant replaces the simpler problem of point queries with a more complex one of ray queries. Initial variants exploit ray tracing cores for accelerated BVH traversal, and subsequent variants use ray-triangle intersections and per-face metadata to detect point-in-element intersections. Although these later variants are more algorithmically complex, they are significantly faster than the reference method thanks to hardware acceleration. Using our approach, we improve the performance of an unstructured volume renderer by up to $4\times$ for tetrahedral meshes and up to $15\times$ for general bilinear element meshes, matching, or out-performing state-of-the-art solutions while simultaneously improving on robustness and ease-of-implementation.},
  archive      = {J_TVCG},
  author       = {Nate Morrical and Ingo Wald and Will Usher and Valerio Pascucci},
  doi          = {10.1109/TVCG.2020.3042930},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2852-2866},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Accelerating unstructured mesh point location with RT cores},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A perceptual color-matching method for examining color
blending in augmented reality head-up display graphics. <em>TVCG</em>,
<em>28</em>(8), 2834–2851. (<a
href="https://doi.org/10.1109/TVCG.2020.3044715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented reality (AR) offers new ways to visualize information on-the-go. As noted in related work, AR graphics presented via optical see-through AR displays are particularly prone to color blending, whereby intended graphic colors may be perceptually altered by real-world backgrounds, ultimately degrading usability. This work adds to this body of knowledge by presenting a methodology for assessing AR interface color robustness , as quantitatively measured via shifts in the CIE color space, and qualitatively assessed in terms of users’ perceived color name. We conducted a human factors study where twelve participants examined eight AR colors atop three real-world backgrounds as viewed through an in-vehicle AR head-up display (HUD); a type of optical see-through display used to project driving-related information atop the forward-looking road scene. Participants completed visual search tasks, matched the perceived AR HUD color against the WCS color palette, and verbally named the perceived color. We present analysis that suggests blue, green, and yellow AR colors are relatively robust, while red and brown are not, and discuss the impact of chromaticity shift and dispersion on outdoor AR interface design. While this work presents a case study in transportation, the methodology is applicable to a wide range of AR displays in many application domains and settings.},
  archive      = {J_TVCG},
  author       = {Joseph L. Gabbard and Missie Smith and Coleman Merenda and Gary Burnett and David R. Large},
  doi          = {10.1109/TVCG.2020.3044715},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2834-2851},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A perceptual color-matching method for examining color blending in augmented reality head-up display graphics},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A local graph-based structure for processing gigantic
aggregated 3D point clouds. <em>TVCG</em>, <em>28</em>(8), 2822–2833.
(<a href="https://doi.org/10.1109/TVCG.2020.3042588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an original workflow for structuring a point cloud generated from several scans. Our representation is based on a set of local graphs. Each graph is constructed from the depth map provided by each scan. The graphs are then connected together via the overlapping areas, and careful consideration of the redundant points in these regions leads to a piecewise and globally consistent structure for the underlying surface sampled by the point cloud. The proposed workflow allows structuring aggregated point clouds, scan after scan, whatever the number of acquisitions and the number of points per acquisition, even on computers with very limited memory capacities. To show that our structure can be highly relevant for the community, where the gigantic amount of data represents a real scientific challenge per se, we present an algorithm based on this structure capable of resampling billions of points on standard computers. This application is particularly attractive for simplifying and visualizing gigantic point clouds representing very large-scale scenes (buildings, urban scenes, historical sites...), which often require a prohibitive number of points to describe them accurately.},
  archive      = {J_TVCG},
  author       = {Arnaud Bletterer and Frédéric Payan and Marc Antonini},
  doi          = {10.1109/TVCG.2020.3042588},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2822-2833},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A local graph-based structure for processing gigantic aggregated 3D point clouds},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3D curve creation on and around physical objects with mobile
AR. <em>TVCG</em>, <em>28</em>(8), 2809–2821. (<a
href="https://doi.org/10.1109/TVCG.2020.3049006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent advance in motion tracking (e.g., Visual Inertial Odometry) allows the use of a mobile phone as a 3D pen, thus significantly benefiting various mobile Augmented Reality (AR) applications based on 3D curve creation. However, when creating 3D curves on and around physical objects with mobile AR, tracking might be less robust or even lost due to camera occlusion or textureless scenes. This motivates us to study how to achieve natural interaction with minimum tracking errors during close interaction between a mobile phone and physical objects. To this end, we contribute an elicitation study on input point and phone grip, and a quantitative study on tracking errors. Based on the results, we present a system for direct 3D drawing with an AR-enabled mobile phone as a 3D pen, and interactive correction of 3D curves with tracking errors in mobile AR. We demonstrate the usefulness and effectiveness of our system for two applications: in-situ 3D drawing, and direct 3D measurement.},
  archive      = {J_TVCG},
  author       = {Hui Ye and Kin Chung Kwan and Hongbo Fu},
  doi          = {10.1109/TVCG.2020.3049006},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2809-2821},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {3D curve creation on and around physical objects with mobile AR},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Errata to “understanding multimodal user gesture and speech
behavior for object manipulation in augmented reality using
elicitation.” <em>TVCG</em>, <em>28</em>(7), 2808. (<a
href="https://doi.org/10.1109/TVCG.2022.3164438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {• All time information was reported as milliseconds but given as 100ths of a second.• Additionally, the time differences between gesture initiation and speech initiation were reported as being 60\% of their actual value. This resulted in those time differences being reported as 6 times less than they were (e.g., 130ms should be 780ms).},
  archive      = {J_TVCG},
  author       = {Adam S. Williams and Jason Garcia and Francisco Ortega},
  doi          = {10.1109/TVCG.2022.3164438},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2808},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Errata to “Understanding multimodal user gesture and speech behavior for object manipulation in augmented reality using elicitation”},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VisExPreS: A visual interactive toolkit for user-driven
evaluations of embeddings. <em>TVCG</em>, <em>28</em>(7), 2791–2807. (<a
href="https://doi.org/10.1109/TVCG.2020.3039106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although popularly used in big-data analytics, dimensionality reduction is a complex, black-box technique whose outcome is difficult to interpret and evaluate. In recent years, a number of quantitative and visual methods have been proposed for analyzing low-dimensional embeddings. On the one hand, quantitative methods associate numeric identifiers to qualitative characteristics of these embeddings; and, on the other hand, visual techniques allow users to interactively explore these embeddings and make decisions. However, in the former case, users do not have control over the analysis, while in the latter case. assessment decisions are entirely dependent on the user&#39;s perception and expertise. In order to bridge the gap between the two, in this article, we present VisExPreS, a visual interactive toolkit that enables a user-driven assessment of low-dimensional embeddings. VisExPreS is based on three novel techniques namely PG-LAPS, PG-GAPS, and RepSubset, that generate interpretable explanations of the preserved local and global structures in embeddings. In the first two techniques, the VisExPreS system proactively guides users during every step of the analysis. We demonstrate the utility of VisExPreS in interpreting, analyzing, and evaluating embeddings from different dimensionality reduction algorithms using multiple case studies and an extensive user study.},
  archive      = {J_TVCG},
  author       = {Aindrila Ghosh and Mona Nashaat and James Miller and Shaikh Quader},
  doi          = {10.1109/TVCG.2020.3039106},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2791-2807},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VisExPreS: A visual interactive toolkit for user-driven evaluations of embeddings},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Umbra: A visual analysis approach for defense construction
against inference attacks on sensitive information. <em>TVCG</em>,
<em>28</em>(7), 2776–2790. (<a
href="https://doi.org/10.1109/TVCG.2020.3037670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collecting and analyzing anonymous personal information is required as a part of data analysis processes, such as medical diagnosis and restaurant recommendation. Such data should ostensibly be stored so that specific individual information cannot be disclosed. Unfortunately, inference attacks—integrating background knowledge and intelligent models—hinder classic sanitization techniques like syntactic anonymity and differential privacy from exhaustively protecting sensitive information. As a solution, we introduce a three-stage approach empowered within a visual interface, which depicts underlying inference behaviors via a Bayesian Network and supports a customized defense against inference attacks from unknown adversaries. In particular, our approach visually explains the process details of the underlying privacy preserving models, allowing users to verify if the results sufficiently satisfy the requirements of privacy preservation. We demonstrate the effectiveness of our approach through two case studies and expert reviews.},
  archive      = {J_TVCG},
  author       = {Xumeng Wang and Chris Bryan and Yiran Li and Rusheng Pan and Yanling Liu and Wei Chen and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2020.3037670},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2776-2790},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Umbra: A visual analysis approach for defense construction against inference attacks on sensitive information},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tanglement resolution in clothing simulation with explicit
convergence. <em>TVCG</em>, <em>28</em>(7), 2764–2775. (<a
href="https://doi.org/10.1109/TVCG.2020.3039566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new discrete collision handling method (DCH method) that can be used for resolving tanglements in clothing simulation, based on the existing continuous collision handling methods (CCH methods). The proposed method performs intersection analysis of the clothing mesh at every time step, and stores the result in the form of coloring the vertices, edges, and triangles. Referring to the coloring, the method resolves the tanglements in the out-to-in manner by applying the proposed operations, the triangle shrinkage and vertex pull, to the triangles and vertices around the intersection path. We take note of the CCH methods that use some small tolerance value to defend the round-off errors for the purpose of preventing false negatives. This work gives a second thought to that tolerance value, and proposes a new DCH method which uses the tolerance value for the resolution purpose. Under certain conditions, the method turns out to guarantee resolution of the tanglements in a finite number of time steps.},
  archive      = {J_TVCG},
  author       = {Ick-Hoon Cha and Hyeong-Seok Ko},
  doi          = {10.1109/TVCG.2020.3039566},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2764-2775},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Tanglement resolution in clothing simulation with explicit convergence},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time shadow detection from live outdoor videos for
augmented reality. <em>TVCG</em>, <em>28</em>(7), 2748–2763. (<a
href="https://doi.org/10.1109/TVCG.2020.3041100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulating shadow interactions between real and virtual objects is important for augmented reality (AR), in which accurately and efficiently detecting real shadows from live videos is a crucial step. Most of the existing methods are capable of processing only scenes captured under a fixed viewpoint. In contrast, this article proposes a new framework for shadow detection in live outdoor videos captured under moving viewpoints. The framework splits each frame into a tracked region, which is the region tracked from the previous video frame through optical flow analysis, and an emerging region, which is newly introduced into the scene due to the moving viewpoint. The framework subsequently extracts features based on the intensity profiles surrounding the boundaries of candidate shadow regions. These features are then utilized to both correct erroneous shadow boundaries for the tracked region and to detect shadow boundaries for the emerging region by a Bayesian learning module. To remove spurious shadows, spatial layout constraints are further considered for emerging regions. The experimental results demonstrate that the proposed framework outperforms the state-of-the-art shadow tracking and detection algorithms on a variety of challenging cases in real time, including shadows on backgrounds with complex textures, nonplanar shadows, fast-moving shadows with changing typologies, and shadows cast by nonrigid objects. The quantitative experiments show that our method outperforms the best existing method, achieving a 33.3\% increase in the average $F_{measure}$ on a self-collected database. Coupled with an image-based shadow-casting method, the proposed framework generates realistic shadow interaction results. This capability will be particularly beneficial for supporting AR applications.},
  archive      = {J_TVCG},
  author       = {Yanli Liu and Xingming Zou and Songhua Xu and Guanyu Xing and Housheng Wei and Yanci Zhang},
  doi          = {10.1109/TVCG.2020.3041100},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2748-2763},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Real-time shadow detection from live outdoor videos for augmented reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time denoising of volumetric path tracing for direct
volume rendering. <em>TVCG</em>, <em>28</em>(7), 2734–2747. (<a
href="https://doi.org/10.1109/TVCG.2020.3037680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct volume rendering (DVR) using volumetric path tracing (VPT) is a scientific visualization technique that simulates light transport with objects’ matter using physically-based lighting models. Monte Carlo (MC) path tracing is often used with surface models, yet its application for volumetric models is difficult due to the complexity of integrating MC light-paths in volumetric media with none or smooth material boundaries. Moreover, auxiliary geometry-buffers (G-buffers) produced for volumes are typically very noisy, failing to guide image denoisers relying on that information to preserve image details. This makes existing real-time denoisers, which take noise-free G-buffers as their input, less effective when denoising VPT images. We propose the necessary modifications to an image-based denoiser previously used when rendering surface models, and demonstrate effective denoising of VPT images. In particular, our denoising exploits temporal coherence between frames, without relying on noise-free G-buffers, which has been a common assumption of existing denoisers for surface-models. Our technique preserves high-frequency details through a weighted recursive least squares that handles heterogeneous noise for volumetric models. We show for various real data sets that our method improves the visual fidelity and temporal stability of VPT during classic DVR operations such as camera movements, modifications of the light sources, and editions to the volume transfer function.},
  archive      = {J_TVCG},
  author       = {Jose A. Iglesias-Guitian and Prajita Mane and Bochang Moon},
  doi          = {10.1109/TVCG.2020.3037680},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2734-2747},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Real-time denoising of volumetric path tracing for direct volume rendering},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rainbow dash: Intuitiveness, interpretability and
memorability of the rainbow color scheme in visualization.
<em>TVCG</em>, <em>28</em>(7), 2722–2733. (<a
href="https://doi.org/10.1109/TVCG.2020.3035823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {After demonstrating that rainbow colors are still commonly used in scientific publications, we comparatively evaluate the rainbow and sequential color schemes on choropleth and isarithmic maps in an empirical user study with 544 participants to examine if a) people intuitively associate order for the colors in these schemes, b) they can successfully conduct perceptual and semantic map reading and recall tasks with quantitative data where order may have implicit or explicit importance. We find that there is little to no agreement in ordering of rainbow colors while sequential colors are indeed intuitively ordered by the participants with a strong dark is more bias . Sequential colors facilitate most quantitative map reading tasks better than the rainbow colors, whereas rainbow colors competitively facilitate extracting specific values from a map, and may support hue recall better than sequential. We thus contribute to dark- versus light is more bias debate, demonstrate why and when rainbow colors may impair performance, and add further nuance to our understanding of this highly popular, yet highly criticized color scheme.},
  archive      = {J_TVCG},
  author       = {Izabela M. Gołbiowska and Arzu Çöltekin},
  doi          = {10.1109/TVCG.2020.3035823},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2722-2733},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Rainbow dash: Intuitiveness, interpretability and memorability of the rainbow color scheme in visualization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Patch textures: Hardware support for mesh colors.
<em>TVCG</em>, <em>28</em>(7), 2710–2721. (<a
href="https://doi.org/10.1109/TVCG.2020.3039777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mesh Colors provide an effective alternative to standard texture mapping. They significantly simplify the asset production pipeline by removing the need for defining a mapping and eliminate rendering artifacts due to seams. This article addresses the problem that using Mesh Colors for real-time rendering has not been practical, due to the absence of hardware support. We show that it is possible to provide full hardware texture filtering support for Mesh Colors with minimal changes to existing GPUs by introducing a hardware-friendly representation for Mesh Colors that we call Patch Textures , which can have quadrilateral or triangular topology. We discuss the hardware modifications needed for storing and filtering Patch Textures, including anisotropic filtering. This article extends our previous work by discussing and comparing patch edge-handling approaches, including an option for sampling the textures of neighboring patches using an adjacency map. We also provide extensive discussions regarding data duplication, a partial implementation present in existing hardware, and the difficulties with providing a similar hardware support for Ptex.},
  archive      = {J_TVCG},
  author       = {Ian Mallett and Larry Seiler and Cem Yuksel},
  doi          = {10.1109/TVCG.2020.3039777},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2710-2721},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Patch textures: Hardware support for mesh colors},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Output-sensitive avatar representations for immersive
telepresence. <em>TVCG</em>, <em>28</em>(7), 2697–2709. (<a
href="https://doi.org/10.1109/TVCG.2020.3037360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a system design and implementation for output-sensitive reconstruction, transmission and rendering of 3D video avatars in distributed virtual environments. In our immersive telepresence system, users are captured by multiple RGBD sensors connected to a server that performs geometry reconstruction based on viewing feedback from remote telepresence parties. This feedback and reconstruction loop enables visibility-aware level-of-detail reconstruction of video avatars regarding geometry and texture data, and considers individual and groups of collocated users. Our evaluation reveals that our approach leads to a significant reduction of reconstruction times, network bandwidth requirements and round-trip times as well as rendering costs in many situations.},
  archive      = {J_TVCG},
  author       = {Adrian Kreskowski and Stephan Beck and Bernd Froehlich},
  doi          = {10.1109/TVCG.2020.3037360},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2697-2709},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Output-sensitive avatar representations for immersive telepresence},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-level area balancing of clustered graphs.
<em>TVCG</em>, <em>28</em>(7), 2682–2696. (<a
href="https://doi.org/10.1109/TVCG.2020.3038154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a multi-level area balancing technique for laying out clustered graphs to facilitate a comprehensive understanding of the complex relationships that exist in various fields, such as life sciences and sociology. Clustered graphs are often used to model relationships that are accompanied by attribute-based grouping information. Such information is essential for robust data analysis, such as for the study of biological taxonomies or educational backgrounds. Hence, the ability to smartly arrange textual labels and packing graphs within a certain screen space is therefore desired to successfully convey the attribute data . Here we propose to hierarchically partition the input screen space using Voronoi tessellations in multiple levels of detail. In our method, the position of textual labels is guided by the blending of constrained forces and the forces derived from centroidal Voronoi cells. The proposed algorithm considers three main factors: (1) area balancing, (2) schematized space partitioning, and (3) hairball management. We primarily focus on area balancing, which aims to allocate a uniform area for each textual label in the diagram. We achieve this by first untangling a general graph to a clustered graph through textual label duplication, and then coupling with spanning-tree-like visual integration. We illustrate the feasibility of our approach with examples and then evaluate our method by comparing it with well-known conventional approaches and collecting feedback from domain experts.},
  archive      = {J_TVCG},
  author       = {Hsiang-Yun Wu and Martin Nöllenburg and Ivan Viola},
  doi          = {10.1109/TVCG.2020.3038154},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2682-2696},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-level area balancing of clustered graphs},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ManhattanFusion: Online dense reconstruction of indoor
scenes from depth sequences. <em>TVCG</em>, <em>28</em>(7), 2668–2681.
(<a href="https://doi.org/10.1109/TVCG.2020.3036868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new framework for online dense 3D reconstruction of indoor scenes by using only depth sequences. This research is particularly useful in cases with a poor light condition or in a nearly featureless indoor environment. The lack of RGB information makes long-range camera pose estimation difficult in a large indoor environment. The key idea of our research is to take advantage of the geometric prior of Manhattan scenes in each stage of the reconstruction pipeline with the specific aim to reduce the cumulative registration error and overall odometry drift in a long sequence. This idea is further boosted by local Manhattan frame growing and the local-to-global strategy that leads to implicit loop closure handling for a large indoor scene. Our proposed pipeline, namely ManhattanFusion, starts with planar alignment and local pose optimization where the Manhattan constraints are imposed to create detailed local segments. These segments preserve intrinsic scene geometry by minimizing the odometry drift even under complex and long trajectories. The final model is generated by integrating all local segments into a global volumetric representation under the constraint of Manhattan frame-based registration across segments. Our algorithm outperforms others that use depth data only in terms of both the mean distance error and the absolute trajectory error, and it is also very competitive compared with RGB-D based reconstruction algorithms. Moreover, our algorithm outperforms the state-of-the-art in terms of the surface area coverage by 10-40 percent, largely due to the usefulness and effectiveness of the Manhattan assumption through the reconstruction pipeline.},
  archive      = {J_TVCG},
  author       = {Mahdi Yazdanpour and Guoliang Fan and Weihua Sheng},
  doi          = {10.1109/TVCG.2020.3036868},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2668-2681},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ManhattanFusion: Online dense reconstruction of indoor scenes from depth sequences},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning adaptive sampling and reconstruction for volume
visualization. <em>TVCG</em>, <em>28</em>(7), 2654–2667. (<a
href="https://doi.org/10.1109/TVCG.2020.3039340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A central challenge in data visualization is to understand which data samples are required to generate an image of a data set in which the relevant information is encoded. In this article, we make a first step towards answering the question of whether an artificial neural network can predict where to sample the data with higher or lower density, by learning of correspondences between the data, the sampling patterns and the generated images. We introduce a novel neural rendering pipeline, which is trained end-to-end to generate a sparse adaptive sampling structure from a given low-resolution input image, and reconstructs a high-resolution image from the sparse set of samples. For the first time, to the best of our knowledge, we demonstrate that the selection of structures that are relevant for the final visual representation can be jointly learned together with the reconstruction of this representation from these structures. Therefore, we introduce differentiable sampling and reconstruction stages, which can leverage back-propagation based on supervised losses solely on the final image. We shed light on the adaptive sampling patterns generated by the network pipeline and analyze its use for volume visualization including isosurface and direct volume rendering.},
  archive      = {J_TVCG},
  author       = {Sebastian Weiss and Mustafa IşIk and Justus Thies and Rüdiger Westermann},
  doi          = {10.1109/TVCG.2020.3039340},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2654-2667},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Learning adaptive sampling and reconstruction for volume visualization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploratory volumetric deep earth visualization by 2.5D
interactive compositing. <em>TVCG</em>, <em>28</em>(7), 2641–2653. (<a
href="https://doi.org/10.1109/TVCG.2020.3037226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this contribution we consider the visualization of global, deep Earth volume datasets for display and researcher interaction. While the algorithms and data analysis techniques that produce such volumetric results have become more sophisticated, the manner of visualizing these findings can be improved. We address the challenge of making an illustrative, exploratory visualization of a global geoscience dataset using a combined seismic tomography result, the primary means by which geoscientists infer structure and process in the deep Earth. We present a novel, interactive graphical application suite and associated workflow that uses an intuitive 2.5D layer compositing approach. This allows the user to adjust the separation between data-slices, control graphics variables such as color mapping, opacity and compositing, and facilitate exploration and annotation of the architecture of the lithosphere. Graphics outputs from our applications are enabled for immersive systems such as dome displays. In a case study we visualize the deep Earth structure beneath the Indian Ocean region. We anticipate that the application methodology will find use in the visualization of multiple datasets representing aspects of the Earth&#39;s deep interior and atmosphere, and in the interaction with the increasing number of rich datasets from missions to our neighboring planets.},
  archive      = {J_TVCG},
  author       = {Peter E. Morse and Anya M. Reading and Tobias Stål},
  doi          = {10.1109/TVCG.2020.3037226},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2641-2653},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploratory volumetric deep earth visualization by 2.5D interactive compositing},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhancing static charts with data-driven animations.
<em>TVCG</em>, <em>28</em>(7), 2628–2640. (<a
href="https://doi.org/10.1109/TVCG.2020.3037300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Static visual attributes such as color and shape are used with great success in visual charts designed to be displayed in static, hard-copy form. However, nowadays digital displays become ubiquitous in the visualization of any form of data, lifting the confines of static presentations. In this article, we propose incorporating data-driven animations to bring static charts to life, with the purpose of encoding and emphasizing certain attributes of the data. We lay out a design space for data-driven animated effects and experiment with three versatile effects, marching ants , geometry deformation and gradual appearance . For each, we provide practical details regarding their mode of operation and extent of interaction with existing visual encodings. We examine the impact and effectiveness of our enhancements through an empirical user study to assess preference as well as gauge the influence of animated effects on human perception in terms of speed and accuracy of visual understanding.},
  archive      = {J_TVCG},
  author       = {Min Lu and Noa Fish and Shuaiqi Wang and Joel Lanir and Daniel Cohen-Or and Hui Huang},
  doi          = {10.1109/TVCG.2020.3037300},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2628-2640},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Enhancing static charts with data-driven animations},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient representation and optimization for TPMS-based
porous structures. <em>TVCG</em>, <em>28</em>(7), 2615–2627. (<a
href="https://doi.org/10.1109/TVCG.2020.3037697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this approach, we present an efficient topology and geometry optimization of triply periodic minimal surfaces (TPMS) based porous shell structures, which can be represented, analyzed, optimized and stored directly using functions. The proposed framework is directly executed on functions instead of remeshing (tetrahedral/hexahedral), and this framework substantially improves the controllability and efficiency. Specifically, a valid TPMS-based porous shell structure is first constructed by function expressions. The porous shell permits continuous and smooth changes of geometry (shell thickness) and topology (porous period). The porous structures also inherit several of the advantageous properties of TPMS, such as smoothness, full connectivity (no closed hollows), and high controllability. Then, the problem of filling an object’s interior region with porous shell can be formulated into a constraint optimization problem with two control parameter functions. Finally, an efficient topology and geometry optimization scheme is presented to obtain optimized scale-varying porous shell structures. In contrast to traditional heuristic methods for TPMS, our work directly optimize both the topology and geometry of TPMS-based structures. Various experiments have shown that our proposed porous structures have obvious advantages in terms of efficiency and effectiveness.},
  archive      = {J_TVCG},
  author       = {Jiangbei Hu and Shengfa Wang and Baojun Li and Fengqi Li and Zhongxuan Luo and Ligang Liu},
  doi          = {10.1109/TVCG.2020.3037697},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2615-2627},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Efficient representation and optimization for TPMS-based porous structures},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Direct transmittance estimation in heterogeneous
participating media using approximated taylor expansions. <em>TVCG</em>,
<em>28</em>(7), 2602–2614. (<a
href="https://doi.org/10.1109/TVCG.2020.3035516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating the transmittance between two points along a ray is a key component in solving the light transport through heterogeneous participating media and entails computing an intractable exponential of the integrated medium&#39;s extinction coefficient. While algorithms for estimating this transmittance exist, there is a lack of theoretical knowledge about their behaviour, which also prevent new theoretically sound algorithms from being developed. For this purpose, we introduce a new class of unbiased transmittance estimators based on random sampling or truncation of a Taylor expansion of the exponential function. In contrast to classical tracking algorithms, these estimators are non-analogous to the physical light transport process and directly sample the underlying extinction function without performing incremental advancement. We present several versions of the new class of estimators, based on either importance sampling or Russian roulette to provide finite unbiased estimators of the infinite Taylor series expansion. We also show that the well known ratio tracking algorithm can be seen as a special case of the new class of estimators. Lastly, we conduct performance evaluations on both the central processing unit (CPU) and the graphics processing unit (GPU), and the results demonstrate that the new algorithms outperform traditional algorithms for heterogeneous mediums.},
  archive      = {J_TVCG},
  author       = {Daniel Jönsson and Joel Kronander and Jonas Unger and Thomas B. Schön and Magnus Wrenninge},
  doi          = {10.1109/TVCG.2020.3035516},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2602-2614},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Direct transmittance estimation in heterogeneous participating media using approximated taylor expansions},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Crowd navigation in VR: Exploring haptic rendering of
collisions. <em>TVCG</em>, <em>28</em>(7), 2589–2601. (<a
href="https://doi.org/10.1109/TVCG.2020.3041341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) is a valuable experimental tool for studying human movement, including the analysis of interactions during locomotion tasks for developing crowd simulation algorithms. However, these studies are generally limited to distant interactions in crowds, due to the difficulty of rendering realistic sensations of collisions in VR. In this article, we explore the use of wearable haptics to render contacts during virtual crowd navigation. We focus on the behavioral changes occurring with or without haptic rendering during a navigation task in a dense crowd, as well as on potential after-effects introduced by the use haptic rendering. Our objective is to provide recommendations for designing VR setup to study crowd navigation behavior. To the end, we designed an experiment (N=23) where participants navigated in a crowded virtual train station without, then with, and then again without haptic feedback of their collisions with virtual characters. Results show that providing haptic feedback improved the overall realism of the interaction, as participants more actively avoided collisions. We also noticed a significant after-effect in the users’ behavior when haptic rendering was once again disabled in the third part of the experiment. Nonetheless, haptic feedback did not have any significant impact on the users’ sense of presence and embodiment.},
  archive      = {J_TVCG},
  author       = {Florian Berton and Fabien Grzeskowiak and Alexandre Bonneau and Alberto Jovane and Marco Aggravi and Ludovic Hoyet and Anne-Hélène Olivier and Claudio Pacchierotti and Julien Pettré},
  doi          = {10.1109/TVCG.2020.3041341},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2589-2601},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Crowd navigation in VR: Exploring haptic rendering of collisions},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computational design of self-actuated deformable solids via
shape memory material. <em>TVCG</em>, <em>28</em>(7), 2577–2588. (<a
href="https://doi.org/10.1109/TVCG.2020.3039613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging 4D printing techniques open new horizons for fabricating self-actuated deformable objects by combing strength of 3D printing and stimuli-responsive shape memory materials. This article focuses on designing self-actuated deformable solids for 4D printing such that a solid can be programmed into a temporary shape and later recovers to its original shape after heating. To avoid a high material cost, we choose a dual-material strategy that mixes an expensive thermo-responsive shape memory polymer (SMP) material with a common elastic material, which however leads to undesired deformation at the shape programming stage. We model this shape programming process as two elastic models with different parameters linked by a median shape based on customizing a constitutive model of thermo-responsive SMPs. Taking this material modeling as a foundation, we formulate our design problem as a nonconvex optimization to find the distribution of SMP materials over the whole object as well as the median shape, and develop an efficient and parallelizable method to solve it. We show that our proposed approach is able to design self-actuated deformable objects that cannot be achieved by state of the art approaches, and demonstrate their usefulness with three example applications.},
  archive      = {J_TVCG},
  author       = {Yucheng Sun and Wenqing Ouyang and Zhongyuan Liu and Ning Ni and Yann Savoye and Peng Song and Ligang Liu},
  doi          = {10.1109/TVCG.2020.3039613},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2577-2588},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Computational design of self-actuated deformable solids via shape memory material},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Augmenting parallel coordinates plots with color-coded
stacked histograms. <em>TVCG</em>, <em>28</em>(7), 2563–2576. (<a
href="https://doi.org/10.1109/TVCG.2020.3038446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Parallel Histogram Plot (PHP), a technique that overcomes the innate limitations of parallel coordinates plot (PCP) by attaching stacked-bar histograms with discrete color schemes to PCP. The color-coded histograms enable users to see an overview of the whole data without cluttering or scalability issues. Each rectangle in the PHP histograms is color coded according to the data ranking by a selected attribute. This color-coding scheme allows users to visually examine relationships between attributes, even between those that are displayed far apart, without repositioning or reordering axes. We adopt the Visual Information Seeking Mantra so that the polylines of the original PCP can be used to show details of a small number of selected items when the cluttering problem subsides. We also design interactions, such as a focus+context technique, to help users investigate small regions of interest in a space-efficient manner. We provide a real-world example in which PHP is effectively utilized compared with other visualizations, and we perform a controlled user study to evaluate the performance of PHP in helping users estimate the correlation between attributes. The results demonstrate that the performance of PHP was consistent in the estimation of correlations between two attributes regardless of the distance between them.},
  archive      = {J_TVCG},
  author       = {Jinwook Bok and Bohyoung Kim and Jinwook Seo},
  doi          = {10.1109/TVCG.2020.3038446},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2563-2576},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Augmenting parallel coordinates plots with color-coded stacked histograms},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AR-loupe: Magnified augmented reality by combining an
optical see-through head-mounted display and a loupe. <em>TVCG</em>,
<em>28</em>(7), 2550–2562. (<a
href="https://doi.org/10.1109/TVCG.2020.3037284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head-mounted loupes can increase the user’s visual acuity to observe the details of an object. On the other hand, optical see-through head-mounted displays (OST-HMD) are able to provide virtual augmentations registered with real objects. In this article, we propose AR-Loupe, combining the advantages of loupes and OST-HMDs, to offer augmented reality in the user’s magnified field-of-vision. Specifically, AR-Loupe integrates a commercial OST-HMD, Magic Leap One, and binocular Galilean magnifying loupes, with customized 3D-printed attachments. We model the combination of user’s eye, screen of OST-HMD, and the optical loupe as a pinhole camera. The calibration of AR-Loupe involves interactive view segmentation and an adapted version of stereo single point active alignment method (Stereo-SPAAM). We conducted a two-phase multi-user study to evaluate AR-Loupe. The users were able to achieve sub-millimeter accuracy ( $0.82\;\mathrm{mm}$ ) on average, which is significantly ( $p &amp;lt; 0.001$ ) smaller compared to normal AR guidance ( $1.49\;\mathrm{mm}$ ). The mean calibration time was $268.46\;s$ . With the increased size of real objects through optical magnification and the registered augmentation, AR-Loupe can aid users in high-precision tasks with better visual acuity and higher accuracy.},
  archive      = {J_TVCG},
  author       = {Long Qian and Tianyu Song and Mathias Unberath and Peter Kazanzides},
  doi          = {10.1109/TVCG.2020.3037284},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2550-2562},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {AR-loupe: Magnified augmented reality by combining an optical see-through head-mounted display and a loupe},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collaborative work in augmented reality: A survey.
<em>TVCG</em>, <em>28</em>(6), 2530–2549. (<a
href="https://doi.org/10.1109/TVCG.2020.3032761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Augmented Reality (AR), users perceive virtual content anchored in the real world. It is used in medicine, education, games, navigation, maintenance, product design, and visualization, in both single-user and multi-user scenarios. Multi-user AR has received limited attention from researchers, even though AR has been in development for more than two decades. We present the state of existing work at the intersection of AR and Computer-Supported Collaborative Work (AR-CSCW), by combining a systematic survey approach with an exploratory, opportunistic literature search. We categorize 65 papers along the dimensions of space, time, role symmetry (whether the roles of users are symmetric), technology symmetry (whether the hardware platforms of users are symmetric), and output and input modalities. We derive design considerations for collaborative AR environments, and identify under-explored research topics. These include the use of heterogeneous hardware considerations and 3D data exploration research areas. This survey is useful for newcomers to the field, readers interested in an overview of CSCW in AR applications, and domain experts seeking up-to-date information.},
  archive      = {J_TVCG},
  author       = {Mickael Sereno and Xiyao Wang and Lonni Besançon and Michael J. McGuffin and Tobias Isenberg},
  doi          = {10.1109/TVCG.2020.3032761},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2530-2549},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Collaborative work in augmented reality: A survey},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). WYSIWYG design of hypnotic line art. <em>TVCG</em>,
<em>28</em>(6), 2517–2529. (<a
href="https://doi.org/10.1109/TVCG.2020.3032734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypnotic line art is a modern form in which white narrow curved ribbons, with the width and direction varying along each path over a black background, provide a keen sense of 3D objects regarding surface shapes and topological contours. However, the procedure of manually creating such line art work can be quite tedious and time-consuming. In this article, we present an interactive system that offers a What-You-See-Is-What-You-Get (WYSIWYG) scheme for producing hypnotic line art images by integrating and placing evenly-spaced streamlines in tensor fields. With an input picture segmented, the user just needs to sketch a few illustrative strokes to guide the construction of a tensor field for each part of the objects therein. Specifically, we propose a new method which controls, with great precision, the aesthetic layout and artistic drawing of an array of streamlines in each tensor field to emulate the style of hypnotic line art. Given several parameters for streamlines such as density, thickness, and sharpness, our system is capable of generating professional-level hypnotic line art work. With great ease of use, it allows art designers to explore a wide variety of possibilities to obtain hypnotic line art results of their own preferences.},
  archive      = {J_TVCG},
  author       = {Chih-Kuo Yeh and Zhanping Liu and I-Hsuan Lin and Eugene Zhang and Tong-Yee Lee},
  doi          = {10.1109/TVCG.2020.3032734},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2517-2529},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {WYSIWYG design of hypnotic line art},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visualizing graph neural networks with CorGIE: Corresponding
a graph to its embedding. <em>TVCG</em>, <em>28</em>(6), 2500–2516. (<a
href="https://doi.org/10.1109/TVCG.2022.3148197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are a class of powerful machine learning tools that model node relations for making predictions of nodes or links. GNN developers rely on quantitative metrics of the predictions to evaluate a GNN, but similar to many other neural networks, it is difficult for them to understand if the GNN truly learns characteristics of a graph as expected. We propose an approach to corresponding an input graph to its node embedding (aka latent space), a common component of GNNs that is later used for prediction. We abstract the data and tasks, and develop an interactive multi-view interface called CorGIE to instantiate the abstraction. As the key function in CorGIE, we propose the K-hop graph layout to show topological neighbors in hops and their clustering structure. To evaluate the functionality and usability of CorGIE, we present how to use CorGIE in two usage scenarios, and conduct a case study with five GNN experts. Availability: Open-source code at https://github.com/zipengliu/corgie-ui/ , supplemental materials &amp; video at https://osf.io/tr3sb/ .},
  archive      = {J_TVCG},
  author       = {Zipeng Liu and Yang Wang and Jürgen Bernard and Tamara Munzner},
  doi          = {10.1109/TVCG.2022.3148197},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2500-2516},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualizing graph neural networks with CorGIE: Corresponding a graph to its embedding},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual cascade analytics of large-scale spatiotemporal data.
<em>TVCG</em>, <em>28</em>(6), 2486–2499. (<a
href="https://doi.org/10.1109/TVCG.2021.3071387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many spatiotemporal events can be viewed as contagions. These events implicitly propagate across space and time by following cascading patterns, expanding their influence, and generating event cascades that involve multiple locations. Analyzing such cascading processes presents valuable implications in various urban applications, such as traffic planning and pollution diagnostics. Motivated by the limited capability of the existing approaches in mining and interpreting cascading patterns, we propose a visual analytics system called VisCas. VisCas combines an inference model with interactive visualizations and empowers analysts to infer and interpret the latent cascading patterns in the spatiotemporal context. To develop VisCas, we address three major challenges 1) generalized pattern inference; 2) implicit influence visualization; and 3) multifaceted cascade analysis. For the first challenge, we adapt the state-of-the-art cascading network inference technique to general urban scenarios, where cascading patterns can be reliably inferred from large-scale spatiotemporal data. For the second and third challenges, we assemble a set of effective visualizations to support location navigation, influence inspection, and cascading exploration, and facilitate the in-depth cascade analysis. We design a novel influence view based on a three-fold optimization strategy for analyzing the implicit influences of the inferred patterns. We demonstrate the capability and effectiveness of VisCas with two case studies conducted on real-world traffic congestion and air pollution datasets with domain experts.},
  archive      = {J_TVCG},
  author       = {Zikun Deng and Di Weng and Yuxuan Liang and Jie Bao and Yu Zheng and Tobias Schreck and Mingliang Xu and Yingcai Wu},
  doi          = {10.1109/TVCG.2021.3071387},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2486-2499},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual cascade analytics of large-scale spatiotemporal data},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). View composition algebra for ad hoc comparison.
<em>TVCG</em>, <em>28</em>(6), 2470–2485. (<a
href="https://doi.org/10.1109/TVCG.2022.3152515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparison is a core task in visual analysis. Although there are numerous guidelines to help users design effective visualizations to aid known comparison tasks, there are few techniques available when users want to make ad hoc comparisons between marks, trends, or charts during data exploration and visual analysis. For instance, to compare voting count maps from different years, two stock trends in a line chart, or a scatterplot of country GDPs with a textual summary of the average GDP. Ideally, users can directly select the comparison targets and compare them, however what elements of a visualization should be candidate targets, which combinations of targets are safe to compare, and what comparison operations make sense? This article proposes a conceptual model that lets users compose combinations of values, marks, legend elements, and charts using a set of composition operators that summarize, compute differences, merge, and model their operands. We further define a View Composition Algebra ( VCA ) that is compatible with datacube-based visualizations, derive an interaction design based on this algebra that supports ad hoc visual comparisons, and illustrate its utility through several use cases.},
  archive      = {J_TVCG},
  author       = {Eugene Wu},
  doi          = {10.1109/TVCG.2022.3152515},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2470-2485},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {View composition algebra for ad hoc comparison},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding missing links in bipartite networks with
MissBiN. <em>TVCG</em>, <em>28</em>(6), 2457–2469. (<a
href="https://doi.org/10.1109/TVCG.2020.3032984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of bipartite networks is critical in a variety of application domains, such as exploring entity co-occurrences in intelligence analysis and investigating gene expression in bio-informatics. One important task is missing link prediction, which infers the existence of unseen links based on currently observed ones. In this article, we propose a visual analysis system, MissBiN, to involve analysts in the loop for making sense of link prediction results. MissBiN equips a novel method for link prediction in a bipartite network by leveraging the information of bi-cliques in the network. It also provides an interactive visualization for understanding the algorithm outputs. The design of MissBiN is based on three high-level analysis questions (what, why, and how) regarding missing links, which are distilled from the literature and expert interviews. We conducted quantitative experiments to assess the performance of the proposed link prediction algorithm, and interviewed two experts from different domains to demonstrate the effectiveness of MissBiN as a whole. We also provide a comprehensive usage scenario to illustrate the usefulness of the tool in an application of intelligence analysis.},
  archive      = {J_TVCG},
  author       = {Jian Zhao and Maoyuan Sun and Francine Chen and Patrick Chiu},
  doi          = {10.1109/TVCG.2020.3032984},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2457-2469},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Understanding missing links in bipartite networks with MissBiN},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SSR-TVD: Spatial super-resolution for time-varying data
analysis and visualization. <em>TVCG</em>, <em>28</em>(6), 2445–2456.
(<a href="https://doi.org/10.1109/TVCG.2020.3032123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present SSR-TVD, a novel deep learning framework that produces coherent spatial super-resolution (SSR) of time-varying data (TVD) using adversarial learning. In scientific visualization, SSR-TVD is the first work that applies the generative adversarial network (GAN) to generate high-resolution volumes for three-dimensional time-varying data sets. The design of SSR-TVD includes a generator and two discriminators (spatial and temporal discriminators). The generator takes a low-resolution volume as input and outputs a synthesized high-resolution volume. To capture spatial and temporal coherence in the volume sequence, the two discriminators take the synthesized high-resolution volume(s) as input and produce a score indicating the realness of the volume(s). Our method can work in the in situ visualization setting by downscaling volumetric data from selected time steps as the simulation runs and upscaling downsampled volumes to their original resolution during postprocessing. To demonstrate the effectiveness of SSR-TVD, we show quantitative and qualitative results with several time-varying data sets of different characteristics and compare our method against volume upscaling using bicubic interpolation and a solution solely based on CNN.},
  archive      = {J_TVCG},
  author       = {Jun Han and Chaoli Wang},
  doi          = {10.1109/TVCG.2020.3032123},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2445-2456},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SSR-TVD: Spatial super-resolution for time-varying data analysis and visualization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SEG-MAT: 3D shape segmentation using medial axis transform.
<em>TVCG</em>, <em>28</em>(6), 2430–2444. (<a
href="https://doi.org/10.1109/TVCG.2020.3032566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting arbitrary 3D objects into constituent parts that are structurally meaningful is a fundamental problem encountered in a wide range of computer graphics applications. Existing methods for 3D shape segmentation suffer from complex geometry processing and heavy computation caused by using low-level features and fragmented segmentation results due to the lack of global consideration. We present an efficient method, called SEG-MAT , based on the medial axis transform (MAT) of the input shape. Specifically, with the rich geometrical and structural information encoded in the MAT, we are able to develop a simple and principled approach to effectively identify the various types of junctions between different parts of a 3D shape. Extensive evaluations and comparisons show that our method outperforms the state-of-the-art methods in terms of segmentation quality and is also one order of magnitude faster.},
  archive      = {J_TVCG},
  author       = {Cheng Lin and Lingjie Liu and Changjian Li and Leif Kobbelt and Bin Wang and Shiqing Xin and Wenping Wang},
  doi          = {10.1109/TVCG.2020.3032566},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2430-2444},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SEG-MAT: 3D shape segmentation using medial axis transform},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SAniHead: Sketching animal-like 3D character heads using a
view-surface collaborative mesh generative network. <em>TVCG</em>,
<em>28</em>(6), 2415–2429. (<a
href="https://doi.org/10.1109/TVCG.2020.3030330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the game and film industries, modeling 3D heads plays a very important role in designing characters. Although human head modeling has been researched for a long time, few works have focused on animal-like heads, which are of more diverse shapes and richer geometric details. In this article, we present SAniHead , an interactive system for creating animal-like heads with a mesh representation from dual-view sketches. Our core technical contribution is a view-surface collaborative mesh generative network. Initially, a graph convolutional neural network (GCNN) is trained to learn the deformation of a template mesh to fit the shape of sketches, giving rise to a coarse model. It is then projected into vertex maps where image-to-image translation networks are performed for detail inference. After back-projecting the inferred details onto the meshed surface, a new GCNN is trained for further detail refinement. The modules of view-based detail inference and surface-based detail refinement are conducted in an alternating cascaded fashion, collaboratively improving the model. A refinement sketching interface is also implemented to support direct mesh manipulation. Experimental results show the superiority of our approach and the usability of our interactive system. Our work also contributes a 3D animal head dataset with corresponding line drawings.},
  archive      = {J_TVCG},
  author       = {Dong Du and Xiaoguang Han and Hongbo Fu and Feiyang Wu and Yizhou Yu and Shuguang Cui and Ligang Liu},
  doi          = {10.1109/TVCG.2020.3030330},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2415-2429},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SAniHead: Sketching animal-like 3D character heads using a view-surface collaborative mesh generative network},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reconstruction of dexterous 3D motion data from a flexible
magnetic sensor with deep learning and structure-aware filtering.
<em>TVCG</em>, <em>28</em>(6), 2400–2414. (<a
href="https://doi.org/10.1109/TVCG.2020.3031632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose IM3D+, a novel approach to reconstructing 3D motion data from a flexible magnetic flux sensor array using deep learning and a structure-aware temporal bilateral filter. Computing the 3D configuration of markers (inductor-capacitor (LC) coils) from flux sensor data is difficult because the existing numerical approaches suffer from system noise, dead angles, the need for initialization, and limitations in the sensor array’s layout. We solve these issues with deep neural networks to learn the regression from the simulation flux values to the LC coils’ 3D configuration, which can be applied to the actual LC coils at any location and orientation within the capture volume. To cope with the influence of system noise and the dead-angle limitation caused by the characteristics of the hardware and sensing principle, we propose a structure-aware temporal bilateral filter for reconstructing motion sequences. Our method can track various movements, including fingers that manipulate objects, beetles that move inside a vivarium with leaves and soil, and the flow of opaque fluid. Since no power supply is needed for the lightweight wireless markers, our method can robustly track movements for a very long time, making it suitable for various types of observations whose tracking is difficult with existing motion-tracking systems. Furthermore, the flexibility of the flux sensor layout allows users to reconfigure it based on their own applications, thus making our approach suitable for a variety of virtual reality applications.},
  archive      = {J_TVCG},
  author       = {Jiawei Huang and Ryo Sugawara and Kinfung Chu and Taku Komura and Yoshifumi Kitamura},
  doi          = {10.1109/TVCG.2020.3031632},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2400-2414},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Reconstruction of dexterous 3D motion data from a flexible magnetic sensor with deep learning and structure-aware filtering},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multicriteria scalable graph drawing via stochastic gradient
descent, <span
class="math inline">(<em>S</em><em>G</em><em>D</em>)<sup>2</sup></span>(SGD)2.
<em>TVCG</em>, <em>28</em>(6), 2388–2399. (<a
href="https://doi.org/10.1109/TVCG.2022.3155564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Readability criteria, such as distance or neighborhood preservation, are often used to optimize node-link representations of graphs to enable the comprehension of the underlying data. With few exceptions, graph drawing algorithms typically optimize one such criterion, usually at the expense of others. We propose a layout approach, Multicriteria Scalable Graph Drawing via Stochastic Gradient Descent, $(SGD)^{2}$ , that can handle multiple readability criteria. $(SGD)^{2}$ can optimize any criterion that can be described by a differentiable function. Our approach is flexible and can be used to optimize several criteria that have already been considered earlier (e.g., obtaining ideal edge lengths, stress, neighborhood preservation) as well as other criteria which have not yet been explicitly optimized in such fashion (e.g., node resolution, angular resolution, aspect ratio). The approach is scalable and can handle large graphs. A variation of the underlying approach can also be used to optimize many desirable properties in planar graphs, while maintaining planarity. Finally, we provide quantitative and qualitative evidence of the effectiveness of $(SGD)^{2}$ : we analyze the interactions between criteria, measure the quality of layouts generated from $(SGD)^{2}$ as well as the runtime behavior, and analyze the impact of sample sizes. The source code is available on github and we also provide an interactive demo for small graphs.},
  archive      = {J_TVCG},
  author       = {Reyan Ahmed and Felice De Luca and Sabin Devkota and Stephen Kobourov and Mingwei Li},
  doi          = {10.1109/TVCG.2022.3155564},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2388-2399},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multicriteria scalable graph drawing via stochastic gradient descent, $(SGD)^{2}$(SGD)2},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multicriteria optimization for dynamic demers cartograms.
<em>TVCG</em>, <em>28</em>(6), 2376–2387. (<a
href="https://doi.org/10.1109/TVCG.2022.3151227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cartograms are popular for visualizing numerical data for administrative regions in thematic maps. When there are multiple data values per region (over time or from different datasets) shown as animated or juxtaposed cartograms, preserving the viewer’s mental map in terms of stability between multiple cartograms is another important criterion alongside traditional cartogram criteria such as maintaining adjacencies. We present a method to compute stable stable Demers cartograms, where each region is shown as a square scaled proportionally to the given numerical data and similar data yield similar cartograms. We enforce orthogonal separation constraints using linear programming, and measure quality in terms of keeping adjacent regions close (cartogram quality) and using similar positions for a region between the different data values (stability). Our method guarantees the ability to connect most lost adjacencies with minimal-length planar orthogonal polylines. Experiments show that our method yields good quality and stability on multiple quality criteria.},
  archive      = {J_TVCG},
  author       = {Soeren Nickel and Max Sondag and Wouter Meulemans and Stephen Kobourov and Jaakko Peltonen and Martin Nöllenburg},
  doi          = {10.1109/TVCG.2022.3151227},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2376-2387},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multicriteria optimization for dynamic demers cartograms},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Emotion-preserving blendshape update with real-time face
tracking. <em>TVCG</em>, <em>28</em>(6), 2364–2375. (<a
href="https://doi.org/10.1109/TVCG.2020.3033838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blendshape representations are widely used in facial animation. Consistent semantics must be maintained for all the blendshapes to build the blendshapes of one character. However, this is difficult for real characters because the face shape of the same semantics varies significantly across identities. Previous studies have handled this issue by asking users to perform a set of predefined expressions with specified semantics. We observe that facial emotions can be used to define semantics. Herein, we propose a real-time technique that directly updates blendshapes without predefined expressions. Its aim is to preserve semantics based on the emotion information extracted from an arbitrary facial motion sequence. In addition, we have designed corresponding algorithms to efficiently update blendshapes with large- and middle-scale face shapes and fine-scale facial details, such as wrinkles, in a real-time face tracking system. The experimental results indicate that using a commodity RGBD sensor, we can achieve real-time online blendshape updates with well-preserved semantics and user-specific facial features and details.},
  archive      = {J_TVCG},
  author       = {Zhibo Wang and Jingwang Ling and Chengzeng Feng and Ming Lu and Feng Xu},
  doi          = {10.1109/TVCG.2020.3033838},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2364-2375},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Emotion-preserving blendshape update with real-time face tracking},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AMM: Adaptive multilinear meshes. <em>TVCG</em>,
<em>28</em>(6), 2350–2363. (<a
href="https://doi.org/10.1109/TVCG.2022.3165392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive representations are increasingly indispensable for reducing the in-memory and on-disk footprints of large-scale data. Usual solutions are designed broadly along two themes: reducing data precision, e.g. , through compression, or adapting data resolution, e.g. , using spatial hierarchies. Recent research suggests that combining the two approaches, i.e. , adapting both resolution and precision simultaneously, can offer significant gains over using them individually. However, there currently exist no practical solutions to creating and evaluating such representations at scale. In this work, we present a new resolution-precision-adaptive representation to support hybrid data reduction schemes and offer an interface to existing tools and algorithms. Through novelties in spatial hierarchy, our representation, Adaptive Multilinear Meshes (AMM), provides considerable reduction in the mesh size. AMM creates a piecewise multilinear representation of uniformly sampled scalar data and can selectively relax or enforce constraints on conformity, continuity, and coverage, delivering a flexible adaptive representation. AMM also supports representing the function using mixed-precision values to further the achievable gains in data reduction. We describe a practical approach to creating AMM incrementally using arbitrary orderings of data and demonstrate AMM on six types of resolution and precision datastreams. By interfacing with state-of-the-art rendering tools through VTK, we demonstrate the practical and computational advantages of our representation for visualization techniques. With an open-source release of our tool to create AMM, we make such evaluation of data reduction accessible to the community, which we hope will foster new opportunities and future data reduction schemes.},
  archive      = {J_TVCG},
  author       = {Harsh Bhatia and Duong Hoang and Nate Morrical and Valerio Pascucci and Peer-Timo Bremer and Peter Lindstrom},
  doi          = {10.1109/TVCG.2022.3165392},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2350-2363},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {AMM: Adaptive multilinear meshes},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A visual analytics approach for hardware system monitoring
with streaming functional data analysis. <em>TVCG</em>, <em>28</em>(6),
2338–2349. (<a href="https://doi.org/10.1109/TVCG.2022.3165348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world applications involve analyzing time-dependent phenomena, which are intrinsically functional, consisting of curves varying over a continuum (e.g., time). When analyzing continuous data, functional data analysis (FDA) provides substantial benefits, such as the ability to study the derivatives and to restrict the ordering of data. However, continuous data inherently has infinite dimensions, and for a long time series, FDA methods often suffer from high computational costs. The analysis problem becomes even more challenging when updating the FDA results for continuously arriving data. In this paper, we present a visual analytics approach for monitoring and reviewing time series data streamed from a hardware system with a focus on identifying outliers by using FDA. To perform FDA while addressing the computational problem, we introduce new incremental and progressive algorithms that promptly generate the magnitude-shape (MS) plot, which conveys both the functional magnitude and shape outlyingness of time series data. In addition, by using an MS plot in conjunction with an FDA version of principal component analysis, we enhance the analyst&#39;s ability to investigate the visually-identified outliers. We illustrate the effectiveness of our approach with two use scenarios using real-world datasets. The resulting tool is evaluated by industry experts using real-world streaming datasets.},
  archive      = {J_TVCG},
  author       = {Shilpika and Takanori Fujiwara and Naohisa Sakamoto and Jorji Nonaka and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2022.3165348},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2338-2349},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A visual analytics approach for hardware system monitoring with streaming functional data analysis},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VAC-CNN: A visual analytics system for comparative studies
of deep convolutional neural networks. <em>TVCG</em>, <em>28</em>(6),
2326–2337. (<a href="https://doi.org/10.1109/TVCG.2022.3165347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of Convolutional Neural Networks (CNNs) in recent years has triggered significant breakthroughs in many machine learning (ML) applications. The ability to understand and compare various CNN models available is thus essential. The conventional approach with visualizing each model&#39;s quantitative features, such as classification accuracy and computational complexity, is not sufficient for a deeper understanding and comparison of the behaviors of different models. Moreover, most of the existing tools for assessing CNN behaviors only support comparison between two models and lack the flexibility of customizing the analysis tasks according to user needs. This paper presents a visual analytics system, VAC-CNN ( V isual A nalytics for C omparing CNNs ), that supports the in-depth inspection of a single CNN model as well as comparative studies of two or more models. The ability to compare a larger number of (e.g., tens of) models especially distinguishes our system from previous ones. With a carefully designed model visualization and explaining support, VAC-CNN facilitates a highly interactive workflow that promptly presents both quantitative and qualitative information at each analysis stage. We demonstrate VAC-CNN&#39;s effectiveness for assisting novice ML practitioners in evaluating and comparing multiple CNN models through two use cases and one preliminary evaluation study using the image classification tasks on the ImageNet dataset.},
  archive      = {J_TVCG},
  author       = {Xiwei Xuan and Xiaoyu Zhang and Oh-Hyun Kwon and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2022.3165347},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2326-2337},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VAC-CNN: A visual analytics system for comparative studies of deep convolutional neural networks},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image-based visualization of large volumetric data using
moments. <em>TVCG</em>, <em>28</em>(6), 2314–2325. (<a
href="https://doi.org/10.1109/TVCG.2022.3165346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel image-based representation to interactively visualize large and arbitrarily structured volumetric data. This image-based representation is created from a fixed view and models the scalar densities along each viewing ray. Then, any transfer function can be applied and changed interactively to visualize the data. In detail, we transform the density in each pixel to the Fourier basis and store Fourier coefficients of a bounded signal, i.e. bounded trigonometric moments. To keep this image-based representation compact, we adaptively determine the number of moments in each pixel and present a novel coding and quantization strategy. Additionally, we perform spatial and temporal interpolation of our image representation and discuss the visualization of introduced uncertainties. Moreover, we use our representation to add single scattering illumination. Lastly, we achieve accurate results even with changes in the view configuration. We evaluate our approach on two large volume datasets and a time-dependent SPH dataset.},
  archive      = {J_TVCG},
  author       = {Tobias Rapp and Christoph Peters and Carsten Dachsbacher},
  doi          = {10.1109/TVCG.2022.3165346},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2314-2325},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Image-based visualization of large volumetric data using moments},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GNN-surrogate: A hierarchical and adaptive graph neural
network for parameter space exploration of unstructured-mesh ocean
simulations. <em>TVCG</em>, <em>28</em>(6), 2301–2313. (<a
href="https://doi.org/10.1109/TVCG.2022.3165345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose GNN-Surrogate, a graph neural network-based surrogate model to explore the parameter space of ocean climate simulations. Parameter space exploration is important for domain scientists to understand the influence of input parameters (e.g., wind stress) on the simulation output (e.g., temperature). The exploration requires scientists to exhaust the complicated parameter space by running a batch of computationally expensive simulations. Our approach improves the efficiency of parameter space exploration with a surrogate model that predicts the simulation outputs accurately and efficiently. Specifically, GNN-Surrogate predicts the output field with given simulation parameters so scientists can explore the simulation parameter space with visualizations from user-specified visual mappings. Moreover, our graph-based techniques are designed for unstructured meshes, making the exploration of simulation outputs on irregular grids efficient. For efficient training, we generate hierarchical graphs and use adaptive resolutions. We give quantitative and qualitative evaluations on the MPAS-Ocean simulation to demonstrate the effectiveness and efficiency of GNN-Surrogate. Source code is publicly available at https://github.com/trainsn/GNN-Surrogate .},
  archive      = {J_TVCG},
  author       = {Neng Shi and Jiayi Xu and Skylar W. Wurster and Hanqi Guo and Jonathan Woodring and Luke P. Van Roekel and Han-Wei Shen},
  doi          = {10.1109/TVCG.2022.3165345},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2301-2313},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GNN-surrogate: A hierarchical and adaptive graph neural network for parameter space exploration of unstructured-mesh ocean simulations},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guest editors’ introduction: Special section on IEEE
PacificVis 2022. <em>TVCG</em>, <em>28</em>(6), 2299–2300. (<a
href="https://doi.org/10.1109/TVCG.2022.3165393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special section of the IEEE Transactions on Visualization and Computer Graphics (IEEE TVCG) presents the five most highly rated papers from the 2022 IEEE Pacific Visualization Symposium (IEEE PacificVis). This year, IEEE PacificVis was scheduled to be hosted by the University of Tsukuba and held in Tsukuba, Japan, from April 11 to 14, 2022. IEEE PacificVis, sponsored by the IEEE Visualization and Graphics Technical Committee (VGTC), aims to foster greater exchange between visualization researchers and practitioners, especially in the Asia-Pacific region. This forum has grown to be a truly international event, attracting submissions and attendees from many countries in the Asia-Pacific, Europe, America, and beyond. Thus, IEEE PacificVis is serving the additional purposes of sharing the latest advances in visualization with researchers and practitioners in the region and introducing research developments in the region to the broader international visualization research community.},
  archive      = {J_TVCG},
  author       = {Nan Cao and Timo Ropinski and Jian Zhao},
  doi          = {10.1109/TVCG.2022.3165393},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2299-2300},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Guest editors&#39; introduction: Special section on IEEE PacificVis 2022},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VGTC virtual reality academy. <em>TVCG</em>, <em>28</em>(5),
xix–xx. (<a href="https://doi.org/10.1109/TVCG.2022.3157523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The IEEE VGTC Virtual Reality Academy was established in 2022 to highlight the accomplishments of the leaders in the field. Criteria for election to the Academy include: • Cumulative and momentous contributions to research and/or development. • Broader influence on the field, the community, and on the work of others. • Significant service and/or active participation in the community. • Awardees of the IEEE VGTC Virtual Reality Lifetime Achievement and Technical Achievement Awards will be inducted the year they receive their award unless already an Academy Member. },
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2022.3157523},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {xix-xx},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VGTC virtual reality academy},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VGTC virtual reality awards program chair message.
<em>TVCG</em>, <em>28</em>(5), xii. (<a
href="https://doi.org/10.1109/TVCG.2022.3157524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The IEEE VGTC Virtual Reality Awards program was expanded this year with additional areas and with the establishment of the Virtual Reality Academy, the biggest expansion since the VR Awards started in 2005:},
  archive      = {J_TVCG},
  author       = {Henry Fuchs and Federico Gil},
  doi          = {10.1109/TVCG.2022.3157524},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {xii},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VGTC virtual reality awards program chair message},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IEEE VR 2022 visualization and graphics technical committee
(VGTC) statement. <em>TVCG</em>, <em>28</em>(5), viii. (<a
href="https://doi.org/10.1109/TVCG.2022.3156686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the IEEE VR 2022 Visualization and Graphics Technical Committee (VGTC) Statement.},
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2022.3156686},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {viii},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IEEE VR 2022 visualization and graphics technical committee (VGTC) statement},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IEEE VR 2022 message from the journal paper chairs and guest
editors. <em>TVCG</em>, <em>28</em>(5), vii. (<a
href="https://doi.org/10.1109/TVCG.2022.3156766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this special issue of IEEE Transactions on Visualization and Computer Graphics (TVCG), we are pleased to present a subset of papers from the 29th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2022), held virtually March 12-16, 2022, in Christchurch, New Zealand.},
  archive      = {J_TVCG},
  author       = {Luciana Nedel and Ferran Argelaguet and Lili Wang and Jeanine Stefannuci and Daisuke Iwai},
  doi          = {10.1109/TVCG.2022.3156766},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {vii},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IEEE VR 2022 message from the journal paper chairs and guest editors},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). IEEE VR 2022 introducing the special issue. <em>TVCG</em>,
<em>28</em>(5), vi. (<a
href="https://doi.org/10.1109/TVCG.2022.3156765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Welcome to the 11 th IEEE Transactions on Visualization and Computer Graphics (TVCG) special issue on IEEE Virtual Reality and 3D User Interfaces. This volume contains a total of 29 full papers selected for and presented at the IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2022), held fully virtual in Christchurch, New Zealand from March 12 to 16, 2022.},
  archive      = {J_TVCG},
  author       = {Klaus Mueller and Doug Bowman},
  doi          = {10.1109/TVCG.2022.3156765},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {vi},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IEEE VR 2022 introducing the special issue},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Validating simulation-based evaluation of redirected walking
systems. <em>TVCG</em>, <em>28</em>(5), 2288–2298. (<a
href="https://doi.org/10.1109/TVCG.2022.3150466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing effective strategies for redirected walking requires extensive evaluations across a variety of factors that influence performance. Because these large-scale experiments are often not practical with user studies, researchers have instead utilized simulations to systematically test different algorithm parameters, physical space configurations, and virtual walking paths. Although simulation offers an efficient way to evaluate redirected walking algorithms, it remains an open question whether this evaluation methodology is ecologically valid. In this paper, we investigate the interaction between locomotion behavior and redirection gains at a micro-level (across small path segments) and macro-level (across an entire experience). This examination involves analyzing data from real users and comparing algorithm performance metrics with a simulated user model. The results identify specific properties of user locomotion behavior that influence the application of redirected walking gains and resets. Overall, we found that the simulation provided a conservative estimate of the average performance with real users and observed that performance trends when comparing two redirected walking algorithms were preserved. In general, these results indicate that simulation is an empirically valid evaluation methodology for redirected walking algorithms.},
  archive      = {J_TVCG},
  author       = {Mahdi Azmandian and Rhys Yahata and Timofey Grechkin and Jerald Thomas and Evan Suma Rosenberg},
  doi          = {10.1109/TVCG.2022.3150466},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2288-2298},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Validating simulation-based evaluation of redirected walking systems},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive redirection: A context-aware redirected walking
meta-strategy. <em>TVCG</em>, <em>28</em>(5), 2277–2287. (<a
href="https://doi.org/10.1109/TVCG.2022.3150500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous research has established redirected walking as a potential answer to exploring large virtual environments via natural locomotion within a limited physical space. However, much of the previous work has either focused on investigating human perception of redirected walking illusions or developing novel redirection techniques. In this paper, we take a broader look at the problem and formalize the concept of a complete redirected walking system. This work establishes the theoretical foundations for combining multiple redirection strategies into a unified framework known as adaptive redirection . This meta-strategy adapts based on the context, switching between a suite of strategies with a priori knowledge of their performance under the various circumstances. This paper also introduces a novel static planning strategy that optimizes gain parameters for a predetermined virtual path, known as the Combinatorially Optimized Pre-Planned Exploration Redirector (COPPER). We conducted a simulation-based experiment that demonstrates how adaptation rules can be determined empirically using machine learning, which involves partitioning the spectrum of contexts into regions according to the redirection strategy that performs best. Adaptive redirection provides a foundation for making redirected walking work in practice and can be extended to improve performance in the future as new techniques are integrated into the framework.},
  archive      = {J_TVCG},
  author       = {Mahdi Azmandian and Rhys Yahata and Timofey Grechkin and Evan Suma Rosenberg},
  doi          = {10.1109/TVCG.2022.3150500},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2277-2287},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Adaptive redirection: A context-aware redirected walking meta-strategy},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Breaking plausibility without breaking presence - evidence
for the multi-layer nature of plausibility. <em>TVCG</em>,
<em>28</em>(5), 2267–2276. (<a
href="https://doi.org/10.1109/TVCG.2022.3150496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel theoretical model recently introduced coherence and plausibility as the essential conditions of XR experiences, challenging contemporary presence-oriented concepts. This article reports on two experiments validating this model, which assumes coherence activation on three layers (cognition, perception, and sensation) as the potential sources leading to a condition of plausibility and from there to other XR qualia such as presence or body ownership. The experiments introduce and utilize breaks in plausibility (in analogy to breaks in presence): We induce incoherence on the perceptual and the cognitive layer simultaneously by a simulation of object behaviors that do not conform to the laws of physics, i.e., gravity. We show that this manipulation breaks plausibility and hence confirm that it results in the desired effects in the theorized condition space but that the breaks in plausibility did not affect presence. In addition, we show that a cognitive manipulation by a storyline framing is too weak to successfully counteract the strong bottom-up inconsistencies. Both results are in line with the predictions of the recently introduced three-layer model of coherence and plausibility, which incorporates well-known top-down and bottom-up rivalries and its theorized increased independence between plausibility and presence.},
  archive      = {J_TVCG},
  author       = {Larissa Brübach and Franziska Westermeier and Carolin Wienrich and Marc Erich Latoschik},
  doi          = {10.1109/TVCG.2022.3150496},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2267-2276},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Breaking plausibility without breaking presence - evidence for the multi-layer nature of plausibility},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video see-through mixed reality with focus cues.
<em>TVCG</em>, <em>28</em>(5), 2256–2266. (<a
href="https://doi.org/10.1109/TVCG.2022.3150504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces the first approach to video see-through mixed reality with full support for focus cues. By combining the flexibility to adjust the focus distance found in varifocal designs with the robustness to eye-tracking error found in multifocal designs, our novel display architecture reliably delivers focus cues over a large workspace. In particular, we introduce gaze-contingent layered displays and mixed reality focal stacks, an efficient representation of mixed reality content that lends itself to fast processing for driving layered displays in real time. We thoroughly evaluate this approach by building a complete end-to-end pipeline for capture, render, and display of focus cues in video see-through displays that uses only off-the-shelf hardware and compute components.},
  archive      = {J_TVCG},
  author       = {Christoph Ebner and Shohei Mori and Peter Mohr and Yifan Peng and Dieter Schmalstieg and Gordon Wetzstein and Denis Kalkofen},
  doi          = {10.1109/TVCG.2022.3150504},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2256-2266},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Video see-through mixed reality with focus cues},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The one-man-crowd: Single user generation of crowd motions
using virtual reality. <em>TVCG</em>, <em>28</em>(5), 2245–2255. (<a
href="https://doi.org/10.1109/TVCG.2022.3150507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd motion data is fundamental for understanding and simulating realistic crowd behaviours. Such data is usually collected through controlled experiments to ensure that both desired individual interactions and collective behaviours can be observed. It is however scarce, due to ethical concerns and logistical difficulties involved in its gathering, and only covers a few typical crowd scenarios. In this work, we propose and evaluate a novel Virtual Reality based approach lifting the limitations of real-world experiments for the acquisition of crowd motion data. Our approach immerses a single user in virtual scenarios where he/she successively acts each crowd member. By recording the past trajectories and body movements of the user, and displaying them on virtual characters, the user progressively builds the overall crowd behaviour by him/herself. We validate the feasibility of our approach by replicating three real experiments, and compare both the resulting emergent phenomena and the individual interactions to existing real datasets. Our results suggest that realistic collective behaviours can naturally emerge from virtual crowd data generated using our approach, even though the variety in behaviours is lower than in real situations. These results provide valuable insights to the building of virtual crowd experiences, and reveal key directions for further improvements.},
  archive      = {J_TVCG},
  author       = {Tairan Yin and Ludovic Hoyet and Marc Christie and Marie-Paule Cani and Julien Pettré},
  doi          = {10.1109/TVCG.2022.3150507},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2245-2255},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The one-man-crowd: Single user generation of crowd motions using virtual reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Omnidirectional galvanic vestibular stimulation in virtual
reality. <em>TVCG</em>, <em>28</em>(5), 2234–2244. (<a
href="https://doi.org/10.1109/TVCG.2022.3150506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose omnidirectional galvanic vestibular stimulation (GVS) to mitigate cybersickness in virtual reality applications. One of the most accepted theories indicates that Cybersickness is caused by the visually induced impression of ego motion while physically remaining at rest. As a result of this sensory mismatch, people associate negative symptoms with VR and sometimes avoid the technology altogether. To reconcile the two contradicting sensory perceptions, we investigate GVS to stimulate the vestibular canals behind our ears with low-current electrical signals that are specifically attuned to the visually displayed camera motion. We describe how to calibrate and generate the appropriate GVS signals in real-time for pre-recorded omnidirectional videos exhibiting ego-motion in all three spatial directions. For validation, we conduct an experiment presenting real-world 360° videos shot from a moving first-person perspective in a VR head-mounted display. Our findings indicate that GVS is able to significantly reduce discomfort for cybersickness-susceptible VR users, creating a deeper and more enjoyable immersive experience for many people.},
  archive      = {J_TVCG},
  author       = {Colin Groth and Jan-Philipp Tauscher and Nikkel Heesen and Max Hattenbach and Susana Castillo and Marcus Magnor},
  doi          = {10.1109/TVCG.2022.3150506},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2234-2244},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Omnidirectional galvanic vestibular stimulation in virtual reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online projector deblurring using a convolutional neural
network. <em>TVCG</em>, <em>28</em>(5), 2223–2233. (<a
href="https://doi.org/10.1109/TVCG.2022.3150465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projector deblurring is an important technology for dynamic projection mapping (PM), where the distance between a projector and a projection surface changes in time. However, conventional projector deblurring techniques do not support dynamic PM because they need to project calibration patterns to estimate the amount of defocus blur each time the surface moves. We present a deep neural network that can compensate for defocus blur in dynamic PM. The primary contribution of this paper is a unique network structure that consists of an extractor and a generator. The extractor explicitly estimates a defocus blur map and a luminance attenuation map. These maps are then injected into the middle layers of the generator network that computes the compensation image. We also propose a pseudo-projection technique for synthesizing physically plausible training data, considering the geometric misregistration that potentially happens in actual PM systems. We conducted simulation and actual PM experiments and confirmed that: (1) the proposed network structure is more suitable than a simple, more general structure for projector deblurring; (2) the network trained with the proposed pseudo-projection technique can compensate projection images for defocus blur artifacts in dynamic PM; and (3) the network supports the translation speed of the surface movement within a certain range that covers normal human motions.},
  archive      = {J_TVCG},
  author       = {Yuta Kageyama and Daisuke Iwai and Kosuke Sato},
  doi          = {10.1109/TVCG.2022.3150465},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2223-2233},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Online projector deblurring using a convolutional neural network},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust tightly-coupled visual-inertial odometry with
pre-built maps in high latency situations. <em>TVCG</em>,
<em>28</em>(5), 2212–2222. (<a
href="https://doi.org/10.1109/TVCG.2022.3150495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel monocular visual-inertial odometry system with pre-built maps deployed on the remote server, which can robustly run in real-time on a mobile device even in high latency situations. By tightly coupling VIO with geometric priors from pre-built maps, our system can tolerate the high latency and low frequency of global localization service, which is especially suitable for practical applications when the localization service is deployed on the remote server. Firstly, sparse point clouds are obtained from the dense mesh by the ray casting method according to the localization results. The dense mesh can be reconstructed from the point clouds generated by Structure-from-Motion. We directly use the sparse point clouds in feature tracking and state update to suppress drift. In the process of feature tracking, the high local accuracy of VIO is fully utilized to effectively remove outliers and make our system robust. The experiments on EurocMav datasets and simulation datasets show that compared with state-of-the-art methods, our method can achieve better results in terms of both precision and robustness. The effectiveness of the proposed method is further demonstrated through a real-time AR demo on a mobile phone with the aid of visual localization on the remote server.},
  archive      = {J_TVCG},
  author       = {Hujun Bao and Weijian Xie and Quanhao Qian and Danpeng Chen and Shangjin Zhai and Nan Wang and Guofeng Zhang},
  doi          = {10.1109/TVCG.2022.3150495},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2212-2222},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Robust tightly-coupled visual-inertial odometry with pre-built maps in high latency situations},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prepare for ludicrous speed: Marker-based instantaneous
binocular rolling shutter localization. <em>TVCG</em>, <em>28</em>(5),
2201–2211. (<a href="https://doi.org/10.1109/TVCG.2022.3150485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a marker-based geometric framework for the high-frequency absolute 3D pose estimation of a binocular camera system by using the data captured during the exposure of a single rolling shutter scanline. In contrast to existing approaches enforcing temporal or motion models among scanlines (e.g. linear motion, constant velocity or small motion assumptions), we strive to determine the pose from instantaneous binocular capture (i.e. without using data from previous scanlines) and achieve drift-free pose estimation. We leverage the projective invariants of a novel rigid planar pattern, to both define a geometric reference as well as to determine 2D-3D correspondences from raw edge detection measurements from individual scanlines. Moreover, to tackle the ensuing multi-view estimation problem, achieve real-time operation, and minimize latency, we develop a pair of custom solvers leveraging our geometric setup. To mitigate sensitivity to noise, we propose a geometrically consistent measurement refinement mechanism. We verify the quality of our solvers by comparing with state of the art general solvers for absolute pose estimation of generalized cameras. Finally, we demonstrate the effectiveness of our proposed approach with an FPGA-based implementation which achieves a localization throughput of 129.6 KHz with a $1.5\ \mu \mathsf{s}$ latency.},
  archive      = {J_TVCG},
  author       = {Juan Carlos Dibene and Yazmin Maldonado and Leonardo Trujillo and Enrique Dunn},
  doi          = {10.1109/TVCG.2022.3150485},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2201-2211},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Prepare for ludicrous speed: Marker-based instantaneous binocular rolling shutter localization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Duplicated reality for co-located augmented reality
collaboration. <em>TVCG</em>, <em>28</em>(5), 2190–2200. (<a
href="https://doi.org/10.1109/TVCG.2022.3150520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When two or more users attempt to collaborate in the same space with Augmented Reality, they often encounter conflicting intentions regarding the occupation of the same working area and self-positioning around such without mutual interference. Augmented Reality is a powerful tool for communicating ideas and intentions during a co-assisting task that requires multi-disciplinary expertise. To relax the constraint of physical co-location, we propose the concept of Duplicated Reality, where a digital copy of a 3D region of interest of the users&#39; environment is reconstructed in real-time and visualized in-situ through an Augmented Reality user interface. This enables users to remotely annotate the region of interest while being co-located with others in Augmented Reality. We perform a user study to gain an in-depth understanding of the proposed method compared to an in-situ augmentation, including collaboration, effort, awareness, usability, and the quality of the task. The result indicates almost identical objective and subjective results, except a decrease in the consulting user&#39;s awareness of co-located users when using our method. The added benefit from duplicating the working area into a designated consulting area opens up new interaction paradigms to be further investigated for future co-located Augmented Reality collaboration systems.},
  archive      = {J_TVCG},
  author       = {Kevin Yu and Ulrich Eck and Frieder Pankratz and Marc Lazarovici and Dirk Wilhelm and Nassir Navab},
  doi          = {10.1109/TVCG.2022.3150520},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2190-2200},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Duplicated reality for co-located augmented reality collaboration},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effects of transparency on perceived humanness: Implications
for rendering skin tones using optical see-through displays.
<em>TVCG</em>, <em>28</em>(5), 2179–2189. (<a
href="https://doi.org/10.1109/TVCG.2022.3150521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current optical see-through displays in the field of augmented reality are limited in their ability to display colors with low lightness in the hue, saturation, lightness (HSL) color space, causing such colors to appear transparent. This hardware limitation may add unintended bias into scenarios with virtual humans. Humans have varying skin tones including HSL colors with low lightness. When virtual humans are displayed with optical see-through devices, people with low lightness skin tones may be displayed semi-transparently while those with high lightness skin tones will be displayed more opaquely. For example, a Black avatar may appear semi-transparent in the same scene as a White avatar who will appear more opaque. We present an exploratory user study ( $\mathrm{N}=160$ ) investigating whether differing opacity levels result in dehumanizing avatar and human faces. Results support that dehumanization occurs as opacity decreases. This suggests that in similar lighting, low lightness skin tones (e.g., Black faces) will be viewed as less human than high lightness skin tones (e.g., White faces). Additionally, the perceived emotionality of virtual human faces also predicts perceived humanness. Angry faces were seen overall as less human, and at lower opacity levels happy faces were seen as more human. Our results suggest that additional research is needed to understand the effects and interactions of emotionality and opacity on dehumanization. Further, we provide evidence that unintentional racial bias may be added when developing for optical see-through devices using virtual humans. We highlight the potential bias and discuss implications and directions for future research.},
  archive      = {J_TVCG},
  author       = {Tabitha C. Peck and Jessica J. Good and Austin Erickson and Isaac Bynum and Gerd Bruder},
  doi          = {10.1109/TVCG.2022.3150521},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2179-2189},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effects of transparency on perceived humanness: Implications for rendering skin tones using optical see-through displays},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A virtual reality based system for the screening and
classification of autism. <em>TVCG</em>, <em>28</em>(5), 2168–2178. (<a
href="https://doi.org/10.1109/TVCG.2022.3150489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autism - also known as Autism Spectrum Disorders or Autism Spectrum Conditions - is a neurodevelopmental condition characterized by repetitive behaviours and differences in communication and social interaction. As a consequence, many autistic individuals may struggle in everyday life, which sometimes manifests in depression, unemployment, or addiction. One crucial problem in patient support and treatment is the long waiting time to diagnosis, which was approximated to seven months on average. Yet, the earlier an intervention can take place the better the patient can be supported, which was identified as a crucial factor. We propose a system to support the screening of Autism Spectrum Disorders based on a virtual reality social interaction, namely a shopping experience, with an embodied agent. During this everyday interaction, behavioral responses are tracked and recorded. We analyze this behavior with machine learning approaches to classify participants from an autistic participant sample in comparison to a typically developed individuals control sample with high accuracy, demonstrating the feasibility of the approach. We believe that such tools can strongly impact the way mental disorders are assessed and may help to further find objective criteria and categorization.},
  archive      = {J_TVCG},
  author       = {Marta Robles and Negar Namdarian and Julia Otto and Evelyn Wassiljew and Nassir Navab and Christine M. Falter-Wagner and Daniel Roth},
  doi          = {10.1109/TVCG.2022.3150489},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2168-2178},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A virtual reality based system for the screening and classification of autism},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Instant reality: Gaze-contingent perceptual optimization for
3D virtual reality streaming. <em>TVCG</em>, <em>28</em>(5), 2157–2167.
(<a href="https://doi.org/10.1109/TVCG.2022.3150522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Media streaming, with an edge-cloud setting, has been adopted for a variety of applications such as entertainment, visualization, and design. Unlike video/audio streaming where the content is usually consumed passively, virtual reality applications require 3D assets stored on the edge to facilitate frequent edge-side interactions such as object manipulation and viewpoint movement. Compared to audio and video streaming, 3D asset streaming often requires larger data sizes and yet lower latency to ensure sufficient rendering quality, resolution, and latency for perceptual comfort. Thus, streaming 3D assets faces remarkably additional than streaming audios/videos, and existing solutions often suffer from long loading time or limited quality. To address this challenge, we propose a perceptually-optimized progressive 3D streaming method for spatial quality and temporal consistency in immersive interactions. On the cloud-side, our main idea is to estimate perceptual importance in 2D image space based on user gaze behaviors, including where they are looking and how their eyes move. The estimated importance is then mapped to 3D object space for scheduling the streaming priorities for edge-side rendering. Since this computational pipeline could be heavy, we also develop a simple neural network to accelerate the cloud-side scheduling process. We evaluate our method via subjective studies and objective analysis under varying network conditions (from 3G to 5G) and edge devices (HMD and traditional displays), and demonstrate better visual quality and temporal consistency than alternative solutions.},
  archive      = {J_TVCG},
  author       = {Shaoyu Chen and Budmonde Duinkharjav and Xin Sun and Li-Yi Wei and Stefano Petrangeli and Jose Echevarria and Claudio Silva and Qi Sun},
  doi          = {10.1109/TVCG.2022.3150522},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2157-2167},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Instant reality: Gaze-contingent perceptual optimization for 3D virtual reality streaming},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VirtualCube: An immersive 3D video communication system.
<em>TVCG</em>, <em>28</em>(5), 2146–2156. (<a
href="https://doi.org/10.1109/TVCG.2022.3150512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The VirtualCube system is a 3D video conference system that attempts to overcome some limitations of conventional technologies. The key ingredient is VirtualCube, an abstract representation of a real-world cubicle instrumented with RGBD cameras for capturing the user&#39;s 3D geometry and texture. We design VirtualCube so that the task of data capturing is standardized and significantly simplified, and everything can be built using off-the-shelf hardware. We use VirtualCubes as the basic building blocks of a virtual conferencing environment, and we provide each VirtualCube user with a surrounding display showing life-size videos of remote participants. To achieve real-time rendering of remote participants, we develop the V-Cube View algorithm, which uses multi-view stereo for more accurate depth estimation and Lumi-Net rendering for better rendering quality. The VirtualCube system correctly preserves the mutual eye gaze between participants, allowing them to establish eye contact and be aware of who is visually paying attention to them. The system also allows a participant to have side discussions with remote participants as if they were in the same room. Finally, the system sheds lights on how to support the shared space of work items (e.g., documents and applications) and track participants&#39; visual attention to work items.},
  archive      = {J_TVCG},
  author       = {Yizhong Zhang and Jiaolong Yang and Zhen Liu and Ruicheng Wang and Guojun Chen and Xin Tong and Baining Guo},
  doi          = {10.1109/TVCG.2022.3150512},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2146-2156},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VirtualCube: An immersive 3D video communication system},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Augmenting immersive telepresence experience with a virtual
body. <em>TVCG</em>, <em>28</em>(5), 2135–2145. (<a
href="https://doi.org/10.1109/TVCG.2022.3150473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose augmenting immersive telepresence by adding a virtual body, representing the user&#39;s own arm motions, as realized through a head-mounted display and a 360-degree camera. Previous research has shown the effectiveness of having a virtual body in simulated environments; however, research on whether seeing one&#39;s own virtual arms increases presence or preference for the user in an immersive telepresence setup is limited. We conducted a study where a host introduced a research lab while participants wore a head-mounted display which allowed them to be telepresent at the host&#39;s physical location via a 360-degree camera, either with or without a virtual body. We first conducted a pilot study of 20 participants, followed by a pre-registered 62 participant confirmatory study. Whereas the pilot study showed greater presence and preference when the virtual body was present, the confirmatory study failed to replicate these results, with only behavioral measures suggesting an increase in presence. After analyzing the qualitative data and modeling interactions, we suspect that the quality and style of the virtual arms, and the contrast between animation and video, led to individual differences in reactions to the virtual body which subsequently moderated feelings of presence.},
  archive      = {J_TVCG},
  author       = {Nikunj Arora and Markku Suomalainen and Matti Pouke and Evan G. Center and Katherine J. Mimnaugh and Alexis P. Chambers and Sakaria Pouke and Steven M. LaValle},
  doi          = {10.1109/TVCG.2022.3150473},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2135-2145},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Augmenting immersive telepresence experience with a virtual body},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic multi-projection mapping based on parallel intensity
control. <em>TVCG</em>, <em>28</em>(5), 2125–2134. (<a
href="https://doi.org/10.1109/TVCG.2022.3150488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projection mapping using multiple projectors is promising for spatial augmented reality; however, it is difficult to apply it to dynamic scenes. This is because the conventional method decides all pixel intensities of multiple images simultaneously based on the global optimization method, and it is hard to reduce the latency from motion to projection. To mitigate this, we propose a novel method of controlling the intensity based on a pixel-parallel calculation for each projector in real-time with low latency. This parallel calculation leverages the insight that the projected pixels from different projectors in overlapping areas can be approximated independently if the pixel is sufficiently small relative to the surface structure. Additionally, our pixel-parallel calculation method allows a distributed system configuration, such that the number of projectors can be increased to form a network for high scalability. We demonstrate a seamless mapping into dynamic scenes at 360 fps with a 9.5-ms latency using ten cameras and four projectors.},
  archive      = {J_TVCG},
  author       = {Takashi Nomoto and Wanlong Li and Hao-Lun Peng and Yoshihiro Watanabe},
  doi          = {10.1109/TVCG.2022.3150488},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2125-2134},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dynamic multi-projection mapping based on parallel intensity control},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stereopsis only: Validation of a monocular depth cues
reduced gamified virtual reality with reaction time measurement.
<em>TVCG</em>, <em>28</em>(5), 2114–2124. (<a
href="https://doi.org/10.1109/TVCG.2022.3150486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visual depth perception is composed of monocular and binocular depth cues. Studies show that in absence of binocular depth cues the performance of visuomotor tasks like pointing to or grasping objects is limited. Thus, binocular depth cues are of great importance for motor control required in everyday life. However, binocular depth cues like retinal disparity (basis for stereopsis) might be influenced due to developmental disorders of the visual system. For example, amblyopia in which one eye&#39;s visual input is not processed leads to loss of stereopsis. The primary amblyopia treatment is occlusion of the healthy eye to force the amblyopic eye to train. However, improvements in stereopsis are poor. Therefore, binocular treatments arose that equilibrate both eyes&#39; visual input to enable binocular vision. However, most approaches rely on divided stimuli which do not account for loss of stereopsis. We created a Virtual Reality (VR) with reduced monocular depth cues in which a stereoscopic task is shown to both eyes simultaneously, consisting of two balls jumping towards the user. One ball appears closer to the user which must be identified. To evaluate the task performance the reaction time is measured. We validated our approach with 18 participants with stereopsis under three contrast settings including one leading to monocular vision. The number of correct responses reduces from 90\% under binocular vision to 52\% under monocular vision corresponding to random guessing. Our results indicate that it is possible to disable monocular depth cues and create a dynamic stereoscopic task inside a VR.},
  archive      = {J_TVCG},
  author       = {Wolfgang Mehringer and Markus Wirth and Daniel Roth and Georg Michelson and Bjoern M Eskofier},
  doi          = {10.1109/TVCG.2022.3150486},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2114-2124},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Stereopsis only: Validation of a monocular depth cues reduced gamified virtual reality with reaction time measurement},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The impact of embodiment and avatar sizing on personal space
in immersive virtual environments. <em>TVCG</em>, <em>28</em>(5),
2102–2113. (<a href="https://doi.org/10.1109/TVCG.2022.3150483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we examine how embodiment and manipulation of a self-avatar&#39;s dimensions — specifically the arm length — affect users&#39; judgments of the personal space around them in an immersive virtual environment. In the real world, personal space is the immediate space around the body in which physical interactions are possible. Personal space is increasingly studied in virtual environments because of its importance to social interactions. Here, we specifically look at two components of personal space, interpersonal and peripersonal space, and how they are affected by embodiment and the sizing of a self-avatar. We manipulated embodiment, hypothesizing that higher levels of embodiment will result in larger measures of interpersonal space and smaller measures of peripersonal space. Likewise, we manipulated the arm length of a self-avatar, hypothesizing that while interpersonal space would change with changing arm length, peripersonal space would not. We found that the representation of both interpersonal and peripersonal space change when the user experiences differing levels of embodiment in accordance with our hypotheses, and that only interpersonal space was sensitive to changes in the dimensions of a self-avatar&#39;s arms. These findings provide increased understanding of the role of embodiment and self-avatars in the regulation of personal space, and provide foundations for improved design of social interaction in virtual environments.},
  archive      = {J_TVCG},
  author       = {Lauren E. Buck and Soumyajit Chakraborty and Bobby Bodenheimer},
  doi          = {10.1109/TVCG.2022.3150483},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2102-2113},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The impact of embodiment and avatar sizing on personal space in immersive virtual environments},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparing direct and indirect methods of audio quality
evaluation in virtual reality scenes of varying complexity.
<em>TVCG</em>, <em>28</em>(5), 2091–2101. (<a
href="https://doi.org/10.1109/TVCG.2022.3150491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many quality evaluation methods are used to assess uni-modal audio or video content without considering perceptual, cognitive, and interactive aspects present in virtual reality (VR) settings. Consequently, little is known regarding the repercussions of the employed evaluation method, content, and subject behavior on the quality ratings in VR. This mixed between- and within-subjects study uses four subjective audio quality evaluation methods (viz. multiple-stimulus with and without reference for direct scaling, and rank-order elimination and pairwise comparison for indirect scaling) to investigate the contributing factors present in multi-modal 6-DoF VR on quality ratings of real-time audio rendering. For each between-subjects employed method, two sets of conditions in five VR scenes were evaluated within-subjects. The conditions targeted relevant attributes for binaural audio reproduction using scenes with various amounts of user interactivity. Our results show all referenceless methods produce similar results using both condition sets. However, rank-order elimination proved to be the fastest method, required the least amount of repetitive motion, and yielded the highest discrimination between spatial conditions. Scene complexity was found to be a main effect within results, with behavioral and task load index results implying more complex scenes and interactive aspects of 6-DoF VR can impede quality judgments.},
  archive      = {J_TVCG},
  author       = {Thomas Robotham and Olli S. Rummukainen and Miriam Kurz and Marie Eckert and Emanuël A. P. Habets},
  doi          = {10.1109/TVCG.2022.3150491},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2091-2101},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comparing direct and indirect methods of audio quality evaluation in virtual reality scenes of varying complexity},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Studying the effects of congruence of auditory and visual
stimuli on virtual reality experiences. <em>TVCG</em>, <em>28</em>(5),
2080–2090. (<a href="https://doi.org/10.1109/TVCG.2022.3150514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies in virtual reality (VR) have introduced numerous multisensory simulation techniques for more immersive VR experiences. However, although they primarily focus on expanding sensory types or increasing individual sensory quality, they lack consensus in designing appropriate interactions between different sensory stimuli. This paper explores how the congruence between auditory and visual (AV) stimuli, which are the sensory stimuli typically provided by VR devices, affects the cognition and experience of VR users as a critical interaction factor in promoting multisensory integration. We defined the types of (in)congruence between AV stimuli, and then designed 12 virtual spaces with different types or degrees of congruence between AV stimuli. We then evaluated the presence, immersion, motion sickness, and cognition changes in each space. We observed the following key findings: 1) there is a limit to the degree of temporal or spatial incongruence that can be tolerated, with few negative effects on user experience until that point is exceeded; 2) users are tolerant of semantic incongruence; 3) a simulation that considers synesthetic congruence contributes to the user&#39;s sense of immersion and presence. Based on these insights, we identified the essential considerations for designing sensory simulations in VR and proposed future research directions.},
  archive      = {J_TVCG},
  author       = {Hayeon Kim and In-Kwon Lee},
  doi          = {10.1109/TVCG.2022.3150514},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2080-2090},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Studying the effects of congruence of auditory and visual stimuli on virtual reality experiences},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PoVRPoint: Authoring presentations in mobile virtual
reality. <em>TVCG</em>, <em>28</em>(5), 2069–2079. (<a
href="https://doi.org/10.1109/TVCG.2022.3150474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) has the potential to support mobile knowledge workers by complementing traditional input devices with a large three-dimensional output space and spatial input. Previous research on supporting VR knowledge work explored domains such as text entry using physical keyboards and spreadsheet interaction using combined pen and touch input. Inspired by such work, this paper probes the VR design space for authoring presentations in mobile settings. We propose PoVRPoint-a set of tools coupling pen- and touch-based editing of presentations on mobile devices, such as tablets, with the interaction capabilities afforded by VR. We study the utility of extended display space to, for example, assist users in identifying target slides, supporting spatial manipulation of objects on a slide, creating animations, and facilitating arrangements of multiple, possibly occluded shapes or objects. Among other things, our results indicate that 1) the wide field of view afforded by VR results in significantly faster target slide identification times compared to a tablet-only interface for visually salient targets; and 2) the three-dimensional view in VR enables significantly faster object reordering in the presence of occlusion compared to two baseline interfaces. A user study further confirmed that the interaction techniques were found to be usable and enjoyable.},
  archive      = {J_TVCG},
  author       = {Verena Biener and Travis Gesslein and Daniel Schneider and Felix Kawala and Alexander Otte and Per Ola Kristensson and Michel Pahud and Eyal Ofek and Cuauhtli Campos and Matjaž Kljun and Klen Čopič Pucihar and Jens Grubert},
  doi          = {10.1109/TVCG.2022.3150474},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2069-2079},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PoVRPoint: Authoring presentations in mobile virtual reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mood-driven colorization of virtual indoor scenes.
<em>TVCG</em>, <em>28</em>(5), 2058–2068. (<a
href="https://doi.org/10.1109/TVCG.2022.3150513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the challenging tasks in virtual scene design for Virtual Reality (VR) is causing it to invoke a particular mood in viewers. The subjective nature of moods brings uncertainty to the purpose. We propose a novel approach to automatic adjustment of the colors of textures for objects in a virtual indoor scene, enabling it to match a target mood. A dataset of 25,000 images, including building/home interiors, was used to train a classifier with the features extracted via deep learning. It contributes to an optimization process that colorizes virtual scenes automatically according to the target mood. Our approach was tested on four different indoor scenes, and we conducted a user study demonstrating its efficacy through statistical analysis with the focus on the impact of the scenes experienced with a VR headset.},
  archive      = {J_TVCG},
  author       = {Michael Solah and Haikun Huang and Jiachuan Sheng and Tian Feng and Marc Pomplun and Lap-Fai Yu},
  doi          = {10.1109/TVCG.2022.3150513},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2058-2068},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mood-driven colorization of virtual indoor scenes},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Do you need another hand? Investigating dual body
representations during anisomorphic 3D manipulation. <em>TVCG</em>,
<em>28</em>(5), 2047–2057. (<a
href="https://doi.org/10.1109/TVCG.2022.3150501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In virtual reality, several manipulation techniques distort users&#39; motions, for example to reach remote objects or increase precision. These techniques can become problematic when used with avatars, as they create a mismatch between the real performed action and the corresponding displayed action, which can negatively impact the sense of embodiment. In this paper, we propose to use a dual representation during anisomorphic interaction. A co-located representation serves as a spatial reference and reproduces the exact users&#39; motion, while an interactive representation is used for distorted interaction. We conducted two experiments, investigating the use of dual representations with amplified motion (with the Go-Go technique) and decreased motion (with the PRISM technique). Two visual appearances for the interactive representation and the co-located one were explored. This exploratory study investigating dual representations in this context showed that people globally preferred having a single representation, but opinions diverged for the Go-Go technique. Also, we could not find significant differences in terms of performance. While interacting seemed more important than showing exact movements for agency during out-of-reach manipulation, people felt more in control of the realistic arm during close manipulation.},
  archive      = {J_TVCG},
  author       = {Diane Dewez and Ludovic Hoyet and Anatole Lécuyer and Ferran Argelaguet},
  doi          = {10.1109/TVCG.2022.3150501},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2047-2057},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Do you need another hand? investigating dual body representations during anisomorphic 3D manipulation},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Remote research on locomotion interfaces for virtual
reality: Replication of a lab-based study on teleporting interfaces.
<em>TVCG</em>, <em>28</em>(5), 2037–2046. (<a
href="https://doi.org/10.1109/TVCG.2022.3150475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wide availability of consumer-oriented virtual reality (VR) equipment has enabled researchers to recruit existing VR owners to participate remotely using their own equipment. Yet, there are many differences between lab environments and home environments, as well as differences between participant samples recruited for lab studies and remote studies. This paper replicates a lab-based experiment on VR locomotion interfaces using a remote sample. Participants completed a triangle-completion task (travel two path legs, then point to the path origin) using their own VR equipment in a remote, unsupervised setting. Locomotion was accomplished using two versions of the teleporting interface varying in availability of rotational self-motion cues. The size of the traveled path and the size of the surrounding virtual environment were also manipulated. Results from remote participants largely mirrored lab results, with overall better performance when rotational self-motion cues were available. Some differences also occurred, including a tendency for remote participants to rely less on nearby landmarks, perhaps due to increased competence with using the teleporting interface to update self-location. This replication study provides insight for VR researchers on aspects of lab studies that may or may not replicate remotely.},
  archive      = {J_TVCG},
  author       = {Jonathan W. Kelly and Melynda Hoover and Taylor A. Doty and Alex Renner and Moriah Zimmerman and Kimberly Knuth and Lucia A. Cherep and Stephen B. Gilbert},
  doi          = {10.1109/TVCG.2022.3150475},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2037-2046},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Remote research on locomotion interfaces for virtual reality: Replication of a lab-based study on teleporting interfaces},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FrictShoes: Providing multilevel nonuniform friction
feedback on shoes in VR. <em>TVCG</em>, <em>28</em>(5), 2026–2036. (<a
href="https://doi.org/10.1109/TVCG.2022.3150492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many haptic feedback methods have been proposed to enhance realism in virtual reality (VR). However, friction on the feet in VR, which renders feedback as if walking on different terrains or ground textures or stepping on objects is still less explored. Herein, we propose a wearable device, FrictShoes a pair of foot accessories, to provide multilevel nonuniform friction feedback to feet. This is achieved by the independent functioning of six brakes on six wheels underneath each FrictShoe, which allows the friction levels of the wheels from each to be either matched or to vary. We conducted a magnitude estimation study to understand users&#39; distinguishability of friction force magnitudes (or levels). Based on the results, we performed an exploratory study to realize how users adjust and map the multilevel nonuniform friction patterns to common VR terrains or ground textures. Finally, a VR experience study was conducted to evaluate the performance of the proposed multilevel nonuniform friction feedback to the feet in VR experiences.},
  archive      = {J_TVCG},
  author       = {Chih-An Tsao and Tzu-Chun Wu and Hsin-Ruey Tsai and Tzu-Yun Wei and Fang-Ying Liao and Sean Chapman and Bing-Yu Chen},
  doi          = {10.1109/TVCG.2022.3150492},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2026-2036},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FrictShoes: Providing multilevel nonuniform friction feedback on shoes in VR},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The effect of context switching, focal switching distance,
binocular and monocular viewing, and transient focal blur on human
performance in optical see-through augmented reality. <em>TVCG</em>,
<em>28</em>(5), 2014–2025. (<a
href="https://doi.org/10.1109/TVCG.2022.3150503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In optical see-through augmented reality (AR), information is often distributed between real and virtual contexts, and often appears at different distances from the user. To integrate information, users must repeatedly switch context and change focal distance. If the user&#39;s task is conducted under time pressure, they may attempt to integrate information while their eye is still changing focal distance, a phenomenon we term transient focal blur . Previously, Gabbard, Mehra, and Swan (2018) examined these issues, using a text-based visual search task on a one-eye optical see-through AR display. This paper reports an experiment that partially replicates and extends this task on a custom-built AR Haploscope. The experiment examined the effects of context switching, focal switching distance, binocular and monocular viewing, and transient focal blur on task performance and eye fatigue. Context switching increased eye fatigue but did not decrease performance. Increasing focal switching distance increased eye fatigue and decreased performance. Monocular viewing also increased eye fatigue and decreased performance. The transient focal blur effect resulted in additional performance decrements, and is an addition to knowledge about AR user interface design issues.},
  archive      = {J_TVCG},
  author       = {Mohammed Safayet Arefin and Nate Phillips and Alexander Plopski and Joseph L. Gabbard and J. Edward Swan},
  doi          = {10.1109/TVCG.2022.3150503},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2014-2025},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The effect of context switching, focal switching distance, binocular and monocular viewing, and transient focal blur on human performance in optical see-through augmented reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ScanGAN360: A generative model of realistic scanpaths for
360° images. <em>TVCG</em>, <em>28</em>(5), 2003–2013. (<a
href="https://doi.org/10.1109/TVCG.2022.3150502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding and modeling the dynamics of human gaze behavior in 360° environments is crucial for creating, improving, and developing emerging virtual reality applications. However, recruiting human observers and acquiring enough data to analyze their behavior when exploring virtual environments requires complex hardware and software setups, and can be time-consuming. Being able to generate virtual observers can help overcome this limitation, and thus stands as an open problem in this medium. Particularly, generative adversarial approaches could alleviate this challenge by generating a large number of scanpaths that reproduce human behavior when observing new scenes, essentially mimicking virtual observers. However, existing methods for scanpath generation do not adequately predict realistic scanpaths for 360° images. We present ScanGAN360, a new generative adversarial approach to address this problem. We propose a novel loss function based on dynamic time warping and tailor our network to the specifics of 360° images. The quality of our generated scanpaths outperforms competing approaches by a large margin, and is almost on par with the human baseline. ScanGAN360 allows fast simulation of large numbers of virtual observers, whose behavior mimics real users, enabling a better understanding of gaze behavior, facilitating experimentation, and aiding novel applications in virtual reality and beyond.},
  archive      = {J_TVCG},
  author       = {Daniel Martin and Ana Serrano and Alexander W. Bergman and Gordon Wetzstein and Belen Masia},
  doi          = {10.1109/TVCG.2022.3150502},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2003-2013},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ScanGAN360: A generative model of realistic scanpaths for 360° images},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synthesizing personalized construction safety training
scenarios for VR training. <em>TVCG</em>, <em>28</em>(5), 1993–2002. (<a
href="https://doi.org/10.1109/TVCG.2022.3150510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Construction industry has the largest number of preventable fatal injuries, providing effective safety training practices can play a significant role in reducing the number of fatalities. Building on recent advancements in virtual reality-based training, we devised a novel approach to synthesize construction safety training scenarios to train users on how to proficiently inspect the potential hazards on construction sites in virtual reality. Given the training specifications such as individual training preferences and target training time, we synthesize personalized VR training scenarios through an optimization approach. We validated our approach by conducting user studies where users went through our personalized guidance VR training, free exploration VR training, or slides training. Results suggest that personalized guidance VR training approach can more effectively improve users&#39; construction hazard inspection skills.},
  archive      = {J_TVCG},
  author       = {Wanwan Li and Haikun Huang and Tomay Solomon and Behzad Esmaeili and Lap-Fai Yu},
  doi          = {10.1109/TVCG.2022.3150510},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {1993-2002},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Synthesizing personalized construction safety training scenarios for VR training},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SEAR: Scaling experiences in multi-user augmented reality.
<em>TVCG</em>, <em>28</em>(5), 1982–1992. (<a
href="https://doi.org/10.1109/TVCG.2022.3150467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present the design, implementation, and evaluation of SEAR, a collaborative framework for Scaling Experiences in multi-user Augmented Reality (AR). Most AR systems benefit from computer vision (CV) algorithms to detect, classify, or recognize physical objects for augmentation. A widely used acceleration method for mobile AR is to offload the compute-intensive tasks ( e.g. , CV algorithms) to the network edge. However, we show that the end-to-end latency, an important metric of mobile AR, may dramatically increase when offloading AR tasks from a large number of concurrent users to the edge. SEAR tackles this scalability issue through the innovation of a lightweight collaborative local caching scheme. Our key observation is that nearby AR users may share some common interests, and may even have overlapped views to augment ( e.g. , when playing a multi-user AR game). Thus, SEAR opportunistically exchanges the results of offloaded AR tasks among users when feasible and leverages compute resources on mobile devices to relieve, if necessary, the edge workload by intelligently reusing these results. We build a prototype of SEAR to demonstrate its efficacy in scaling AR experiences. We conduct extensive evaluations through both real-world experiments and trace-driven simulations. We observe that SEAR not only reduces the end-to-end latency, by up to 130×, compared to the state-of-the-art adaptive edge offloading scheme, but also achieves high object-recognition accuracy for mobile AR.},
  archive      = {J_TVCG},
  author       = {Wenxiao Zhang and Bo Han and Pan Hui},
  doi          = {10.1109/TVCG.2022.3150467},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {1982-1992},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SEAR: Scaling experiences in multi-user augmented reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Words of estimative correlation: Studying verbalizations of
scatterplots. <em>TVCG</em>, <em>28</em>(4), 1967–1981. (<a
href="https://doi.org/10.1109/TVCG.2020.3023537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language and visualization are being increasingly deployed together for supporting data analysis in different ways, from multimodal interaction to enriched data summaries and insights. Yet, researchers still lack systematic knowledge on how viewers verbalize their interpretations of visualizations, and how they interpret verbalizations of visualizations in such contexts. We describe two studies aimed at identifying characteristics of data and charts that are relevant in such tasks. The first study asks participants to verbalize what they see in scatterplots that depict various levels of correlations. The second study then asks participants to choose visualizations that match a given verbal description of correlation. We extract key concepts from responses, organize them in a taxonomy and analyze the categorized responses. We observe that participants use a wide range of vocabulary across all scatterplots, but particular concepts are preferred for higher levels of correlation. A comparison between the studies reveals the ambiguity of some of the concepts. We discuss how the results could inform the design of multimodal representations aligned with the data and analytical tasks, and present a research roadmap to deepen the understanding about visualizations and natural language.},
  archive      = {J_TVCG},
  author       = {Rafael Henkin and Cagatay Turkay},
  doi          = {10.1109/TVCG.2020.3023537},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1967-1981},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Words of estimative correlation: Studying verbalizations of scatterplots},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncertainty visualization of 2D morse complex ensembles
using statistical summary maps. <em>TVCG</em>, <em>28</em>(4),
1955–1966. (<a href="https://doi.org/10.1109/TVCG.2020.3022359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Morse complexes are gradient-based topological descriptors with close connections to Morse theory. They are widely applicable in scientific visualization as they serve as important abstractions for gaining insights into the topology of scalar fields. Data uncertainty inherent to scalar fields due to randomness in their acquisition and processing, however, limits our understanding of Morse complexes as structural abstractions. We, therefore, explore uncertainty visualization of an ensemble of 2D Morse complexes that arises from scalar fields coupled with data uncertainty. We propose several statistical summary maps as new entities for quantifying structural variations and visualizing positional uncertainties of Morse complexes in ensembles. Specifically, we introduce three types of statistical summary maps – the probabilistic map , the significance map , and the survival map – to characterize the uncertain behaviors of gradient flows. We demonstrate the utility of our proposed approach using wind, flow, and ocean eddy simulation datasets.},
  archive      = {J_TVCG},
  author       = {Tushar M. Athawale and Dan Maljovec and Lin Yan and Chris R. Johnson and Valerio Pascucci and Bei Wang},
  doi          = {10.1109/TVCG.2020.3022359},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1955-1966},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Uncertainty visualization of 2D morse complex ensembles using statistical summary maps},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards better caption supervision for object detection.
<em>TVCG</em>, <em>28</em>(4), 1941–1954. (<a
href="https://doi.org/10.1109/TVCG.2021.3138933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As training high-performance object detectors requires expensive bounding box annotations, recent methods resort to free-available image captions. However, detectors trained on caption supervision perform poorly because captions are usually noisy and cannot provide precise location information. To tackle this issue, we present a visual analysis method, which tightly integrates caption supervision with object detection to mutually enhance each other. In particular, object labels are first extracted from captions, which are utilized to train the detectors. Then, the objects detected from images are fed into caption supervision for further improvement. To effectively loop users into the object detection process, a node-link-based set visualization supported by a multi-type relational co-clustering algorithm is developed to explain the relationships between the extracted labels and the images with detected objects. The co-clustering algorithm clusters labels and images simultaneously by utilizing both their representations and their relationships. Quantitative evaluations and a case study are conducted to demonstrate the efficiency and effectiveness of the developed method in improving the performance of object detectors.},
  archive      = {J_TVCG},
  author       = {Changjian Chen and Jing Wu and Xiaohan Wang and Shouxing Xiang and Song-Hai Zhang and Qifeng Tang and Shixia Liu},
  doi          = {10.1109/TVCG.2021.3138933},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1941-1954},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards better caption supervision for object detection},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Touch and beyond: Comparing physical and virtual reality
visualizations. <em>TVCG</em>, <em>28</em>(4), 1930–1940. (<a
href="https://doi.org/10.1109/TVCG.2020.3023336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We compare physical and virtual reality (VR) versions of simple data visualizations and explore how the addition of virtual annotation and filtering tools affects how viewers solve basic data analysis tasks. We report on two studies, inspired by previous examinations of data physicalizations. The first study examines differences in how viewers interact with physical hand-scale, virtual hand-scale, and virtual table-scale visualizations and the impact that the different forms had on viewer&amp;#x2019;s problem solving behavior. A second study examines how interactive annotation and filtering tools might support new modes of use that transcend the limitations of physical representations. Our results highlight challenges associated with virtual reality representations and hint at the potential of interactive annotation and filtering tools in VR visualizations.},
  archive      = {J_TVCG},
  author       = {Kurtis Danyluk and Teoman Ulusoy and Wei Wei and Wesley Willett},
  doi          = {10.1109/TVCG.2020.3023336},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1930-1940},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Touch and beyond: Comparing physical and virtual reality visualizations},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TimeTubesX: A query-driven visual exploration of observable,
photometric, and polarimetric behaviors of blazars. <em>TVCG</em>,
<em>28</em>(4), 1917–1929. (<a
href="https://doi.org/10.1109/TVCG.2020.3025090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blazars are celestial bodies of high interest to astronomers. In particular, through the analysis of photometric and polarimetric observations of blazars, astronomers aim to understand the physics of the blazar&#39;s relativistic jet. However, it is challenging to recognize correlations and time variations of the observed polarization, intensity, and color of the emitted light. In our prior study, we proposed TimeTubes to visualize a blazar dataset as a 3D volumetric tube. In this paper, we build primarily on the TimeTubes representation of blazar datasets to present a new visual analytics environment named TimeTubesX, into which we have integrated sophisticated feature and pattern detection techniques for effective location of observable and recurring time variation patterns in long-term, multi-dimensional datasets. Automatic feature extraction detects time intervals corresponding to well-known blazar behaviors. Dynamic visual querying allows users to search long-term observations for time intervals similar to a time interval of interest (query-by-example) or a sketch of temporal patterns (query-by-sketch). Users are also allowed to build up another visual query guided by the time interval of interest found in the previous process and refine the results. We demonstrate how TimeTubesX has been used successfully by domain experts for the detailed analysis of blazar datasets and report on the results.},
  archive      = {J_TVCG},
  author       = {Naoko Sawada and Makoto Uemura and Johanna Beyer and Hanspeter Pfister and Issei Fujishiro},
  doi          = {10.1109/TVCG.2020.3025090},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1917-1929},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TimeTubesX: A query-driven visual exploration of observable, photometric, and polarimetric behaviors of blazars},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synthesizing mesh deformation sequences with bidirectional
LSTM. <em>TVCG</em>, <em>28</em>(4), 1906–1916. (<a
href="https://doi.org/10.1109/TVCG.2020.3028961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesizing realistic 3D mesh deformation sequences is a challenging but important task in computer animation. To achieve this, researchers have long been focusing on shape analysis to develop new interpolation and extrapolation techniques. However, such techniques have limited learning capabilities and therefore often produce unrealistic deformation. Although there are already networks defined on individual meshes, deep architectures that operate directly on mesh sequences with temporal information remain unexplored due to the following major barriers: irregular mesh connectivity, rich temporal information, and varied deformation. To address these issues, we utilize convolutional neural networks defined on triangular meshes along with a shape deformation representation to extract useful features, followed by long short-term memory (LSTM) that iteratively processes the features. To fully respect the bidirectional nature of actions, we propose a new share-weight bidirectional scheme to better synthesize deformations. An extensive evaluation shows that our approach outperforms existing methods in sequence generation, both qualitatively and quantitatively.},
  archive      = {J_TVCG},
  author       = {Yi-Ling Qiao and Yu-Kun Lai and Hongbo Fu and Lin Gao},
  doi          = {10.1109/TVCG.2020.3028961},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1906-1916},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Synthesizing mesh deformation sequences with bidirectional LSTM},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time hair simulation with neural interpolation.
<em>TVCG</em>, <em>28</em>(4), 1894–1905. (<a
href="https://doi.org/10.1109/TVCG.2020.3029823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditionally, reduced hair simulation methods are either restricted to heuristic approximations or bound to specific hairstyles. We introduce the first CNN-integrated framework for simulating various hairstyles. The approach produces visually realistic hairs with an interactive speed. To address the technical challenges, our hair simulation pipeline is designed as a two-stage process. First, we present a fully-convolutional neural interpolator as the backbone generator to compute dynamic weights for guide hair interpolation. Then, we adopt a second generator to produce fine-scale displacements to enhance the hair details. We train the neural interpolator with a dedicated loss function and the displacement generator with an adversarial discriminator. Experimental results demonstrate that our method is effective, efficient, and superior to the state-of-the-art on a wide variety of hairstyles. We further propose a performance-driven digital avatar system and an interactive hairstyle editing tool to illustrate the practical applications.},
  archive      = {J_TVCG},
  author       = {Qing Lyu and Menglei Chai and Xiang Chen and Kun Zhou},
  doi          = {10.1109/TVCG.2020.3029823},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1894-1905},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Real-time hair simulation with neural interpolation},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the plausibility of virtual body animation features in
virtual reality. <em>TVCG</em>, <em>28</em>(4), 1880–1893. (<a
href="https://doi.org/10.1109/TVCG.2020.3025175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present two experiments to assess the relative impact of different levels of body animation fidelity on plausibility illusion (Psi). The first experiment presents a virtual character that is not controlled by the user ($n=13$n=13), while the second experiment presents a user-controlled virtual avatar ($n=24$n=24, all male). Psi concerns how realistic and coherent the events in a virtual environment look and feel and is part of Slater&amp;#x0027;s proposition of two orthogonal components of presence in virtual reality (VR). In the experiments, the face, hands, upper and lower bodies of the character or self-avatar were manipulated to present different degrees of animation fidelity, such as no animation, procedural animation, and motion captured animation. Participants started the experiment experiencing the best animation configuration. Then, animation features were reduced to limit the amount of captured information made available to the system. Participants had to move from this basic animation configuration towards a more complete one, and declare when the avatar animation realism felt equivalent to the initial and most complete configuration, which could happen before all animation features were maxed out. Participants in the self-avatar experiment were also asked to rate how each animation feature affected their sense of control of the virtual body. We found that a virtual body with upper and lower body animated using eight tracked rigid bodies and inverse kinematics (IK) was often perceived as equivalent to a professional capture pipeline relying on 53 markers. Compared to what standard VR kits in the market are offering, i.e., a tracked headset and two hand controllers, we found that foot tracking, followed by mouth animation and finger tracking, were the features that added the most to the sense of control of a self-representing avatar. In addition, these features were often among the first to be improved in both experiments.},
  archive      = {J_TVCG},
  author       = {Henrique Galvan Debarba and Sylvain Chagu&amp;#x00E9; and Caecilia Charbonnier},
  doi          = {10.1109/TVCG.2020.3025175},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1880-1893},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {On the plausibility of virtual body animation features in virtual reality},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MulayCap: Multi-layer human performance capture using a
monocular video camera. <em>TVCG</em>, <em>28</em>(4), 1862–1879. (<a
href="https://doi.org/10.1109/TVCG.2020.3027763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce MulayCap, a novel human performance capture method using a monocular video camera without the need for pre-scanning. The method uses &amp;#x201C;multi-layer&amp;#x201D; representations for geometry reconstruction and texture rendering, respectively. For geometry reconstruction, we decompose the clothed human into multiple geometry layers, namely a body mesh layer and a garment piece layer. The key technique behind is a Garment-from-Video (GfV) method for optimizing the garment shape and reconstructing the dynamic cloth to fit the input video sequence, based on a cloth simulation model which is effectively solved with gradient descent. For texture rendering, we decompose each input image frame into a shading layer and an albedo layer, and propose a method for fusing a fixed albedo map and solving for detailed garment geometry using the shading layer. Compared with existing single view human performance capture systems, our &amp;#x201C;multi-layer&amp;#x201D; approach bypasses the tedious and time consuming scanning step for obtaining a human specific mesh template. Experimental results demonstrate that MulayCap produces realistic rendering of dynamically changing details that has not been achieved in any previous monocular video camera systems. Benefiting from its fully semantic modeling, MulayCap can be applied to various important editing applications, such as cloth editing, re-targeting, relighting, and AR applications.},
  archive      = {J_TVCG},
  author       = {Zhaoqi Su and Weilin Wan and Tao Yu and Lingjie Liu and Lu Fang and Wenping Wang and Yebin Liu},
  doi          = {10.1109/TVCG.2020.3027763},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1862-1879},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MulayCap: Multi-layer human performance capture using a monocular video camera},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixed labeling: Integrating internal and external labels.
<em>TVCG</em>, <em>28</em>(4), 1848–1861. (<a
href="https://doi.org/10.1109/TVCG.2020.3027368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present an algorithm capable of mixed labeling of 2D and 3D objects. In mixed labeling, the given objects are labeled with both internal labels placed (at least partially) over the objects and external labels placed in the space around the objects and connected with the labeled objects with straight-line leaders. The proposed algorithm determines the position and type of each label based on the user-specified ambiguity threshold and eliminates overlaps between the labels, as well as between the internal labels and the straight-line leaders of external labels. The algorithm is a screen-space technique; it operates in an image where the 2D objects or projected 3D objects are encoded. In other words, we can use the algorithm whenever we can render the objects to an image, which makes the algorithm fit for use in many domains. The algorithm operates in real-time, giving the results immediately. Finally, we present results from an expert evaluation, in which a professional illustrator has evaluated the label layouts produced with the proposed algorithm.},
  archive      = {J_TVCG},
  author       = {Ladislav Čmolík and Václav Pavlovec and Hsiang-Yun Wu and Martin Nöllenburg},
  doi          = {10.1109/TVCG.2020.3027368},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1848-1861},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mixed labeling: Integrating internal and external labels},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low rank matrix approximation for 3D geometry filtering.
<em>TVCG</em>, <em>28</em>(4), 1835–1847. (<a
href="https://doi.org/10.1109/TVCG.2020.3026785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a robust normal estimation method for both point clouds and meshes using a low rank matrix approximation algorithm. First, we compute a local isotropic structure for each point and find its similar, non-local structures that we organize into a matrix. We then show that a low rank matrix approximation algorithm can robustly estimate normals for both point clouds and meshes. Furthermore, we provide a new filtering method for point cloud data to smooth the position data to fit the estimated normals. We show the applications of our method to point cloud filtering, point set upsampling, surface reconstruction, mesh denoising, and geometric texture removal. Our experiments show that our method generally achieves better results than existing methods.},
  archive      = {J_TVCG},
  author       = {Xuequan Lu and Scott Schaefer and Jun Luo and Lizhuang Ma and Ying He},
  doi          = {10.1109/TVCG.2020.3026785},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1835-1847},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Low rank matrix approximation for 3D geometry filtering},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight bilateral convolutional neural networks for
interactive single-bounce diffuse indirect illumination. <em>TVCG</em>,
<em>28</em>(4), 1824–1834. (<a
href="https://doi.org/10.1109/TVCG.2020.3023129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physically correct, noise-free global illumination is crucial in physically-based rendering, but often takes a long time to compute. Recent approaches have exploited sparse sampling and filtering to accelerate this process but still cannot achieve interactive performance. It is partly due to the time-consuming ray sampling even at 1 sample per pixel, and partly because of the complexity of deep neural networks. To address this problem, we propose a novel method to generate plausible single-bounce indirect illumination for dynamic scenes in interactive framerates. In our method, we first compute direct illumination and then use a lightweight neural network to predict screen space indirect illumination. Our neural network is designed explicitly with bilateral convolution layers and takes only essential information as input (direct illumination, surface normals, and 3D positions). Also, our network maintains the coherence between adjacent image frames efficiently without heavy recurrent connections. Compared to state-of-the-art works, our method produces single-bounce indirect illumination of dynamic scenes with higher quality and better temporal coherence and runs at interactive framerates.},
  archive      = {J_TVCG},
  author       = {Hanggao Xin and Shaokun Zheng and Kun Xu and Ling-Qi Yan},
  doi          = {10.1109/TVCG.2020.3023129},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1824-1834},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Lightweight bilateral convolutional neural networks for interactive single-bounce diffuse indirect illumination},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning-based inverse bi-scale material fitting from
tabular BRDFs. <em>TVCG</em>, <em>28</em>(4), 1810–1823. (<a
href="https://doi.org/10.1109/TVCG.2020.3026021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relating small-scale structures to large-scale appearance is a key element in material appearance design. Bi-scale material design requires finding small-scale structures – meso-scale geometry and micro-scale BRDFs – that produce a desired large-scale appearance expressed as a macro-scale BRDF. The adjustment of small-scale geometry and reflectances to achieve a desired appearance can become a tedious trial-and-error process. We present a learning-based solution to fit a target macro-scale BRDF with a combination of a meso-scale geometry and micro-scale BRDF. We confront challenges in representation at both scales. At the large scale we need macro-scale BRDFs that are both compact and expressive. At the small scale we need diverse combinations of geometric patterns and potentially spatially varying micro-BRDFs. For large-scale macro-BRDFs, we propose a novel 2D subset of a tabular BRDF representation that well preserves important appearance features for learning. For small-scale details, we represent geometries and BRDFs in different categories with different physical parameters to define multiple independent continuous search spaces. To build the mapping between large-scale macro-BRDFs and small-scale details, we propose an end-to-end model that takes the subset BRDF as input and performs classification and parameter estimation on small-scale details to find an accurate reconstruction. Compared with other fitting methods, our learning-based solution provides higher reconstruction accuracy and covers a wider gamut of appearance.},
  archive      = {J_TVCG},
  author       = {Weiqi Shi and Julie Dorsey and Holly Rushmeier},
  doi          = {10.1109/TVCG.2020.3026021},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1810-1823},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Learning-based inverse bi-scale material fitting from tabular BRDFs},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HeadJoystick: Improving flying in VR using a novel
leaning-based interface. <em>TVCG</em>, <em>28</em>(4), 1792–1809. (<a
href="https://doi.org/10.1109/TVCG.2020.3025084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flying in virtual reality (VR) using standard handheld controllers can be cumbersome and contribute to unwanted side effects such as motion sickness and disorientation. This article investigates a novel hands-free flying interface&amp;#x2014;HeadJoystick, where the user moves their head similar to a joystick handle toward the target direction to control virtual translation velocity. The user sits on a regular office swivel chair and rotates it physically to control virtual rotation using 1:1 mapping. We evaluated short-term (Study 1) and extended usage effects through repeated usage (Study 2) of the HeadJoystick versus handheld interfaces in two within-subject studies, where participants flew through a sequence of increasingly difficult tunnels in the sky. Using the HeadJoystick instead of handheld interfaces improved both user experience and performance, in terms of accuracy, precision, ease of learning, ease of use, usability, long-term use, presence, immersion, sensation of self-motion, workload, and enjoyment in both studies. These findings demonstrate the benefits of using leaning-based interfaces for VR flying and potentially similar telepresence applications such as remote flight with quadcopter drones. From a theoretical perspective, we also show how leaning-based motion cueing interacts with full physical rotation to improve user experience and performance compared to the gamepad.},
  archive      = {J_TVCG},
  author       = {Abraham M. Hashemian and Matin Lotfaliei and Ashu Adhikari and Ernst Kruijff and Bernhard E. Riecke},
  doi          = {10.1109/TVCG.2020.3025084},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1792-1809},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HeadJoystick: Improving flying in VR using a novel leaning-based interface},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FeatureEnVi: Visual analytics for feature engineering using
stepwise selection and semi-automatic extraction approaches.
<em>TVCG</em>, <em>28</em>(4), 1773–1791. (<a
href="https://doi.org/10.1109/TVCG.2022.3141040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The machine learning (ML) life cycle involves a series of iterative steps, from the effective gathering and preparation of the data—including complex feature engineering processes—to the presentation and improvement of results, with various algorithms to choose from in every step. Feature engineering in particular can be very beneficial for ML, leading to numerous improvements such as boosting the predictive results, decreasing computational times, reducing excessive noise, and increasing the transparency behind the decisions taken during the training. Despite that, while several visual analytics tools exist to monitor and control the different stages of the ML life cycle (especially those related to data and algorithms), feature engineering support remains inadequate. In this paper, we present FeatureEnVi, a visual analytics system specifically designed to assist with the feature engineering process. Our proposed system helps users to choose the most important feature, to transform the original features into powerful alternatives, and to experiment with different feature generation combinations. Additionally, data space slicing allows users to explore the impact of features on both local and global scales. FeatureEnVi utilizes multiple automatic feature selection techniques; furthermore, it visually guides users with statistical evidence about the influence of each feature (or subsets of features). The final outcome is the extraction of heavily engineered features, evaluated by multiple validation metrics. The usefulness and applicability of FeatureEnVi are demonstrated with two use cases and a case study. We also report feedback from interviews with two ML experts and a visualization researcher who assessed the effectiveness of our system.},
  archive      = {J_TVCG},
  author       = {Angelos Chatzimparmpas and Rafael M. Martins and Kostiantyn Kucher and Andreas Kerren},
  doi          = {10.1109/TVCG.2022.3141040},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1773-1791},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FeatureEnVi: Visual analytics for feature engineering using stepwise selection and semi-automatic extraction approaches},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FAME: 3D shape generation via functionality-aware model
evolution. <em>TVCG</em>, <em>28</em>(4), 1758–1772. (<a
href="https://doi.org/10.1109/TVCG.2020.3029759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a modeling tool which can evolve a set of 3D objects in a functionality-aware manner. Our goal is for the evolution to generate large and diverse sets of plausible 3D objects for data augmentation, constrained modeling, as well as open-ended exploration to possibly inspire new designs. Starting with an initial population of 3D objects belonging to one or more functional categories, we evolve the shapes through part recombination to produce generations of hybrids or crossbreeds between parents from the heterogeneous shape collection. Evolutionary selection of offsprings is guided both by a functional plausibility score derived from functionality analysis of shapes in the initial population and user preference, as in a design gallery. Since cross-category hybridization may result in offsprings not belonging to any of the known functional categories, we develop a means for functionality partial matching to evaluate functional plausibility on partial shapes. We show a variety of plausible hybrid shapes generated by our functionality-aware model evolution, which can complement existing datasets as training data and boost the performance of contemporary data-driven segmentation schemes, especially in challenging cases. Our tool supports constrained modeling, allowing users to restrict or steer the model evolution with functionality labels. At the same time, unexpected yet functional object prototypes can emerge during open-ended exploration owing to structure breaking when evolving a heterogeneous collection.},
  archive      = {J_TVCG},
  author       = {Yanran Guan and Han Liu and Kun Liu and Kangxue Yin and Ruizhen Hu and Oliver van Kaick and Yan Zhang and Ersin Yumer and Nathan Carr and Radomir Mech and Hao Zhang},
  doi          = {10.1109/TVCG.2020.3029759},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1758-1772},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FAME: 3D shape generation via functionality-aware model evolution},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accurate dynamic SLAM using CRF-based long-term consistency.
<em>TVCG</em>, <em>28</em>(4), 1745–1757. (<a
href="https://doi.org/10.1109/TVCG.2020.3028218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate camera pose estimation is essential and challenging for real world dynamic 3D reconstruction and augmented reality applications. In this article, we present a novel RGB-D SLAM approach for accurate camera pose tracking in dynamic environments. Previous methods detect dynamic components only across a short time-span of consecutive frames. Instead, we provide a more accurate dynamic 3D landmark detection method, followed by the use of long-term consistency via conditional random fields, which leverages long-term observations from multiple frames. Specifically, we first introduce an efficient initial camera pose estimation method based on distinguishing dynamic from static points using graph-cut RANSAC. These static/dynamic labels are used as priors for the unary potential in the conditional random fields, which further improves the accuracy of dynamic 3D landmark detection. Evaluation using the TUM and Bonn RGB-D dynamic datasets shows that our approach significantly outperforms state-of-the-art methods, providing much more accurate camera trajectory estimation in a variety of highly dynamic environments. We also show that dynamic 3D reconstruction can benefit from the camera poses estimated by our RGB-D SLAM approach.},
  archive      = {J_TVCG},
  author       = {Zheng-Jun Du and Shi-Sheng Huang and Tai-Jiang Mu and Qunhe Zhao and Ralph R. Martin and Kun Xu},
  doi          = {10.1109/TVCG.2020.3028218},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1745-1757},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Accurate dynamic SLAM using CRF-based long-term consistency},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A probability density-based visual analytics approach to
forecast bias calibration. <em>TVCG</em>, <em>28</em>(4), 1732–1744. (<a
href="https://doi.org/10.1109/TVCG.2020.3025072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biases inevitably occur in numerical weather prediction (NWP) due to an idealized numerical assumption for modeling chaotic atmospheric systems. Therefore, the rapid and accurate identification and calibration of biases is crucial for NWP in weather forecasting. Conventional approaches, such as various analog post-processing forecast methods, have been designed to aid in bias calibration. However, these approaches fail to consider the spatiotemporal correlations of forecast bias, which can considerably affect calibration efficacy. In this article, we propose a novel bias pattern extraction approach based on forecasting-observation probability density by merging historical forecasting and observation datasets. Given a spatiotemporal scope, our approach extracts and fuses bias patterns and automatically divides regions with similar bias patterns. Termed BicaVis, our spatiotemporal bias pattern visual analytics system is proposed to assist experts in drafting calibration curves on the basis of these bias patterns. To verify the effectiveness of our approach, we conduct two case studies with real-world reanalysis datasets. The feedback collected from domain experts confirms the efficacy of our approach.},
  archive      = {J_TVCG},
  author       = {Renpei Huang and Quan Li and Li Chen and Xiaoru Yuan},
  doi          = {10.1109/TVCG.2020.3025072},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1732-1744},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A probability density-based visual analytics approach to forecast bias calibration},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A framework for evaluating dashboards in healthcare.
<em>TVCG</em>, <em>28</em>(4), 1715–1731. (<a
href="https://doi.org/10.1109/TVCG.2022.3147154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of ‘information overload’, effective information provision is essential for enabling rapid response and critical decision making. In making sense of diverse information sources, dashboards have become an indispensable tool, providing fast, effective, adaptable, and personalized access to information for professionals and the general public alike. However, these objectives place heavy requirements on dashboards as information systems in usability and effective design. Understanding these issues is challenging given the absence of consistent and comprehensive approaches to dashboard evaluation. In this article we systematically review literature on dashboard implementation in healthcare, where dashboards have been employed widely, and where there is widespread interest for improving the current state of the art, and subsequently analyse approaches taken towards evaluation. We draw upon consolidated dashboard literature and our own observations to introduce a general definition of dashboards which is more relevant to current trends, together with seven evaluation scenarios - task performance, behaviour change, interaction workflow, perceived engagement, potential utility, algorithm performance and system implementation. These scenarios distinguish different evaluation purposes which we illustrate through measurements, example studies, and common challenges in evaluation study design. We provide a breakdown of each evaluation scenario, and highlight some of the more subtle questions. We demonstrate the use of the proposed framework by a design study guided by this framework. We conclude by comparing this framework with existing literature, outlining a number of active discussion points and a set of dashboard evaluation best practices for the academic, clinical and software development communities alike.},
  archive      = {J_TVCG},
  author       = {Mengdie Zhuang and David Concannon and Ed Manley},
  doi          = {10.1109/TVCG.2022.3147154},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1715-1731},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A framework for evaluating dashboards in healthcare},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Editorial. <em>TVCG</em>, <em>28</em>(4), 1714. (<a
href="https://doi.org/10.1109/TVCG.2022.3149021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_TVCG},
  author       = {Klaus Mueller},
  doi          = {10.1109/TVCG.2022.3149021},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1714},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Editorial},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Surface remeshing: A systematic literature review of methods
and research directions. <em>TVCG</em>, <em>28</em>(3), 1680–1713. (<a
href="https://doi.org/10.1109/TVCG.2020.3016645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triangle meshes are used in many important shape-related applications including geometric modeling, animation production, system simulation, and visualization. However, these meshes are typically generated in raw form with several defects and poor-quality elements, obstructing them from practical application. Over the past decades, different surface remeshing techniques have been presented to improve these poor-quality meshes prior to the downstream utilization. A typical surface remeshing algorithm converts an input mesh into a higher quality mesh with consideration of given quality requirements as well as an acceptable approximation to the input mesh. In recent years, surface remeshing has gained significant attention from researchers and engineers, and several remeshing algorithms have been proposed. However, there has been no survey article on remeshing methods in general with a defined search strategy and article selection mechanism covering the recent approaches in surface remeshing domain with a good connection to classical approaches. In this article, we present a survey on surface remeshing techniques, classifying all collected articles in different categories and analyzing specific methods with their advantages, disadvantages, and possible future improvements. Following the systematic literature review methodology, we define step-by-step guidelines throughout the review process, including search strategy, literature inclusion/exclusion criteria, article quality assessment, and data extraction. With the aim of literature collection and classification based on data extraction, we summarized collected articles, considering the key remeshing objectives, the way the mesh quality is defined and improved, and the way their techniques are compared with other previous methods. Remeshing objectives are described by angle range control, feature preservation, error control, valence optimization, and remeshing compatibility. The metrics used in the literature for the evaluation of surface remeshing algorithms are discussed. Meshing techniques are compared with other related methods via a comprehensive table with indices of the method name, the remeshing challenge met and solved, the category the method belongs to, and the year of publication. We expect this survey to be a practical reference for surface remeshing in terms of literature classification, method analysis, and future prospects.},
  archive      = {J_TVCG},
  author       = {Dawar Khan and Alexander Plopski and Yuichiro Fujimoto and Masayuki Kanbara and Gul Jabeen and Yongjie Jessica Zhang and Xiaopeng Zhang and Hirokazu Kato},
  doi          = {10.1109/TVCG.2020.3016645},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1680-1713},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Surface remeshing: A systematic literature review of methods and research directions},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Showing data about people: A design space of
anthropographics. <em>TVCG</em>, <em>28</em>(3), 1661–1679. (<a
href="https://doi.org/10.1109/TVCG.2020.3023013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When showing data about people, visualization designers and data journalists often use design strategies that presumably help the audience relate to those people. The term anthropographics has been recently coined to refer to this practice and the resulting visualizations. Anthropographics is a rich and growing area, but the work so far has remained scattered. Despite preliminary empirical work and a few web essays written by practitioners, there is a lack of clear language for thinking about and communicating about anthropographics. We address this gap by introducing a conceptual framework and a design space for anthropographics. Our design space consists of seven elementary design dimensions that can be reasonably hypothesized to have some effect on prosocial feelings or behavior. It extends a previous design space and is informed by an analysis of 105 visualizations collected from newspapers, websites, and research articles. We use our conceptual framework and design space to discuss trade-offs, common design strategies, as well as future opportunities for design and research in the area of anthropographics.},
  archive      = {J_TVCG},
  author       = {Luiz Morais and Yvonne Jansen and Nazareno Andrade and Pierre Dragicevic},
  doi          = {10.1109/TVCG.2020.3023013},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1661-1679},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Showing data about people: A design space of anthropographics},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visualizing movement control optimization landscapes.
<em>TVCG</em>, <em>28</em>(3), 1648–1660. (<a
href="https://doi.org/10.1109/TVCG.2020.3018187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large body of animation research focuses on optimization of movement control, either as action sequences or policy parameters. However, as closed-form expressions of the objective functions are often not available, our understanding of the optimization problems is limited. Building on recent work on analyzing neural network training, we contribute novel visualizations of high-dimensional control optimization landscapes; this yields insights into why control optimization is hard and why common practices like early termination and spline-based action parameterizations make optimization easier. For example, our experiments show how trajectory optimization can become increasingly ill-conditioned with longer trajectories, but parameterizing control as partial target states—e.g., target angles converted to torques using a PD-controller—can act as an efficient preconditioner. Both our visualizations and quantitative empirical data also indicate that neural network policy optimization scales better than trajectory optimization for long planning horizons. Our work advances the understanding of movement optimization and our visualizations should also provide value in educational use.},
  archive      = {J_TVCG},
  author       = {Perttu Hämäläinen and Juuso Toikka and Amin Babadi and C. Karen Liu},
  doi          = {10.1109/TVCG.2020.3018187},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1648-1660},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualizing movement control optimization landscapes},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VERTIGo: A visual platform for querying and exploring large
multilayer networks. <em>TVCG</em>, <em>28</em>(3), 1634–1647. (<a
href="https://doi.org/10.1109/TVCG.2021.3067820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real world data can be modeled by a graph with a set of nodes interconnected to each other by multiple relationships. Such a rich graph is called multilayer graph or network. Providing useful visualization tools to support the query process for such graphs is challenging. Although many approaches have addressed the visual query construction, few efforts have been done to provide a contextualized exploration of query results and suggestion strategies to refine the original query. This is due to several issues such as i) the size of the graphs ii) the large number of retrieved results and iii) the way they can be organized to facilitate their exploration. In this article, we present VERTIGo , a novel visual platform to query, explore and support the analysis of large multilayer graphs. VERTIGo provides coordinated views to navigate and explore the large set of retrieved results at different granularity levels. In addition, the proposed system supports the refinement of the query by visual suggestions to guide the user through the exploration process. Two examples and a user study demonstrate how VERTIGo can be used to perform visual analysis (query, exploration, and suggestion) on real world multilayer networks.},
  archive      = {J_TVCG},
  author       = {Erick Cuenca and Arnaud Sallaberry and Dino Ienco and Pascal Poncelet},
  doi          = {10.1109/TVCG.2021.3067820},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1634-1647},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VERTIGo: A visual platform for querying and exploring large multilayer networks},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Placement retargeting of virtual avatars to dissimilar
indoor environments. <em>TVCG</em>, <em>28</em>(3), 1619–1633. (<a
href="https://doi.org/10.1109/TVCG.2020.3018458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapidly developing technologies are realizing a 3D telepresence, in which geographically separated users can interact with each other through their virtual avatars. In this article, we present novel methods to determine the avatar’s position in an indoor space to preserve the semantics of the user’s position in a dissimilar indoor space with different space configurations and furniture layouts. To this end, we first perform a user survey on the preferred avatar placements for various indoor configurations and user placements, and identify a set of related attributes, including interpersonal relation, visual attention, pose, and spatial characteristics, and quantify these attributes with a set of features. By using the obtained dataset and identified features, we train a neural network that predicts the similarity between two placements. Next, we develop an avatar placement method that preserves the semantics of the placement of the remote user in a different space as much as possible. We show the effectiveness of our methods by implementing a prototype AR-based telepresence system and user evaluations.},
  archive      = {J_TVCG},
  author       = {Leonard Yoon and Dongseok Yang and Jaehyun Kim and ChoongHo Chung and Sung-Hee Lee},
  doi          = {10.1109/TVCG.2020.3018458},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1619-1633},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Placement retargeting of virtual avatars to dissimilar indoor environments},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Parallax free registration for augmented reality optical
see-through displays in the peripersonal space. <em>TVCG</em>,
<em>28</em>(3), 1608–1618. (<a
href="https://doi.org/10.1109/TVCG.2020.3021534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Egocentric augmented reality (AR) interfaces are quickly becoming a key asset for assisting high precision activities in the peripersonal space in several application fields. In these applications, accurate and robust registration of computer-generated information to the real scene is hard to achieve with traditional Optical See-Through (OST) displays given that it relies on the accurate calibration of the combined eye-display projection model. The calibration is required to efficiently estimate the projection parameters of the pinhole model that encapsulate the optical features of the display and whose values vary according to the position of the user’s eye. In this article, we describe an approach that prevents any parallax-related AR misregistration at a pre-defined working distance in OST displays with infinity focus; our strategy relies on the use of a magnifier placed in front of the OST display, and features a proper parameterization of the virtual rendering camera achieved through a dedicated calibration procedure that accounts for the contribution of the magnifier. We model the registration error due to the viewpoint parallax outside the ideal working distance. Finally, we validate our strategy on a OST display, and we show that sub-millimetric registration accuracy can be achieved for working distances of $\pm 100$ mm around the focal length of the magnifier.},
  archive      = {J_TVCG},
  author       = {Vincenzo Ferrari and Nadia Cattari and Umberto Fontana and Fabrizio Cutolo},
  doi          = {10.1109/TVCG.2020.3021534},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1608-1618},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Parallax free registration for augmented reality optical see-through displays in the peripersonal space},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling movement-induced errors in AC electromagnetic
trackers. <em>TVCG</em>, <em>28</em>(3), 1597–1607. (<a
href="https://doi.org/10.1109/TVCG.2020.3019700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Error analysis of electromagnetic motion tracking systems is of growing interest to many researchers. Under sensor movement, it is logical to presume that the error in position and orientation measurements will increase due to the linearization used in the algorithms, among other reasons. In this article, we analyze theoretically the error, that results from linearization, in position measurement of the Polhemus tracking system for a moving sensor. We derive formulas to estimate this error in terms of the sensor position and speed. Then, we verify these formulas by numerical simulations.},
  archive      = {J_TVCG},
  author       = {Mutaz Tuffaha and Øyvind Stavdahl and Ann-Katrin Stensdotter},
  doi          = {10.1109/TVCG.2020.3019700},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1597-1607},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Modeling movement-induced errors in AC electromagnetic trackers},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mach-RT: A many chip architecture for high performance ray
tracing. <em>TVCG</em>, <em>28</em>(3), 1585–1596. (<a
href="https://doi.org/10.1109/TVCG.2020.3021048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data movement, particularly access to the main memory, has been the bottleneck of most computing problems. Ray tracing is no exception. We propose an unconventional solution that combines a ray ordering scheme that minimizes access to the scene data with a large on-chip buffer acting as near-compute storage that is spread over multiple chips. We demonstrate the effectiveness of our approach by introducing Mach-RT ( Ma ny ch ip - R ay T racing), a new hardware architecture for accelerating ray tracing. Extending the concept of dual streaming, we optimize the main memory accesses to a level that allows the same memory system to service multiple processor chips at the same time. While a multiple chip solution might seem to imply increased energy consumption as well, because of the reduced memory traffic we are able to demonstrate, performance increases while maintaining reasonable energy usage compared to academic and commercial architectures. This article extends our previous work E. Vasiou, K. Shkurko, E. Brunvand, and C. Yuksel, “Mach-RT: A many chip architecture for high-performance ray tracing,” in Proc. High-Perform. Graph. Conf. , 2019 with design space exploration of the L3 cache size, more detailed evaluation of energy and memory performance, a discussion of energy delay product, and a brief exploration of boards with 16 chips. We also introduce new treelet enqueueing logic for the predictive scheduler.},
  archive      = {J_TVCG},
  author       = {Elena Vasiou and Konstantin Shkurko and Erik Brunvand and Cem Yuksel},
  doi          = {10.1109/TVCG.2020.3021048},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1585-1596},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mach-RT: A many chip architecture for high performance ray tracing},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning safety through public serious games: A study of
“prepare for impact” on a very large, international sample of players.
<em>TVCG</em>, <em>28</em>(3), 1573–1584. (<a
href="https://doi.org/10.1109/TVCG.2020.3022340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed a growing interest in serious games (SGs), i.e., digital games for education and training. However, although the potential scalability of SGs to large player populations is often praised in the literature, available SG evaluations did not provide evidence of it because they did not study learning on large, varied, international samples in naturalistic conditions. This article considers a SG that educates players about aircraft cabin safety. It presents the first study of learning in a SG intervention conducted in naturalistic conditions with a very large, worldwide sample, which includes 45,000 players who accepted to answer a knowledge questionnaire before and after playing the game, and more than 400,000 players whose in-game behavior was analyzed. Results show that the SG led to improvement in players’ knowledge, assessed with different metrics. Moreover, analysis of repeated play shows that participants improved their in-game safety behavior over time. We also focus on the role of making errors in the game, showing how they led to improvement in knowledge. Finally, we highlight the theoretical models, such as error-based learning and Protection Motivation Theory, that oriented the game design, and can be reused to create SGs for other domains.},
  archive      = {J_TVCG},
  author       = {Luca Chittaro and Fabio Buttussi},
  doi          = {10.1109/TVCG.2020.3022340},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1573-1584},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Learning safety through public serious games: A study of “Prepare for impact” on a very large, international sample of players},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interactive deep colorization and its application for image
compression. <em>TVCG</em>, <em>28</em>(3), 1557–1572. (<a
href="https://doi.org/10.1109/TVCG.2020.3021510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent methods based on deep learning have shown promise in converting grayscale images to colored ones. However, most of them only allow limited user inputs (no inputs, only global inputs, or only local inputs), to control the output colorful images. The possible difficulty lies in how to differentiate the influences of different inputs. To solve this problem, we propose a two-stage deep colorization method allowing users to control the results by flexibly setting global inputs and local inputs. The key steps include enabling color themes as global inputs by extracting $K$ mean colors and generating $K$ -color maps to define a global theme loss, and designing a loss function to differentiate the influences of different inputs without causing artifacts. We also propose a color theme recommendation method to help users choose color themes. Based on the colorization model, we further propose an image compression scheme, which supports variable compression ratios in a single network. Experiments on colorization show that our method can flexibly control the colorized results with only a few inputs and generate state-of-the-art results. Experiments on compression show that our method achieves much higher image quality at the same compression ratio when compared to the state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Yi Xiao and Jin Wu and Jie Zhang and Peiyao Zhou and Yan Zheng and Chi-Sing Leung and Ladislav Kavan},
  doi          = {10.1109/TVCG.2020.3021510},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1557-1572},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive deep colorization and its application for image compression},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Impact of information placement and user representations in
VR on performance and embodiment. <em>TVCG</em>, <em>28</em>(3),
1545–1556. (<a href="https://doi.org/10.1109/TVCG.2020.3021342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human sensory processing is sensitive to the proximity of stimuli to the body. It is therefore plausible that these perceptual mechanisms also modulate the detectability of content in VR, depending on its location. We evaluate this in a user study and further explore the impact of the user&#39;s representation during interaction. We also analyze how embodiment and motor performance are influenced by these factors. In a dual-task paradigm, participants executed a motor task, either through virtual hands, virtual controllers, or a keyboard. Simultaneously, they detected visual stimuli appearing in different locations. We found that, while actively performing a motor task in the virtual environment, performance in detecting additional visual stimuli is higher when presented near the user&#39;s body. This effect is independent of how the user is represented and only occurs when the user is also engaged in a secondary task. We further found improved motor performance and increased embodiment when interacting through virtual tools and hands in VR, compared to interacting with a keyboard. This article contributes to better understanding the detectability of visual content in VR, depending on its location in the virtual environment, as well as the impact of different user representations on information processing, embodiment, and motor performance.},
  archive      = {J_TVCG},
  author       = {Sofia Seinfeld and Tiare Feuchtner and Johannes Pinzek and Jörg Müller},
  doi          = {10.1109/TVCG.2020.3021342},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1545-1556},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Impact of information placement and user representations in VR on performance and embodiment},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Global conformal parameterization via an implementation of
holomorphic quadratic differentials. <em>TVCG</em>, <em>28</em>(3),
1529–1544. (<a href="https://doi.org/10.1109/TVCG.2020.3016574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an algorithm to compute global conformal parameterizations of high-genus meshes, which is based on an implementation of holomorphic quadratic differentials. First, we design a novel diffusion method which is capable of computing a pole-free discrete harmonic measured foliation. Second, we propose a definition for discrete holomorphic quadratic differential which consists of a horizontal and a vertical harmonic measured foliation. Third, we present a practical algorithm to approximate the discrete natural coordinates for a holomorphic quadratic differential, which represents a flat metric with cones conformal to the original metric, i.e., a parameterization. Finally, we apply the discrete natural coordinates for parameterization of high genus meshes. Our parameterization method is global conformal and simple to implement. The advantage of our method over the approach based on holomorphic differential one-forms is that ours has a larger space of parameterizations. We demonstrate our approach with hundreds of configurations on dozens of meshes to show its robustness on conformal parameterization.},
  archive      = {J_TVCG},
  author       = {Hui Zhao and Shaodong Wang and Wencheng Wang},
  doi          = {10.1109/TVCG.2020.3016574},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1529-1544},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Global conformal parameterization via an implementation of holomorphic quadratic differentials},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geometry-driven detection, tracking and visual analysis of
viscous and gravitational fingers. <em>TVCG</em>, <em>28</em>(3),
1514–1528. (<a href="https://doi.org/10.1109/TVCG.2020.3017568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Viscous and gravitational flow instabilities cause a displacement front to break up into finger-like fluids. The detection and evolutionary analysis of these fingering instabilities are critical in multiple scientific disciplines such as fluid mechanics and hydrogeology. However, previous detection methods of the viscous and gravitational fingers are based on density thresholding, which provides limited geometric information of the fingers. The geometric structures of fingers and their evolution are important yet little studied in the literature. In this article, we explore the geometric detection and evolution of the fingers in detail to elucidate the dynamics of the instability. We propose a ridge voxel detection method to guide the extraction of finger cores from three-dimensional (3D) scalar fields. After skeletonizing finger cores into skeletons, we design a spanning tree based approach to capture how fingers branch spatially from the finger skeletons. Finally, we devise a novel geometric-glyph augmented tracking graph to study how the fingers and their branches grow, merge, and split over time. Feedback from earth scientists demonstrates the usefulness of our approach to performing spatio-temporal geometric analyses of fingers.},
  archive      = {J_TVCG},
  author       = {Jiayi Xu and Soumya Dutta and Wenbin He and Joachim Moortgat and Han-Wei Shen},
  doi          = {10.1109/TVCG.2020.3017568},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1514-1528},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Geometry-driven detection, tracking and visual analysis of viscous and gravitational fingers},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ChartSeer: Interactive steering exploratory visual analysis
with machine intelligence. <em>TVCG</em>, <em>28</em>(3), 1500–1513. (<a
href="https://doi.org/10.1109/TVCG.2020.3018724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During exploratory visual analysis (EVA), analysts need to continually determine which subsequent activities to perform, such as which data variables to explore or how to present data variables visually. Due to the vast combinations of data variables and visual encodings that are possible, it is often challenging to make such decisions. Further, while performing local explorations, analysts often fail to attend to the holistic picture that is emerging from their analysis, leading them to improperly steer their EVA. These issues become even more impactful in the real world analysis scenarios where EVA occurs in multiple asynchronous sessions that could be completed by one or more analysts. To address these challenges, this work proposes ChartSeer, a system that uses machine intelligence to enable analysts to visually monitor the current state of an EVA and effectively identify future activities to perform. ChartSeer utilizes deep learning techniques to characterize analyst-created data charts to generate visual summaries and recommend appropriate charts for further exploration based on user interactions. A case study was first conducted to demonstrate the usage of ChartSeer in practice, followed by a controlled study to compare ChartSeer’s performance with a baseline during EVA tasks. The results demonstrated that ChartSeer enables analysts to adequately understand current EVA status and advance their analysis by creating charts with increased coverage and visual encoding diversity.},
  archive      = {J_TVCG},
  author       = {Jian Zhao and Mingming Fan and Mi Feng},
  doi          = {10.1109/TVCG.2020.3018724},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1500-1513},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ChartSeer: Interactive steering exploratory visual analysis with machine intelligence},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A simple and stable centeredness measure for 3D curve
skeleton extraction. <em>TVCG</em>, <em>28</em>(3), 1486–1499. (<a
href="https://doi.org/10.1109/TVCG.2020.3018483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods for extracting 3D curve skeletons mostly suffer from the difficulty of finding the center points of 3D shapes and tedious manual adjustments of the thresholds for pruning spurious branches due to the influence of shape boundary perturbations. In this article, we present a method based on medial surfaces of 3D shapes for the convenient and fast extraction of high-quality curve skeletons. Our main contribution is a simple and stable centeredness measure. It is based on simulating fire propagation via the scheme of inside-out evolution from the interior to the boundary, differentiating it from existing methods that use the scheme of outside-in evolution from the boundary to the interior. Thus, our measure is much more localized, and it can be implemented with a high degree of parallelism. In addition, we propose measures to effectively suppress the influence of details to obtain a stable measurement, and employ minimum set covers to optimize the center points to generate compact skeletons, which enables spurious branches to be effectively excluded without the tedious work of manually adjusting thresholds. Our experiments show the superiority of our method over existing methods, including its convenient generation of clean, compact and centered curve skeletons while running much faster than state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Lei Li and Wencheng Wang and Yiyao Chu},
  doi          = {10.1109/TVCG.2020.3018483},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1486-1499},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A simple and stable centeredness measure for 3D curve skeleton extraction},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A colorization framework for monochrome-color dual-lens
systems using a deep convolutional network. <em>TVCG</em>,
<em>28</em>(3), 1469–1485. (<a
href="https://doi.org/10.1109/TVCG.2020.3022480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In monochrome-color dual-lens systems, the monochrome camera can capture images with higher quality than the color camera. To obtain high quality color images, a better approach is to colorize the gray images from the monochrome camera with the color images from the color camera serving as a reference. In addition, the colorization may fail in some cases, which makes the estimation of the colorization quality a necessary step before outputting the colorization result. To solve these problems, we propose a deep convolutional network based framework. 1) In the colorization module, the proposed colorization CNN uses deep feature representations, attention operation, 3-D regulation and color correction to make use of colors of multiple pixels in the reference image for colorizing each pixel in the input gray image. 2) In the colorization quality estimation module, based on the symmetry property of colorization, we propose to utilize the colorization CNN again to colorize the gray map of the original reference color image using the first-time colorization result from the colorization module as reference. Then, the quality loss of the second-time colorization result can be used for estimating the colorization quality. Experimental results show that our method can largely outperform the state-of-the-art colorization methods and estimate the colorization quality accurately as well.},
  archive      = {J_TVCG},
  author       = {Xuan Dong and Weixin Li and Xiaoyan Hu and Xiaojie Wang and Yunhong Wang},
  doi          = {10.1109/TVCG.2020.3022480},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1469-1485},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A colorization framework for monochrome-color dual-lens systems using a deep convolutional network},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3D virtual pancreatography. <em>TVCG</em>, <em>28</em>(3),
1457–1468. (<a href="https://doi.org/10.1109/TVCG.2020.3020958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present 3D virtual pancreatography (VP), a novel visualization procedure and application for non-invasive diagnosis and classification of pancreatic lesions, the precursors of pancreatic cancer. Currently, non-invasive screening of patients is performed through visual inspection of 2D axis-aligned CT images, though the relevant features are often not clearly visible nor automatically detected. VP is an end-to-end visual diagnosis system that includes: A machine learning based automatic segmentation of the pancreatic gland and the lesions, a semi-automatic approach to extract the primary pancreatic duct, a machine learning based automatic classification of lesions into four prominent types, and specialized 3D and 2D exploratory visualizations of the pancreas, lesions and surrounding anatomy. We combine volume rendering with pancreas- and lesion-centric visualizations and measurements for effective diagnosis. We designed VP through close collaboration and feedback from expert radiologists, and evaluated it on multiple real-world CT datasets with various pancreatic lesions and case studies examined by the expert radiologists.},
  archive      = {J_TVCG},
  author       = {Shreeraj Jadhav and Konstantin Dmitriev and Joseph Marino and Matthew Barish and Arie E. Kaufman},
  doi          = {10.1109/TVCG.2020.3020958},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1457-1468},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {3D virtual pancreatography},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). What, how, and why are visual assets used in industrial
augmented reality? A systematic review and classification in
maintenance, assembly, and training (from 1997 to 2019). <em>TVCG</em>,
<em>28</em>(2), 1443–1456. (<a
href="https://doi.org/10.1109/TVCG.2020.3014614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial Augmented Reality (iAR) has demonstrated its advantages to communicate technical information in the fields of maintenance, assembly, and training. However, literature is scattered among different visual assets (i.e., AR visual user interface elements associated with a real scene). In this work, we present a systematic literature review of visual assets used in these industrial fields. We searched five databases, initially finding 1757 papers. Then, we selected 122 iAR papers from 1997 to 2019 and extracted 348 visual assets. We propose a classification for visual assets according to (i) what is displayed, (ii) how it conveys information (frame of reference, color coding, animation), and, (iii) why it is used. Our review shows that product models, text and auxiliary models are, in order, the most common, with each most often used to support operating, checking and locating tasks respectively. Other visual assets are scarcely used. Product and auxiliary models are commonly rendered world-fixed, color coding is not used as often as expected, while animations are limited to product and auxiliary model. This survey provides a snapshot of over 20 years of literature in iAR, useful to understand established practices to orientate in iAR interface design and to present future research directions.},
  archive      = {J_TVCG},
  author       = {Michele Gattullo and Alessandro Evangelista and Antonio E. Uva and Michele Fiorentino and Joseph L. Gabbard},
  doi          = {10.1109/TVCG.2020.3014614},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1443-1456},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {What, how, and why are visual assets used in industrial augmented reality? a systematic review and classification in maintenance, assembly, and training (From 1997 to 2019)},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Do multisensory stimuli benefit the virtual reality
experience? A systematic review. <em>TVCG</em>, <em>28</em>(2),
1428–1442. (<a href="https://doi.org/10.1109/TVCG.2020.3010088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of virtual reality (VR) applications rely on audiovisual stimuli and do not exploit the addition of other sensory cues that could increase the potential of VR. This systematic review surveys the existing literature on multisensory VR and the impact of haptic, olfactory, and taste cues over audiovisual VR. The goal is to identify the extent to which multisensory stimuli affect the VR experience, which stimuli are used in multisensory VR, the type of VR setups used, and the application fields covered. An analysis of the 105 studies that met the eligibility criteria revealed that 84.8 percent of the studies show a positive impact of multisensory VR experiences. Haptics is the most commonly used stimulus in multisensory VR systems (86.6 percent). Non-immersive and immersive VR setups are preferred over semi-immersive setups. Regarding the application fields, a considerable part was adopted by health professionals and science and engineering professionals. We further conclude that smell and taste are still underexplored, and they can bring significant value to VR applications. More research is recommended on how to synthesize and deliver these stimuli, which still require complex and costly apparatus be integrated into the VR experience in a controlled and straightforward manner.},
  archive      = {J_TVCG},
  author       = {Miguel Melo and Guilherme Gonçalves and Pedro Monteiro and Hugo Coelho and José Vasconcelos-Raposo and Maximino Bessa},
  doi          = {10.1109/TVCG.2020.3010088},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1428-1442},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Do multisensory stimuli benefit the virtual reality experience? a systematic review},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Volumetric head-mounted display with locally adaptive focal
blocks. <em>TVCG</em>, <em>28</em>(2), 1415–1427. (<a
href="https://doi.org/10.1109/TVCG.2020.3011468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A commercial head-mounted display (HMD) for virtual reality (VR) presents three-dimensional imagery with a fixed focal distance. The VR HMD with a fixed focus can cause visual discomfort to an observer. In this article, we propose a novel design of a compact VR HMD supporting near-correct focus cues over a wide depth of field (from 18 cm to optical infinity). The proposed HMD consists of a low-resolution binary backlight, a liquid crystal display panel, and focus-tunable lenses. In the proposed system, the backlight locally illuminates the display panel that is floated by the focus-tunable lens at a specific distance. The illumination moment and the focus-tunable lens’ focal power are synchronized to generate focal blocks at the desired distances. The distance of each focal block is determined by depth information of three-dimensional imagery to provide near-correct focus cues. We evaluate the focus cue fidelity of the proposed system considering the fill factor and resolution of the backlight. Finally, we verify the display performance with experimental results.},
  archive      = {J_TVCG},
  author       = {Dongheon Yoo and Seungjae Lee and Youngjin Jo and Jaebum Cho and Suyeon Choi and Byoungho Lee},
  doi          = {10.1109/TVCG.2020.3011468},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1415-1427},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Volumetric head-mounted display with locally adaptive focal blocks},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TransVis: Integrated distant and close reading of othello
translations. <em>TVCG</em>, <em>28</em>(2), 1397–1414. (<a
href="https://doi.org/10.1109/TVCG.2020.3012778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studying variation among time-evolved translations is a valuable research area for cultural heritage. Understanding how and why translations vary reveals cultural, ideological, and even political influences on literature as well as author relations. In this article, we introduce a novel integrated visual application to support distant and close reading of a collection of Othello translations. We present a new interactive application that provides an alignment overview of all the translations and their correspondences in parallel with smooth zooming and panning capability to integrate distant and close reading within the same view. We provide a range of filtering and selection options to customize the alignment overview as well as focus on specific subsets. Selection and filtering are responsive to expert user preferences and update the analytical text metrics interactively. Also, we introduce a customized view for close reading which preserves the history of selections and the alignment overview state and enables backtracing and re-examining them. Finally, we present a new Term-Level Comparisons view (TLC) to compare and convey relative term weighting in the context of an alignment. Our visual design is guided by, used and evaluated by a domain expert specialist in German translations of Shakespeare.},
  archive      = {J_TVCG},
  author       = {Mohammad Alharbi and Robert S Laramee and Tom Cheesman},
  doi          = {10.1109/TVCG.2020.3012778},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1397-1414},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TransVis: Integrated distant and close reading of othello translations},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Soft articulated characters in projective dynamics.
<em>TVCG</em>, <em>28</em>(2), 1385–1396. (<a
href="https://doi.org/10.1109/TVCG.2020.3010236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a fast and robust solver to simulate continuum-based deformable models with constraints, in particular, rigid-body and joint constraints useful for soft articulated characters. Our method embeds the degrees of freedom of both articulated rigid bodies and deformable bodies in one unified constrained optimization problem, thus coupling the deformable and rigid bodies. Inspired by Projective Dynamics which is a fast numerical solver to simulate deformable objects, we also propose a novel local/global solver that takes full advantage of the pre-factorized system matrices to accelerate the solve of our constrained optimization problem. Therefore, our method can efficiently simulate character models, with rigid-body parts (bones) being correctly coupled with deformable parts (flesh). Our method is stable because backward Euler time integration is applied to both rigid and deformable degrees of freedom. Our unified optimization problem is rigorously derived from constrained Newtonian mechanics. When simulating only articulated rigid bodies as a special case, our method converges to the state-of-the-art rigid body simulators.},
  archive      = {J_TVCG},
  author       = {Jing Li and Tiantian Liu and Ladislav Kavan},
  doi          = {10.1109/TVCG.2020.3010236},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1385-1396},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Soft articulated characters in projective dynamics},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time dynamic bokeh rendering with efficient look-up
table sampling. <em>TVCG</em>, <em>28</em>(2), 1373–1384. (<a
href="https://doi.org/10.1109/TVCG.2020.3014474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a real-time bokeh rendering technique that splats pre-computed sprites but takes dynamic visibilities and intrinsic appearances into account at runtime. To attain alias-free looks without excessive sampling on a lens, the visibilities of strong highlights are densely sampled using rasterization, while regular objects are sparsely sampled using conventional defocus-blur rendering. The intrinsic appearance is dynamically transformed from a precomputed look-up table, which encodes radial aberrations against image distances in a compact 2D texture. Our solution can render complex bokeh effects without undersampling artifacts in real time, and greatly improve the photorealism of defocus-blur rendering.},
  archive      = {J_TVCG},
  author       = {Yuna Jeong and Seung Youp Baek and Yechan Seok and Gi Beom Lee and Sungkil Lee},
  doi          = {10.1109/TVCG.2020.3014474},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1373-1384},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Real-time dynamic bokeh rendering with efficient look-up table sampling},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parallel computation of 3D clipped voronoi diagrams.
<em>TVCG</em>, <em>28</em>(2), 1363–1372. (<a
href="https://doi.org/10.1109/TVCG.2020.3012288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing the Voronoi diagram of a given set of points in a restricted domain (e.g., inside a 2D polygon, on a 3D surface, or within a volume) has many applications. Although existing algorithms can compute 2D and surface Voronoi diagrams in parallel on graphics hardware, computing clipped Voronoi diagrams within volumes remains a challenge. This article proposes an efficient GPU algorithm to tackle this problem. A preprocessing step discretizes the input volume into a tetrahedral mesh. Then, unlike existing approaches which use the bisecting planes of the Voronoi cells to clip the tetrahedra, we use the four planes of each tetrahedron to clip the Voronoi cells. This strategy drastically simplifies the computation, and as a result, it outperforms state-of-the-art CPU methods up to an order of magnitude.},
  archive      = {J_TVCG},
  author       = {Xiaohan Liu and Lei Ma and Jianwei Guo and Dong-Ming Yan},
  doi          = {10.1109/TVCG.2020.3012288},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1363-1372},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Parallel computation of 3D clipped voronoi diagrams},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multisensory proximity and transition cues for improving
target awareness in narrow field of view augmented reality displays.
<em>TVCG</em>, <em>28</em>(2), 1342–1362. (<a
href="https://doi.org/10.1109/TVCG.2021.3116673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented reality applications allow users to enrich their real surroundings with additional digital content. However, due to the limited field of view of augmented reality devices, it can sometimes be difficult to become aware of newly emerging information inside or outside the field of view. Typical visual conflicts like clutter and occlusion of augmentations occur and can be further aggravated especially in the context of dense information spaces. In this article, we evaluate how multisensory cue combinations can improve the awareness for moving out-of-view objects in narrow field of view augmented reality displays. We distinguish between proximity and transition cues in either visual, auditory or tactile manner. Proximity cues are intended to enhance spatial awareness of approaching out-of-view objects while transition cues inform the user that the object just entered the field of view. In study 1, user preference was determined for 6 different cue combinations via forced-choice decisions. In study 2, the 3 most preferred modes were then evaluated with respect to performance and awareness measures in a divided attention reaction task. Both studies were conducted under varying noise levels. We show that on average the Visual-Tactile combination leads to 63\% and Audio-Tactile to 65\% faster reactions to incoming out-of-view augmentations than their Visual-Audio counterpart, indicating a high usefulness of tactile transition cues. We further show a detrimental effect of visual and audio noise on performance when feedback included visual proximity cues. Based on these results, we make recommendations to determine which cue combination is appropriate for which application.},
  archive      = {J_TVCG},
  author       = {Christina Trepkowski and Alexander Marquardt and Tom David Eibich and Yusuke Shikanai and Jens Maiero and Kiyoshi Kiyokawa and Ernst Kruijff and Johannes Schöning and Peter König},
  doi          = {10.1109/TVCG.2021.3116673},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1342-1362},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multisensory proximity and transition cues for improving target awareness in narrow field of view augmented reality displays},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning on 3D meshes with laplacian encoding and pooling.
<em>TVCG</em>, <em>28</em>(2), 1317–1327. (<a
href="https://doi.org/10.1109/TVCG.2020.3014449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D models are commonly used in computer vision and graphics. With the wider availability of mesh data, an efficient and intrinsic deep learning approach to processing 3D meshes is in great need. Unlike images, 3D meshes have irregular connectivity, requiring careful design to capture relations in the data. To utilize the topology information while staying robust under different triangulations, we propose to encode mesh connectivity using Laplacian spectral analysis, along with mesh feature aggregation blocks (MFABs) that can split the surface domain into local pooling patches and aggregate global information amongst them. We build a mesh hierarchy from fine to coarse using Laplacian spectral clustering, which is flexible under isometric transformations. Inside the MFABs there are pooling layers to collect local information and multi-layer perceptrons to compute vertex features of increasing complexity. To obtain the relationships among different clusters, we introduce a Correlation Net to compute a correlation matrix, which can aggregate the features globally by matrix multiplication with cluster features. Our network architecture is flexible enough to be used on meshes with different numbers of vertices. We conduct several experiments including shape segmentation and classification, and our method outperforms state-of-the-art algorithms for these tasks on the ShapeNet and COSEG datasets.},
  archive      = {J_TVCG},
  author       = {Yi-Ling Qiao and Lin Gao and Jie Yang and Paul L. Rosin and Yu-Kun Lai and Xilin Chen},
  doi          = {10.1109/TVCG.2020.3014449},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1317-1327},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Learning on 3D meshes with laplacian encoding and pooling},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Large-scale architectural asset extraction from panoramic
imagery. <em>TVCG</em>, <em>28</em>(2), 1301–1316. (<a
href="https://doi.org/10.1109/TVCG.2020.3010694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a system to extract architectural assets from large-scale collections of panoramic imagery. We automatically rectify and crop parts of the panoramic image that contain dominant planes, and then use object detection to extract assets such as façades and windows. We also provide various tools to identify attributes of the assets to determine the asset quality and index the assets for search. In addition, we propose a User Interface (UI) to visualize and query assets. Finally, we present applications for urban modeling and texture synthesis.},
  archive      = {J_TVCG},
  author       = {Peihao Zhu and Wamiq Reyaz Para and Anna Frühstück and John Femiani and Peter Wonka},
  doi          = {10.1109/TVCG.2020.3010694},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1301-1316},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Large-scale architectural asset extraction from panoramic imagery},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid graph visualizations with ChordLink: Algorithms,
experiments, and applications. <em>TVCG</em>, <em>28</em>(2), 1288–1300.
(<a href="https://doi.org/10.1109/TVCG.2020.3016055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world networks are globally sparse but locally dense. Typical examples are social networks, biological networks, and information networks. This double structural nature makes it difficult to adopt a homogeneous visualization model that clearly conveys both an overview of the network and the internal structure of its communities at the same time. As a consequence, the use of hybrid visualizations has been proposed. For instance, NodeTrix combines node-link and matrix-based representations (Henry et al. , 2007). In this article we describe ChordLink , a hybrid visualization model that embeds chord diagrams, used to represent dense subgraphs, into a node-link diagram, which shows the global network structure. The visualization makes it possible to interactively highlight the structure of a community while keeping the rest of the layout stable. We discuss the intriguing algorithmic challenges behind the ChordLink model, present a prototype system that implements it, and illustrate case studies on real-world networks.},
  archive      = {J_TVCG},
  author       = {Lorenzo Angori and Walter Didimo and Fabrizio Montecchiani and Daniele Pagliuca and Alessandra Tappini},
  doi          = {10.1109/TVCG.2020.3016055},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1288-1300},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Hybrid graph visualizations with ChordLink: Algorithms, experiments, and applications},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Facial expression retargeting from human to avatar made
easy. <em>TVCG</em>, <em>28</em>(2), 1274–1287. (<a
href="https://doi.org/10.1109/TVCG.2020.3013876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression retargeting from humans to virtual characters is a useful technique in computer graphics and animation. Traditional methods use markers or blendshapes to construct a mapping between the human and avatar faces. However, these approaches require a tedious 3D modeling process, and the performance relies on the modelers’ experience. In this article, we propose a brand-new solution to this cross-domain expression transfer problem via nonlinear expression embedding and expression domain translation. We first build low-dimensional latent spaces for the human and avatar facial expressions with variational autoencoder. Then we construct correspondences between the two latent spaces guided by geometric and perceptual constraints. Specifically, we design geometric correspondences to reflect geometric matching and utilize a triplet data structure to express users’ perceptual preference of avatar expressions. A user-friendly method is proposed to automatically generate triplets for a system allowing users to easily and efficiently annotate the correspondences. Using both geometric and perceptual correspondences, we trained a network for expression domain translation from human to avatar. Extensive experimental results and user studies demonstrate that even nonprofessional users can apply our method to generate high-quality facial expression retargeting results with less time and effort.},
  archive      = {J_TVCG},
  author       = {Juyong Zhang and Keyu Chen and Jianmin Zheng},
  doi          = {10.1109/TVCG.2020.3013876},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1274-1287},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Facial expression retargeting from human to avatar made easy},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluating the effects of non-isomorphic rotation on 3D
manipulation tasks in mixed reality simulation. <em>TVCG</em>,
<em>28</em>(2), 1261–1273. (<a
href="https://doi.org/10.1109/TVCG.2020.3010247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a hyper-natural interaction technique in 3D user interfaces, non-isomorphic rotation has been considered an effective approach for rotation tasks, where a static or dynamic control-display gain can be applied to amplify or attenuate a rotation. However, it is not clear whether non-isomorphic rotation can benefit 6-degree-of-freedom (6-DOF) manipulation tasks in AR and VR. In this article, we extended the usability studies of non-isomorphic rotation from rotation-only tasks to 6-DOF manipulation tasks and analyzed the collected data using a 2-component model. Using a mixed reality (MR) simulation approach, we also investigated whether environment (AR or VR) had an impact on 3D manipulation tasks. The results reveal that although both static and dynamic non-isomorphic rotation techniques could save time and effort in ballistic phases, only dynamic non-isomorphic rotation was significantly faster than isomorphic rotation. Interestingly, while environment had no significant impact on overall user performance, we found evidence that it could affect fine-tuning in correction phases. We also found that most participants preferred AR over VR, indicating that environmental visual realism could be helpful to improve user experience.},
  archive      = {J_TVCG},
  author       = {Zihan Gao and Huiqiang Wang and Hongwu Lv and Moshu Wang and Yifan Qi},
  doi          = {10.1109/TVCG.2020.3010247},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1261-1273},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evaluating the effects of non-isomorphic rotation on 3D manipulation tasks in mixed reality simulation},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting specular reflections and cast shadows to estimate
reflectance and illumination of dynamic indoor scenes. <em>TVCG</em>,
<em>28</em>(2), 1249–1260. (<a
href="https://doi.org/10.1109/TVCG.2020.2976986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of Mixed Reality (MR) is to achieve a seamless and realistic blending between real and virtual worlds. This requires the estimation of reflectance properties and lighting characteristics of the real scene. One of the main challenges within this task consists in recovering such properties using a single RGB-D camera. In this article, we introduce a novel framework to recover both the position and color of multiple light sources as well as the specular reflectance of real scene surfaces. This is achieved by detecting and incorporating information from both specular reflections and cast shadows. Our approach is capable of handling any textured surface and considers both static and dynamic light sources. Its effectiveness is demonstrated through a range of applications including visually-consistent mixed reality scenarios (e.g., correct real specularity removal, coherent shadows in terms of shape and intensity) and retexturing where the texture of the scene is altered whereas the incident lighting is preserved.},
  archive      = {J_TVCG},
  author       = {Salma Jiddi and Philippe Robert and Eric Marchand},
  doi          = {10.1109/TVCG.2020.2976986},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1249-1260},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Detecting specular reflections and cast shadows to estimate reflectance and illumination of dynamic indoor scenes},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep recursive embedding for high-dimensional data.
<em>TVCG</em>, <em>28</em>(2), 1237–1248. (<a
href="https://doi.org/10.1109/TVCG.2021.3122388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embedding high-dimensional data onto a low-dimensional manifold is of both theoretical and practical value. In this article, we propose to combine deep neural networks (DNN) with mathematics-guided embedding rules for high-dimensional data embedding. We introduce a generic deep embedding network (DEN) framework, which is able to learn a parametric mapping from high-dimensional space to low-dimensional space, guided by well-established objectives such as Kullback-Leibler (KL) divergence minimization. We further propose a recursive strategy, called deep recursive embedding (DRE), to make use of the latent data representations for boosted embedding performance. We exemplify the flexibility of DRE by different architectures and loss functions, and benchmarked our method against the two most popular embedding methods, namely, t-distributed stochastic neighbor embedding (t-SNE) and uniform manifold approximation and projection (UMAP). The proposed DRE method can map out-of-sample data and scale to extremely large datasets. Experiments on a range of public datasets demonstrated improved embedding performance in terms of local and global structure preservation, compared with other state-of-the-art embedding methods. Code is available at https://github.com/tao-aimi/DeepRecursiveEmbedding .},
  archive      = {J_TVCG},
  author       = {Zixia Zhou and Xinrui Zu and Yuanyuan Wang and Boudewijn P. F. Lelieveldt and Qian Tao},
  doi          = {10.1109/TVCG.2021.3122388},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1237-1248},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Deep recursive embedding for high-dimensional data},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ConfusionFlow: A model-agnostic visualization for temporal
analysis of classifier confusion. <em>TVCG</em>, <em>28</em>(2),
1222–1236. (<a href="https://doi.org/10.1109/TVCG.2020.3012063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifiers are among the most widely used supervised machine learning algorithms. Many classification models exist, and choosing the right one for a given task is difficult. During model selection and debugging, data scientists need to assess classifiers&#39; performances, evaluate their learning behavior over time, and compare different models. Typically, this analysis is based on single-number performance measures such as accuracy. A more detailed evaluation of classifiers is possible by inspecting class errors. The confusion matrix is an established way for visualizing these class errors, but it was not designed with temporal or comparative analysis in mind. More generally, established performance analysis systems do not allow a combined temporal and comparative analysis of class-level information. To address this issue, we propose ConfusionFlow, an interactive, comparative visualization tool that combines the benefits of class confusion matrices with the visualization of performance characteristics over time. ConfusionFlow is model-agnostic and can be used to compare performances for different model types, model architectures, and/or training and test datasets. We demonstrate the usefulness of ConfusionFlow in a case study on instance selection strategies in active learning. We further assess the scalability of ConfusionFlow and present a use case in the context of neural network pruning.},
  archive      = {J_TVCG},
  author       = {Andreas Hinterreiter and Peter Ruch and Holger Stitz and Martin Ennemoser and Jürgen Bernard and Hendrik Strobelt and Marc Streit},
  doi          = {10.1109/TVCG.2020.3012063},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1222-1236},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ConfusionFlow: A model-agnostic visualization for temporal analysis of classifier confusion},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conceptual metaphor and graphical convention influence the
interpretation of line graphs. <em>TVCG</em>, <em>28</em>(2), 1209–1221.
(<a href="https://doi.org/10.1109/TVCG.2021.3088343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many metaphors in language reflect conceptual metaphors that structure thought. In line with metaphorical expressions such as ‘high number’, experiments show that people associate larger numbers with upward space. Consistent with this metaphor, high numbers are conventionally depicted in high positions on the $y$ -axis of line graphs. People also associate good and bad (emotional valence) with upward and downward locations, in line with metaphorical expressions such as ‘uplifting’ and ‘down in the dumps’. Graphs depicting good quantities (e.g., vacation days) are consistent with graphical convention and the valence metaphor, because ‘more’ of the good quantity is represented by higher $y$ -axis positions. In contrast, graphs depicting bad quantities (e.g., murders) are consistent with graphical convention, but not the valence metaphor, because more of the bad quantity is represented by higher (rather than lower) $y$ -axis positions. We conducted two experiments ( N = 300 per experiment) where participants answered questions about line graphs depicting good and bad quantities. For some graphs, we inverted the conventional axis ordering of numbers. Line graphs that aligned (versus misaligned) with valence metaphors (up = good) were easier to interpret, but this beneficial effect did not outweigh the adverse effect of inverting the axis numbering. Line graphs depicting good (versus bad) quantities were easier to interpret, as were graphs that depicted quantity using the $x$ -axis (versus $y$ -axis). Our results suggest that conceptual metaphors matter for the interpretation of line graphs. However, designers of line graphs are warned against subverting graphical convention to align with conceptual metaphors.},
  archive      = {J_TVCG},
  author       = {Greg Woodin and Bodo Winter and Lace Padilla},
  doi          = {10.1109/TVCG.2021.3088343},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1209-1221},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Conceptual metaphor and graphical convention influence the interpretation of line graphs},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active colorization for cartoon line drawings.
<em>TVCG</em>, <em>28</em>(2), 1198–1208. (<a
href="https://doi.org/10.1109/TVCG.2020.3009949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the animation industry, the colorization of raw sketch images is a vitally important but very time-consuming task. This article focuses on providing a novel solution that semiautomatically colorizes a set of images using a single colorized reference image. Our method is able to provide coherent colors for regions that have similar semantics to those in the reference image. An active-learning-based framework is used to match local regions, followed by mixed-integer quadratic programming (MIQP) which considers the spatial contexts to further refine the matching results. We efficiently utilize user interactions to achieve high accuracy in the final colorized images. Experiments show that our method outperforms the current state-of-the-art deep learning based colorization method in terms of color coherency with the reference image. The region matching framework could potentially be applied to other applications, such as color transfer.},
  archive      = {J_TVCG},
  author       = {Shu-Yu Chen and Jia-Qi Zhang and Lin Gao and Yue He and Shihong Xia and Min Shi and Fang-Lue Zhang},
  doi          = {10.1109/TVCG.2020.3009949},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1198-1208},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Active colorization for cartoon line drawings},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Meshless approximation and helmholtz-hodge decomposition of
vector fields. <em>TVCG</em>, <em>28</em>(2), 1–14. (<a
href="https://doi.org/10.1109/TVCG.2020.3016588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of vector fields is crucial for the understanding of several physical phenomena, such as natural events (e.g., analysis of waves), diffusive processes, electric and electromagnetic fields. While previous work has been focused mainly on the analysis of 2D or 3D vector fields on volumes or surfaces, we address the meshless analysis of a vector field defined on an arbitrary domain, without assumptions on its dimension and discretisation. The meshless approximation of the Helmholtz-Hodge decomposition of a vector field is achieved by expressing the potential of its components as a linear combination of radial basis functions and by computing the corresponding conservative, irrotational, and harmonic components as solution to a least-squares or to a differential problem. To this end, we identify the conditions on the kernel of the radial basis functions that guarantee the existence of their derivatives. Finally, we demonstrate our approach on 2D and 3D vector fields measured by sensors or generated through simulation.},
  archive      = {J_TVCG},
  author       = {Giuseppe Patané},
  doi          = {10.1109/TVCG.2020.3016588},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Meshless approximation and helmholtz-hodge decomposition of vector fields},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VIS 2021 program committee. <em>TVCG</em>, <em>28</em>(1),
xxxvii–xl. (<a href="https://doi.org/10.1109/TVCG.2021.3114915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the VIS 2021 Program Committee.},
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2021.3114915},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xxxvii-xl},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VIS 2021 program committee},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VIS 2021 area curation committee. <em>TVCG</em>,
<em>28</em>(1), xxxvi. (<a
href="https://doi.org/10.1109/TVCG.2021.3125551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the VIS 2021 Area Curation Committee.},
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2021.3125551},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xxxvi},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VIS 2021 area curation committee},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VIS 2021 executive committee. <em>TVCG</em>, <em>28</em>(1),
xxxv. (<a href="https://doi.org/10.1109/TVCG.2021.3125595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the VIS 2021 Executive Committee.},
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2021.3125595},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xxxv},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VIS 2021 executive committee},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VIS 2021 steering committee. <em>TVCG</em>, <em>28</em>(1),
xxxiv. (<a href="https://doi.org/10.1109/TVCG.2021.3125547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the VIS 2021 Steering Committee.},
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2021.3125547},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xxxiv},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VIS 2021 steering committee},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VIS 2021 conference committee. <em>TVCG</em>,
<em>28</em>(1), xxxi–xxxiii. (<a
href="https://doi.org/10.1109/TVCG.2021.3114894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the VIS 2021 Conference Committee.},
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2021.3114894},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xxxi-xxxiii},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VIS 2021 conference committee},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IEEE visualization and graphics technical committee (VGTC):
Http://vgtc.org/. <em>TVCG</em>, <em>28</em>(1), xxiv. (<a
href="https://doi.org/10.1109/TVCG.2021.3114900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the IEEE Visualization and Graphics Technical Committee.},
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2021.3114900},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xxiv},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IEEE visualization and graphics technical committee (VGTC): http://vgtc.org/},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VIS 2021 best papers committee. <em>TVCG</em>,
<em>28</em>(1), xli–xliii. (<a
href="https://doi.org/10.1109/TVCG.2021.3114935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the VIS 2021 Best Papers Committee.},
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2021.3114935},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xli-xliii},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VIS 2021 best papers committee},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preface. <em>TVCG</em>, <em>28</em>(1), xiv–xxiii. (<a
href="https://doi.org/10.1109/TVCG.2021.3114891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This February 2022 issue of the IEEE Transactions on Visualization and Computer Graphics (TVCG) contains the proceedings of IEEE VIS 2021, held online on October 24-29, 2021, with General Chairs from Tulane University and Universidade de Sao Paulo. With IEEE VIS 2021, the conference series is in its 32nd year.},
  archive      = {J_TVCG},
  author       = {Bongshin Lee and Silvia Miksch and Anders Ynnerman and Anastasia Bezerianos and Jian Chen and Wei Chen and Christopher Collins and Michael Gleicher and Eduard Gröller and Alexander Lex and Bernhard Preim and Jinwook Seo and Ruediger Westermann and Jing Yang and Xiaoru Yuan and Han-Wei Shen and Jean-Daniel Fekete and Shixia Liu},
  doi          = {10.1109/TVCG.2021.3114891},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xiv-xxiii},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Preface},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Welcome. <em>TVCG</em>, <em>28</em>(1), xiii. (<a
href="https://doi.org/10.1109/TVCG.2021.3115333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have been honored to oversee the organization of IEEE VIS 2021. This marked the first year in which the SciVis, InfoVis, and VAST conferences were merged into a single, unified IEEE VIS. When we began planning for VIS in 2019, we envisioned a great experience for our community, who would be able to enjoy the innovative scientific program provided by a unified VIS, while delighting in the hospitable and lively atmosphere of New Orleans. But even the best laid plans cannot account for all contingencies. The emergence of the COVID-19 pandemic changed and complicated much in our lives, including the planning for VIS 2021. Moreover, transition to a unified conference was not without its challenges as well. Despite the difficulties, our community showed, once again, its strength and resilience. Our organizing committee created an exciting conference with a rich program full of high-quality research with all challenges being continuously and admirably addressed. As general chairs, our primary responsibility was to watch these wonderful volunteers conduct truly inspiring work under extreme constraints. For this, we are deeply indebted.},
  archive      = {J_TVCG},
  author       = {Brian Summa and Luis Gustavo Nonato},
  doi          = {10.1109/TVCG.2021.3115333},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xiii},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Welcome},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Message from the editor-in-chief. <em>TVCG</em>,
<em>28</em>(1), xii. (<a
href="https://doi.org/10.1109/TVCG.2021.3114914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Welcome to the January 2022 issue of the IEEE Transactions on Visualization and Computer Graphics (TVCG). This is traditionally the time of the year when we feature the IEEE VIS special issue and this year is no different. But there is one significant difference — IEEE VIS is now fully unified. There is no more IEEE VAST, IEEE InfoVis and IEEE SciVis; there is just IEEE VIS, tiled into six overarching areas: Applications (24 papers), Analytics &amp; Decisions (19), Theoretical &amp; Empirical (24), Representations &amp; Interaction (19), Data Transformations (14) and Systems &amp; Rendering (11). The conference was scheduled to take place in New Orleans (LA) but was moved to a virtual event due to the global coronavirus pandemic. This virtual conference took place from October 24-29, 2021. Contained in this special issue are the top 110 papers selected by the unified program committee from a total of 441 submissions. In addition, this issue also contains the Best Paper of the 2021 IEEE Large Scale Data Analysis and Visualization (LDAV) Symposium and the Best Paper of the 2021 IEEE Symposium on Visualization for Cyber Security (VizSec).},
  archive      = {J_TVCG},
  author       = {Klaus Mueller},
  doi          = {10.1109/TVCG.2021.3114914},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xii},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Message from the editor-in-chief},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic narrative summarization for visualizing cyber
security logs and incident reports. <em>TVCG</em>, <em>28</em>(1),
1182–1190. (<a href="https://doi.org/10.1109/TVCG.2021.3114843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber security logs and incident reports describe a narrative, but in practice analysts view the data in tables where it can be difficult to follow the narrative. Narrative visualizations are useful, but common examples use a summarized narrative instead of the full story&#39;s narrative; it is unclear how to automatically generate these summaries. This paper presents (1) a narrative summarization algorithm to reduce the size and complexity of cyber security narratives with a user-customizable summarization level, and (2) a narrative visualization tailored for incident reports and network logs. An evaluation on real incident reports shows that the summarization algorithm reduces false positives and improves average precision by 41\% while reducing average incident report size up to 79\%. Together, the visualization and summarization algorithm generate compact representations of cyber narratives that earned praise from a SOC analyst. We further demonstrate that the summarization algorithm can apply to other types of dynamic graphs by automatically generating a summary of the Les Misérables character interaction graph. We find that the list of main characters in the automatically generated summary has substantial agreement with human-generated summaries. A version of this paper, data, and code is freely available at https://osf.io/ekzbp/ .},
  archive      = {J_TVCG},
  author       = {Robert Gove},
  doi          = {10.1109/TVCG.2021.3114843},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1182-1190},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Automatic narrative summarization for visualizing cyber security logs and incident reports},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-aware predictive scheduling for distributed-memory ray
tracing. <em>TVCG</em>, <em>28</em>(1), 1172–1181. (<a
href="https://doi.org/10.1109/TVCG.2021.3114838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific ray tracing now can include realistic shading and material properties, but tracing rays of various depths to conclusion through partitioned data is inefficient. For such data, many ray scheduling methods have demonstrated improved rendering performance. However, synchronicity and non-adaptivity inherent in prior methods hinder further performance optimizations. In this paper, we attempt to relax these constraints. Specifically, we incorporate prediction models capable of dynamically adjusting levels of speculation in ray-data queries, making ray scheduling highly adaptable to a spectrum of scene characteristics. In addition, we organize rays in a tree of speculation nodes, where speculation is coordinated pairwise within a subtree of adaptive ray groups, facilitating concurrency and parallelism. Compared to prior non-predictive methods, we achieve up to three times higher throughput for volume and geometry rendering on a distributed system, making our method fit for both interactive and offline applications.},
  archive      = {J_TVCG},
  author       = {Hyungman Park and Donald Fussell and Paul Navrátil},
  doi          = {10.1109/TVCG.2021.3114838},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1172-1181},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Data-aware predictive scheduling for distributed-memory ray tracing},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sibyl: Understanding and addressing the usability challenges
of machine learning in high-stakes decision making. <em>TVCG</em>,
<em>28</em>(1), 1161–1171. (<a
href="https://doi.org/10.1109/TVCG.2021.3114864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) is being applied to a diverse and ever-growing set of domains. In many cases, domain experts - who often have no expertise in ML or data science - are asked to use ML predictions to make high-stakes decisions. Multiple ML usability challenges can appear as result, such as lack of user trust in the model, inability to reconcile human-ML disagreement, and ethical concerns about oversimplification of complex problems to a single algorithm output. In this paper, we investigate the ML usability challenges that present in the domain of child welfare screening through a series of collaborations with child welfare screeners. Following the iterative design process between the ML scientists, visualization researchers, and domain experts (child screeners), we first identified four key ML challenges and honed in on one promising explainable ML technique to address them (local factor contributions). Then we implemented and evaluated our visual analytics tool, Sibyl, to increase the interpretability and interactivity of local factor contributions. The effectiveness of our tool is demonstrated by two formal user studies with 12 non-expert participants and 13 expert participants respectively. Valuable feedback was collected, from which we composed a list of design implications as a useful guideline for researchers who aim to develop an interpretable and interactive visualization tool for ML prediction models deployed for child welfare screeners and other similar domain experts.},
  archive      = {J_TVCG},
  author       = {Alexandra Zytek and Dongyu Liu and Rhema Vaithianathan and Kalyan Veeramachaneni},
  doi          = {10.1109/TVCG.2021.3114864},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1161-1171},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Sibyl: Understanding and addressing the usability challenges of machine learning in high-stakes decision making},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Causal support: Modeling causal inferences with
visualizations. <em>TVCG</em>, <em>28</em>(1), 1150–1160. (<a
href="https://doi.org/10.1109/TVCG.2021.3114824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysts often make visual causal inferences about possible data-generating models. However, visual analytics (VA) software tends to leave these models implicit in the mind of the analyst, which casts doubt on the statistical validity of informal visual “insights”. We formally evaluate the quality of causal inferences from visualizations by adopting causal support —a Bayesian cognition model that learns the probability of alternative causal explanations given some data—as a normative benchmark for causal inferences. We contribute two experiments assessing how well crowdworkers can detect (1) a treatment effect and (2) a confounding relationship. We find that chart users&#39; causal inferences tend to be insensitive to sample size such that they deviate from our normative benchmark. While interactively cross-filtering data in visualizations can improve sensitivity, on average users do not perform reliably better with common visualizations than they do with textual contingency tables. These experiments demonstrate the utility of causal support as an evaluation framework for inferences in VA and point to opportunities to make analysts&#39; mental models more explicit in VA software.},
  archive      = {J_TVCG},
  author       = {Alex Kale and Yifan Wu and Jessica Hullman},
  doi          = {10.1109/TVCG.2021.3114824},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1150-1160},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Causal support: Modeling causal inferences with visualizations},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). From jam session to recital: Synchronous communication and
collaboration around data in organizations. <em>TVCG</em>,
<em>28</em>(1), 1139–1149. (<a
href="https://doi.org/10.1109/TVCG.2021.3114760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior research on communicating with visualization has focused on public presentation and asynchronous individual consumption, such as in the domain of journalism. The visualization research community knows comparatively little about synchronous and multimodal communication around data within organizations, from team meetings to executive briefings. We conducted two qualitative interview studies with individuals who prepare and deliver presentations about data to audiences in organizations. In contrast to prior work, we did not limit our interviews to those who self-identify as data analysts or data scientists. Both studies examined aspects of speaking about data with visual aids such as charts, dashboards, and tables. One study was a retrospective examination of current practices and difficulties, from which we identified three scenarios involving presentations of data. We describe these scenarios using an analogy to musical performance: small collaborative team meetings are akin to jam session , while more structured presentations can range from semi-improvisational performances among peers to formal recitals given to executives or customers. In our second study, we grounded the discussion around three design probes, each examining a different aspect of presenting data: the progressive reveal of visualization to direct attention and advance a narrative, visualization presentation controls that are hidden from the audience&#39;s view, and the coordination of a presenter&#39;s video with interactive visualization. Our distillation of interviewees&#39; responses surfaced twelve themes, from ways of authoring presentations to creating accessible and engaging audience experiences.},
  archive      = {J_TVCG},
  author       = {Matthew Brehmer and Robert Kosara},
  doi          = {10.1109/TVCG.2021.3114760},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1139-1149},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {From jam session to recital: Synchronous communication and collaboration around data in organizations},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A critical reflection on visualization research: Where do
decision making tasks hide? <em>TVCG</em>, <em>28</em>(1), 1128–1138.
(<a href="https://doi.org/10.1109/TVCG.2021.3114813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been widely suggested that a key goal of visualization systems is to assist decision making, but is this true? We conduct a critical investigation on whether the activity of decision making is indeed central to the visualization domain. By approaching decision making as a user task , we explore the degree to which decision tasks are evident in visualization research and user studies. Our analysis suggests that decision tasks are not commonly found in current visualization task taxonomies and that the visualization field has yet to leverage guidance from decision theory domains on how to study such tasks. We further found that the majority of visualizations addressing decision making were not evaluated based on their ability to assist decision tasks. Finally, to help expand the impact of visual analytics in organizational as well as casual decision making activities, we initiate a research agenda on how decision making assistance could be elevated throughout visualization research.},
  archive      = {J_TVCG},
  author       = {Evanthia Dimara and John Stasko},
  doi          = {10.1109/TVCG.2021.3114813},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1128-1138},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A critical reflection on visualization research: Where do decision making tasks hide?},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Knowledge rocks: Adding knowledge assistance to
visualization systems. <em>TVCG</em>, <em>28</em>(1), 1117–1127. (<a
href="https://doi.org/10.1109/TVCG.2021.3114687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Knowledge Rocks, an implementation strategy and guideline for augmenting visualization systems to knowledge-assisted visualization systems, as defined by the KAVA model. Visualization systems become more and more sophisticated. Hence, it is increasingly important to support users with an integrated knowledge base in making constructive choices and drawing the right conclusions. We support the effective reactivation of visualization software resources by augmenting them with knowledge-assistance. To provide a general and yet supportive implementation strategy, we propose an implementation process that bases on an application-agnostic architecture. This architecture is derived from existing knowledge-assisted visualization systems and the KAVA model. Its centerpiece is an ontology that is able to automatically analyze and classify input data, linked to a database to store classified instances. We discuss design decisions and advantages of the KR framework and illustrate its broad area of application in diverse integration possibilities of this architecture into an existing visualization system. In addition, we provide a detailed case study by augmenting an it-security system with knowledge-assistance facilities.},
  archive      = {J_TVCG},
  author       = {Anna-Pia Lohfink and Simon D. Duque Anton and Heike Leitte and Christoph Garth},
  doi          = {10.1109/TVCG.2021.3114687},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1117-1127},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Knowledge rocks: Adding knowledge assistance to visualization systems},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GenNI: Human-AI collaboration for data-backed text
generation. <em>TVCG</em>, <em>28</em>(1), 1106–1116. (<a
href="https://doi.org/10.1109/TVCG.2021.3114845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Table2Text systems generate textual output based on structured data utilizing machine learning. These systems are essential for fluent natural language interfaces in tools such as virtual assistants; however, left to generate freely these ML systems often produce misleading or unexpected outputs. GenNI (Generation Negotiation Interface) is an interactive visual system for high-level human-AI collaboration in producing descriptive text. The tool utilizes a deep learning model designed with explicit control states. These controls allow users to globally constrain model generations, without sacrificing the representation power of the deep learning models. The visual interface makes it possible for users to interact with AI systems following a Refine-Forecast paradigm to ensure that the generation system acts in a manner human users find suitable. We report multiple use cases on two experiments that improve over uncontrolled generation approaches, while at the same time providing fine-grained control. A demo and source code are available at https://genni.vizhub.ai.},
  archive      = {J_TVCG},
  author       = {Hendrik Strobelt and Jambay Kinley and Robert Krueger and Johanna Beyer and Hanspeter Pfister and Alexander M. Rush},
  doi          = {10.1109/TVCG.2021.3114845},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1106-1116},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GenNI: Human-AI collaboration for data-backed text generation},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Communicating visualizations without visuals: Investigation
of visualization alternative text for people with visual impairments.
<em>TVCG</em>, <em>28</em>(1), 1095–1105. (<a
href="https://doi.org/10.1109/TVCG.2021.3114846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alternative text is critical in communicating graphics to people who are blind or have low vision. Especially for graphics that contain rich information, such as visualizations, poorly written or an absence of alternative texts can worsen the information access inequality for people with visual impairments. In this work, we consolidate existing guidelines and survey current practices to inspect to what extent current practices and recommendations are aligned. Then, to gain more insight into what people want in visualization alternative texts, we interviewed 22 people with visual impairments regarding their experience with visualizations and their information needs in alternative texts. The study findings suggest that participants actively try to construct an image of visualizations in their head while listening to alternative texts and wish to carry out visualization tasks (e.g., retrieve specific values) as sighted viewers would. The study also provides ample support for the need to reference the underlying data instead of visual elements to reduce users&#39; cognitive burden. Informed by the study, we provide a set of recommendations to compose an informative alternative text.},
  archive      = {J_TVCG},
  author       = {Crescentia Jung and Shubham Mehta and Atharva Kulkarni and Yuhang Zhao and Yea-Seul Kim},
  doi          = {10.1109/TVCG.2021.3114846},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1095-1105},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Communicating visualizations without visuals: Investigation of visualization alternative text for people with visual impairments},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards understanding sensory substitution for accessible
visualization: An interview study. <em>TVCG</em>, <em>28</em>(1),
1084–1094. (<a href="https://doi.org/10.1109/TVCG.2021.3114829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For all its potential in supporting data analysis, particularly in exploratory situations, visualization also creates barriers: accessibility for blind and visually impaired individuals. Regardless of how effective a visualization is, providing equal access for blind users requires a paradigm shift for the visualization research community. To enact such a shift, it is not sufficient to treat visualization accessibility as merely another technical problem to overcome. Instead, supporting the millions of blind and visually impaired users around the world who have equally valid needs for data analysis as sighted individuals requires a respectful, equitable, and holistic approach that includes all users from the onset. In this paper, we draw on accessibility research methodologies to make inroads towards such an approach. We first identify the people who have specific insight into how blind people perceive the world: orientation and mobility (O&amp;M) experts, who are instructors that teach blind individuals how to navigate the physical world using non-visual senses. We interview 10 O&amp;M experts—all of them blind—to understand how best to use sensory substitution other than the visual sense for conveying spatial layouts. Finally, we investigate our qualitative findings using thematic analysis. While blind people in general tend to use both sound and touch to understand their surroundings, we focused on auditory affordances and how they can be used to make data visualizations accessible—using sonification and auralization. However, our experts recommended supporting a combination of senses—sound and touch—to make charts accessible as blind individuals may be more familiar with exploring tactile charts. We report results on both sound and touch affordances, and conclude by discussing implications for accessible visualization for blind individuals.},
  archive      = {J_TVCG},
  author       = {Pramod Chundury and Biswaksen Patnaik and Yasmin Reyazuddin and Christine Tang and Jonathan Lazar and Niklas Elmqvist},
  doi          = {10.1109/TVCG.2021.3114829},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1084-1094},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards understanding sensory substitution for accessible visualization: An interview study},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accessible visualization via natural language descriptions:
A four-level model of semantic content. <em>TVCG</em>, <em>28</em>(1),
1073–1083. (<a href="https://doi.org/10.1109/TVCG.2021.3114770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization.},
  archive      = {J_TVCG},
  author       = {Alan Lundgard and Arvind Satyanarayan},
  doi          = {10.1109/TVCG.2021.3114770},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1073-1083},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Accessible visualization via natural language descriptions: A four-level model of semantic content},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DDLVis: Real-time visual query of spatiotemporal data
distribution via density dictionary learning. <em>TVCG</em>,
<em>28</em>(1), 1062–1072. (<a
href="https://doi.org/10.1109/TVCG.2021.3114762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual query of spatiotemporal data is becoming an increasingly important function in visual analytics applications. Various works have been presented for querying large spatiotemporal data in real time. However, the real-time query of spatiotemporal data distribution is still an open challenge. As spatiotemporal data become larger, methods of aggregation, storage and querying become critical. We propose a new visual query system that creates a low-memory storage component and provides real-time visual interactions of spatiotemporal data. We first present a peak-based kernel density estimation method to produce the data distribution for the spatiotemporal data. Then a novel density dictionary learning approach is proposed to compress temporal density maps and accelerate the query calculation. Moreover, various intuitive query interactions are presented to interactively gain patterns. The experimental results obtained on three datasets demonstrate that the presented system offers an effective query for visual analytics of spatiotemporal data.},
  archive      = {J_TVCG},
  author       = {Chenhui Li and George Baciu and Yunzhe Wang and Junjie Chen and Changbo Wang},
  doi          = {10.1109/TVCG.2021.3114762},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1062-1072},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DDLVis: Real-time visual query of spatiotemporal data distribution via density dictionary learning},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Compass: Towards better causal analysis of urban time
series. <em>TVCG</em>, <em>28</em>(1), 1051–1061. (<a
href="https://doi.org/10.1109/TVCG.2021.3114875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spatial time series generated by city sensors allow us to observe urban phenomena like environmental pollution and traffic congestion at an unprecedented scale. However, recovering causal relations from these observations to explain the sources of urban phenomena remains a challenging task because these causal relations tend to be time-varying and demand proper time series partitioning for effective analyses. The prior approaches extract one causal graph given long-time observations, which cannot be directly applied to capturing, interpreting, and validating dynamic urban causality. This paper presents Compass, a novel visual analytics approach for in-depth analyses of the dynamic causality in urban time series. To develop Compass, we identify and address three challenges: detecting urban causality, interpreting dynamic causal relations, and unveiling suspicious causal relations. First, multiple causal graphs over time among urban time series are obtained with a causal detection framework extended from the Granger causality test. Then, a dynamic causal graph visualization is designed to reveal the time-varying causal relations across these causal graphs and facilitate the exploration of the graphs along the time. Finally, a tailored multi-dimensional visualization is developed to support the identification of spurious causal relations, thereby improving the reliability of causal analyses. The effectiveness of Compass is evaluated with two case studies conducted on the real-world urban datasets, including the air pollution and traffic speed datasets, and positive feedback was received from domain experts.},
  archive      = {J_TVCG},
  author       = {Zikun Deng and Di Weng and Xiao Xie and Jie Bao and Yu Zheng and Mingliang Xu and Wei Chen and Yingcai Wu},
  doi          = {10.1109/TVCG.2021.3114875},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1051-1061},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Compass: Towards better causal analysis of urban time series},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Where can we help? A visual analytics approach to diagnosing
and improving semantic segmentation of movable objects. <em>TVCG</em>,
<em>28</em>(1), 1040–1050. (<a
href="https://doi.org/10.1109/TVCG.2021.3114855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a critical component in autonomous driving and has to be thoroughly evaluated due to safety concerns. Deep neural network (DNN) based semantic segmentation models are widely used in autonomous driving. However, it is challenging to evaluate DNN-based models due to their black-box-like nature, and it is even more difficult to assess model performance for crucial objects, such as lost cargos and pedestrians, in autonomous driving applications. In this work, we propose VASS , a V isual A nalytics approach to diagnosing and improving the accuracy and robustness of S emantic S egmentation models, especially for critical objects moving in various driving scenes. The key component of our approach is a context-aware spatial representation learning that extracts important spatial information of objects, such as position, size, and aspect ratio, with respect to given scene contexts. Based on this spatial representation, we first use it to create visual summarization to analyze models&#39; performance. We then use it to guide the generation of adversarial examples to evaluate models&#39; spatial robustness and obtain actionable insights. We demonstrate the effectiveness of VASS via two case studies of lost cargo detection and pedestrian detection in autonomous driving. For both cases, we show quantitative evaluation on the improvement of models&#39; performance with actionable insights obtained from VASS .},
  archive      = {J_TVCG},
  author       = {Wenbin He and Lincan Zou and Arvind Kumar Shekar and Liang Gou and Liu Ren},
  doi          = {10.1109/TVCG.2021.3114855},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1040-1050},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Where can we help? a visual analytics approach to diagnosing and improving semantic segmentation of movable objects},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual evaluation for autonomous driving. <em>TVCG</em>,
<em>28</em>(1), 1030–1039. (<a
href="https://doi.org/10.1109/TVCG.2021.3114777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous driving technologies often use state-of-the-art artificial intelligence algorithms to understand the relationship between the vehicle and the external environment, to predict the changes of the environment, and then to plan and control the behaviors of the vehicle accordingly. The complexity of such technologies makes it challenging to evaluate the performance of autonomous driving systems and to find ways to improve them. The current approaches to evaluating such autonomous driving systems largely use a single score to indicate the overall performance of a system, but domain experts have difficulties in understanding how individual components or algorithms in an autonomous driving system may contribute to the score. To address this problem, we collaborate with domain experts on autonomous driving algorithms, and propose a visual evaluation method for autonomous driving. Our method considers the data generated in all components during the whole process of autonomous driving, including perception results, planning routes, prediction of obstacles, various controlling parameters, and evaluation of comfort. We develop a visual analytics workflow to integrate an evaluation mathematical model with adjustable parameters, support the evaluation of the system from the level of the overall performance to the level of detailed measures of individual components, and to show both evaluation scores and their contributing factors. Our implemented visual analytics system provides an overview evaluation score at the beginning and shows the animation of the dynamic change of the scores at each period. Experts can interactively explore the specific component at different time periods and identify related factors. With our method, domain experts not only learn about the performance of an autonomous driving system, but also identify and access the problematic parts of each component. Our visual evaluation system can be applied to the autonomous driving simulation system and used for various evaluation cases. The results of using our system in some simulation cases and the feedback from involved domain experts confirm the usefulness and efficiency of our method in helping people gain in-depth insight into autonomous driving systems.},
  archive      = {J_TVCG},
  author       = {Yijie Hou and Chengshun Wang and Junhong Wang and Xiangyang Xue and Xiaolong Luke Zhang and Jun Zhu and Dongliang Wang and Siming Chen},
  doi          = {10.1109/TVCG.2021.3114777},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1030-1039},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual evaluation for autonomous driving},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geo-context aware study of vision-based autonomous driving
models and spatial video data. <em>TVCG</em>, <em>28</em>(1), 1019–1029.
(<a href="https://doi.org/10.1109/TVCG.2021.3114853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based deep learning (DL) methods have made great progress in learning autonomous driving models from large-scale crowd-sourced video datasets. They are trained to predict instantaneous driving behaviors from video data captured by on-vehicle cameras. In this paper, we develop a geo-context aware visualization system for the study of Autonomous Driving Model (ADM) predictions together with large-scale ADM video data. The visual study is seamlessly integrated with the geographical environment by combining DL model performance with geospatial visualization techniques. Model performance measures can be studied together with a set of geospatial attributes over map views. Users can also discover and compare prediction behaviors of multiple DL models in both city-wide and street-level analysis, together with road images and video contents. Therefore, the system provides a new visual exploration platform for DL model designers in autonomous driving. Use cases and domain expert evaluation show the utility and effectiveness of the visualization system.},
  archive      = {J_TVCG},
  author       = {Suphanut Jamonnak and Ye Zhao and Xinyi Huang and Md Amiruzzaman},
  doi          = {10.1109/TVCG.2021.3114853},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1019-1029},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Geo-context aware study of vision-based autonomous driving models and spatial video data},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lumos: Increasing awareness of analytic behavior during
visual data analysis. <em>TVCG</em>, <em>28</em>(1), 1009–1018. (<a
href="https://doi.org/10.1109/TVCG.2021.3114827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual data analysis tools provide people with the agency and flexibility to explore data using a variety of interactive functionalities. However, this flexibility may introduce potential consequences in situations where users unknowingly overemphasize or underemphasize specific subsets of the data or attribute space they are analyzing. For example, users may overemphasize specific attributes and/or their values (e.g., Gender is always encoded on the X axis), underemphasize others (e.g., Religion is never encoded), ignore a subset of the data (e.g., older people are filtered out), etc. In response, we present Lumos, a visual data analysis tool that captures and shows the interaction history with data to increase awareness of such analytic behaviors. Using in-situ (at the place of interaction) and ex-situ (in an external view) visualization techniques, Lumos provides real-time feedback to users for them to reflect on their activities. For example, Lumos highlights datapoints that have been previously examined in the same visualization (in-situ) and also overlays them on the underlying data distribution (i.e., baseline distribution) in a separate visualization (ex-situ). Through a user study with 24 participants, we investigate how Lumos helps users&#39; data exploration and decision-making processes. We found that Lumos increases users&#39; awareness of visual data analysis practices in real-time, promoting reflection upon and acknowledgement of their intentions and potentially influencing subsequent interactions.},
  archive      = {J_TVCG},
  author       = {Arpit Narechania and Adam Coscia and Emily Wall and Alex Endert},
  doi          = {10.1109/TVCG.2021.3114827},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1009-1018},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Lumos: Increasing awareness of analytic behavior during visual data analysis},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving visualization interpretation using
counterfactuals. <em>TVCG</em>, <em>28</em>(1), 998–1008. (<a
href="https://doi.org/10.1109/TVCG.2021.3114779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex, high-dimensional data is used in a wide range of domains to explore problems and make decisions. Analysis of high-dimensional data, however, is vulnerable to the hidden influence of confounding variables, especially as users apply ad hoc filtering operations to visualize only specific subsets of an entire dataset. Thus, visual data-driven analysis can mislead users and encourage mistaken assumptions about causality or the strength of relationships between features. This work introduces a novel visual approach designed to reveal the presence of confounding variables via counterfactual possibilities during visual data analysis. It is implemented in CoFact, an interactive visualization prototype that determines and visualizes counterfactual subsets to better support user exploration of feature relationships. Using publicly available datasets, we conducted a controlled user study to demonstrate the effectiveness of our approach; the results indicate that users exposed to counterfactual visualizations formed more careful judgments about feature-to-outcome relationships.},
  archive      = {J_TVCG},
  author       = {Smiti Kaul and David Borland and Nan Cao and David Gotz},
  doi          = {10.1109/TVCG.2021.3114779},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {998-1008},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Improving visualization interpretation using counterfactuals},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The weighted average illusion: Biases in perceived mean
position in scatterplots. <em>TVCG</em>, <em>28</em>(1), 987–997. (<a
href="https://doi.org/10.1109/TVCG.2021.3114783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scatterplots can encode a third dimension by using additional channels like size or color (e.g. bubble charts). We explore a potential misinterpretation of trivariate scatterplots, which we call the weighted average illusion , where locations of larger and darker points are given more weight toward x- and y-mean estimates. This systematic bias is sensitive to a designer&#39;s choice of size or lightness ranges mapped onto the data. In this paper, we quantify this bias against varying size/lightness ranges and data correlations. We discuss possible explanations for its cause by measuring attention given to individual data points using a vision science technique called the centroid method. Our work illustrates how ensemble processing mechanisms and mental shortcuts can significantly distort visual summaries of data, and can lead to misjudgments like the demonstrated weighted average illusion.},
  archive      = {J_TVCG},
  author       = {Matt-Heun Hong and Jessica K. Witt and Danielle Albers Szafir},
  doi          = {10.1109/TVCG.2021.3114783},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {987-997},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The weighted average illusion: Biases in perceived mean position in scatterplots},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VisQA: X-raying vision and language reasoning in
transformers. <em>TVCG</em>, <em>28</em>(1), 976–986. (<a
href="https://doi.org/10.1109/TVCG.2021.3114683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering systems target answering open-ended textual questions given input images. They are a testbed for learning high-level reasoning with a primary use in HCI, for instance assistance for the visually impaired. Recent research has shown that state-of-the-art models tend to produce answers exploiting biases and shortcuts in the training data, and sometimes do not even look at the input image, instead of performing the required reasoning steps. We present VisQA, a visual analytics tool that explores this question of reasoning vs. bias exploitation. It exposes the key element of state-of-the-art neural models — attention maps in transformers. Our working hypothesis is that reasoning steps leading to model predictions are observable from attention distributions, which are particularly useful for visualization. The design process of VisQA was motivated by well-known bias examples from the fields of deep learning and vision-language reasoning and evaluated in two ways. First, as a result of a collaboration of three fields, machine learning, vision and language reasoning, and data analytics, the work lead to a better understanding of bias exploitation of neural models for VQA, which eventually resulted in an impact on its design and training through the proposition of a method for the transfer of reasoning patterns from an oracle model. Second, we also report on the design of VisQA, and a goal-oriented evaluation of VisQA targeting the analysis of a model decision process from multiple experts, providing evidence that it makes the inner workings of models accessible to users.},
  archive      = {J_TVCG},
  author       = {Théo Jaunet and Corentin Kervadec and Romain Vuillemot and Grigory Antipov and Moez Baccouche and Christian Wolf},
  doi          = {10.1109/TVCG.2021.3114683},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {976-986},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VisQA: X-raying vision and language reasoning in transformers},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Left, right, and gender: Exploring interaction traces to
mitigate human biases. <em>TVCG</em>, <em>28</em>(1), 966–975. (<a
href="https://doi.org/10.1109/TVCG.2021.3114862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human biases impact the way people analyze data and make decisions. Recent work has shown that some visualization designs can better support cognitive processes and mitigate cognitive biases (i.e., errors that occur due to the use of mental “shortcuts”). In this work, we explore how visualizing a user&#39;s interaction history (i.e., which data points and attributes a user has interacted with) can be used to mitigate potential biases that drive decision making by promoting conscious reflection of one&#39;s analysis process. Given an interactive scatterplot-based visualization tool, we showed interaction history in real-time while exploring data (by coloring points in the scatterplot that the user has interacted with), and in a summative format after a decision has been made (by comparing the distribution of user interactions to the underlying distribution of the data). We conducted a series of in-lab experiments and a crowd-sourced experiment to evaluate the effectiveness of interaction history interventions toward mitigating bias. We contextualized this work in a political scenario in which participants were instructed to choose a committee of 10 fictitious politicians to review a recent bill passed in the U.S. state of Georgia banning abortion after 6 weeks, where things like gender bias or political party bias may drive one&#39;s analysis process. We demonstrate the generalizability of this approach by evaluating a second decision making scenario related to movies. Our results are inconclusive for the effectiveness of interaction history (henceforth referred to as interaction traces) toward mitigating biased decision making. However, we find some mixed support that interaction traces, particularly in a summative format, can increase awareness of potential unconscious biases.},
  archive      = {J_TVCG},
  author       = {Emily Wall and Arpit Narechania and Adam Coscia and Jamal Paden and Alex Endert},
  doi          = {10.1109/TVCG.2021.3114862},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {966-975},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Left, right, and gender: Exploring interaction traces to mitigate human biases},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual arrangements of bar charts influence comparisons in
viewer takeaways. <em>TVCG</em>, <em>28</em>(1), 955–965. (<a
href="https://doi.org/10.1109/TVCG.2021.3114823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Well-designed data visualizations can lead to more powerful and intuitive processing by a viewer. To help a viewer intuitively compare values to quickly generate key takeaways, visualization designers can manipulate how data values are arranged in a chart to afford particular comparisons. Using simple bar charts as a case study, we empirically tested the comparison affordances of four common arrangements: vertically juxtaposed, horizontally juxtaposed, overlaid, and stacked. We asked participants to type out what patterns they perceived in a chart and we coded their takeaways into types of comparisons. In a second study, we asked data visualization design experts to predict which arrangement they would use to afford each type of comparison and found both alignments and mismatches with our findings. These results provide concrete guidelines for how both human designers and automatic chart recommendation systems can make visualizations that help viewers extract the “right” takeaway.},
  archive      = {J_TVCG},
  author       = {Cindy Xiong and Vidya Setlur and Benjamin Bach and Eunyee Koh and Kylie Lin and Steven Franconeri},
  doi          = {10.1109/TVCG.2021.3114823},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {955-965},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual arrangements of bar charts influence comparisons in viewer takeaways},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interactive data comics. <em>TVCG</em>, <em>28</em>(1),
944–954. (<a href="https://doi.org/10.1109/TVCG.2021.3114849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates how to make data comics interactive. Data comics are an effective and versatile means for visual communication, leveraging the power of sequential narration and combined textual and visual content, while providing an overview of the storyline through panels assembled in expressive layouts. While a powerful static storytelling medium that works well on paper support, adding interactivity to data comics can enable non-linear storytelling, personalization, levels of details, explanations, and potentially enriched user experiences. This paper introduces a set of operations tailored to support data comics narrative goals that go beyond the traditional linear, immutable storyline curated by a story author. The goals and operations include adding and removing panels into pre-defined layouts to support branching, change of perspective, or access to detail-on-demand, as well as providing and modifying data, and interacting with data representation, to support personalization and reader-defined data focus. We propose a lightweight specification language, COMICSCRIPT, for designers to add such interactivity to static comics. To assess the viability of our authoring process, we recruited six professional illustrators, designers and data comics enthusiasts and asked them to craft an interactive comic, allowing us to understand authoring workflow and potential of our approach. We present examples of interactive comics in a gallery. This initial step towards understanding the design space of interactive comics can inform the design of creation tools and experiences for interactive storytelling.},
  archive      = {J_TVCG},
  author       = {Zezhong Wang and Hugo Romat and Fanny Chevalier and Nathalie Henry Riche and Dave Murray-Rust and Benjamin Bach},
  doi          = {10.1109/TVCG.2021.3114849},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {944-954},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive data comics},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kineticharts: Augmenting affective expressiveness of charts
in data stories with animation design. <em>TVCG</em>, <em>28</em>(1),
933–943. (<a href="https://doi.org/10.1109/TVCG.2021.3114775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data stories often seek to elicit affective feelings from viewers. However, how to design affective data stories remains under-explored. In this work, we investigate one specific design factor, animation, and present Kineticharts, an animation design scheme for creating charts that express five positive affects: joy, amusement, surprise, tenderness, and excitement. These five affects were found to be frequently communicated through animation in data stories. Regarding each affect, we designed varied kinetic motions represented by bar charts, line charts, and pie charts, resulting in 60 animated charts for the five affects. We designed Kineticharts by first conducting a need-finding study with professional practitioners from data journalism and then analyzing a corpus of affective motion graphics to identify salient kinetic patterns. We evaluated Kineticharts through two user studies. The results suggest that Kineticharts can accurately convey affects, and improve the expressiveness of data stories, as well as enhance user engagement without hindering data comprehension compared to the animation design from DataClips, an authoring tool for data videos.},
  archive      = {J_TVCG},
  author       = {Xingyu Lan and Yang Shi and Yanqiu Wu and Xiaohan Jiao and Nan Cao},
  doi          = {10.1109/TVCG.2021.3114775},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {933-943},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Kineticharts: Augmenting affective expressiveness of charts in data stories with animation design},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A design space for applying the freytag’s pyramid structure
to data stories. <em>TVCG</em>, <em>28</em>(1), 922–932. (<a
href="https://doi.org/10.1109/TVCG.2021.3114774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data stories integrate compelling visual content to communicate data insights in the form of narratives. The narrative structure of a data story serves as the backbone that determines its expressiveness, and it can largely influence how audiences perceive the insights. Freytag&#39;s Pyramid is a classic narrative structure that has been widely used in film and literature. While there are continuous recommendations and discussions about applying Freytag&#39;s Pyramid to data stories, little systematic and practical guidance is available on how to use Freytag&#39;s Pyramid for creating structured data stories. To bridge this gap, we examined how existing practices apply Freytag&#39;s Pyramid by analyzing stories extracted from 103 data videos. Based on our findings, we proposed a design space of narrative patterns, data flows, and visual communications to provide practical guidance on achieving narrative intents, organizing data facts, and selecting visual design techniques through story creation. We evaluated the proposed design space through a workshop with 25 participants. Results show that our design space provides a clear framework for rapid storyboarding of data stories with Freytag&#39;s Pyramid.},
  archive      = {J_TVCG},
  author       = {Leni Yang and Xian Xu and XingYu Lan and Ziyan Liu and Shunan Guo and Yang Shi and Huamin Qu and Nan Cao},
  doi          = {10.1109/TVCG.2021.3114774},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {922-932},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A design space for applying the freytag&#39;s pyramid structure to data stories},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EVis: Visually analyzing environmentally driven events.
<em>TVCG</em>, <em>28</em>(1), 912–921. (<a
href="https://doi.org/10.1109/TVCG.2021.3114867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Earth scientists are increasingly employing time series data with multiple dimensions and high temporal resolution to study the impacts of climate and environmental changes on Earth&#39;s atmosphere, biosphere, hydrosphere, and lithosphere. However, the large number of variables and varying time scales of antecedent conditions contributing to natural phenomena hinder scientists from completing more than the most basic analyses. In this paper, we present EVis (Environmental Visualization), a new visual analytics prototype to help scientists analyze and explore recurring environmental events (e.g. rock fracture, landslides, heat waves, floods) and their relationships with high dimensional time series of continuous numeric environmental variables, such as ambient temperature and precipitation. EVis provides coordinated scatterplots, heatmaps, histograms, and RadViz for foundational analyses. These features allow users to interactively examine relationships between events and one, two, three, or more environmental variables. EVis also provides a novel visual analytics approach to allowing users to discover temporally lagging relationships related to antecedent conditions between events and multiple variables, a critical task in Earth sciences. In particular, this latter approach projects multivariate time series onto trajectories in a 2D space using RadViz, and clusters the trajectories for temporal pattern discovery. Our case studies with rock cracking data and interviews with domain experts from a range of sub-disciplines within Earth sciences illustrate the extensive applicability and usefulness of EVis.},
  archive      = {J_TVCG},
  author       = {Tinghao Feng and Jing Yang and Martha-Cary Eppes and Zhaocong Yang and Faye Moser},
  doi          = {10.1109/TVCG.2021.3114867},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {912-921},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EVis: Visually analyzing environmentally driven events},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sequen-c: A multilevel overview of temporal event sequences.
<em>TVCG</em>, <em>28</em>(1), 901–911. (<a
href="https://doi.org/10.1109/TVCG.2021.3114868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building a visual overview of temporal event sequences with an optimal level-of-detail (i.e. simplified but informative) is an ongoing challenge - expecting the user to zoom into every important aspect of the overview can lead to missing insights. We propose a technique to build a multilevel overview of event sequences, whose granularity can be transformed across sequence clusters (vertical level-of-detail) or longitudinally (horizontal level-of-detail), using hierarchical aggregation and a novel cluster data representation Align-Score-Simplify. By default, the overview shows an optimal number of sequence clusters obtained through the average silhouette width metric - then users are able to explore alternative optimal sequence clusterings. The vertical level-of-detail of the overview changes along with the number of clusters, whilst the horizontal level-of-detail refers to the level of summarization applied to each cluster representation. The proposed technique has been implemented into a visualization system called Sequence Cluster Explorer (Sequen-C) that allows multilevel and detail-on-demand exploration through three coordinated views, and the inspection of data attributes at cluster, unique sequence, and individual sequence level. We present two case studies using real-world datasets in the healthcare domain: CUREd and MIMIC-III; which demonstrate how the technique can aid users to obtain a summary of common and deviating pathways, and explore data attributes for selected patterns.},
  archive      = {J_TVCG},
  author       = {Jessica Magallanes and Tony Stone and Paul D Morris and Suzanne Mason and Steven Wood and Maria-Cruz Villa-Uriol},
  doi          = {10.1109/TVCG.2021.3114868},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {901-911},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Sequen-C: A multilevel overview of temporal event sequences},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). KD-box: Line-segment-based KD-tree for interactive
exploration of large-scale time-series data. <em>TVCG</em>,
<em>28</em>(1), 890–900. (<a
href="https://doi.org/10.1109/TVCG.2021.3114865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-series data-usually presented in the form of lines-plays an important role in many domains such as finance, meteorology, health, and urban informatics. Yet, little has been done to support interactive exploration of large-scale time-series data, which requires a clutter-free visual representation with low-latency interactions. In this paper, we contribute a novel line-segment-based KD-tree method to enable interactive analysis of many time series. Our method enables not only fast queries over time series in selected regions of interest but also a line splatting method for efficient computation of the density field and selection of representative lines. Further, we develop KD-Box, an interactive system that provides rich interactions, e.g., timebox, attribute filtering, and coordinated multiple views. We demonstrate the effectiveness of KD-Box in supporting efficient line query and density field computation through a quantitative comparison and show its usefulness for interactive visual analysis on several real-world datasets.},
  archive      = {J_TVCG},
  author       = {Yue Zhao and Yunhai Wang and Jian Zhang and Chi-Wing Fu and Mingliang Xu and Dominik Moritz},
  doi          = {10.1109/TVCG.2021.3114865},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {890-900},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {KD-box: Line-segment-based KD-tree for interactive exploration of large-scale time-series data},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time visual analysis of high-volume social media posts.
<em>TVCG</em>, <em>28</em>(1), 879–889. (<a
href="https://doi.org/10.1109/TVCG.2021.3114800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breaking news and first-hand reports often trend on social media platforms before traditional news outlets cover them. The real-time analysis of posts on such platforms can reveal valuable and timely insights for journalists, politicians, business analysts, and first responders, but the high number and diversity of new posts pose a challenge. In this work, we present an interactive system that enables the visual analysis of streaming social media data on a large scale in real-time. We propose an efficient and explainable dynamic clustering algorithm that powers a continuously updated visualization of the current thematic landscape as well as detailed visual summaries of specific topics of interest. Our parallel clustering strategy provides an adaptive stream with a digestible but diverse selection of recent posts related to relevant topics. We also integrate familiar visual metaphors that are highly interlinked for enabling both explorative and more focused monitoring tasks. Analysts can gradually increase the resolution to dive deeper into particular topics. In contrast to previous work, our system also works with non-geolocated posts and avoids extensive preprocessing such as detecting events. We evaluated our dynamic clustering algorithm and discuss several use cases that show the utility of our system.},
  archive      = {J_TVCG},
  author       = {Johannes Knittel and Steffen Koch and Tan Tang and Wei Chen and Yingcai Wu and Shixia Liu and Thomas Ertl},
  doi          = {10.1109/TVCG.2021.3114800},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {879-889},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Real-time visual analysis of high-volume social media posts},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MiningVis: Visual analytics of the bitcoin mining economy.
<em>TVCG</em>, <em>28</em>(1), 868–878. (<a
href="https://doi.org/10.1109/TVCG.2021.3114821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a visual analytics tool, MiningVis, to explore the long-term historical evolution and dynamics of the Bitcoin mining ecosystem. Bitcoin is a cryptocurrency that attracts much attention but remains difficult to understand. Particularly important to the success, stability, and security of Bitcoin is a component of the system called “mining.” Miners are responsible for validating transactions and are incentivized to participate by the promise of a monetary reward. Mining pools have emerged as collectives of miners that ensure a more stable and predictable income. MiningVis aims to help analysts understand the evolution and dynamics of the Bitcoin mining ecosystem, including mining market statistics, multi-measure mining pool rankings, and pool hopping behavior. Each of these features can be compared to external data concerning pool characteristics and Bitcoin news. In order to assess the value of MiningVis, we conducted online interviews and insight-based user studies with Bitcoin miners. We describe research questions tackled and insights made by our participants and illustrate practical implications for visual analytics systems for Bitcoin mining.},
  archive      = {J_TVCG},
  author       = {Natkamon Tovanich and Nicolas Soulié and Nicolas Heulot and Petra Isenberg},
  doi          = {10.1109/TVCG.2021.3114821},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {868-878},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MiningVis: Visual analytics of the bitcoin mining economy},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A visualization approach for monitoring order processing in
e-commerce warehouse. <em>TVCG</em>, <em>28</em>(1), 857–867. (<a
href="https://doi.org/10.1109/TVCG.2021.3114878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficiency of warehouses is vital to e-commerce. Fast order processing at the warehouses ensures timely deliveries and improves customer satisfaction. However, monitoring, analyzing, and manipulating order processing in the warehouses in real time are challenging for traditional methods due to the sheer volume of incoming orders, the fuzzy definition of delayed order patterns, and the complex decision-making of order handling priorities. In this paper, we adopt a data-driven approach and propose OrderMonitor, a visual analytics system that assists warehouse managers in analyzing and improving order processing efficiency in real time based on streaming warehouse event data. Specifically, the order processing pipeline is visualized with a novel pipeline design based on the sedimentation metaphor to facilitate real-time order monitoring and suggest potentially abnormal orders. We also design a novel visualization that depicts order timelines based on the Gantt charts and Marey&#39;s graphs. Such a visualization helps the managers gain insights into the performance of order processing and find major blockers for delayed orders. Furthermore, an evaluating view is provided to assist users in inspecting order details and assigning priorities to improve the processing performance. The effectiveness of OrderMonitor is evaluated with two case studies on a real-world warehouse dataset.},
  archive      = {J_TVCG},
  author       = {Junxiu Tang and Yuhua Zhou and Tan Tang and Di Weng and Boyang Xie and Lingyun Yu and Huaqiang Zhang and Yingcai Wu},
  doi          = {10.1109/TVCG.2021.3114878},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {857-867},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A visualization approach for monitoring order processing in E-commerce warehouse},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VideoModerator: A risk-aware framework for multimodal video
moderation in e-commerce. <em>TVCG</em>, <em>28</em>(1), 846–856. (<a
href="https://doi.org/10.1109/TVCG.2021.3114781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video moderation, which refers to remove deviant or explicit content from e-commerce livestreams, has become prevalent owing to social and engaging features. However, this task is tedious and time consuming due to the difficulties associated with watching and reviewing multimodal video content, including video frames and audio clips. To ensure effective video moderation, we propose VideoModerator, a risk-aware framework that seamlessly integrates human knowledge with machine insights. This framework incorporates a set of advanced machine learning models to extract the risk-aware features from multimodal video content and discover potentially deviant videos. Moreover, this framework introduces an interactive visualization interface with three views, namely, a video view, a frame view, and an audio view. In the video view, we adopt a segmented timeline and highlight high-risk periods that may contain deviant information. In the frame view, we present a novel visual summarization method that combines risk-aware features and video context to enable quick video navigation. In the audio view, we employ a storyline-based design to provide a multi-faceted overview which can be used to explore audio content. Furthermore, we report the usage of VideoModerator through a case scenario and conduct experiments and a controlled user study to validate its effectiveness.},
  archive      = {J_TVCG},
  author       = {Tan Tang and Yanhong Wu and Yingcai Wu and Lingyun Yu and Yuhong Li},
  doi          = {10.1109/TVCG.2021.3114781},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {846-856},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VideoModerator: A risk-aware framework for multimodal video moderation in E-commerce},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TacticFlow: Visual analytics of ever-changing tactics in
racket sports. <em>TVCG</em>, <em>28</em>(1), 835–845. (<a
href="https://doi.org/10.1109/TVCG.2021.3114832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event sequence mining is often used to summarize patterns from hundreds of sequences but faces special challenges when handling racket sports data. In racket sports (e.g., tennis and badminton), a player hitting the ball is considered a multivariate event consisting of multiple attributes (e.g., hit technique and ball position). A rally (i.e., a series of consecutive hits beginning with one player serving the ball and ending with one player winning a point) thereby can be viewed as a multivariate event sequence. Mining frequent patterns and depicting how patterns change over time is instructive and meaningful to players who want to learn more short-term competitive strategies (i.e., tactics) that encompass multiple hits. However, players in racket sports usually change their tactics rapidly according to the opponent&#39;s reaction, resulting in ever-changing tactic progression. In this work, we introduce a tailored visualization system built on a novel multivariate sequence pattern mining algorithm to facilitate explorative identification and analysis of various tactics and tactic progression. The algorithm can mine multiple non-overlapping multivariate patterns from hundreds of sequences effectively. Based on the mined results, we propose a glyph-based Sankey diagram to visualize the ever-changing tactic progression and support interactive data exploration. Through two case studies with four domain experts in tennis and badminton, we demonstrate that our system can effectively obtain insights about tactic progression in most racket sports. We further discuss the strengths and the limitations of our system based on domain experts&#39; feedback.},
  archive      = {J_TVCG},
  author       = {Jiang Wu and Dongyu Liu and Ziyang Guo and Qingyang Xu and Yingcai Wu},
  doi          = {10.1109/TVCG.2021.3114832},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {835-845},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TacticFlow: Visual analytics of ever-changing tactics in racket sports},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Augmenting sports videos with VisCommentator. <em>TVCG</em>,
<em>28</em>(1), 824–834. (<a
href="https://doi.org/10.1109/TVCG.2021.3114806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualizing data in sports videos is gaining traction in sports analytics, given its ability to communicate insights and explicate player strategies engagingly. However, augmenting sports videos with such data visualizations is challenging, especially for sports analysts, as it requires considerable expertise in video editing. To ease the creation process, we present a design space that characterizes augmented sports videos at an element-level (what the constituents are) and clip-level (how those constituents are organized) . We do so by systematically reviewing 233 examples of augmented sports videos collected from TV channels, teams, and leagues. The design space guides selection of data insights and visualizations for various purposes. Informed by the design space and close collaboration with domain experts, we design VisCommentator, a fast prototyping tool, to eases the creation of augmented table tennis videos by leveraging machine learning-based data extractors and design space-based visualization recommendations. With VisCommentator, sports analysts can create an augmented video by selecting the data to visualize instead of manually drawing the graphical marks . Our system can be generalized to other racket sports (e.g ., tennis, badminton) once the underlying datasets and models are available. A user study with seven domain experts shows high satisfaction with our system, confirms that the participants can reproduce augmented sports videos in a short period, and provides insightful implications into future improvements and opportunities.},
  archive      = {J_TVCG},
  author       = {Zhutian Chen and Shuainan Ye and Xiangtong Chu and Haijun Xia and Hui Zhang and Huamin Qu and Yingcai Wu},
  doi          = {10.1109/TVCG.2021.3114806},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {824-834},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Augmenting sports videos with VisCommentator},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NeuroCartography: Scalable automatic visual summarization of
concepts in deep neural networks. <em>TVCG</em>, <em>28</em>(1),
813–823. (<a href="https://doi.org/10.1109/TVCG.2021.3114858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing research on making sense of deep neural networks often focuses on neuron-level interpretation, which may not adequately capture the bigger picture of how concepts are collectively encoded by multiple neurons. We present Neurocartography, an interactive system that scalably summarizes and visualizes concepts learned by neural networks. It automatically discovers and groups neurons that detect the same concepts, and describes how such neuron groups interact to form higher-level concepts and the subsequent predictions. Neurocartography introduces two scalable summarization techniques: (1) neuron clustering groups neurons based on the semantic similarity of the concepts detected by neurons (e.g., neurons detecting “dog faces” of different breeds are grouped); and (2) neuron embedding encodes the associations between related concepts based on how often they co-occur (e.g., neurons detecting “dog face” and “dog tail” are placed closer in the embedding space). Key to our scalable techniques is the ability to efficiently compute all neuron pairs&#39; relationships, in time linear to the number of neurons instead of quadratic time. Neurocartography scales to large data, such as the ImageNet dataset with 1.2M images. The system&#39;s tightly coordinated views integrate the scalable techniques to visualize the concepts and their relationships, projecting the concept associations to a 2D space in Neuron Projection View, and summarizing neuron clusters and their relationships in Graph View. Through a large-scale human evaluation, we demonstrate that our technique discovers neuron groups that represent coherent, human-meaningful concepts. And through usage scenarios, we describe how our approaches enable interesting and surprising discoveries, such as concept cascades of related and isolated concepts. The Neurocartography visualization runs in modern browsers and is open-sourced.},
  archive      = {J_TVCG},
  author       = {Haekyu Park and Nilaksh Das and Rahul Duggal and Austin P. Wright and Omar Shaikh and Fred Hohman and Duen Horng Polo Chau},
  doi          = {10.1109/TVCG.2021.3114858},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {813-823},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NeuroCartography: Scalable automatic visual summarization of concepts in deep neural networks},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). M2Lens: Visualizing and explaining multimodal models for
sentiment analysis. <em>TVCG</em>, <em>28</em>(1), 802–812. (<a
href="https://doi.org/10.1109/TVCG.2021.3114794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis aims to recognize people&#39;s attitudes from multiple communication channels such as verbal content (i.e., text), voice, and facial expressions. It has become a vibrant and important research topic in natural language processing. Much research focuses on modeling the complex intra- and inter-modal interactions between different communication channels. However, current multimodal models with strong performance are often deep-learning-based techniques and work like black boxes. It is not clear how models utilize multimodal information for sentiment predictions. Despite recent advances in techniques for enhancing the explainability of machine learning models, they often target unimodal scenarios (e.g., images, sentences), and little research has been done on explaining multimodal models. In this paper, we present an interactive visual analytics system, M2 Lens, to visualize and explain multimodal models for sentiment analysis. M2 Lens provides explanations on intra- and inter-modal interactions at the global, subset, and local levels. Specifically, it summarizes the influence of three typical interaction types (i.e., dominance, complement, and conflict) on the model predictions. Moreover, M2 Lens identifies frequent and influential multimodal features and supports the multi-faceted exploration of model behaviors from language, acoustic, and visual modalities. Through two case studies and expert interviews, we demonstrate our system can help users gain deep insights into the multimodal models for sentiment analysis.},
  archive      = {J_TVCG},
  author       = {Xingbo Wang and Jianben He and Zhihua Jin and Muqiao Yang and Yong Wang and Huamin Qu},
  doi          = {10.1109/TVCG.2021.3114794},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {802-812},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {M2Lens: Visualizing and explaining multimodal models for sentiment analysis},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards visual explainable active learning for zero-shot
classification. <em>TVCG</em>, <em>28</em>(1), 791–801. (<a
href="https://doi.org/10.1109/TVCG.2021.3114793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot classification is a promising paradigm to solve an applicable problem when the training classes and test classes are disjoint. Achieving this usually needs experts to externalize their domain knowledge by manually specifying a class-attribute matrix to define which classes have which attributes. Designing a suitable class-attribute matrix is the key to the subsequent procedure, but this design process is tedious and trial-and-error with no guidance. This paper proposes a visual explainable active learning approach with its design and implementation called semantic navigator to solve the above problems. This approach promotes human-AI teaming with four actions (ask, explain, recommend, respond) in each interaction loop. The machine asks contrastive questions to guide humans in the thinking process of attributes. A novel visualization called semantic map explains the current status of the machine. Therefore analysts can better understand why the machine misclassifies objects. Moreover, the machine recommends the labels of classes for each attribute to ease the labeling burden. Finally, humans can steer the model by modifying the labels interactively, and the machine adjusts its recommendations. The visual explainable active learning approach improves humans&#39; efficiency of building zero-shot classification models interactively, compared with the method without guidance. We justify our results with user studies using the standard benchmarks for zero-shot classification.},
  archive      = {J_TVCG},
  author       = {Shichao Jia and Zeyu Li and Nuo Chen and Jiawan Zhang},
  doi          = {10.1109/TVCG.2021.3114793},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {791-801},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards visual explainable active learning for zero-shot classification},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human-in-the-loop extraction of interpretable concepts in
deep learning models. <em>TVCG</em>, <em>28</em>(1), 780–790. (<a
href="https://doi.org/10.1109/TVCG.2021.3114837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interpretation of deep neural networks (DNNs) has become a key topic as more and more people apply them to solve various problems and making critical decisions. Concept-based explanations have recently become a popular approach for post-hoc interpretation of DNNs. However, identifying human-understandable visual concepts that affect model decisions is a challenging task that is not easily addressed with automatic approaches. We present a novel human-in-the-Ioop approach to generate user-defined concepts for model interpretation and diagnostics. Central to our proposal is the use of active learning, where human knowledge and feedback are combined to train a concept extractor with very little human labeling effort. We integrate this process into an interactive system, ConceptExtract. Through two case studies, we show how our approach helps analyze model behavior and extract human-friendly concepts for different machine learning tasks and datasets and how to use these concepts to understand the predictions, compare model performance and make suggestions for model refinement. Quantitative experiments show that our active learning approach can accurately extract meaningful visual concepts. More importantly, by identifying visual concepts that negatively affect model performance, we develop the corresponding data augmentation strategy that consistently improves model performance.},
  archive      = {J_TVCG},
  author       = {Zhenge Zhao and Panpan Xu and Carlos Scheidegger and Liu Ren},
  doi          = {10.1109/TVCG.2021.3114837},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {780-790},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Human-in-the-loop extraction of interpretable concepts in deep learning models},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AffectiveTDA: Using topological data analysis to improve
analysis and explainability in affective computing. <em>TVCG</em>,
<em>28</em>(1), 769–779. (<a
href="https://doi.org/10.1109/TVCG.2021.3114784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an approach utilizing Topological Data Analysis to study the structure of face poses used in affective computing, i.e., the process of recognizing human emotion. The approach uses a conditional comparison of different emotions, both respective and irrespective of time, with multiple topological distance metrics, dimension reduction techniques, and face subsections (e.g., eyes, nose, mouth, etc.). The results confirm that our topology-based approach captures known patterns, distinctions between emotions, and distinctions between individuals, which is an important step towards more robust and explainable emotion recognition by machines.},
  archive      = {J_TVCG},
  author       = {Hamza Elhamdadi and Shaun Canavan and Paul Rosen},
  doi          = {10.1109/TVCG.2021.3114784},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {769-779},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {AffectiveTDA: Using topological data analysis to improve analysis and explainability in affective computing},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interactive dimensionality reduction for comparative
analysis. <em>TVCG</em>, <em>28</em>(1), 758–768. (<a
href="https://doi.org/10.1109/TVCG.2021.3114807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding the similarities and differences between groups of datasets is a fundamental analysis task. For high-dimensional data, dimensionality reduction (DR) methods are often used to find the characteristics of each group. However, existing DR methods provide limited capability and flexibility for such comparative analysis as each method is designed only for a narrow analysis target, such as identifying factors that most differentiate groups. This paper presents an interactive DR framework where we integrate our new DR method, called ULCA (unified linear comparative analysis), with an interactive visual interface. ULCA unifies two DR schemes, discriminant analysis and contrastive learning, to support various comparative analysis tasks. To provide flexibility for comparative analysis, we develop an optimization algorithm that enables analysts to interactively refine ULCA results. Additionally, the interactive visualization interface facilitates interpretation and refinement of the ULCA results. We evaluate ULCA and the optimization algorithm to show their efficiency as well as present multiple case studies using real-world datasets to demonstrate the usefulness of this framework.},
  archive      = {J_TVCG},
  author       = {Takanori Fujiwara and Xinhai Wei and Jian Zhao and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2021.3114807},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {758-768},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive dimensionality reduction for comparative analysis},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VizSnippets: Compressing visualization bundles into
representative previews for browsing visualization collections.
<em>TVCG</em>, <em>28</em>(1), 747–757. (<a
href="https://doi.org/10.1109/TVCG.2021.3114841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualization collections, accessed by platforms such as Tableau Online or Power Bl, are used by millions of people to share and access diverse analytical knowledge in the form of interactive visualization bundles. Result snippets, compact previews of these bundles, are presented to users to help them identify relevant content when browsing collections. Our engagement with Tableau product teams and review of existing snippet designs on five platforms showed us that current practices fail to help people judge the relevance of bundles because they include only the title and one image. Users frequently need to undertake the time-consuming endeavour of opening a bundle within its visualization system to examine its many views and dashboards. In response, we contribute the first systematic approach to visualization snippet design. We propose a framework for snippet design that addresses eight key challenges that we identify. We present a computational pipeline to compress the visual and textual content of bundles into representative previews that is adaptive to a provided pixel budget and provides high information density with multiple images and carefully chosen keywords. We also reflect on the method of visual inspection through random sampling to gain confidence in model and parameter choices.},
  archive      = {J_TVCG},
  author       = {Michael Oppermann and Tamara Munzner},
  doi          = {10.1109/TVCG.2021.3114841},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {747-757},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VizSnippets: Compressing visualization bundles into representative previews for browsing visualization collections},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DIEL: Interactive visualization beyond the here and now.
<em>TVCG</em>, <em>28</em>(1), 737–746. (<a
href="https://doi.org/10.1109/TVCG.2021.3114796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive visualization design and research have primarily focused on local data and synchronous events. However, for more complex use cases-e.g., remote database access and streaming data sources-developers must grapple with distributed data and asynchronous events. Currently, constructing these use cases is difficult and time-consuming; developers are forced to operationally program low-level details like asynchronous database querying and reactive event handling. This approach is in stark contrast to modern methods for browser-based interactive visualization, which feature high-level declarative specifications. In response, we present DIEL, a declarative framework that supports asynchronous events over distributed data. As in many declarative languages, DIEL developers specify only what data they want, rather than procedural steps for how to assemble it. Uniquely, DIEL models asynchronous events (e.g., user interactions, server responses) as streams of data that are captured in event logs. To specify the state of a visualization at any time, developers write declarative queries over the data and event logs; DIEL compiles and optimizes a corresponding dataflow graph, and automatically generates necessary low-level distributed systems details. We demonstrate DIEL&#39;S performance and expressivity through example interactive visualizations that make diverse use of remote data and asynchronous events. We further evaluate DIEL&#39;S usability using the Cognitive Dimensions of Notations framework, revealing wins such as ease of change, and compromises such as premature commitments.},
  archive      = {J_TVCG},
  author       = {Yifan Wu and Remco Chang and Joseph M. Hellerstein and Arvind Satyanarayan and Eugene Wu},
  doi          = {10.1109/TVCG.2021.3114796},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {737-746},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DIEL: Interactive visualization beyond the here and now},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rotate or wrap? Interactive visualisations of cyclical data
on cylindrical or toroidal topologies. <em>TVCG</em>, <em>28</em>(1),
727–736. (<a href="https://doi.org/10.1109/TVCG.2021.3114693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we report on a study of visual representations for cyclical data and the effect of interactively wrapping a bar chart ‘around its boundaries’. Compared to linear bar chart, polar (or radial) visualisations have the advantage that cyclical data can be presented continuously without mentally bridging the visual ‘cut’ across the left-and-right boundaries. To investigate this hypothesis and to assess the effect the cut has on analysis performance, this paper presents results from a crowdsourced, controlled experiment with 72 participants comparing new continuous panning technique to linear bar charts ( interactive wrapping ). Our results show that bar charts with interactive wrapping lead to less errors compared to standard bar charts or polar charts. Inspired by these results, we generalise the concept of interactive wrapping to other visualisations for cyclical or relational data. We describe a design space based on the concept of one-dimensional wrapping and two-dimensional wrapping, linked to two common 3D topologies; cylinder and torus that can be used to metaphorically explain one- and two-dimensional wrapping. This design space suggests that interactive wrapping is widely applicable to many different data types.},
  archive      = {J_TVCG},
  author       = {Kun-Ting Chen and Tim Dwyer and Benjamin Bach and Kim Marriott},
  doi          = {10.1109/TVCG.2021.3114693},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {727-736},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Rotate or wrap? interactive visualisations of cyclical data on cylindrical or toroidal topologies},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling just noticeable differences in charts.
<em>TVCG</em>, <em>28</em>(1), 718–726. (<a
href="https://doi.org/10.1109/TVCG.2021.3114874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the fundamental tasks in visualization is to compare two or more visual elements. However, it is often difficult to visually differentiate graphical elements encoding a small difference in value, such as the heights of similar bars in bar chart or angles of similar sections in pie chart. Perceptual laws can be used in order to model when and how we perceive this difference. In this work, we model the perception of Just Noticeable Differences (JNDs), the minimum difference in visual attributes that allow faithfully comparing similar elements, in charts. Specifically, we explore the relation between JNDs and two major visual variables: the intensity of visual elements and the distance between them, and study it in three charts: bar chart, pie chart and bubble chart. Through an empirical study, we identify main effects on JND for distance in bar charts, intensity in pie charts, and both distance and intensity in bubble charts. By fitting a linear mixed effects model, we model JND and find that JND grows as the exponential function of variables. We highlight several usage scenarios that make use of the JND modeling in which elements below the fitted JND are detected and enhanced with secondary visual cues for better discrimination.},
  archive      = {J_TVCG},
  author       = {Min Lu and Joel Lanir and Chufeng Wang and Yucong Yao and Wen Zhang and Oliver Deussen and Hui Huang},
  doi          = {10.1109/TVCG.2021.3114874},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {718-726},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Modeling just noticeable differences in charts},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rethinking the ranks of visual channels. <em>TVCG</em>,
<em>28</em>(1), 707–717. (<a
href="https://doi.org/10.1109/TVCG.2021.3114684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data can be visually represented using visual channels like position, length or luminance. An existing ranking of these visual channels is based on how accurately participants could report the ratio between two depicted values. There is an assumption that this ranking should hold for different tasks and for different numbers of marks. However, there is surprisingly little existing work that tests this assumption, especially given that visually computing ratios is relatively unimportant in real-world visualizations, compared to seeing, remembering, and comparing trends and motifs, across displays that almost universally depict more than two values. To simulate the information extracted from a glance at a visualization, we instead asked participants to immediately reproduce a set of values from memory after they were shown the visualization. These values could be shown in a bar graph (position (bar)), line graph (position (line)), heat map (luminance), bubble chart (area), misaligned bar graph (length), or ‘wind map’ (angle). With a Bayesian multilevel modeling approach, we show how the rank positions of visual channels shift across different numbers of marks (2, 4 or 8) and for bias, precision, and error measures. The ranking did not hold, even for reproductions of only 2 marks, and the new probabilistic ranking was highly inconsistent for reproductions of different numbers of marks. Other factors besides channel choice had an order of magnitude more influence on performance, such as the number of values in the series (e.g., more marks led to larger errors), or the value of each mark (e.g., small values were systematically overestimated). Every visual channel was worse for displays with 8 marks than 4, consistent with established limits on visual memory. These results point to the need for a body of empirical studies that move beyond two-value ratio judgments as a baseline for reliably ranking the quality of a visual channel, including testing new tasks (detection of trends or motifs), timescales (immediate computation, or later comparison), and the number of values (from a handful, to thousands).},
  archive      = {J_TVCG},
  author       = {Caitlyn M. McColeman and Fumeng Yang and Timothy F. Brady and Steven Franconeri},
  doi          = {10.1109/TVCG.2021.3114684},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {707-717},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Rethinking the ranks of visual channels},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context matters: A theory of semantic discriminability for
perceptual encoding systems. <em>TVCG</em>, <em>28</em>(1), 697–706. (<a
href="https://doi.org/10.1109/TVCG.2021.3114780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People&#39;s associations between colors and concepts influence their ability to interpret the meanings of colors in information visualizations. Previous work has suggested such effects are limited to concepts that have strong, specific associations with colors. However, although a concept may not be strongly associated with any colors, its mapping can be disambiguated in the context of other concepts in an encoding system. We articulate this view in semantic discriminability theory, a general framework for understanding conditions determining when people can infer meaning from perceptual features. Semantic discriminability is the degree to which observers can infer a unique mapping between visual features and concepts. Semantic discriminability theory posits that the capacity for semantic discriminability for a set of concepts is constrained by the difference between the feature-concept association distributions across the concepts in the set. We define formal properties of this theory and test its implications in two experiments. The results show that the capacity to produce semantically discriminable colors for sets of concepts was indeed constrained by the statistical distance between color-concept association distributions (Experiment 1). Moreover, people could interpret meanings of colors in bar graphs insofar as the colors were semantically discriminable, even for concepts previously considered “non-colorable” (Experiment 2). The results suggest that colors are more robust for visual communication than previously thought.},
  archive      = {J_TVCG},
  author       = {Kushin Mukherjee and Brian Yin and Brianne E. Sherman and Laurent Lessard and Karen B. Schloss},
  doi          = {10.1109/TVCG.2021.3114780},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {697-706},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Context matters: A theory of semantic discriminability for perceptual encoding systems},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Untidy data: The unreasonable effectiveness of tables.
<em>TVCG</em>, <em>28</em>(1), 686–696. (<a
href="https://doi.org/10.1109/TVCG.2021.3114830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Working with data in table form is usually considered a preparatory and tedious step in the sensemaking pipeline; a way of getting the data ready for more sophisticated visualization and analytical tools. But for many people, spreadsheets — the quintessential table tool — remain a critical part of their information ecosystem, allowing them to interact with their data in ways that are hidden or abstracted in more complex tools. This is particularly true for data workers [61] , people who work with data as part of their job but do not identify as professional analysts or data scientists. We report on a qualitative study of how these workers interact with and reason about their data. Our findings show that data tables serve a broader purpose beyond data cleanup at the initial stage of a linear analytic flow: users want to see and “get their hands on” the underlying data throughout the analytics process, reshaping and augmenting it to support sensemaking. They reorganize, mark up, layer on levels of detail, and spawn alternatives within the context of the base data. These direct interactions and human-readable table representations form a rich and cognitively important part of building understanding of what the data mean and what they can do with it. We argue that interactive tables are an important visualization idiom in their own right; that the direct data interaction they afford offers a fertile design space for visual analytics; and that sense making can be enriched by more flexible human-data interaction than is currently supported in visual analytics tools.},
  archive      = {J_TVCG},
  author       = {Lyn Bartram and Michael Correll and Melanie Tory},
  doi          = {10.1109/TVCG.2021.3114830},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {686-696},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Untidy data: The unreasonable effectiveness of tables},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning objectives, insights, and assessments: How
specification formats impact design. <em>TVCG</em>, <em>28</em>(1),
676–685. (<a href="https://doi.org/10.1109/TVCG.2021.3114811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the ubiquity of communicative visualizations, specifying communicative intent during design is ad hoc. Whether we are selecting from a set of visualizations, commissioning someone to produce them, or creating them ourselves, an effective way of specifying intent can help guide this process. Ideally, we would have a concise and shared specification language. In previous work, we have argued that communicative intents can be viewed as a learning/assessment problem (i.e., what should the reader learn and what test should they do well on). Learning-based specification formats are linked (e.g., assessments are derived from objectives) but some may more effectively specify communicative intent. Through a large-scale experiment, we studied three specification types: learning objectives, insights, and assessments. Participants, guided by one of these specifications, rated their preferences for a set of visualization designs. Then, we evaluated the set of visualization designs to assess which specification led participants to prefer the most effective visualizations. We find that while all specification types have benefits over no-specification, each format has its own advantages. Our results show that learning objective-based specifications helped participants the most in visualization selection. We also identify situations in which specifications may be insufficient and assessments are vital.},
  archive      = {J_TVCG},
  author       = {Elsie Lee-Robbins and Shiqing He and Eytan Adar},
  doi          = {10.1109/TVCG.2021.3114811},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {676-685},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Learning objectives, insights, and assessments: How specification formats impact design},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding data visualization design practice.
<em>TVCG</em>, <em>28</em>(1), 665–675. (<a
href="https://doi.org/10.1109/TVCG.2021.3114959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Professional roles for data visualization designers are growing in popularity, and interest in relationships between the academic research and professional practice communities is gaining traction. However, despite the potential for knowledge sharing between these communities, we have little understanding of the ways in which practitioners design in real-world, professional settings. Inquiry in numerous design disciplines indicates that practitioners approach complex situations in ways that are fundamentally different from those of researchers. In this work, I take a practice-led approach to understanding visualization design practice on its own terms. Twenty data visualization practitioners were interviewed and asked about their design process, including the steps they take, how they make decisions, and the methods they use. Findings suggest that practitioners do not follow highly systematic processes, but instead rely on situated forms of knowing and acting in which they draw from precedent and use methods and principles that are determined appropriate in the moment. These findings have implications for how visualization researchers understand and engage with practitioners, and how educators approach the training of future data visualization designers.},
  archive      = {J_TVCG},
  author       = {Paul Parsons},
  doi          = {10.1109/TVCG.2021.3114959},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {665-675},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Understanding data visualization design practice},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Professional differences: A comparative study of
visualization task performance and spatial ability across disciplines.
<em>TVCG</em>, <em>28</em>(1), 654–664. (<a
href="https://doi.org/10.1109/TVCG.2021.3114805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem-driven visualization work is rooted in deeply understanding the data, actors, processes, and workflows of a target domain. However, an individual&#39;s personality traits and cognitive abilities may also influence visualization use. Diverse user needs and abilities raise natural questions for specificity in visualization design: Could individuals from different domains exhibit performance differences when using visualizations? Are any systematic variations related to their cognitive abilities? This study bridges domain-specific perspectives on visualization design with those provided by cognition and perception. We measure variations in visualization task performance across chemistry, computer science, and education, and relate these differences to variations in spatial ability. We conducted an online study with over 60 domain experts consisting of tasks related to pie charts, isocontour plots, and 3D scatterplots, and grounded by a well-documented spatial ability test. Task performance (correctness) varied with profession across more complex visualizations (isocontour plots and scatterplots), but not pie charts, a comparatively common visualization. We found that correctness correlates with spatial ability, and the professions differ in terms of spatial ability. These results indicate that domains differ not only in the specifics of their data and tasks, but also in terms of how effectively their constituent members engage with visualizations and their cognitive traits. Analyzing participants&#39; confidence and strategy comments suggests that focusing on performance neglects important nuances, such as differing approaches to engage with even common visualizations and potential skill transference. Our findings offer a fresh perspective on discipline-specific visualization with specific recommendations to help guide visualization design that celebrates the uniqueness of the disciplines and individuals we seek to serve.},
  archive      = {J_TVCG},
  author       = {Kyle Wm. Hall and Anthony Kouroupis and Anastasia Bezerianos and Danielle Albers Szafir and Christopher Collins},
  doi          = {10.1109/TVCG.2021.3114805},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {654-664},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Professional differences: A comparative study of visualization task performance and spatial ability across disciplines},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CoUX: Collaborative visual analysis of think-aloud usability
test videos for digital interfaces. <em>TVCG</em>, <em>28</em>(1),
643–653. (<a href="https://doi.org/10.1109/TVCG.2021.3114822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reviewing a think-aloud video is both time-consuming and demanding as it requires UX (user experience) professionals to attend to many behavioral signals of the user in the video. Moreover, challenges arise when multiple UX professionals need to collaborate to reduce bias and errors. We propose a collaborative visual analytics tool, CoUX, to facilitate UX evaluators collectively reviewing think-aloud usability test videos of digital interfaces. CoUX seamlessly supports usability problem identification, annotation, and discussion in an integrated environment. To ease the discovery of usability problems, CoUX visualizes a set of problem-indicators based on acoustic, textual, and visual features extracted from the video and audio of a think-aloud session with machine learning. CoUX further enables collaboration amongst UX evaluators for logging, commenting, and consolidating the discovered problems with a chatbox-like user interface. We designed CoUX based on a formative study with two UX experts and insights derived from the literature. We conducted a user study with six pairs of UX practitioners on collaborative think-aloud video analysis tasks. The results indicate that CoUX is useful and effective in facilitating both problem identification and collaborative teamwork. We provide insights into how different features of CoUX were used to support both independent analysis and collaboration. Furthermore, our work highlights opportunities to improve collaborative usability test video analysis.},
  archive      = {J_TVCG},
  author       = {Ehsan Jahangirzadeh Soure and Emily Kuang and Mingming Fan and Jian Zhao},
  doi          = {10.1109/TVCG.2021.3114822},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {643-653},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CoUX: Collaborative visual analysis of think-aloud usability test videos for digital interfaces},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic polygon layout for primal-dual visualization of
hypergraphs. <em>TVCG</em>, <em>28</em>(1), 633–642. (<a
href="https://doi.org/10.1109/TVCG.2021.3114759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {N-ary relationships, which relate $N$ entities where $N$ is not necessarily two, can be visually represented as polygons whose vertices are the entities of the relationships. Manually generating a high-quality layout using this representation is labor-intensive. In this paper, we provide an automatic polygon layout generation algorithm for the visualization of N-ary relationships. At the core of our algorithm is a set of objective functions motivated by a number of design principles that we have identified. These objective functions are then used in an optimization framework that we develop to achieve high-quality layouts. Recognizing the duality between entities and relationships in the data, we provide a second visualization in which the roles of entities and relationships in the original data are reversed. This can lead to additional insight about the data. Furthermore, we enhance our framework for a joint optimization on the primal layout (original data) and the dual layout (where the roles of entities and relationships are reversed). This allows users to inspect their data using two complementary views. We apply our visualization approach to a number of datasets that include co-authorship data and social contact pattern data.},
  archive      = {J_TVCG},
  author       = {Botong Qu and Eugene Zhang and Yue Zhang},
  doi          = {10.1109/TVCG.2021.3114759},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {633-642},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Automatic polygon layout for primal-dual visualization of hypergraphs},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint t-SNE for comparable projections of multiple
high-dimensional datasets. <em>TVCG</em>, <em>28</em>(1), 623–632. (<a
href="https://doi.org/10.1109/TVCG.2021.3114765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Joint t-Stochastic Neighbor Embedding (Joint t-SNE), a technique to generate comparable projections of multiple high-dimensional datasets. Although t-SNE has been widely employed to visualize high-dimensional datasets from various domains, it is limited to projecting a single dataset. When a series of high-dimensional datasets, such as datasets changing over time, is projected independently using t-SNE, misaligned layouts are obtained. Even items with identical features across datasets are projected to different locations, making the technique unsuitable for comparison tasks. To tackle this problem, we introduce edge similarity, which captures the similarities between two adjacent time frames based on the Graphlet Frequency Distribution (GFD). We then integrate a novel loss term into the t-SNE loss function, which we call vector constraints, to preserve the vectors between projected points across the projections, allowing these points to serve as visual landmarks for direct comparisons between projections. Using synthetic datasets whose ground-truth structures are known, we show that Joint t-SNE outperforms existing techniques, including Dynamic t-SNE, in terms of local coherence error, Kullback-Leibler divergence, and neighborhood preservation. We also showcase a real-world use case to visualize and compare the activation of different layers of a neural network.},
  archive      = {J_TVCG},
  author       = {Yinqiao Wang and Lu Chen and Jaemin Jo and Yunhai Wang},
  doi          = {10.1109/TVCG.2021.3114765},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {623-632},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Joint t-SNE for comparable projections of multiple high-dimensional datasets},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient dual-hierarchy t-SNE minimization.
<em>TVCG</em>, <em>28</em>(1), 614–622. (<a
href="https://doi.org/10.1109/TVCG.2021.3114817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {t-distributed Stochastic Neighbour Embedding (t-SNE) has become a standard for exploratory data analysis, as it is capable of revealing clusters even in complex data while requiring minimal user input. While its run-time complexity limited it to small datasets in the past, recent efforts improved upon the expensive similarity computations and the previously quadratic minimization. Nevertheless, t-SNE still has high runtime and memory costs when operating on millions of points. We present a novel method for executing the t-SNE minimization. While our method overall retains a linear runtime complexity, we obtain a significant performance increase in the most expensive part of the minimization. We achieve a significant improvement without a noticeable decrease in accuracy even when targeting a 3D embedding. Our method constructs a pair of spatial hierarchies over the embedding, which are simultaneously traversed to approximate many N-body interactions at once. We demonstrate an efficient GPGPU implementation and evaluate its performance against state-of-the-art methods on a variety of datasets.},
  archive      = {J_TVCG},
  author       = {Mark van de Ruit and Markus Billeter and Elmar Eisemann},
  doi          = {10.1109/TVCG.2021.3114817},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {614-622},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {An efficient dual-hierarchy t-SNE minimization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rapid labels: Point-feature labeling on GPU. <em>TVCG</em>,
<em>28</em>(1), 604–613. (<a
href="https://doi.org/10.1109/TVCG.2021.3114854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Labels, short textual annotations are an important component of data visualizations, illustrations, infographics, and geographical maps. In interactive applications, the labeling method responsible for positioning the labels should not take the resources from the application itself. In other words, the labeling method should provide the result as fast as possible. In this work, we propose a greedy point-feature labeling method running on GPU. In contrast to existing methods that position the labels sequentially, the proposed method positions several labels in parallel. Yet, we guarantee that the positioned labels will not overlap, nor will they overlap important visual features. When the proposed method is searching for the label position of a point-feature, the available label candidates are evaluated with respect to overlaps with important visual features, conflicts with label candidates of other point-features, and their ambiguity. The evaluation of each label candidate is done in constant time independently from the number of point-features, the number of important visual features, and the resolution of the created image. Our measurements indicate that the proposed method is able to position more labels than existing greedy methods that do not evaluate conflicts between the label candidates. At the same time, the proposed method achieves a significant increase in performance. The increase in performance is mainly due to the parallelization and the efficient evaluation of label candidates.},
  archive      = {J_TVCG},
  author       = {Vaclav Pavlovec and Ladislav Cmolik},
  doi          = {10.1109/TVCG.2021.3114854},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {604-613},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Rapid labels: Point-feature labeling on GPU},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pyramid-based scatterplots sampling for progressive and
streaming data visualization. <em>TVCG</em>, <em>28</em>(1), 593–603.
(<a href="https://doi.org/10.1109/TVCG.2021.3114880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a pyramid-based scatterplot sampling technique to avoid overplotting and enable progressive and streaming visualization of large data. Our technique is based on a multiresolution pyramid-based decomposition of the underlying density map and makes use of the density values in the pyramid to guide the sampling at each scale for preserving the relative data densities and outliers. We show that our technique is competitive in quality with state-of-the-art methods and runs faster by about an order of magnitude. Also, we have adapted it to deliver progressive and streaming data visualization by processing the data in chunks and updating the scatterplot areas with visible changes in the density map. A quantitative evaluation shows that our approach generates stable and faithful progressive samples that are comparable to the state-of-the-art method in preserving relative densities and superior to it in keeping outliers and stability when switching frames. We present two case studies that demonstrate the effectiveness of our approach for exploring large data.},
  archive      = {J_TVCG},
  author       = {Xin Chen and Jian Zhang and Chi-Wing Fu and Jean-Daniel Fekete and Yunhai Wang},
  doi          = {10.1109/TVCG.2021.3114880},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {593-603},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Pyramid-based scatterplots sampling for progressive and streaming data visualization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A memory efficient encoding for ray tracing large
unstructured data. <em>TVCG</em>, <em>28</em>(1), 583–592. (<a
href="https://doi.org/10.1109/TVCG.2021.3114869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In theory, efficient and high-quality rendering of unstructured data should greatly benefit from modern GPUs, but in practice, GPUs are often limited by the large amount of memory that large meshes require for element representation and for sample reconstruction acceleration structures. We describe a memory-optimized encoding for large unstructured meshes that efficiently encodes both the unstructured mesh and corresponding sample reconstruction acceleration structure, while still allowing for fast random-access sampling as required for rendering. We demonstrate that for large data our encoding allows for rendering even the 2.9 billion element Mars Lander on a single off-the-shelf GPU-and the largest 6.3 billion version on a pair of such GPUs.},
  archive      = {J_TVCG},
  author       = {Ingo Wald and Nate Morrical and Stefan Zellmann},
  doi          = {10.1109/TVCG.2021.3114869},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {583-592},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A memory efficient encoding for ray tracing large unstructured data},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Probabilistic occlusion culling using confidence maps for
high-quality rendering of large particle data. <em>TVCG</em>,
<em>28</em>(1), 573–582. (<a
href="https://doi.org/10.1109/TVCG.2021.3114788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving high rendering quality in the visualization of large particle data, for example from large-scale molecular dynamics simulations, requires a significant amount of sub-pixel super-sampling, due to very high numbers of particles per pixel. Although it is impossible to super-sample all particles of large-scale data at interactive rates, efficient occlusion culling can decouple the overall data size from a high effective sampling rate of visible particles. However, while the latter is essential for domain scientists to be able to see important data features, performing occlusion culling by sampling or sorting the data is usually slow or error-prone due to visibility estimates of insufficient quality. We present a novel probabilistic culling architecture for super-sampled high-quality rendering of large particle data. Occlusion is dynamically determined at the sub-pixel level, without explicit visibility sorting or data simplification. We introduce confidence maps to probabilistically estimate confidence in the visibility data gathered so far. This enables progressive, confidence-based culling, helping to avoid wrong visibility decisions. In this way, we determine particle visibility with high accuracy, although only a small part of the data set is sampled. This enables extensive super-sampling of (partially) visible particles for high rendering quality, at a fraction of the cost of sampling all particles. For real-time performance with millions of particles, we exploit novel features of recent GPU architectures to group particles into two hierarchy levels, combining fine-grained culling with high frame rates.},
  archive      = {J_TVCG},
  author       = {Mohamed Ibrahim and Peter Rautek and Guido Reina and Marco Agus and Markus Hadwiger},
  doi          = {10.1109/TVCG.2021.3114788},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {573-582},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Probabilistic occlusion culling using confidence maps for high-quality rendering of large particle data},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Differentiable direct volume rendering. <em>TVCG</em>,
<em>28</em>(1), 562–572. (<a
href="https://doi.org/10.1109/TVCG.2021.3114769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a differentiable volume rendering solution that provides differentiability of all continuous parameters of the volume rendering process. This differentiable renderer is used to steer the parameters towards a setting with an optimal solution of a problem-specific objective function. We have tailored the approach to volume rendering by enforcing a constant memory footprint via analytic inversion of the blending functions. This makes it independent of the number of sampling steps through the volume and facilitates the consideration of small-scale changes. The approach forms the basis for automatic optimizations regarding external parameters of the rendering process and the volumetric density field itself. We demonstrate its use for automatic viewpoint selection using differentiable entropy as objective, and for optimizing a transfer function from rendered images of a given volume. Optimization of per-voxel densities is addressed in two different ways: First, we mimic inverse tomography and optimize a 3D density field from images using an absorption model. This simplification enables comparisons with algebraic reconstruction techniques and state-of-the-art differentiable path tracers. Second, we introduce a novel approach for tomographic reconstruction from images using an emission-absorption model with post-shading via an arbitrary transfer function.},
  archive      = {J_TVCG},
  author       = {Sebastian Weiss and Rüdiger Westermann},
  doi          = {10.1109/TVCG.2021.3114769},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {562-572},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Differentiable direct volume rendering},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Measuring and explaining the inter-cluster reliability of
multidimensional projections. <em>TVCG</em>, <em>28</em>(1), 551–561.
(<a href="https://doi.org/10.1109/TVCG.2021.3114833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Steadiness and Cohesiveness, two novel metrics to measure the inter-cluster reliability of multidimensional projection (MDP), specifically how well the inter-cluster structures are preserved between the original high-dimensional space and the low-dimensional projection space. Measuring inter-cluster reliability is crucial as it directly affects how well inter-cluster tasks (e.g., identifying cluster relationships in the original space from a projected view) can be conducted; however, despite the importance of inter-cluster tasks, we found that previous metrics, such as Trustworthiness and Continuity, fail to measure inter-cluster reliability. Our metrics consider two aspects of the inter-cluster reliability: Steadiness measures the extent to which clusters in the projected space form clusters in the original space, and Cohesiveness measures the opposite. They extract random clusters with arbitrary shapes and positions in one space and evaluate how much the clusters are stretched or dispersed in the other space. Furthermore, our metrics can quantify pointwise distortions, allowing for the visualization of inter-cluster reliability in a projection, which we call a reliability map. Through quantitative experiments, we verify that our metrics precisely capture the distortions that harm inter-cluster reliability while previous metrics have difficulty capturing the distortions. A case study also demonstrates that our metrics and the reliability map 1) support users in selecting the proper projection techniques or hyperparameters and 2) prevent misinterpretation while performing inter-cluster tasks, thus allow an adequate identification of inter-cluster structure.},
  archive      = {J_TVCG},
  author       = {Hyeon Jeon and Hyung-Kwon Ko and Jaemin Jo and Youngtaek Kim and Jinwook Seo},
  doi          = {10.1109/TVCG.2021.3114833},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {551-561},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Measuring and explaining the inter-cluster reliability of multidimensional projections},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attribute-based explanation of non-linear embeddings of
high-dimensional data. <em>TVCG</em>, <em>28</em>(1), 540–550. (<a
href="https://doi.org/10.1109/TVCG.2021.3114870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embeddings of high-dimensional data are widely used to explore data, to verify analysis results, and to communicate information. Their explanation, in particular with respect to the input attributes, is often difficult. With linear projects like PCA the axes can still be annotated meaningfully. With non-linear projections this is no longer possible and alternative strategies such as attribute-based color coding are required. In this paper, we review existing augmentation techniques and discuss their limitations. We present the Non-Linear Embeddings Surveyor (NoLiES) that combines a novel augmentation strategy for projected data (rangesets) with interactive analysis in a small multiples setting. Rangesets use a set-based visualization approach for binned attribute values that enable the user to quickly observe structure and detect outliers. We detail the link between algebraic topology and rangesets and demonstrate the utility of NoLiES in case studies with various challenges (complex attribute value distribution, many attributes, many data points) and a real-world application to understand latent features of matrix completion in thermodynamics.},
  archive      = {J_TVCG},
  author       = {Jan-Tobias Sohns and Michaela Schmitt and Fabian Jirasek and Hans Hasse and Heike Leitte},
  doi          = {10.1109/TVCG.2021.3114870},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {540-550},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Attribute-based explanation of non-linear embeddings of high-dimensional data},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Revisiting dimensionality reduction techniques for visual
cluster analysis: An empirical study. <em>TVCG</em>, <em>28</em>(1),
529–539. (<a href="https://doi.org/10.1109/TVCG.2021.3114694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality Reduction (DR) techniques can generate 2D projections and enable visual exploration of cluster structures of high-dimensional datasets. However, different DR techniques would yield various patterns, which significantly affect the performance of visual cluster analysis tasks. We present the results of a user study that investigates the influence of different DR techniques on visual cluster analysis. Our study focuses on the most concerned property types, namely the linearity and locality, and evaluates twelve representative DR techniques that cover the concerned properties. Four controlled experiments were conducted to evaluate how the DR techniques facilitate the tasks of 1) cluster identification, 2) membership identification, 3) distance comparison, and 4) density comparison, respectively. We also evaluated users&#39; subjective preference of the DR techniques regarding the quality of projected clusters. The results show that: 1) Non-linear and Local techniques are preferred in cluster identification and membership identification; 2) Linear techniques perform better than non-linear techniques in density comparison; 3) UMAP (Uniform Manifold Approximation and Projection) and t-SNE (t-Distributed Stochastic Neighbor Embedding) perform the best in cluster identification and membership identification; 4) NMF (Nonnegative Matrix Factorization) has competitive performance in distance comparison; 5) t-SNLE (t-Distributed Stochastic Neighbor Linear Embedding) has competitive performance in density comparison.},
  archive      = {J_TVCG},
  author       = {Jiazhi Xia and Yuchen Zhang and Jie Song and Yang Chen and Yunhai Wang and Shixia Liu},
  doi          = {10.1109/TVCG.2021.3114694},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {529-539},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Revisiting dimensionality reduction techniques for visual cluster analysis: An empirical study},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Explanatory journeys: Visualising to understand and explain
administrative justice paths of redress. <em>TVCG</em>, <em>28</em>(1),
518–528. (<a href="https://doi.org/10.1109/TVCG.2021.3114818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Administrative justice concerns the relationships between individuals and the state. It includes redress and complaints on decisions of a child&#39;s education, social care, licensing, planning, environment, housing and homelessness. However, if someone has a complaint or an issue, it is challenging for people to understand different possible redress paths and explore what path is suitable for their situation. Explanatory visualisation has the potential to display these paths of redress in a clear way, such that people can see, understand and explore their options. The visualisation challenge is further complicated because information is spread across many documents, laws, guidance and policies and requires judicial interpretation. Consequently, there is not a single database of paths of redress. In this work we present how we have co-designed a system to visualise administrative justice paths of redress. Simultaneously, we classify, collate and organise the underpinning data, from expert workshops, heuristic evaluation and expert critical reflection. We make four contributions: (i) an application design study of the explanatory visualisation tool (Artemus), (ii) coordinated and co-design approach to aggregating the data, (iii) two in-depth case studies in housing and education demonstrating explanatory paths of redress in administrative law, and (iv) reflections on the expert co-design process and expert data gathering and explanatory visualisation for administrative justice and law.},
  archive      = {J_TVCG},
  author       = {Jonathan C. Roberts and Peter Butcher and Ann Sherlock and Sarah Nason},
  doi          = {10.1109/TVCG.2021.3114818},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {518-528},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Explanatory journeys: Visualising to understand and explain administrative justice paths of redress},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). E-ffective: A visual analytic system for exploring the
emotion and effectiveness of inspirational speeches. <em>TVCG</em>,
<em>28</em>(1), 508–517. (<a
href="https://doi.org/10.1109/TVCG.2021.3114789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What makes speeches effective has long been a subject for debate, and until today there is broad controversy among public speaking experts about what factors make a speech effective as well as the roles of these factors in speeches. Moreover, there is a lack of quantitative analysis methods to help understand effective speaking strategies. In this paper, we propose E-ffective, a visual analytic system allowing speaking experts and novices to analyze both the role of speech factors and their contribution in effective speeches. From interviews with domain experts and investigating existing literature, we identified important factors to consider in inspirational speeches. We obtained the generated factors from multi-modal data that were then related to effectiveness data. Our system supports rapid understanding of critical factors in inspirational speeches, including the influence of emotions by means of novel visualization methods and interaction. Two novel visualizations include E-spiral (that shows the emotional shifts in speeches in a visually compact way) and E-script (that connects speech content with key speech delivery information). In our evaluation we studied the influence of our system on experts&#39; domain knowledge about speech factors. We further studied the usability of the system by speaking novices and experts on assisting analysis of inspirational speech effectiveness.},
  archive      = {J_TVCG},
  author       = {Kevin Maher and Zeyuan Huang and Jiancheng Song and Xiaoming Deng and Yu-Kun Lai and Cuixia Ma and Hao Wang and Yong-Jin Liu and Hongan Wang},
  doi          = {10.1109/TVCG.2021.3114789},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {508-517},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {E-ffective: A visual analytic system for exploring the emotion and effectiveness of inspirational speeches},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gender in 30 years of IEEE visualization. <em>TVCG</em>,
<em>28</em>(1), 497–507. (<a
href="https://doi.org/10.1109/TVCG.2021.3114787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an exploratory analysis of gender representation among the authors, committee members, and award winners at the IEEE Visualization (VIS) conference over the last 30 years. Our goal is to provide descriptive data on which diversity discussions and efforts in the community can build. We look in particular at the gender of VIS authors as a proxy for the community at large. We consider measures of overall gender representation among authors, differences in careers, positions in author lists, and collaborations. We found that the proportion of female authors has increased from 9\% in the first five years to 22\% in the last five years of the conference. Over the years, we found the same representation of women in program committees and slightly more women in organizing committees. Women are less likely to appear in the last author position, but more in the middle positions. In terms of collaboration patterns, female authors tend to collaborate more than expected with other women in the community. All non-gender related data is available on https://osf.io/ydfj4/ and the gender-author matching can be accessed through https://nyu.databrary.org/volume/1301.},
  archive      = {J_TVCG},
  author       = {Natkamon Tovanich and Pierre Dragicevic and Petra Isenberg},
  doi          = {10.1109/TVCG.2021.3114787},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {497-507},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Gender in 30 years of IEEE visualization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VITALITY: Promoting serendipitous discovery of academic
literature with transformers &amp; visual analytics. <em>TVCG</em>,
<em>28</em>(1), 486–496. (<a
href="https://doi.org/10.1109/TVCG.2021.3114820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are a few prominent practices for conducting reviews of academic literature, including searching for specific keywords on Google Scholar or checking citations from some initial seed paper(s). These approaches serve a critical purpose for academic literature reviews, yet there remain challenges in identifying relevant literature when similar work may utilize different terminology (e.g., mixed-initiative visual analytics papers may not use the same terminology as papers on model-steering, yet the two topics are relevant to one another). In this paper, we introduce a system, VITALITY, intended to complement existing practices. In particular, VITALITY promotes serendipitous discovery of relevant literature using transformer language models, allowing users to find semantically similar papers in a word embedding space given (1) a list of input paper(s) or (2) a working abstract. VITALITY visualizes this document-level embedding space in an interactive 2-D scatterplot using dimension reduction. VITALITY also summarizes meta information about the document corpus or search query, including keywords and co-authors, and allows users to save and export papers for use in a literature review. We present qualitative findings from an evaluation of VITALITY, suggesting it can be a promising complementary technique for conducting academic literature reviews. Furthermore, we contribute data from 38 popular data visualization publication venues in VITALITY, and we provide scrapers for the open-source community to continue to grow the list of supported venues.},
  archive      = {J_TVCG},
  author       = {Arpit Narechania and Alireza Karduni and Ryan Wesslen and Emily Wall},
  doi          = {10.1109/TVCG.2021.3114820},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {486-496},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VITALITY: Promoting serendipitous discovery of academic literature with transformers &amp; visual analytics},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Seek for success: A visualization approach for understanding
the dynamics of academic careers. <em>TVCG</em>, <em>28</em>(1),
475–485. (<a href="https://doi.org/10.1109/TVCG.2021.3114790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to achieve academic career success has been a long-standing research question in social science research. With the growing availability of large-scale well-documented academic profiles and career trajectories, scholarly interest in career success has been reinvigorated, which has emerged to be an active research domain called the Science of Science (i.e., SciSci). In this study, we adopt an innovative dynamic perspective to examine how individual and social factors will influence career success over time. We propose ACSeeker , an interactive visual analytics approach to explore the potential factors of success and how the influence of multiple factors changes at different stages of academic careers. We first applied a Multi-factor Impact Analysis framework to estimate the effect of different factors on academic career success over time. We then developed a visual analytics system to understand the dynamic effects interactively. A novel timeline is designed to reveal and compare the factor impacts based on the whole population. A customized career line showing the individual career development is provided to allow a detailed inspection. To validate the effectiveness and usability of ACSeeker , we report two case studies and interviews with a social scientist and general researchers.},
  archive      = {J_TVCG},
  author       = {Yifang Wang and Tai-Quan Peng and Huihua Lu and Haoren Wang and Xiao Xie and Huamin Qu and Yingcai Wu},
  doi          = {10.1109/TVCG.2021.3114790},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {475-485},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Seek for success: A visualization approach for understanding the dynamics of academic careers},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visualization equilibrium. <em>TVCG</em>, <em>28</em>(1),
465–474. (<a href="https://doi.org/10.1109/TVCG.2021.3114842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world strategic settings, people use information displays to make decisions. In these settings, an information provider chooses which information to provide to strategic agents and how to present it, and agents formulate a best response based on the information and their anticipation of how others will behave. We contribute the results of a controlled online experiment to examine how the provision and presentation of information impacts people&#39;s decisions in a congestion game. Our experiment compares how different visualization approaches for displaying this information, including bar charts and hypothetical outcome plots, and different information conditions, including where the visualized information is private versus public (i.e., available to all agents), affect decision making and welfare. We characterize the effects of visualization anticipation, referring to changes to behavior when an agent goes from alone having access to a visualization to knowing that others also have access to the visualization to guide their decisions. We also empirically identify the visualization equilibrium, i.e., the visualization for which the visualized outcome of agents&#39; decisions matches the realized decisions of the agents who view it. We reflect on the implications of visualization equilibria and visualization anticipation for designing information displays for real-world strategic settings.},
  archive      = {J_TVCG},
  author       = {Paula Kayongo and Glenn Sun and Jason Hartline and Jessica Hullman},
  doi          = {10.1109/TVCG.2021.3114842},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {465-474},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualization equilibrium},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effect of uncertainty visualizations on myopic loss aversion
and the equity premium puzzle in retirement investment decisions.
<em>TVCG</em>, <em>28</em>(1), 454–464. (<a
href="https://doi.org/10.1109/TVCG.2021.3114692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many households, investing for retirement is one of the most significant decisions and is fraught with uncertainty. In a classic study in behavioral economics, Benartzi and Thaler (1999) found evidence using bar charts that investors exhibit myopic loss aversion in retirement decisions: Investors overly focus on the potential for short-term losses, leading them to invest less in riskier assets and miss out on higher long-term returns. Recently, advances in uncertainty visualizations have shown improvements in decision-making under uncertainty in a variety of tasks. In this paper, we conduct a controlled and incentivized crowdsourced experiment replicating Benartzi and Thaler (1999) and extending it to measure the effect of different uncertainty representations on myopic loss aversion. Consistent with the original study, we find evidence of myopic loss aversion with bar charts and find that participants make better investment decisions with longer evaluation periods. We also find that common uncertainty representations such as interval plots and bar charts achieve the highest mean expected returns while other uncertainty visualizations lead to poorer long-term performance and strong effects on the equity premium. Qualitative feedback further suggests that different uncertainty representations lead to visual reasoning heuristics that can either mitigate or encourage a focus on potential short-term losses. We discuss implications of our results on using uncertainty visualizations for retirement decisions in practice and possible extensions for future work.},
  archive      = {J_TVCG},
  author       = {Ryan Wesslen and Alireza Karduni and Douglas Markant and Wenwen Dou},
  doi          = {10.1109/TVCG.2021.3114692},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {454-464},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effect of uncertainty visualizations on myopic loss aversion and the equity premium puzzle in retirement investment decisions},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visualizing uncertainty in probabilistic graphs with network
hypothetical outcome plots (NetHOPs). <em>TVCG</em>, <em>28</em>(1),
443–453. (<a href="https://doi.org/10.1109/TVCG.2021.3114679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic graphs are challenging to visualize using the traditional node-link diagram. Encoding edge probability using visual variables like width or fuzziness makes it difficult for users of static network visualizations to estimate network statistics like densities, isolates, path lengths, or clustering under uncertainty. We introduce Network Hypothetical Outcome Plots (NetHOPs), a visualization technique that animates a sequence of network realizations sampled from a network distribution defined by probabilistic edges. NetHOPs employ an aggregation and anchoring algorithm used in dynamic and longitudinal graph drawing to parameterize layout stability for uncertainty estimation. We present a community matching algorithm to enable visualizing the uncertainty of cluster membership and community occurrence. We describe the results of a study in which 51 network experts used NetHOPs to complete a set of common visual analysis tasks and reported how they perceived network structures and properties subject to uncertainty. Participants&#39; estimates fell, on average, within 11\% of the ground truth statistics, suggesting NetHOPs can be a reasonable approach for enabling network analysts to reason about multiple properties under uncertainty. Participants appeared to articulate the distribution of network statistics slightly more accurately when they could manipulate the layout anchoring and the animation speed. Based on these findings, we synthesize design recommendations for developing and using animated visualizations for probabilistic networks.},
  archive      = {J_TVCG},
  author       = {Dongping Zhang and Eytan Adar and Jessica Hullman},
  doi          = {10.1109/TVCG.2021.3114679},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {443-453},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualizing uncertainty in probabilistic graphs with network hypothetical outcome plots (NetHOPs)},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SPEULER: Semantics-preserving euler diagrams. <em>TVCG</em>,
<em>28</em>(1), 433–442. (<a
href="https://doi.org/10.1109/TVCG.2021.3114834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating comprehensible visualizations of highly overlapping set-typed data is a challenging task due to its complexity. To facilitate insights into set connectivity and to leverage semantic relations between intersections, we propose a fast two-step layout technique for Euler diagrams that are both well-matched and well-formed. Our method conforms to established form guidelines for Euler diagrams regarding semantics, aesthetics, and readability. First, we establish an initial ordering of the data, which we then use to incrementally create a planar, connected, and monotone dual graph representation. In the next step, the graph is transformed into a circular layout that maintains the semantics and yields simple Euler diagrams with smooth curves. When the data cannot be represented by simple diagrams, our algorithm always falls back to a solution that is not well-formed but still well-matched, whereas previous methods often fail to produce expected results. We show the usefulness of our method for visualizing set-typed data using examples from text analysis and infographics. Furthermore, we discuss the characteristics of our approach and evaluate our method against state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Rebecca Kehlbeck and Jochen Görtler and Yunhai Wang and Oliver Deussen},
  doi          = {10.1109/TVCG.2021.3114834},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {433-442},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SPEULER: Semantics-preserving euler diagrams},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). F2-bubbles: Faithful bubble set construction and flexible
editing. <em>TVCG</em>, <em>28</em>(1), 422–432. (<a
href="https://doi.org/10.1109/TVCG.2021.3114761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose F2-Bubbles, a set overlay visualization technique that addresses overlapping artifacts and supports interactive editing with intelligent suggestions. The core of our method is a new, efficient set overlay construction algorithm that approximates the optimal set overlay by considering set elements and their non-set neighbors. Thanks to the efficiency of the algorithm, interactive editing is achieved, and with intelligent suggestions, users can easily and flexibly edit visualizations through direct manipulations with local adaptations. A quantitative comparison with state-of-the-art set visualization techniques and case studies demonstrate the effectiveness of our method and suggests that F2-Bubbles is a helpful technique for set visualization.},
  archive      = {J_TVCG},
  author       = {Yunhai Wang and Da Cheng and Zhirui Wang and Jian Zhang and Liang Zhou and Gaoqi He and Oliver Deussen},
  doi          = {10.1109/TVCG.2021.3114761},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {422-432},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {F2-bubbles: Faithful bubble set construction and flexible editing},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Examining effort in 1D uncertainty communication using
individual differences in working memory and NASA-TLX. <em>TVCG</em>,
<em>28</em>(1), 411–421. (<a
href="https://doi.org/10.1109/TVCG.2021.3114803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As uncertainty visualizations for general audiences become increasingly common, designers must understand the full impact of uncertainty communication techniques on viewers&#39; decision processes. Prior work demonstrates mixed performance outcomes with respect to how individuals make decisions using various visual and textual depictions of uncertainty. Part of the inconsistency across findings may be due to an over-reliance on task accuracy, which cannot, on its own, provide a comprehensive understanding of how uncertainty visualization techniques support reasoning processes. In this work, we advance the debate surrounding the efficacy of modern 1D uncertainty visualizations by conducting converging quantitative and qualitative analyses of both the effort and strategies used by individuals when provided with quantile dotplots, density plots, interval plots, mean plots, and textual descriptions of uncertainty. We utilize two approaches for examining effort across uncertainty communication techniques: a measure of individual differences in working-memory capacity known as an operation span (OSPAN) task and self-reports of perceived workload via the NASA-TLX. The results reveal that both visualization methods and working-memory capacity impact participants&#39; decisions. Specifically, quantile dotplots and density plots (i.e., distributional annotations) result in more accurate judgments than interval plots, textual descriptions of uncertainty, and mean plots (i.e., summary annotations). Additionally, participants&#39; open-ended responses suggest that individuals viewing distributional annotations are more likely to employ a strategy that explicitly incorporates uncertainty into their judgments than those viewing summary annotations. When comparing quantile dotplots to density plots, this work finds that both methods are equally effective for low-working-memory individuals. However, for individuals with high-working-memory capacity, quantile dotplots evoke more accurate responses with less perceived effort. Given these results, we advocate for the inclusion of converging behavioral and subjective workload metrics in addition to accuracy performance to further disambiguate meaningful differences among visualization techniques.},
  archive      = {J_TVCG},
  author       = {Spencer C. Castro and P. Samuel Quinan and Helia Hosseinpour and Lace Padilla},
  doi          = {10.1109/TVCG.2021.3114803},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {411-421},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Examining effort in 1D uncertainty communication using individual differences in working memory and NASA-TLX},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GlyphCreator: Towards example-based automatic generation of
circular glyphs. <em>TVCG</em>, <em>28</em>(1), 400–410. (<a
href="https://doi.org/10.1109/TVCG.2021.3114877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circular glyphs are used across disparate fields to represent multidimensional data. However, although these glyphs are extremely effective, creating them is often laborious, even for those with professional design skills. This paper presents GlyphCreator, an interactive tool for the example-based generation of circular glyphs. Given an example circular glyph and multidimensional input data, GlyphCreator promptly generates a list of design candidates, any of which can be edited to satisfy the requirements of a particular representation. To develop GlyphCreator, we first derive a design space of circular glyphs by summarizing relationships between different visual elements. With this design space, we build a circular glyph dataset and develop a deep learning model for glyph parsing. The model can deconstruct a circular glyph bitmap into a series of visual elements. Next, we introduce an interface that helps users bind the input data attributes to visual elements and customize visual styles. We evaluate the parsing model through a quantitative experiment, demonstrate the use of GlyphCreator through two use scenarios, and validate its effectiveness through user interviews.},
  archive      = {J_TVCG},
  author       = {Lu Ying and Tan Tang and Yuzhe Luo and Lvkeshen Shen and Xiao Xie and Lingyun Yu and Yingcai Wu},
  doi          = {10.1109/TVCG.2021.3114877},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {400-410},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GlyphCreator: Towards example-based automatic generation of circular glyphs},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generative design inspiration for glyphs with diatoms.
<em>TVCG</em>, <em>28</em>(1), 389–399. (<a
href="https://doi.org/10.1109/TVCG.2021.3114792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Diatoms, a technique that generates design inspiration for glyphs by sampling from palettes of mark shapes, encoding channels, and glyph scaffold shapes. Diatoms allows for a degree of randomness while respecting constraints imposed by columns in a data table: their data types and domains as well as semantic associations between columns as specified by the designer. We pair this generative design process with two forms of interactive design externalization that enable comparison and critique of the design alternatives. First, we incorporate a familiar small multiples configuration in which every data point is drawn according to a single glyph design, coupled with the ability to page between alternative glyph designs. Second, we propose a small permutables design gallery, in which a single data point is drawn according to each alternative glyph design, coupled with the ability to page between data points. We demonstrate an implementation of our technique as an extension to Tableau featuring three example palettes, and to better understand how Diatoms could fit into existing design workflows, we conducted interviews and chauffeured demos with 12 designers. Finally, we reflect on our process and the designers&#39; reactions, discussing the potential of our technique in the context of visualization authoring systems. Ultimately, our approach to glyph design and comparison can kickstart and inspire visualization design, allowing for the serendipitous discovery of shape and channel combinations that would have otherwise been overlooked.},
  archive      = {J_TVCG},
  author       = {Matthew Brehmer and Robert Kosara and Carmen Hull},
  doi          = {10.1109/TVCG.2021.3114792},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {389-399},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Generative design inspiration for glyphs with diatoms},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VBridge: Connecting the dots between features and data to
explain healthcare models. <em>TVCG</em>, <em>28</em>(1), 378–388. (<a
href="https://doi.org/10.1109/TVCG.2021.3114836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) is increasingly applied to Electronic Health Records (EHRs) to solve clinical prediction tasks. Although many ML models perform promisingly, issues with model transparency and interpretability limit their adoption in clinical practice. Directly using existing explainable ML techniques in clinical settings can be challenging. Through literature surveys and collaborations with six clinicians with an average of 17 years of clinical experience, we identified three key challenges, including clinicians&#39; unfamiliarity with ML features, lack of contextual information, and the need for cohort-level evidence. Following an iterative design process, we further designed and developed VBridge, a visual analytics tool that seamlessly incorporates ML explanations into clinicians&#39; decision-making workflow. The system includes a novel hierarchical display of contribution-based feature explanations and enriched interactions that connect the dots between ML features, explanations, and data. We demonstrated the effectiveness of VBridge through two case studies and expert interviews with four clinicians, showing that visually associating model explanations with patients&#39; situational records can help clinicians better interpret and use model predictions when making clinician decisions. We further derived a list of design implications for developing future explainable ML tools to support clinical decision-making.},
  archive      = {J_TVCG},
  author       = {Furui Cheng and Dongyu Liu and Fan Du and Yanna Lin and Alexandra Zytek and Haomin Li and Huamin Qu and Kalyan Veeramachaneni},
  doi          = {10.1109/TVCG.2021.3114836},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {378-388},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VBridge: Connecting the dots between features and data to explain healthcare models},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FairRankVis: A visual analytics framework for exploring
algorithmic fairness in graph mining models. <em>TVCG</em>,
<em>28</em>(1), 368–377. (<a
href="https://doi.org/10.1109/TVCG.2021.3114850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph mining is an essential component of recommender systems and search engines. Outputs of graph mining models typically provide a ranked list sorted by each item&#39;s relevance or utility. However, recent research has identified issues of algorithmic bias in such models, and new graph mining algorithms have been proposed to correct for bias. As such, algorithm developers need tools that can help them uncover potential biases in their models while also exploring the impacts of correcting for biases when employing fairness-aware algorithms. In this paper, we present FairRankVis, a visual analytics framework designed to enable the exploration of multi-class bias in graph mining algorithms. We support both group and individual fairness levels of comparison. Our framework is designed to enable model developers to compare multi-class fairness between algorithms (for example, comparing PageRank with a debiased PageRank algorithm) to assess the impacts of algorithmic debiasing with respect to group and individual fairness. We demonstrate our framework through two usage scenarios inspecting algorithmic fairness.},
  archive      = {J_TVCG},
  author       = {Tiankai Xie and Yuxin Ma and Jian Kang and Hanghang Tong and Ross Maciejewski},
  doi          = {10.1109/TVCG.2021.3114850},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {368-377},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FairRankVis: A visual analytics framework for exploring algorithmic fairness in graph mining models},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual analysis of hyperproperties for understanding model
checking results. <em>TVCG</em>, <em>28</em>(1), 357–367. (<a
href="https://doi.org/10.1109/TVCG.2021.3114866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model checkers provide algorithms for proving that a mathematical model of a system satisfies a given specification. In case of a violation, a counterexample that shows the erroneous behavior is returned. Understanding these counterexamples is challenging, especially for hyperproperty specifications, i.e., specifications that relate multiple executions of a system to each other. We aim to facilitate the visual analysis of such counterexamples through our H yper V is tool, which provides interactive visualizations of the given model, specification, and counterexample. Within an iterative and interdisciplinary design process, we developed visualization solutions that can effectively communicate the core aspects of the model checking result. Specifically, we introduce graphical representations of binary values for improving pattern recognition, color encoding for better indicating related aspects, visually enhanced textual descriptions, as well as extensive cross-view highlighting mechanisms. Further, through an underlying causal analysis of the counterexample, we are also able to identify values that contributed to the violation and use this knowledge for both improved encoding and highlighting. Finally, the analyst can modify both the specification of the hyperproperty and the system directly within H yper V is and initiate the model checking of the new version. In combination, these features notably support the analyst in understanding the error leading to the counterexample as well as iterating the provided system and specification. We ran multiple case studies with H yper V is and tested it with domain experts in qualitative feedback sessions. The participants&#39; positive feedback confirms the considerable improvement over the manual, text-based status quo and the value of the tool for explaining hyperproperties.},
  archive      = {J_TVCG},
  author       = {Tom Horak and Norine Coenen and Niklas Metzger and Christopher Hahn and Tamara Flemisch and Julián Méndez and Dennis Dimov and Bernd Finkbeiner and Raimund Dachselt},
  doi          = {10.1109/TVCG.2021.3114866},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {357-367},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual analysis of hyperproperties for understanding model checking results},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An evaluation-focused framework for visualization
recommendation algorithms. <em>TVCG</em>, <em>28</em>(1), 346–356. (<a
href="https://doi.org/10.1109/TVCG.2021.3114814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although we have seen a proliferation of algorithms for recommending visualizations, these algorithms are rarely compared with one another, making it difficult to ascertain which algorithm is best for a given visual analysis scenario. Though several formal frameworks have been proposed in response, we believe this issue persists because visualization recommendation algorithms are inadequately specified from an evaluation perspective. In this paper, we propose an evaluation-focused framework to contextualize and compare a broad range of visualization recommendation algorithms. We present the structure of our framework, where algorithms are specified using three components: (1) a graph representing the full space of possible visualization designs, (2) the method used to traverse the graph for potential candidates for recommendation, and (3) an oracle used to rank candidate designs. To demonstrate how our framework guides the formal comparison of algorithmic performance, we not only theoretically compare five existing representative recommendation algorithms, but also empirically compare four new algorithms generated based on our findings from the theoretical comparison. Our results show that these algorithms behave similarly in terms of user performance, highlighting the need for more rigorous formal comparisons of recommendation algorithms to further clarify their benefits in various analysis scenarios.},
  archive      = {J_TVCG},
  author       = {Zehua Zeng and Phoebe Moh and Fan Du and Jane Hoffswell and Tak Yeon Lee and Sana Malik and Eunyee Koh and Leilani Battle},
  doi          = {10.1109/TVCG.2021.3114814},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {346-356},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {An evaluation-focused framework for visualization recommendation algorithms},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interactive visual pattern search on graph data via graph
representation learning. <em>TVCG</em>, <em>28</em>(1), 335–345. (<a
href="https://doi.org/10.1109/TVCG.2021.3114857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs are a ubiquitous data structure to model processes and relations in a wide range of domains. Examples include control-flow graphs in programs and semantic scene graphs in images. Identifying subgraph patterns in graphs is an important approach to understand their structural properties. We propose a visual analytics system GraphQ to support human-in-the-loop, example-based, subgraph pattern search in a database containing many individual graphs. To support fast, interactive queries, we use graph neural networks (GNNs) to encode a graph as fixed-length latent vector representation, and perform subgraph matching in the latent space. Due to the complexity of the problem, it is still difficult to obtain accurate one-to-one node correspondences in the matching results that are crucial for visualization and interpretation. We, therefore, propose a novel GNN for node-alignment called NeuroAlign, to facilitate easy validation and interpretation of the query results. GraphQ provides a visual query interface with a query editor and a multi-scale visualization of the results, as well as a user feedback mechanism for refining the results with additional constraints. We demonstrate GraphQ through two example usage scenarios: analyzing reusable subroutines in program workflows and semantic scene graph search in images. Quantitative experiments show that NeuroAlign achieves 19\%-29\% improvement in node-alignment accuracy compared to baseline GNN and provides up to 100× speedup compared to combinatorial algorithms. Our qualitative study with domain experts confirms the effectiveness for both usage scenarios.},
  archive      = {J_TVCG},
  author       = {Huan Song and Zeng Dai and Panpan Xu and Liu Ren},
  doi          = {10.1109/TVCG.2021.3114857},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {335-345},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive visual pattern search on graph data via graph representation learning},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). STRATISFIMAL LAYOUT: A modular optimization model for laying
out layered node-link network visualizations. <em>TVCG</em>,
<em>28</em>(1), 324–334. (<a
href="https://doi.org/10.1109/TVCG.2021.3114756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Node-link visualizations are a familiar and powerful tool for displaying the relationships in a network. The readability of these visualizations highly depends on the spatial layout used for the nodes. In this paper, we focus on computing layered layouts, in which nodes are aligned on a set of parallel axes to better expose hierarchical or sequential relationships. Heuristic-based layouts are widely used as they scale well to larger networks and usually create readable, albeit sub-optimal, visualizations. We instead use a layout optimization model that prioritizes optimality - as compared to scalability - because an optimal solution not only represents the best attainable result, but can also serve as a baseline to evaluate the effectiveness of layout heuristics. We take an important step towards powerful and flexible network visualization by proposing S tratisfimal L ayout , a modular integer-linear-programming formulation that can consider several important readability criteria simultaneously — crossing reduction, edge bendiness, and nested and multi-layer groups. The layout can be adapted to diverse use cases through its modularity. Individual features can be enabled and customized depending on the application. We provide open-source and documented implementations of the layout, both for web-based and desktop visualizations. As a proof-of-concept, we apply it to the problem of visualizing complicated SQL queries, which have features that we believe cannot be addressed by existing layout optimization models. We also include a benchmark network generator and the results of an empirical evaluation to assess the performance trade-offs of our design choices. A full version of this paper with all appendices, data, and source code is available at osf.io/qdyt9 with live examples at https://visdunneright.github.io/stratisfimal/ .},
  archive      = {J_TVCG},
  author       = {Sara di Bartolomeo and Mirek Riedewald and Wolfgang Gatterbauer and Cody Dunne},
  doi          = {10.1109/TVCG.2021.3114756},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {324-334},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {STRATISFIMAL LAYOUT: A modular optimization model for laying out layered node-link network visualizations},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Edge-path bundling: A less ambiguous edge bundling approach.
<em>TVCG</em>, <em>28</em>(1), 313–323. (<a
href="https://doi.org/10.1109/TVCG.2021.3114795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge bundling techniques cluster edges with similar attributes (i.e. similarity in direction and proximity) together to reduce the visual clutter. All edge bundling techniques to date implicitly or explicitly cluster groups of individual edges, or parts of them, together based on these attributes. These clusters can result in ambiguous connections that do not exist in the data. Confluent drawings of networks do not have these ambiguities, but require the layout to be computed as part of the bundling process. We devise a new bundling method, Edge-Path bundling, to simplify edge clutter while greatly reducing ambiguities compared to previous bundling techniques. Edge-Path bundling takes a layout as input and clusters each edge along a weighted, shortest path to limit its deviation from a straight line. Edge-Path bundling does not incur independent edge ambiguities typically seen in all edge bundling methods, and the level of bundling can be tuned through shortest path distances, Euclidean distances, and combinations of the two. Also, directed edge bundling naturally emerges from the model. Through metric evaluations, we demonstrate the advantages of Edge-Path bundling over other techniques.},
  archive      = {J_TVCG},
  author       = {Markus Wallinger and Daniel Archambault and David Auber and Martin Nöllenburg and Jaakko Peltonen},
  doi          = {10.1109/TVCG.2021.3114795},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {313-323},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Edge-path bundling: A less ambiguous edge bundling approach},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A domain-oblivious approach for learning concise
representations of filtered topological spaces for clustering.
<em>TVCG</em>, <em>28</em>(1), 302–312. (<a
href="https://doi.org/10.1109/TVCG.2021.3114872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persistence diagrams have been widely used to quantify the underlying features of filtered topological spaces in data visualization. In many applications, computing distances between diagrams is essential; however, computing these distances has been challenging due to the computational cost. In this paper, we propose a persistence diagram hashing framework that learns a binary code representation of persistence diagrams, which allows for fast computation of distances. This framework is built upon a generative adversarial network (GAN) with a diagram distance loss function to steer the learning process. Instead of using standard representations, we hash diagrams into binary codes, which have natural advantages in large-scale tasks. The training of this model is domain-oblivious in that it can be computed purely from synthetic, randomly created diagrams. As a consequence, our proposed method is directly applicable to various datasets without the need for retraining the model. These binary codes, when compared using fast Hamming distance, better maintain topological similarity properties between datasets than other vectorized representations. To evaluate this method, we apply our framework to the problem of diagram clustering and we compare the quality and performance of our approach to the state-of-the-art. In addition, we show the scalability of our approach on a dataset with 10k persistence diagrams, which is not possible with current techniques. Moreover, our experimental results demonstrate that our method is significantly faster with the potential of less memory usage, while retaining comparable or better quality comparisons.},
  archive      = {J_TVCG},
  author       = {Yu Qin and Brittany Terese Fasy and Carola Wenk and Brian Summa},
  doi          = {10.1109/TVCG.2021.3114872},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {302-312},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A domain-oblivious approach for learning concise representations of filtered topological spaces for clustering},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wasserstein distances, geodesics and barycenters of merge
trees. <em>TVCG</em>, <em>28</em>(1), 291–301. (<a
href="https://doi.org/10.1109/TVCG.2021.3114839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a unified computational framework for the estimation of distances, geodesics and barycenters of merge trees. We extend recent work on the edit distance [104] and introduce a new metric, called the Wasserstein distance between merge trees, which is purposely designed to enable efficient computations of geodesics and barycenters. Specifically, our new distance is strictly equivalent to the $L$2-Wasserstein distance between extremum persistence diagrams, but it is restricted to a smaller solution space, namely, the space of rooted partial isomorphisms between branch decomposition trees. This enables a simple extension of existing optimization frameworks [110] for geodesics and barycenters from persistence diagrams to merge trees. We introduce a task-based algorithm which can be generically applied to distance, geodesic, barycenter or cluster computation. The task-based nature of our approach enables further accelerations with shared-memory parallelism. Extensive experiments on public ensembles and SciVis contest benchmarks demonstrate the efficiency of our approach - with barycenter computations in the orders of minutes for the largest examples - as well as its qualitative ability to generate representative barycenter merge trees, visually summarizing the features of interest found in the ensemble. We show the utility of our contributions with dedicated visualization applications: feature tracking, temporal reduction and ensemble clustering. We provide a lightweight C++ implementation that can be used to reproduce our results.},
  archive      = {J_TVCG},
  author       = {Mathieu Pont and Jules Vidal and Julie Delon and Julien Tierny},
  doi          = {10.1109/TVCG.2021.3114839},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {291-301},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Wasserstein distances, geodesics and barycenters of merge trees},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interactive exploration of physically-observable objective
vortices in unsteady 2D flow. <em>TVCG</em>, <em>28</em>(1), 281–290.
(<a href="https://doi.org/10.1109/TVCG.2021.3115565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art computation and visualization of vortices in unsteady fluid flow employ objective vortex criteria, which makes them independent of reference frames or observers. However, objectivity by itself, although crucial, is not sufficient to guarantee that one can identify physically-realizable observers that would perceive or detect the same vortices. Moreover, a significant challenge is that a single reference frame is often not sufficient to accurately observe multiple vortices that follow different motions. This paper presents a novel framework for the exploration and use of an interactively-chosen set of observers, of the resulting relative velocity fields, and of objective vortex structures. We show that our approach facilitates the objective detection and visualization of vortices relative to well-adapted reference frame motions, while at the same time guaranteeing that these observers are in fact physically realizable. In order to represent and manipulate observers efficiently, we make use of the low-dimensional vector space structure of the Lie algebra of physically-realizable observer motions. We illustrate that our framework facilitates the efficient choice and guided exploration of objective vortices in unsteady 2D flow, on planar as well as on spherical domains, using well-adapted reference frames.},
  archive      = {J_TVCG},
  author       = {Xingdi Zhang and Markus Hadwiger and Thomas Theußl and Peter Rautek},
  doi          = {10.1109/TVCG.2021.3115565},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {281-290},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive exploration of physically-observable objective vortices in unsteady 2D flow},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). STNet: An end-to-end generative framework for synthesizing
spatiotemporal super-resolution volumes. <em>TVCG</em>, <em>28</em>(1),
270–280. (<a href="https://doi.org/10.1109/TVCG.2021.3114815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present STNet, an end-to-end generative framework that synthesizes spatiotemporal super-resolution volumes with high fidelity for time-varying data. STNet includes two modules: a generator and a spatiotemporal discriminator. The input to the generator is two low-resolution volumes at both ends, and the output is the intermediate and the two-ending spatiotemporal super-resolution volumes. The spatiotemporal discriminator, leveraging convolutional long short-term memory, accepts a spatiotemporal super-resolution sequence as input and predicts a conditional score for each volume based on its spatial (the volume itself) and temporal (the previous volumes) information. We propose an unsupervised pre-training stage using cycle loss to improve the generalization of STNet. Once trained, STNet can generate spatiotemporal super-resolution volumes from low-resolution ones, offering scientists an option to save data storage (i.e., sparsely sampling the simulation output in both spatial and temporal dimensions). We compare STNet with the baseline bicubic+linear interpolation, two deep learning solutions ( $\mathsf{SSR}+\mathsf{TSF}$ , STD), and a state-of-the-art tensor compression solution (TTHRESH) to show the effectiveness of STNet.},
  archive      = {J_TVCG},
  author       = {Jun Han and Hao Zheng and Danny Z. Chen and Chaoli Wang},
  doi          = {10.1109/TVCG.2021.3114815},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {270-280},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {STNet: An end-to-end generative framework for synthesizing spatiotemporal super-resolution volumes},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scope2Screen: Focus+context techniques for pathology tumor
assessment in multivariate image data. <em>TVCG</em>, <em>28</em>(1),
259–269. (<a href="https://doi.org/10.1109/TVCG.2021.3114786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspection of tissues using a light microscope is the primary method of diagnosing many diseases, notably cancer. Highly multiplexed tissue imaging builds on this foundation, enabling the collection of up to 60 channels of molecular information plus cell and tissue morphology using antibody staining. This provides unique insight into disease biology and promises to help with the design of patient-specific therapies. However, a substantial gap remains with respect to visualizing the resulting multivariate image data and effectively supporting pathology workflows in digital environments on screen. We, therefore, developed Scope2Screen, a scalable software system for focus+context exploration and annotation of whole-slide, high-plex, tissue images. Our approach scales to analyzing 100GB images of 10 9 or more pixels per channel, containing millions of individual cells. A multidisciplinary team of visualization experts, microscopists, and pathologists identified key image exploration and annotation tasks involving finding, magnifying, quantifying, and organizing regions of interest (ROIs) in an intuitive and cohesive manner. Building on a scope-to-screen metaphor, we present interactive lensing techniques that operate at single-cell and tissue levels. Lenses are equipped with task-specific functionality and descriptive statistics, making it possible to analyze image features, cell types, and spatial arrangements (neighborhoods) across image channels and scales. A fast sliding-window search guides users to regions similar to those under the lens; these regions can be analyzed and considered either separately or as part of a larger image collection. A novel snapshot method enables linked lens configurations and image statistics to be saved, restored, and shared with these regions. We validate our designs with domain experts and apply Scope2Screen in two case studies involving lung and colorectal cancers to discover cancer-relevant image features.},
  archive      = {J_TVCG},
  author       = {Jared Jessup and Robert Krueger and Simon Warchol and John Hoffer and Jeremy Muhlich and Cecily C. Ritch and Giorgio Gaglia and Shannon Coy and Yu-An Chen and Jia-Ren Lin and Sandro Santagata and Peter K. Sorger and Hanspeter Pfister},
  doi          = {10.1109/TVCG.2021.3114786},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {259-269},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scope2Screen: Focus+Context techniques for pathology tumor assessment in multivariate image data},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Loon: Using exemplars to visualize large-scale microscopy
data. <em>TVCG</em>, <em>28</em>(1), 248–258. (<a
href="https://doi.org/10.1109/TVCG.2021.3114766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Which drug is most promising for a cancer patient? A new microscopy-based approach for measuring the mass of individual cancer cells treated with different drugs promises to answer this question in only a few hours. However, the analysis pipeline for extracting data from these images is still far from complete automation: human intervention is necessary for quality control for preprocessing steps such as segmentation, adjusting filters, removing noise, and analyzing the result. To address this workflow, we developed Loon, a visualization tool for analyzing drug screening data based on quantitative phase microscopy imaging. Loon visualizes both derived data such as growth rates and imaging data. Since the images are collected automatically at a large scale, manual inspection of images and segmentations is infeasible. However, reviewing representative samples of cells is essential, both for quality control and for data analysis. We introduce a new approach for choosing and visualizing representative exemplar cells that retain a close connection to the low-level data. By tightly integrating the derived data visualization capabilities with the novel exemplar visualization and providing selection and filtering capabilities, Loon is well suited for making decisions about which drugs are suitable for a specific patient.},
  archive      = {J_TVCG},
  author       = {Devin Lange and Eddie Polanco and Robert Judson-Torres and Thomas Zangle and Alexander Lex},
  doi          = {10.1109/TVCG.2021.3114766},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {248-258},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Loon: Using exemplars to visualize large-scale microscopy data},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ThreadStates: State-based visual analysis of disease
progression. <em>TVCG</em>, <em>28</em>(1), 238–247. (<a
href="https://doi.org/10.1109/TVCG.2021.3114840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A growing number of longitudinal cohort studies are generating data with extensive patient observations across multiple timepoints. Such data offers promising opportunities to better understand the progression of diseases. However, these observations are usually treated as general events in existing visual analysis tools. As a result, their capabilities in modeling disease progression are not fully utilized. To fill this gap, we designed and implemented ThreadStates, an interactive visual analytics tool for the exploration of longitudinal patient cohort data. The focus of ThreadStates is to identify the states of disease progression by learning from observation data in a human-in-the-loop manner. We propose a novel Glyph Matrix design and combine it with a scatter plot to enable seamless identification, observation, and refinement of states. The disease progression patterns are then revealed in terms of state transitions using Sankey-based visualizations. We employ sequence clustering techniques to find patient groups with distinctive progression patterns, and to reveal the association between disease progression and patient-level features. The design and development were driven by a requirement analysis and iteratively refined based on feedback from domain experts over the course of a 10-month design study. Case studies and expert interviews demonstrate that ThreadStates can successively summarize disease states, reveal disease progression, and compare patient groups.},
  archive      = {J_TVCG},
  author       = {Qianwen Wang and Tali Mazor and Theresa A Harbig and Ethan Cerami and Nils Gehlenborg},
  doi          = {10.1109/TVCG.2021.3114840},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {238-247},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ThreadStates: State-based visual analysis of disease progression},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). COVID-view: Diagnosis of COVID-19 using chest CT.
<em>TVCG</em>, <em>28</em>(1), 227–237. (<a
href="https://doi.org/10.1109/TVCG.2021.3114851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant work has been done towards deep learning (DL) models for automatic lung and lesion segmentation and classification of COVID-19 on chest CT data. However, comprehensive visualization systems focused on supporting the dual visual+DL diagnosis of COVID-19 are non-existent. We present COVID-view , a visualization application specially tailored for radiologists to diagnose COVID-19 from chest CT data. The system incorporates a complete pipeline of automatic lungs segmentation, localization/isolation of lung abnormalities, followed by visualization, visual and DL analysis, and measurement/quantification tools. Our system combines the traditional 2D workflow of radiologists with newer 2D and 3D visualization techniques with DL support for a more comprehensive diagnosis. COVID-view incorporates a novel DL model for classifying the patients into positive/negative COVID-19 cases, which acts as a reading aid for the radiologist using COVID-view and provides the attention heatmap as an explainable DL for the model output. We designed and evaluated COVID-view through suggestions, close feedback and conducting case studies of real-world patient data by expert radiologists who have substantial experience diagnosing chest CT scans for COVID-19, pulmonary embolism, and other forms of lung infections. We present requirements and task analysis for the diagnosis of COVID-19 that motivate our design choices and results in a practical system which is capable of handling real-world patient cases.},
  archive      = {J_TVCG},
  author       = {Shreeraj Jadhav and Gaofeng Deng and Marlene Zawin and Arie E. Kaufman},
  doi          = {10.1109/TVCG.2021.3114851},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {227-237},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {COVID-view: Diagnosis of COVID-19 using chest CT},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Natural language to visualization by neural machine
translation. <em>TVCG</em>, <em>28</em>(1), 217–226. (<a
href="https://doi.org/10.1109/TVCG.2021.3114848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supporting the translation from natural language (NL) query to visualization (NL2VIS) can simplify the creation of data visualizations because if successful, anyone can generate visualizations by their natural language from the tabular data. The state-of-the-art NL2VIS approaches ( e.g. , NL4DV and FlowSense) are based on semantic parsers and heuristic algorithms, which are not end-to-end and are not designed for supporting (possibly) complex data transformations. Deep neural network powered neural machine translation models have made great strides in many machine translation tasks, which suggests that they might be viable for NL2VIS as well. In this paper, we present ncNet , a Transformer-based sequence-to-sequence model for supporting NL2VIS, with several novel visualization-aware optimizations, including using attention-forcing to optimize the learning process, and visualization-aware rendering to produce better visualization results. To enhance the capability of machine to comprehend natural language queries, ncNet is also designed to take an optional chart template ( e.g. , a pie chart or a scatter plot) as an additional input, where the chart template will be served as a constraint to limit what could be visualized. We conducted both quantitative evaluation and user study, showing that ncNet achieves good accuracy in the nvBench benchmark and is easy-to-use.},
  archive      = {J_TVCG},
  author       = {Yuyu Luo and Nan Tang and Guoliang Li and Jiawei Tang and Chengliang Chai and Xuedi Qin},
  doi          = {10.1109/TVCG.2021.3114848},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {217-226},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Natural language to visualization by neural machine translation},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VizLinter: A linter and fixer framework for data
visualization. <em>TVCG</em>, <em>28</em>(1), 206–216. (<a
href="https://doi.org/10.1109/TVCG.2021.3114804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the rising popularity of automated visualization tools, existing systems tend to provide direct results which do not always fit the input data or meet visualization requirements. Therefore, additional specification adjustments are still required in real-world use cases. However, manual adjustments are difficult since most users do not necessarily possess adequate skills or visualization knowledge. Even experienced users might create imperfect visualizations that involve chart construction errors. We present a framework, VizLinter, to help users detect flaws and rectify already-built but defective visualizations. The framework consists of two components, (1) a visualization linter, which applies well-recognized principles to inspect the legitimacy of rendered visualizations, and (2) a visualization fixer, which automatically corrects the detected violations according to the linter. We implement the framework into an online editor prototype based on Vega-Lite specifications. To further evaluate the system, we conduct an in-lab user study. The results prove its effectiveness and efficiency in identifying and fixing errors for data visualizations.},
  archive      = {J_TVCG},
  author       = {Qing Chen and Fuling Sun and Xinyue Xu and Zui Chen and Jiazhe Wang and Nan Cao},
  doi          = {10.1109/TVCG.2021.3114804},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {206-216},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VizLinter: A linter and fixer framework for data visualization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). KG4Vis: A knowledge graph-based approach for visualization
recommendation. <em>TVCG</em>, <em>28</em>(1), 195–205. (<a
href="https://doi.org/10.1109/TVCG.2021.3114863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualization recommendation or automatic visualization generation can significantly lower the barriers for general users to rapidly create effective data visualizations, especially for those users without a background in data visualizations. However, existing rule-based approaches require tedious manual specifications of visualization rules by visualization experts. Other machine learning-based approaches often work like black-box and are difficult to understand why a specific visualization is recommended, limiting the wider adoption of these approaches. This paper fills the gap by presenting KG4Vis, a knowledge graph (KG)-based approach for visualization recommendation. It does not require manual specifications of visualization rules and can also guarantee good explainability. Specifically, we propose a framework for building knowledge graphs, consisting of three types of entities (i.e., data features, data columns and visualization design choices) and the relations between them, to model the mapping rules between data and effective visualizations. A TransE-based embedding technique is employed to learn the embeddings of both entities and relations of the knowledge graph from existing dataset-visualization pairs. Such embeddings intrinsically model the desirable visualization rules. Then, given a new dataset, effective visualizations can be inferred from the knowledge graph with semantically meaningful rules. We conducted extensive evaluations to assess the proposed approach, including quantitative comparisons, case studies and expert interviews. The results demonstrate the effectiveness of our approach.},
  archive      = {J_TVCG},
  author       = {Haotian Li and Yong Wang and Songheng Zhang and Yangqiu Song and Huamin Qu},
  doi          = {10.1109/TVCG.2021.3114863},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {195-205},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {KG4Vis: A knowledge graph-based approach for visualization recommendation},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kori: Interactive synthesis of text and charts in data
documents. <em>TVCG</em>, <em>28</em>(1), 184–194. (<a
href="https://doi.org/10.1109/TVCG.2021.3114802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Charts go hand in hand with text to communicate complex data and are widely adopted in news articles, online blogs, and academic papers. They provide graphical summaries of the data, while text explains the message and context. However, synthesizing information across text and charts is difficult; it requires readers to frequently shift their attention. We investigated ways to support the tight coupling of text and charts in data documents. To understand their interplay, we analyzed the design space of chart-text references through news articles and scientific papers. Informed by the analysis, we developed a mixed-initiative interface enabling users to construct interactive references between text and charts. It leverages natural language processing to automatically suggest references as well as allows users to manually construct other references effortlessly. A user study complemented with algorithmic evaluation of the system suggests that the interface provides an effective way to compose interactive data documents.},
  archive      = {J_TVCG},
  author       = {Shahid Latif and Zheng Zhou and Yoon Kim and Fabian Beck and Nam Wook Kim},
  doi          = {10.1109/TVCG.2021.3114802},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {184-194},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Kori: Interactive synthesis of text and charts in data documents},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A mixed-initiative approach to reusing infographic charts.
<em>TVCG</em>, <em>28</em>(1), 173–183. (<a
href="https://doi.org/10.1109/TVCG.2021.3114856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infographic bar charts have been widely adopted for communicating numerical information because of their attractiveness and memorability. However, these infographics are often created manually with general tools, such as PowerPoint and Adobe Illustrator, and merely composed of primitive visual elements, such as text blocks and shapes. With the absence of chart models, updating or reusing these infographics requires tedious and error-prone manual edits. In this paper, we propose a mixed-initiative approach to mitigate this pain point. On one hand, machines are adopted to perform precise and trivial operations, such as mapping numerical values to shape attributes and aligning shapes. On the other hand, we rely on humans to perform subjective and creative tasks, such as changing embellishments or approving the edits made by machines. We encapsulate our technique in a PowerPoint add-in prototype and demonstrate the effectiveness by applying our technique on a diverse set of infographic bar chart examples.},
  archive      = {J_TVCG},
  author       = {Weiwei Cui and Jinpeng Wang and He Huang and Yun Wang and Chin-Yew Lin and Haidong Zhang and Dongmei Zhang},
  doi          = {10.1109/TVCG.2021.3114856},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {173-183},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A mixed-initiative approach to reusing infographic charts},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MultiVision: Designing analytical dashboards with deep
learning based recommendation. <em>TVCG</em>, <em>28</em>(1), 162–172.
(<a href="https://doi.org/10.1109/TVCG.2021.3114826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We contribute a deep-learning-based method that assists in designing analytical dashboards for analyzing a data table. Given a data table, data workers usually need to experience a tedious and time-consuming process to select meaningful combinations of data columns for creating charts. This process is further complicated by the needs of creating dashboards composed of multiple views that unveil different perspectives of data. Existing automated approaches for recommending multiple-view visualizations mainly build on manually crafted design rules, producing sub-optimal or irrelevant suggestions. To address this gap, we present a deep learning approach for selecting data columns and recommending multiple charts. More importantly, we integrate the deep learning models into a mixed-initiative system. Our model could make recommendations given optional user-input selections of data columns. The model, in turn, learns from provenance data of authoring logs in an offline manner. We compare our deep learning model with existing methods for visualization recommendation and conduct a user study to evaluate the usefulness of the system.},
  archive      = {J_TVCG},
  author       = {Aoyu Wu and Yun Wang and Mengyu Zhou and Xinyi He and Haidong Zhang and Huamin Qu and Dongmei Zhang},
  doi          = {10.1109/TVCG.2021.3114826},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {162-172},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MultiVision: Designing analytical dashboards with deep learning based recommendation},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). THALIS: Human-machine analysis of longitudinal symptoms in
cancer therapy. <em>TVCG</em>, <em>28</em>(1), 151–161. (<a
href="https://doi.org/10.1109/TVCG.2021.3114810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although cancer patients survive years after oncologic therapy, they are plagued with long-lasting or permanent residual symptoms, whose severity, rate of development, and resolution after treatment vary largely between survivors. The analysis and interpretation of symptoms is complicated by their partial co-occurrence, variability across populations and across time, and, in the case of cancers that use radiotherapy, by further symptom dependency on the tumor location and prescribed treatment. We describe THALIS, an environment for visual analysis and knowledge discovery from cancer therapy symptom data, developed in close collaboration with oncology experts. Our approach leverages unsupervised machine learning methodology over cohorts of patients, and, in conjunction with custom visual encodings and interactions, provides context for new patients based on patients with similar diagnostic features and symptom evolution. We evaluate this approach on data collected from a cohort of head and neck cancer patients. Feedback from our clinician collaborators indicates that THALIS supports knowledge discovery beyond the limits of machines or humans alone, and that it serves as a valuable tool in both the clinic and symptom research.},
  archive      = {J_TVCG},
  author       = {Carla Floricel and Nafiul Nipu and Mikayla Biggs and Andrew Wentzel and Guadalupe Canahuate and Lisanne Van Dijk and Abdallah Mohamed and C.David Fuller and G.Elisabeta Marai},
  doi          = {10.1109/TVCG.2021.3114810},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {151-161},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {THALIS: Human-machine analysis of longitudinal symptoms in cancer therapy},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gosling: A grammar-based toolkit for scalable and
interactive genomics data visualization. <em>TVCG</em>, <em>28</em>(1),
140–150. (<a href="https://doi.org/10.1109/TVCG.2021.3114876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combination of diverse data types and analysis tasks in genomics has resulted in the development of a wide range of visualization techniques and tools. However, most existing tools are tailored to a specific problem or data type and offer limited customization, making it challenging to optimize visualizations for new analysis tasks or datasets. To address this challenge, we designed Gosling-a grammar for interactive and scalable genomics data visualization. Gosling balances expressiveness for comprehensive multi-scale genomics data visualizations with accessibility for domain scientists. Our accompanying JavaScript toolkit called Gosling.js provides scalable and interactive rendering. Gosling.js is built on top of an existing platform for web-based genomics data visualization to further simplify the visualization of common genomics data formats. We demonstrate the expressiveness of the grammar through a variety of real-world examples. Furthermore, we show how Gosling supports the design of novel genomics visualizations. An online editor and examples of Gosling.js, its source code, and documentation are available at https://gosling.js.org .},
  archive      = {J_TVCG},
  author       = {Sehi LYi and Qianwen Wang and Fritz Lekschas and Nils Gehlenborg},
  doi          = {10.1109/TVCG.2021.3114876},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {140-150},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Gosling: A grammar-based toolkit for scalable and interactive genomics data visualization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An automated approach to reasoning about task-oriented
insights in responsive visualization. <em>TVCG</em>, <em>28</em>(1),
129–139. (<a href="https://doi.org/10.1109/TVCG.2021.3114782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Authors often transform a large screen visualization for smaller displays through rescaling, aggregation and other techniques when creating visualizations for both desktop and mobile devices (i.e., responsive visualization). However, transformations can alter relationships or patterns implied by the large screen view, requiring authors to reason carefully about what information to preserve while adjusting their design for the smaller display. We propose an automated approach to approximating the loss of support for task-oriented visualization insights (identification, comparison, and trend) in responsive transformation of a source visualization. We operationalize identification, comparison, and trend loss as objective functions calculated by comparing properties of the rendered source visualization to each realized target (small screen) visualization. To evaluate the utility of our approach, we train machine learning models on human ranked small screen alternative visualizations across a set of source visualizations. We find that our approach achieves an accuracy of 84\% (random forest model) in ranking visualizations. We demonstrate this approach in a prototype responsive visualization recommender that enumerates responsive transformations using Answer Set Programming and evaluates the preservation of task-oriented insights using our loss measures. We discuss implications of our approach for the development of automated and semi-automated responsive visualization recommendation.},
  archive      = {J_TVCG},
  author       = {Hyeok Kim and Ryan Rossi and Abhraneel Sarma and Dominik Moritz and Jessica Hullman},
  doi          = {10.1109/TVCG.2021.3114782},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {129-139},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {An automated approach to reasoning about task-oriented insights in responsive visualization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TIVEE: Visual exploration and explanation of badminton
tactics in immersive visualizations. <em>TVCG</em>, <em>28</em>(1),
118–128. (<a href="https://doi.org/10.1109/TVCG.2021.3114861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tactic analysis is a major issue in badminton as the effective usage of tactics is the key to win. The tactic in badminton is defined as a sequence of consecutive strokes. Most existing methods use statistical models to find sequential patterns of strokes and apply 2D visualizations such as glyphs and statistical charts to explore and analyze the discovered patterns. However, in badminton, spatial information like the shuttle trajectory, which is inherently 3D, is the core of a tactic. The lack of sufficient spatial awareness in 2D visualizations largely limited the tactic analysis of badminton. In this work, we collaborate with domain experts to study the tactic analysis of badminton in a 3D environment and propose an immersive visual analytics system, TIVEE, to assist users in exploring and explaining badminton tactics from multi-levels. Users can first explore various tactics from the third-person perspective using an unfolded visual presentation of stroke sequences. By selecting a tactic of interest, users can turn to the first-person perspective to perceive the detailed kinematic characteristics and explain its effects on the game result. The effectiveness and usefulness of TIVEE are demonstrated by case studies and an expert interview.},
  archive      = {J_TVCG},
  author       = {Xiangtong Chu and Xiao Xie and Shuainan Ye and Haolin Lu and Hongguang Xiao and Zeqing Yuan and Zhutian Chen and Hui Zhang and Yingcai Wu},
  doi          = {10.1109/TVCG.2021.3114861},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {118-128},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TIVEE: Visual exploration and explanation of badminton tactics in immersive visualizations},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). What’s the situation with situated visualization? A survey
and perspectives on situatedness. <em>TVCG</em>, <em>28</em>(1),
107–117. (<a href="https://doi.org/10.1109/TVCG.2021.3114835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Situated visualization is an emerging concept within visualization, in which data is visualized in situ, where it is relevant to people. The concept has gained interest from multiple research communities, including visualization, human-computer interaction (HCI) and augmented reality. This has led to a range of explorations and applications of the concept, however, this early work has focused on the operational aspect of situatedness leading to inconsistent adoption of the concept and terminology. First, we contribute a literature survey in which we analyze 44 papers that explicitly use the term “situated visualization” to provide an overview of the research area, how it defines situated visualization, common application areas and technology used, as well as type of data and type of visualizations. Our survey shows that research on situated visualization has focused on technology-centric approaches that foreground a spatial understanding of situatedness. Secondly, we contribute five perspectives on situatedness (space, time, place, activity, and community) that together expand on the prevalent notion of situatedness in the corpus. We draw from six case studies and prior theoretical developments in HCI. Each perspective develops a generative way of looking at and working with situatedness in design and research. We outline future directions, including considering technology, material and aesthetics, leveraging the perspectives for design, and methods for stronger engagement with target audiences. We conclude with opportunities to consolidate situated visualization research.},
  archive      = {J_TVCG},
  author       = {Nathalie Bressa and Henrik Korsgaard and Aurélien Tabard and Steven Houben and Jo Vermeulen},
  doi          = {10.1109/TVCG.2021.3114835},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {107-117},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {What&#39;s the situation with situated visualization? a survey and perspectives on situatedness},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring the personal informatics analysis gap: “There’s a
lot of bacon.” <em>TVCG</em>, <em>28</em>(1), 96–106. (<a
href="https://doi.org/10.1109/TVCG.2021.3114798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personal informatics research helps people track personal data for the purposes of self-reflection and gaining self-knowledge. This field, however, has predominantly focused on the data collection and insight-generation elements of self-tracking, with less attention paid to flexible data analysis. As a result, this inattention has led to inflexible analytic pipelines that do not reflect or support the diverse ways people want to engage with their data. This paper contributes a review of personal informatics and visualization research literature to expose a gap in our knowledge for designing flexible tools that assist people engaging with and analyzing personal data in personal contexts, what we call the personal informatics analysis gap. We explore this gap through a multistage longitudinal study on how asthmatics engage with personal air quality data, and we report how participants: were motivated by broad and diverse goals; exhibited patterns in the way they explored their data; engaged with their data in playful ways; discovered new insights through serendipitous exploration; and were reluctant to use analysis tools on their own. These results present new opportunities for visual analysis research and suggest the need for fundamental shifts in how and what we design when supporting personal data analysis.},
  archive      = {J_TVCG},
  author       = {Jimmy Moore and Pascal Goffin and Jason Wiese and Miriah Meyer},
  doi          = {10.1109/TVCG.2021.3114798},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {96-106},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring the personal informatics analysis gap: “There&#39;s a lot of bacon”},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Propagating visual designs to numerous plots and dashboards.
<em>TVCG</em>, <em>28</em>(1), 86–95. (<a
href="https://doi.org/10.1109/TVCG.2021.3114828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of developing an infrastructure for providing visualization and visual analytics (VIS) tools to epidemiologists and modeling scientists, we encountered a technical challenge for applying a number of visual designs to numerous datasets rapidly and reliably with limited development resources. In this paper, we present a technical solution to address this challenge. Operationally, we separate the tasks of data management, visual designs, and plots and dashboard deployment in order to streamline the development workflow. Technically, we utilize: an ontology to bring datasets, visual designs, and deployable plots and dashboards under the same management framework; multi-criteria search and ranking algorithms for discovering potential datasets that match a visual design; and a purposely-design user interface for propagating each visual design to appropriate datasets (often in tens and hundreds) and quality-assuring the propagation before the deployment. This technical solution has been used in the development of the RAMPVIS infrastructure for supporting a consortium of epidemiologists and modeling scientists through visualization.},
  archive      = {J_TVCG},
  author       = {Saiful Khan and Phong H. Nguyen and Alfie Abdul-Rahman and Benjamin Bach and Min Chen and Euan Freeman and Cagatay Turkay},
  doi          = {10.1109/TVCG.2021.3114828},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {86-95},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Propagating visual designs to numerous plots and dashboards},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards replacing physical testing of granular materials
with a topology-based model. <em>TVCG</em>, <em>28</em>(1), 76–85. (<a
href="https://doi.org/10.1109/TVCG.2021.3114819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the study of packed granular materials, the performance of a sample (e.g., the detonation of a high-energy explosive) often correlates to measurements of a fluid flowing through it. The “effective surface area,” the surface area accessible to the airflow, is typically measured using a permeametry apparatus that relates the flow conductance to the permeable surface area via the Carman-Kozeny equation. This equation allows calculating the flow rate of a fluid flowing through the granules packed in the sample for a given pressure drop. However, Carman-Kozeny makes inherent assumptions about tunnel shapes and flow paths that may not accurately hold in situations where the particles possess a wide distribution in shapes, sizes, and aspect ratios, as is true with many powdered systems of technological and commercial interest. To address this challenge, we replicate these measurements virtually on micro-CT images of the powdered material, introducing a new Pore Network Model based on the skeleton of the Morse-Smale complex. Pores are identified as basins of the complex, their incidence encodes adjacency, and the conductivity of the capillary between them is computed from the cross-section at their interface. We build and solve a resistive network to compute an approximate laminar fluid flow through the pore structure. We provide two means of estimating flow-permeable surface area: (i) by direct computation of conductivity, and (ii) by identifying dead-ends in the flow coupled with isosurface extraction and the application of the Carman-Kozeny equation, with the aim of establishing consistency over a range of particle shapes, sizes, porosity levels, and void distribution patterns.},
  archive      = {J_TVCG},
  author       = {Aniketh Venkat and Attila Gyulassy and Graham Kosiba and Amitesh Maiti and Henry Reinstein and Richard Gee and Peer-Timo Bremer and Valerio Pascucci},
  doi          = {10.1109/TVCG.2021.3114819},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {76-85},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards replacing physical testing of granular materials with a topology-based model},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MatExplorer: Visual exploration on predicting ionic
conductivity for solid-state electrolytes. <em>TVCG</em>,
<em>28</em>(1), 65–75. (<a
href="https://doi.org/10.1109/TVCG.2021.3114812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lithium ion batteries (LIBs) are widely used as important energy sources for mobile phones, electric vehicles, and drones. Experts have attempted to replace liquid electrolytes with solid electrolytes that have wider electrochemical window and higher stability due to the potential safety risks, such as electrolyte leakage, flammable solvents, poor thermal stability, and many side reactions caused by liquid electrolytes. However, finding suitable alternative materials using traditional approaches is very difficult due to the incredibly high cost in searching. Machine learning (ML)-based methods are currently introduced and used for material prediction. However, learning tools designed for domain experts to conduct intuitive performance comparison and analysis of ML models are rare. In this case, we propose an interactive visualization system for experts to select suitable ML models and understand and explore the predication results comprehensively. Our system uses a multifaceted visualization scheme designed to support analysis from various perspectives, such as feature distribution, data similarity, model performance, and result presentation. Case studies with actual lab experiments have been conducted by the experts, and the final results confirmed the effectiveness and helpfulness of our system.},
  archive      = {J_TVCG},
  author       = {Jiansu Pu and Hui Shao and Boyang Gao and Zhengguo Zhu and Yanlin Zhu and Yunbo Rao and Yong Xiang},
  doi          = {10.1109/TVCG.2021.3114812},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {65-75},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MatExplorer: Visual exploration on predicting ionic conductivity for solid-state electrolytes},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SightBi: Exploring cross-view data relationships with
biclusters. <em>TVCG</em>, <em>28</em>(1), 54–64. (<a
href="https://doi.org/10.1109/TVCG.2021.3114801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple-view visualization (MV) has been heavily used in visual analysis tools for sensemaking of data in various domains (e.g., bioinformatics, cybersecurity and text analytics). One common task of visual analysis with multiple views is to relate data across different views. For example, to identify threats, an intelligence analyst needs to link people from a social network graph with locations on a crime-map, and then search for and read relevant documents. Currently, exploring cross-view data relationships heavily relies on view-coordination techniques (e.g., brushing and linking), which may require significant user effort on many trial-and-error attempts, such as repetitiously selecting elements in one view, and then observing and following elements highlighted in other views. To address this, we present SightBi, a visual analytics approach for supporting cross-view data relationship explorations. We discuss the design rationale of SightBi in detail, with identified user tasks regarding the use of cross-view data relationships. SightBi formalizes cross-view data relationships as biclusters, computes them from a dataset, and uses a bi-context design that highlights creating stand-alone relationship-views. This helps preserve existing views and offers an overview of cross-view data relationships to guide user exploration. Moreover, SightBi allows users to interactively manage the layout of multiple views by using newly created relationship-views. With a usage scenario, we demonstrate the usefulness of SightBi for sensemaking of cross-view data relationships.},
  archive      = {J_TVCG},
  author       = {Maoyuan Sun and Abdul Rahman Shaikh and Hamed Alhoori and Jian Zhao},
  doi          = {10.1109/TVCG.2021.3114801},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {54-64},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SightBi: Exploring cross-view data relationships with biclusters},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic snapping for guided multi-view visualization
design. <em>TVCG</em>, <em>28</em>(1), 43–53. (<a
href="https://doi.org/10.1109/TVCG.2021.3114860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual information displays are typically composed of multiple visualizations that are used to facilitate an understanding of the underlying data. A common example are dashboards, which are frequently used in domains such as finance, process monitoring and business intelligence. However, users may not be aware of existing guidelines and lack expert design knowledge when composing such multi-view visualizations. In this paper, we present semantic snapping, an approach to help non-expert users design effective multi-view visualizations from sets of pre-existing views. When a particular view is placed on a canvas, it is “aligned” with the remaining views-not with respect to its geometric layout, but based on aspects of the visual encoding itself, such as how data dimensions are mapped to channels. Our method uses an on-the-fly procedure to detect and suggest resolutions for conflicting, misleading, or ambiguous designs, as well as to provide suggestions for alternative presentations. With this approach, users can be guided to avoid common pitfalls encountered when composing visualizations. Our provided examples and case studies demonstrate the usefulness and validity of our approach.},
  archive      = {J_TVCG},
  author       = {Yngve S. Kristiansen and Laura Garrison and Stefan Bruckner},
  doi          = {10.1109/TVCG.2021.3114860},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {43-53},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Semantic snapping for guided multi-view visualization design},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature curves and surfaces of 3D asymmetric tensor fields.
<em>TVCG</em>, <em>28</em>(1), 33–42. (<a
href="https://doi.org/10.1109/TVCG.2021.3114808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D asymmetric tensor fields have found many applications in science and engineering domains, such as fluid dynamics and solid mechanics. 3D asymmetric tensors can have complex eigenvalues, which makes their analysis and visualization more challenging than 3D symmetric tensors. Existing research in tensor field visualization focuses on 2D asymmetric tensor fields and 3D symmetric tensor fields. In this paper, we address the analysis and visualization of 3D asymmetric tensor fields. We introduce six topological surfaces and one topological curve, which lead to an eigenvalue space based on the tensor mode that we define. In addition, we identify several non-topological feature surfaces that are nonetheless physically important. Included in our analysis are the realizations that triple degenerate tensors are structurally stable and form curves, unlike the case for 3D symmetric tensors fields. Furthermore, there are two different ways of measuring the relative strengths of rotation and angular deformation in the tensor fields, unlike the case for 2D asymmetric tensor fields. We extract these feature surfaces using the A-patches algorithm. However, since three of our feature surfaces are quadratic, we develop a method to extract quadratic surfaces at any given accuracy. To facilitate the analysis of eigenvector fields, we visualize a hyperstreamline as a tree stem with the other two eigenvectors represented as thorns in the real domain or the dual-eigenvectors as leaves in the complex domain. To demonstrate the effectiveness of our analysis and visualization, we apply our approach to datasets from solid mechanics and fluid dynamics.},
  archive      = {J_TVCG},
  author       = {Shih-Hsuan Hung and Yue Zhang and Harry Yeh and Eugene Zhang},
  doi          = {10.1109/TVCG.2021.3114808},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {33-42},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Feature curves and surfaces of 3D asymmetric tensor fields},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Perception! Immersion! Empowerment! Superpowers as
inspiration for visualization. <em>TVCG</em>, <em>28</em>(1), 22–32. (<a
href="https://doi.org/10.1109/TVCG.2021.3114844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore how the lens of fictional superpowers can help characterize how visualizations empower people and provide inspiration for new visualization systems. Researchers and practitioners often tout visualizations&#39; ability to “make the invisible visible” and to “enhance cognitive abilities.” Meanwhile superhero comics and other modern fiction often depict characters with similarly fantastic abilities that allow them to see and interpret the world in ways that transcend traditional human perception. We investigate the intersection of these domains, and show how the language of superpowers can be used to characterize existing visualization systems and suggest opportunities for new and empowering ones. We introduce two frameworks: The first characterizes seven underlying mechanisms that form the basis for a variety of visual superpowers portrayed in fiction. The second identifies seven ways in which visualization tools and interfaces can instill a sense of empowerment in the people who use them. Building on these observations, we illustrate a diverse set of “visualization superpowers” and highlight opportunities for the visualization community to create new systems and interactions that empower new experiences with data Material and illustrations are available under CC-BY 4.0 at osf.io/8yhfz.},
  archive      = {J_TVCG},
  author       = {Wesley Willett and Bon Adriel Aseniero and Sheelagh Carpendale and Pierre Dragicevic and Yvonne Jansen and Lora Oehlberg and Petra Isenberg},
  doi          = {10.1109/TVCG.2021.3114844},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {22-32},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Perception! immersion! empowerment! superpowers as inspiration for visualization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IRVINE: A design study on analyzing correlation patterns of
electrical engines. <em>TVCG</em>, <em>28</em>(1), 11–21. (<a
href="https://doi.org/10.1109/TVCG.2021.3114797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this design study, we present IRVINE, a Visual Analytics (VA) system, which facilitates the analysis of acoustic data to detect and understand previously unknown errors in the manufacturing of electrical engines. In serial manufacturing processes, signatures from acoustic data provide valuable information on how the relationship between multiple produced engines serves to detect and understand previously unknown errors. To analyze such signatures, IRVINE leverages interactive clustering and data labeling techniques, allowing users to analyze clusters of engines with similar signatures, drill down to groups of engines, and select an engine of interest. Furthermore, IRVINE allows to assign labels to engines and clusters and annotate the cause of an error in the acoustic raw measurement of an engine. Since labels and annotations represent valuable knowledge, they are conserved in a knowledge database to be available for other stakeholders. We contribute a design study, where we developed IRVINE in four main iterations with engineers from a company in the automotive sector. To validate IRVINE, we conducted a field study with six domain experts. Our results suggest a high usability and usefulness of IRVINE as part of the improvement of a real-world manufacturing process. Specifically, with IRVINE domain experts were able to label and annotate produced electrical engines more than 30\% faster.},
  archive      = {J_TVCG},
  author       = {Joscha Eirich and Jakob Bonart and Dominik Jäckle and Michael Sedlmair and Ute Schmid and Kai Fischbach and Tobias Schreck and Jürgen Bernard},
  doi          = {10.1109/TVCG.2021.3114797},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {11-21},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IRVINE: A design study on analyzing correlation patterns of electrical engines},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simultaneous matrix orderings for graph collections.
<em>TVCG</em>, <em>28</em>(1), 1–10. (<a
href="https://doi.org/10.1109/TVCG.2021.3114773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Undirected graphs are frequently used to model phenomena that deal with interacting objects, such as social networks, brain activity and communication networks. The topology of an undirected graph $G$ can be captured by an adjacency matrix; this matrix in turn can be visualized directly to give insight into the graph structure. Which visual patterns appear in such a matrix visualization crucially depends on the ordering of its rows and columns. Formally defining the quality of an ordering and then automatically computing a high-quality ordering are both challenging problems; however, effective heuristics exist and are used in practice. Often, graphs do not exist in isolation but as part of a collection of graphs on the same set of vertices, for example, brain scans over time or of different people. To visualize such graph collections, we need a single ordering that works well for all matrices simultaneously. The current state-of-the-art solves this problem by taking a (weighted) union over all graphs and applying existing heuristics. However, this union leads to a loss of information, specifically in those parts of the graphs which are different. We propose a collection-aware approach to avoid this loss of information and apply it to two popular heuristic methods: leaf order and barycenter. The de-facto standard computational quality metrics for matrix ordering capture only block-diagonal patterns (cliques). Instead, we propose to use Moran&#39;s $I$, a spatial auto-correlation metric, which captures the full range of established patterns. Moran&#39;s $I$ refines previously proposed stress measures. Furthermore, the popular leaf order method heuristically optimizes a similar measure which further supports the use of Moran&#39;s $I$ in this context. An ordering that maximizes Moran&#39;s $I$ can be computed via solutions to the Traveling Salesperson Problem (TSP); orderings that approximate the optimal ordering can be computed more efficiently, using any of the approximation algorithms for metric TSP. We evaluated our methods for simultaneous orderings on real-world datasets using Moran&#39;s $I$ as the quality metric. Our results show that our collection-aware approach matches or improves performance compared to the union approach, depending on the similarity of the graphs in the collection. Specifically, our Moran&#39;s $I$-based collection-aware leaf order implementation consistently outperforms other implementations. Our collection-aware implementations carry no significant additional computational costs.},
  archive      = {J_TVCG},
  author       = {Nathan van Beusekom and Wouter Meulemans and Bettina Speckmann},
  doi          = {10.1109/TVCG.2021.3114773},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Simultaneous matrix orderings for graph collections},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
