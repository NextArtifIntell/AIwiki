<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>THMS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="thms---126">THMS - 126</h2>
<ul>
<li><details>
<summary>
(2022). Synergistic digital twin and holographic
augmented-reality-guided percutaneous puncture of respiratory liver
tumor. <em>THMS</em>, <em>52</em>(6), 1364–1374. (<a
href="https://doi.org/10.1109/THMS.2022.3185089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thermal ablation is an exciting new minimally invasive treatment that destroys liver tumors without removing them. It uses image guidance to place a needle through the skin into a liver tumor, which is highly dependent on surgeons’ experience. With the development of digital medicine, augmented reality (AR) has become a more intuitive and safer way to achieve real-time navigation. However, the technology is still in its infancy due to its limited accuracy and real-time performance. To address these problems, we syncretized the holographic AR with the digital twin technique to track the dynamic surgical scene and provide the 3-D navigation of heterogeneous target regions via internal motion prediction. To tackle the dilemma of real-time performance and precise internal motion estimation, a dynamic adaptation scheme is proposed to compensate for the time cost induced by the external/internal correlation model and data transmission. We carried out a series of experiments to validate our methods. With the proposed external/internal correlation model, the average estimation errors of the tumor and vessels are 2.18 and 2.79 mm, respectively. Besides, we performed in vivo experiments on two beagle dogs with an artificial lesion in their liver, respectively, and the puncture accuracy of our method are 2.5 and 2.17 mm. The results show that on one hand, our method can fulfill the real-time requirement of AR via using the intraoperative data, which is also more precise than that with preoperative data. On the other hand, our method can provide more 3-D information for surgeons, such as vessels, which can well ensure the safety of operation.},
  archive      = {J_THMS},
  author       = {Yangyang Shi and Xuesong Deng and Yuqi Tong and Ruotong Li and Yanfang Zhang and Lijie Ren and Weixin Si},
  doi          = {10.1109/THMS.2022.3185089},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1364-1374},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Synergistic digital twin and holographic augmented-reality-guided percutaneous puncture of respiratory liver tumor},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Applying sonification to sketching in the air with mobile AR
devices. <em>THMS</em>, <em>52</em>(6), 1352–1363. (<a
href="https://doi.org/10.1109/THMS.2022.3186592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With more and more mobile devices (such as smart phones and tablets) supporting augmented reality (AR), using these devices to sketch in mid-air has become a popular application direction. However, due to the small display size of these devices, the user&#39;s field of view is limited and cannot see the context of graphics, which leads to the drawn graphics deviating from their expectations. In this article, we applied sonification technology to mid-air sketching with mobile AR device, and proposed a new method to address this problem. In our first experiment, we verified the feasibility of our method. Our experimental results showed that sonification can effectively reduce the deviation caused by a narrow field of view. In our second experiment, we further explored the application ability of this method in a wider range of sketching. Our experimental results showed that sonification can improve the aspect ratio of the drawn graphics. In addition, the results of the NASA-TLX questionnaire showed that the participants&#39; mental demand and effort decreased significantly, and their subjective performance increased significantly. We proposed a new method, which can effectively improve the accuracy of mid-air sketching with a mobile AR device.},
  archive      = {J_THMS},
  author       = {Haonan Xu and Fei Lyu and Jin Huang and Huawei Tu},
  doi          = {10.1109/THMS.2022.3186592},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1352-1363},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Applying sonification to sketching in the air with mobile AR devices},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interactive extended reality techniques in information
visualization. <em>THMS</em>, <em>52</em>(6), 1338–1351. (<a
href="https://doi.org/10.1109/THMS.2022.3211317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive techniques, such as virtual reality, augmented reality, and mixed reality, take immersive displays as carriers to provide immersive experience. A large number of approaches focus on the visualization of scientific data in immersive environments while just a few methods concentrate on interactive information visualization (InfoVis) in an immersive environment, although InfoVis has been extended to the 3-D space for a long time. In the era of data explosion, the traditional 2-D space is unable to convey large amounts of abstract information in an intuitive way. Meanwhile, desktop-based 3-D InfoVis generally leads to visual conflict and confusion owing to limited display size and field of vision. In this survey, we search for the interactive techniques in immersive InfoVis and summarize their commonalities and discuss their differences and potential trends. The data types of abstract information in InfoVis can be categorized into graph/network data, high-dimensional and multivariate data, time-varying data, and text and document data. Besides, the visual presentation of information in immersive environments is also summarized, especially for charts, plots, and diagrams, which are some basic components of InfoVis techniques. We also described the immersive applications of InfoVis techniques, including the tools or frameworks on immersive analytics and infographics. The discussion about the traditional nonimmersive and the immersive methods in data visualizations show that the latter one has the potential to become an alternative to explore massive information in the future.},
  archive      = {J_THMS},
  author       = {Richen Liu and Min Gao and Lijun Wang and Xiaohan Wang and Yuzhe Xiang and Aolin Zhang and Jiazhi Xia and Yi Chen and Siming Chen},
  doi          = {10.1109/THMS.2022.3211317},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1338-1351},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Interactive extended reality techniques in information visualization},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ultrawideband ranging in dynamic dense human networks.
<em>THMS</em>, <em>52</em>(6), 1327–1337. (<a
href="https://doi.org/10.1109/THMS.2022.3175414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-line-of-sight (NLoS) conditions are a known performance limiting factor in ultrawideband (UWB) ranging accuracy, particularly in environments with densely packed dynamically moving humans, i.e., crowds. As UWB technology is recently seeing wide deployment in consumer devices, the effects of NLoS conditions due to human body shadowing will greatly degrade the performance of related ranging and localization applications. We thus propose the round-robin ranging protocol, an extension of alternative double-sided two-way ranging, which scales well for high rate sampling of dense networks and further propose a cooperative statistical approach based on multidimensional scaling (MDS) to mitigate NLoS conditions. Experimental validation was performed in a densely packed dynamically moving human environment. Nine subjects walked naturally within a 16-m 2 motion capture area while wearing UWB sensors at the foot, wrist, torso, and head and reflective optical motion capture markers. UWB accuracy of the 36 intersubject ranges was computed as the difference in range estimates between the UWB sensors and the optical motion capture system. Results showed that the proposed MDS approach reduced ranging accuracy errors by 11%, 13%, 16%, and 32% at the foot, wrist, torso, and head, respectively. Body placement had a significant effect on the ranging performance in that transceivers placed on the head and foot reduced ranging errors by 84% and 63% compared with the traditional torso placement. These results suggest that the proposed approaches could significantly improve the performance in UWB ranging applications involving human bodies such as human movement monitoring, athletics, logistics, and social contact tracing applications.},
  archive      = {J_THMS},
  author       = {David Chiasson and Peter B. Shull},
  doi          = {10.1109/THMS.2022.3175414},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1327-1337},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Ultrawideband ranging in dynamic dense human networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EchoWrite 2.0: A lightweight zero-shot text-entry system
based on acoustics. <em>THMS</em>, <em>52</em>(6), 1313–1326. (<a
href="https://doi.org/10.1109/THMS.2022.3175416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limited by size, shape, and other factors, it is rather inconvenient to interact with new smart devices by traditional methods. Acoustic-based methods following a machine learning approach have been put forward to resolve this problem in previous works. But they possess limitations of heavy training overhead, low performance for unseen users, and intensive computation cost. Following our previous work in this area, we further overcome shortcomings of existing work and propose a lightweight and zero-shot text-entry system for unseen users based on acoustic sensing. The key novelty of this work is proposing a new model training strategy including dataset construction and augmentation methods to effectively enhance generalization ability of a simple learning model with as few training data as possible, based on our insight into the problem. We design and implement a real-time Android application system called EchoWrite 2.0 to validate our idea with extensive experiments. Results show that EchoWrite 2.0 can recognize digits, English letters, and words with an accuracy of 85.3%, 73.2%, and 96.9%, respectively, for unseen users without providing any data to the learning model. The comparison with related work in different aspects shows overall superiority of EchoWrite 2.0 .},
  archive      = {J_THMS},
  author       = {Yongpan Zou and Zhihong Xiao and Shicong Hong and Zishuo Guo and Kaishun Wu},
  doi          = {10.1109/THMS.2022.3175416},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1313-1326},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {EchoWrite 2.0: A lightweight zero-shot text-entry system based on acoustics},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A model for estimating the leg mechanical work required to
walk with an elastically suspended backpack. <em>THMS</em>,
<em>52</em>(6), 1303–1312. (<a
href="https://doi.org/10.1109/THMS.2021.3137012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mechanical work performed by the individual legs affects the metabolic cost of locomotion. The effects of an elastically suspended backpack (ESB) on the mechanical work performed by the individual legs have not yet been quantified. This article explores the impact of variables, such as the stiffness and damper of an ESB, walking speed, and load mass, on the leg mechanical work (LMW). A model integrating an improved bipedal walking submodel and a spring−mass−damper submodel is proposed to estimate the mechanical work performed by the individual legs (LMW model). Experimental data were collected to estimate the accuracy of the proposed model. Seven subjects walked with a loaded ESB prototype at speeds ranging from 3.6 to 6.0 km/h with the suspension engaged and with the suspension locked out. The measured mechanical work performed by the individual legs was compared to the LMW model estimates. The proposed model estimates corresponded well with the empirical results (average R 2 = 0.909; estimated average error 3.6%). The LMW model was then used to simulate the effects of variables. The ESB produces positive or negative effects under different variables. With increasing ESB stiffness, the ESB first produces positive effects, then negative effects, and finally approaches the rigid backpack effect. The ESB damper also affects the magnitude of the effect. The smaller the damping, the larger the effect. These results could assist engineers trying to design ESB to minimize the mechanical work performed by the legs which may also minimize the metabolic energy cost.},
  archive      = {J_THMS},
  author       = {Yuquan Leng and Xin Lin and Lianxin Yang and Kuangen Zhang and Xinxing Chen and Chenglong Fu},
  doi          = {10.1109/THMS.2021.3137012},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1303-1312},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A model for estimating the leg mechanical work required to walk with an elastically suspended backpack},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AI-CardioCare: Artificial intelligence based device for
cardiac health monitoring. <em>THMS</em>, <em>52</em>(6), 1292–1302. (<a
href="https://doi.org/10.1109/THMS.2022.3211460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiac disorders are one of the leading causes of mortality around the globe and early diagnosis of heart diseases can be beneficial for its mitigation. In this article, an artificial intelligence (AI) based device has been proposed, which allows for an automatic and real-time diagnosis of cardiac diseases based on deep learning techniques. The heart sound (phonocardiogram) signal is acquired by a customized designed stethoscope and the signal is processed before analysis using AI methods for the classification of four major cardiac diseases (Aortic Stenosis, Mitral Regurgitation, Mitral Stenosis, and Mitral Valve Prolapse). Two deep learning-based neural networks, one-dimensional (1-D) convolutional neural network (CNN) and spectrogram based 2-D-CNN models from the analysis of these signals has been integrated with a low-cost single-board processor to make a standalone device. All data processing is done in a single hardware setup and user interface is provided allowing the user to control the data accessibility and visibility to generate the diagnostic report. As a result, the developed device has demonstrated to be a valuable low-cost diagnostic tool for both medical professionals and personal usage at home.},
  archive      = {J_THMS},
  author       = {Rakesh Chandra Joshi and Juwairiya Siraj Khan and Vinay Kumar Pathak and Malay Kishore Dutta},
  doi          = {10.1109/THMS.2022.3211460},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1292-1302},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {AI-CardioCare: Artificial intelligence based device for cardiac health monitoring},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimization of HD-sEMG-based cross-day hand gesture
classification by optimal feature extraction and data augmentation.
<em>THMS</em>, <em>52</em>(6), 1281–1291. (<a
href="https://doi.org/10.1109/THMS.2022.3175408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–machine interaction requires accurate recognition of human intentions (e.g., via hand gestures). Here, we assessed the cross-day robustness of widely used hand gesture classification techniques applied to high-density surface electromyogram (HD-sEMG) signals (256 channels). Our evaluation covered techniques in each stage of the classification framework: first, 50 temporal-spectral-spatial domain features, second, 15 feature optimization techniques, and third, seven classifiers. Moreover, although HD-sEMG provides sufficient neuromuscular information, some of the channels may present low signal-to-noise ratio and should therefore be treated as outliers. Accordingly, we performed our evaluation with, first, all outlier channels retained, and second, removal of the features corresponding to poor-quality channels and substitution with interpolated values from neighbor channels. The impact of sliding window and data augmentation was also investigated. We examined the results on a 35-gesture classification task using HD-sEMG acquired from 20 subjects on two sessions in separate days. The results showed that interpolation of features from outlier channels significantly improved the performance in most cases. Use of a sliding window and of data augmentation contributed to a higher classification accuracy. For the classification of 11 selected gestures of common daily use, the support vector machine classifier achieved the highest classification accuracy of 91.9% in a cross-day validation protocol using an optimal combination of 13 features (each extracted from sliding windows), feature optimization by linear discriminant analysis, and data augmentation. Our work can serve as a technique-screening tool on cross-day applications of human–machine interactions.},
  archive      = {J_THMS},
  author       = {Xinyu Jiang and Xiangyu Liu and Jiahao Fan and Xinming Ye and Chenyun Dai and Edward A. Clancy and Dario Farina and Wei Chen},
  doi          = {10.1109/THMS.2022.3175408},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1281-1291},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Optimization of HD-sEMG-based cross-day hand gesture classification by optimal feature extraction and data augmentation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised domain adaptation for gesture identification
against electrode shift. <em>THMS</em>, <em>52</em>(6), 1271–1280. (<a
href="https://doi.org/10.1109/THMS.2022.3179956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface electromyogram (sEMG)-based hand gesture recognition, which interprets commands given by humans through sEMG signals, performs well in many studies. However, its recognition accuracy drops dramatically due to electrode shift since the distributions of motion classes are changed. Although calibrating the system with newly collected samples after electrode shift maintains the accuracy, collecting labeled samples is inconvenient and time-consuming since the procedure is rigid. However, the calibration may not work properly without label, especially when the change is significant. This study proposes a user friendly and convenient calibration method for hand gesture recognition by an unsupervised domain adaptation method, which only obtains the unlabeled samples of preselected benchmark classes from users in calibration. The change of benchmark classes is captured by unlabeled samples by a clustering method. The other classes are estimated based on the benchmark classes by regression models. As a result, the information of all classes is used to calibrate the system. Linear discriminant analysis is used to demonstrate our model. A dataset with ten subjects is collected to verify the performance empirically. Experimental results confirm that our method utilizes the unlabeled benchmark class samples in calibration and achieves 75.55% average accuracy. Our method is more robust to electrode shift and improves around 8.5% accuracy consistently on all subjects compared with the methods without calibration or label information in calibration. Although the accuracy of our method is slightly less than the ones using label calibration samples, our calibration data collection is more convenient and less complicated.},
  archive      = {J_THMS},
  author       = {Patrick P. K. Chan and Qiuxia Li and Yinfeng Fang and Linyi Xu and Kairu Li and Honghai Liu and Daniel S. Yeung},
  doi          = {10.1109/THMS.2022.3179956},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1271-1280},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Unsupervised domain adaptation for gesture identification against electrode shift},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A variational bayesian gaussian mixture-nonnegative matrix
factorization model to extract movement primitives for robust control.
<em>THMS</em>, <em>52</em>(6), 1258–1270. (<a
href="https://doi.org/10.1109/THMS.2022.3194593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative matrix factorization (NMF) is a powerful tool for parameter estimation applied in numerous robotics applications, such as path planning, motion trajectory prediction, and motion intention detection. In particular, NMF has been successfully used to extract simplified and organized movement primitives from myoelectric signal (MES) for robust control of multi-degree of freedom humanoid robots. However, MES is typically contaminated by complex noise sources. The system performance often degrades due to the simplified Gaussian assumption of the noise distribution in existing NMF methods. Furthermore, most existing NMF models are unable to automatically determine the rank of the latent matrices. To address these issues, this article presents a hybrid variational Bayesian Gaussian mixture and NMF (GMNMF) model with a finite Gaussian mixture model adopted to fit the mixed noise density function of MES. In addition, the automatic relevant determination criterion is applied to automatically infer the number of movement primitives. The coordinate descent update rules for the proposed model are formulated by mean-field variational Bayesian inference. We assess the model performance on five synthetic noise distribution functions and an experimental MES dataset to perform six wrist movements. The results demonstrate that GMNMF yields low error and high robustness in extracting the movement primitives over four competitive methods for robust cybernetic control.},
  archive      = {J_THMS},
  author       = {Hongbo Xie and Kerrie Mengersen and Changan Di and Yongjian Zhang and Justin Lipman and Sabine Van Huffel},
  doi          = {10.1109/THMS.2022.3194593},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1258-1270},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A variational bayesian gaussian mixture-nonnegative matrix factorization model to extract movement primitives for robust control},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Myoelectric control with fixed convolution-based time-domain
feature extraction: Exploring the spatio–temporal interaction.
<em>THMS</em>, <em>52</em>(6), 1247–1257. (<a
href="https://doi.org/10.1109/THMS.2022.3146053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The role of feature extraction in electromyogram (EMG) based pattern recognition has recently been emphasized with several publications promoting deep learning (DL) solutions that outperform traditional methods. It has been shown that the ability of DL models to extract temporal, spatial, and spatio–temporal information provides significant enhancements to the performance and generalizability of myoelectric control. Despite these advancements, it can be argued that DL models are computationally very expensive, requiring long training times, increased training data, and high computational resources, yielding solutions that may not yet be feasible for clinical translation given the available technology. The aim of this paper is, therefore, to leverage the benefits of spatio–temporal DL concepts into a computationally feasible and accurate traditional feature extraction method. Specifically, the proposed novel method extracts a set of well-known time-domain features into a matrix representation, convolves them with predetermined fixed filters, and temporally evolves the resulting features over a short and long-term basis to extract the EMG temporal dynamics. The proposed method, based on Fixed Spatio–Temporal Convolutions, offers significant reductions in the computational costs, while demonstrating a solution that can compete with, and even outperform, recent DL models. Experimental tests were performed on sparse-and high-density EMG (HD-EMG) signals databases, across a total of 44 subjects performing a maximum of 53 movements. Despite the simplification compared to deep approaches, our results show that the proposed solution significantly reduces the classification error rates by 3% to 10% in comparison to recent DL models, while being efficient for real-time implementations.},
  archive      = {J_THMS},
  author       = {Rami N. Khushaba and Ali H. Al-Timemy and Oluwarotimi Williams Samuel and Erik J. Scheme},
  doi          = {10.1109/THMS.2022.3146053},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1247-1257},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Myoelectric control with fixed convolution-based time-domain feature extraction: Exploring the Spatio–Temporal interaction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Assessment of deep learning-based heart rate estimation
using remote photoplethysmography under different illuminations.
<em>THMS</em>, <em>52</em>(6), 1236–1246. (<a
href="https://doi.org/10.1109/THMS.2022.3207755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote photoplethysmography (rPPG) monitors heart rate (HR) without requiring physical contact, which has applications. Deep learning based rPPG has demonstrated superior performance over the traditional approaches in controlled context. However, the lighting situation in indoor space is typically complex, with uneven light distribution and frequent variations in illumination. It lacks a fair comparison of different methods under different illuminations using the same dataset. In this article, we present a public dataset, namely the BeiHang University remote photoplethysmography (BH-rPPG) dataset, which contains data from 35 subjects under three illuminations: 1) low; 2) medium; and 3) high illumination. We also provide the ground truth HR measured by an oximeter. We evaluate the performance of three deep learning-based methods (Deepphys, rPPGNet, and Physnet) to that of four traditional methods (CHROM, GREEN, ICA, and POS) using two public datasets: 1) UBFC-rPPG; 2) the BH-rPPG. The experimental results demonstrate that traditional methods are more resistant to fluctuating illuminations. We found that the Physnet achieves lowest mean absolute error among deep learning based method under medium illumination, whereas the CHROM achieves 1.04 beats per minute, outperforming the Physnet by 80 $\%$ . Additionally, we investigate potential methods for improving performance of deep learning based methods. We find that brightness augmentation make model more robust to variation illumination. These findings suggest that while developing deep learning based HR estimation algorithms, illumination variation should be taken into account. This work serves as a benchmark for rPPG performance evaluation and it opens a pathway for future investigation into deep learning based rPPG under illumination variations.},
  archive      = {J_THMS},
  author       = {Ze Yang and Haofei Wang and Feng Lu},
  doi          = {10.1109/THMS.2022.3207755},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1236-1246},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Assessment of deep learning-based heart rate estimation using remote photoplethysmography under different illuminations},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Task-dependent adaptations in closed-loop motor control
based on electrotactile feedback. <em>THMS</em>, <em>52</em>(6),
1227–1235. (<a href="https://doi.org/10.1109/THMS.2021.3134556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans systematically adapt their strategies for closed-loop control based on visual feedback according to the dynamics of the system. Tactile feedback is a key element in many human–machine interfaces, but it is not known if and how well human control adapts to changes in system dynamics when information about the system state is provided using this type of feedback. In this study, 11 participants tracked a pseudorandom trajectory with a virtual, position- or velocity-controlled plant using a joystick. Visual or electrotactile feedback provided the instantaneous error between the target and generated trajectory. Frequency-domain system identification indicated that human control adapted in similar ways to the different control modes (i.e., position/velocity control) for both feedback modalities. For the plant dynamics modeled as gain and integrator, the human controller behaved as a low-pass filter and gain, respectively (under the assumption of quasi-linear behavior). However, while tracking quality was largely similar for both control modes with visual feedback, velocity control enabled substantially worse control with electrotactile feedback compared to position control. Furthermore, for both control modes, the crossover frequency of open-loop transfer functions was lower for electrotactile feedback (0.9 and 1.1 rad/s) than for visual feedback (1.5 and 1.7 rad/s) indicating limited control bandwidth. To summarize, closed-loop control based on electrotactile feedback enables natural adaptations in human control strategy, which is encouraging for tactile feedback-controlled human–machine interfaces, but the lower control bandwidth and lower tracking quality with velocity control may impose functional limitations.},
  archive      = {J_THMS},
  author       = {Jakob L. Dideriksen and Irene Uriarte Mercader and Strahinja Dosen},
  doi          = {10.1109/THMS.2021.3134556},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1227-1235},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Task-dependent adaptations in closed-loop motor control based on electrotactile feedback},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling and prediction of user stability and comfortability
on autonomous wheelchairs with 3-d mapping. <em>THMS</em>,
<em>52</em>(6), 1216–1226. (<a
href="https://doi.org/10.1109/THMS.2022.3195775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional manual wheelchairs have a fixed seat with no movement or angle adjustment, which can seriously affect the user&#39;s comfort and greatly limit user experience. However, the electric wheelchair relies on strong intelligence and automatic features; it can not only realize the multidegree freedom adjustment of the human body and the seat but also has a rich and powerful man–machine control interface, which greatly facilitates and improves the user experience. This study upgraded a Permobil C400-powered wheelchair with multisensor data fusion technology to enrich its terrain recognition, tipping stability, and comfortability prediction. The tipping stability modeling of the wheelchair dummy system is carried out using multibody dynamics and vibration mechanics to obtain the tipping stability limit and the comfort evaluation of the wheelchair vibration acceleration on the human body during travel. Based on the elevation mapping method, the wheelchair can estimate the terrain from the local point of view at any point in time. At the same time, the RGB-D depth camera is connected to the robot operating system (ROS) system, and the open-source algorithm package RTAB-MAP is used to complete the MAP construction and collect the 3-D point-cloud terrain data. Then, the real 3-D terrain files are generated through the point-cloud stitching technology for stability simulation of the wheelchair–human system. The tipping stability and comfort indexes of the wheelchair–human system when passing over different physical terrains can be obtained. The experimental results show that the IMU data located on the human chest agree well with the simulation analysis data and are suitable for a variety of complex real-terrain conditions, verifying the accuracy of the wheelchair–human system dynamics model and the feasibility of the simulation analysis process. Thus, this modeling and simulation method can predict wheelchair stability and user comfortability well and ensure a high-performance experience.},
  archive      = {J_THMS},
  author       = {Haitao Luo and Zongming Yang and Peng Yin and Johnell O. Brooks and Bing Li},
  doi          = {10.1109/THMS.2022.3195775},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1216-1226},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Modeling and prediction of user stability and comfortability on autonomous wheelchairs with 3-D mapping},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Seizing the opportunity for automation—how traffic density
determines truck drivers’ use of cruise control. <em>THMS</em>,
<em>52</em>(6), 1205–1215. (<a
href="https://doi.org/10.1109/THMS.2022.3212335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze how traffic congestion affects the usage of cruise control in real-traffic situations. Usually, a negative relationship is assumed, but empirical evidence in naturalistic settings is rare. We make use of a large sample of truck drivers ( $N$ = 562) in Germany. We take advantage of the volatile traffic density imposed by the first implementation of COVID-19-related measures in Germany and analyze truck drivers resulting application of cruise control using random effects models. We match official traffic density information and the share of cruise control usage as recorded by the telematics information of each truck per day. We find that on average traffic density does have the expected effect: more traffic leads to a lower usage of cruise control. However, we find great heterogeneity among drivers and a strong tendency to react nonlinear to changes in traffic density. Additionally, comparing traffic types, heavy-goods vehicle traffic density has a stronger impact on drivers&#39; usage of cruise control than private traffic density. Again, not all drivers react similarly: we investigate eight different reaction patterns to varying types of congestion. Our results show that even in a favorable environment such as low traffic congestion, the use of cruise control cannot be taken for granted: about 25%–34% of the drivers (depending on traffic type) do not respond to changes in traffic density at all. Our results may help to inform models of traffic flow and traffic automation and may serve as a premise for more realistic assumptions about human behavior in traffic.},
  archive      = {J_THMS},
  author       = {Christin Hoffmann and Kirsten Thommes},
  doi          = {10.1109/THMS.2022.3212335},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1205-1215},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Seizing the opportunity for Automation—How traffic density determines truck drivers&#39; use of cruise control},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning for detection and risk assessment of
lifting action. <em>THMS</em>, <em>52</em>(6), 1196–1204. (<a
href="https://doi.org/10.1109/THMS.2022.3212666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Repetitive occupational lifting has been shown to create an increased risk for incidence of back pain. Ergonomic workstations that promote proper lifting technique can reduce risk, but it is difficult to assess the workstations without constant risk monitoring. Machine learning systems using inertial measurement unit (IMU) data have been successful in various human activity recognition (HAR) applications, but limited work has been done regarding tasks for which it is difficult to collect significant amounts of data, such as manual lifting tasks. In this article, we discuss why traditional methods of data expansion may fail to improve performance on IMU data, and we present a machine learning system capable of detecting lifting action for assessing the risk for back pain using a relatively small amount of data. The proposed models outperform baseline HAR models and function on raw time-series data with minimal preprocessing for efficient real-time application.},
  archive      = {J_THMS},
  author       = {Brennan Thomas and Ming-Lun Lu and Rashmi Jha and Joseph Bertrand},
  doi          = {10.1109/THMS.2022.3212666},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1196-1204},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Machine learning for detection and risk assessment of lifting action},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Alert! The appearance of moded input devices in the modern
airliner cockpit. <em>THMS</em>, <em>52</em>(6), 1186–1195. (<a
href="https://doi.org/10.1109/THMS.2021.3137022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automation in the flight deck of a modern airliner has grown in functionality over the past several decades, improving safety and operational efficiency. As the sophistication of automation has increased, some traditional input devices in the cockpit, such as throttle levers, yokes, and switches, have unobtrusively changed the way they work. Instead of operating the same way at all times, the behavior of these input devices has become context-sensitive (i.e., “moded”). It is well known in human factors design that nonsalient moded input devices require increased cognitive workload and can result in a startle, surprise, and confusion. An analysis of modern airliner cockpits identified three types of moded input devices: 1) disabled, 2) alternative behavior, and 3) command override. The implications of these results on cockpit design, certification, and flight crew training are discussed.},
  archive      = {J_THMS},
  author       = {Lance Sherry and Robert Mauro and Oleksandra Donnelly},
  doi          = {10.1109/THMS.2021.3137022},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1186-1195},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Alert! the appearance of moded input devices in the modern airliner cockpit},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online change point detection in application with
transition-aware activity recognition. <em>THMS</em>, <em>52</em>(6),
1176–1185. (<a href="https://doi.org/10.1109/THMS.2022.3185533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transition-aware activity recognition is an inherent component of online health monitoring and ambient assisted living. An explosion of technology breakthroughs in wireless sensor networks, wearable computing, and mobile computing has facilitated this. However, real time, dynamic activity recognition is still challenging in practice. As reported in the existing literature, machine learning techniques are successfully used on the presegmented data to deliver transition-aware activity recognition systems. However, these strategies are frequently ineffective when used in a near-real-time context. This article presents an online change point detection (OCPD) strategy to segment the continuous multivariate time-series smartphone sensor data and its application in a transition-aware activity recognition framework. The proposed OCPD strategy is based on the hypothesis-and-verification principle. After the online data stream segmentation using the proposed OCPD strategy, feature engineering is performed to retain the essential features. Then, synthetic minority oversampling technique (SMOTE) is applied to balance the dataset. Finally, practical experiments are carried out to verify the suggested frameworks’ efficiency and reliability. The results reveal that the proposed OCPD strategy with ensemble classifier achieves a greater recognition rate (F-Measure: 99.80%) compared to methods stated in the literature.},
  archive      = {J_THMS},
  author       = {Dipanwita Thakur and Suparna Biswas},
  doi          = {10.1109/THMS.2022.3185533},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1176-1185},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Online change point detection in application with transition-aware activity recognition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Formalizing distributed situation awareness in multi-agent
networks. <em>THMS</em>, <em>52</em>(6), 1166–1175. (<a
href="https://doi.org/10.1109/THMS.2022.3142109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The version of distributed situation awareness (DSA) used in this article originated in human factors/ergonomics. Typically, this has involved the study of human operators working in teams and has used various forms of concept maps to qualitatively describe the information that team members use. In this article, we apply the concept of DSA to multi-agent teams (where team members might be human or automation) and extend the concept using the formal properties of Bayesian belief networks. In particular, we show how the Bayesian belief network can define DSA and how such a network can (using expectation maximization) adapt, even on the basis of limited information. The approach was considered in terms of situation awareness levels of perception (i.e., sensor state values conversion to belief) and projection (in terms of prediction and mission missing values estimation accuracy).},
  archive      = {J_THMS},
  author       = {Sagir M. Yusuf and Chris Baber},
  doi          = {10.1109/THMS.2022.3142109},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1166-1175},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Formalizing distributed situation awareness in multi-agent networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-explaining abilities of an intelligent agent for
transparency in a collaborative driving context. <em>THMS</em>,
<em>52</em>(6), 1155–1165. (<a
href="https://doi.org/10.1109/THMS.2022.3202900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A critical challenge in human-autonomy teaming is for human players to comprehend their nonhuman teammates (agents). Transparency in agents&#39; behaviors is the key for such comprehension, which may be obtained by embedding a self-explanation ability into the agent to explain its own behaviors. Previous studies have relied on searching for the executed functions and logics to generate explanations for behaviors of goal-following logic-based agents. With the increasing number of functions and logics, current methods, such as component and process-based methods, have become impractical. This article proposes a new method exploiting the agent&#39;s artificial situation awareness states for generating explanations that involves several techniques: A Bayesian network, fuzzy theory, and Hamming distance. Our new method is evaluated in a collaborative driving context, in which a significant number of accidents recently occurred around the globe due to the lack of understanding of the autopilot agents. Using an autonomous driving simulator called Carla, two typical scenarios in collaborative driving, namely, traffic light and overtaking situations, are used. The findings show that the new method potentially reduces the search space in generating explanations and exhibits better computational performance and a lower cognitive workload. This work is important to calibrate human trust and to enhance comprehension of the agent.},
  archive      = {J_THMS},
  author       = {Rinta Kridalukmana and Haiyan Lu and Mohsen Naderpour},
  doi          = {10.1109/THMS.2022.3202900},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1155-1165},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Self-explaining abilities of an intelligent agent for transparency in a collaborative driving context},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The level of measurement of subjective situation awareness
and its dimensions in the situation awareness rating technique (SART).
<em>THMS</em>, <em>52</em>(6), 1147–1154. (<a
href="https://doi.org/10.1109/THMS.2021.3121960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Situation awareness (SA), a measure of how well a person understands the situation, is frequently used to evaluate the safety and effectiveness of critical systems that depend on human behavior. While there are objective ways of measuring SA, subjective assessments, such as the SA rating technique (SART), are still widely used. However, it is not clear what the level of measurement is for SART-measured SA or its constituent dimensions This is a significant gap because the level of measurement determines what mathematics and statistics can be meaningfully used to synthesize and evaluate measures. This research uses a previously developed method for determining the level of measurement of psychometric ratings to evaluate the level of measurement of SART and its elements. Results show that all of the dimensions of SA can be treated as interval in most situations, but that each is on a separate interval scale. This result casts doubt on the validity of the formula SART uses to compute SA from its subcomponents. We ultimately discuss our results and explore future research directions.},
  archive      = {J_THMS},
  author       = {Matthew L. Bolton and Elliot Biltekoff and Laura Humphrey},
  doi          = {10.1109/THMS.2021.3121960},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1147-1154},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {The level of measurement of subjective situation awareness and its dimensions in the situation awareness rating technique (SART)},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluation of a decision-based invocation strategy for
adaptive support for air traffic control. <em>THMS</em>, <em>52</em>(6),
1135–1146. (<a href="https://doi.org/10.1109/THMS.2022.3208817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air traffic controller workload is a limiting factor in the current air traffic management system. Adaptive support systems have the potential to balance controller workload and gain acceptance as they provide support during times of need. Challenges in the design of adaptive support systems are to decide when and how to trigger support. The goal of this study is to gain empirical insights into these challenges through a human-in-the-loop experiment, featuring a simplified air traffic control environment in which a novel triggering mechanism uses the quality of the controller&#39;s decisions to determine when support is needed. The designed system seeks to prevent high workload conditions by providing resolution advisories when the controller exceeds a threshold of “self-complicating” decisions. Results indicate that the new system is indeed capable of increasing the efficiency and safety compared to full manual control without intervention. More adaptive support, however, increased the frustration of participants, decreased acceptance, and did not result in improved workload ratings. These findings suggest that, unless we can better infer human intent in complex work environments, adaptive support at the level of decision-making is problematic. A potentially more fruitful direction is to provide support at the level of information integration, with full decision-making authority with the human.},
  archive      = {J_THMS},
  author       = {Martijn IJtsma and Clark Borst and Marinus M. van Paassen and Max Mulder},
  doi          = {10.1109/THMS.2022.3208817},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1135-1146},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Evaluation of a decision-based invocation strategy for adaptive support for air traffic control},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human decision-making modeling and cooperative controller
design for human–agent interaction systems. <em>THMS</em>,
<em>52</em>(6), 1122–1134. (<a
href="https://doi.org/10.1109/THMS.2022.3185333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the problem of human intervention and autonomous controller design for human–agent interaction systems is addressed. In particular, a human drift diffusion model is developed for second-order linear multiagent systems subject to unknown external disturbances. The proposed human drift diffusion model can model human decision-making behavior using multiple sources of human decision-making information. Accurate human intervention timing is obtained by setting varying thresholds for different decision-making information. In addition, a fixed-time sliding mode adaptive behavioural controller is developed to execute human decisions in the framework of the null-space based behavioral control method. The controller guarantees that agents can follow human commands given an arbitrary initial task error and can finish tasks in fixed time. A simulation under various scenarios shows that the proposed human drift diffusion model is able to provide appropriate human intervention and the proposed controller can guarantee human command fulfillment within fixed time.},
  archive      = {J_THMS},
  author       = {Jie Huang and Wenhua Wu and Zhenyi Zhang and Guoqing Tian and Song Zheng and Yutao Chen},
  doi          = {10.1109/THMS.2022.3185333},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1122-1134},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Human decision-making modeling and cooperative controller design for Human–Agent interaction systems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Touch semantics for intuitive physical manipulation of
humanoids. <em>THMS</em>, <em>52</em>(6), 1111–1121. (<a
href="https://doi.org/10.1109/THMS.2022.3207699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rather than systematically programming joint or task trajectories, having a human physically manipulate the robot for direct adjustments is more intuitive, saves time, and increases usability, especially for nonexperts. Interactive motion generation or repositioning of humanoid robots through direct human-touch manipulation is not an easy task, especially for high-level multijoint maneuvers. We propose a set of design rules for generating intuitive touch semantics called the “two-touch kinematic chain paradigm.” Our method interprets user touch intentions to allow motions ranging from low-level single joint control to high-level whole-body task control with posture generation, stepping, and walking. The goal is to provide the user with an intuitive protocol for physical humanoid manipulation that can serve the purpose of any application. The generated set of touch semantics is embodied in a finite state machine-based framework using a task-space quadratic programming controller to interpret human touch using capacitive sensors embedded in the humanoid shell, and force-torque sensors located at the ankles and wrists. A position-controlled humanoid robot is used to assess the utility and function of our proposed touch semantics for physical manipulation. Furthermore, a user study with nonexperts examines how our approach is perceived in practice.},
  archive      = {J_THMS},
  author       = {Christopher Yee Wong and Saeid Samadi and Wael Suleiman and Abderrahmane Kheddar},
  doi          = {10.1109/THMS.2022.3207699},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1111-1121},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Touch semantics for intuitive physical manipulation of humanoids},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Smart rollators aid devices: Current trends and challenges.
<em>THMS</em>, <em>52</em>(6), 1103–1110. (<a
href="https://doi.org/10.1109/THMS.2022.3202558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobility loss has a major impact on autonomy. Smart rollators have been proposed to enhance human abilities when conventional devices are not enough. Many human–robot interaction systems have been proposed in the last decade in this area. Comparative analysis shows that mechanical issues aside, they mainly differ in first, equipped sensors and actuators; second, input interface; third, operation modes, and fourth adaptation capabilities. This article presents a review and a tentative taxonomy of approaches during the last 6 years. In total, 92 papers have been reviewed. We have discarded works not focused on humanrobot interaction or focused only on mechanical adaptation. A critical analysis is provided after the review and classification, highlighting systems tested with their target population.},
  archive      = {J_THMS},
  author       = {Gabriela Verdezoto and Joaquin Ballesteros and Cristina Urdiales},
  doi          = {10.1109/THMS.2022.3202558},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1103-1110},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Smart rollators aid devices: Current trends and challenges},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). <span class="math inline"><em>Q</em></span>-mapping:
Learning user-preferred operation mappings with operation-action value
function. <em>THMS</em>, <em>52</em>(6), 1090–1102. (<a
href="https://doi.org/10.1109/THMS.2022.3207372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User interfaces have been designed to fit typical users and their usage styles as assumed by designers. However, it is impossible to cover all the possible use cases. To address this problem, we propose $Q$ -Mapping, which is a method for user interfaces to acquire the operation mapping, or mapping from user operations to their effects. $Q$ -Mapping has an advantage over previous techniques in that it can acquire operation mapping interactively. The core idea of $Q$ -Mapping is that what a user selects as an ideal action has a tendency to be the same as the action that has the highest $Q$ -value. On the basis of this concept, we defined the operation-action value function, which can be calculated from the value that a user expects to gain when a particular mapping is given in that state and is updated each time an operation occurs. We conducted a simulation experiment and a user study to investigate the $Q$ -Mapping performance and the effects of the acquisition of interactive operation mapping. The simulation results showed that the changeability of operation mapping could be controlled by a coefficient called the balancing parameter. As for the user study, we found that $Q$ -Mapping with a balancing parameter that decays with time was able to acquire operation mapping that was easy for users to understand. These results demonstrate the importance of balancing consistency and adaptability in the interactive acquisition of operation mapping.},
  archive      = {J_THMS},
  author       = {Riki Satogata and Mitsuhiko Kimoto and Yosuke Fukuchi and Kohei Okuoka and Michita Imai},
  doi          = {10.1109/THMS.2022.3207372},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1090-1102},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {$Q$-mapping: Learning user-preferred operation mappings with operation-action value function},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Public opinion about the benefit, risk, and acceptance of
aerial manipulation systems. <em>THMS</em>, <em>52</em>(5), 1069–1085.
(<a href="https://doi.org/10.1109/THMS.2022.3164775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aerial manipulation systems are an emerging subclass of unmanned aerial vehicles (UAVs or “drones”) that use a mobile arm to manipulate their environment. Public opinion is an important consideration for drones, but past public opinion polls have focused on drones without attached arms and have relied on text-based surveys. Study 1 ( N = 190) assessed participants’ perceived benefit, risk, and acceptance of aerial manipulation systems across five applications, using an animation-based online public opinion survey. Study 2 ( N = 194) assessed the influence of stimulus sampling on public opinion of aerial manipulation systems by replicating Study 1 using an alternative set of animations. Study 3 ( N = 396) assessed the influence of using animations versus a text control on perceptions of the poll and aerial manipulation systems, as well as the influence of survey platform (YouGov Direct versus Mechanical Turk). Results show that delivery applications are perceived as more beneficial than several other applications; that people’s opinions and imagining of drones were different if they watched animations versus read text; and that stimulus and population sampling influence the results of public opinion polls about drones.},
  archive      = {J_THMS},
  author       = {Jamy Li and Farrokh Janabi-Sharifi},
  doi          = {10.1109/THMS.2022.3164775},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {1069-1085},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Public opinion about the benefit, risk, and acceptance of aerial manipulation systems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of human whole-body joint torques during overhead
work with a passive exoskeleton. <em>THMS</em>, <em>52</em>(5),
1060–1068. (<a href="https://doi.org/10.1109/THMS.2021.3128892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Overheadwork is classifiedas one of the major risk factors for the onset of shoulder work-related musculoskeletal disorders and muscle fatigue. Upper-limb exoskeletons can be used to assist workers during the execution of industrial overhead tasks to prevent such disorders. Twelve novice participants have been equipped with inertial and force/torque sensors to simultaneously estimate the whole-body kinematics and the joint torques (i.e., internal articular stress) by means of a probabilistic estimator, while performing an overhead task with a pointing tool. An evaluation has been performed to analyze the effect at the whole-body level by considering the conditions of wearing and not-wearing PAEXO, a passive exoskeleton for upper-limb support during overhead work. Results point out that PAEXO provides a reduction of the whole-body joint effort across the experimental task blocks (from 66% to 86%). Moreover, the analysis along with five different body areas shows that 1) the exoskeleton provides support at the human shoulders by reducing the joint effort at the targeted limbs, and 2) that part of the internal wrenches is intuitively transferred from the upper body to the thighs and legs, which is shown with an increment of the torques at the legs joints. The promising outcomes show that the probabilistic estimation algorithm can be used as a validation metric to quantitatively assess PAEXO performances, paving thus the way for the next challenging milestone, such as the optimization of the human joint torques via adaptive exoskeleton control.},
  archive      = {J_THMS},
  author       = {Claudia Latella and Yeshasvi Tirupachuri and Luca Tagliapietra and Lorenzo Rapetti and Benjamin Schirrmeister and Jonas Bornmann and Daša Gorjan and Jernej Čamernik and Pauline Maurice and Lars Fritzsche and Jose Gonzalez-Vargas and Serena Ivaldi and Jan Babič and Francesco Nori and Daniele Pucci},
  doi          = {10.1109/THMS.2021.3128892},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {1060-1068},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Analysis of human whole-body joint torques during overhead work with a passive exoskeleton},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fuzzy inference-based driving control system for
pushrim-activated power-assisted wheelchairs considering user
characteristics. <em>THMS</em>, <em>52</em>(5), 1049–1059. (<a
href="https://doi.org/10.1109/THMS.2022.3195890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pushrim-activated power-assisted wheelchairs that assist the driving power by electric motors are effective in the daily lives of physically disabled people in terms of expanding the range of daily activities and comfortable driving even on rough and uphill roads. Meanwhile, users with unbalanced pushing power of the right and left arms due to some disability have difficulty controlling the wheelchair as he or she requests. For example, users with a lower-right arm strength have difficulty simultaneously driving forward and to the left. Therefore, a power-assist control system that considers the wheelchair operation characteristics of the individual user should be designed. The present article proposes a driving control system of pushrim-activated power-assisted wheelchairs with the target position estimation based on a learning model of individual operation characteristics. An individual estimation model based on fuzzy inference is constructed based on the right- and left-pushing torque information, which is properly revised to improve the position estimation accuracy. In addition, an operation training system with visual presentations to improve the proficiency of the user is effectively used. Experimental results using trial subjects, including a physically disabled person with arthrogryposis multiplex congenita, show that assisted driving intended by users can be realized, which verifies the effectiveness of the proposed driving control system and training system.},
  archive      = {J_THMS},
  author       = {Hirokazu Seki and Tatsuya Kuramoto},
  doi          = {10.1109/THMS.2022.3195890},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {1049-1059},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Fuzzy inference-based driving control system for pushrim-activated power-assisted wheelchairs considering user characteristics},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human–machine cooperative steering control considering
mitigating human–machine conflict based on driver trust. <em>THMS</em>,
<em>52</em>(5), 1036–1048. (<a
href="https://doi.org/10.1109/THMS.2022.3190683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To reduce the impact of human–machine conflict on vehicle safety, this study proposes a novel human–machine cooperative steering control approach from the perspective of driver trust in the machine. The relationship between driver trust in the machine and driving skill is analyzed by the chi-square test method, and an online cooperative algorithm is designed using fuzzy control for different conditions, which assigns control authority based on driver trust under safe conditions and gives most of the authority to the machine to ensure safety under dangerous conditions. The machine is designed using model predictive control as an alternative controller parallel to the driver. To implement the proposed approach, a simulation platform that includes drivers and a test vehicle is established. Based on the driving data of human drivers collected in field tests, a two-point visual driver model is established to simulate steering behaviors and reflect physical workload. The parameters of the driver model are identified by a particle swarm optimization method to represent different drivers. The effectiveness of the approach, such as guaranteeing vehicle safety and reducing physical workload and human–machine conflict, is verified by simulations under typical conditions and obstacle avoidance conditions based on veDYNA vehicle dynamics software.},
  archive      = {J_THMS},
  author       = {Zhuqing Shi and Hong Chen and Ting Qu and Shuyou Yu},
  doi          = {10.1109/THMS.2022.3190683},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {1036-1048},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Human–Machine cooperative steering control considering mitigating Human–Machine conflict based on driver trust},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An adaptive time budget adjustment strategy based on a
take-over performance model for passive fatigue. <em>THMS</em>,
<em>52</em>(5), 1025–1035. (<a
href="https://doi.org/10.1109/THMS.2021.3121665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As human-machine collaborative driving systems, highly automated driving vehicles require human drivers to take over when take-over requests are triggered. Extensive studies have shown that drivers’ take-over performance is affected by their fatigue state, traffic conditions, and the take-over time budget (TB). However, there is still a paucity of a systematic understanding of how these factors affect take-over performance, which prevents the implementation of adaptive take-over systems. This study establishes a highly accurate take-over performance prediction model to systematically explore the effects of these factors on take-over performance and to propose an adaptive TB adjustment strategy for highly automated driving vehicles. First, we propose metrics to evaluate drivers’ fatigue states and the relative positions of surrounding traffic. Second, a generalized additive model is established to predict take-over performance and accurately evaluate the influence of the aforementioned factors on take-over performance. Based on the model, we propose an adaptive adjustment strategy of the TB for take-over systems and demonstrate its effectiveness by a verification experiment. This study contributes to understanding the influence of drivers’ passive fatigue states, the relative positions of surrounding traffic, and the TB on drivers’ take-over performance as well as to the development of adaptive take-over systems for highly automated vehicles.},
  archive      = {J_THMS},
  author       = {Qingkun Li and Zhenyuan Wang and Wenjun Wang and Chao Zeng and Guofa Li and Quan Yuan and Bo Cheng},
  doi          = {10.1109/THMS.2021.3121665},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {1025-1035},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {An adaptive time budget adjustment strategy based on a take-over performance model for passive fatigue},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interruption management in the context of take-over-requests
in conditional driving automation. <em>THMS</em>, <em>52</em>(5),
1015–1024. (<a href="https://doi.org/10.1109/THMS.2022.3194006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drivers of partially automated vehicles are relieved from parts of the driving tasks allocated to the automated driver. This reduction in driving demands encourages them to engage with nondriving related tasks, which may impair awareness of the road environment once a takeover request (TOR) is initiated. This article examined the four suggested strategies drivers that take to regain control following a TOR, from the perspective of interruption management principles. Thirty students participated in a simulated study of two drives, where we manipulated TOR alerts, time to regain control, and potential road hazards. We hypothesized that all four interruption management strategies will be observed. Our hypothesis was confirmed. Four strategies were identified. Most drivers chose strategy 2 to accept and initiate the takeover immediately after the TOR started. The second frequent strategy was to reject the TOR but look at the road . Drivers’ strategy choices changed following alert type and the chronological drive order. With simulated driving experience (i.e., second drive), drivers postponed taking control, adapting to the time budget. Yet, inaccurate understanding of the situation or over-trust affected the chosen strategy. We conclude that interruption management principles are beneficial for studying how drivers respond to TORs and evaluating options to improve TOR performance.},
  archive      = {J_THMS},
  author       = {Avinoam Borowsky and Noa Zangi and Tal Oron-Gilad},
  doi          = {10.1109/THMS.2022.3194006},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {1015-1024},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Interruption management in the context of take-over-requests in conditional driving automation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online learning human behavior for a class of
human-in-the-loop systems via adaptive inverse optimal control.
<em>THMS</em>, <em>52</em>(5), 1004–1014. (<a
href="https://doi.org/10.1109/THMS.2022.3155369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance the machines’ intelligence, it is important for them to learn how humans perform tasks. In this article, the issue of online adaptive learning human behavior is addressed for a class of human-in-the-loop (HiTL) systems using the state measurement only. The hypothesis underlying our study is that human behavior can be described by a linear quadratic optimal control model with an unknown weighting matrix for the quadratic cost function. In this model, the weighting matrix depicts the human tradeoff of various objectives. Our aim is thus to only use the system state measurement for learning the weighting matrix under the condition that human feedback gain matrix is unknown. A novel adaptive inverse optimal control approach to online learning human behavior is proposed for the HiTL system, which integrates adaptive estimation and linear matrix inequality (LMI) optimization techniques. Our approach consists of two steps: First, an adaptive law is developed to learn the human feedback gain matrix online using the system state measurement only, and second, the weighting matrix of human cost function is retrieved by solving an LMI optimization problem with the learned feedback gain matrix. Finally, simulation and experiment results on a steering assist system of intelligent vehicles are presented to illustrate the effectiveness of the proposed method.},
  archive      = {J_THMS},
  author       = {Huai-Ning Wu},
  doi          = {10.1109/THMS.2022.3155369},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {1004-1014},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Online learning human behavior for a class of human-in-the-loop systems via adaptive inverse optimal control},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Electrodermal responses to driving maneuvers in a motion
sickness inducing real-world driving scenario. <em>THMS</em>,
<em>52</em>(5), 994–1003. (<a
href="https://doi.org/10.1109/THMS.2022.3188924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion sickness is a phenomenon attracting increasing attention with the ever-growing popularity of highly automated driving. Understanding motion sickness is of significant interest in the context of self-driven vehicles because, in this case, all occupants of the vehicle are passengers and, therefore, more susceptible to motion sickness. In this article, we report the findings of a study wherein motion sickness was induced in 40% of the participants while driving in real-world conditions. By recording various psychophysiological parameters continuously (electrodermal activity, skin temperature, heart rate, and heart rate variability), we investigate the feasibility of using these to objectively assess motion sickness. Furthermore, the instantaneous physiological reactions of participants to unpleasant driving maneuvers are examined. The changes in the electrodermal activity show a strong correlation with the subjective ratings of motion sickness levels as reported by the participants. The phasic component of the electrodermal activity suggests differences between participants that are susceptible to motion sickness and those who are not. Several driving maneuvers (accelerations, cornering, and driving over speed bumps) were identified as events triggering significant electrodermal responses. These responses could be the result of a mismatch between visual and vestibular perception acting as an aversive, arousing stimulus. While, in this work, the driving maneuvers were partially overlapping and nonuniform, our results pave the way for future investigation of physiological responses to single driving events and their relation to motion sickness with the potential to identify real-time markers of possibly unpleasant driving maneuvers.},
  archive      = {J_THMS},
  author       = {Elena N. Schneider and Benedikt Buchheit and Philipp Flotho and Mayur J. Bhamborae and Farah I. Corona-Strauss and Florian Dauth and Mohamad Alayan and Daniel J. Strauss},
  doi          = {10.1109/THMS.2022.3188924},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {994-1003},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Electrodermal responses to driving maneuvers in a motion sickness inducing real-world driving scenario},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Situated visual alarm displays support machine fitness
assessment for nonexplainable automation. <em>THMS</em>, <em>52</em>(5),
984–993. (<a href="https://doi.org/10.1109/THMS.2022.3155714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determine if situated visual alarm displays can support machine fitness assessment (MFA), facilitating improved hazard recognition and alarm accuracy assessment in the presence of inaccurate alarms. Poor performance of opaque automation is more difficult to detect, which increases the likelihood of cascades resulting in overall system failure. MFA reduces the negative impact of poor automation performance. Integrated alarm visualizations were shown to 32 nurses for 10 cases focused on patient outcome and 17 focused on alarm quality, all using real patient data. Five of the ten outcome cases would ultimately result in an emergency (unbeknownst to the nurse). Alarm cases ended with a true, false, or unnecessary alarm. Responses for nurses’ concern, confidence, alarm quality, and intended response were recorded. Qualitative analysis of interviews was performed. Using the situated visual alarm displays, nurses reported less confidence (6.5 vs. 9.1, p &lt; 0.001), more concern (5.4 vs. 1.6, p &lt; 0.001), and more urgent responses for emergency cases. Their alarm event detection was better than the alarms’ detection (0.608 vs. 0.438, p &lt; 0.001), as was their interpretation accuracy (0.453 vs. 0.243, p &lt; 0.001). Nurses showed differentiated concern for emergency cases, nonemergency cases with alarms, and those without alarms (5.4 vs. 3.8 vs. 1.6, p &lt; 0.001). Situated visual alarm displays combining visual trends with alarm signals improves detection of hazardous events and mitigates the negative effects of poor opaque automation performance.},
  archive      = {J_THMS},
  author       = {Michael F. Rayo and Chelsea R. Horwood and Morgan C. Fitzgerald and Marisa R. Grayson and Mahmoud Abdel-Rasoul and Susan D. Moffatt-Bruce},
  doi          = {10.1109/THMS.2022.3155714},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {984-993},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Situated visual alarm displays support machine fitness assessment for nonexplainable automation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Anticipating user intentions in customer care dialogue
systems. <em>THMS</em>, <em>52</em>(5), 973–983. (<a
href="https://doi.org/10.1109/THMS.2022.3184400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate the case of human-machine dialogues in the specific domain of commercial customer care. We built a corpus of conversations between users and a customer-care chatbot of an Italian Telecom Company, focusing on a sample of conversations where users contact the service asking for explanations about billing issues or overcharges. We observed that users’ requests are often vague, generic or incomprehensible. In such cases, commercial dialogue systems typically ask for clarifications or further details to fully understand users’ specific requests. However, from the corpus analysis it appeared that chatbot&#39;s clarifying requests may result in ineffective interactions, with users eventually giving up the conversation or switching to a human agent for a faster query resolution. A recovery strategy is thus needed to anticipate users’ information needs, or intentions. We address this issue resorting to GEN-DS, a dialogue system based on symbolic data-to-text generation. GEN-DS analyzes the user-company contextual relational knowledge, with the aim to generate more relevant answers to unclear questions. In this article, we describe the GEN-DS architecture along with the experiments we carried out to evaluate its output. Results from an offline human evaluation show significant improvements of GEN-DS compared to the original system. These improvements concern properties such as utility, necessity, understandability, and quickness of the information communicated in the dialogue. We believe that GEN-DS techniques may find application in all the dialogue systems that need to manage vague requests and must rely on relational knowledge.},
  archive      = {J_THMS},
  author       = {Alessandro Mazzei and Luca Anselma and Manuela Sanguinetti and Amon Rapp and Dario Mana and Md. Murad Hossain and Viviana Patti and Rossana Simeoni and Lucia Longo},
  doi          = {10.1109/THMS.2022.3184400},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {973-983},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Anticipating user intentions in customer care dialogue systems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An intelligent interaction framework for teleoperation based
on human-machine cooperation. <em>THMS</em>, <em>52</em>(5), 963–972.
(<a href="https://doi.org/10.1109/THMS.2022.3178590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing teleoperation technologies cannot guide robots to complete tasks quickly and accurately in unstructured environments. Therefore, this article proposes an intelligent interaction framework for teleoperation based on human-machine cooperation. The framework is divided into three layers: 1) perception, 2) decision-making, and 3) execution layers. In the perception layer, machines require high-precision environmental information, and human interaction requires rapid feedback on environmental changes. Therefore, a fast three-dimensional reconstruction method combining rough reconstruction and fine reconstruction is proposed. In the decision-making layer, a bare-hand interaction method based on natural interaction and an alignment assistance method based on constraint recognition are proposed. The combination of these two methods realizes the coordinated control of robot movement by humans and computers. In the execution layer, a vision-based error compensation method for assistance is proposed to decrease measurement errors and delays and reduce differences between virtual and real scenes. Finally, experimental results obtained by 15 nonprofessional volunteers show that the proposed method is efficient and user friendly.},
  archive      = {J_THMS},
  author       = {Guanglong Du and Yongda Deng and Wing W. Y. Ng and Di Li},
  doi          = {10.1109/THMS.2022.3178590},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {963-972},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {An intelligent interaction framework for teleoperation based on human-machine cooperation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A review on human–machine trust evaluation: Human-centric
and machine-centric perspectives. <em>THMS</em>, <em>52</em>(5),
952–962. (<a href="https://doi.org/10.1109/THMS.2022.3144956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As complex autonomous systems become increasingly ubiquitous, their deployment and integration into our daily lives will become a significant endeavor. Human–machine trust relationship is now acknowledged as one of the primary aspects that characterize a successful integration. In the context of human–machine interaction (HMI), proper use of machines and autonomous systems depends both on the human and machine counterparts. On one hand, it depends on how well the human relies on the machine regarding the situation or task at hand based on willingness and experience. On the other hand, it depends on how well the machine carries out the task and how well it conveys important information on how the job is done. Furthermore, proper calibration of trust for effective HMI requires the factors affecting trust to be properly accounted for and their relative importance to be rightly quantified. In this article, the functional understanding of human–machine trust is viewed from two perspectives—human-centric and machine- centric. The human aspect of the discussion outlines factors, scales, and approaches, which are available to measure and calibrate human trust. The discussion on the machine aspect spans trustworthy artificial intelligence, built-in machine assurances, and ethical frameworks of trustworthy machines.},
  archive      = {J_THMS},
  author       = {Biniam Gebru and Lydia Zeleke and Daniel Blankson and Mahmoud Nabil and Shamila Nateghi and Abdollah Homaifar and Edward Tunstel},
  doi          = {10.1109/THMS.2022.3144956},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {952-962},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A review on Human–Machine trust evaluation: Human-centric and machine-centric perspectives},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On computer mouse pointing model online identification and
endpoint prediction. <em>THMS</em>, <em>52</em>(5), 941–951. (<a
href="https://doi.org/10.1109/THMS.2021.3131660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new simplified pointing model as a feedback-based dynamical system, including both human and computer sides of the process. It takes into account the commutation between the correction and ballistic phases in pointing tasks. We use the mouse position increment signal from noisy experimental data to achieve our main objectives: to estimate the model parameters online and predict the task endpoint. Some estimation tools and validation results, applying linear regression techniques on the experimental data are presented. We also compare with a similar prediction algorithm to show the potential of our algorithm’s implementation.},
  archive      = {J_THMS},
  author       = {Anatolii Khalin and Rosane Ushirobira and Denis Efimov and Géry Casiez},
  doi          = {10.1109/THMS.2021.3131660},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {941-951},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {On computer mouse pointing model online identification and endpoint prediction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Latent representation in human–robot interaction with
explicit consideration of periodic dynamics. <em>THMS</em>,
<em>52</em>(5), 928–940. (<a
href="https://doi.org/10.1109/THMS.2022.3182909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a new data-driven framework for analyzing periodic physical human–robot interaction (pHRI) in latent state space. The model representing pHRI is critical for elaborating human understanding and/or robot control during pHRI. Recent advancements in deep learning technology would allow us to train such a model on a dataset collected from the actual pHRI. Our framework is based on a variational recurrent neural network (VRNN), which can process time-series data generated by a pHRI. This study modifies VRNN to explicitly integrate the latent dynamics from robot to human and to distinguish it from a human state estimate module. Furthermore, to analyze periodic motions, such as walking, we integrate VRNN with a new recurrent network based on reservoir computing (RC), which has random and fixed connections between numerous neurons. By boosting RC into a complex domain, periodic behavior can be represented as phase rotation in the complex domain without decaying the amplitude. A rope rotation/swinging experiment was used to validate the proposed framework. The proposed framework, trained on the collected experiment dataset, achieved the latent state space in which variation in periodic motions can be distinguished. The best prediction accuracy of the human observations and robot actions was obtained in such a well-distinguished space.},
  archive      = {J_THMS},
  author       = {Taisuke Kobayashi and Shingo Murata and Tetsunari Inamura},
  doi          = {10.1109/THMS.2022.3182909},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {928-940},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Latent representation in Human–Robot interaction with explicit consideration of periodic dynamics},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transparency analysis of a passive heavy load comanipulation
arm. <em>THMS</em>, <em>52</em>(5), 918–927. (<a
href="https://doi.org/10.1109/THMS.2022.3156887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For collaborative robotics applications, many systems have addressed general handling issues in recent years. Depending on the industrial context, these systems bring physical and cognitive feelings that result in the acceptance of their users. Transparency qualifies the ability of a robot to follow the movements imposed by the operator without noting any resistant effort. The objective of this article is to develop a methodology that mixes the approach highlighting the human factors and their correlation to robotic criteria in the case of a passive manipulation arm with six degrees of freedom produced by the Neoditech company. An exploratory study was then conducted to measure criteria such as time, speed, and effort during manipulation along with criteria based on a technological level, personality and technology acceptance model-method questionnaires. From there, we uncovered a correlation between the user’s personality, particularly their lack of neuroticism, and their means of evaluating the device through its usefulness, comfort, and indicators of mechanical behavior. This study is a preliminary analysis of user behaviors and traits that affect technological acceptance when dealing with a comanipulation arm. This work offers a framework for similar future analyses and recommends mechanical adjustments to the arm for increased user acceptance.},
  archive      = {J_THMS},
  author       = {Thomas Muller and Kévin Subrin and Denis Joncheray and Alain Billon and Sébastien Garnier},
  doi          = {10.1109/THMS.2022.3156887},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {918-927},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Transparency analysis of a passive heavy load comanipulation arm},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchy in algorithm-based feedback to patients working
with a robotic rehabilitation system: Toward user-experience
optimization. <em>THMS</em>, <em>52</em>(5), 907–917. (<a
href="https://doi.org/10.1109/THMS.2022.3170831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When robotic systems are developed for individualized training during rehabilitation, providing effective feedback to the users is essential. We aimed to create a rule-based set of guidelines for the desired hierarchy, timing, content, and modality of feedback such a system should provide to users, such that they receive the necessary feedback, at the relevant time, in a way that enhances their performance, and does not encumber it. We conducted four focus groups with 20 stroke clinicians. The clinicians described the guiding principles they use when giving feedback to patients, and noted different output should be provided to the patient versus the clinician by the rehabilitation system. They delineated a hierarchy for providing feedback during the exercise set: success on the task is the primary goal, and feedback should be given on this aspect first. Once success is achieved, feedback should be given on the quality of the movement. Only when the task is successfully completed, with no compensatory movements (i.e., high quality), feedback should be given on movement speed. Using a follow-up survey and the member-checking approach, it was revealed that this hierarchical structure applies to early stages in the rehabilitation process, and that quality of movement becomes paramount as the rehabilitation process progresses. The clinicians expressed their desire to receive a full report on the patient&#39;s performance in each exercise session, which is more comprehensive than the feedback provided to the patient in real time. We conclude with a set of guidelines for developing automated feedback for patient populations.},
  archive      = {J_THMS},
  author       = {Daphne Fruchter and Ronit Feingold Polak and Sigal Berman and Shelly Levy-Tzedek},
  doi          = {10.1109/THMS.2022.3170831},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {907-917},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Hierarchy in algorithm-based feedback to patients working with a robotic rehabilitation system: Toward user-experience optimization},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interaction task motion learning for human–robot interaction
control. <em>THMS</em>, <em>52</em>(5), 894–906. (<a
href="https://doi.org/10.1109/THMS.2022.3184051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional applications of robot manipulators are mainly limited to tracking control tasks, in which the desired objectives are specified as desired positions or trajectories. In such applications, a convenient and easy way of programming robots is the traditional teach-and-playback method. However, advances in sensing and robotic technologies have led to the requirements of more demanding tasks, in which robots may need to interact with a human or follow the human’s instructions in performing a sequence of more complex tasks. In such applications, it is not sufficient to just learn the positions or motion and play it back using a robot controller. In this article, a task learning approach is proposed for human–robot interaction systems, where a set of interaction behaviors is formulated and solved by specifying the task requirements in terms of potential energy. The motion behaviors demonstrated by humans can thus be acquired by the robot by seeking the appropriate task parameters of the dynamic potential energy function. To play back and combine the tasks in a sequential way by using a single controller, a new robot controller is also proposed. Lyapunov-like analysis is adopted to guarantee the stability of the control system, and experimental results are given to validate the performance of the proposed learning and control strategies.},
  archive      = {J_THMS},
  author       = {Shangke Lyu and Nithish Muthuchamy Selvaraj and Chien Chern Cheah},
  doi          = {10.1109/THMS.2022.3184051},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {894-906},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Interaction task motion learning for Human–Robot interaction control},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Review of closed-loop brain–machine interface systems from a
control perspective. <em>THMS</em>, <em>52</em>(5), 877–893. (<a
href="https://doi.org/10.1109/THMS.2021.3138677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, brain–machine interface (BMI) technology has made great progress in controlling external devices and restoring motor function for people with disabilities. To better optimize BMI system performance, in this article, we summarize and describe a universal closed-loop BMI system framework and review the latest developments over the past ten years from a control perspective. First, the basic BMI systems with open-loop and closed-loop structures are introduced in chronological order. Second, the units of the universal closed-loop BMI system, i.e., the decoder, encoder, and auxiliary controller, are reviewed and summarized in terms of principles, categories, and algorithms. Finally, from research and practical perspectives, the importance of biomimetic brain models, great challenges, and future developments are discussed based on current progress. With this analysis of the universal closed-loop framework, this review can provide necessary theoretical guidance for the research and development of BMI systems.},
  archive      = {J_THMS},
  author       = {Hongguang Pan and Haoqian Song and Qi Zhang and Wenyu Mi},
  doi          = {10.1109/THMS.2021.3138677},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {877-893},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Review of closed-loop Brain–Machine interface systems from a control perspective},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multifactor authentication system using simplified EEG
brain–computer interface. <em>THMS</em>, <em>52</em>(5), 867–876. (<a
href="https://doi.org/10.1109/THMS.2022.3196142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a scheme for multifactor authentication based on electroencephalography (EEG) signal analysis. A solution for EEG signal acquisition and recording of acquisition results has been implemented, a machine learning model has been developed and trained, then a classifier that determines the user&#39;s login procedure familiarity has been built, and a solution to carry out the described experiments has been implemented, in the form of a mobile application. Besides, a multifactor authentication system, based on the EEG signal combined with user image verification, using the brain–computer interface system with a single EEG electrode based on the NeuroSky MindWave device was proposed. Based on the defined scenarios, experiments were conducted, followed by a survey on the research group, and the obtained results were analyzed. In the case of an experiment related to login simulation, a high classification accuracy rate was obtained, both for the classifier itself (83.33%) and the proposed user authentication system (77.78%). Analyzing the results of the EEG signal recording used in the classification, it seems that the proposed solution is promising not only due to the high accuracy and a low false rejections rate but also through confirmed associations in the analysis of brain wave signal, corresponding to the results of research in the literature. A proposed multi-factor authentication system based on image selection and EEG analysis can be implemented in many areas as a modern solution in securing IT systems.},
  archive      = {J_THMS},
  author       = {Katarzyna Białas and Michał Kedziora and Rafał Chałupnik and Houbing Herbert Song},
  doi          = {10.1109/THMS.2022.3196142},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {867-876},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Multifactor authentication system using simplified EEG Brain–Computer interface},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time adjustment of tracking offsets through a
brain-computer interface for weight perception in virtual reality.
<em>THMS</em>, <em>52</em>(5), 855–866. (<a
href="https://doi.org/10.1109/THMS.2022.3182910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provision of the perception of pseudo-weight through tracking offsets in virtual reality (VR) allows users to estimate the weight of virtual objects. However, the contribution of the user&#39;s real-time perception to such illusions is unknown. Here, we focus on this issue using a brain-computer interface (BCI), through which the user&#39;s perception of the weight of virtual objects can be detected in real-time and used to adjust the tracking offset in a closed loop. We first trained a computational model with electroencephalography (EEG) data by asking users to imagine lifting a heavy or a light ball. With this model, the user&#39;s perception of the object weight could be detected through the BCI in real-time to adjust the tracking offset, thereby enabling further generation of a more realistic visual sensation. Then, we evaluated the effects of the BCI tracking offset on the perception of the weights of three virtual objects used to simulate real objects, namely, tennis, billiard, and bowling balls. Our results showed that the BCI tracking offset could assist participants in generating perceived weights for virtual objects in VR. We further showed that our approach can provide weight perception through real-time adjustment of the tracking offset, which might be useful for new virtual objects that appear suddenly in the virtual environment. Additionally, most participants (78%) preferred this BCI tracking offset system for weight perception. This article provides the first quantification of weight perception for a virtual object with a BCI that can be used to adjust the tracking offset in real-time for pseudo-weight perception in VR.},
  archive      = {J_THMS},
  author       = {Xupeng Ye and Jinyi Long},
  doi          = {10.1109/THMS.2022.3182910},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {855-866},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Real-time adjustment of tracking offsets through a brain-computer interface for weight perception in virtual reality},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Subject-independent classification of p300 event-related
potentials using a small number of training subjects. <em>THMS</em>,
<em>52</em>(5), 843–854. (<a
href="https://doi.org/10.1109/THMS.2022.3189576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intersubject variability present in electroencephalographic (EEG) signals can affect the performance of the brain–computer interface (BCI) systems. Despite the significant progress in the field, the variability in neural data remains one of the most critical challenges in constructing accurate predictive models of human intention. As a result, the majority of the previous studies have focused either on devising subject-specific signal processing and machine learning algorithms, used some data from a target user to update and calibrate a pretrained classifier, or have used data collected from a relatively large number of training subjects to construct generic classifiers for new subjects. In this work, we investigate the feasibility of using a relatively small number of training subjects to achieve subject-independent classification of event-related potentials (ERPs) in P300-based BCIs. To this end, we employ convolutional neural networks (CNNs) and propose a leave-one-subject-out cross-validation (LOSO-CV) for model selection; that is to say, for tuning CNN hyperparameters including number of layers, filters, kernel size, and epoch. The utility of the proposed model selection is warranted because LOSO-CV simulates the effect of subject-independent classification within the training data. The entire process of training (including model selection) is validated by applying another LOSO-CV external to the training process. Our empirical results obtained on four publicly available datasets confirm the capability of LOSO-CV model selection with CNN to capture intrinsic ERP features from a small group of subjects to classify observations collected from unseen subjects.},
  archive      = {J_THMS},
  author       = {Berdakh Abibullaev and Kassymzhomart Kunanbayev and Amin Zollanvari},
  doi          = {10.1109/THMS.2022.3189576},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {843-854},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Subject-independent classification of p300 event-related potentials using a small number of training subjects},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal reconstruction of human motion from scarce
multimodal data. <em>THMS</em>, <em>52</em>(5), 833–842. (<a
href="https://doi.org/10.1109/THMS.2022.3163184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wearable sensing has emerged as a promising solution for enabling unobtrusive and ergonomic measurements of the human motion. However, the reconstruction performance of these devices strongly depends on the quality and the number of sensors, which are typically limited by wearability and economic constraints. A promising approach to minimize the number of sensors is to exploit dimensionality reduction approaches that fuse prior information with insufficient sensing signals, through minimum variance estimation. These methods were successfully used for static hand pose reconstruction, but their translation to motion reconstruction has not been attempted yet. In this work, we propose the usage of functional principal component analysis to decompose multimodal, time-varying motion profiles in terms of linear combinations of basis functions. Functional decomposition enables the estimation of the a priori covariance matrix, and hence the fusion of scarce and noisy measured data with a priori information. We also consider the problem of identifying which elemental variables to measure as the most informative for a given class of tasks. We applied our method to two different datasets of upper limb motion D1 (joint trajectories) and D2 (joint trajectories + EMG data) considering an optimal set of measures (four joints for D1 out of seven, three joints, and eight EMGs for D2 out of seven and twelve, respectively). We found that our approach enables the reconstruction of upper limb motion with a median error of $0.013 \pm 0.006$ rad for D1 (relative median error 0.9%), and $0.038 \pm 0.023$ rad and $0.003 \pm 0.002$ mV for D2 (relative median error 2.9% and 5.1%, respectively).},
  archive      = {J_THMS},
  author       = {Giuseppe Averta and Matilde Iuculano and Paolo Salaris and Matteo Bianchi},
  doi          = {10.1109/THMS.2022.3163184},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {833-842},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Optimal reconstruction of human motion from scarce multimodal data},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video-based automatic wrist flexion and extension
classification. <em>THMS</em>, <em>52</em>(5), 824–832. (<a
href="https://doi.org/10.1109/THMS.2022.3164776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A computer vision method was developed to automatically measure wrist flexion and extension from a 2-D video for occupational health and safety research. Marker-less tracked skeletal joints of the elbow, wrist, and hand estimated the wrist flexion/extension angle between the hand and forearm. Based on the estimated angles, wrist posture was classified as flexion (palmar bending), neutral (no bending), or extension (dorsal bending) for each cycle of hand movement. Applying to a set of laboratory videos of a simulated repetitive motion task, we demonstrated the feasibility of using this algorithm for assessing the state of hand activities during manual work. Tested on 1464 frames from 61 recorded videos for 16 participants, the algorithm achieved an average performance of 72.40% correct, per-class accuracy. The sensitivity and specificity for flexion were 66.16% and 91.47%, respectively. The sensitivity and specificity for extension were 77.12% and 89.72%, respectively. This compared favorably against a previously reported consistency rate of 57% between human analyst estimates and wrist electrogoniometer measured wrist flexion/extension angles. We also applied this technique to 262 video frames of hand flexion instances selected from industrial field video data. For these videos, the average correct per-class accuracy was 76.03% in comparison to human observers. The sensitivity and specificity for flexion were 69.23% and 94.17%, respectively, and the sensitivity and specificity for extension were 91.95% and 80.57%, respectively.},
  archive      = {J_THMS},
  author       = {Cheng-Hsien Lee and Yu Hen Hu and Stephen Bao and Robert G. Radwin},
  doi          = {10.1109/THMS.2022.3164776},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {824-832},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Video-based automatic wrist flexion and extension classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An online multi-index approach to human ergonomics
assessment in the workplace. <em>THMS</em>, <em>52</em>(5), 812–823. (<a
href="https://doi.org/10.1109/THMS.2021.3133807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Work-related musculoskeletal disorders (WMSDs) remain one of the major occupational safety and health problems in the European Union nowadays. Thus, continuous tracking of workers’ exposure to the factors that may contribute to their develop- ment is paramount. This article introduces an online approach to monitor kinematic and dynamic quantities on the workers, providing on the spot an estimate of the physical load required in their daily jobs. A set of ergonomic indexes is defined to account for multiple potential contributors to WMSDs, also giving importance to the subject-specific requirements of the workers. To evaluate the proposed framework, a thorough experimental analysis was conducted on 12 human subjects considering tasks that represent typical working activities in the manufacturing sector. For each task, the ergonomic indexes that better explain the underlying physical load were identified, following a statistical analysis, and supported by the outcome of a surface electromyography analysis. A comparison was also made with a well-recognized and standard tool to evaluate human ergonomics in the workplace, to highlight the benefits introduced by the proposed framework. Results demonstrate the high potential of the proposed framework in identifying the physical risk factors, and therefore, to adopt preventive measures. Another equally important contribution of this article is the creation of a comprehensive database on human kinodynamic measurements, which hosts multiple sensory data of healthy subjects performing typical industrial tasks.},
  archive      = {J_THMS},
  author       = {Marta Lorenzini and Wansoo Kim and Arash Ajoudani},
  doi          = {10.1109/THMS.2021.3133807},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {812-823},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {An online multi-index approach to human ergonomics assessment in the workplace},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Supporting peripheral perception in distributed teams by
enforced exposure to chat messages. <em>THMS</em>, <em>52</em>(5),
802–811. (<a href="https://doi.org/10.1109/THMS.2022.3183546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Members of colocated teams benefit from being able to peripherally perceive ongoing conversations separating the useful information from the rest of the ambient sound. Instead of oral communication, distributed teams usually rely on chat. We developed an approach to supporting peripheral perception in distributed teams by enforced exposure to chat messages and implemented it as a chat client for Slack. The idea is to expose the team members to receive messages in order to emulate natural peripheral perception of oral communication that can be observed in colocated teams. We assessed how well enforced exposure to chat messages supports peripheral perception by following communication intensity, message relevance, and distraction in two experiment settings: content-based and periodical message displaying. The experiments were performed with four student teams in a week to two weeks’ time span. Seven team members had our chat client installed, while the rest of them (14) did not and used the unadapted chat clients they commonly use. The experiments were accompanied by a survey. Overall, enforced exposure to chat messages was perceived positively by the participants, especially in content-based message displaying. Communication intensity was clearly higher there, too. A strong correlation between the distraction caused by enforced exposure to chat messages and the communication intensity has been confirmed.},
  archive      = {J_THMS},
  author       = {Miroslav Novotný and Valentino Vranić},
  doi          = {10.1109/THMS.2022.3183546},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {802-811},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Supporting peripheral perception in distributed teams by enforced exposure to chat messages},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The b737 MAX 8 accidents as operational experiences with
automation transparency. <em>THMS</em>, <em>52</em>(4), 794–797. (<a
href="https://doi.org/10.1109/THMS.2022.3164774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automation transparency has come into prominence in the human-machine systems literature, with researchers offering definitions, models, design frameworks, and empirical findings. Missing from the literature are reflections on the actual operational experiences of human crews interacting with automation conceived from different transparency perspectives. The Boeing 737 MAX 8 accidents present a tragic case of a catastrophic failure of automation transparency. We explore this case with an emphasis on extracting design and regulatory insights for the nuclear power and other safety-critical domains.},
  archive      = {J_THMS},
  author       = {Greg A. Jamieson and Gyrd Skraaning and Jeffrey Joe},
  doi          = {10.1109/THMS.2022.3164774},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {794-797},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {The b737 MAX 8 accidents as operational experiences with automation transparency},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep object detector with attentional spatiotemporal LSTM
for space human–robot interaction. <em>THMS</em>, <em>52</em>(4),
784–793. (<a href="https://doi.org/10.1109/THMS.2022.3144951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global temporal information and local semantic information are essential cues for high-performance online object detection in videos. However, despite their promising detection accuracy in most cases, most state-of-the-art approaches have following two limitations: invalid background/scale suppression and inadequate temporal information mining between frames. Many jobs currently focus on temporal information learning based on a single frame. In this article, we propose an attentional global–local information learning network; this is one of the first attempts to fully use both types of information between frames. Attention maps are creatively utilized to transfer temporal contexts between frames. This also effectively alleviates the adverse effects of scale changes. Furthermore, empowered by a detailed framework, a proposed detector effectively uses multilevel feature extraction. Given these contributions, the proposed detector achieves state-of-the-art performance on challenging benchmarks. Finally, practical experiments are conducted on a space human–robot interaction platform.},
  archive      = {J_THMS},
  author       = {Jiahui Yu and Hongwei Gao and Yongquan Chen and Dalin Zhou and Jinguo Liu and Zhaojie Ju},
  doi          = {10.1109/THMS.2022.3144951},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {784-793},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Deep object detector with attentional spatiotemporal LSTM for space Human–Robot interaction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting human trust calibration in automation: A
convolutional neural network approach. <em>THMS</em>, <em>52</em>(4),
774–783. (<a href="https://doi.org/10.1109/THMS.2021.3137015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a general lack of studies that are aimed at monitoring and detecting an operator&#39;s trust calibration, even though detecting someone&#39;s adjusted trust towards automation is essential to prevent misuse and disuse of automation. The goal of this article is to propose a convolutional neural network (CNN) based framework to estimate operators’ trust levels and detect their trust calibration in automation using image features of electroencephalogram (EEG) signals preserving temporal, spectral, and spatial information. Thirteen participants performed a set of automated Air Force multiattribute task battery tasks that differed in reliability (High/Low) and credibility (High/Low) levels. The proposed framework was compared with three machine learning methods—naïve bayes, support vector machine, multilayer perceptron—in terms of accuracy, sensitivity, and specificity of trust estimation and detection of trust calibration. Results of this article showed that the proposed framework had the highest performance of both trust estimation and detection of trust calibration in automation compared to the other comparison methods. This indicates that the proposed framework using the CNN classifier with the image-based EEG features could be an applicable model for estimating multilevel trust and detecting trust calibration during human-automation interaction. Also, it can help to prevent disuse and misuse of automation by estimating operators’ trust levels and monitoring their trust calibration in automation.},
  archive      = {J_THMS},
  author       = {Sanghyun Choo and Chang S. Nam},
  doi          = {10.1109/THMS.2021.3137015},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {774-783},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Detecting human trust calibration in automation: A convolutional neural network approach},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic role-based access control policy for smart grid
applications: An offline deep reinforcement learning approach.
<em>THMS</em>, <em>52</em>(4), 761–773. (<a
href="https://doi.org/10.1109/THMS.2022.3163185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Role-based access control (RBAC) is adopted in the information and communication technology domain for authentication purposes. However, due to a very large number of entities within organizational access control (AC) systems, static RBAC management can be inefficient, costly, and can lead to cybersecurity threats. In this article, a novel hybrid RBAC model is proposed, based on the principles of offline deep reinforcement learning (RL) and Bayesian belief networks. The considered framework utilizes a fully offline RL agent, which models the behavioral history of users as a Bayesian belief-based trust indicator. Thus, the initial static RBAC policy is improved in a dynamic manner through off-policy learning while guaranteeing compliance of the internal users with the security rules of the system. By deploying our implementation within the smart grid domain and specifically within a Distributed Energy Resources (DER) ecosystem, we provide an end-to-end proof of concept of our model. Finally, detailed analysis and evaluation regarding the offline training phase of the RL agent are provided, while the online deployment of the hybrid RL-based RBAC model into the DER ecosystem highlights its key operation features and salient benefits over traditional RBAC models.},
  archive      = {J_THMS},
  author       = {Georgios Fragkos and Jay Johnson and Eirini Eleni Tsiropoulou},
  doi          = {10.1109/THMS.2022.3163185},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {761-773},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Dynamic role-based access control policy for smart grid applications: An offline deep reinforcement learning approach},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SSGNN: A macro and microfacial expression recognition graph
neural network combining spatial and spectral domain features.
<em>THMS</em>, <em>52</em>(4), 747–760. (<a
href="https://doi.org/10.1109/THMS.2022.3163211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition from macroexpression and microexpression has been widely used in applications such as human–computer interaction, learning status evaluation, and mental disorder diagnosis. However, due to the complexity of human macroexpressions, recognizing macroexpressions with high accuracy is a challenging task. Moreover, the short duration and low movement intensity of microexpressions make its recognition more difficult. For MM-FER (macro and microfacial expression recognition), the key information can be more efficiently expressed by a graph. In this article, a novel framework based on graph neural network named SSGNN (spatial and spectral domain features based on a graph neural network) is designed to extract spatial and spectral domain features from facial images for MM-FER, which can efficiently recognize both macroexpressions and microexpressions under the same model. SSGNN consists of two parts, SPAGNN and SPEGNN, which are used to extract spectral and spatial domain features, respectively. Experiments proved that jointly using the spectral and spatial information extracted by SSGNN can largely improve the performance of MM-FER when the training sample is limited. First, the influences of different neighbors and samples to the model performance was analyzed. Then, the contribution of SPAGNN and SPEGNN were evaluated. It was discovered that fusing the result of SPAGNN and SPEGNN at decision level further improved the performance of MM-FER. Experiment proved that SSGNN can recognize microexpression acquired by various sensors with higher accuracy under different image resolutions and image formats than the compared state-of-the-art methods in most cases. A cross-dataset experiment demonstrated the generalization ability of SSGNN.},
  archive      = {J_THMS},
  author       = {Junjie Zhang and Guangmin Sun and Kun Zheng and Sarah Mazhar and Xiaohui Fu and Yu Li and Hui Yu},
  doi          = {10.1109/THMS.2022.3163211},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {747-760},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {SSGNN: A macro and microfacial expression recognition graph neural network combining spatial and spectral domain features},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). WiGRUNT: WiFi-enabled gesture recognition using
dual-attention network. <em>THMS</em>, <em>52</em>(4), 736–746. (<a
href="https://doi.org/10.1109/THMS.2022.3163189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gestures constitute an important form of nonverbal communication where bodily actions are used for delivering messages alone or in parallel with spoken words. Recently, there exists an emerging trend of WiFi sensing-enabled gesture recognition due to its inherent merits like remote sensing, non-line-of-sight covering, and privacy-friendly. However, current WiFi-based approaches mainly reply on domain-specific training since they don’t know “where to look” and “when to look.” To this end, we propose WiGRUNT, a WiFi-enabled gesture recognition system using dual-attention network, to mimic how a keen human being intercepting a gesture regardless of the environment variations. The key insight is to train the network to dynamically focus on the domain-independent features of a gesture on the WiFi channel state information via a spatial-temporal dual-attention mechanism. WiGRUNT roots in a deep residual network (ResNet) backbone to evaluate the importance of spatial-temporal clues and exploit their inbuilt sequential correlations for fine-grained gesture recognition. We evaluate WiGRUNT on the open Widar3 dataset and show that it significantly outperforms its state-of-the-art rivals by achieving the best-ever performance in-domain or cross-domain.},
  archive      = {J_THMS},
  author       = {Yu Gu and Xiang Zhang and Yantong Wang and Meng Wang and Huan Yan and Yusheng Ji and Zhi Liu and Jianhua Li and Mianxiong Dong},
  doi          = {10.1109/THMS.2022.3163189},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {736-746},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {WiGRUNT: WiFi-enabled gesture recognition using dual-attention network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Virtual keyboards with real-time and robust deep
learning-based gesture recognition. <em>THMS</em>, <em>52</em>(4),
725–735. (<a href="https://doi.org/10.1109/THMS.2022.3165165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In head-mounted display devices for augmented reality and virtual reality, external signals are often entered using a virtual keyboard (VKB). Among various user interfaces for VKBs, hand gestures are widely used because they are fast and intuitive. This work proposes a gesture-recognition (GR)-based VKB algorithm that is accurate in any environment and operates in real time. Specifically, the proposed ambidextrous VKB layouts reduce the total finger travel distance on one-hand VKB layouts. Additionally, a fast typing action is proposed to use characteristics when previous and current keys are adjacent. To be robust in any environment, we utilize a deep learning (DL)-based GR method in the proposed VKB algorithm. To train DL networks, seven classes are defined and an automated dataset generation method is proposed to reduce the necessary time and effort. The proposed one-hand VKB layout with the fast typing action shows a 1.5× faster typing speed than the popular ABC keyboard layout. Furthermore, the proposed ambidextrous VKB layout brings an additional 52% improvement compared with the proposed one-hand VKB layout. The proposed DL-based GR method implemented on the well-known YOLOv3 machine learning framework shows a mean average precision rate of 95% for images including background colors similar to skin color. The proposed DL-based GR method for one-hand and ambidextrous VKBs achieves around 41 frames per second on a software platform, which allows real-time processing.},
  archive      = {J_THMS},
  author       = {Tae-Ho Lee and Sunwoong Kim and Taehyun Kim and Jin-Sung Kim and Hyuk-Jae Lee},
  doi          = {10.1109/THMS.2022.3165165},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {725-735},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Virtual keyboards with real-time and robust deep learning-based gesture recognition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). U-WeAr: User recognition on wearable devices through arm
gesture. <em>THMS</em>, <em>52</em>(4), 713–724. (<a
href="https://doi.org/10.1109/THMS.2022.3170829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of wearable devices equipped with inertial sensors has become increasingly pervasive. It has been widely demonstrated in the literature that inertial signals acquired by these sensors can be used by machine learning algorithms to predict actions performed and/or to recognize the identities of the person wearing the sensors. In this article, we present a hardware/software system for arm gesture recognition, identity recognition, and verification of a person based on inertial sensors. The hardware part is a custom wristband that consists of a computing unit, a wireless communication unit, and an inertial sensor. The software part is an algorithm based on recurrent neural networks that is able to process the signals coming from the sensor and to return a prediction. To validate the system, a dataset consisting of 25 symbols drawn with the arm is collected. These symbols are performed by 33 subjects. We conduct two evaluations: 1) performance evaluation for arm gesture recognition, user recognition and verification; and 2) usability assessment of the system. The performance of the three recognition tasks indicate that this system can be reliably applied in real environments with an accuracy above 96% for gesture recognition, an accuracy of about 85% for user identification, and an equal error rate of about 13% for user verification. The outcome of the usability test proves a great satisfaction from the users in terms of high simplicity in the use of the wristband and goodness of the machine learning predictions.},
  archive      = {J_THMS},
  author       = {Simone Bianco and Paolo Napoletano and Alberto Raimondi and Mirko Rima},
  doi          = {10.1109/THMS.2022.3170829},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {713-724},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {U-WeAr: User recognition on wearable devices through arm gesture},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ASL trigger recognition in mixed activity/signing sequences
for RF sensor-based user interfaces. <em>THMS</em>, <em>52</em>(4),
699–712. (<a href="https://doi.org/10.1109/THMS.2021.3131675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past decade has seen great advancements in speech recognition for control of interactive devices, personal assistants, and computer interfaces. However, deaf and hard-of-hearing (HoH) individuals, whose primary mode of communication is sign language, cannot use voice-controlled interfaces. Although there has been significant work in video-based sign language recognition, video is not effective in the dark and has raised privacy concerns in the deaf community when used in the context of human ambient intelligence. RF sensors have been recently proposed as a new modality that can be effective under the circumstances where video is not. This article considers the problem of recognizing a trigger sign (wake word) in the context of daily living, where gross motor activities are interwoven with signing sequences. The proposed approach exploits multiple RF data domain representations (time-frequency, range-Doppler, and range-angle) for sequential classification of mixed motion data streams. The recognition accuracy of signs with varying kinematic properties is compared and used to make recommendations on appropriate trigger sign selection for RF-sensor-based user interfaces. The proposed approach achieves a trigger sign detection rate of 98.9% and a classification accuracy of 92% for 15 ASL words and three gross motor activities.},
  archive      = {J_THMS},
  author       = {Emre Kurtoğlu and Ali C. Gurbuz and Evie A. Malaia and Darrin Griffin and Chris Crawford and Sevgi Z. Gurbuz},
  doi          = {10.1109/THMS.2021.3131675},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {699-712},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {ASL trigger recognition in mixed Activity/Signing sequences for RF sensor-based user interfaces},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sign language recognition based on r(2+1)d with
spatial–temporal–channel attention. <em>THMS</em>, <em>52</em>(4),
687–698. (<a href="https://doi.org/10.1109/THMS.2022.3144000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous work utilized three-dimensional (3-D) convolutional neural networks (CNNs) tomodel the spatial appearance and temporal evolution concurrently for sign language recognition (SLR) and exhibited impressive performance. However, there are still challenges for 3-D CNN-based methods. First, motion information plays a more significant role than spatial content in sign language. Therefore, it is still questionable whether to treat space and time equally and model them jointly by heavy 3-D convolutions in a unified approach. Second, because of the interference from the highly redundant information in sign videos, it is still nontrivial to effectively extract discriminative spatiotemporal features related to sign language. In this study, deep R(2+1)D was adopted for separate spatial and temporal modeling and demonstrated that decomposing 3-D convolution filters into independent spatial and temporal convolutions facilitates the optimization process in SLR. A lightweight spatial–temporal–channel attention module, including two submodules called channel–temporal attention and spatial–temporal attention, was proposed to make the network concentrate on the significant information along spatial, temporal, and channel dimensions by combining squeeze and excitation attention with self-attention. By embedding this module into R(2+1)D, superior or comparable results to the state-of-the-art methods on the CSL-500, Jester, and EgoGesture datasets were obtained, which demonstrated the effectiveness of the proposed method.},
  archive      = {J_THMS},
  author       = {Xiangzu Han and Fei Lu and Jianqin Yin and Guohui Tian and Jun Liu},
  doi          = {10.1109/THMS.2022.3144000},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {687-698},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Sign language recognition based on R(2+1)D with Spatial–Temporal–Channel attention},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modified spatio-temporal matched filtering for brain
responses classification. <em>THMS</em>, <em>52</em>(4), 677–686. (<a
href="https://doi.org/10.1109/THMS.2022.3168421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we apply the method of spatio-temporal filtering (STF) to electroencephalographic (EEG) data processing for brain responses classification. The method operates similarly to linear discriminant analysis (LDA) but contrary to most applied classifiers, it uses the whole recorded EEG signal as a source of information instead of the precisely selected brain responses, only. This way it avoids the limitations of LDA and improves the classification accuracy. We emphasize the significance of the STF learning phase. To preclude the negative influence of super–Gaussian artifacts on accomplishment of this phase, we apply the discrete cosine transform (DCT) based method for their rejection. Later, we estimate the noise covariance matrix using all data available, and we improve the STF template construction. The further modifications are related with the constructed filters operation and consist in the changes of the STF interpretation rules. Consequently, a new tool for evoked potentials (EPs) classification has been developed. Applied to the analysis of signals stored in a publicly available database, prepared for the assessment of modern algorithms aimed in EPs detection (in the frames of the 2019 IFMBE Scientific Challenge), it allowed to achieve the second best result, very close to the best one, and significantly better than the ones achieved by other contestants of the challenge.},
  archive      = {J_THMS},
  author       = {Marian P. Kotas and Michal Piela and Sonia H. Contreras-Ortiz},
  doi          = {10.1109/THMS.2022.3168421},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {677-686},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Modified spatio-temporal matched filtering for brain responses classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A neural-inspired architecture for EEG-based auditory
attention detection. <em>THMS</em>, <em>52</em>(4), 668–676. (<a
href="https://doi.org/10.1109/THMS.2022.3176212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans have the ability to focus on one of the sound sources in a noisy scene, which is critical for everyday communication. Auditory attention detection (AAD) seeks to detect selective attention from one’s brain signals. For AAD to be useful in brain–computer interface applications, new approaches with low computational cost, high classification performance, and low latency are required to be developed. In this study, we proposed a novel neural-inspired architecture to mimic the neural computation and coding strategy in the brain for electroencephalography-based AAD. We validated our model through data visualization, and conducted experiments on two publicly available databases. For both KUL and DTU databases, it outperforms both linear and convolutional neural network (CNN) models with consistent improvements from 1 s to 5 s decision windows in terms of detection accuracy. Although the accuracy of the proposed neural-inspired model is inferior to the state-of-the-art spatio-spectral feature (SSF)-CNN model, the computational cost of our model is less than 1% of SSF-CNN’s. Moreover, the neural-inspired decoder is more hardware friendly and energy-efficient due to its biological computing scheme. Overall, the proposed neural-inspired architecture realizes a fast, accurate, and low energy expenditure AAD, which is a big step forward towards practical neuro-steered hearing aids.},
  archive      = {J_THMS},
  author       = {Siqi Cai and Peiwen Li and Enze Su and Qi Liu and Longhan Xie},
  doi          = {10.1109/THMS.2022.3176212},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {668-676},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A neural-inspired architecture for EEG-based auditory attention detection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cognitive workload impacts of simulated visibility changes
during search and surveillance tasks quantified by functional near
infrared spectroscopy. <em>THMS</em>, <em>52</em>(4), 658–667. (<a
href="https://doi.org/10.1109/THMS.2022.3155368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional near infrared spectroscopy (fNIRS) enables investigation of hemodynamics and cortical activation in lab and field settings. Studies utilizing fNIRS in realistic complex environments are limited, and studies with real-time applications often lack the accompaniment of realistic settings or scenarios. Investigations of neuronal activation focused on improving human performance during complex and cognitively taxing tasks, like those experienced by unmanned aerial systems sensor operators, have yielded a knowledge gap regarding studies with ecological validity, and distinction between condition difficulties. This gap resulted from limitations in experimental design to account for realistic experiences within dynamic environments, missing fundamental transitional load details of realistic cognitive tasks. This article sought to evaluate cognitive workload in conjunction with behavioral task performance in novice sensor operators. During the experimental protocol, operators engaged with complex tasks under distinct task load conditions, implemented using a high-fidelity simulator and changes in time of day. Linear mixed effects models and multiple contrast post hoc results confirmed that simulated time-of-day visibility changes effectively modulated task load, with distinctions between easy, medium and hard conditions. Using a visibility through changes in time of day paradigm is an effective method for mimicking real-life conditions of sensor operators; however, disengagement may occur if subjects are not adequately trained.},
  archive      = {J_THMS},
  author       = {Jaime Kerr and Pratusha Reddy and Patricia A. Shewokis and Kurtulus Izzetoglu},
  doi          = {10.1109/THMS.2022.3155368},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {658-667},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Cognitive workload impacts of simulated visibility changes during search and surveillance tasks quantified by functional near infrared spectroscopy},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incorporating EEG and EMG patterns to evaluate BCI-based
long-term motor training. <em>THMS</em>, <em>52</em>(4), 648–657. (<a
href="https://doi.org/10.1109/THMS.2022.3168425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-computer interfaces (BCIs) provide users with a direct communication pathway between the brain and the peripheral environment. BCI-controlled devices have the potential to assist disabled patients in regaining motor functions. However, it remains unclear what happens to the functional coupling between the brain and muscle after BCI-based long-term motor training. Therefore, we developed a neurofeedback training method for long-term motor training that combines visual scenes and electrical stimulation. During the experiment, we collected electroencephalography (EEG) and electromyography (EMG) data from 20 subjects to explore their neurophysiological responses and the EEG-EMG coupling relationship. Event-related desynchronization (ERD), root mean square (rms) analysis, transfer entropy (TE) patterns, and other techniques were used to evaluate the cortical muscle response. Compared with the initial states, the ERD and rms significantly improved after long-term motor training. However, there was no significant difference in BCI performance. Directional TE values revealed the cortical muscle mechanism. These results demonstrate that incorporating EEG and EMG patterns to evaluate and establish a BCI-based motor training method is feasible. Furthermore, this article could provide evidence for functional coupling mechanisms for cortical muscles and motor rehabilitation.},
  archive      = {J_THMS},
  author       = {Zhongpeng Wang and Beibei He and Yijie Zhou and Long Chen and Bin Gu and Shuang Liu and Minpeng Xu and Feng He and Dong Ming},
  doi          = {10.1109/THMS.2022.3168425},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {648-657},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Incorporating EEG and EMG patterns to evaluate BCI-based long-term motor training},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiobjective control design for human–machine systems with
safety performance constraints. <em>THMS</em>, <em>52</em>(4), 636–647.
(<a href="https://doi.org/10.1109/THMS.2021.3116123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the problem of multiobjective control design for a class of human–machine systems (HMSs) with safety performance constraints containing state constraints and input constraints. The HMSs under consideration not only monitor the human but also need to take proper actions to both the human and machine. A model of controlled hidden Markov jump system (CHMJS) is applied to represent the HMSs. Based on the CHMJS model, a sufficient condition for the practical mean square stability of the unconstrained HMSs is first derived using a stochastic Lyapunov functional. A sufficient condition for ensuring the safety performance constraints of the HMSs is also deduced by employing reachability analysis and set invariance theory. Subsequently, a bilinear matrix inequality-based control design is presented to guarantee both the practical mean square stability and safety performance constraints of the HMSs. A multiobjective optimization problem (MOP) is then formulated to determine a feedback controller for the human and a human-assistance controller for the machine such that both the practical mean square stability and safety performance constraints as well as the less human intervention can be satisfied. An algorithm that mixes the multiobjective particle swarm optimization and linear matrix inequality technique is developed to solve this MOP. Finally, a lane departure example is given to illustrate its effectiveness.},
  archive      = {J_THMS},
  author       = {Xiu-Mei Zhang and Huai-Ning Wu},
  doi          = {10.1109/THMS.2021.3116123},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {636-647},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Multiobjective control design for Human–Machine systems with safety performance constraints},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Influence of mobile robots on human safety perception and
system productivity in wholesale and retail trade environments: A pilot
study. <em>THMS</em>, <em>52</em>(4), 624–635. (<a
href="https://doi.org/10.1109/THMS.2021.3134553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring human safety has been one of the most critical considerations within the field of human–robot interaction. To explore the effects of working with autonomous robots on human coworkers’ perceived workload and job performance, two experiments were conducted in this study. Eight participants were recruited in the first experiment. Results revealed an increase in both subjective and objective workload measurements: compared to the baseline “no robot” to “empty payload” condition, the sum of NASA Task Load Index (NASA-TLX) scores increased from 129.3 (58.8) to 147.6 (53.7) ( $p$ -value = 0.041) and the pupil diameter increased from 4.07 (0.64) mm to 4.11 (0.67) mm; while working with a full payload robot, the sum of NASA-TLX scores increased to 151.7 (55.9) ( $p$ -value = 0.010) and the pupil diameter increased to 4.12 (0.66) mm. Increased task completion time (3.1% for empty payload condition and 6.6% for full payload condition) showed a decrease in human productivity. However, this slight human output reduction was compensated by the substantial gain from the robot. Similar effects of autonomous mobile robots on participants’ NASA-TLX scores and task completion time were observed in the second experiment, where eight participants performed the order picking and sorting tasks in a high-fidelity grocery store setting. Results from the study suggested the feasibility of applying fully autonomous mobile robots in Wholesale and Retail Trade settings to improve human–robot team productivity while prioritizing physical safety and reasonable increases in mental workload.},
  archive      = {J_THMS},
  author       = {Yuhao Chen and Chizhao Yang and Yu Gu and Boyi Hu},
  doi          = {10.1109/THMS.2021.3134553},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {624-635},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Influence of mobile robots on human safety perception and system productivity in wholesale and retail trade environments: A pilot study},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluating cybersickness of walking on an omnidirectional
treadmill in virtual reality. <em>THMS</em>, <em>52</em>(4), 613–623.
(<a href="https://doi.org/10.1109/THMS.2022.3175407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybersickness is a type of motion sickness that may occur during a virtual reality (VR) experience. Many studies have proposed solutions to mitigate cybersickness, while navigating a virtual environment with controllers or walking over a floor. However, reducing the levels of cybersickness while physically walking on an omnidirectional treadmill has been largely overlooked. In this article, we performed a within-subject study, where 34 novice participants underwent four visual conditions while walking in a virtual maze over an omnidirectional treadmill. In the control condition, the movement speed was reduced of the half compared to a standard navigation speed, a movement speed smoothing was added, and the user’s virtual body was represented. The other three conditions changed one of the visual parameters of the control condition: in the standard speed condition, the speed reduction was not performed; for the no smoothing condition, the smoothing was not performed; and for the no avatar condition, the user’s avatar was removed. Results showed that the standard speed condition was reported to induce a significant level of cybersickness compared to the control and no avatar conditions. Nevertheless, standard speed was also the condition most preferred to navigate a virtual environment. This suggests the need to find a tradeoff between the easiness to move quickly in a virtual environment and the cybersickness that can be induced. We provide a discussion of the obtained results and their implications for the design of VR experiences while users walk upon an omnidirectional treadmill.},
  archive      = {J_THMS},
  author       = {Jesse Lohman and Luca Turchet},
  doi          = {10.1109/THMS.2022.3175407},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {613-623},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Evaluating cybersickness of walking on an omnidirectional treadmill in virtual reality},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Talking cars, doubtful users—a population study in virtual
reality. <em>THMS</em>, <em>52</em>(4), 602–612. (<a
href="https://doi.org/10.1109/THMS.2022.3168437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous vehicles represent a significant development in our society, and their acceptance will largely depend on trust. This study investigates strategies to increase trust and acceptance by making the cars’ decisions. For this purpose, we created a virtual reality (VR) experiment with a self-explaining autonomous car, providing participants with verbal cues about crucial traffic decisions. First, we investigated attitudes toward self-driving cars among 7850 participants using a simplified version of the Technology Acceptance Model (TAM) questionnaire. Results revealed that female participants are less accepting than male participants, and that there is a general decline among all genders. Otherwise in general, a self-explaining car has a positive impact on trust and perceived usefulness. Surprisingly, it adversely affected the intention to use and perceived ease of use. This entails dissociation of trust from the other items of the questionnaire. Second, we analyzed behavioral of 26 750 participants to investigate the effect of self-explaining systems on head movements during the VR drive. We observed significant differences in head movements during the critical events and the baseline periods of the drive between the three driving conditions. Additionally, we demonstrated positive correlations between head movement parameters and the TAM scores, where trust showed the lowest correlation. This provides further evidence of the dissociation of trust from other TAM items. These findings illustrate the benefits of combining subjective questionnaire data with objective behavioral data. Overall, the outcomes indicate a partial dissociation of self-reported trust from intention to use and objective behavioral data.},
  archive      = {J_THMS},
  author       = {Shadi Derakhshan and Farbod Nosrat Nezami and Maximilian Alexander Wächter and Artur Czeszumski and Ashima Keshava and Hristofor Lukanov and Marc Vidal De Palol and Gordon Pipa and Peter König},
  doi          = {10.1109/THMS.2022.3168437},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {602-612},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Talking cars, doubtful Users—A population study in virtual reality},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A virtual-reality-based training and assessment system for
bridge inspectors with an assistant drone. <em>THMS</em>,
<em>52</em>(4), 591–601. (<a
href="https://doi.org/10.1109/THMS.2022.3155373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over 600 000 bridges in the U.S. must be inspected every two years to identify flaws, defects, or potential problems that may need follow-up maintenance. Bridge inspection has adopted unmanned aerial vehicles (or drones) for improving safety, efficiency, and cost-effectiveness. Although drones can operate in an autonomous mode, keeping inspectors in the loop is critical for complex tasks in bridge inspection. Therefore, inspectors need to develop the skill and confidence to operate drones in their jobs. This article presents the design and development of a virtual-reality-based training and assessment system for inspectors assisted by a drone in bridge inspection. The system is composed of four integrated modules: a simulated bridge inspection developed in Unity, an interface that allows a trainee to operate the drone in simulation using a remote controller, data monitoring and analysis to provide real-time in-task feedback to trainees to assist their learning, and a post-study assessment supporting personalized training. This article also conducts a proof-of-concept pilot study to illustrate the functionality of this system. The study demonstrated that the training and assessment system for bridge inspection with an assistant drone, as a tool for the early-stage training, can objectively identify the training needs of individuals in detail and, further, help them develop the skill and confidence in collaborating with a drone in bridge inspection. The system has built a modeling and analysis platform for exploring advanced solutions to the human–drone cooperative inspection of civil infrastructure.},
  archive      = {J_THMS},
  author       = {Yu Li and Muhammad Monjurul Karim and Ruwen Qin},
  doi          = {10.1109/THMS.2022.3155373},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {591-601},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A virtual-reality-based training and assessment system for bridge inspectors with an assistant drone},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design eye-tracking augmented reality headset to reduce
cognitive load in repetitive parcel scanning task. <em>THMS</em>,
<em>52</em>(4), 578–590. (<a
href="https://doi.org/10.1109/THMS.2022.3179954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Repetitive tasks widely exist in applied fields of human-computer interaction. One underestimated example is parcel scanning, which has consistent operation difficulty but comprises multiple processes (e.g., label seeking and scanning, result confirming, and parcel relocating), involving respective cognitive requirements. Many devices are developed to facilitate repetitive operations, but few are to reduce fluctuating cognitive load throughout task processes. We present the eye-tracking augmented reality headset that integrates foveated vision detection and smooth pursuit of eye tracking and investigate how it can reduce cognitive load in the repetitive task. In total, 33 participants completed a set of parcel scanning tasks with the headset and their visual and cognitive performance were assessed. The results show that the headset maintained high scanning efficiency and lower cognitive load across the tasks with varying difficulties and it significantly reduced the participants’ cognitive load during the processes of barcode seeking and scanning and result confirmation. The headset demonstrated good usability and ease of use. Implications for how the case study result could be used in generalizing applications are discussed.},
  archive      = {J_THMS},
  author       = {Zihan Yan and Yufei Wu and Yiyang Li and Yifei Shan and Xiangdong Li and Preben Hansen},
  doi          = {10.1109/THMS.2022.3179954},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {578-590},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Design eye-tracking augmented reality headset to reduce cognitive load in repetitive parcel scanning task},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Projected augmented reality to guide manual precision tasks:
An alternative to head mounted displays. <em>THMS</em>, <em>52</em>(4),
567–577. (<a href="https://doi.org/10.1109/THMS.2021.3129715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented reality (AR) devices are gaining popularity in industrial development and healthcare as they provide information that would not be accessible in a rapid and intuitive way. Head-mounted displays dominate in this field and are currently being comprehensively tested. Alongside its informative function, AR can be used to steer the user&#39;s actions to aid complex or high precision tasks. This is the case in surgery, which is recently seeing the development of ad-hoc head-mounted displays to meet the requirements of safety, ergonomics, and reliability. However, head-mounted displays are subject to perceptual problems that can affect their use in delicate and demanding tasks. This article aims to evaluate projected AR as an alternative to head mounted displays (HMDs) when accurate guidance on the surface is needed. We directly compare them in a user study and evaluate both user accuracy and user perception to assess whether projected AR can be a practical and useful paradigm for precision manual tasks. Ten users performed tracing trajectory tasks under the guidance of an HMD and a projected AR device. Three accuracy levels were quantitatively tested: 0.5, 1, and 2 mm. Statistical analysis showed no significant difference in the accuracy of the two AR visualization modes, whereas the user perception assessment revealed statistical differences in virtual-to-real perception and visual discomfort. The quantitative results of this article proved that both technologies can guide manual precision tasks with the same accuracy, but projected AR features some perceptual advantages.},
  archive      = {J_THMS},
  author       = {Virginia Mamone and Fabrizio Cutolo and Sara Condino and Vincenzo Ferrari},
  doi          = {10.1109/THMS.2021.3129715},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {567-577},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Projected augmented reality to guide manual precision tasks: An alternative to head mounted displays},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Driver-pedestrian perceptual models demonstrate coupling:
Implications for vehicle automation. <em>THMS</em>, <em>52</em>(4),
557–566. (<a href="https://doi.org/10.1109/THMS.2022.3158201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing vehicle automation that accommodates other road users and exhibits familiar behaviors may enhance traffic safety, efficiency, and fairness, leading to tolerance of the technology. However, the interdependence between vehicle automation and other road users makes them more challenging than typical control and path planning tasks. Through the lens of joint activity theory, we model driver and pedestrian behavior to explore how they balance and negotiate competing risk and velocity goals through movement. Joint activity theory informs an interpretation of these movements as signals, which can be associated with perceptual processes. We use simulation-based inference to estimate parameters of coupled driver and pedestrian perceptual models using naturalistic driving data. Perceptual models provide links between the processes guiding evaluation of risk and velocity maintenance, and how they govern driver acceleration and pedestrian walking. We found that the coupled simulations describe how drivers adjust their yielding behavior in the face of pedestrian risk, and how risk affects pedestrians’ decisions to cross. Dynamic risk and velocity parameters predicted safety, efficiency, and fairness outcomes, suggesting that the parameters and their dynamic perceptual models describe important components of the interactions. Traditional approaches employ static, summary predictors, which may fail to capture their continuous evolution and negotiation over time. Dynamic models of the interaction between drivers and pedestrians can inform vehicle automation by identifying deviations from communication norms, extracting interaction features, and evaluating communication and coordination.},
  archive      = {J_THMS},
  author       = {Joshua E. Domeyer and John D. Lee and Heishiro Toyoda and Bruce Mehler and Bryan Reimer},
  doi          = {10.1109/THMS.2022.3158201},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {557-566},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Driver-pedestrian perceptual models demonstrate coupling: Implications for vehicle automation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Obstacle avoidance in highly automated cars: Can progressive
haptic shared control make it safer and smoother? <em>THMS</em>,
<em>52</em>(4), 547–556. (<a
href="https://doi.org/10.1109/THMS.2022.3155370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haptic shared control has proven to be an effective method to assist a driver in controlling a vehicle. This method is now being considered for use in developing strategies for smooth transitions between manual and autonomous driving modes. This article has two objectives. First, it proposes to adapt an existing haptic shared control strategy to achieve transitions between manual and autonomous modes and to evaluate this approach with real drivers on a driving simulator. Second, it proposes to evaluate four different transition profiles in an obstacle-avoidance context. The first profile is a gradual transition from the autonomous mode to shared control mode, followed by another transition from the shared control mode to autonomous mode once the obstacle is passed. The second is a gradual transition from autonomous mode to manual mode. The third is a binary transition from autonomous mode to manual mode. Finally, in the fourth condition, the driver overrides the autonomous mode. These transition profiles were evaluated in curves and straight lines on a driving simulator. The results first validated the use of the haptic shared control strategy to execute transitions between manual and autonomous modes. The distribution of the torques delivered by the automation system and the driver corresponded to the progression of the expected sharing level. Second, the gradual transitions showed advantages over binary transitions and the override of the autonomous mode, both in terms of steering performance and subjective evaluation.},
  archive      = {J_THMS},
  author       = {Béatrice Pano and Philippe Chevrel and Fabien Claveau and Chouki Sentouh and Franck Mars},
  doi          = {10.1109/THMS.2022.3155370},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {547-556},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Obstacle avoidance in highly automated cars: Can progressive haptic shared control make it safer and smoother?},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Activity and stress estimation based on OpenPose and
electrocardiogram for user-focused level-4-vehicles. <em>THMS</em>,
<em>52</em>(4), 538–546. (<a
href="https://doi.org/10.1109/THMS.2022.3155375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing vehicle automation changes the role of humans in the car, which imposes new requirements on the design of in-vehicle software and hardware for flexible interior concepts. An option to meet these requirements is the development of user-focused automation based on combined user and context monitoring in real time. The system behavior may be dynamically adapted by adjusting the driving style or the interior lighting. Here, we present a hierarchical approach on the basis of semantically motivated low-level features for activity and stress recognition based on OpenPose and electrocardiogram data. A driving simulator study with 29 participants was conducted to determine the potential of the approach. Participants had to accomplish different tasks: manual driving (MD); mobile office work with varying task load levels (high task load: MO-HT, low task load: MO-LT); and relaxing (REL) during automated driving. The validation revealed that our model is able to correctly distinguish between different activities using only a set of primitive features (average precision: driving: 76% and mobile office work: 93%, relaxing: 86%). Furthermore, we evaluated a person-independent and a person-specific approach for stress detection and found that both strategies show similar trends in accordance with our predictions (person-independent: stress detected in MO-HT: 22%, MO-LT: 18%, MD: 18%, REL: 15%; person-specific: stress detected in MO-HT: 79%, MO-LT: 72%, MD: 65%, and REL: 50%). These results demonstrate the efficacy of using a lightweight semantic approach for activity recognition and stress detection as basis for user-focused vehicle automation.},
  archive      = {J_THMS},
  author       = {Fabian Walocha and Uwe Drewitz and Klas Ihme},
  doi          = {10.1109/THMS.2022.3155375},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {538-546},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Activity and stress estimation based on OpenPose and electrocardiogram for user-focused level-4-vehicles},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). IEEE systems, man, and cybernetics society. <em>THMS</em>,
<em>52</em>(3), C3. (<a
href="https://doi.org/10.1109/THMS.2022.3172583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2022.3172583},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {C3},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE systems, man, and cybernetics society},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). IEEE systems, man, and cybernetics society. <em>THMS</em>,
<em>52</em>(3), C2. (<a
href="https://doi.org/10.1109/THMS.2022.3172581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2022.3172581},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {C2},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE systems, man, and cybernetics society},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Theory and design considerations for the user experience of
smart environments. <em>THMS</em>, <em>52</em>(3), 522–535. (<a
href="https://doi.org/10.1109/THMS.2022.3142112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the infusion of computation into workplaces and homes, various service settings, and everyday objects, scholars in human–computer interaction (HCI) and related domains have begun to consider the research and design implications not only of smart “things,” but of smart environments . Much of the work on smart environments to date has focused on smart homes; related work in HCI explores user values for smart homes, means of interacting with computation in smart homes (e.g., interfaces and agents), how to balance the needs of multiple stakeholders, and how to preserve user trust and autonomy. However, the smart environments of the future will not always fit the smart home mold of a coalescence of products that exist to automate and ease everyday tasks for the end users. They will be both user-focused and goal-focused, public and private, large and small, and ephemeral and long-lasting. It will benefit the field to look at smart environments as a unit of analysis—including what these different types of environments have in common and what they do not—from a systemic, user experience design-oriented view. In this survey article, we review prior research on smart environments and various related bodies of literature. Informed by our literature review, we articulate five lenses that distinguish different types of smart environments from one another. We then propose research directions for future work on this topic.},
  archive      = {J_THMS},
  author       = {Samantha Reig and Terrence Fong and Jodi Forlizzi and Aaron Steinfeld},
  doi          = {10.1109/THMS.2022.3142112},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {522-535},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Theory and design considerations for the user experience of smart environments},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Is my siri the same as your siri? An exploration of users’
mental model of virtual personal assistants, implications for trust.
<em>THMS</em>, <em>52</em>(3), 512–521. (<a
href="https://doi.org/10.1109/THMS.2021.3107493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual personal assistants (VPAs) are becoming so widely available that considerations are being made as to whether to begin including them in self-driving vehicles. While research has been done exploring human interactions with single VPAs, there has been a little work exploring human interactions and human mental models with interconnected systems. As companies like Amazon consider whether to integrate Alexa in their self-driving car, research needs to be done to explore whether individual&#39;s mental model of these systems is of a single system or if every embodiment of the VPA (e.g., echo) represents a different VPA. Knowing this will allow researchers and practitioners to apply existing models of trust, and predict whether high trust in the Siri that exists in an iPhone will carry over into high (and potentially miscalibrated) trust in Apple&#39;s Siri-directed self-driving vehicle. Results indicate that there is not one consistent mental model that users have, and provides the framework for greater exploration into individual differences and the determinants that affect users’ mental model.},
  archive      = {J_THMS},
  author       = {Nathan L. Tenhundfeld and Hannah M. Barr and Emily H. O&#39;Hear and Kristin Weger},
  doi          = {10.1109/THMS.2021.3107493},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {512-521},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Is my siri the same as your siri? an exploration of users’ mental model of virtual personal assistants, implications for trust},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An investigation of drivers’ dynamic situational trust in
conditionally automated driving. <em>THMS</em>, <em>52</em>(3), 501–511.
(<a href="https://doi.org/10.1109/THMS.2021.3131676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding how trust is built over time is essential, as trust plays an important role in the acceptance and adoption of automated vehicles (AVs). This study aims to investigate the effects of system performance and participants’ trust preconditions on dynamic situational trust during takeover transitions. We evaluate the dynamic situational trust of 42 participants using both self-reported and behavioral measures while watching 30 videos with takeover scenarios. The study is a 3 by 2 mixed-subjects design, where the within-subjects variable is the system performance (i.e., accuracy levels of 95%, 80%, and 70%) and the between-subjects variable is the preconditions of the participants’ trust (i.e., overtrust and undertrust). Our results showed that participants quickly adjusted their self-reported situational trust levels, which were consistent with different accuracy levels of system performance in both trust preconditions. However, participants’ behavioral situational trust was affected by their trust preconditions across different accuracy levels. For instance, the overtrust precondition significantly increased the agreement fraction compared to the undertrust precondition. The undertrust precondition significantly decreased the switch fraction compared to the overtrust precondition. These results have important implications for designing an invehicle trust calibration system for conditional AVs.},
  archive      = {J_THMS},
  author       = {Jackie Ayoub and Lilit Avetisyan and Mustapha Makki and Feng Zhou},
  doi          = {10.1109/THMS.2021.3131676},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {501-511},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {An investigation of drivers’ dynamic situational trust in conditionally automated driving},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Factors affecting pedestrians’ trust in automated vehicles:
Literature review and theoretical model. <em>THMS</em>, <em>52</em>(3),
490–500. (<a href="https://doi.org/10.1109/THMS.2021.3112956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated vehicles (AVs) are one critical application area of artificial intelligence (AI). However, a lack of appropriate trust can be a major barrier to successfully introducing AVs into the market. The objective of this study is to summarize and synthesize the existing literature to gain a greater understanding of factors that will potentially influence the development of pedestrians’ trust in AVs over time. Since AVs will become part of a larger infrastructure system that influences more than just AV users, they should also be accepted by pedestrians and other road users. There is a need for pedestrians to form appropriate levels of trust toward AVs to achieve safe interaction with such vehicles in circumstances characterized by uncertainty and vulnerability. Consequently, factors relevant to the building of this appropriate trust must be understood. By integrating the reviewed empirical studies and related theories, a theoretical model has been proposed and developed, comprising three layers of variability in pedestrian-AV trust (dispositional trust, situational trust, and learned trust). Given that this is an emerging field of research, much still remains unknown, and this review identifies several gaps in current knowledge for each layer of trust, as well as providing suggestions for consideration in future studies. Additionally, the proposed model of pedestrian-AV trust can be useful to transportation researchers, practitioners, designers, and AV manufacturers for designing AVs and related transportation systems for the purposes of successfully integrating AVs into society, and calibrating pedestrians’ trust to the appropriate level.},
  archive      = {J_THMS},
  author       = {Siyuan Zhou and Xu Sun and Bingjian Liu and Gary Burnett},
  doi          = {10.1109/THMS.2021.3112956},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {490-500},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Factors affecting pedestrians’ trust in automated vehicles: Literature review and theoretical model},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dissociation between users’ explicit and implicit attitudes
toward artificial intelligence: An experimental study. <em>THMS</em>,
<em>52</em>(3), 481–489. (<a
href="https://doi.org/10.1109/THMS.2021.3125280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The latest developments in the field of artificial intelligence (AI) have given rise to many ethical and socio-economic concerns. Nonetheless, the impact of AI technologies is evident and tangible in our everyday life. This dichotomy leads to mixed feelings toward AI: people recognize the positive impact of AI, but they also show concerns, especially about their privacy and security. In this article, we try to understand whether the implicit and explicit attitudes toward AI are coherent. We investigated explicit and implicit attitudes toward AI by combining a self-report measure and an implicit measure, i.e., the implicit association test. We analyzed the explicit and implicit responses of 829 participants. Results revealed that while most of the participants explicitly express a positive attitude toward AI, their implicit responses seem to point in the opposite direction. Results also show that, in both the explicit and implicit measures, females show a more negative attitude than males, and people who work in the field of AI are inclined to be positive toward AI.},
  archive      = {J_THMS},
  author       = {Valentina Fietta and Francesca Zecchinato and Brigida Di Stasi and Mirko Polato and Merylin Monaro},
  doi          = {10.1109/THMS.2021.3125280},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {481-489},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Dissociation between users’ explicit and implicit attitudes toward artificial intelligence: An experimental study},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey of hybrid human-artificial intelligence for social
computing. <em>THMS</em>, <em>52</em>(3), 468–480. (<a
href="https://doi.org/10.1109/THMS.2021.3131683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the convergence of modern computing technology and social sciences, both theoretical research and practical applications of social computing have been extended to new domains. In particular, social computing was significantly influenced by the recent advances of artificial intelligence (AI). However, the conventional technologies of AI have various drawbacks in dealing with complicated and dynamic problems. Such deficiency can be rectified by hybrid human-artificial intelligence (H-AI), which integrates both human intelligence and AI into one unity, forming a new enhanced intelligence. H-AI in dealing with social problems shows some advantages over the conventional AI. This article firstly reviews the latest research progresses of AI in social computing. Secondly, it summarizes typical challenges AI faces in social computing, which motivate the necessity to introduce H-AI to tackle social-oriented problems. Finally, we discuss the concept of H-AI and propose a holistic architecture of H-AI in social computing, which consists of three layers: object layer, intelligent processing layer, and application layer. The proposed architecture shows that H-AI has significant advantages over AI in solving social problems.},
  archive      = {J_THMS},
  author       = {Wenxi Wang and Huansheng Ning and Feifei Shi and Sahraoui Dhelim and Weishan Zhang and Liming Chen},
  doi          = {10.1109/THMS.2021.3131683},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {468-480},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A survey of hybrid human-artificial intelligence for social computing},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intuitiveness in active teaching. <em>THMS</em>,
<em>52</em>(3), 458–467. (<a
href="https://doi.org/10.1109/THMS.2021.3121666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While machine learning (ML) gives rise to astonishing results in automated systems, it is usually at the cost of large data requirements. This makes many successful algorithms from ML unsuitable for human-machine interaction, where the machine must learn from a small number of training samples that can be provided by a user within a reasonable time frame. Fortunately, the user can tailor the training data they create to be as useful as possible, severely limiting its necessary size—as long as they know about the machine’s requirements and limitations. Of course, acquiring this knowledge can in turn be cumbersome and costly. This raises the question of how easy ML algorithms are to interact with. In this work, we address this issue by analyzing the intuitiveness of certain algorithms when they are actively taught by users. After developing a theoretical framework of intuitiveness as a property of algorithms, we introduce an active teaching paradigm involving a prototypical two-dimensional spatial learning task as a method to judge the efficacy of human-machine interactions. Finally, we present and discuss the results of a large-scale user study into the performance and teaching strategies of 800 users interacting with two prominent ML algorithms in our system, providing first evidence for the role of intuition as an important factor impacting human-machine interaction.},
  archive      = {J_THMS},
  author       = {Jan Philip Göpfert and Ulrike Kuhl and Lukas Hindemith and Heiko Wersing and Barbara Hammer},
  doi          = {10.1109/THMS.2021.3121666},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {458-467},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Intuitiveness in active teaching},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A tool to facilitate the cross-cultural design process using
deep learning. <em>THMS</em>, <em>52</em>(3), 445–457. (<a
href="https://doi.org/10.1109/THMS.2021.3126699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-cultural design requires designers to understand other foreign cultures, selecting suitable cultural elements, and finally incorporate them into product design. Traditionally, this process is time-consuming and relies to a significant extent on designers’ cultural awareness and design skills. This article proposes a new tool for designers to select and integrate cultural elements in the cross-cultural design process. The proposed approach utilizes state-of-the-art deep learning techniques, which begins by automatically selecting the most suitable style image from all cultural image candidates. Then, the deep-learning-based style transfer technique is introduced to automatically produce a design image that has the same content as the uploaded design content image, and also has the cultural style of the selected style image. To the best of our knowledge, this is the first work that extends deep learning techniques to facilitate cross-cultural design. The tool received positive feedback in a usability evaluation. The empirical results show that our approach can effectively increase designers’ cultural awareness in respect of four cultural element dimensions (color, material, pattern and form). It is an innovative and efficient tool to help designers with idea generation and fast prototyping, although some participants argued that the tool would only assist designers, rather than replace humans.},
  archive      = {J_THMS},
  author       = {Leijing Zhou and Xu Sun and Guannan Mu and Jiayi Wu and Jiangping Zhou and Qiuning Wu and Yaorun Zhang and Yufan Xi and N.D. Güneş and Siyang Song},
  doi          = {10.1109/THMS.2021.3126699},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {445-457},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A tool to facilitate the cross-cultural design process using deep learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-stream sequence learning framework for human
interaction recognition. <em>THMS</em>, <em>52</em>(3), 435–444. (<a
href="https://doi.org/10.1109/THMS.2021.3138708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human interaction recognition (HIR) is challenging due to multiple humans’ involvement and their mutual interaction in a single frame, generated from their movements. Mainstream literature is based on three-dimensional (3-D) convolutional neural networks (CNNs), processing only visual frames, where human joints data play a vital role in accurate interaction recognition. Therefore, this article proposes a multistream network for HIR that intelligently learns from skeletons’ key points and spatiotemporal visual representations. The first stream localises the joints of the human body using a pose estimation model and transmits them to a 1-D CNN and bidirectional long short-term memory to efficiently extract the features of the dynamic movements of each human skeleton. The second stream feeds the series of visual frames to a 3-D convolutional neural network to extract the discriminative spatiotemporal features. Finally, the outputs of both streams are integrated via fully connected layers that precisely classify the ongoing interactions between humans. To validate the performance of the proposed network, we conducted a comprehensive set of experiments on two benchmark datasets, UT-interaction and TV human interaction, and found 1.15% and 10.0% improvement in the accuracy.},
  archive      = {J_THMS},
  author       = {Umair Haroon and Amin Ullah and Tanveer Hussain and Waseem Ullah and Muhammad Sajjad and Khan Muhammad and Mi Young Lee and Sung Wook Baik},
  doi          = {10.1109/THMS.2021.3138708},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {435-444},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A multi-stream sequence learning framework for human interaction recognition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simple but effective: Upper-body geometric features for
traffic command gesture recognition. <em>THMS</em>, <em>52</em>(3),
423–434. (<a href="https://doi.org/10.1109/THMS.2021.3121649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing traffic command gestures with high accuracy and quick response at a low computational cost is a requisite for driver assistance or autonomous driving. However, it has been understudied for a long time. Existing research takes advantage of increasing development in human action recognition but pays little attention to onboard conditions. In this article, we propose a simple but effective recognition model based on human upper-body geometric features and a long short-term memory (LSTM) network. The handcrafted geometric features can easily be calculated with estimated 2-D human keypoints at a low computational cost but are discriminative and sufficient in classification. Offline and online inferences are implemented to comprehensively evaluate the proposed model. For the sake of robustness required in the automotive domain, dual voting is designed to filter the output in online inference. On the recently published Chinese traffic police gesture (CTPG) dataset, the presented approach is the best with a remarkable improvement of approximately 8% compared to previous LSTM-based methods with handcrafted spatial features and is competitive with advanced GCN-based deep learning methods. The tradeoff pattern is explored to demonstrate how accuracy and response time alter with different training and inference strategies so that a balanced setup can be manually chosen under various application scenarios. Field tests are also carried out with an experimental vehicle, and the results uncover the present gap between research and practical application to some extent, moving a step closer to real-life traffic command gesture recognition.},
  archive      = {J_THMS},
  author       = {Sijia Wang and Kun Jiang and Junjie Chen and Mengmeng Yang and Zheng Fu and Diange Yang},
  doi          = {10.1109/THMS.2021.3121649},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {423-434},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Simple but effective: Upper-body geometric features for traffic command gesture recognition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shared control in robot teleoperation with improved
potential fields. <em>THMS</em>, <em>52</em>(3), 410–422. (<a
href="https://doi.org/10.1109/THMS.2022.3155716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In shared control teleoperation, the robot assists the user in accomplishing the desired task. Rather than simply executing the user’s command, the robot attempts to integrate it with information from the environment, such as obstacle and/or goal locations, and it modifies its behavior accordingly. In this article, we propose a real-time shared control teleoperation framework based on an artificial potential field approach improved by the dynamic generation of escape points around the obstacles. These escape points are virtual attractive points in the potential field that the robot can follow to overcome the obstacles more easily. The selection of which escape point to follow is done in real time by solving a soft-constrained problem optimizing the reaching of the most probable goal, estimated from the user’s action. Our proposal has been extensively compared with two state-of-the-art approaches in a static cluttered environment and a dynamic setup with randomly moving objects. Experimental results showed the efficacy of our method in terms of quantitative and qualitative metrics. For example, it significantly decreases the time to complete the tasks and the user’s intervention, and it helps reduce the failure rate. Moreover, we received positive feedback from the users that tested our proposal. Finally, the proposed framework is compatible with both mobile and manipulator robots.},
  archive      = {J_THMS},
  author       = {Alberto Gottardi and Stefano Tortora and Elisa Tosello and Emanuele Menegatti},
  doi          = {10.1109/THMS.2022.3155716},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {410-422},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Shared control in robot teleoperation with improved potential fields},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shared intelligence for robot teleoperation via BMI.
<em>THMS</em>, <em>52</em>(3), 400–409. (<a
href="https://doi.org/10.1109/THMS.2021.3137035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel shared intelligence system for brain-machine interface (BMI) teleoperated mobile robots where user’s intention and robot’s intelligence are concurrent elements equally participating in the decision process. We designed the system to rely on policies guiding the robot’s behavior according to the current situation. We hypothesized that the fusion of these policies would lead to the identification of the next, most probable, location of the robot in accordance with the user’s expectations. We asked 13 healthy subjects to evaluate the system during teleoperated navigation tasks in a crowded office environment with a keyboard (reliable interface) and with 2-class motor imagery (MI) BMI (uncertain control channel). Experimental results show that our shared intelligence system 1) allows users to efficiently teleoperate the robot in both control modalities; 2) it ensures a level of BMI navigation performances comparable to the keyboard control; 3) it actively assists BMI users in accomplishing the tasks. These results highlight the importance of investigating advanced human-machine interaction (HMI) strategies and introducing robotic intelligence to improve the performances of BMI actuated devices.},
  archive      = {J_THMS},
  author       = {Gloria Beraldo and Luca Tonin and José del R. Millán and Emanuele Menegatti},
  doi          = {10.1109/THMS.2021.3137035},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {400-409},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Shared intelligence for robot teleoperation via BMI},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synthetic vs human emotional faces: What changes in humans’
decoding accuracy. <em>THMS</em>, <em>52</em>(3), 390–399. (<a
href="https://doi.org/10.1109/THMS.2021.3129714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considered the increasing use of assistive technologies in the shape of virtual agents, it is necessary to investigate those factors which characterize and affect the interaction between the user and the agent, among these emerges the way in which people interpret and decode synthetic emotions, i.e., emotional expressions conveyed by virtual agents. For these reasons, an article is proposed, which involved 278 participants split in differently aged groups (young, middle-aged, and elders). Within each age group, some participants were administered a “naturalistic decoding task,” a recognition task of human emotional faces, while others were administered a “synthetic decoding task” namely emotional expressions conveyed by virtual agents. Participants were required to label pictures of female and male humans or virtual agents of different ages (young, middle-aged, and old) displaying static expressions of disgust, anger, sadness, fear, happiness, surprise, and neutrality. Results showed that young participants showed better recognition performances (compared to older groups) of anger, sadness, and neutrality, while female participants showed better recognition performances (compared to males) of sadness, fear, and neutrality; sadness and fear were better recognized when conveyed by real human faces, while happiness, surprise, and neutrality were better recognized when represented by virtual agents. Young faces were better decoded when expressing anger and surprise, middle-aged faces were better decoded when expressing sadness, fear, and happiness, while old faces were better decoded in the case of disgust; on average, female faces where better decoded compared to male ones.},
  archive      = {J_THMS},
  author       = {Terry Amorese and Marialucia Cuciniello and Alessandro Vinciarelli and Gennaro Cordasco and Anna Esposito},
  doi          = {10.1109/THMS.2021.3129714},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {390-399},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Synthetic vs human emotional faces: What changes in humans’ decoding accuracy},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A review of evaluation practices of gesture generation in
embodied conversational agents. <em>THMS</em>, <em>52</em>(3), 379–389.
(<a href="https://doi.org/10.1109/THMS.2022.3149173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embodied conversational agents (ECAs) are often designed to produce nonverbal behavior to complement or enhance their verbal communication. One such form of the nonverbal behavior is co-speech gesturing, which involves movements that the agent makes with its arms and hands that are paired with verbal communication. Co-speech gestures for ECAs can be created using different generation methods, divided into rule-based and data-driven processes, with the latter, gaining traction because of the increasing interest from the applied machine learning community. However, reports on gesture generation methods use a variety of evaluation measures, which hinders comparison. To address this, we present a systematic review on co-speech gesture generation methods for iconic, metaphoric, deictic, and beat gestures, including reported evaluation methods. We review 22 studies that have an ECA with a human-like upper body that uses co-speech gesturing in social human-agent interaction. This includes studies that use human participants to evaluate performance. We found most studies use a within-subject design and rely on a form of subjective evaluation, but without a systematic approach. We argue that the field requires more rigorous and uniform tools for co-speech gesture evaluation, and formulate recommendations for empirical evaluation, including standardized phrases and example scenarios to help systematically test generative models across studies. Furthermore, we also propose a checklist that can be used to report relevant information for the evaluation of generative models, as well as to evaluate co-speech gesture use.},
  archive      = {J_THMS},
  author       = {Pieter Wolfert and Nicole Robinson and Tony Belpaeme},
  doi          = {10.1109/THMS.2022.3149173},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {379-389},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A review of evaluation practices of gesture generation in embodied conversational agents},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward active physical human–robot interaction: Quantifying
the human state during interactions. <em>THMS</em>, <em>52</em>(3),
367–378. (<a href="https://doi.org/10.1109/THMS.2021.3138684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unanticipated physical actions from the robot on humans [active physical human–robot interaction (pHRI)] may be inevitable with the deployment of robots in human-populated environments. However, it is still unclear how humans would perceive such actions and how the robot should execute them in a physically and psychologically safe manner. The objective of this article is to explore the possibility of quantifying the humans’ physical and mental state during an active physical interaction with a robot, by means of a laboratory experiment. We hypothesize that the active robot actions could cause measurable alterations in users’ data, which could be related to their perceptions and personalities. In the experiment, the user plays a visual game using the robot, which has a hidden task that results in active physical actions on the user. We collect data from physical and physiological sensors, and the perceptions and personalities via questionnaires and a semi-structured interview. Statistical analysis and clustering of the data collected from a total of 35 participants showed the relationships between participants’ physical and physiological data and their age, gender, perception, and personalities. Further developments based on these exploratory outcomes can be used to implement an active pHRI controller that can account for both the physical and the mental state of users.},
  archive      = {J_THMS},
  author       = {Yue Hu and Naoko Abe and Mehdi Benallegue and Natsuki Yamanobe and Gentiane Venture and Eiichi Yoshida},
  doi          = {10.1109/THMS.2021.3138684},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {367-378},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Toward active physical Human–Robot interaction: Quantifying the human state during interactions},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Task-oriented user evaluation on critiquing-based
recommendation chatbots. <em>THMS</em>, <em>52</em>(3), 354–366. (<a
href="https://doi.org/10.1109/THMS.2021.3131674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dialogue-based conversational recommender systems (DCRSs) have become a new trend in recommender systems (RSs), allowing users to communicate with the system in natural language to facilitate feedback provision and product exploration. However, little work has been done to empirically study user perception of and interaction with such systems and, more importantly, how to best support users in providing feedback on the recommendation they receive. In this article, we aim to develop effective critiquing mechanisms for DCRS to improve its feedback elicitation process (i.e., allowing users to critique the current recommendation during the dialogue). Specifically, we have implemented three prototype systems featuring three different critiquing techniques, respectively, i.e., user-initiated critiquing, progressive system-suggested critiquing , and cascading system-suggested critiquing . We have then conducted two task-oriented user studies involving 292 subjects to evaluate the three prototypes. In particular, we consider two typical types of user tasks in RSs: basic recommendation task (BRT, i.e., looking for items according to the user’s preferences), and exploration-oriented task (EOT, i.e., exploring different types of items). Results show that EOT stimulates more user interaction, while BRT results in higher user satisfaction. Moreover, when users perform EOT, the type of critiquing techniques is more likely to influence user perception and moderate the relationships between certain interaction metrics and users’ perceived serendipity. The findings suggest effective critiquing techniques to enhance the interaction between users and the recommendation chatbot when the system makes recommendations for different purposes.},
  archive      = {J_THMS},
  author       = {Wanling Cai and Yucheng Jin and Li Chen},
  doi          = {10.1109/THMS.2021.3131674},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {354-366},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Task-oriented user evaluation on critiquing-based recommendation chatbots},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Can a chatbot comfort humans? Studying the impact of a
supportive chatbot on users’ self-perceived stress. <em>THMS</em>,
<em>52</em>(3), 343–353. (<a
href="https://doi.org/10.1109/THMS.2021.3113643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is part of a project that explores the potential of chatbots for providing online emotional support to humans tailored to stressors. Based on a number of empirical studies, we have developed a socially interactive agent able to have simple dialogues with stressed humans seeking for emotional support. In the current article, we address the question to what extent this chatbot is effective in helping users cope with stressful situations. To this end, we present a study in which participants were asked to interact with our proposed chatbot for three days. Participants are distributed over the following three conditions—namely: 1) receiving support from the chatbot, knowing the support is computer-generated; 2) receiving support from the chatbot, while believing the support is human-generated; 3) not receiving any support. During the three days, participants’ self-reported stress levels are measured on a daily basis before and after each interaction. Results indicate that the best results are obtained in the ‘human’ condition, while the worst results are obtained in the “computer” condition. These findings lead us to conclude that the presumed sender of a stress support message (i.e., a human or a computer) might be more important than the content of the message.},
  archive      = {J_THMS},
  author       = {Lenin Medeiros and Tibor Bosse and Charlotte Gerritsen},
  doi          = {10.1109/THMS.2021.3113643},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {343-353},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Can a chatbot comfort humans? studying the impact of a supportive chatbot on users’ self-perceived stress},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Assistant robot enhances the perceived communication quality
of people with dementia: A proof of concept. <em>THMS</em>,
<em>52</em>(3), 332–342. (<a
href="https://doi.org/10.1109/THMS.2021.3112957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Almost all older people with dementia have progressive communication difficulties, which lead to increased social isolation and negative emotions. Thus, providing communication assistance for them is essential. This paper explores the feasibility of using social robots to assist older people with dementia in their face-to-face communication with others. We designed the behavior of a humanoid Pepper robot and made a Wizard of Oz prototype that the robot can serve as a personal memory assistant. The robot stores personal information for older people and assists in their communication through voice and screen display. In a video-based study with 88 participants, we investigated the effects of this assistive robot from a third-person observer perspective. Data were collected and analyzed using both three-way MANCOVAs for quantitative analysis and conventional content analysis for qualitative data. The results revealed that, by providing memory support, the robot significantly improved the observer&#39;s perceptions of an older person with dementia, including her perceived communication ability and performance, and personal image. Meanwhile, the communication is perceived to be significantly more effective when the robot assisted an older person. The willingness of others to communicate with more senior people also increased accordingly. Based on these findings, we present guidelines that may inform the design and development of communication assistant robots for older people with dementia.},
  archive      = {J_THMS},
  author       = {Di Zhou and Emilia I. Barakova and Pengcheng An and Matthias Rauterberg},
  doi          = {10.1109/THMS.2021.3112957},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {332-342},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Assistant robot enhances the perceived communication quality of people with dementia: A proof of concept},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial special issue interaction with artificial
intelligence systems: New human-centered perspectives and challenges.
<em>THMS</em>, <em>52</em>(3), 326–331. (<a
href="https://doi.org/10.1109/THMS.2022.3172516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special section focus on the interaction with artificial intelligence (AI) systems using human-centered applications. AI methods are being applied to numerous areas, including medicine, security, transportation, industry, smart homes and cities, business, social sciences, and psychology. AI is currently a part of our daily lives. People interact continuously with AI: it is inside houses, computers, mobile phones, and applications. AI can make predictions and give suggestions for movies, songs, or future purchases based on our previous choices. It affects the society and economy. People are fascinated by AI in the ways it improves and facilitates human life (improving health care and discharging workers from heavy or dangerous jobs). People are also concerned with AI’s implementation risks, such as ethical, security, and privacy issues. There are also concerns that AI machines may replace humans in various activities. AI researchers and practitioners have been facing these issues and further research is needed to design technical and regulatory applicable solutions. This special issue (SI) investigates a broad range of issues deriving from human interaction with AI. We encouraged interdisciplinary and multidisciplinary contributions toward understanding how AI could improve human life in various fields.},
  archive      = {J_THMS},
  author       = {Merylin Monaro and Emilia Barakova and Nicol Navarin},
  doi          = {10.1109/THMS.2022.3172516},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {326-331},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Editorial special issue interaction with artificial intelligence systems: New human-centered perspectives and challenges},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). IEEE systems, man, and cybernetics society. <em>THMS</em>,
<em>52</em>(2), C3. (<a
href="https://doi.org/10.1109/THMS.2022.3157107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2022.3157107},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {C3},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE systems, man, and cybernetics society},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022d). IEEE systems, man, and cybernetics society. <em>THMS</em>,
<em>52</em>(2), C2. (<a
href="https://doi.org/10.1109/THMS.2022.3157113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2022.3157113},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {C2},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE systems, man, and cybernetics society},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SPEye: A calibration-free gaze-driven text entry technique
based on smooth pursuit. <em>THMS</em>, <em>52</em>(2), 312–323. (<a
href="https://doi.org/10.1109/THMS.2021.3123202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaze-based text entry is undoubtedly one of the most useful applications of eye-tracking technology for human-machine interaction, both in the assistive context (users with severe motor disabilities can exploit such writing modalities to communicate with the world) and as a way to allow touchless text input in everyday life. Different eye-driven text entry methods have been developed to date, and almost all of them require preliminary calibration procedures to work correctly. When a short text, such as a password or a PIN, needs to be entered without using hands or voice, calibration may be perceived as an unnecessary nuisance (and may not be properly maintained in public places due to “ambient noise,” caused, for example, by nearby people). Inadequate calibration may also be a problem in case of assistive uses. In this article we present SPEye , a calibration-free eye-controlled writing technique based on smooth pursuit. Although its writing speed is significantly lower than that of ordinary calibrated methods, the absence of an initial calibration makes it suitable for short text entry. The technique has been tested through several experiments, obtaining good performances in terms of key strokes per character and total error rate metrics, and receiving positive feedback from the participants in the tests.},
  archive      = {J_THMS},
  author       = {Marco Porta and Piercarlo Dondi and Alice Pianetta and Virginio Cantoni},
  doi          = {10.1109/THMS.2021.3123202},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {312-323},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {SPEye: A calibration-free gaze-driven text entry technique based on smooth pursuit},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Binocular feature fusion and spatial attention mechanism
based gaze tracking. <em>THMS</em>, <em>52</em>(2), 302–311. (<a
href="https://doi.org/10.1109/THMS.2022.3145097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaze tracking is widely used in driver safety driving, visual impairment detection, virtual reality, human robot interaction, and reading process tracking. However, varying illumination, various head poses, different distances between human and cameras, occlusion of hair or glasses, and low-quality images pose huge challenges to accurate gaze tracking. In this article, based on binocular feature fusion and convolution neural network, a novel method of gaze tracking is proposed, in which local binocular spatial attention mechanism (LBSAM) and global binocular spatial attention mechanism (GBSAM) are integrated into the network model to improve the accuracy. Furthermore, the proposed method is validated on the GazeCapture database. In addition, four groups of comparative experiments have been conducted: between binocular feature fusion model and binocular data fusion model; among the local binocular spatial attention model, the local binocular channel attention model, and the model without local binocular attention mechanism; between the model with GBSAM and that without GBSAM; and between the proposed method and other state-of-the-art approaches. The experimental results verify the advantages of binocular feature fusion, LBSAM and GBSAM, and the effectiveness of the proposed method.},
  archive      = {J_THMS},
  author       = {Lihong Dai and Jinguo Liu and Zhaojie Ju},
  doi          = {10.1109/THMS.2022.3145097},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {302-311},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Binocular feature fusion and spatial attention mechanism based gaze tracking},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). L-sign: Large-vocabulary sign gestures recognition system.
<em>THMS</em>, <em>52</em>(2), 290–301. (<a
href="https://doi.org/10.1109/THMS.2022.3146787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding sign gestures is an essential step to helping individuals with hearing impaired. The existing works can only identify a small set of gestures accurately and the accuracy rate drops sharply with an increasing number of gestures. Because there are two challenges—a large number of similar gestures in sign language and the various signing speed of different people. Based on commercial smart bracelets, this article proposes a large-vocabulary sign language recognition system (which we call L-sign). First, we propose an entropy-based forward and backward matching algorithm to segment each gesture signal. Second, we design a gesture recognizer including a candidate gesture generator and semantic-based voter. The candidate gesture generator is aimed at providing candidate gesture designs based on a 3-branch convolutional neural network. The purpose of a semantic-based voter is to select the target gesture from candidate gestures by scoring, where the semantic distances between the last gesture in the current sentence and any candidate gestures is calculated, and a multilayer k-means algorithm is proposed to obtain a multilayer sign word structure to complete the scores of candidate gestures. Lastly, we deployed L-sign on the MYO bracelet. For 200 commonly used Chinese sign gestures, the experimental results show that the average accuracy rate was greater than 90%.},
  archive      = {J_THMS},
  author       = {Zhiwen Zheng and Qingshan Wang and Dejun Yang and Qi Wang and Wei Huang and Yinlong Xu},
  doi          = {10.1109/THMS.2022.3146787},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {290-301},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {L-sign: Large-vocabulary sign gestures recognition system},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SoDar: Multitarget gesture recognition based on SIMO doppler
radar. <em>THMS</em>, <em>52</em>(2), 276–289. (<a
href="https://doi.org/10.1109/THMS.2022.3149408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, various intelligent activity recognition systems have been developed based on radio frequency signals such as radar, Wi-Fi, and radio frequency identification (RFID). When only one target is present, these systems can often provide high accuracy in recognizing different activities. However, such activity identification systems often fail to work due to signal interference when multiple targets coexist. To address this problem, we propose a multitarget gesture recognition system, named SoDar, based on a commercial single-input multi-output (SIMO) dual-channel Doppler radar. First, we employ endpoint detection, low-pass filtering, and discrete wavelet transform for data preprocessing. Then, we design a multitarget signal separation algorithm by maximizing the signal-to-noise ratio, and further refine the obtained signal based on principle component analysis. Afterward, we put forward a two-stage feature extraction method to extract both static and dynamic features from each separated signal. Finally, a classification model is trained to recognize the gestures of multiple targets. To verify the performance of SoDar, we selected nine different combinations of six gestures for two targets and collected more than 8000 data samples. Experimental results showed that the accuracy of two-target gesture recognition is above 90%.},
  archive      = {J_THMS},
  author       = {Zhiwen Yu and Dong Zhang and Zhu Wang and Qi Han and Bin Guo and Qi Wang},
  doi          = {10.1109/THMS.2022.3149408},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {276-289},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {SoDar: Multitarget gesture recognition based on SIMO doppler radar},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Can visually impaired use gestures to interact with
computers? A cognitive load perspective. <em>THMS</em>, <em>52</em>(2),
267–275. (<a href="https://doi.org/10.1109/THMS.2022.3144002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Production of gestures does not require vision. Everyone—even those who are blind since birth—produces hand gestures while interacting with others. In our previous study, a gesture-based technique called Dactylology has been developed for the visually impaired to help them interact with computers. Cognitive load is an important indicator and a critical research issue while designing and adopting such new techniques. Hence, a study was conducted to compare the performance on a task between two computer input techniques (i.e., Dactylology and Braille) under varying levels of cognitive load (low, medium, and high), introduced by manipulating the task complexity. For the purpose of the study, 14 visually impaired participants were trained on Dactylology and Braille techniques to interact with the computer. The task performance was measured through the response time (RT) and false responses (FR). The results confirm that the participants had significantly lower RT and committed fewer FR in the Dactylology technique than the Braille under all cognitive load conditions. Altogether, these results render sufficient support to consider gesture-based Dactylology as a potential technique for the visually impaired to interact with the computer.},
  archive      = {J_THMS},
  author       = {Gourav Modanwal and Shashi Bhushan Rai and Aishwarya Jaiswal and Tushar Singh and Kishor Sarawadekar},
  doi          = {10.1109/THMS.2022.3144002},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {267-275},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Can visually impaired use gestures to interact with computers? a cognitive load perspective},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EEG-based auditory attention detection via frequency and
channel neural attention. <em>THMS</em>, <em>52</em>(2), 256–266. (<a
href="https://doi.org/10.1109/THMS.2021.3125283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans have the ability to pay attention to one of the sound sources in a multispeaker acoustic environment. Auditory attention detection (AAD) seeks to detect the attended speaker from one’s brain signals that will enable many innovative human–machine systems. However, effective representation learning of electroencephalography (EEG) signals remains a challenge. In this article, we propose a neural attention mechanism that dynamically assigns differentiated weights to the subbands and the channels of EEG signals to derive discriminative representations for AAD. In the nutshell, we would like to build a computational attention mechanism, i.e., neural attention, to model the auditory attention in human brain. We incorporate the proposed neural attention into an AAD system, and validate the neural attention mechanism through comprehensive experiments on two publicly available datasets. The experimental results demonstrate that the proposed system significantly outperforms the state-of-the-art reference baselines.},
  archive      = {J_THMS},
  author       = {Siqi Cai and Enze Su and Longhan Xie and Haizhou Li},
  doi          = {10.1109/THMS.2021.3125283},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {256-266},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {EEG-based auditory attention detection via frequency and channel neural attention},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multiplayer VR live concert with information exchange
through feedback modulated by EEG signals. <em>THMS</em>,
<em>52</em>(2), 248–255. (<a
href="https://doi.org/10.1109/THMS.2021.3134555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the emergence of brain-computer interfaces, social applications of this technology wherein users interact with each other are infrequent. Thus, we develop a novel system consisting of a multiplayer virtual reality live concert in which the brain activity of the audience is used to elicit visual effects, based on the $\beta$ / $\alpha$ ratio signal obtained from the single-channel electroencephalogram (EEG) recorder. In this way, the system interconnects users through neurofeedback that can potentially be used to cause a “sense of unity” between users. In this work, we introduce the configuration, implementation and specifications of the system, proposing a local configuration and a client configuration. Function testing of the system performance for generating visual effects and quality control of the signal is conducted. A preliminary study to test the system is also shown, and the $\beta$ / $\alpha$ ratio signal and questionnaire are briefly analyzed. We propose the usability of the system for future research on the influence of music and emotions on group interactions.},
  archive      = {J_THMS},
  author       = {Ángel Muñoz-González and Shohei Kobayashi and Ryota Horie},
  doi          = {10.1109/THMS.2021.3134555},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {248-255},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A multiplayer VR live concert with information exchange through feedback modulated by EEG signals},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EEG correlates of driving performance. <em>THMS</em>,
<em>52</em>(2), 232–247. (<a
href="https://doi.org/10.1109/THMS.2021.3137032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Techniques for monitoring human performance traditionally rely on subjective responses and task-specific scoring, yet research suggests EEG could offer multiple performance metrics with high temporal resolution and accuracy that could be leveraged for human–computer interaction purposes. The objective of the presented work is to investigate which EEG responses correlate with task performance and evaluate whether combinations of these produce effective predictive models, facilitating further understanding of the psychological link to performance. A user study was conducted with 32 participants required to negotiate a driving course with the ambition of learning and improving ability on the course during an EEG recording session. EEG was filtered and post-processed to find power spectral density in alpha ( $\alpha$ ), beta ( $\beta$ ), delta ( $\delta$ ), and theta ( $\theta$ ) frequency bands, as well as frontal alpha asymmetry (FAA). The initial laps were considered a baseline and an average performance improvement was calculated over the remaining laps in terms of percentage improvement in duration of track traversal. Results demonstrate event related desynchronization with increased task performance in the alpha ( p =. 000), delta ( p =. 000), and theta ( p =. 000) bands, as well as evidence of a relationship between overall change in FAA and task efficiency. A full electrode analysis identifies $\delta _{{\rm F}_{4}}$ as the optimal for predicting collisions, with efficiency best predicted by a combination of $\beta _{{\rm O}_{\rm z}}$ and $\delta _{{\rm F}_{4}}$ .},
  archive      = {J_THMS},
  author       = {Thomas G. Simpson and Karen Rafferty},
  doi          = {10.1109/THMS.2021.3137032},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {232-247},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {EEG correlates of driving performance},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A technical framework for musical biofeedback in stroke
rehabilitation. <em>THMS</em>, <em>52</em>(2), 220–231. (<a
href="https://doi.org/10.1109/THMS.2021.3137013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a technical framework aimed at facilitating musical biofeedback research in poststroke movement rehabilitation. The framework comprises wireless wearable inertial sensors and software built with inexpensive and open-source tools. The software enables layered and adjustable music synthesis and has a generic movement–music mapping module. Using this, we designed digital musical interactions for balance, sit-to-stand, and gait training. Preliminary trials with subacute stroke patients indicated that the interactions were clinically feasible. Expert interviews with a focus group of clinicians were also conducted, where these interactions were deemed as meaningful and relevant to clinical protocols, with comprehensible feedback (albeit sometimes unpleasant or disturbing) for several patient types. We carried out system benchmarking, finding that the system has sufficiently short loop delays ( $\sim$ 90 ms) and a healthy sensing range ( $&amp;gt;$ 9 m) and is computationally efficient (11.1% peak CPU usage on a quad-core processor). Future studies will focus on using this framework with patients to both develop the interactions further and measure their effects on motor learning, performance retention, and psychological factors to help gauge their true clinical potential.},
  archive      = {J_THMS},
  author       = {Prithvi Kantan and Erika G. Spaich and Sofia Dahl},
  doi          = {10.1109/THMS.2021.3137013},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {220-231},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A technical framework for musical biofeedback in stroke rehabilitation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A computer vision approach for estimating lifting load
contributors to injury risk. <em>THMS</em>, <em>52</em>(2), 207–219. (<a
href="https://doi.org/10.1109/THMS.2022.3148339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Safety practitioners widely use the lifting index (LI) to determine workers’ lifting risk but are hampered by the difficulties of estimating the lifting load without intervention or intrusive sensors. This study proposes a computer vision method for estimating the LI across varying lifting loads. The proposed method can also predict the Brog rating of perceived exertion (RPE), a measure associated with the lifting load. A controlled lifting experiment was conducted to demonstrate the approach. Thirty participants performed 2176 lifting tasks at three LI levels. These levels were controlled by varying the lifting load and fixing other task variables (e.g., the lifting distance). The proposed method combined the pose estimation (OpenPose) and the optical flow estimation (SelFlow) techniques for extracting the participants’ body motion and posture features; a facial expression recognition algorithm (OpenFace) built upon the facial action unit coding system (FACS) was used to extract the participants’ facial features. The extracted features were combined and used to develop prediction models. The best-performing model was an integration of the 1-D convolutional neural network and the long short-term memory network. It achieved an area under curve of 0.890 in classifying the LI and a root mean square of 2.264 in predicting the participants’ RPE. Critical indicators were identified by investigating the contribution of the features through interpretable machine learning techniques. In summary, this study demonstrates a nonintrusive method for lifting risk assessment and discovers behavioral indicators that predict changes in the LI and RPE due to varying loads.},
  archive      = {J_THMS},
  author       = {Guoyang Zhou and Vaneet Aggarwal and Ming Yin and Denny Yu},
  doi          = {10.1109/THMS.2022.3148339},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {207-219},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A computer vision approach for estimating lifting load contributors to injury risk},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiview video-based 3-d pose estimation of patients in
computer-assisted rehabilitation environment (CAREN). <em>THMS</em>,
<em>52</em>(2), 196–206. (<a
href="https://doi.org/10.1109/THMS.2022.3142108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computer-assisted rehabilitation environment (CAREN) system plays an important role in the training of rehabilitation patients, where the capture of the patient&#39;s 3-D pose and gait is critical for assessing the patient&#39;s requirements for effective training. Vision-based methods are highly effective for this task due to their low cost, high speed, and noninterference. Although various general vision-based pose estimation methods were developed recently, their performance is limited in the CAREN system due to the specific environment. To address these problems, we propose an improved framework for accurate 2-D and 3-D pose estimation for the CAREN system through using multiview videos. First, for 2-D pose estimation, we propose a coarse-to-fine heatmap shrinking (CFHS) strategy that gradually reduces the kernel size of the heatmap of joints during training to improve the performance. Second, to further obtain 3-D pose estimations, we propose a novel spatial-temporal perception network that fuses the 2-D results from multiple views and multiple moments; multiview early fusion uses complementary spatial information from different views, and multimoment late fusion leverages temporal information from the sequential input for higher accuracy. The experimental results, based on CAREN videos of 225 orthopedic patients, showed that the accuracy of 2-D human pose estimations with the CFHS training strategy reached 99.85% PCKh@0.5. For 3-D results, the mean per joint position error was 25.22 mm, and the 3DPCK reached 98.71%, which outperformed existing general video-based methods. The results showed that the proposed system is capable of estimating human poses with high accuracy for clinical applications.},
  archive      = {J_THMS},
  author       = {Wei Xu and Donghai Xiang and Guotai Wang and Ruisong Liao and Ming Shao and Kang Li},
  doi          = {10.1109/THMS.2022.3142108},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {196-206},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Multiview video-based 3-D pose estimation of patients in computer-assisted rehabilitation environment (CAREN)},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cognitive workload assessment of prosthetic devices: A
review of literature and meta-analysis. <em>THMS</em>, <em>52</em>(2),
181–195. (<a href="https://doi.org/10.1109/THMS.2022.3143998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limb amputation can cause severe functional disability for the performance of activities of daily living. Previous studies have found differences in cognitive demands imposed by prosthetic devices due to variations in their design. The objectives of this article were to 1) identify the range of cognitive workload (CW) assessment techniques used in prior studies comparing different prosthetic devices, 2) identify the device configurations or features that reduced CW of users, and 3) provide guidelines for designing future prosthetic devices to reduce CW. A literature search was conducted using Compendex, Inspec, Web of Science, Proquest, IEEE, Engineering Research Database, PubMed, Cochrane, and Google Scholar. Forty-three studies met the inclusion criteria. Findings suggested that CW of prosthetic devices was assessed using physiological, task performance, and subjective measures. However, due to the limitations of these methods, there is a need for more theoretical and model-based approaches to quantify CW. Device configurations such as hybrid input signals and use of multimodal feedback can reduce CW of prosthetic devices. Furthermore, to evaluate the effectiveness of a training strategy for reducing CW and improving device usability, both task performance and subjective measures should be considered. Based on the literature review, a set of guidelines was provided to improve the usability of future prosthetic devices and reduce CW.},
  archive      = {J_THMS},
  author       = {Junho Park and Maryam Zahabi},
  doi          = {10.1109/THMS.2022.3143998},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {181-195},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Cognitive workload assessment of prosthetic devices: A review of literature and meta-analysis},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantitative physical ergonomics assessment of teleoperation
interfaces. <em>THMS</em>, <em>52</em>(2), 169–180. (<a
href="https://doi.org/10.1109/THMS.2022.3149167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human factors and ergonomics are the essential constituents of teleoperation interfaces, which can significantly affect the human operator’s performance. Thus, a quantitative evaluation of these elements and the ability to establish reliable comparison bases for different teleoperation interfaces are the keys to select the most suitable one for a particular application. However, most of the works on teleoperation have so far focused on the stability analysis and the transparency improvement of these systems and do not cover the important usability aspects. In this article, we propose a foundation to build a general framework for the analysis of human factors and ergonomics in employing diverse teleoperation interfaces. The proposed framework will go beyond the traditional subjective analyses of usability by complementing it with online measurements of human body configurations. As a result, multiple quantitative metrics, such as joints’ usage, range of motion comfort, center of mass divergence, and posture comfort, are introduced. To demonstrate the potential of the proposed framework, two different teleoperation interfaces are considered, and real-world experiments with 11 participants performing a simulated industrial remote pick-and-place task are conducted. The quantitative results of this analysis are provided, and compared with subjective questionnaires, illustrating the effectiveness of the proposed framework.},
  archive      = {J_THMS},
  author       = {Soheil Gholami and Marta Lorenzini and Elena De Momi and Arash Ajoudani},
  doi          = {10.1109/THMS.2022.3149167},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {169-180},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Quantitative physical ergonomics assessment of teleoperation interfaces},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OAST: Obstacle avoidance system for teleoperation of UAVs.
<em>THMS</em>, <em>52</em>(2), 157–168. (<a
href="https://doi.org/10.1109/THMS.2022.3142107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel flight assistance system, obstacle avoidance system for teleoperation (OAST), whose main role is to make teleoperation of small multirotor unmanned aerial vehicles (UAVs) safer and more efficient in closed spaces. The OAST allows the operator to avoid obstacles while keeping a liberty of movement. The UAV is controlled through a 3-D haptic controller and the OAST amends the user input to increase safety and efficiency of the teleoperation. The design of the OAST is verified in computerized experiments. Moreover, a simulation involving 20 participants is carried out to validate the proposed scheme. This experiment shows that the OAST improves the completion time of the scenarios by 41% on average while reducing the workload of the operator from 57 to 27 points on the NASA Task Load Index test. The number of collisions with the environment is all but eliminated in these scenarios.},
  archive      = {J_THMS},
  author       = {Hugo Courtois and Nabil Aouf and Kenan Ahiska and Marco Cecotti},
  doi          = {10.1109/THMS.2022.3142107},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {157-168},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {OAST: Obstacle avoidance system for teleoperation of UAVs},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022e). IEEE systems, man, and cybernetics society. <em>THMS</em>,
<em>52</em>(1), C3. (<a
href="https://doi.org/10.1109/THMS.2022.3140958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2022.3140958},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {C3},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE systems, man, and cybernetics society},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Curb negotiation with dynamic human–robotic wheelchair
collaboration. <em>THMS</em>, <em>52</em>(1), 149–155. (<a
href="https://doi.org/10.1109/THMS.2021.3131672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wheelchair users often face architectural barriers such as curbs, limiting their accessibility, mobility, and participation in their communities. The mobility enhancement robotic (MEBot) wheelchair was developed to navigate over such architectural barriers. Its application allows wheelchair users to negotiate curbs automatically while the user remains in control. The application was optimized from a manual to a semiautomated process based on wheelchair users’ feedback. The optimized application was evaluated by experienced wheelchair users who navigated over curbs of different heights. Participants evaluated MEBot in terms of effectiveness, workload demand, and usability. Ten participants successfully ascended and descended curbs of different heights at an average completion time of 55.7 ± 19.5 and 30.3 ± 9.1 s, respectively. MEBot maintained stability during the process, while participants reported low levels of effort, frustration, and overall cognitive demand to operate MEBot. Furthermore, participants were satisfied with the ease of learning and using the MEBot curb negotiation application to overcome the curbs but suggested less wheel adjustment for comfort and a faster pace to overcome curbs during real-world conditions.},
  archive      = {J_THMS},
  author       = {Jorge L. Candiotti and Brandon J. Daveler and Sivashankar Sivakanthan and Garrett G. Grindle and Rosemarie Cooper and Rory A. Cooper},
  doi          = {10.1109/THMS.2021.3131672},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {149-155},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Curb negotiation with dynamic Human–Robotic wheelchair collaboration},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey of 17 indoor travel assistance systems for blind
and visually impaired people. <em>THMS</em>, <em>52</em>(1), 134–148.
(<a href="https://doi.org/10.1109/THMS.2021.3121645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Not only has information technology evolved rapidly, but the spatial cognition theory for blind and visually impaired (BVI) people has also made great strides, which has opened up a new opportunity for indoor travel assistance systems (ITASs). However, there are still some issues that have not been effectively addressed due to the lack of guidance of the spatial cognition theory. Thus, this article presents a comparative survey among ITASs proposed in the last four years in an effort to inform researchers and developers about system problems and challenges and inform BVI people about the various types and functions of the ITAS. This article will also make researchers and developers aware of the importance of the spatial cognition theory. Furthermore, we give predictions for future trends based on a detailed analysis of 17 ITASs.},
  archive      = {J_THMS},
  author       = {Jie Wang and Erwu Liu and Yuanzhe Geng and Xinyu Qu and Rui Wang},
  doi          = {10.1109/THMS.2021.3121645},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {134-148},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A survey of 17 indoor travel assistance systems for blind and visually impaired people},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vision processing for assistive vision: A deep reinforcement
learning approach. <em>THMS</em>, <em>52</em>(1), 123–133. (<a
href="https://doi.org/10.1109/THMS.2021.3121661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is increasing interest in using computer vision and machine learning to enhance human decision making with computer-mediated assistive vision systems. In particular, retinal implants are a rapidly advancing technology offering individuals suffering vision loss due to retinal dystrophies, an opportunity to restore partial vision. However, the visual representations achievable with current and near-term implants are severely limited in resolution and contrast, placing high importance on the selection of visual features to convey via the implant. Using vision processing algorithms on camera-captured input, functional outcomes can be enhanced with such devices. To this end, we propose a novel end-to-end vision processing pipeline for prosthetic vision that learns task-salient visual filters in simulation offline via deep reinforcement learning (DRL). Once learnt, these filters are deployable on a prosthetic vision device to process camera-captured images and produce task-guiding scene representations in real-time. We show how a set of learnt visual features enabling a virtual agent to optimally perform the task of navigation in a 3-D environment can be extracted and applied to enhance the same features in real world images. We evaluate and validate our proposed approach quantitatively and qualitatively using simulated prosthetic vision. To our knowledge, this is the first application of DRL to the derivation of scene representations for human-centric computer-mediated displays such as prosthetic vision devices.},
  archive      = {J_THMS},
  author       = {Jack White and Tatiana Kameneva and Chris McCarthy},
  doi          = {10.1109/THMS.2021.3121661},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {123-133},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Vision processing for assistive vision: A deep reinforcement learning approach},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SAIL: A deep-learning-based system for automatic gait
assessment from TUG videos. <em>THMS</em>, <em>52</em>(1), 110–122. (<a
href="https://doi.org/10.1109/THMS.2021.3123232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait disorders are common in the elderly people, seriously hinder patients’ mobility and sometimes indicate underlying severe neurological diseases. Timely and automatic diagnosis of gait disorders is greatly desired. Existing methods with wearable devices put burdens on patients. We establish a video-based algorithm named SAIL to perform contactless gait assessment automatically. The SAIL contains three parts, namely, skeleton detector , parameter extractor , and gait classifier . Using a pose estimation algorithm, the skeleton detector converts RGB videos to a human skeleton sequence. Then, the parameter extractor extracts gait parameters from skeletons with a signal detection technique. Finally, a trained Support vector machine is used as a gait classifier to detect abnormal gait. The SAIL achieves 86.2% sensitivity and 98.5% specificity for abnormal gait detection on our SAIL-TUG dataset, outperforming general clinic doctors with 76.4% and 97.4%, respectively. Nine gait parameters and the binary gait classification result are included in the final gait report. We implement an automatic gait assessment system based on SAIL and deployed the user-interface software in more than 60 hospitals for practical applications. More than 30 000 gait reports have been automatically generated. Moreover, we establish a publicly available dataset named SAIL-TUG including 404 annotated Timed “Up &amp; Go” videos.},
  archive      = {J_THMS},
  author       = {Yanhong Wang and Qiaosha Zou and Yanmin Tang and Qing Wang and Jing Ding and Xin Wang and C.-J. Richard Shi},
  doi          = {10.1109/THMS.2021.3123232},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {110-122},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {SAIL: A deep-learning-based system for automatic gait assessment from TUG videos},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CNN confidence estimation for rejection-based hand gesture
classification in myoelectric control. <em>THMS</em>, <em>52</em>(1),
99–109. (<a href="https://doi.org/10.1109/THMS.2021.3123186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have been widely utilized to identify hand gestures from surface electromyography (sEMG) signals. However, due to the nonstationary characteristics of sEMG, the classification accuracy usually degrades significantly in the daily living environment involving complex hand movements. To further improve the reliability of a classifier, unconfident classifications are expected to be identified and rejected. In this study, we propose a novel approach to estimate the probability of correctness for each classification. Specifically, a confidence estimation model is established to generate confidence scores (ConfScore) based on posterior probabilities of CNN, and an objective function is designed to train the parameters of this model. In addition, a comprehensive metric that combines the true acceptance rate (TAR) and the true rejection rate (TRR) is proposed to evaluate the rejection performance of ConfScore, so that the tradeoff between system security and control lag could be fully considered. The effectiveness of ConfScore is verified using data from public databases and our online platform. The experimental results illustrate that ConfScore can better reflect the correctness of CNN classifications than traditional confidence features, i.e., maximum posterior probability and entropy of the probability vector. Moreover, the rejection performance is observed to be less sensitive to variations in rejection thresholds.},
  archive      = {J_THMS},
  author       = {Tianzhe Bao and Syed Ali Raza Zaidi and Sheng Quan Xie and Pengfei Yang and Zhi-Qiang Zhang},
  doi          = {10.1109/THMS.2021.3123186},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {99-109},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {CNN confidence estimation for rejection-based hand gesture classification in myoelectric control},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evolutionary game and collaboration mechanism of
human-computer interaction for future intelligent aircraft cockpit based
on system dynamics. <em>THMS</em>, <em>52</em>(1), 87–98. (<a
href="https://doi.org/10.1109/THMS.2021.3116115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of artificial intelligence in designing the future aircraft cockpit may lead to conflicts especially during the human-computer decision-making process. To harmonize the human-computer relationship, the evolutionary game and collaboration mechanism based on system dynamics (SDs) is proposed in this article. The evolutionary game model is established containing the flight crew and the intelligent decision-making system as players. The profit matrix of different game strategies is constructed, and the time-varying dynamic characteristics of the human-computer evolution game is analyzed. Based on the SDs theory, the human-computer collaboration model is constructed. Its functional component modules are respectively defined by the behavioral characteristics of pilots, decision-making performance, and human-computer evolutionary game. The coupling relationship between them is also presented. With the background of typical task scenarios, a dynamic simulation case of the human-computer collaboration is given. The impacts of task load change rate, task load level, and decision-making performance related parameters are analyzed. The evolutionary laws of human-computer game strategies are thereby revealed. This work can provide modeling methods for human-computer interaction in the aircraft cockpit.},
  archive      = {J_THMS},
  author       = {Xia Zhang and Youchao Sun and Yanjun Zhang},
  doi          = {10.1109/THMS.2021.3116115},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {87-98},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Evolutionary game and collaboration mechanism of human-computer interaction for future intelligent aircraft cockpit based on system dynamics},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bringing a vehicle to a controlled stop: Effectiveness of a
dual-control scheme for identifying driver drowsiness and preventing
lane departures under partial driving automation requiring
hands-on-wheel. <em>THMS</em>, <em>52</em>(1), 74–86. (<a
href="https://doi.org/10.1109/THMS.2021.3123171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial driving automation systems execute sustained lateral and longitudinal control, while humans are required to supervise the automated driving features. Similar with drivers using manual driving settings, those using automated driving systems may also experience drowsiness. However, existing systems that aim to detect driver drowsiness tend to be unreliable. This study applies a dual-control scheme in the partial driving automation context in which the driver must keep their hands on the vehicle’s steering wheel. This scheme executes a partial steering control when a vehicle lane departure is anticipated (because of inappropriate torque input), and then, activates a deceleration control if the driver does not properly perform the required action. To determine whether the driver is supervising the partial driving automation, the scheme attempts to create an opportunity for driver–automation interactions. Thus, the controller’s objectives are twofold: safety control and driver state identification. This study investigated the effectiveness of the scheme for identifying driver drowsiness and preventing lane departures using only vehicle information. Twenty drivers participated in a fixed-base driving simulator experiment in a sleep-inducing environment. While we observed cases in which the system could effectively bring the vehicle to a controlled stop, the timeliness and accuracy of the driver state identification remained as issues owing to indirect links between the drivers’ drowsiness level and controller activation. We conclude that although the dual-control scheme is a useful mechanism to avoid lane departures, the driver state identification needs to be improved to ensure timely and effective detection of driver drowsiness.},
  archive      = {J_THMS},
  author       = {Yuichi Saito and Makoto Itoh and Toshiyuki Inagaki},
  doi          = {10.1109/THMS.2021.3123171},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {74-86},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Bringing a vehicle to a controlled stop: Effectiveness of a dual-control scheme for identifying driver drowsiness and preventing lane departures under partial driving automation requiring hands-on-wheel},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A naturalistic driving study of feedback timing and
financial incentives in promoting speed limit compliance. <em>THMS</em>,
<em>52</em>(1), 64–73. (<a
href="https://doi.org/10.1109/THMS.2021.3117234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inappropriate speed choice increases crash risk. Emerging technologies, such as in-vehicle systems, can provide real-time and post-drive feedback to alert or educate drivers of their unsafe speed choices. Financial incentives, such as insurance reductions, can also be integrated with feedback. Although previous research supports the benefits of these approaches, research is limited in establishing their relative effectiveness and their long-term impact on speed limit compliance. A naturalistic driving study with 58 participants was conducted to investigate the effects of post-drive feedback versus financial incentives, both provided in conjunction with real-time feedback, on mitigating speeding behaviors. The study included a four-week long baseline data collection period (no feedback), a ten-week intervention period examining three feedback types in a between subjects design (real-time only; real-time and financial incentives; and real-time and post-drive feedback), and a two-week post-intervention period (feedback removed). Both real-time feedback alone and in conjunction with financial incentives were effective in raising speed limit compliance, but the effects did not sustain after the removal of feedback&amp;#x002F;incentives. Post-drive feedback, which was provided in the vehicle after a trip and as aggregated feedback on a website, does not appear to provide any benefits toward speed limit compliance; potential factors, including (voluntary) access frequency of post-drive feedback website, are discussed. Future research should continue to explore the feedback&amp;#x002F;incentive design space&amp;#x2014;carefully considering individual differences in driver characteristics and motivations&amp;#x2014;for supporting speed limit compliance and other risky driving behaviors.},
  archive      = {J_THMS},
  author       = {Huei-Yen Winnie Chen and Birsen Donmez},
  doi          = {10.1109/THMS.2021.3117234},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {64-73},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A naturalistic driving study of feedback timing and financial incentives in promoting speed limit compliance},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Information-based control of robots in search-and-rescue
missions with human prior knowledge. <em>THMS</em>, <em>52</em>(1),
52–63. (<a href="https://doi.org/10.1109/THMS.2021.3113642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multirobot systems provide a scalable and robust solution for monitoring tasks. In time-intensive missions such as search and rescue, the inclusion of a human has the potential advantage of incorporating prior knowledge about the target location or dynamics. In this article, we develop a general information-theoretic framework to control multiple autonomous robots in search and rescue missions that include a human teleoperator. Human prior knowledge is modeled to capture the target location and dynamics, and mutual-information-based control is formulated to let autonomous robots weigh between two strategies: independent search or assisting the human by staying in proximity. The control actions optimize a weighted sum of normalized mutual information calculated using particle-filtered estimates of the target and the reference robot. We implement the framework to simulate two widely different scenarios designed after search-and-rescue missions from literature, and incorporate varying levels of accuracy in human prior knowledge. Our results indicate that the mission performance depends on how robots weigh between the two strategies, with the amount of the optimal control effort shared between strategies affected by prior knowledge and the number of robots. Comparison with existing strategies points to the benefits of an information-based control in situations where human prior knowledge is inaccurate. The proposed information-theoretic abstraction of the human–robot interaction can be implemented on a wide variety of scenarios and the results highlight the role of human prior knowledge toward effective robotic assistance in time-intensive missions.},
  archive      = {J_THMS},
  author       = {Rafal Krzysiak and Sachit Butail},
  doi          = {10.1109/THMS.2021.3113642},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {52-63},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Information-based control of robots in search-and-rescue missions with human prior knowledge},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Would you imagine yourself negotiating with a robot,
jennifer? Why not? <em>THMS</em>, <em>52</em>(1), 41–51. (<a
href="https://doi.org/10.1109/THMS.2021.3121664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the improvement of intelligent systems and robotics, social robots are becoming part of our society. To accomplish complex tasks, robots and humans may need to collaborate, and when necessary, they need to negotiate with each other. While designing such socially interacting robots, it is crucial to consider human factors such as facial expression, emotions, and body language. Since gestures play a crucial role in interaction, this article studies the effect of gestures in human–robot negotiation experiments. Additionally, it compares the performance of variants of the well-known negotiation tactics (i.e., time-based and behavior-based) in automated negotiation literature in the context of human–robot negotiations. Our experimental results support the finding in automated negotiation. That is, the robot gained higher utility when it imitates its opponent’s bidding strategy than employing a time-based negotiation strategy. When adopting a behavior-based technique, there is a statistically significant effect of gestures on the underlying negotiation process, and, therefore, on negotiation outcome.},
  archive      = {J_THMS},
  author       = {Reyhan Aydoğan and Onur Keskin and Umut Çakan},
  doi          = {10.1109/THMS.2021.3121664},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {41-51},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Would you imagine yourself negotiating with a robot, jennifer? why not?},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finite-time observer-based variable impedance control of
cable-driven continuum manipulators. <em>THMS</em>, <em>52</em>(1),
26–40. (<a href="https://doi.org/10.1109/THMS.2021.3129708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human&amp;#x2013;robot interaction has been widely studied, where compliant and safe human&amp;#x2013;robot interaction of continuum manipulators in the constrained environment is one of the key issues that have not been well addressed. In this study, finite-time observer-based variable impedance control of cable-driven continuum manipulators (CDCM) is proposed to overcome the limitation of existing methods. First, the pseudo-rigid modeling method is utilized to establish the kinematics and dynamics of the CDCM. Then, a variable impedance controller with selected controller parameters for the CDCM is designed to realize compliant and safe human&amp;#x2013;robot interaction operations with force&amp;#x2013;position coupling constraint, which is rarely studied in the literature. In order to realize the closed-loop variable impedance controller, a finite-time observer is designed to estimate acceleration feedbacks, which can avoid the difficulty of directly sensing the interaction forces and shows excellent robust stability in noisy environments. On this basis, by combining the advantages of the variable impedance controller and finite-time observer, the finite-time observer-based variable impedance controller is proposed, and the stabilities of the proposed method are analyzed. Finally, the feasibility of the proposed control scheme for the CDCM is demonstrated by some numerical simulations.},
  archive      = {J_THMS},
  author       = {Xu Liang and Guangping He and Tingting Su and Weiqun Wang and Can Huang and Quanliang Zhao and Zeng-Guang Hou},
  doi          = {10.1109/THMS.2021.3129708},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {26-40},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Finite-time observer-based variable impedance control of cable-driven continuum manipulators},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A review of human–machine cooperation in the robotics
domain. <em>THMS</em>, <em>52</em>(1), 12–25. (<a
href="https://doi.org/10.1109/THMS.2021.3131684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) technology has greatly expanded human capabilities through perception, understanding, action, and learning. The future of AI depends on cooperation between humans and AI. In addition to a fully automated or manually controlled machine, a machine can work in tandem with a human with different levels of assistance and automation. Machines and humans cooperate in different ways. Three strategies for cooperation are described in this article, as well as the nesting relationships among different control methods and cooperation strategies. Based on human thinking and behavior, a hierarchical human&amp;#x2013;machine cooperation (HMC) framework is improved and extended to design safe, efficient, and attractive systems. We review the common methods of perception, decision-making, and execution in the HMC framework. Future applications and trends of HMC are also discussed.},
  archive      = {J_THMS},
  author       = {Canjun Yang and Yuanchao Zhu and Yanhu Chen},
  doi          = {10.1109/THMS.2021.3131684},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {12-25},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A review of Human&amp;#x2013;Machine cooperation in the robotics domain},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Eye-tracking for performance evaluation and workload
estimation in space telerobotic training. <em>THMS</em>, <em>52</em>(1),
1–11. (<a href="https://doi.org/10.1109/THMS.2021.3107519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring the mental workload of operators is of paramount importance in space telerobotic training and other teleoperation tasks. Instead of the estimation of task-specific workload, this article aims at investigating the impact of two significant confounding factors (time-pressure and latency) on space teleoperation and explored the use of eye-tracking technology for factor-induced mental workload estimation and performance evaluation. Ten subjects teleoperated a Canadarm2 robot to complete a complex on-orbit assembly task in our photo-realistic training simulator while wearing a head-mounted eye-tracker. To understand how time-pressure and latency influence eye-tracking features works, we first performed the statistical analysis on various features with respect to a single factor and across multiple groups. Next, eye-tracking features extracted from segment data and trial data is used to identify the mental workload induced by confounding factors, which can be used for developing personalized training programs and guaranteeing safe teleoperation. Furthermore, to improve the recognition performance using segment data, we propose the activity ratio and time ratio to characterize the informative segments. Finally, the relationship between simulator-defined performance measures and eye-tracking features is examined. Results show that fixation duration, saccade frequency and duration, pupil diameter, and index of pupillary activity are significant features that can be used in both factor-induced mental workload estimation and task performance evaluation.},
  archive      = {J_THMS},
  author       = {Yao Guo and Daniel Freer and Fani Deligianni and Guang-Zhong Yang},
  doi          = {10.1109/THMS.2021.3107519},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Eye-tracking for performance evaluation and workload estimation in space telerobotic training},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
