<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TBD_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tbd---129">TBD - 129</h2>
<ul>
<li><details>
<summary>
(2022). Multi-relation extraction via a global-local graph
convolutional network. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(6), 1716–1728. (<a
href="https://doi.org/10.1109/TBDATA.2022.3144151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Relation extraction (RE) extracts the semantic relations among entities in a sentence, which converts the unstructured text into structured and easy-to-understand information. Although RE has been studied over decades, it still faces two kinds of research challenges that are not well addressed thus far: 1) joint consideration of the global sentence structure and the local entity interaction, and 2) effective solution to the overlapping triplets within the same sentence. To tackle these issues, in this paper, we present global-local graph-based convolutional network towards multi-relation extraction, GAME for short. In particular, we devise two layers of graph convolutional network (GCN) with different structures to complete the feature extraction, which effectively improves the capability of relation extraction. Moreover, we implement the GCN layers via the pure GCN model and graph attention network respectively for further comparison. Besides, we adopt a classification strategy to extract relation among entity pairs, assisting in solving the more complicated problem of overlapping triplets in RE. Extensive experiments have been conducted on two widely-used benchmark datasets, demonstrating that our model significantly outperforms several state-of-the-art methods. As a side product, we have released our data, codes and parameter settings to facilitate other researchers.},
  archive  = {J},
  author   = {Harry Cheng and Lizi Liao and Linmei Hu and Liqiang Nie},
  doi      = {10.1109/TBDATA.2022.3144151},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1716-1728},
  title    = {Multi-relation extraction via a global-local graph convolutional network},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SOREL: Efficient and secure ORE-based range query over
outsourced data. <em>IEEE Transactions on Big Data</em>, <em>8</em>(6),
1702–1715. (<a
href="https://doi.org/10.1109/TBDATA.2021.3089986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Outsourcing data to the cloud has become popular due to the big data challenges. However, security concerns compel the outsourced data to be encrypted before sending them to the cloud, which lowers their utility and efficiency. The range query plays a significant role in common queries. Consequently, how to efficiently support the range query over encrypted data has become an important challenge. Previously reported schemes either achieve efficiency only in a specific operation or have severe defects in scalability. To address these limitations, we propose a framework, called SOREL, which simultaneously considers security, efficiency and scalability. Specifically, we first propose a new efficient and Secure Order Revealing Encryption (SORE) scheme, which is more secure than bit-based ORE schemes. Then, by employing the proposed SORE scheme, we design a novel index within our framework SOREL to support efficient updating and query operations over encrypted data. Detailed security analysis shows that our SOREL achieves the desirable security requirements. Additionally, results from extensive evaluations indicate that i) SORE outperforms other alternative schemes by at least 8×; and ii) SOREL is at least 3× faster than the comparative schemes with range query operation in the best case while ensuring the competitiveness with insertion operation.},
  archive  = {J},
  author   = {Songnian Zhang and Suprio Ray and Rongxing Lu},
  doi      = {10.1109/TBDATA.2021.3089986},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1702-1715},
  title    = {SOREL: Efficient and secure ORE-based range query over outsourced data},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hyperbolic graph attention network. <em>IEEE Transactions on
Big Data</em>, <em>8</em>(6), 1690–1701. (<a
href="https://doi.org/10.1109/TBDATA.2021.3081431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graph neural network (GNN) has shown superior performance in dealing with structured graphs, which has attracted considerable research attention recently. Most of the existing GNNs are designed in euclidean spaces; however, real-world spatial structured data can be non-euclidean surfaces (e.g., hyperbolic spaces). For example, biologists may inspect the geometric shape of a protein surface to determine its interaction with other biomolecules for drug discovery. Although there is growing research on generalizing GNNs to non-euclidean surfaces, the works in these fields are still scarce. In this article, we exploit the graph attention network to learn robust node representations of graphs in hyperbolic spaces. As the gyrovector space framework provides an elegant algebraic formalism for hyperbolic geometry, we utilize this framework to learn the graph representations in hyperbolic spaces. Specifically, we first use the operations defined in the framework to transform the features in a graph; and we exploit the proximity in the product of hyperbolic spaces to model the multi-head attention mechanism in the non-Euclidean setting; afterward, we further devise a parallel strategy using logarithmic and exponential maps to improve the efficiency of our proposed model. The comprehensive experimental results demonstrate the effectiveness of the proposed model, compared with state-of-the-art methods.},
  archive  = {J},
  author   = {Yiding Zhang and Xiao Wang and Chuan Shi and Xunqiang Jiang and Yanfang Ye},
  doi      = {10.1109/TBDATA.2021.3081431},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1690-1701},
  title    = {Hyperbolic graph attention network},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Temporal causal modelling on large volume enterprise data.
<em>IEEE Transactions on Big Data</em>, <em>8</em>(6), 1678–1689. (<a
href="https://doi.org/10.1109/TBDATA.2021.3053879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Structural causal modelling (SCM) with its intervention analysis is one of the promising modelling approach that assists in data driven decision making. SCM not only overcomes the black box modelling associated with most of the classification algorithms but also gives enterprises an opportunity to perform intervention analysis without having to perform randomized controlled experiments. But the large volume of enterprises’ data pose challenges in learning the causal structure as existing algorithms are not suitable to learn from data present in Distributed File System (DFS). Hence algorithm presented in this paper, proposes a novel variation to PC-Stable algorithm to efficiently learn the causal structure from data present in DFS - thus enabling temporal causal modelling on large volume time-series data. The proposed learning algorithm is used to determine the causal story associated with churn in telecommunication industry and flight delay in airline industry. Our model identifies and quantifies the respective causal factors for unfavourable events churn and flight delay.},
  archive  = {J},
  author   = {Ram Mohan and Santanu Chaudhury and Brejesh Lall},
  doi      = {10.1109/TBDATA.2021.3053879},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1678-1689},
  title    = {Temporal causal modelling on large volume enterprise data},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep tensor CCA for multi-view learning. <em>IEEE
Transactions on Big Data</em>, <em>8</em>(6), 1664–1677. (<a
href="https://doi.org/10.1109/TBDATA.2021.3079234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present Deep Tensor Canonical Correlation Analysis (DTCCA), a method to learn complex nonlinear transformations of multiple views (more than two) of data such that the resulting representations are linearly correlated in high order. The high-order correlation of given multiple views is modeled by covariance tensor, which is different from most CCA formulations relying solely on the pairwise correlations. Parameters of transformations of each view are jointly learned by maximizing the high-order canonical correlation. To solve the resulting problem, we reformulate it as the best sum of rank-1 approximation, which can be efficiently solved by existing tensor decomposition method. DTCCA is a nonlinear extension of tensor CCA (TCCA) via deep networks. Comparing with kernel TCCA, DTCCA not only can deal with arbitrary dimensions of the input data, but also does not need to maintain the training data for computing representations of any given data point. Hence, DTCCA as a unified model can efficiently overcome the scalable issue of TCCA for either high-dimensional multi-view data or a large amount of views, and it also naturally extends TCCA for learning nonlinear representation. Extensive experiments on four multi-view data sets demonstrate the effectiveness of the proposed method.},
  archive  = {J},
  author   = {Hok Shing Wong and Li Wang and Raymond Chan and Tieyong Zeng},
  doi      = {10.1109/TBDATA.2021.3079234},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1664-1677},
  title    = {Deep tensor CCA for multi-view learning},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-relational data characterization by tensors: Tensor
inversion. <em>IEEE Transactions on Big Data</em>, <em>8</em>(6),
1650–1663. (<a
href="https://doi.org/10.1109/TBDATA.2021.3079265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Tensors have been applied extensively in engineering and data analytics. Pertinent problems include dimensionality reduction, data mining, data modeling, data learning, etc. Recent research attention has been paid to solve tensor equations. Existing solutions to tensor equations are mostly based on the iterative approach due to lack of sufficient theoretical framework governing how to find the inverse of an arbitrary tensor. In this work, we aim to establish a new theoretical framework missing from the literature so that a new algorithm is devised to determine the exact inverse of an arbitrary tensor, which is beyond the capability of the current iterative algorithms. To solve tensor equations, we classify a tensor as an invertible tensor or a pseudo invertible tensor in the Moore-Penrose’s sense. We present theorems to derive the general formula of both inverse and pseudo inverse of an arbitrary tensor so that the inverse of an arbitrary tensor can be constructed from the tensor itself and its partial inverse. A new tensor inversion algorithm is introduced to carry out the exact inverse or the Moore-Penrose inverse should it not be invertible. Thus, the solution to a tensor equation can be obtained immediately if such an inverse or pseudo inverse is calculated. The main contribution of our proposed approach is that we can always solve any tensor equation while additional restrictions have to be imposed for the existing iterative algorithms to converge on the other hand. The memory- and computational-complexities of our proposed new approach and existing iterative algorithms are also analyzed and compared. In order to demonstrate the applicability of our proposed tensor-inversion algorithm, we apply the tensor-inversion algorithm to the multi-relational data query problem for studying real-world web data. Moreover, numerical experiments are performed to demonstrate the effectiveness of our proposed new approach in terms of convergence together with memory- and computational-complexities.},
  archive  = {J},
  author   = {Shih Yu Chang and Hsiao-Chun Wu},
  doi      = {10.1109/TBDATA.2021.3079265},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1650-1663},
  title    = {Multi-relational data characterization by tensors: Tensor inversion},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-branch decoder network approach to adaptive temporal
data selection and reconstruction for big scientific simulation data.
<em>IEEE Transactions on Big Data</em>, <em>8</em>(6), 1637–1649. (<a
href="https://doi.org/10.1109/TBDATA.2021.3092174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A key challenge in scientific simulation is that the simulation outputs often require intensive I/O and storage space to store the results for effective post hoc analysis. This article focuses on a quality-aware adaptive temporal data selection and reconstruction problem where the goal is to adaptively select simulation data samples at certain key timesteps in situ and reconstruct the discarded samples with quality assurance during post hoc analysis. This problem is motivated by the limitation of current solutions that a significant amount of simulation data samples are either discarded or aggregated during the sampling process, leading to inaccurate modeling of the simulated phenomena. Two unique challenges exist: 1) the sampling decisions have to be made in situ and adapted to the dynamics of the complex scientific simulation data; 2) the reconstruction error must be strictly bounded to meet the application requirement. To address the above challenges, we develop DeepSample , an error-controlled convolutional neural network framework, that jointly integrates a set of coherent multi-branch deep decoders to effectively reconstruct the simulation data with rigorous quality assurance. The results on two real-world scientific simulation applications show that DeepSample significantly outperforms other state-of-the-art methods on both sampling efficiency and reconstructed simulation data quality.},
  archive  = {J},
  author   = {Yang Zhang and Hanqi Guo and Lanyu Shang and Dong Wang and Tom Peterka},
  doi      = {10.1109/TBDATA.2021.3092174},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1637-1649},
  title    = {A multi-branch decoder network approach to adaptive temporal data selection and reconstruction for big scientific simulation data},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerating ML/DL applications with hierarchical caching on
deduplication storage clusters. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(6), 1622–1636. (<a
href="https://doi.org/10.1109/TBDATA.2021.3106345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Large scale machine learning (ML) and deep learning (DL) platforms face challenges when integrated with deduplication enabled storage clusters. In the quest to achieve smart and efficient storage utilization, removal of duplicate data introduces bottlenecks, since deduplication alters the I/O transaction layout of the storage system. Therefore, it is critical to address such deduplication overhead for acceleration of ML/DL computation in deduplication storage. Existing state of the art ML/DL storage solutions such as Alluxio and AutoCache adopt non deduplication-aware caching mechanisms, which lacks the much needed performance boost when adopted in deduplication enabled ML/DL clusters. In this paper, we introduce Redup , which eliminates the performance drop caused by enabling deduplication in ML/DL storage clusters. At the core, is a Redup Caching Manager (RDCM), composed of a 2-tier deduplication layout-aware caching mechanism. The RDCM provides an abstraction of the underlying deduplication storage layout to ML/DL applications and provisions a decoupled acceleration of object reconstruction during ML/DL read operations. Our Redup evaluation shows negligible performance drop in ML/DL training performances as compared to a cluster without deduplication, whilst significantly outperforming Alluxio and AutoCache in terms of various performance metrics.},
  archive  = {J},
  author   = {Prince Hamandawana and Awais Khan and Jongik Kim and Tae-Sun Chung},
  doi      = {10.1109/TBDATA.2021.3106345},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1622-1636},
  title    = {Accelerating ML/DL applications with hierarchical caching on deduplication storage clusters},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A game theoretic approach for addressing domain-shift in
big-data. <em>IEEE Transactions on Big Data</em>, <em>8</em>(6),
1610–1621. (<a
href="https://doi.org/10.1109/TBDATA.2021.3077832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, a novel approach is presented to mitigate the issue of domain shift observed in big-data classification. Since little information is available about the shift, we introduce a ”distortion model”, and obtain additional data-samples to represent the shift. Next, a deep neural network (DNN), referred as ”classifier,” is used to compensate for the shift by learning through these additional samples while maintaining performance on training samples. As the exact magnitude of domain shift is uncertain, we compensate for the optimal expected shift by formulating a zero-sum game. In the proposed game, the distortion model is viewed as the maximizing player which increases the domain shift while the classifier becomes the minimizing player that reduces the impact of domain shift on learning. The Nash solution of the game, whose existence is demonstrated mathematically, provides the domain shift and its optimal adaptation through the classifier. To solve the proposed game for the Nash solution, a direct error-driven learning scheme is introduced where a cost function is derived and solved for each layer in the DNN and the distortion model. Comprehensive mathematical and simulation study is presented to demonstrate the efficacy of the approach.},
  archive  = {J},
  author   = {Krishnan Raghavan and Sarangapani Jagannathan and V. A. Samaranayake},
  doi      = {10.1109/TBDATA.2021.3077832},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1610-1621},
  title    = {A game theoretic approach for addressing domain-shift in big-data},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards efficient local causal structure learning. <em>IEEE
Transactions on Big Data</em>, <em>8</em>(6), 1592–1609. (<a
href="https://doi.org/10.1109/TBDATA.2021.3062937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Local causal structure learning aims to discover and distinguish direct causes (parents) and direct effects (children) of a variable of interest from data. While emerging successes have been made, existing methods need to search a large space to distinguish direct causes from direct effects of a target variable T . To tackle this issue, we propose a novel Efficient Local Causal Structure learning algorithm, named ELCS. Specifically, we first propose the concept of N-structures, then design an efficient Markov Blanket (MB) discovery subroutine to integrate MB learning with N-structures to learn the MB of T and simultaneously distinguish direct causes from direct effects of T . With the proposed MB subroutine, ELCS starts from the target variable, sequentially finds MBs of variables connected to the target variable and simultaneously constructs local causal structures over MBs until the direct causes and direct effects of the target variable have been distinguished. Using eight Bayesian networks the extensive experiments have validated that ELCS achieves better accuracy and efficiency than the state-of-the-art algorithms.},
  archive  = {J},
  author   = {Shuai Yang and Hao Wang and Kui Yu and Fuyuan Cao and Xindong Wu},
  doi      = {10.1109/TBDATA.2021.3062937},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1592-1609},
  title    = {Towards efficient local causal structure learning},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A two-phase method to balance the result of distributed
graph repartitioning. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(6), 1580–1591. (<a
href="https://doi.org/10.1109/TBDATA.2021.3070194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the increase in popularity of graph structured data arising in different areas such as Web, social network, communication network, knowledge graph, etc., there is a growing need for partitioning and repartitioning large graph data in a distributed system. However, the existing graph repartitioning methods are known for poor efficiency in the distributed environment and most of them lack a balance mechanism between edge cut and load balance. In this article, we introduce a new two-phase method to improve the result of distributed graph repartitioning. We first design a local method to identify all the potential candidate vertices that could improve the graph repartitioning result in load balance and edge cut at once in each partition locally. After that, we propose to migrate the selected vertices among the given initial partitions to improve the result of graph repartitioning. During this procedure, we propose to adopt a synchronous vertex migration method to balance both the edge cuts and load balance problems. Extensive experimental results demonstrate that the proposed method is more efficient than the existing methods in several aspects such as communication cost, running time, edge cut, and load balance. We also run SSSP and PageRank applications based on the graph repartitioning result on Giraph to indicate the efficiency of the proposed method.},
  archive  = {J},
  author   = {He Li and Jianbin Huang and Hang Yuan and Jiangtao Cui and Xiaoke Ma and Shaojie Qiao and Xindong Wu},
  doi      = {10.1109/TBDATA.2021.3070194},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1580-1591},
  title    = {A two-phase method to balance the result of distributed graph repartitioning},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EvoSets: Tracking the sensitivity of dimensionality
reduction results across subspaces. <em>IEEE Transactions on Big
Data</em>, <em>8</em>(6), 1566–1579. (<a
href="https://doi.org/10.1109/TBDATA.2021.3079200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Dimensionality reduction is commonly used for identifying and analyzing patterns in the visual analysis of multi-dimensional datasets. The selection of subspaces is a core building block in projecting high-dimensional data to low-dimensional space, which is usually illustrated as a scatterplot for analysts to easily understand and explore. This process involves human prior knowledge and domain-specific requirements. Thus, quantifying and tracking the changes of dimensionality reduction results across subspaces remain challenging. Existing methods can neither quantify the subsets-based changes of dimensionality reduction results when switching subspaces, nor automatically and comprehensively display the overall and subtle differences among dimensionality reduction results. To address this, we developed EvoSets , a novel visual analytics system designed to help users understand how subspaces affect dimensionality reduction results. The effects are quantified based on the distribution of subsets within projections to tracking the sensitivity of dimensionality reduction results across subspaces. In addition, the system supports the exploration of the overall evolution of the dimensionality reduction results for helping users track the convergence and divergence behavior changes of subsets based on an extended Bubble Sets visualization. Similarities are intuitively illustrated, and dissimilarities are highlighted among the generated dimensionality reduction results across subspaces based on different layout constraints. The usefulness and effectiveness of the system are further evaluated with a user study and two case studies on multi-dimensional datasets.},
  archive  = {J},
  author   = {Guodao Sun and Sujia Zhu and Qi Jiang and Wang Xia and Ronghua Liang},
  doi      = {10.1109/TBDATA.2021.3079200},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1566-1579},
  title    = {EvoSets: Tracking the sensitivity of dimensionality reduction results across subspaces},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-Traj2Graph identifying fine-grained driving style with
GPS trajectory data via multi-task learning. <em>IEEE Transactions on
Big Data</em>, <em>8</em>(6), 1550–1565. (<a
href="https://doi.org/10.1109/TBDATA.2021.3063048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Driving behaviour understanding is of vital importance in improving transportation safety and promoting the development of Intelligent Transportation Systems (ITS). As a long-standing research topic in driving behaviour analysis, driving style identification is non-trivial. Almost all previous studies emphasize the research on the granularity of an entire trip or a driver. Inspired by the fact that an aggressive driver may drive safely at some time. In this article, based on the widely available GPS trajectory big data that records the driving behaviours implicitly, we propose a multi-task learning (MTL) framework called semi-Traj2Graph to recognize the fine-grained driving styles in the temporal dimension accurately. The MTL framework can incorporate the learning capability of graph representation in extracting high-level and interpretable features regarding complex driving behaviours and semi-supervised in exploiting unlabelled data and reducing labelling effort. More specifically, in the graph representation learning, a multi-view graph is first built to capture a more complete view of driving behaviours from the raw GPS trajectory data, then graph convolutional neural networks (Graph-CNNs) are applied. In the semi-supervised learning, a pseudo-label labelling is adopted to make use of the unlabelled data. We evaluate the proposed framework extensively based on two taxi trajectory datasets collected from the city of Beijing and Chongqing, China, respectively. Experimental results show that semi-Traj2Graph outperforms compared to other baselines, achieving an overall accuracy of around 90 percent. We also implement the framework on users’ smartphones via the collaborative cloud-edge computation manner to demonstrate the system usability in real cases.},
  archive  = {J},
  author   = {Chao Chen and Qiang Liu and Xingchen Wang and Chengwu Liao and Daqing Zhang},
  doi      = {10.1109/TBDATA.2021.3063048},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1550-1565},
  title    = {Semi-Traj2Graph identifying fine-grained driving style with GPS trajectory data via multi-task learning},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mining diversified top-&lt;inline-formula&gt;&lt;tex-math
notation=“LaTeX”&gt;<span
class="math inline"><em>r</em></span>&lt;/tex-math&gt;&lt;mml:math
xmlns:mml=“http://www.w3.org/1998/math/MathML”&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic
xlink:href=“yuan-ieq1-3058294.gif”
xmlns:xlink=“http://www.w3.org/1999/xlink”/&gt;&lt;/inline-formula&gt;
lasting cohesive subgraphs on temporal networks. <em>IEEE Transactions
on Big Data</em>, <em>8</em>(6), 1537–1549. (<a
href="https://doi.org/10.1109/TBDATA.2021.3058294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Temporal subgraph mining is recently ubiquitous. Identifying diversified and lasting ingredients is a fundamental problem in analyzing temporal networks. In this paper, we investigate the problem of finding diversified lasting cohesive subgraphs from temporal networks. Specifically, we first introduce a new model, called maximal lasting &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$(k,\sigma)$&lt;/tex-math&gt;&lt;/inline-formula&gt; -core, for characterizing lasting cohesive subgraphs on temporal networks so as to the nodes in the subgraph are connected densely and also the subgraph&#39;s structure remains unchanged for a period of time. To enhance the diversity of results, we then formulate a diversified lasting cohesive subgraphs problem, which finds &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$r$&lt;/tex-math&gt;&lt;/inline-formula&gt; maximal lasting &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$(k,\sigma)$&lt;/tex-math&gt;&lt;/inline-formula&gt; -cores with maximum coverage regarding the number of vertices and timestamps. Unfortunately, we show that the optimization problem is NP-hard. To tackle this issue, we first devise a greedy algorithm named GreLC with (1-1/e) approximation ratio. However, GreLC has prohibitively high time and space complexity, resulting in poor scalability. Then, an improved DFS-based search algorithm called TopLC with 1/4 approximation ratio is proposed to lower the computational cost. Finally, empirical studies on six real-world temporal networks demonstrate that the proposed solutions perform efficiently and accurately, and our model is better than temporal cohesive subgraphs detected by existing approaches.},
  archive  = {J},
  author   = {Longlong Lin and Pingpeng Yuan and Rong-Hua Li and Hai Jin},
  doi      = {10.1109/TBDATA.2021.3058294},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1537-1549},
  title    = {Mining diversified top-&lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$r$&lt;/tex-math&gt;&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=&quot;yuan-ieq1-3058294.gif&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;/&gt;&lt;/inline-formula&gt; lasting cohesive subgraphs on temporal networks},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical particle swarm optimization-incorporated latent
factor analysis for large-scale incomplete matrices. <em>IEEE
Transactions on Big Data</em>, <em>8</em>(6), 1524–1536. (<a
href="https://doi.org/10.1109/TBDATA.2021.3090905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A Stochastic Gradient Descent (SGD)-based Latent Factor Analysis (LFA) model is highly efficient in representative learning on a High-Dimensional and Sparse (HiDS) matrix, where the learning rate adaptation is vital in its efficiency and practicability. The learning rate adaptation of an SGD-based LFA model can be achieved efficiently by learning rate evolution with an evolutionary computing algorithm. However, a resultant model commonly suffers from twofold premature convergence issues, i.e., a) the premature convergence of the learning rate swarm relying on an evolution algorithm, and b) the premature convergence of an LFA model relying on the compound effects of evolution-based learning rate adaptation and adopted optimization algorithm. Aiming at addressed such issues, this work proposes an H ierarchical P article swarm optimization-incorporated L atent factor analysis (HPL) model with a two-layered structure. The first layer pre-trains desired latent factors with a position-transitional particle swarm optimization-based LFA model with learning rate adaptation; while the second layer performs latent factor refinement with a newly-proposed mini-batch particle swarm optimization algorithm. Experimental results on four HiDS matrices generated by industrial applications demonstrate that an HPL model can well handle the mentioned premature convergence issues, thereby achieving highly-accurate representation to HiDS matrices.},
  archive  = {J},
  author   = {Jia Chen and Xin Luo and MengChu Zhou},
  doi      = {10.1109/TBDATA.2021.3090905},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1524-1536},
  title    = {Hierarchical particle swarm optimization-incorporated latent factor analysis for large-scale incomplete matrices},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding and conquering the difficulties in identifying
third-party libraries from millions of android apps. <em>IEEE
Transactions on Big Data</em>, <em>8</em>(6), 1511–1523. (<a
href="https://doi.org/10.1109/TBDATA.2021.3093244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the thriving of the Android ecosystem, codes are widely reused in Android apps in the form of third-party libraries. Recent research shows that emerging third-party libraries may introduce a lot of privacy risks and other security threats. Nevertheless, current approaches on libraries identification are far away from the demand for accuracy and efficiency. In this article, we present LibHawkeye, a new clustering-based technique to identify third-party libraries in millions of Android apps. Our approach utilizes four different kinds of dependencies inside Android apps to build intra-app dependency graphs but discards package homogeny which is heavily depended upon by most previous works. What’s more, we propose three steps of refinement to eliminate false positives in the initial result as much as possible. The experiment on 1,000 apps reports that compared to existing tools, LibHawkeye can precisely identify at least 26.5 percent more libraries. We also evaluate it with 3,987,206 Android apps published in Google Play, and the accuracy of sampled libraries from the clustering result is 93.25 percent. Results show that LibHawkeye significantly outperforms the state-of-the-art tools without loss of scalability.},
  archive  = {J},
  author   = {Yanghua Zhang and Jice Wang and Hexiang Huang and Yuqing Zhang and Peng Liu},
  doi      = {10.1109/TBDATA.2021.3093244},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1511-1523},
  title    = {Understanding and conquering the difficulties in identifying third-party libraries from millions of android apps},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the merge of k-NN graph. <em>IEEE Transactions on Big
Data</em>, <em>8</em>(6), 1496–1510. (<a
href="https://doi.org/10.1109/TBDATA.2021.3101517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {k -nearest neighbor graph is a fundamental data structure in many disciplines such as information retrieval, data-mining, pattern recognition, and machine learning, etc. In the literature, considerable research has been focusing on how to efficiently build an approximate k -nearest neighbor graph ( k -NN graph) for a fixed dataset. Unfortunately, a closely related issue of how to merge two existing k -NN graphs has been overlooked. In this paper, we address the issue of k -NN graph merging in two different scenarios. In the first scenario, a symmetric merge algorithm is proposed to combine two approximate k -NN graphs. The algorithm facilitates large-scale processing by the efficient merging of k -NN graphs that are produced in parallel. In the second scenario, a joint merge algorithm is proposed to expand an existing k -NN graph with a raw dataset. The algorithm enables the incremental construction of a hierarchical approximate k -NN graph. Superior performance is attained when leveraging the hierarchy for NN search of various data types, dimensionality, and distance measures.},
  archive  = {J},
  author   = {Wan-Lei Zhao and Hui Wang and Peng-Cheng Lin and Chong-Wah Ngo},
  doi      = {10.1109/TBDATA.2021.3101517},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1496-1510},
  title    = {On the merge of k-NN graph},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Long-term origin-destination demand prediction with graph
deep learning. <em>IEEE Transactions on Big Data</em>, <em>8</em>(6),
1481–1495. (<a
href="https://doi.org/10.1109/TBDATA.2021.3063553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Accurate long-term origin-destination demand (OD) prediction can help understand traffic flow dynamics, which plays an essential role in urban transportation planning. However, the main challenge originates from the complex and dynamic spatial-temporal correlation of the time-varying traffic information. In response, a graph deep learning model for long-term OD prediction (ST-GDL) is proposed in this article, which is among the pioneering work that obtains both short-term and long-term OD predictions simultaneously. ST-GDL avoids the conventional multi-step forecasting and thus prevents learning from prediction errors, rendering better long-term forecasts. The proposed method captures time attributes from multiple time scales, namely closeness, periodicity, and trend, to study the features with temporal dynamics. In addition, two gate mechanisms are introduced over the vanilla convolution operation to alleviates the error accumulation issue of typical recurrent forecast in long-term OD prediction. A method based on graph convolution is proposed to capture the dynamic spatial relationship, which projects the transportation network into a graphical time-series. Finally, the long-term OD prediction results are obtained by combining the extracted spatio-temporal features with external features from the meteorological information. Case studies on practical datasets show that the proposed model is superior to existing methods in long-term OD prediction problems.},
  archive  = {J},
  author   = {Xiexin Zou and Shiyao Zhang and Chenhan Zhang and James J.Q. Yu and Edward Chung},
  doi      = {10.1109/TBDATA.2021.3063553},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1481-1495},
  title    = {Long-term origin-destination demand prediction with graph deep learning},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An overview of healthcare data analytics with applications
to the COVID-19 pandemic. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(6), 1463–1480. (<a
href="https://doi.org/10.1109/TBDATA.2021.3103458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the era of big data, standard analysis tools may be inadequate for making inference and there is a growing need for more efficient and innovative ways to collect, process, analyze and interpret the massive and complex data. We provide an overview of challenges in big data problems and describe how innovative analytical methods, machine learning tools and metaheuristics can tackle general healthcare problems with a focus on the current pandemic. In particular, we give applications of modern digital technology, statistical methods,data platforms and data integration systems to improve diagnosis and treatment of diseases in clinical research and novel epidemiologic tools to tackle infection source problems, such as finding Patient Zero in the spread of epidemics. We make the case that analyzing and interpreting big data is a very challenging task that requires a multi-disciplinary effort to continuously create more effective methodologies and powerful tools to transfer data information into knowledge that enables informed decision making.},
  archive  = {J},
  author   = {Zhe Fei and Yevgen Ryeznik and Oleksandr Sverdlov and Chee Wei Tan and Weng Kee Wong},
  doi      = {10.1109/TBDATA.2021.3103458},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1463-1480},
  title    = {An overview of healthcare data analytics with applications to the COVID-19 pandemic},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Frequent subgraph mining algorithms in static and temporal
graph-transaction settings: A survey. <em>IEEE Transactions on Big
Data</em>, <em>8</em>(6), 1443–1462. (<a
href="https://doi.org/10.1109/TBDATA.2021.3072001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Networks are known as perfect tools for modeling various types of systems. In the literature of network mining, frequent subgraph mining is considered as the essence of mining network data. In this problem, the dataset is composed of networks representing multiple independent systems or one system at multiple time stamps. The cores of mining frequent subgraphs are graph and subgraph isomorphism. Due to the complexities of these problems, the frequent subgraph mining algorithms proposed in the literature employ various heuristics for candidate generation, duplicate subgraphs pruning, and support computation. In this survey, we provide a classification of proposed algorithms in the literature. The algorithms for static networks have found numerous applications. Therefore, these algorithms will be reviewed in detail. Besides, it is discussed that consideration of temporality of data can impact the derived insight and attracted substantial attention in recent years. However, prior surveys have not comprehensively examined the algorithms of frequent subgraph mining in a database of temporal networks represented as network snapshots. Therefore, the algorithms proposed for mining frequent subgraphs in temporal networks are reviewed. Moreover, most of the surveys have focused on main-memory algorithms. Here, we review disk-based, parallel, and distributed algorithms proposed for mining frequent subgraphs.},
  archive  = {J},
  author   = {Ali Jazayeri and Christopher C. Yang},
  doi      = {10.1109/TBDATA.2021.3072001},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1443-1462},
  title    = {Frequent subgraph mining algorithms in static and temporal graph-transaction settings: A survey},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on integrity auditing for data storage in the
cloud: From single copy to multiple replicas. <em>IEEE Transactions on
Big Data</em>, <em>8</em>(5), 1428–1442. (<a
href="https://doi.org/10.1109/TBDATA.2020.3029209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The rapid advancement of cloud computing has promoted the development of cloud storage services. One of the biggest concerns of cloud users is whether the completeness and recoverability of data can be guaranteed when cloud servers encounter problems. Only when the integrity of data is fully guaranteed can users consume cloud storage with confidence, especially in a complicated cloud environment with multiple clouds. However, the literature still lacks a thorough survey on cloud data integrity auditing for both single copy and multiple replicas. In this article, we survey and compare existing auditing schemes for single copy and multiple replicas based on a set of criteria. Based on our review and analysis, we discuss open issues, potential applications and future directions in the field of the integrity auditing in the cloud, including the implications of such trendy topics as merging blockchain and edge computing into data integrity auditing.},
  archive  = {J},
  author   = {Angtai Li and Yu Chen and Zheng Yan and Xiaokang Zhou and Shohei Shimizu},
  doi      = {10.1109/TBDATA.2020.3029209},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1428-1442},
  title    = {A survey on integrity auditing for data storage in the cloud: From single copy to multiple replicas},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RCIVMM: A route choice-based interactive voting map matching
approach for complex urban road networks. <em>IEEE Transactions on Big
Data</em>, <em>8</em>(5), 1414–1427. (<a
href="https://doi.org/10.1109/TBDATA.2021.3057095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the enhancement of location-acquisition technologies, GPS trajectories play an essential role in data-driven intelligent transportation applications, which requires an accurate approach to match raw GPS trajectories to road segments on a digital map. However, for complex urban roads containing elevated roads and surface roads, map matching for low-frequency GPS data is still challenging. This article aims to address the biases and instability problem in existing approaches. To this end, we combine the spatial-temporal characteristics of GPS data in complex roads with driving behaviours and present a novel global map matching method including truncated density clustering algorithm, statistic features based spatial-temporal analysis, and driving-behaviour-based track modification. Additionally, a weighted-matrix based interactive voting algorithm is proposed to select the best results from a global perspective. The experiments are conducted with two real GPS trajectory datasets under three road conditions. The result shows that our approach outperforms state-of-art approaches for urban complex road networks in both accuracy and efficiency.},
  archive  = {J},
  author   = {Yaying Zhang and Xinyuan Sui},
  doi      = {10.1109/TBDATA.2021.3057095},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1414-1427},
  title    = {RCIVMM: A route choice-based interactive voting map matching approach for complex urban road networks},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient and privacy preserving approximation of
distributed statistical queries. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(5), 1399–1413. (<a
href="https://doi.org/10.1109/TBDATA.2021.3052516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, an increasing amount of data is collected in different and often, not cooperative, databases. The problem of privacy-preserving, distributed calculations over separate databases and, a relative to it, the issue of private data release was intensively investigated. However, despite a considerable progress, computational complexity and consequently, the performance of the computations, due to an increasing size of data, remains a limiting factor in real-world deployments. Especially in the case of privacy-preserving computations. In this paper, we suggest sampling as a method of improving computational performance. Sampling was a topic of extensive research in the past that recently received a boost of interest. We provide a sampling method targeted at separate, non-collaborating, vertically partitioned datasets. The method is exemplified and tested on an approximation of intersection set both with and without a privacy-preserving mechanism. An analysis of the bound on the error as a function of the sample size is discussed and a heuristic algorithm is suggested to further improve the performance. The algorithms were implemented and experimental results confirm the validity of the approach.},
  archive  = {J},
  author   = {Philip Derbeko and Shlomi Dolev and Ehud Gudes and Jeffrey D. Ullman},
  doi      = {10.1109/TBDATA.2021.3052516},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1399-1413},
  title    = {Efficient and privacy preserving approximation of distributed statistical queries},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A knowledge-enhanced multi-view framework for drug-target
interaction prediction. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(5), 1387–1398. (<a
href="https://doi.org/10.1109/TBDATA.2021.3051673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Motivation: The prediction of drug-target interaction (DTI) from heterogeneous biological data is critical to predict drugs and therapeutic targets for known diseases such as tumor and bowel disease. The study of DTI based on drug representation learning can strengthen or integrate our knowledge of pharmacological and chemical phenomena. Therefore, there is a strong motivation to develop effective methods that can detect these potential drug-target interactions. Results: We have developed a novel Knowledge-Enhanced Multi-View framework (KEMV) to predict unknown DTIs from pharmacological data and chemical data on a large scale. The proposed method consists of two steps: (i) learning more comprehensive drug representations via the proposed multi-view attention mechanism, which bridges pharmacological and chemical information, and interactively summarizes the attention values depending on varying interactions between different pairs of drug features. (ii) predicting unknown drug-target interactions based on the drug and target representations. The method is tested on real-world dataset KEGG with three classes of important drug–target interactions involving enzymes, ion channels, and G-protein-coupled receptors. Our framework is proven to uncover potential DTIs with scientific evidences explaining the mechanism of the interactions through the processing of high-dimensional, heterogeneous, and sparse drug data. Availability: The originality of the proposed method lies in the attentive integration of pharmacological and chemical information for representation of drug candidate compounds and the prediction of drug-target interaction toward drug discovery in a unified framework. Our results are reproducible and and code is available at: https://github.com/YuanKQ/DTI-Prediction .},
  archive  = {J},
  author   = {Ying Shen and Yilin Zhang and Kaiqi Yuan and Dagang Li and Haitao Zheng},
  doi      = {10.1109/TBDATA.2021.3051673},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1387-1398},
  title    = {A knowledge-enhanced multi-view framework for drug-target interaction prediction},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DMP: Content delivery with dynamic movement pattern in
vehicular networks. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(5), 1371–1386. (<a
href="https://doi.org/10.1109/TBDATA.2021.3050777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Vehicular ad-hoc networks (VANETs) have been widely studied in intelligent transportation. Content delivery is an important topic that attracts many researchers. Due to vehicles that may have intermittent connections and uncertain routes, it is difficult to select an appropriate node. In this paper, we analyze the movement pattern of vehicles from real taxis’ trajectories and propose a framework for delivery prediction, which aims to select appropriate nodes. First, we propose the framework which consists of a contact clique model, a social clique model, and a prediction model based on Markov chains, to characterize the movement pattern of vehicles. Second, we capture dynamic movement patterns by dividing the time requirement into equal length slots and construct clique sequences. Based on the fact that the sociality of nodes has strong temporal correlations, we utilize the prediction model to derive future cliques and evaluate two kinds of delivery performance in the future. Finally, we design a content delivery algorithm with dynamic movement pattern (DMP) to select the appropriate node. In our experiment, DMP performs better than that of other methods in terms of overhead, average hops. Also, as the number of nodes increases, our algorithm keeps small fluctuations in node sociality.},
  archive  = {J},
  author   = {Weiyi Huang and Peng Li and Bo Li and Tao Zhang},
  doi      = {10.1109/TBDATA.2021.3050777},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1371-1386},
  title    = {DMP: Content delivery with dynamic movement pattern in vehicular networks},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A memory network information retrieval model for
identification of news misinformation. <em>IEEE Transactions on Big
Data</em>, <em>8</em>(5), 1358–1370. (<a
href="https://doi.org/10.1109/TBDATA.2020.3048961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The speed and volume at which misinformation spreads on social media have motivated efforts to automate fact-checking which begins with stance detection. For fake news stance detection, for example, many classification-based models have been proposed often with high complexity and hand-crafted features. Although these models can achieve high accuracy scores on a targeted small corpus of fake news, few are evaluated on a larger corpus of fake and conspiracy sites due to efficiency limitations and the lack of compatibility with the actual fact-checking process. In this article, we propose a practical two-stage stance detection model that is tailored to the real-life problem. Specifically, we integrate an information retrieval system with an end to end memory network model to sort articles based on their relevance to the claim and then identify the fine-grained stance of each relevant article towards its given claim. We evaluate our model on the Fake News Challenge dataset (FNC-1). The results show that the performance of our model is comparable to those of the state-of-the-art models, average weighted accuracy of 82.1, while it closely follows the real-life process of fact-checking. We also validate our model with a large dataset from a real-life fact-checking website (i.e., Snopes.com ), and the findings demonstrate the capability of the model in distinguishing false from true news headlines.},
  archive  = {J},
  author   = {Nima Ebadi and Mohsen Jozani and Kim-Kwang Raymond Choo and Paul Rad},
  doi      = {10.1109/TBDATA.2020.3048961},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1358-1370},
  title    = {A memory network information retrieval model for identification of news misinformation},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). State-aware load shedding from input event streams in
complex event processing. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(5), 1340–1357. (<a
href="https://doi.org/10.1109/TBDATA.2020.3047438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In complex event processing (CEP), load shedding is performed to maintain a given latency bound during overload situations when there is a limitation on resources. However, shedding load implies degradation in the quality of results (QoR). Therefore, it is crucial to perform load shedding in a way that has the lowest impact on QoR. Researchers, in the CEP domain, propose to drop either events or partial matches (PMs) in overload cases. They assign utilities to events or PMs by considering either the importance of events or the importance of PMs but not both together. In this article, we combine these approaches where we propose to assign a utility to an event by considering both the event importance and the importance of PMs. We propose two load shedding approaches for CEP systems. The first approach drops events from PMs, while the second approach drops events from windows. We adopt a probabilistic model that uses the type and position of an event in a window and the state of a PM to assign a utility to an event. We, also, propose an approach to predict a utility threshold that is used to drop the required amount of events to maintain a given latency bound. By extensive evaluations on two real-world datasets and several representative queries, we show that, in the majority of cases, our load shedding approach outperforms state-of-the-art load shedding approaches, w.r.t. QoR.},
  archive  = {J},
  author   = {Ahmad Slo and Sukanya Bhowmik and Kurt Rothermel},
  doi      = {10.1109/TBDATA.2020.3047438},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1340-1357},
  title    = {State-aware load shedding from input event streams in complex event processing},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting fine-grained air quality based on deep neural
networks. <em>IEEE Transactions on Big Data</em>, <em>8</em>(5),
1326–1339. (<a
href="https://doi.org/10.1109/TBDATA.2020.3047078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Nowadays, many cities are suffering from air pollution problems, which endangered the health of the young and elderly for breathing problems. For supporting the government’s policy-making and people’s decision making, it is important to predict future fine-grained air quality. In this article, we predict the air quality of the next 48 hours for each monitoring station and the daily average air quality of the next 7 days for a city, considering air quality data, meteorology data, and weather forecast data. Based on the domain knowledge about air pollution, we propose a deep neural network based approach, entitled DeepAir. Our approach consists of a deep distributed fusion network for station-level short-term prediction and a deep cascaded fusion network for the city-level long-term forecast. With the data transformation preprocessing, the former network adopts a neural distributed architecture to fuse heterogeneous urban data for simultaneously capturing the direct and indirect factors affecting air quality. The latter network takes a neural cascaded architecture to learn the dynamic influences from previously existing data and future predicted data on future air quality. We have deployed a real-time system on the cloud, providing fine-grained air quality forecasts for 300+ Chinese cities every hour. Our system mainly consists of three components: data crawler, task scheduler, and prediction model, which are implemented with a multi-task architecture to improve the system’s efficiency and stability. Based on the datasets from three-year nine Chinese cities, experimental results demonstrate the advantages of our proposed method.},
  archive  = {J},
  author   = {Xiuwen Yi and Zhewen Duan and Ruiyuan Li and Junbo Zhang and Tianrui Li and Yu Zheng},
  doi      = {10.1109/TBDATA.2020.3047078},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1326-1339},
  title    = {Predicting fine-grained air quality based on deep neural networks},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple-perspective clustering of passive wi-fi sensing
trajectory data. <em>IEEE Transactions on Big Data</em>, <em>8</em>(5),
1312–1325. (<a
href="https://doi.org/10.1109/TBDATA.2020.3045154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Information about the spatiotemporal flow of humans within an urban context has a wide plethora of applications. Currently, although there are many different approaches to collect such data, there lacks a standardized framework to analyze it. The focus of this article is on the analysis of the data collected through passive Wi-Fi sensing, as such passively collected data can have a wide coverage at low cost. We propose a systematic approach by using unsupervised machine learning methods, namely &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; -means clustering and hierarchical agglomerative clustering (HAC) to analyze data collected through such a passive Wi-Fi sniffing method. We examine three aspects of clustering of the data, namely by time, by person, and by location, and we present the results obtained by applying our proposed approach on a real-world dataset collected over five months.},
  archive  = {J},
  author   = {Zann Koh and Yuren Zhou and Billy Pik Lik Lau and Chau Yuen and Bige Tunçer and Keng Hua Chong},
  doi      = {10.1109/TBDATA.2020.3045154},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1312-1325},
  title    = {Multiple-perspective clustering of passive wi-fi sensing trajectory data},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SCCS: Smart cloud commuting system with shared autonomous
vehicles. <em>IEEE Transactions on Big Data</em>, <em>8</em>(5),
1301–1311. (<a
href="https://doi.org/10.1109/TBDATA.2020.3041263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emergence of autonomous vehicles (AVs) offers the potential to fundamentally transform the way how urban transport systems be designed and deployed, and alter the way we view private car ownership. In this article we advocate a forward-looking, ambitious and disruptive smart cloud commuting system (SCCS) for future smart cities based on shared AVs. Employing giant pools of AVs of varying sizes, SCCS seeks to supplant and integrate various modes of transport – most of personal vehicles, low ridership public buses, and taxis used in today’s private and public transport systems – in a unified, on-demand fashion, and provides passengers with a fast, convenient, and low cost transport service for their daily commuting needs. To explore feasibility and efficiency gains of the proposed SCCS, we model SCCS as a queueing system with passengers’ trip demands (as jobs) being served by the AVs (as servers). Using a 1-year real trip dataset from Shenzhen China, we quantify (i) how design choices, such as the numbers of depots and AVs, affect the passenger waiting time and vehicle utilization; and (ii) how much efficiency gains (i.e., reducing the number of service vehicles, and improving the vehicle utilization) can be obtained by SCCS comparing to the current taxi system. Our results demonstrate that the proposed SCCS framework can serve the trip demands with 22 percent fewer vehicles and 37 percent more vehicle utilization, which shed lights on the design feasibility of future smart transportation systems.},
  archive  = {J},
  author   = {Menghai Pan and Yanhua Li and Zhi-Li Zhang and Jun Luo},
  doi      = {10.1109/TBDATA.2020.3041263},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1301-1311},
  title    = {SCCS: Smart cloud commuting system with shared autonomous vehicles},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). CGAIL: Conditional generative adversarial imitation
learning—an application in taxi drivers’ strategy learning. <em>IEEE
Transactions on Big Data</em>, <em>8</em>(5), 1288–1300. (<a
href="https://doi.org/10.1109/TBDATA.2020.3039810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Smart passenger-seeking strategies employed by taxi drivers contribute not only to drivers’ incomes, but also higher quality of service passengers received. Therefore, understanding taxi drivers’ behaviors and learning the good passenger-seeking strategies are crucial to boost taxi drivers’ well-being and public transportation quality of service. However, we observe that drivers’ preferences of choosing which area to find the next passenger are diverse and dynamic across locations and drivers. It is hard to learn the location-dependent preferences given the partial data (i.e., an individual driver’s trajectory may not cover all locations). In this article, we make the first attempt to develop conditional generative adversarial imitation learning (cGAIL) model, as a unifying collective inverse reinforcement learning framework that learns the driver’s decision-making preferences and policies by transferring knowledge across taxi driver agents and across locations. Our evaluation results on three months of taxi GPS trajectory data in Shenzhen, China, demonstrate that the driver’s preferences and policies learned from cGAIL are on average 36.2 percent more accurate than those learned from other state-of-the-art baseline approaches.},
  archive  = {J},
  author   = {Xin Zhang and Yanhua Li and Xun Zhou and Jun Luo},
  doi      = {10.1109/TBDATA.2020.3039810},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1288-1300},
  title    = {CGAIL: Conditional generative adversarial imitation Learning—An application in taxi drivers’ strategy learning},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DAAC: Digital asset access control in a unified blockchain
based e-health system. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(5), 1273–1287. (<a
href="https://doi.org/10.1109/TBDATA.2020.3037914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The use of the Internet of Things and modern technologies has boosted the expansion of e-health solutions significantly and allowed access to better health services and remote monitoring of patients. Every service provider usually implements its information system to manage and access patient data for its unique purpose. Hence, the interoperability among independent e-health service providers is still a major challenge. From the structure of stored data to its large volume, the design of each such big data system varies, hence the cooperation among different e-health systems is almost impossible. In addition to this, the security and privacy of patient information is a challenging task. Building a unified solution for all creates significant business and economic issues. In this article, we present a solution to migrate existing e-health systems to a unified Blockchain-based model, where access to large scale medical data of patients can be achieved seamlessly by any service provider. A core blockchain network connects individual &amp; independent e-health systems without requiring them to modify their internal processes. Access to patient data in the form of digital assets stored in off-chain storage is controlled through patient-centric channels and policy transactions. Through emulation, we show that the proposed solution can interconnect different e-health systems efficiently.},
  archive  = {J},
  author   = {Sujit Biswas and Kashif Sharif and Fan Li and Iqbal Alam and Saraju P. Mohanty},
  doi      = {10.1109/TBDATA.2020.3037914},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1273-1287},
  title    = {DAAC: Digital asset access control in a unified blockchain based E-health system},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Memory scaling of cloud-based big data systems: A hybrid
approach. <em>IEEE Transactions on Big Data</em>, <em>8</em>(5),
1259–1272. (<a
href="https://doi.org/10.1109/TBDATA.2020.3035522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {When deploying applications with dynamic and intensive memory footprint to big data systems on public clouds, one important yet challenging question to answer is how to select a specific instance type whose memory capacity is large enough to prevent out-of-memory errors while the cost is minimized without violating performance requirements. The state-of-the-practice solution is trial and error, causing both performance overhead and additional monetary cost. This article investigates two memory scaling mechanisms in public clouds: physical memory (good performance and high cost) and virtual memory (degraded performance and no additional cost). In order to analyze the trade-off between performance and cost of the two scaling options, a performance-cost model is developed that is driven by a lightweight analytic prediction approach through a compact representation of the memory footprint. In addition, for those scenarios when the footprint is unavailable, a meta-model-based prediction method is proposed using just-in-time migration mechanisms. The proposed techniques have been extensively evaluated with various benchmarks and real-world applications on Amazon Web Services: the performance-cost model is highly accurate and the proposed just-in-time migration approach reduces the monetary cost by up to 66 percent.},
  archive  = {J},
  author   = {Xinying Wang and Cong Xu and Ke Wang and Feng Yan and Dongfang Zhao},
  doi      = {10.1109/TBDATA.2020.3035522},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1259-1272},
  title    = {Memory scaling of cloud-based big data systems: A hybrid approach},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An effective 2-dimension graph partitioning for work
stealing assisted graph processing on multi-FPGAs. <em>IEEE Transactions
on Big Data</em>, <em>8</em>(5), 1247–1258. (<a
href="https://doi.org/10.1109/TBDATA.2020.3035090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multi-FPGA architectures have gained great interests in accelerating large-scale graph processing with great potential on high throughput and energy efficiency. As a beneficial complement, work stealing functions effectively to balance the computational workload on different FPGAs dynamically. Unfortunately, existing graph partitioning schemes originally designed in distributed settings potentially mismatch the work stealing-enabled multi-FPGA situations, where the computation is balanced while the communication overhead is unprecedentedly significant. In this paper, we present a 2-dimension balanced graph partitioning for work stealing assisted graph systems on multi-FPGAs, which can reduce communication overhead while preserving the optimal performance of work stealing. Our approach is novel by 1) exploring the tradeoff between load balance dimension and communication dimension in work-stealing-enabled graph processing system for the optimal performance, and 2) optimizing the memory access sequences to improve the granularity of graph partitioning for high-throughput graph analytics. Our experimental results show that our approach achieves 1.63x &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\sim$&lt;/tex-math&gt;&lt;/inline-formula&gt; 2.56x speedups compared with state-of-the-art FPGA-based graph processing systems.},
  archive  = {J},
  author   = {Fan Zhang and Long Zheng and Xiaofei Liao and Xinqiao Lv and Hai Jin and Jiang Xiao},
  doi      = {10.1109/TBDATA.2020.3035090},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1247-1258},
  title    = {An effective 2-dimension graph partitioning for work stealing assisted graph processing on multi-FPGAs},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collaborative filtering with network representation learning
for citation recommendation. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(5), 1233–1246. (<a
href="https://doi.org/10.1109/TBDATA.2020.3034976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Citation recommendation plays an important role in the context of scholarly big data, where finding relevant papers has become more difficult because of information overload. Applying traditional collaborative filtering (CF) to citation recommendation is challenging due to the cold start problem and the lack of paper ratings. To address these challenges, in this article, we propose a collaborative filtering with network representation learning framework for citation recommendation, namely CNCRec, which is a hybrid user-based CF considering both paper content and network topology. It aims at recommending citations in heterogeneous academic information networks. CNCRec creates the paper rating matrix based on attributed citation network representation learning, where the attributes are topics extracted from the paper text information. Meanwhile, the learned representations of attributed collaboration network is utilized to improve the selection of nearest neighbors. By harnessing the power of network representation learning, CNCRec is able to make full use of the whole citation network topology compared with previous context-aware network-based models. Extensive experiments on both DBLP and APS datasets show that the proposed method outperforms state-of-the-art methods in terms of precision, recall, and MRR (Mean Reciprocal Rank). Moreover, CNCRec can better solve the data sparsity problem compared with other CF-based baselines.},
  archive  = {J},
  author   = {Wei Wang and Tao Tang and Feng Xia and Zhiguo Gong and Zhikui Chen and Huan Liu},
  doi      = {10.1109/TBDATA.2020.3034976},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1233-1246},
  title    = {Collaborative filtering with network representation learning for citation recommendation},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Domain adaptive network embedding. <em>IEEE Transactions on
Big Data</em>, <em>8</em>(5), 1220–1232. (<a
href="https://doi.org/10.1109/TBDATA.2020.3034201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent works reveal that network embedding techniques enable many machine learning models to handle diverse downstream tasks on graph-structured data. However, as previous methods usually focus on learning embedding for a single network, they cannot learn representations transferable on multiple networks. Hence, it is important to design a network embedding algorithm that supports downstream model transferring on different networks, known as domain adaptation. In this article, we propose a Domain Adaptive Network Embedding framework, which applies Graph Convolutional Network to learn transferable embedding. In DANE, nodes from multiple networks are encoded to vectors via a shared and aligned embedding space. The distribution of embedding on different networks are further aligned by Adversarial Learning Regularization. To achieve better performance in scenarios where labels are provided, DANE adopts a cross-entropy error term of the GCN framework and class centroid aligning method. Moreover, DANE&#39;s advantages in learning transferable network embedding can be guaranteed theoretically. Extensive experiments reflect that the proposed framework outperforms other well-recognized network embedding baselines in cross-network domain adaptation tasks, and the semi-supervised components improve the performance significantly.},
  archive  = {J},
  author   = {Guojie Song and Yizhou Zhang and Lingjun Xu and Haibing Lu},
  doi      = {10.1109/TBDATA.2020.3034201},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1220-1232},
  title    = {Domain adaptive network embedding},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient interactive global cellular signal strength
visualization. <em>IEEE Transactions on Big Data</em>, <em>8</em>(5),
1209–1219. (<a
href="https://doi.org/10.1109/TBDATA.2020.3029559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Cellular Signal Strength (CSS), defined as the signal power received by mobile phones, is an important aspect of geographic information flow analysis, because the density of such information can reflect the urbanization variables such as population, gross domestic product, built-up area, electric power consumption, etc. Despite the importance, the real-time analysis of global CSS distribution remains a challenging problem due to the large data scale. In this article, a Display-driven Computing (DisDC) technique is designed and applied to provide efficient large scale interactive CSS visualization, generating results by calculating the value of each pixel that directly for display. Specifically, we present an efficient CSS measurement algorithm, which introduces spatial indexes and a corresponding query strategy; besides, an optimized parallel computing architecture is proposed to ensure the ability of real-time visualization. Experiments show that our approach obviously outperforms traditional methods and is capable of handling more than 40 million base stations in real-time. Moreover, an online demonstration is provided at https://github.com/MemoryMmy/CSSMap .},
  archive  = {J},
  author   = {Mengyu Ma and Luo Chen and Xue Ouyang and Xiaoran Liu and Jun Li and Ning Jing},
  doi      = {10.1109/TBDATA.2020.3029559},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1209-1219},
  title    = {Efficient interactive global cellular signal strength visualization},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep convolutional neural network based medical concept
normalization. <em>IEEE Transactions on Big Data</em>, <em>8</em>(5),
1195–1208. (<a
href="https://doi.org/10.1109/TBDATA.2020.3021389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Medical concept normalization is a critical problem in biomedical research and clinical applications. In this article, we focus on normalizing diagnostic and operation names in Chinese discharge summaries to standard concepts, which is formulated as a semantic matching problem. However, non-standard Chinese expressions, short-text normalization, heterogeneity of tasks and flexible input of disambiguation mentions pose critical challenges in our problem. We propose two models, the basic model and flexible model, to tackle these problems. The basic model solves the core problem (the first three challenges) in ambiguous mentions normalization, while the flexible model deals with flexible input of ambiguous mentions and further explores the correlation among them. Specifically, in the basic model, we present a general framework to disambiguate a diagnosis and its corresponding operation simultaneously, which introduces a tensor generator and a novel multi-view convolutional neural network (CNN) with a multi-task shared structure. We propose that the key to address non-standard expressions and the short-text problem is to incorporate a matching tensor with multiple granularities. Then a multi-view CNN is adopted to extract semantic matching patterns. Finally, the multi-task shared structure allows the model to exploit medical correlations between diagnosis and operation mentions to better perform disambiguation tasks. Subsequently, we design a flexible model based on the basic model. Specifically, we add a flexible attention layer to all procedure representation vectors, and then apply a flexible multi-task scheme to share the correlated information. Comprehensive experimental analysis indicates that our model outperforms existing baselines, demonstrating the effectiveness and robustness of our model.},
  archive  = {J},
  author   = {Guojie Song and Qingqing Long and Yi Luo and Yiming Wang and Yilun Jin},
  doi      = {10.1109/TBDATA.2020.3021389},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1195-1208},
  title    = {Deep convolutional neural network based medical concept normalization},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GGraph: An efficient structure-aware approach for iterative
graph processing. <em>IEEE Transactions on Big Data</em>, <em>8</em>(5),
1182–1194. (<a
href="https://doi.org/10.1109/TBDATA.2020.3019641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Many iterative graph processing systems have recently been developed to analyze graphs. Although they are effective from different aspects, there is an important issue that has not been addressed yet. A real-world graph follows the power-law property, in which a small number of vertices have high degrees (i.e., are connected to most other vertices in the graph). These vertices are called hot-vertices and usually require more iterations to converge. In the existing solutions, these hot-vertices may be allocated to many or even all graph partitions along with other vertices that are easy to converge. As the result, the partitions with hot-vertices have to be loaded repeatedly (and consequently the system suffers from high data access cost), although perhaps only a few vertices in these partitions are active. To cope with this issue, we develop an efficient open source graph partition manager, called GGraph, which can be integrated into the existing graph processing systems to efficiently support iterative graph processing, by taking into account the power-law property of the graph structure. It uses a novel graph repartitioning scheme with low overhead to dynamically partition the hot-vertices together, so as to avoid loading the inactive vertices in the same partition as the repeatedly processed hot-vertices. By such means, it not only enables less data access cost, but also enables the privileged processing of the hot-vertices. In order to further increase the convergence speed, a scheduling algorithm is further proposed in this work to prioritize the processing of the hot-vertices with low overhead. To demonstrate the efficiency of GGraph, we plug it into four state-of-the-art graph processing systems, i.e., Gemini, GraphChi, Chaos, and GridGraph, and experimental results show that GGraph improves their performance by up to 3.2 times, 3.8 times, 3.9 times, 3.5 times, respectively.},
  archive  = {J},
  author   = {Beibei Si and Yuxuan Liang and Jin Zhao and Yu Zhang and Xiaofei Liao and Hai Jin and Haikun Liu and Lin Gu},
  doi      = {10.1109/TBDATA.2020.3019641},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1182-1194},
  title    = {GGraph: An efficient structure-aware approach for iterative graph processing},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-label graph convolutional network representation
learning. <em>IEEE Transactions on Big Data</em>, <em>8</em>(5),
1169–1181. (<a
href="https://doi.org/10.1109/TBDATA.2020.3019478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Knowledge representation of networked systems is fundamental in many disciplines. To date, existing methods for representation learning primarily focus on networks with simplex labels, yet real-world objects (nodes) are inherently complex in nature and often contain rich semantics or labels. For example, a user may belong to diverse interest groups of a social network, resulting in multi-label networks for many applications. A multi-label network not only has multiple labels for each node, the labels are often highly correlated making existing methods ineffective or even fail to handle such correlation for node representation learning. In this article, we propose a novel multi-label graph convolutional network (MuLGCN) for learning node representation. To fully explore label-label correlation and network topology structures, we propose to model a multi-label network as two Siamese GCNs: a node-node-label graph and a label-label-node graph. The two GCNs each handle one aspect of representation learning for nodes and labels, respectively, and are seamlessly integrated in one objective function. The learned label representations can effectively preserve the intra-label interaction and node label properties, and are aggregated to enhance the node representation learning under a unified training framework. Experiments and comparisons on multi-label node classification validate the effectiveness of our proposed approach.},
  archive  = {J},
  author   = {Min Shi and Yufei Tang and Xingquan Zhu and Jianxun Liu},
  doi      = {10.1109/TBDATA.2020.3019478},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1169-1181},
  title    = {Multi-label graph convolutional network representation learning},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mobile network traffic prediction based on seasonal adjacent
windows sampling and conditional probability estimation. <em>IEEE
Transactions on Big Data</em>, <em>8</em>(5), 1155–1168. (<a
href="https://doi.org/10.1109/TBDATA.2020.3014049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Mobile operators collect and store the network generatedtraffic data for analysis. Time Series Prediction (TSP) has been used in mobile network traffic data analysis to produce predictive results for network planning and resource allocation. We propose a novel method of predicting mobile network traffic using neural networks based on conditional probability modeling between adjacent data windows. First, we develop a pre-processing method to aggregate the raw traffic log data and sample the aggregated time series to adjacent data windows, as training samples. Second, we use neural networks to parameterize the conditional probability between adjacent data windows and estimate the probability by training the neural networks with sampled data. The estimated conditional probability is then used to ensemble the prediction. Third, we show theoretically that the prediction based on all historical data is equivalent to the prediction based on just previous data window, given the estimation of conditional probability between adjacent data windows. We also analyze computation complexity and show that seasonality will reduce the computational complexity. In the experiment, we compare the prediction performance among the models with different seasonality, sample size and number of hidden layers, and show that the proposed schemes achieve better prediction accuracy than state-of-the-art.},
  archive  = {J},
  author   = {Jin Huang and Ming Xiao},
  doi      = {10.1109/TBDATA.2020.3014049},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1155-1168},
  title    = {Mobile network traffic prediction based on seasonal adjacent windows sampling and conditional probability estimation},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Personalized recommendation in P2P lending based on
risk-return management: A multi-objective perspective. <em>IEEE
Transactions on Big Data</em>, <em>8</em>(4), 1141–1154. (<a
href="https://doi.org/10.1109/TBDATA.2020.2993446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {P2P lending is an increasingly prosperous financial market, where lenders can directly bid and invest on the loans posted by borrowers. However, when facing massive loan requests, it is very difficult and also boring for lenders to choose loan portfolios meeting their ideal expectations. Actually, when choosing loans, most lenders pursue the highest profit with the lowest risk as well as satisfying their hobbies. In this article, we formalize a multi-objective optimization problem to help lenders select loan portfolios. Specifically, the recommending scenario in P2P lending is formulated as a multi-objective optimization problem, where two objective functions are designed for capturing lenders’ multiple demands. On this basis, a multi-objective evolutionary algorithm based on return-risk management named MOEA-RRM is then proposed for the multi-objective optimization problem, which can help lenders choose loan portfolios to meet lenders’ multiple demands. Furthermore, in MOEA-RRM, a decision space dimensionality reduction strategy and an initialization strategy are proposed to improve the performance of algorithm. Finally, experimental results on a real-world P2P lending data set demonstrate the effectiveness of our proposed MOEA-RRM, i.e., the proposed approach can recommend the loan portfolio with a good trade-off between risk and return as well as satisfying the hobbies of lenders.},
  archive  = {J},
  author   = {Lei Zhang and Xinpeng Wu and Hongke Zhao and Fan Cheng and Qi Liu},
  doi      = {10.1109/TBDATA.2020.2993446},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1141-1154},
  title    = {Personalized recommendation in P2P lending based on risk-return management: A multi-objective perspective},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weak supervision learning for object co-segmentation.
<em>IEEE Transactions on Big Data</em>, <em>8</em>(4), 1129–1140. (<a
href="https://doi.org/10.1109/TBDATA.2020.3009983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The booming of multimedia technologies has promoted the diversity of visual big data. To learn common features across heterogeneous image data, the image co-processing has exhibited its advantages over the separate one. Recently, an active topic of image co-processing is the object co-segmentation, which aims at simultaneously extracting and segmenting shared objects from relevant images. In this paper, we address this problem with a weak-supervision-based probabilistic model. We introduce the weakly supervised priors to alleviate the confusion between common foreground and background, thereby facilitating performance improvement. To ensure the validity of potential background prior knowledge, the nodes on four sides of image are respectively leveraged as the labelled queries. After that, we develop quantitative probabilistic metrics for precisely measuring internal consistencies within single image and correlations between multiple images. Combining the intra-image consistencies with the inter-image correlations, we propose an optimized energy function coupled with binary labeling and graph connectivity to carry out the object co-segmentation. Extensively experimental results on real-world datasets demonstrate that the proposed method achieves superior co-segmentation performance to the state-of-the-arts, with a significantly reduced time consumption.},
  archive  = {J},
  author   = {Aiping Huang and Tiesong Zhao},
  doi      = {10.1109/TBDATA.2020.3009983},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1129-1140},
  title    = {Weak supervision learning for object co-segmentation},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relaxed locality preserving supervised discrete hashing.
<em>IEEE Transactions on Big Data</em>, <em>8</em>(4), 1118–1128. (<a
href="https://doi.org/10.1109/TBDATA.2020.3027379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Supervised discrete hashing (SDH) has widely attracted attention because it can directly obtain hash codes rather than relaxing the discrete constraint, which reduces information loss during the quantization process. However, SDH and its extensions tend to ignore the underlying geometry structure of data, while in many cases, data is lying on a low-dimensional manifold. Therefore, based on SDH, we propose a novel manifold learning-based supervised discrete hashing method in this paper to further improve the performance. Our goal is to learn compact hash codes by discovering latent information and using the local structure of data. To this end, we utilize the anchor graph to capture local neighborhood relationship embedding in data for manifold learning. Besides, since the previous methods use a strict binary label matrix for classification, we jointly optimize a relaxed label matrix to increase the discriminative ability of the proposed model. As such, the learned hash codes are more suitable for classification. Compared with SDH and other popular hashing methods, the experimental results on four large-scale datasets show that the proposed hashing method has better performance.},
  archive  = {J},
  author   = {Xiangxi Xu and Zhihui Lai and Yudong Chen},
  doi      = {10.1109/TBDATA.2020.3027379},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1118-1128},
  title    = {Relaxed locality preserving supervised discrete hashing},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Private empirical risk minimization with analytic gaussian
mechanism for healthcare system. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(4), 1107–1117. (<a
href="https://doi.org/10.1109/TBDATA.2020.2997732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the wide range application of machine learning in healthcare for helping humans drive crucial decisions, data privacy becomes an inevitable concern due to the utilization of sensitive data such as patients records and registers of a company. Thus, constructing a privacy preserving machine learning model while still maintaining high accuracy becomes a challenging problem. In this article, we propose two differentially private algorithms, i.e., Output Perturbation with aGM (OPERA) and Gradient Perturbation with aGM (GRPUA) for empirical risk minimization, a useful method to obtain a globally optimal classifier, by leveraging the analytic Gaussian mechanism (aGM) to achieve privacy preservation of sensitive medical data in a healthcare system. We theoretically analyze and prove utility upper bounds of proposed algorithms and compare them with prior algorithms in the literature. The analyses show that in the high privacy regime, our proposed algorithms can achieve a tighter utility bound for both settings: strongly convex and non-strongly convex loss functions. Besides, we evaluate the proposed private algorithms on five benchmark datasets. The simulation results demonstrate that our approaches can achieve higher accuracy and lower objective values compared with existing ones in all three datasets while providing differential privacy guarantees.},
  archive  = {J},
  author   = {Jiahao Ding and Sai Mounika Errapotu and Yuanxiong Guo and Haixia Zhang and Dongfeng Yuan and Miao Pan},
  doi      = {10.1109/TBDATA.2020.2997732},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1107-1117},
  title    = {Private empirical risk minimization with analytic gaussian mechanism for healthcare system},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Momentum-incorporated symmetric non-negative latent factor
models. <em>IEEE Transactions on Big Data</em>, <em>8</em>(4),
1096–1106. (<a
href="https://doi.org/10.1109/TBDATA.2020.3012656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Symmetric high-dimensional and sparse (SHiDS) networks are frequently found in various industrial applications. A symmetric non-negative latent factor (SNLF) model can acquire essential features from them precisely, yet it suffers from slow convergence. To address this issue, this article integrates a generalized momentum method into a symmetric, single latent factor-dependent, non-negative and multiplicative update (S 2 LF-NMU) algorithm, thereby achieving a m omentum-incorporated, s ymmetric, s ingle-latent-factor-dependent n on-negative-multiplicative-update (MS 2 N) algorithm. Based on an MS 2 N algorithm, momentum-incorporated symmetric non-negative latent factor (MSNLF) models are proposed for an SHiDS network, which ensures fast convergence as well as high representative learning ability. Empirical studies on four SHiDS networks from industrial applications demonstrate that compared with state-of-the-art models, the proposed MSNLF models have significantly higher computational efficiency and representative learning ability.},
  archive  = {J},
  author   = {Yurong Zhong and Long Jin and Mingsheng Shang and Xin Luo},
  doi      = {10.1109/TBDATA.2020.3012656},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1096-1106},
  title    = {Momentum-incorporated symmetric non-negative latent factor models},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Off-deployment traffic estimation — a traffic generative
adversarial networks approach. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(4), 1084–1095. (<a
href="https://doi.org/10.1109/TBDATA.2020.3014511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The rapid progress of urbanization has expedited the process of urban planning, e.g. , new residential, commercial areas, which in turn boosts the local travel demand. We propose a novel “off-deployment traffic estimation problem”, namely, to foresee the traffic condition changes of a region prior to the deployment of a construction plan. This problem is important to city planners to evaluate and develop urban deployment plans. However, this task is challenging. Traditional traffic estimation approaches lack the ability to solve this problem, since no data about the impact can be collected before the deployment and old data fails to capture the traffic pattern changes. In this paper, we define the off-deployment traffic estimation problem as a traffic generation problem, and develop a novel deep generative model TrafficGAN that captures the shared patterns across spatial regions of how traffic conditions evolve according to travel demand changes and underlying road network structures. In particular, TrafficGAN captures the road network structures through a dynamic filter in the dynamic convolutional layer. We evaluate our TrafficGAN using a large-scale traffic data collected from Shenzhen, China. Results show that TrafficGAN can more accurately estimate the traffic conditions compared with all baselines. We also showcase that TrafficGAN can identify potential traffic issues in some regions and suggest possible reasons.},
  archive  = {J},
  author   = {Yingxue Zhang and Yanhua Li and Xun Zhou and Xiangnan Kong and Jun Luo},
  doi      = {10.1109/TBDATA.2020.3014511},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1084-1095},
  title    = {Off-deployment traffic estimation — a traffic generative adversarial networks approach},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A scalable algorithm for large-scale unsupervised multi-view
partial least squares. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(4), 1073–1083. (<a
href="https://doi.org/10.1109/TBDATA.2020.3014937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present an unsupervised multi-view partial least squares (PLS) by learning a common latent space from given multi-view data. Although PLS is a frequently used technique for analyzing relationships between two datasets, its extension to more than two views in unsupervised setting is seldom studied. In this article, we fill up the gap, and our model bears similarity to the extension of canonical correlation analysis (CCA) to more than two sets of variables and is built on the findings from analyzing PLS, CCA, and its variants. The resulting problem involves a set of orthogonality constraints on view-specific projection matrices, and is numerically challenging to existing methods that may have numerical instabilities and offer no orthogonality guarantee on view-specific projection matrices. To solve this problem, we propose a stable deflation algorithm that relies on proven numerical linear algebra techniques, can guarantee the orthogonality constraints, and simultaneously maximizes the covariance in the common space. We further adapt our algorithm to efficiently handle large-scale high-dimensional data. Extensive experiments have been conducted to evaluate the algorithm through performing two learning tasks, cross-modal retrieval, and multi-view feature extraction. The results demonstrate that the proposed algorithm outperforms the baselines and is scalable for large-scale high-dimensional datasets.},
  archive  = {J},
  author   = {Li Wang and Ren-Cang Li},
  doi      = {10.1109/TBDATA.2020.3014937},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1073-1083},
  title    = {A scalable algorithm for large-scale unsupervised multi-view partial least squares},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic entity-based named entity recognition under
unconstrained tagging schemes. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(4), 1059–1072. (<a
href="https://doi.org/10.1109/TBDATA.2020.2998770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As increasingly more textual information becomes available, named entity recognition (NER) systems are thriving, benefiting from powerful models and expressive tagging schemes that promote the full use of diverse features at different levels. To improve performance, traditional approaches have focused mainly on changing the structures of NER models but have always ignored the hard constraints and left the NER tagging schemes unchanged. To solve this problem, this article proposes a dynamic entity-based NER approach under unconstrained tagging schemes. To eliminate the constraints, we reorganize widely used tagging schemes and propose two novel unconstrained schemes: one in which tags are assigned to words and entities separately, and one where words and entities are labeled indiscriminately by uniformly taking them as chunks. Associated with the unconstrained tagging schemes, two entity-based neural architectures are also presented that recognize entities at the same time that the sentence is dynamically segmented. Unlike other static NER models that process all the tags after labeling each word, our models address the inputs dynamically by the interactions between the input text and the output labels. The dynamic mechanism can ensure that the entity-level features are included in the NER system, which is helpful for correctly recognizing entities. Except for word embeddings pretrained from unlabeled corpora, no external language-specific knowledge or other resources such as gazetteers are used. The experiments with English, German, Dutch, and Spanish datasets show that our methods can perform very well with different languages. Particularly, the results of the recall rate against the entity’s length reveal that the proposed entity-based models are suitable for recognizing entities with long lengths.},
  archive  = {J},
  author   = {Feng Zhao and Xiangyu Gui and Yafan Huang and Hai Jin and Laurence T. Yang},
  doi      = {10.1109/TBDATA.2020.2998770},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1059-1072},
  title    = {Dynamic entity-based named entity recognition under unconstrained tagging schemes},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast multi-view outlier detection via deep encoder. <em>IEEE
Transactions on Big Data</em>, <em>8</em>(4), 1047–1058. (<a
href="https://doi.org/10.1109/TBDATA.2020.3004057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multi-view outlier detection has a wide range of applications and has been well investigated in recent years. However, 1) most existing state-of-the-art methods cannot efficiently handle outlier detection problem for large-scale multi-view data, since exploring pairwise constraints among different views causes highly-computational cost; 2) the data collected from original heterogeneous feature spaces further increases the consistent difficulty of multi-view outlier detection. To address these issues, we present a fast multi-view outlier detection model via learning a low-rank latent subspace representation with deep encoder architecture, which can not only efficiently identify the outliers for large-scale data even with numerous data views, but also exploit a discriminative common latent subspace shared by all the views. First, we learn a set of orthogonal bases as view-specific dictionaries from a small dataset, which is randomly sampled from the original dataset. Benefitting from view-specific dictionaries, the sampled data is projected and decomposed as a shared and discriminative latent subspace representations, which correspond to the view-consistent and view-specific components across multiple views, respectively. Then, the obtained discriminative latent representations are applied to train the view-specific deep encoders, which can efficiently compute the abnormal score for the remaining instances. Our proposed model can cost-effectively identify the outliers in large-scale datasets from numerous data views with less computational complexity. Experiments conducted on eight real datasets and a synthesis dataset show that our proposed model outperforms the existing ones on effectiveness and efficiency.},
  archive  = {J},
  author   = {Dongdong Hou and Yang Cong and Gan Sun and Jiahua Dong and Jun Li and Kai Li},
  doi      = {10.1109/TBDATA.2020.3004057},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1047-1058},
  title    = {Fast multi-view outlier detection via deep encoder},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A domain-specific bayesian deep-learning approach for air
pollution forecast. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(4), 1034–1046. (<a
href="https://doi.org/10.1109/TBDATA.2020.3005368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Predicting air pollution concentration is crucial and beneficial for public health. This study proposes a domain-specific Bayesian deep-learning model for long-term air pollution forecast in China and the United Kingdom. Our proposed model carries three novelties: First, a domain-specific knowledge is integrated to take into account the strong statistical relationship between PM &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$_{2.5}$&lt;/tex-math&gt;&lt;/inline-formula&gt; and PM &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$_{10}$&lt;/tex-math&gt;&lt;/inline-formula&gt; as a regularization term; Second, an attention layer is included to capture the influential historical feature and the recursive temporal correlation of air quality data; Third, results generated from different multi-step forecast strategies are combined based on corresponding uncertainty measures to improve our model’s performance. Our model outperforms other baseline models. Results show that incorporating Bayesian and domain-specific knowledge into the deep learning model can reduce the prediction errors by a maximum of 3.7% and 12.4%, for Beijing and London, respectively. Specifically, incorporating domain-specific knowledge into the Bayesian deep-learning model reduces prediction errors whilst the integration of Bayesian techniques allows the fusion of different forecast strategies to improve prediction accuracy. In future, additional influential domain-specific features can be added to further improve our deep-learning model’s prediction accuracy and interpretability.},
  archive  = {J},
  author   = {Yang Han and Jacqueline C.K. Lam and Victor O.K. Li and Qi Zhang},
  doi      = {10.1109/TBDATA.2020.3005368},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1034-1046},
  title    = {A domain-specific bayesian deep-learning approach for air pollution forecast},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ATrie group join: A parallel star group join and aggregation
for in-memory column-stores. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(4), 1020–1033. (<a
href="https://doi.org/10.1109/TBDATA.2020.3004520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents a new holistic and efficient approach to big data analysis. We introduce a new parallel algorithm, known as ATrie Group Join (ATGJ) , that integrates join, grouping and aggregation operations to accelerate big data analytical workloads in in-memory column-stores. ATGJ performs a single scan of the fact columns and uses a mixture of data and task parallelism for the optimal use of computing resources. It uses a novel technique to perform group-by and aggregation realising the grouping attributes as a tree shaped deterministic finite automation known as Aggregate Trie or ATrie . ATrie facilitates grouping of attributes and processing of data in tight loops that significantly improves the performance on modern hardware. Unlike other competing algorithms, use of ATrie avoids the creation of multiple data structures with the increasing number of dimension tables and grouping attributes. Also, we demonstrate that ATGJ performs efficiently even when the ATrie becomes bushy. We evaluated the algorithm using Star Schema Benchmark (SSBM) to show that it is significantly faster and scales better than other algorithms for the number of concurrent threads, the number of group-by attributes, the data set size and the query complexity.},
  archive  = {J},
  author   = {Prajwol Sangat and David Taniar and Christopher Messom},
  doi      = {10.1109/TBDATA.2020.3004520},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1020-1033},
  title    = {ATrie group join: A parallel star group join and aggregation for in-memory column-stores},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MiSTR: A multiview structural-temporal learning framework
for rumor detection. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(4), 1007–1019. (<a
href="https://doi.org/10.1109/TBDATA.2021.3107481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the rapid development of web technology, social media platforms have become a breeding ground for rumors. These rumors can threaten people’s health, endanger the economy, and affect the stability of a country. In recent years, to mitigate the problem of rumors, computational detection of rumors has been studied, producing some promising early results. However, how to effectively capture the temporal information of retweet dynamics and the structural information of propagation structure is still neglected. In this article, we innovatively propose a novel Multiview Structural-Temporal Learning Framework for Rumor Detection, MiSTR, to jointly learn the temporal features of retweet dynamics, structural features of propagation graph, and the textual features of source tweet. More specifically, we utilize the timestamp encoding, and timestamp level and sequential level attention mechanisms to learn the temporal correlation among individual retweets. We propose two specific methods to learn the overall representation of propagation structure among users from both microscopic and mesoscopic perspectives. Encouraging empirical results on three real large-scale datasets demonstrate the superiority of our proposed method over the state-of-the-art approaches.},
  archive  = {J},
  author   = {Jianian Li and Peng Bao and Huawei Shen and Xuanya Li},
  doi      = {10.1109/TBDATA.2021.3107481},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1007-1019},
  title    = {MiSTR: A multiview structural-temporal learning framework for rumor detection},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A joint community detection model: Integrating directed and
undirected probabilistic graphical models via factor graph with
attention mechanism. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(4), 994–1006. (<a
href="https://doi.org/10.1109/TBDATA.2021.3104005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Directed and undirected probabilistic graphical models have been successfully used in community detection in recent years, but existing graphical model based methods usually only use one type of probabilistic graphical model to discover communities. However, directed and undirected graphical models have their own advantages for characterizing different network information (attribute information and network topology). Intuitively, we can make use of the merit of both kinds of models by combining them into a unified model. However, combining directed and undirected graphical models is difficult, as they have different properties which prevent parameter sharing and joint training. In this article, we propose a unified model which integrates directed and undirected graphical models by transforming both them into factor graph. In addition, as network topology and attribute information may contain different degrees of noises, we add a selective attention layer to learn the reliable weight of each type of information source in node granularity. For training the model, we derive an iterative belief propagation algorithm to train all the parameters simultaneously. Extensive experiments on real networks and artificial benchmarks show the superiority of our approach over existing methods.},
  archive  = {J},
  author   = {Dongxiao He and Huixin Liu and Zhiyong Feng and Xiaobao Wang and Di Jin and Wenze Song and Yuxiao Huang},
  doi      = {10.1109/TBDATA.2021.3104005},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {994-1006},
  title    = {A joint community detection model: Integrating directed and undirected probabilistic graphical models via factor graph with attention mechanism},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving chinese word representation using four corners
features. <em>IEEE Transactions on Big Data</em>, <em>8</em>(4),
982–993. (<a href="https://doi.org/10.1109/TBDATA.2021.3106582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Intuitively, word representation for logographic languages like Chinese can be enhanced by its internal characteristics. Several research endeavors tried to learn Chinese word embeddings with characters, radicals, or subcharacters containing rich semantic information. In this paper, motivated by Four-Corner Method for Character Indexation, we extract features from four corners of characters with important morphological charactertics. Based on the features from four corners, we propose a model to utilize characters and four corner features of words to capture both semantic and morphological information. Moreover, we apply an attention scheme to integrate internal information dynamically, which includes two strategies to assign different weights for elements according to the word frequency. Experimental results on social news corpus and Chinese Wikipedia Dump show exploiting the four corner morphological features is crucial for capturing the meanings of Chinese words. Meanwhile, the results on word analogy, word similarity, and text classification tasks demonstrate that our approach obtains better results than state-of-the-art approaches.},
  archive  = {J},
  author   = {Hai Jin and Zhaobo Zhang and Pingpeng Yuan},
  doi      = {10.1109/TBDATA.2021.3106582},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {982-993},
  title    = {Improving chinese word representation using four corners features},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Boosting nonnegative matrix factorization based community
detection with graph attention auto-encoder. <em>IEEE Transactions on
Big Data</em>, <em>8</em>(4), 968–981. (<a
href="https://doi.org/10.1109/TBDATA.2021.3103213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Community detection is of great help to understand the structures and functions of complex networks. It has become one of popular research topics in the field of complex networks analysis. Due to the simplicity, flexibility, effectiveness and better interpretability, Nonnegative Matrix Factorization (NMF)-based methods have been widely employed for community detection. However, most existing NMF-based community detection methods are linear and their performance is limited when facing networks with diversified structure information. In view of this, we propose a nonlinear NMF-based method named NMFGAAE, which is composed of two main modules: NMF and Graph Attention Auto-Encoder (GAAE). This approach can boost the performance of NMF-based community detection methods by the aid of graph neural networks and deep clustering. More specifically, GAAE introduces an attention mechanism directed by NMF-based community detection to learn the node representations, while NMF can simultaneously factor these representations to uncover the community structure. We design a unified framework to jointly optimize GAAE and NMF modules, which is very beneficial to obtain better community detection results. We conduct extensive experiments on synthetic and real-world networks. The results show that our NMFGAAE not only performs better than state-of-the-art NMF-based community detection methods, but also outperforms some network representation based baselines. More importantly, NMFGAAE indeed can boost the performance of NMF-based community detection methods.},
  archive  = {J},
  author   = {Chaobo He and Yulong Zheng and Xiang Fei and Hanchao Li and Zeng Hu and Yong Tang},
  doi      = {10.1109/TBDATA.2021.3103213},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {968-981},
  title    = {Boosting nonnegative matrix factorization based community detection with graph attention auto-encoder},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving accuracy and diversity in matching of
recommendation with diversified preference network. <em>IEEE
Transactions on Big Data</em>, <em>8</em>(4), 955–967. (<a
href="https://doi.org/10.1109/TBDATA.2021.3103263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Real-world recommendation systems need to deal with millions of item candidates. Therefore, most practical large-scale recommendation systems usually contain two modules. The matching module aims to efficiently retrieve hundreds of high-quality items from large corpora, while the ranking module aims to generate specific ranks for these items. Recommendation diversity is an essential factor that strongly impacts user experience. There are lots of efforts that have explored recommendation diversity in ranking, while the matching module should take more responsibility for diversity. In this article, we propose a novel Heterogeneous graph neural network framework for diversified recommendation (GraphDR) in matching to improve both recommendation accuracy and diversity. Specifically, GraphDR builds a huge heterogeneous preference network to record different types of user preferences, and conducts a field-level heterogeneous graph attention network for node aggregation. We conduct a neighbor-similarity based loss with a multi-channel matching to improve both accuracy and diversity. In experiments, we conduct extensive online and offline evaluations on a real-world recommendation system with various accuracy and diversity metrics and achieve significant improvements. GraphDR has been deployed on a well-known recommendation system named WeChat Top Stories, which affects millions of users. The source code will be released in https://github.com/lqfarmer/GraphDR .},
  archive  = {J},
  author   = {Ruobing Xie and Qi Liu and Shukai Liu and Ziwei Zhang and Peng Cui and Bo Zhang and Leyu Lin},
  doi      = {10.1109/TBDATA.2021.3103263},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {955-967},
  title    = {Improving accuracy and diversity in matching of recommendation with diversified preference network},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guest editorial: Special issue on social media computing.
<em>IEEE Transactions on Big Data</em>, <em>8</em>(4), 953–954. (<a
href="https://doi.org/10.1109/TBDATA.2021.3119083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The papers in this special section focus on social media computing. With massive amounts of social media data currently available, social media computing has attracted considerable attention in recent years, which aims to represent, analyze, and extract useful patterns from social media data. Social media computing is one typical cross-discipline of computer science, data mining, natural language process, and social sciences. As an emerging research field, there are various open, unexplored, and unidentified problems. There is a great need for computational models for tasks such as social media pattern analysis and prediction on social media data.},
  archive  = {J},
  author   = {Guojie Song and Chuan Shi and Yizhou Sun and Zhiyuan Liu},
  doi      = {10.1109/TBDATA.2021.3119083},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {953-954},
  title    = {Guest editorial: Special issue on social media computing},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on role-oriented network embedding. <em>IEEE
Transactions on Big Data</em>, <em>8</em>(4), 933–952. (<a
href="https://doi.org/10.1109/TBDATA.2021.3131610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, Network Embedding (NE) has become one of the most attractive research topics in machine learning and data mining. NE approaches have achieved promising performance in various graph mining tasks including link prediction and node clustering and classification. A wide variety of NE methods focus on the proximity of networks, they learn community-oriented embedding for each node, where the corresponding representations are similar if two nodes are closer to each other in the network. Meanwhile, there is another type of structural similarity, i.e., role-based similarity, which is completely different from and complementary to the proximity. In order to preserve the role-based structural similarity, the problem of role-oriented NE is raised. However, compared to the community-oriented NE, there are only a few role-oriented embedding approaches proposed recently. Although less explored, considering the importance of roles in analyzing networks and many applications that role-oriented NE can shed light on, it is necessary and timely to provide a comprehensive overview of existing role-oriented NE methods. In this review, we first clarify the differences between community-oriented and role-oriented network embedding. Afterward, we propose a general framework for understanding role-oriented NE and a two-level categorization to better classify existing methods. Then, we select some representative methods according to the proposed categorization and briefly introduce them by discussing their motivation, development, and differences. Moreover, we conduct comprehensive experiments to empirically evaluate these methods on a variety of role-related tasks including node classification and clustering (role discovery), top-k similarity search, and visualization using some widely used synthetic and real-world datasets. Finally, we further discuss the research trend of role-oriented NE from the perspective of applications and point out some potential future directions.},
  archive  = {J},
  author   = {Pengfei Jiao and Xuan Guo and Ting Pan and Wang Zhang and Yulong Pei and Lin Pan},
  doi      = {10.1109/TBDATA.2021.3131610},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {933-952},
  title    = {A survey on role-oriented network embedding},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active and semi-supervised graph neural networks for graph
classification. <em>IEEE Transactions on Big Data</em>, <em>8</em>(4),
920–932. (<a href="https://doi.org/10.1109/TBDATA.2021.3140205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graph classification aims to predict the class labels of graphs and has a wide range of applications in many real-world domains. However, most of existing graph neural networks for graph classification tasks use 90 &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\%$&lt;/tex-math&gt;&lt;/inline-formula&gt; of labeled graphs for training and the remaining 10 &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\%$&lt;/tex-math&gt;&lt;/inline-formula&gt; for testing, which obviously struggle in solving the problem of the scarcity of labeled graphs in real-world graph classification scenarios. And it is arduous to label a large number of graph examples for training because of the difficulty and resource consumption in the tagging process. Motivated by this, we propose a novel active and semi-supervised graph neural network (ASGNN) framework, which endeavors to complete graph classification tasks with a small number of labeled graph examples and available unlabeled graph examples. In our framework, active learning selects high-uncertain and representative graph examples from the test set and add them to the training set after annotation. Semi-supervised learning is utilized to select the high-confidence unlabeled graph examples containing structural information from the test set, and add them to the training set after pseudo labeling. To improve the generalization performance of the graph classification model, multiple GNNs are trained collaboratively for promoting the expressiveness of each other and increasing the reliability of graph classification results. Overall, the ASGNN framework takes fully use of unlabeled graph examples to reinforce graph classification effectively, and can be applied to any existing supervised graph neural networks for graph classification. Experimental results on benchmark graph datasets demonstrate that the proposed framework yields competitive performance on graph classification tasks with only a small number of labeled graph examples.},
  archive  = {J},
  author   = {Yu Xie and Shengze Lv and Yuhua Qian and Chao Wen and Jiye Liang},
  doi      = {10.1109/TBDATA.2021.3140205},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {920-932},
  title    = {Active and semi-supervised graph neural networks for graph classification},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MAFI: GNN-based multiple aggregators and feature
interactions network for fraud detection over heterogeneous graph.
<em>IEEE Transactions on Big Data</em>, <em>8</em>(4), 905–919. (<a
href="https://doi.org/10.1109/TBDATA.2021.3132672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, Graph Neural Networks (GNNs) have been widely used for fraud detection. GNNs first generate node embedding by aggregating neighboring information under different relations, and then use the final node embedding to detect the node’s suspiciousness. However, traditional GNNs employing only a single type of aggregator fail to capture neighbor information from multiple perspectives and treating different relations equally inevitably weakens the semantic information of heterogeneous graphs. Meanwhile, expressive ability of GNNs is limited by using conventional concatenating or averaging operations to update the center node. Also, camouflaged entities could damage GNN-based models. To handle these problems, a novel heterogeneous GNN model called Multiple Aggregators and Feature Interactions Network (MAFI) is proposed in this paper to conduct fraud detection tasks. Concretely, multiple types of aggregators are applied on different relations to aggregate neighbor information and aggregator-level attention is utilized to learn the importance of different aggregators. Also, relation-level attention is leveraged to learn the importance of each relation. Besides, conventional update operations are replaced with vector-wise implicit and explicit feature interactions. Moreover, a trainable neighbor sampler is employed to filter camouflaged fraudsters. Comprehensive experiments on two real-world fraud datasets indicate that the proposed MAFI outperforms existing GNN-based fraud detectors.},
  archive  = {J},
  author   = {Nan Jiang and Fuxian Duan and Honglong Chen and Wei Huang and Ximeng Liu},
  doi      = {10.1109/TBDATA.2021.3132672},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {905-919},
  title    = {MAFI: GNN-based multiple aggregators and feature interactions network for fraud detection over heterogeneous graph},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How to protect ourselves from overlapping community
detection in social networks. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(4), 894–904. (<a
href="https://doi.org/10.1109/TBDATA.2022.3152431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, overlapping community detection algorithms have been paid more and more attention, which not only reveal the real social relations, but also expose the possible communication channels between communities. Those individuals (or people) in the overlapping area are very important to the communities that can promote communication between two or more communities. On the other hand, from the privacy perspective, some people may not want to be found out in the overlapping areas. With this in mind, we raise a question “Can individuals modify their relationships to avoid the community discovery algorithms locating them into overlapping areas?” If this problem could be solved, these people may not need to worry about being disturbed. In particular, we first give three heuristic hiding strategies, i.e., Random Hiding(RH), Based Degree Hiding(DH) and Betweenness Hiding(BH), as comparison, utilizing the randomly the node, information of node degree and node betweenness centrality, respectively. And then, we propose a novel hiding algorithm by exploiting the importance degree of nodes in communities based on which the corresponding social connections are added or deleted called name BIH . Through extensive experiments, we show the effectiveness of the proposed algorithm in moving out a target node from overlapped areas.},
  archive  = {J},
  author   = {Dong Liu and Guoliang Yang and Yanwei Wang and Hu Jin and Enhong Chen},
  doi      = {10.1109/TBDATA.2022.3152431},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {894-904},
  title    = {How to protect ourselves from overlapping community detection in social networks},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-view clustering with self-representation and
structural constraint. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(4), 882–893. (<a
href="https://doi.org/10.1109/TBDATA.2021.3128906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multi-view data effectively model and characterize the underlying complex systems, and multi-view clustering is of great significance for revealing the mechanisms of systems, which groups objects into different clusters with high intra-cluster and low inter-cluster similarity for all views. Current algorithms are criticized for undesirable performance because they solely focus on either the shared features or correlation of objects, failing to address the heterogeneity and structural constraint of various views. To overcome these problems, a novel M ulti-view C lustering with S elf-representation and S tructural C onstraint (MCSSC) is proposed, which is a network-based method by fusing matrix factorization and low-rank representation of various views. Specifically, to remove heterogeneity of multi-view data, a network is constructed for each view, which casts the multi-view clustering into the multi-layer networks clustering problem. To extract the shared features of multiple views, MCSSC factorizes matrices associated with networks by projecting them into a common space and jointly learns an affinity graph for objects in multiple views with self-representation. To facilitate the clustering, the structural constraint is imposed on the affinity graph, where the clusters are identified. Extensive experiments demonstrate that MCSSC significantly outperforms the state-of-the-art in terms of accuracy, implying that the superiority of the proposed method.},
  archive  = {J},
  author   = {Xiaowei Gao and Xiaoke Ma and Wensheng Zhang and Jianbin Huang and He Li and Yanni Li and Jiangtao Cui},
  doi      = {10.1109/TBDATA.2021.3128906},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {882-893},
  title    = {Multi-view clustering with self-representation and structural constraint},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identification of communities with multi-semantics via
bayesian generative model. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(4), 869–881. (<a
href="https://doi.org/10.1109/TBDATA.2021.3131707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Discovering communities is an essential step in the analysis of complex systems, and it has two purposes: to identify functional modules and to interpret semantics. However, most of the existing community detection methods only focused on identify communities, while learning the semantics interpretation of communities has not been fully studied. In this paper, we focused on the problem of identifying communities and learning the semantics interpretation of modules jointly in an end-to-end model. We designed a novel generative model which combines two closely related parts, one for community discovery and the other for content clustering and semantics interpretation. By extracting the potential correlation between these two parts, our new method is not only robust to discovering communities, but also able to provide a community with more than one semantic topic. As for model inference, we developed a variational algorithm from a Bayesian point of view. Experimental results on the artificial benchmark and real networks showed the superior performance of the proposed approach over existing methods in terms of effectiveness and efficiency. We also analyzed semantic interpretability of community detection results through a case study over a large-scale music platform dataset.},
  archive  = {J},
  author   = {Dongxiao He and Yanli Wu and Youyou Wang and Zhizhi Yu and Zhiyong Feng and Xiaobao Wang and Yuxiao Huang},
  doi      = {10.1109/TBDATA.2021.3131707},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {869-881},
  title    = {Identification of communities with multi-semantics via bayesian generative model},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guest editorial: Special issue on network structural
modeling and learning in big data. <em>IEEE Transactions on Big
Data</em>, <em>8</em>(4), 867–868. (<a
href="https://doi.org/10.1109/TBDATA.2022.3162000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The papers in this special section focus on the most recent advances in network structural modeling and learning with applications in the big data field. A variety of phenomena and systems are usually regraded as networks with a set of nodes and relations. Diversified methods and models have been developed for different tasks on networks, such as structural discovery, link prediction, and anomaly detection. With the growing data scale due to the information explosion and scientific development, network structural modeling has become an interesting and new field of research. Furthermore, building and utilizing new technologies for efficient learning, inference, and predication on different types of networks has become the trend to look forward to. In recent years, all kinds of machine learning technologies on networks have been developed rapidly, especially the network embedding and graph neural networks. They induce various methods and achieve satisfactory performance on network tasks, including node clustering or classification and link prediction. Furthermore, network structural modeling on real big data is also interesting.},
  archive  = {J},
  author   = {Di Jin and Wenjun Wang and Guojie Song and Philip S. Yu and Jiawei Han},
  doi      = {10.1109/TBDATA.2022.3162000},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {867-868},
  title    = {Guest editorial: Special issue on network structural modeling and learning in big data},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OpinionRank: Trustworthy website detection using three
valued subjective logic. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(3), 855–866. (<a
href="https://doi.org/10.1109/TBDATA.2020.2994309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {For a web search engine, it is critical to design a mechanism to promote trustworthy websites and eliminate spam ones in the searching results. In this paper, we propose the OpinionRank algorithm to compute the trustworthiness of a website and identify trustworthy ones with high trust values. OpinionRank is essentially a breadth-first-search based algorithm that starts from an existing set of trustworthy websites, also called seeds. Because seeds play a vital role in OpinionRank, we put forward a novel seed selection scheme, named HarMean PageRank algorithm. HarMean combines the results of two seed selection algorithms, i.e. High PageRank and Inverse PageRank, to rank websites based on their trustworthiness. After trustworthy seeds are chosen, OpinionRank iteratively computes the trustworthiness of every website, leveraging trust propagation and trust combination. Using the public dataset WEBSPAM-UK2006, we validate OpinionRank and HarMean PageRank, analyze the impact of seed selection, and evaluate the convergence speed of OpinionRank. Experimental results indicate that OpinionRank can detect more trustworthy websites with fewer seeds, when compared to three state-of-the-art solutions, TrustRank, GoodRank, and Enhanced OpinionWalk algorithms.},
  archive  = {J},
  author   = {Xiaofei Niu and Guangchi Liu and Qing Yang},
  doi      = {10.1109/TBDATA.2020.2994309},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {855-866},
  title    = {OpinionRank: Trustworthy website detection using three valued subjective logic},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Federated forest. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(3), 843–854. (<a
href="https://doi.org/10.1109/TBDATA.2020.2992755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Most real-world data are scattered across different companies or government organizations, and cannot be easily integrated under data privacy and related regulations such as the European Union’s General Data Protection Regulation (GDPR) and China’ Cyber Security Law. Such data islands situation and data privacy &amp;amp; security are two major challenges for applications of artificial intelligence. In this article, we tackle these challenges and propose a privacy-preserving machine learning model, called Federated Forest , which is a lossless learning model of the traditional random forest method, i.e., achieving the same level of accuracy as the non-privacy-preserving approach. Based on it, we developed a secure cross-regional machine learning system that allows a learning process to be jointly trained over different regions’ clients with the same user samples but different attribute sets, processing the data stored in each of them without exchanging their raw data. A novel prediction algorithm was also proposed which could largely reduce the communication overhead. Experiments on both real-world and UCI data sets demonstrate the performance of the Federated Forest is as accurate as of the non-federated version. The efficiency and robustness of our proposed system had been verified. Overall, our model is practical, scalable and extensible for real-life tasks.},
  archive  = {J},
  author   = {Yang Liu and Yingting Liu and Zhijie Liu and Yuxuan Liang and Chuishi Meng and Junbo Zhang and Yu Zheng},
  doi      = {10.1109/TBDATA.2020.2992755},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {843-854},
  title    = {Federated forest},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CrowdExpress: A probabilistic framework for on-time
crowdsourced package deliveries. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(3), 827–842. (<a
href="https://doi.org/10.1109/TBDATA.2020.2991152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Most of current urban logistic systems fail to strike a nice trade-off between speed and cost. An express logistic service often implies a high delivery cost. Crowdsourced logistics is a promising solution to alleviating such contradiction. In this article, we propose a new form of crowdsourced logistics that organizes passengers and packages in a shared room, i.e., using taxis that are already transporting passengers as package hitchhikers to achieve on-time deliveries. It is well-recognized that taxi drivers are good at delivering passengers to their destinations efficiently. As a result, the proposed new urban logistics system has potentials to lower the cost and accelerate package deliveries simultaneously. Specifically, we propose a probabilistic framework containing two phases called CrowdExpress for the on-time package express service. In the first phase, we mine the historical taxi GPS trajectory data offline to build the package transport network. In the second phase, we develop an online taxi scheduling algorithm to adaptively discover the path with the maximum arriving-on-time probability “on-the-fly” upon real-time passenger-sending requests, and direct the package routing accordingly. Finally, we evaluate the system using the real-world taxi data generated by over 19,000 taxis in a month in the city of New York, US. Results show that around 9,500 packages can be successfully delivered daily on time with the success rate over 94 percent.},
  archive  = {J},
  author   = {Chao Chen and Sen Yang and Yasha Wang and Bin Guo and Daqing Zhang},
  doi      = {10.1109/TBDATA.2020.2991152},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {827-842},
  title    = {CrowdExpress: A probabilistic framework for on-time crowdsourced package deliveries},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Urban anomaly analytics: Description, detection, and
prediction. <em>IEEE Transactions on Big Data</em>, <em>8</em>(3),
809–826. (<a href="https://doi.org/10.1109/TBDATA.2020.2991008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Urban anomalies may result in loss of life or property if not handled properly. Automatically alerting anomalies in their early stage or even predicting anomalies before happening is of great value for populations. Recently, data-driven urban anomaly analysis frameworks have been forming, which utilize urban big data and machine learning algorithms to detect and predict urban anomalies automatically. In this survey, we make a comprehensive review of the state-of-the-art research on urban anomaly analytics. We first give an overview of four main types of urban anomalies, traffic anomaly, unexpected crowds, environment anomaly, and individual anomaly. Next, we summarize various types of urban datasets obtained from diverse devices, i.e., trajectory, trip records, CDRs, urban sensors, event records, environment data, social media and surveillance cameras. Subsequently, a comprehensive survey of issues on detecting and predicting techniques for urban anomalies is presented. Finally, research challenges and open problems as discussed.},
  archive  = {J},
  author   = {Mingyang Zhang and Tong Li and Yue Yu and Yong Li and Pan Hui and Yu Zheng},
  doi      = {10.1109/TBDATA.2020.2991008},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {809-826},
  title    = {Urban anomaly analytics: Description, detection, and prediction},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Representation and reinforcement learning for task
scheduling in edge computing. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(3), 795–808. (<a
href="https://doi.org/10.1109/TBDATA.2020.2990558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, many deep reinforcement learning (DRL)-based task scheduling algorithms have been widely used in edge computing (EC) to reduce energy consumption. Unlike the existing algorithms considering fixed and fewer edge nodes (servers) and tasks, in this article, a representation model with a DRL based algorithm is proposed to adapt the dynamic change of nodes and tasks and solve the dimensional disaster in DRL caused by a massive scale. Specifically, 1) we apply the representation learning models to describe the different nodes and tasks in EC, i.e., nodes and tasks are mapped to corresponding vector sub-spaces to reduce the dimensions and store the vector space efficiently. 2) With the space after dimensionality reduction, a DRL-based algorithm is employed to learn the vector representations of nodes and tasks and make scheduling decisions. 3) The experiments are conducted with the real-world data set, and the results show that the proposed representation model with DRL-based algorithm outperforms the baselines 18.04 and 9.94 percent on average regarding energy consumption and service level agreement violation (SLAV), respectively.},
  archive  = {J},
  author   = {Zhiqing Tang and Weijia Jia and Xiaojie Zhou and Wenmian Yang and Yongjian You},
  doi      = {10.1109/TBDATA.2020.2990558},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {795-808},
  title    = {Representation and reinforcement learning for task scheduling in edge computing},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multilayered-and-randomized latent factor model for
high-dimensional and sparse matrices. <em>IEEE Transactions on Big
Data</em>, <em>8</em>(3), 784–794. (<a
href="https://doi.org/10.1109/TBDATA.2020.2988778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {How to extract useful knowledge from a high-dimensional and sparse (HiDS) matrix efficiently is critical for many big data-related applications. A latent factor (LF) model has been widely adopted to address this problem. It commonly relies on an iterative learning algorithm like stochastic gradient descent. However, an algorithm of this kind commonly consumes many iterations to converge, resulting in considerable time cost on large-scale datasets. How to accelerate an LF model&#39;s training process without accuracy loss becomes a vital issue. To address it, this study innovatively proposes a m ultilayered-and-randomized l atent f actor (MLF) model. Its main idea is two-fold: a) adopting randomized-learning to train LFs for implementing a ‘one-iteration’ training process for saving time; and 2) adopting the principle of a generally multilayered structure as in a deep forest or multilayered extreme learning machine to structure its LFs, thereby enhancing its representative learning ability. Empirical studies on six HiDS matrices from real applications demonstrate that compared with state-of-the-art LF models, an MLF model achieves significantly higher computational efficiency with satisfactory prediction accuracy. It has the potential to handle LF analysis on a large scale HiDS matrix with real-time requirements.},
  archive  = {J},
  author   = {Ye Yuan and Qiang He and Xin Luo and Mingsheng Shang},
  doi      = {10.1109/TBDATA.2020.2988778},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {784-794},
  title    = {A multilayered-and-randomized latent factor model for high-dimensional and sparse matrices},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IRDA: Incremental reinforcement learning for dynamic
resource allocation. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(3), 770–783. (<a
href="https://doi.org/10.1109/TBDATA.2020.2988273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Resource allocation problems often manifest as online decision-making tasks where the proper allocation strategy depends on the understanding of the allocation environment and resources workload. Most existing resource allocation methods are based on meticulously designed heuristics which ignore the patterns of incoming tasks, so the dynamics of incoming tasks cannot be properly handled. To address this problem, we mine the task patterns from the large volume of historical allocation data and propose a reinforcement learning model termed IRDA to learn the allocation strategy in an incremental way. We observe that historical allocation data is usually generated from the daily repeated operations, which is not independent and identically distributed. Training with partial of this dataset can make the allocation strategy converged already, thereby wasting a lot of remaining data. To improve the learning efficiency, we partition the whole historical allocation big dataset into multi-batch datasets, which forces the agent to continuously “explore” and learn on the distinct state spaces. IRDA reuses the strategy learned from the previous batch dataset and adapts it to the learning on the next batch dataset, so as to incrementally learn from multi-batch datasets and improve the allocation strategy. We apply the proposed method to handle baggage carousel allocation at Hong Kong International Airport (HKIA). The experimental results show that IRDA is capable of incrementally learning from multi-batch datasets, and improves the baggage carousel resource utilization by around 51.86 percent compared to the current baggage carousel allocation system at HKIA.},
  archive  = {J},
  author   = {Jia Wang and Jiannong Cao and Senzhang Wang and Zhongyu Yao and Wengen Li},
  doi      = {10.1109/TBDATA.2020.2988273},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {770-783},
  title    = {IRDA: Incremental reinforcement learning for dynamic resource allocation},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple riemannian manifold-valued descriptors based image
set classification with multi-kernel metric learning. <em>IEEE
Transactions on Big Data</em>, <em>8</em>(3), 753–769. (<a
href="https://doi.org/10.1109/TBDATA.2020.2982146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The importance of wild video based image set recognition is monotonically increasing due to the large amount of video data being collected by various devices including surveillance cameras, drive recorders, smart phones, and internet. The content of these videos is often complex, and it raises the question of how to perform image set modeling and feature extraction for image set-based classification. In recent years, image set classification methods have advanced considerably by modeling the image set in terms of a covariance matrix, linear subspace, or Gaussian distribution. Moreover, the distinctive geometry spanned by them include Symmetric Positive Definite (SPD) manifold, Grassmannian manifold, and Gaussian embedded Riemannian manifold, respectively. As a matter of fact, most of the approaches just adopt a single geometric model to describe each given image set, which may lose information useful for classification. To tackle this problem, we propose a novel algorithm to model each image set from a multi-geometric perspective. Specifically, the covariance matrix, linear subspace, and Gaussian distribution are applied to set representation simultaneously. In order to fuse these multiple heterogeneous Riemannian manifold-valued features, the well-equipped Riemannian kernel functions are first employed to map them into high dimensional Hilbert spaces. Then, a multi-kernel metric learning framework is devised to embed the learned hybrid kernels into a common lower dimensional subspace to facilitate classification. We conduct experiments on six widely used datasets each representing a different classification task: video-based face recognition, set-based object categorization, video-based emotion recognition, dynamic scene classification, set-based cell identification, and 3D hand pose estimation, to evaluate the classification performance of the proposed algorithm. The extensive experimental results confirm its superiority over the state-of-the-art methods.},
  archive  = {J},
  author   = {Rui Wang and Xiao-Jun Wu and Kai-Xuan Chen and Josef Kittler},
  doi      = {10.1109/TBDATA.2020.2982146},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {753-769},
  title    = {Multiple riemannian manifold-valued descriptors based image set classification with multi-kernel metric learning},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). B4SDC: A blockchain system for security data collection in
MANETs. <em>IEEE Transactions on Big Data</em>, <em>8</em>(3), 739–752.
(<a href="https://doi.org/10.1109/TBDATA.2020.2981438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Security-related data collection is an essential part for attack detection and security measurement in Mobile Ad Hoc Networks (MANETs). A detection node (i.e., collector) should discover available routes to a collection node for data collection and collect security-related data during route discovery for determining reliable routes. However, few studies provide incentives for security-related data collection in MANETs. In this article, we propose B4SDC, a blockchain system for security-related data collection in MANETs. Through controlling the scale of Route REQuest (RREQ) forwarding in route discovery, the collector can constrain its payment and simultaneously make each forwarder of control information (namely RREQs and Route REPlies, in short RREPs) obtain rewards as much as possible to ensure fairness. At the same time, B4SDC avoids collusion attacks with cooperative receipt reporting, and spoofing attacks by adopting a secure digital signature. Based on a novel Proof-of-Stake consensus mechanism by accumulating stakes through message forwarding, B4SDC not only provides incentives for all participating nodes, but also avoids forking and ensures high efficiency and real decentralization. We analyze B4SDC in terms of incentives and security, and evaluate its performance through simulations. The thorough analysis and experimental results show the efficacy and effectiveness of B4SDC.},
  archive  = {J},
  author   = {Gao Liu and Huidong Dong and Zheng Yan and Xiaokang Zhou and Shohei Shimizu},
  doi      = {10.1109/TBDATA.2020.2981438},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {739-752},
  title    = {B4SDC: A blockchain system for security data collection in MANETs},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on the methods and results of data-driven koopman
analysis in the visualization of dynamical systems. <em>IEEE
Transactions on Big Data</em>, <em>8</em>(3), 723–738. (<a
href="https://doi.org/10.1109/TBDATA.2020.2980849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Koopman mode decomposition is a flow analysis technique developed by Igor Mezić in 2004, based upon the Koopman operator first proposed by Bernard Koopman in 1931. Via Koopman decomposition any non-chaotic well-sampled dynamic system – linear, non-linear, laminar or turbulent – is broken down into single-frequency repetitive components (modes). This paper presents a survey consolidating published information regarding data-driven Koopman analysis techniques. It is intended to aid researchers exploring the suitability of data-driven Koopman analysis in anticipation of developing their own modeling. A basic mathematical explanation of Koopman analysis is given with emphasis toward the data-driven Dynamic Mode Decomposition (DMD) solution, which converges to the Koopman operator given a highly-sampled dataset. The four primary uses of Koopman analysis: flow analysis, power grid analysis, building thermal analysis, and biomedical analysis are discussed, along with other public. Finally, weaknesses and problems inherent within Koopman analysis/DMD will be enumerated, alongside potential solutions. Koopman analysis is a computationally complex, yet often suitable method for determining periodic motion in any highly-sampled dataset. When compared to a similar analysis method, Proper Orthogonal Decomposition, Koopman analysis often provides additional detail regarding the structure of less significant modes present, albeit at the cost of increased computational complexity.},
  archive  = {J},
  author   = {Nishaal Parmar and Hazem H. Refai and Thordur Runolfsson},
  doi      = {10.1109/TBDATA.2020.2980849},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {723-738},
  title    = {A survey on the methods and results of data-driven koopman analysis in the visualization of dynamical systems},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Missing air pollution data recovery based on long-short term
context encoder. <em>IEEE Transactions on Big Data</em>, <em>8</em>(3),
711–722. (<a href="https://doi.org/10.1109/TBDATA.2020.2979443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Air pollution has become a global challenge, and obtaining real-time air quality information is urgently needed. Although the governments have been trying their best in delivering accurate air quality reports, missing air pollution data remains a key challenge. Based on the temporal-spatial correlation of the data, we propose a novel long-short term context encoder (LSCE) structure for recovering missing air pollution data. The original context encoder approach based on image completion focuses on reconstructing rectangular missing regions. Differing from traditional methods, our fully convolutional neural network architecture enjoys the following novelties. First, LSCE can recover irregular missing data patterns. Second, we devise two data pre-processing strategies to produce two types of context encoders, namely, the long-short term cutting context encoder (LSCCE) and the long-short term sliding context encoder (LSSCE). Compared with LSCCE, LSSCE increases the number of training data matrixes. Finally, we investigate the significance of adaptive training in addressing different types of missing data. Our simulation results have demonstrated that our approach, especially, LSSCE, can outperform existing missing data recovery methods. Besides, our techniques can be widely applicable for recovering other temporally and spatially correlated missing data, such as vehicular traffic or meteorology data.},
  archive  = {J},
  author   = {Yangwen Yu and Victor O. K. Li and Jacqueline C. K. Lam},
  doi      = {10.1109/TBDATA.2020.2979443},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {711-722},
  title    = {Missing air pollution data recovery based on long-short term context encoder},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stark: Fast and scalable strassen’s matrix multiplication
using apache spark. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(3), 699–710. (<a
href="https://doi.org/10.1109/TBDATA.2020.2977326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents a new fast, highly scalable distributed matrix multiplication algorithm on Apache Spark, called Stark , based on Strassen’s matrix multiplication algorithm. Stark preserves Strassen’s seven multiplications scheme in a distributed environment and thus achieves asymptotically faster execution time. It creates a distributed recursion tree of computation where each level of the tree corresponds to division and combination of distributed matrix blocks stored in the form of Resilient Distributed Datasets (RDDs). It processes each divide and combine step in parallel and memorises the sub-matrices by intelligently tagging matrix blocks in it. To the best of our knowledge, Stark is the first implementation of a distribute Strassen’s algorithm on Spark platform. We also report a detailed complexity analysis for the proposed algorithm, taking into account computation and communication costs. Experimental results suggest that Stark outperforms existing distributed matrix multiplication implementations on Spark – Marlin and MLLib , for high matrix sizes ( &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\geq 16384\times 16384$&lt;/tex-math&gt;&lt;/inline-formula&gt; ). Our experiments reveal optimal block sizes for each matrix size, which is also shown from theoretical analysis. We also show that the experimental and theoretical running times for Stark match closely. It has also been shown experimentally that Stark exhibits strong scalability with increasing number of executors.},
  archive  = {J},
  author   = {Chandan Misra and Sourangshu Bhattacharya and Soumya K. Ghosh},
  doi      = {10.1109/TBDATA.2020.2977326},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {699-710},
  title    = {Stark: Fast and scalable strassen’s matrix multiplication using apache spark},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven web APIs recommendation for building web
applications. <em>IEEE Transactions on Big Data</em>, <em>8</em>(3),
685–698. (<a href="https://doi.org/10.1109/TBDATA.2020.2975587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The ever-increasing popularity of web APIs allows app developers to leverage a set of existing web APIs to achieve their sophisticated objectives. The heavily fragmented distribution of web APIs makes it challenging for an app developer to find appropriate and compatible web APIs. Currently, app developers usually have to manually discover candidate web APIs, verify their compatibility and select appropriate and compatible ones. This process is cumbersome and requires detailed knowledge of web APIs which is often too demanding. It has become a major obstacle to further and broader applications of web APIs. To address this issue, we first propose a web API correlation graph built on extensive data about the compatibility between web APIs. Then, we propose WAR ( W eb A PIs R ecommendation), the first data-driven approach for web APIs recommendation that integrates web API discovery, verification and selection operations based on keywords search over the web API correlation graph. WAR assists app developers without detailed knowledge of web APIs in searching for appropriate and compatible web APIs by typing a few keywords that represent the tasks required to achieve app developers’ objectives. WAR can significantly save app developers’ time and effort in searching for web APIs. We conducted large-scale experiments on 18,478 real-world web APIs and 6,146 real-world apps to demonstrate the usefulness and efficiency of WAR.},
  archive  = {J},
  author   = {Lianyong Qi and Qiang He and Feifei Chen and Xuyun Zhang and Wanchun Dou and Qiang Ni},
  doi      = {10.1109/TBDATA.2020.2975587},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {685-698},
  title    = {Data-driven web APIs recommendation for building web applications},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mining stable communities in temporal networks by
density-based clustering. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(3), 671–684. (<a
href="https://doi.org/10.1109/TBDATA.2020.2974849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Community detection is a fundamental task in graph data mining. Most existing studies in contact networks, collaboration networks, and social networks do not utilize the temporal information associated with edges for community detection. In this article, we study a problem of finding stable communities in a temporal network, where each edge is associated with a timestamp. Our goal is to identify the communities in a temporal network that are stable over time. To efficiently find the stable communities, we develop a new community detection algorithm based on the density-based graph clustering framework. We also propose several carefully-designed pruning techniques to significantly speed up the proposed algorithm. We conduct extensive experiments on four real-life temporal networks to evaluate our algorithm. The results demonstrate the effectiveness and efficiency of the proposed algorithm.},
  archive  = {J},
  author   = {Hongchao Qin and Rong-Hua Li and Guoren Wang and Xin Huang and Ye Yuan and Jeffrey Xu Yu},
  doi      = {10.1109/TBDATA.2020.2974849},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {671-684},
  title    = {Mining stable communities in temporal networks by density-based clustering},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient event inference and context-awareness in internet
of things edge systems. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(3), 658–670. (<a
href="https://doi.org/10.1109/TBDATA.2019.2907978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Internet of Things (IoT) connects physical, cyber and human spaces. Event-based system is one of the cornerstones to help IoT achieve real-time monitoring, context-awareness and intelligent control. In the era of big data, the huge amount and high complexity of event inference rule pose a great challenge to traditional event-based system in its efficiency, especially resources-constrained IoT edge systems. This paper proposes a high-efficiency joint event inference model for real-time context-awareness and decision-making in IoT edge systems. We define different kinds of redundancy relations between event inference models and propose a description mechanism, named event containing graph, to support multi-pattern optimization. Three operations on single-pattern event inference models, Merge , Failure and Output are defined respectively. The joint inference model is established by merging sharing patterns, constructing failure transitions and conditional output to eliminate inter-model redundancies. Experimental results prove that the joint model consumes less computational resources and provides higher performance than other benchmarks. It also verifies and proves that joint model has better optimization effect when processing large number of complex events. Especially in edge computing environment, joint inference model improves the real-time performance and significantly reduces the energy consumption in data transmission from edges to data center.},
  archive  = {J},
  author   = {Meng Ma and Ping Wang},
  doi      = {10.1109/TBDATA.2019.2907978},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {658-670},
  title    = {Efficient event inference and context-awareness in internet of things edge systems},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards ubiquitous intelligent computing: Heterogeneous
distributed deep neural networks. <em>IEEE Transactions on Big
Data</em>, <em>8</em>(3), 644–657. (<a
href="https://doi.org/10.1109/TBDATA.2018.2880978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {For the pursuit of ubiquitous computing, distributed computing systems containing the cloud, edge devices, and Internet-of-Things devices are highly demanded. However, existing distributed frameworks do not tailor for the fast development of Deep Neural Network (DNN), which is the key technique behind many intelligent applications nowadays. Based on prior exploration on distributed deep neural networks (DDNN), we propose Heterogeneous Distributed Deep Neural Network (HDDNN) over the distributed hierarchy, targeting at ubiquitous intelligent computing. While being able to support basic functionalities of DNNs, our framework is optimized for various types of heterogeneity, including heterogeneous computing nodes, heterogeneous neural networks, and heterogeneous system tasks. Besides, our framework features parallel computing, privacy protection and robustness, with other consideration for the combination of heterogeneous distributed system and DNN. Extensive experiments demonstrate that our framework is capable of utilizing hierarchical distributed system better for DNN and tailoring DNN for real-world distributed system properly, which is with low response time, high performance, and better user experience.},
  archive  = {J},
  author   = {Zongpu Zhang and Tao Song and Liwei Lin and Yang Hua and Xufeng He and Zhengui Xue and Ruhui Ma and Haibing Guan},
  doi      = {10.1109/TBDATA.2018.2880978},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {644-657},
  title    = {Towards ubiquitous intelligent computing: Heterogeneous distributed deep neural networks},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Link prediction in knowledge graphs: A hierarchy-constrained
approach. <em>IEEE Transactions on Big Data</em>, <em>8</em>(3),
630–643. (<a href="https://doi.org/10.1109/TBDATA.2018.2867583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Link prediction over a knowledge graph aims to predict the missing head entities &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$h$&lt;/tex-math&gt;&lt;/inline-formula&gt; or tail entities &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$t$&lt;/tex-math&gt;&lt;/inline-formula&gt; and missing relations &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$r$&lt;/tex-math&gt;&lt;/inline-formula&gt; for a triple &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$(h,r,t)$&lt;/tex-math&gt;&lt;/inline-formula&gt; . Recent years have witnessed great advance of knowledge graph embedding based link prediction methods, which represent entities and relations as elements of a continuous vector space. Most methods learn the embedding vectors by optimizing a margin-based loss function, where the margin is used to separate negative and positive triples in the loss function. The loss function utilizes the general structures of knowledge graphs, e.g., the vector of &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$r$&lt;/tex-math&gt;&lt;/inline-formula&gt; is the translation of the vector of &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$h$&lt;/tex-math&gt;&lt;/inline-formula&gt; and &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$t$&lt;/tex-math&gt;&lt;/inline-formula&gt; , and the vector of &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$t$&lt;/tex-math&gt;&lt;/inline-formula&gt; should be the nearest neighbor of the vector of &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$h+r$&lt;/tex-math&gt;&lt;/inline-formula&gt; . However, there are many particular structures, and can be employed to promote the performance of link prediction. One typical structure in knowledge graphs is hierarchical structure, which existing methods have much unexplored. We argue that the hierarchical structures also contain rich inference patterns, and can further enhance the link prediction performance. In this paper, we propose a hierarchy-constrained link prediction method, called hTransM, on the basis of the translation-based knowledge graph embedding methods. It can adaptively determine the optimal margin by detecting the single-step and multi-step hierarchical structures. Moreover, we prove the effectiveness of hTransM theoretically, and experiments over three benchmark datasets and two sub-tasks of link prediction demonstrate the superiority of hTransM.},
  archive  = {J},
  author   = {Manling Li and Yuanzhuo Wang and Denghui Zhang and Yantao Jia and Xueqi Cheng},
  doi      = {10.1109/TBDATA.2018.2867583},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {630-643},
  title    = {Link prediction in knowledge graphs: A hierarchy-constrained approach},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A meta path based method for entity set expansion in
knowledge graph. <em>IEEE Transactions on Big Data</em>, <em>8</em>(3),
616–629. (<a href="https://doi.org/10.1109/TBDATA.2018.2805366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Entity Set Expansion (ESE) is the problem that expands a small set of seed entities into a more complete set, entities of which have common traits. As a popular data mining task, ESE has been widely used in many applications, such as dictionary construction, query suggestion and new brand identification. Existing ESE methods mainly utilize text and Web information. That is, the intrinsic relation among entities is inferred from their occurrences in text or Web. With the surge of knowledge graph in recent years, it is possible to extend entities according to their occurrences in knowledge graph. In this paper, we consider the knowledge graph as a heterogeneous information network (HIN) that contains different types of objects and links, and propose a novel method, called MP_ESE, to extend entities in the HIN. The MP_ESE employs meta paths, a relation sequence connecting entities, in HIN to capture the implicit common traits of seed entities. In addition, an automatic meta path generation method, called SMPG, has been designed to exploit the potential relations among entities. Heuristic learning and PU learning methods are employed to learn the weights of extracted meta paths. With these generated and weighted meta paths, the MP_ESE can effectively extend entities. Comprehensive experiments on real datasets show the effectiveness and efficiency of MP_ESE.},
  archive  = {J},
  author   = {Yuyan Zheng and Chuan Shi and Xiaohuan Cao and Xiaoli Li and Bin Wu},
  doi      = {10.1109/TBDATA.2018.2805366},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {616-629},
  title    = {A meta path based method for entity set expansion in knowledge graph},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neurally-guided semantic navigation in knowledge graph.
<em>IEEE Transactions on Big Data</em>, <em>8</em>(3), 607–615. (<a
href="https://doi.org/10.1109/TBDATA.2018.2805363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this big data era, knowledge becomes increasingly linked, along with the rapid growth in data volume. Connected knowledge is naturally represented and stored as knowledge graphs, which are of more and more importance for many frontier research areas such as machine intelligence. Effectively finding relations between entities in a large knowledge graph plays a key role in many knowledge graph applications, as the most valuable part of a knowledge graph is its rich connectedness, which captures rich information about the objects in the real world. However, due to the intrinsic complexity of real-world knowledge, finding semantically close relations by navigation in a large knowledge graph is very challenging. Canonical graph exploration methods inevitably result in combinatorial explosion especially when the paths connecting two entities are long: the search space is &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$O(d^l)$&lt;/tex-math&gt;&lt;/inline-formula&gt; , where &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$d$&lt;/tex-math&gt;&lt;/inline-formula&gt; is the average graph node degree and &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$l$&lt;/tex-math&gt;&lt;/inline-formula&gt; is the length of the path. In this paper, we will systematically study the semantic navigation problem for large knowledge graphs. Inspired by AlphaGo, which was overwhelmingly successful in the game Go, we designed an efficient semantic navigation method based on a well-tailored Monte Carlo Tree Search algorithm with the unique characteristics of knowledge graphs considered. Extensive experiments on different real-life knowledge bases show that our method is not only effective but also very efficient.},
  archive  = {J},
  author   = {Liang He and Bin Shao and Yanghua Xiao and Yatao Li and Tie-Yan Liu and Enhong Chen and Huanhuan Xia},
  doi      = {10.1109/TBDATA.2018.2805363},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {607-615},
  title    = {Neurally-guided semantic navigation in knowledge graph},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Knowledge graphs for social good: An entity-centric search
engine for the human trafficking domain. <em>IEEE Transactions on Big
Data</em>, <em>8</em>(3), 592–606. (<a
href="https://doi.org/10.1109/TBDATA.2017.2763164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Web advertising related to Human Trafficking (HT) activity has been on the rise in recent years. Answering entity-centric questions over crawled HT Web corpora to assist investigators in the real world is an important social problem, involving many technical challenges. This paper describes a recent entity-centric knowledge graph effort that resulted in a semantic search engine to assist analysts and investigative experts in the HT domain. The overall approach takes as input a large corpus of advertisements crawled from the Web, structures it into an indexed knowledge graph, and enables investigators to satisfy their information needs by posing investigative search queries to a special-purpose semantic execution engine. We evaluated the search engine on real-world data collected from over 90,000 webpages, a significant fraction of which correlates with HT activity. Performance on four relevant categories of questions on a mean average precision metric were found to be promising, outperforming a learning-to-rank approach on three of the four categories. The prototype uses open-source components and scales to terabyte-scale corpora. Principles of the prototype have also been independently replicated, with similarly successful results.},
  archive  = {J},
  author   = {Mayank Kejriwal and Pedro Szekely},
  doi      = {10.1109/TBDATA.2017.2763164},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {592-606},
  title    = {Knowledge graphs for social good: An entity-centric search engine for the human trafficking domain},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FLAG: Faster learning on anchor graph with label predictor
optimization. <em>IEEE Transactions on Big Data</em>, <em>8</em>(3),
579–591. (<a href="https://doi.org/10.1109/TBDATA.2017.2757522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Knowledge graphs have received intensive research interests. When the labels of most nodes or datapoints are missing, anchor graph and hierarchical anchor graph models can be employed. With an anchor graph or hierarchical anchor graph, we only need to optimize the labels of the coarsest anchors, and the labels of datapoints can be inferred from these anchors in a coarse-to-fine manner. The complexity of optimization is therefore reduced to a cubic cost with respect to the number of the coarsest anchors. However, to obtain a high accuracy when a data distribution is complex, the scale of this anchor set still needs to be large, which thus inevitably incurs an expensive computational burden. As such, a challenge in scaling up these models is how to efficiently estimate the labels of these anchors while keeping classification performance. To address this problem, we propose a novel approach that adds an anchor label predictor in the conventional anchor graph and hierarchical anchor graph models. In the proposed approach, the labels of the coarsest anchors are not directly optimized, and instead, we learn a label predictor which estimates the labels of these anchors with their spectral representations. The predictor is optimized with a regularization on all datapoints based on a hierarchical anchor graph, and we show that its solution only involves the inversion of a small-size matrix. Built upon the anchor hierarchy, we design a sparse intra-layer adjacency matrix over these anchors, which can simultaneously accelerate spectral embedding and enhance effectiveness. Our approach is named Faster Learning on Anchor Graph (FLAG) as it improves conventional anchor-graph-based methods in terms of efficiency. Experiments on a variety of publicly available datasets with sizes varying from thousands to millions of samples demonstrate the effectiveness of our approach.},
  archive  = {J},
  author   = {Weijie Fu and Meng Wang and Shijie Hao and Tingting Mu},
  doi      = {10.1109/TBDATA.2017.2757522},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {579-591},
  title    = {FLAG: Faster learning on anchor graph with label predictor optimization},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting urban water quality with ubiquitous data - a
data-driven approach. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(2), 564–578. (<a
href="https://doi.org/10.1109/TBDATA.2020.2972564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Urban water quality is of great importance to our daily lives. Prediction of urban water quality help control water pollution and protect human health. However, predicting the urban water quality is a challenging task since the water quality varies in urban spaces non-linearly and depends on multiple factors, such as meteorology, water usage patterns, and land uses. In this article, we forecast the water quality of a station over the next few hours from a data-driven perspective, using the water quality data, and water hydraulic data reported by existing monitor stations and a variety of data sources we observed in the city, such as meteorology, pipe networks, structure of road networks, and point of interests (POIs). First, we identify the influential factors that affect the urban water quality via extensive experiments. Second, we present a multi-task multi-view learning method to fuse those multiple datasets from different domains into an unified learning model. We evaluate our method with real-world datasets, and the extensive experiments verify the advantages of our method over other baselines and demonstrate the effectiveness of our approach.},
  archive  = {J},
  author   = {Ye Liu and Yuxuan Liang and Kun Ouyang and Shuming Liu and David S. Rosenblum and Yu Zheng},
  doi      = {10.1109/TBDATA.2020.2972564},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {564-578},
  title    = {Predicting urban water quality with ubiquitous data - a data-driven approach},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly supervised cross-modal hashing. <em>IEEE Transactions
on Big Data</em>, <em>8</em>(2), 552–563. (<a
href="https://doi.org/10.1109/TBDATA.2019.2954516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Cross-modal hashing can efficiently retrieve data across different modalities and has been successfully applied in various domains. Although many supervised cross-modal hashing methods have been proposed, they generally focus on two modals only and assume that the labels of training data are sufficient and complete. This assumption is not practical in real scenarios. In this article, we propose the weakly supervised cross-modal hashing (WCHash), which takes into account the widely witnessed weakly supervised information ( incomplete and insufficient labels ) of training data. Specifically, WCHash first optimizes a latent central modality with respect to other modalities. Next, it uses an efficient multi-label weak-label method to enrich the labels of training data and measures the semantic similarity between data points based on the enriched labels. After that, it uses this similarity to guide the correlation maximization between the respective data modals and the central modal and thus achieves the hash functions for cross-modal retrieval. Experimental results on real-world datasets demonstrate that WCHash is more efficient and effective than related state-of-the-art cross-modal hashing methods. WCHash can significantly reduce the complexity of cross-modal hashing on three or more modalities.},
  archive  = {J},
  author   = {Xuanwu Liu and Guoxian Yu and Carlotta Domeniconi and Jun Wang and Guoqiang Xiao and Maozu Guo},
  doi      = {10.1109/TBDATA.2019.2954516},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {552-563},
  title    = {Weakly supervised cross-modal hashing},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding the users and videos by mining a novel danmu
dataset. <em>IEEE Transactions on Big Data</em>, <em>8</em>(2), 535–551.
(<a href="https://doi.org/10.1109/TBDATA.2019.2950411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent years have witnessed a successful rise of the time synchronized gossiping comment , or so-called danmu combined with online videos. This new business mode has enriched communication among users by sending users’ feelings through danmus and sharing these danmus on time synchronized videos. Can danmu communication be helpful for better user behavior modeling or video analyzing? To this question, in this article, preliminary attempts are made on analysis of users and videos by introducing a Danmu dataset which is collected from a real-world danmu-enabled video sharing platform. The dataset contains 1.7 TB of videos and danmus in total across eight video categories. With a focus on the 7.9 million danmus records and 4.8 million video frames, we first perform the basic statistic analysis and high-level semantic analysis. After that, we show some of the previous work on this area, including user behavior modeling, fine-grained video understanding and labeling, video plot generation and image-enhanced semantic understanding. For each application, we also propose its possible future directions. We hope this new dataset will inspire new ideas in areas among language, multimedia, and user understanding.},
  archive  = {J},
  author   = {Guangyi Lv and Kun Zhang and Le Wu and Enhong Chen and Tong Xu and Qi Liu and Weidong He},
  doi      = {10.1109/TBDATA.2019.2950411},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {535-551},
  title    = {Understanding the users and videos by mining a novel danmu dataset},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An algorithm of inductively identifying clusters from
attributed graphs. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(2), 523–534. (<a
href="https://doi.org/10.1109/TBDATA.2020.2964544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Attributed graphs are widely used to represent network data where the attribute information of nodes is available. To address the problem of identifying clusters in attributed graphs, most of existing solutions are developed simply based on certain particular assumptions related to the characteristics of clusters of interest. However, it is yet unknown whether such assumed characteristics are consistent with attributed graphs. To overcome this issue, we innovatively introduce an inductive clustering algorithm that tends to address the clustering problem for attributed graphs without any assumption made on the clusters. To do so, we first process the attribute information to obtain pairwise attribute values that significantly frequently co-occur in adjacent nodes as we believe that they have potential ability to represent the characteristics of a given attributed graph. For two adjacent nodes, their likelihood of being grouped in the same cluster can be weighted by their ability to characterize the graph. Then based on these verifed characteristics instead of assumed ones, a depth-first search strategy is applied to perform the clustering task. Moreover, we are able to classify clusters such that their significance can be indicated. The experimental results demonstrate the performance and usefulness of our algorithm.},
  archive  = {J},
  author   = {Lun Hu and Shicheng Yang and Xin Luo and MengChu Zhou},
  doi      = {10.1109/TBDATA.2020.2964544},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {523-534},
  title    = {An algorithm of inductively identifying clusters from attributed graphs},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Event detection on twitter by mapping unexpected changes in
streaming data into a spatiotemporal lattice. <em>IEEE Transactions on
Big Data</em>, <em>8</em>(2), 508–522. (<a
href="https://doi.org/10.1109/TBDATA.2019.2948594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Many applications seek to make sense of high volume streaming data from social media by identifying spatiotemporal patterns. Events, representing topics that emerge and decay over time, are detected by monitoring for changes in the language being used, but typical approaches do not consider the localisation of events in cities and countries, and within hours, days, and weeks. This work develops and evaluates a new approach to event localisation and ranking that can be applied to Twitter data streams. The proposed approach models the use of language in tweets per city per hour to produce a model that can be used to detect the magnitude of unexpected changes in the use of the language. The approach uses a spatiotemporal lattice structure and a method for traversing between hours, days, and weeks, as well as cities, regions, and countries to identify anomalies in the language used across millions of tweets. The output is a ranked list of events comprising a list of tweets posted within a location and period of time, and characterized by language features of interest. The approach was implemented and tested by comparing events detected across five example domains (suicide, shooting, elections, sports, and sentiment) using 11.7 million tweets from users located in 100 cities and posted within the 203-day study period. Experiments demonstrate that the approach can detect events across a range of application domains.},
  archive  = {J},
  author   = {Zubair Shah and Adam G. Dunn},
  doi      = {10.1109/TBDATA.2019.2948594},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {508-522},
  title    = {Event detection on twitter by mapping unexpected changes in streaming data into a spatiotemporal lattice},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GradientFlow: Optimizing network performance for large-scale
distributed DNN training. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(2), 495–507. (<a
href="https://doi.org/10.1109/TBDATA.2019.2957478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {It is important to scale out deep neural network (DNN) training for reducing model training time. The high communication overhead is one of the major performance bottlenecks for distributed DNN training across multiple GPUs. Our investigations have shown that popular open-source DNN systems could only achieve 2.5 speedup ratio on 64 GPUs connected by 56 Gbps network. To address this problem, we propose a communication backend named GradientFlow for distributed DNN training, and employ a set of network optimization techniques. First, we integrate ring-based allreduce, mixed-precision training, and computation/communication overlap into GradientFlow. Second, we propose lazy allreduce to improve network throughput by fusing multiple communication operations into a single one, and design coarse-grained sparse communication to reduce network traffic by only transmitting important gradient chunks. When training AlexNet and ResNet-50 on the ImageNet dataset using 512 GPUs, our approach could achieve 410.2 and 434.1 speedup ratio, respectively.},
  archive  = {J},
  author   = {Peng Sun and Yonggang Wen and Ruobing Han and Wansen Feng and Shengen Yan},
  doi      = {10.1109/TBDATA.2019.2957478},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {495-507},
  title    = {GradientFlow: Optimizing network performance for large-scale distributed DNN training},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discrete double-bit hashing. <em>IEEE Transactions on Big
Data</em>, <em>8</em>(2), 482–494. (<a
href="https://doi.org/10.1109/TBDATA.2019.2946616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Hashing has been widely used for nearest neighbors search over big data. Hashing encodes high dimensional data points into binary codes. Most hashing methods use the single-bit quantization (SBQ) strategy for coding the data. However, this strategy often encodes neighboring points into totally different bits. Recently, a double-bit quantization (DBQ) strategy was proposed, which can better preserve the similarity of the data. The hashing problems are generally NP-hard, due to the discrete constraints. For tractability, some relaxation methods were proposed by discarding the discrete constraints. However, such a manner makes the hash codes less effective, due to the large quantization error. To obtain high-quality hash codes, some discrete hashing methods were proposed, which directly solve the hashing problem without any relaxations. However, the existing discrete hashing methods can only deal with single-bit hashing. In this paper, we propose a discrete hashing method to solve double-bit hashing problems. To address the difficulty brought by the discrete constraints, we propose a method to transform the discrete hashing problem into an equivalent continuous optimization problem. Then, we devise algorithms based on DC (difference of convex functions) programming to solve the problem. Numerical experiments are provided to show the superiority of the proposed methods.},
  archive  = {J},
  author   = {Shengnan Wang and Chunguang Li},
  doi      = {10.1109/TBDATA.2019.2946616},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {482-494},
  title    = {Discrete double-bit hashing},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PES: Priority edge sampling in streaming triangle
estimation. <em>IEEE Transactions on Big Data</em>, <em>8</em>(2),
470–481. (<a href="https://doi.org/10.1109/TBDATA.2019.2948613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The number of triangles (hereafter denoted by &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\Delta$&lt;/tex-math&gt;&lt;/inline-formula&gt; ) is an important metric to analyze massive graphs. It is also used to compute clustering coefficient in networks. This paper proposes a new algorithm called PES (Priority Edge Sampling) to estimate the number of triangles in the streaming model where we need to minimize the memory window. PES combines edge sampling and reservoir sampling. Compared with the state-of-the-art streaming algorithms, PES outperforms consistently. The results are verified extensively in 48 large real-world networks in different domains and structures. The performance ratio can be as large as 11. More importantly, the ratio grows with data size almost exponentially. This is especially important in the era of big data–while we can tolerate existing algorithms for smaller datasets, our method is indispensable when sampling very large data. In addition to empirical comparisons, we also proved that the estimator is unbiased, and derived the variance.},
  archive  = {J},
  author   = {Roohollah Etemadi and Jianguo Lu},
  doi      = {10.1109/TBDATA.2019.2948613},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {470-481},
  title    = {PES: Priority edge sampling in streaming triangle estimation},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). &lt;Inline-formula&gt;&lt;tex-math notation=“LaTeX”&gt;<span
class="math inline"><em>d</em></span>&lt;/tex-math&gt;&lt;mml:math
xmlns:mml=“http://www.w3.org/1998/math/MathML”&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic
xlink:href=“chen-ieq1-2948338.gif”
xmlns:xlink=“http://www.w3.org/1999/xlink”/&gt;&lt;/inline-formula&gt;-simplexed:
Adaptive delaunay triangulation for performance modeling and prediction
on big data analytics. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(2), 458–469. (<a
href="https://doi.org/10.1109/TBDATA.2019.2948338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Big Data processing systems (e.g., Spark) have a number of resource configuration parameters, such as memory size, CPU allocation, and the number of running nodes. Regular users and even expert administrators struggle to understand the mutual relation between different parameter configurations and the overall performance of the system. In this paper, we address this challenge by proposing a performance prediction framework, called &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$d$&lt;/tex-math&gt;&lt;/inline-formula&gt; -Simplexed, to build performance models with varied configurable parameters on Spark. We take inspiration from the field of Computational Geometry to construct a &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$d$&lt;/tex-math&gt;&lt;/inline-formula&gt; -dimensional mesh using Delaunay Triangulation over a selected set of features. From this mesh, we predict execution time for various feature configurations. To minimize the time and resources in building a bootstrap model with a large number of configuration values, we propose an adaptive sampling technique to allow us to collect as few training points as required. Our evaluation on a cluster of computers using WordCount, PageRank, Kmeans, and Join workloads in HiBench benchmarking suites shows that we can achieve less than 5 percent error rate for estimation accuracy by sampling less than 1 percent of data.},
  archive  = {J},
  author   = {Yuxing Chen and Peter Goetsch and Mohammad A. Hoque and Jiaheng Lu and Sasu Tarkoma},
  doi      = {10.1109/TBDATA.2019.2948338},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {458-469},
  title    = {&lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$d$&lt;/tex-math&gt;&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=&quot;chen-ieq1-2948338.gif&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;/&gt;&lt;/inline-formula&gt;-simplexed: adaptive delaunay triangulation for performance modeling and prediction on big data analytics},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Least squares approximation via sparse subsampled randomized
hadamard transform. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(2), 446–457. (<a
href="https://doi.org/10.1109/TBDATA.2020.2972887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Solving least squares (LS) problems is a major topic in many applications. With recent data explosion, traditional approach is no longer suitable while working with large datasets, instead, randomized algorithms become popular in addressing this issue. In this article we propose a new randomized algorithm - sparse subsampled randomized Hadamard transform (SpSRHT) for solving overdetermined least squares problems. Its unique block structure connects two most commonly used randomized algorithms subsampled randomized Hadamard transform (SRHT) and sparse subspace embedding (SpEmb) and creates a general framework which contains them as special cases. We have shown theoretically that SpSRHT with different parameters reaches the relative-error bound with sketch size ranging from the sketch size required by SpEmb to SRHT. The new algorithm closes the gap between SRHT and SpEmb which provides the possibility of balancing accuracy and efficiency demonstrated in them. This advantage of SpSRHT is also well illustrated in our numerical experiments.},
  archive  = {J},
  author   = {Dan Teng and Xiaowei Zhang and Li Cheng and Delin Chu},
  doi      = {10.1109/TBDATA.2020.2972887},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {446-457},
  title    = {Least squares approximation via sparse subsampled randomized hadamard transform},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncovering local hierarchical overlapping communities at
scale. <em>IEEE Transactions on Big Data</em>, <em>8</em>(2), 432–445.
(<a href="https://doi.org/10.1109/TBDATA.2019.2940450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Real-life systems involving interacting objects are typically modeled as graphs and can often grow very large in size. Revealing the community structure of such systems is crucial in helping us better understand their complex nature. However, the ever-increasing size of real-world graphs and our evolving perception of what a community is, make the task of community detection very challenging. A critical relevant challenge is the discovery of the possibly overlapping communities of a given node in a billion-node graph. This problem is very common in modern large social networks like Facebook and LinkedIn. In this work, we propose a scalable local community detection approach to efficiently unfold the communities of individual target nodes in a given network. Our goal is to reveal the clusters formed around nodes (e.g., users) by leveraging the relations within all different contexts these nodes participate in. Our approach, termed Local Dispersion-aware Link Communities or LDLC, considers the similarity of pairs of links in the graph as well as the extent of their participation in multiple contexts. Then, we determine the order in which we should group the pairs of links so that we form meaningful hierarchical communities. We are not affected by constraints existing in previous techniques such as the need for several seed nodes or the need to collapse multiple overlapping communities to a single community. Our experimental evaluation using ground-truth communities for a wide range of large real-world networks shows that our LDLC algorithm significantly outperforms state-of-the-art methods on both accuracy and efficiency. Moreover, we show that LDLC uncovers very effectively the hierarchical structure of overlapping communities by producing detailed dendrograms.},
  archive  = {J},
  author   = {Panagiotis Liakos and Alexandros Ntoulas and Alex Delis},
  doi      = {10.1109/TBDATA.2019.2940450},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {432-445},
  title    = {Uncovering local hierarchical overlapping communities at scale},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Large-scale and scalable latent factor analysis via
distributed alternative stochastic gradient descent for recommender
systems. <em>IEEE Transactions on Big Data</em>, <em>8</em>(2), 420–431.
(<a href="https://doi.org/10.1109/TBDATA.2020.2973141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Latent factor analysis (LFA) via stochastic gradient descent (SGD) is highly efficient in discovering user and item patterns from high-dimensional and sparse (HiDS) matrices from recommender systems. However, most LFA-based recommender systems adopt a standard SGD algorithm, which suffers limited scalability when addressing big data. On the other hand, most existing parallel SGD solvers are either under the memory-sharing framework designed for a bare machine or suffering high communicational costs, which also greatly limits their applications in large-scale systems. To address the above issues, this article proposes a distributed alternative stochastic gradient descent (DASGD) solver for an LFA-based recommender. Its training-dependences among latent features are decoupled via alternatively fixing one-half of the features to learn the other half following the principle of SGD but in parallel. It&#39;s distribution mechanism consists of efficient data partition, allocation and task parallelization strategies, which greatly reduces its communicational cost for high scalability. Experimental results on three large-scale HiDS matrices generated by real-world applications demonstrate that the proposed DASGD algorithm outperforms state-of-the-art distributed SGD solvers for recommender systems in terms of prediction accuracy as well as scalability. Hence, it is highly useful for training LFA-based recommenders on large scale HiDS matrices with the help of cloud computing facilities.},
  archive  = {J},
  author   = {Xiaoyu Shi and Qiang He and Xin Luo and Yanan Bai and Mingsheng Shang},
  doi      = {10.1109/TBDATA.2020.2973141},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {420-431},
  title    = {Large-scale and scalable latent factor analysis via distributed alternative stochastic gradient descent for recommender systems},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Big data and emergency management: Concepts, methodologies,
and applications. <em>IEEE Transactions on Big Data</em>, <em>8</em>(2),
397–419. (<a href="https://doi.org/10.1109/TBDATA.2020.2972871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent decades have seen a significant increase in the frequency, intensity, and impact of natural disasters and other emergencies, forcing the governments around the world to make emergency response and disaster management national priorities. The growth of extremely large and complex datasets—commonly referred to as big data —and various advances in information and communications technology and computing now support more effective approaches to humanitarian relief, logistical coordination, overall disaster management, and long-term recovery in connection with natural disasters and emergency events. Leveraging big data and technological advances for emergency management has attracted considerable attention in the research community. However, the desired merging of big data and emergency management (BDEM) requires coordinated efforts to align and define interdisciplinary terminologies and methodologies. To date, the key concepts and technologies in this emerging research area have not been coherently discussed in a sufficiently broad and multidisciplinary manner. In this article, an international team presents an overview of the BDEM domain, highlighting a general framework and discussing key challenges from several perspectives. We introduce and summarize typical technologies and applications, organized into the six broad categories of remote sensing, resilient communication networks, mobile communication networks, human mobility and urban sensing, social network analysis, and knowledge graphs. Finally, we outline several directions of future research.},
  archive  = {J},
  author   = {Xuan Song and Haoran Zhang and Rajendra Akerkar and Huawei Huang and Song Guo and Lei Zhong and Yusheng Ji and Andreas L. Opdahl and Hemant Purohit and André Skupin and Akshay Pottathil and Aron Culotta},
  doi      = {10.1109/TBDATA.2020.2972871},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {397-419},
  title    = {Big data and emergency management: Concepts, methodologies, and applications},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual analytics of anomalous user behaviors: A survey.
<em>IEEE Transactions on Big Data</em>, <em>8</em>(2), 377–396. (<a
href="https://doi.org/10.1109/TBDATA.2020.2964169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the pervasive use of information technologies, the increasing availability of data provides new opportunities for understanding user behaviors. Unearthing anomalies in user behavior is of particular importance as it helps signal harmful incidents such as network intrusions, terrorist activities, and financial frauds. In this article, we survey state-of-the-art research work in visual analytics of anomalous user behaviors and classify them into four application domains, which are social interaction, travel, network communication, and financial transaction. We further examine the research work in each category in terms of data types, visualization techniques, and interactive analysis methods. We hope that our survey can provide systematic guidelines for researchers and practitioners to find effective solutions to their research problems in specific application domains. Finally, we discuss trends of academic interest over the past decades and suggest potential directions across visual analytics of these user behaviors for future research.},
  archive  = {J},
  author   = {Yang Shi and Yuyin Liu and Hanghang Tong and Jingrui He and Gang Yan and Nan Cao},
  doi      = {10.1109/TBDATA.2020.2964169},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {377-396},
  title    = {Visual analytics of anomalous user behaviors: A survey},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient big-data access: Taxonomy and a comprehensive
survey. <em>IEEE Transactions on Big Data</em>, <em>8</em>(2), 356–376.
(<a href="https://doi.org/10.1109/TBDATA.2020.3036813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The emerging systems are not only generating huge amounts of data but also expect this data to be analyzed expeditiously to drive online decision-making and control. Thus, identifying the most relevant data and making it available close to the computation becomes a central challenge in driving the big data revolution. Storage systems play a crucial role in enabling efficient access to the stored data and intelligent storage management techniques are thus central to addressing the problem. Generally, as the data volume increases, the marginal utility of an “average” data item tends to decline, which requires greater effort in identifying the most valuable data items and making them available with minimal overhead and latency. Data driven mechanisms have a big role to play in solving this needle-in-the-haystack problem. In this paper we propose a taxonomy to provide a structure for understanding the common issues surrounding these techniques. We discuss these techniques and articulate many research challenges and opportunities.},
  archive  = {J},
  author   = {Anis Alazzawe and Amitangshu Pal and Krishna Kant},
  doi      = {10.1109/TBDATA.2020.3036813},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {356-376},
  title    = {Efficient big-data access: Taxonomy and a comprehensive survey},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parallel secure outsourcing of large-scale nonlinearly
constrained nonlinear programming problems. <em>IEEE Transactions on Big
Data</em>, <em>8</em>(2), 346–355. (<a
href="https://doi.org/10.1109/TBDATA.2018.2821195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Nonlinearly constrained nonlinear programming (NLC-NLP) problems arise in various real-world decision-making fields, such as financial engineering, urban planning, supply chain management, and power system control. They are usually large-scale because of having to consider massive variables and constraints. Solving NLC-NLP problems by employing common algorithms (e.g., gradient projection method (GPM)) is usually computationally-expensive, which challenges common organizations in solving large-scale NLC-NLP problems. To address this issue, an option is to adopt cloud computing for help. However, this raises security concerns since real-world NLC-NLP problems may carry sensitive information. Although previous secure outsourcing algorithms try to protect sensitive information, they still let cloud service tenants bear heavy computation burden. In this paper, we develop a practical secure outsourcing algorithm for using the GPM to solve large-scale NLC-NLP problems. To be more prominent, to accelerate computations and avoid possible memory overflowing, we parallelize the developed algorithm. We implement the developed algorithm on the Amazon Elastic Compute Cloud (EC2) and a laptop, and also offer extensive experiment results to show that the developed algorithm can reduce the tenant’s computing time significantly.},
  archive  = {J},
  author   = {Changqing Luo and Jinlong Ji and Xuhui Chen and Ming Li and Laurence T. Yang and Pan Li},
  doi      = {10.1109/TBDATA.2018.2821195},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {346-355},
  title    = {Parallel secure outsourcing of large-scale nonlinearly constrained nonlinear programming problems},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). I/o workload management for all-flash datacenter storage
systems based on total cost of ownership. <em>IEEE Transactions on Big
Data</em>, <em>8</em>(2), 332–345. (<a
href="https://doi.org/10.1109/TBDATA.2018.2871114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, the capital expenditure of flash-based Solid State Driver (SSDs) keeps declining and the storage capacity of SSDs keeps increasing. As a result, all-flash storage systems have started to become more economically viable for large shared storage installations in datacenters, where metrics like Total Cost of Ownership (TCO) are of paramount importance. On the other hand, flash devices suffer from write amplification, which, if unaccounted, can substantially increase the TCO of a storage system. In this paper, we first develop a TCO model for datacenter all-flash storage systems, and then plug a Write Amplification model (WAF) of NVMe SSDs we build based on empirical data into this TCO model. Our new WAF model accounts for workload characteristics like write rate and percentage of sequential writes. Furthermore, using both the TCO and WAF models as the optimization criterion, we design new flash resource management schemes ( minTCO ) to guide datacenter managers to make workload allocation decisions under the consideration of TCO for SSDs. Based on that, we also develop minTCO-RAID to support RAID SSDs and minTCO-Offline to optimize the offline workload-disk deployment problem during the initialization phase. Experimental results show that minTCO can reduce the TCO and keep relatively high throughput and space utilization of the entire datacenter storage resources.},
  archive  = {J},
  author   = {Zhengyu Yang and Manu Awasthi and Mrinmoy Ghosh and Janki Bhimani and Ningfang Mi},
  doi      = {10.1109/TBDATA.2018.2871114},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {332-345},
  title    = {I/O workload management for all-flash datacenter storage systems based on total cost of ownership},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A stack-centric processing model for iterative processing.
<em>IEEE Transactions on Big Data</em>, <em>8</em>(2), 318–331. (<a
href="https://doi.org/10.1109/TBDATA.2018.2841363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Complex data mining algorithms are processed in multiple iterations, where output of one iteration is used as input for the subsequent iterations. Existing parallel programming frameworks, e.g., MapReduce, Pregel and Spark, adopt the breadth first search (BFS) strategy to process those iterative jobs. They invoke the user-defined functions for every key-value pair or vertex to produce all possible intermediate results for the next iteration. Such BFS strategy incurs high I/O overheads, because normally, the size of intermediate search results of BFS is exponential to the size of original data, making it impossible to maintain those intermediate results in memory. In this paper, we present a new type of parallel programming model, the stack-centric model, where all computations are defined for a stack maintained in the distributed shared memory. The stack can be adaptively split into multiple stacks and disseminated to different compute nodes for parallel processing. The most distinguished feature of the stack-centric model is its support for the depth first search (DFS) algorithm which incurs much less memory overhead than its BFS counterpart. The maximal memory usage of DFS algorithm is determined by the height of its search tree, and hence, it is possible to conduct the computation of DFS algorithm mostly in memory. Our stack-centric model is not a pure DFS framework. It supports the hybrid BFS and DFS algorithms by tuning the trade-off between memory usage and parallelism. To show the advantages of stack-centric model, we implement two algorithms, frequent pattern mining algorithm and DNA sequence matching algorithm, on both stack-centric model and Spark. The memory usage of stack-centric model is 10 times less than the Spark, resulting in a significant performance improvement.},
  archive  = {J},
  author   = {Zhifei Pang and Sai Wu and Gang Chen and Lidan Shou and Ke Chen and Bingsheng He},
  doi      = {10.1109/TBDATA.2018.2841363},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {318-331},
  title    = {A stack-centric processing model for iterative processing},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A shared-memory algorithm for updating tree-based properties
of large dynamic networks. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(2), 302–317. (<a
href="https://doi.org/10.1109/TBDATA.2018.2870136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper presents a network-based template for analyzing large-scale dynamic data. Specifically, we propose a novel shared-memory parallel algorithm for updating tree-based structures or properties, such as connected components (CC) and minimum spanning trees (MST), on dynamic networks. The underlying idea is to update the information in a rooted tree data structure that stores the edges of the network that are most relevant to the analysis. Extensive experiments on real-world and synthetic networks demonstrate that, with the exception of the inherently sequential component for creating the rooted tree, our proposed updatiing algorithm is scalable and, in most cases, also requires significantly less memory, energy, and time than recomputing-from-scratch algorithm. To the best of our knowledge, this is the first parallel algorithm for updating MST on weighted dynamic networks. The rooted-tree based framework that we propose in this paper can be extended for updating other weighted and unweighted tree-based properties such as single source shortest path and betweenness and closeness centrality.},
  archive  = {J},
  author   = {Sriram Srinivasan and Samuel D. Pollard and Boyana Norris and Sajal K. Das and Sanjukta Bhowmick},
  doi      = {10.1109/TBDATA.2018.2870136},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {302-317},
  title    = {A shared-memory algorithm for updating tree-based properties of large dynamic networks},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving bank-level parallelism for in-memory checkpointing
in hybrid memory systems. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(2), 289–301. (<a
href="https://doi.org/10.1109/TBDATA.2018.2865964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Checkpoint/recovery has been widely used in many high available and reliable systems. This paper proposes Shadow , an application-transparent and in-memory checkpointing mechanism based on hybrid memory system composed of DRAM and emerging Non-volatile Memory (NVM). Shadow adopts a pre-copy based checkpointing mechanism to reduce the system downtime. It supports fine-grained and incremental checkpointing at frequencies up to 100 times per second. Under this context, the checkpointing can significantly degrade application performance due to memory contention between applications and the checkpointing process. Previous checkpointing mechanisms on hybrid memory systems have focused on the performance of checkpointing, and have overlooked the impact of memory contention on the application performance. In this paper, we mitigate the memory contention at the bank level by carefully scheduling memory requests to fully leverage the idle time slots of different memory banks. Moreover, if bank conflicts are unavoidable, Shadow promotes the priority of applications’ memory requests to lessen their access latencies. By redesigning the memory controllers of DRAM and NVM, we implement a hardware-assisted checkpointing mechanism that can directly transfer data from working memory to the checkpoint in NVM, without any intervention of CPUs. Our evaluation shows that Shadow can reduce memory bank conflicts between applications and checkpointing by 75 percent, and decrease applications’ memory read request latency by 28 percent on average compared to the pre-copy based checkpointing. Moreover, Shadow can also reduce checkpointing overhead by 42 and 16 percent on average compared to the stop-and-copy and pre-copy based checkpointing approaches, respectively.},
  archive  = {J},
  author   = {Xiaofei Liao and Zhan Zhang and Haikun Liu and Hai Jin},
  doi      = {10.1109/TBDATA.2018.2865964},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {289-301},
  title    = {Improving bank-level parallelism for in-memory checkpointing in hybrid memory systems},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LSTM based phishing detection for big email data. <em>IEEE
Transactions on Big Data</em>, <em>8</em>(1), 278–288. (<a
href="https://doi.org/10.1109/TBDATA.2020.2978915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, cyber criminals have successfully invaded many important information systems by using phishing mail, causing huge losses. The detection of phishing mail from big email data has been paid public attention. However, the camouflage technology of phishing mail is becoming more and more complex, and the existing detection methods are unable to confront with the increasingly complex deception methods and the growing number of emails. In this article, we proposed an LSTM based phishing detection method for big email data. The new method includes two important stages, sample expansion stage and testing stage under sufficient samples. In the sample expansion stage, we combined KNN with K-Means to expand the training data set, so that the size of training samples can meet the needs of in-depth learning. In the testing stage, we first preprocess these samples, including generalization, word segmentation and word vector generation. Then, the preprocessed data is used to train a LSTM model. Finally, on the basis of the trained model, we classify the phishing emails. By experiment, we evaluate the performance of the proposed method, and experimental results show that the accuracy of our phishing detection method can reach 95 percent.},
  archive  = {J},
  author   = {Qi Li and Mingyu Cheng and Junfeng Wang and Bowen Sun},
  doi      = {10.1109/TBDATA.2020.2978915},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {278-288},
  title    = {LSTM based phishing detection for big email data},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fuzzy deep neural learning based on goodman and kruskal's
gamma for search engine optimization. <em>IEEE Transactions on Big
Data</em>, <em>8</em>(1), 268–277. (<a
href="https://doi.org/10.1109/TBDATA.2020.2963982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Search engine optimization (SEO) is a significant problem for enhancing a website&amp;#x0027;s visibility with search engine results. SEO issues, such as Site Popularity, Content Quality, Keyword Density, and Publicity, were not considered during the search engine optimization process. Therefore, the retrieval rate of the existing techniques is inadequate. In this study, Triangular Fuzzy Deep Structured Learning-Based Predictive Page Ranking (TFDSL-PPR) technique is proposed to solve these limitations. First, the TFDSL-PPR technique takes a number of user queries as input in the input layer, and then it employs four hidden layers in order to deeply analyze the web pages based on an input query. The first hidden layer determines the keywords from the user query. The second hidden layer measures the site popularity, content quality, keyword density and publicity of all web pages in the search engine. It then accomplishes Goodman and Kruskal&amp;#x0027;s Gamma Predictive Ranking process in the third hidden layer, where it ranks the web pages by considering their similarities. The proposed TFDSL-PPR technique is applied to the ClueWeb09 Dataset with respect to a variety of user queries. The results are benchmarked by existing methods based on several metrics such as retrieval rate, time, and false-positive rate.},
  archive  = {J},
  author   = {Sethuraman Jayaraman and Manikandan Ramachandran and Rizwan Patan and Mahmoud Daneshmand and Amir H. Gandomi},
  doi      = {10.1109/TBDATA.2020.2963982},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {268-277},
  title    = {Fuzzy deep neural learning based on goodman and kruskal&amp;#x0027;s gamma for search engine optimization},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shadow: Exploiting the power of choice for efficient
shuffling in MapReduce. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(1), 253–267. (<a
href="https://doi.org/10.1109/TBDATA.2019.2943473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {How to reduce the costly cross-rack data transferring is challenging in improving the performance of MapReduce platforms. Previous schemes mainly exploit the data locality in the Map phase to reduce the cross-rack communications. However, the Map locality based schemes may lead to highly skewed distribution of Map tasks across racks in the platform, resulting in serious load imbalance among different cross-rack links during Shuffling. Recent research results show that the slow Shuffling is the root cause of the MapReduce performance degradation. Very limited work has been done for speeding up the Shuffle phase. A notable scheme leverages the principle of the power of choice to balance the network loads on different cross-rack links during Shuffling for a specific type of sampling applications, where processing a random subset of the large-scale data collection is sufficient to derive the final result. The scheme launches a few additional tasks to offer more choices for task selection during Shuffling. However, such a scheme is designed for sampling applications and not applicable to general applications, where all the input data instead of a random subset is processed. In this work, we observe that with high Map locality, the network is mainly saturated in Shuffling but relatively free in the Map phase. A little sacrifice in Map locality may greatly accelerate Shuffling. Based on this, we propose a novel scheme called Shadow for Shuffle-constrained general applications, which strikes a trade-off between Map locality and Shuffling load balance. Specifically, Shadow iteratively chooses an original Map task from the most heavily loaded rack and creates a duplicated task for it on the most lightly loaded rack. During processing, Shadow makes a choice between an original task and its replica by efficiently pre-estimating the job execution time. We conduct extensive experiments to evaluate the Shadow design. Results show that Shadow greatly reduces the cross-rack skewness by 36.6 percent and the job execution time by 26 percent compared to existing schemes.},
  archive  = {J},
  author   = {Sijie Wu and Hanhua Chen and Hai Jin and Shadi Ibrahim},
  doi      = {10.1109/TBDATA.2019.2943473},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {253-267},
  title    = {Shadow: Exploiting the power of choice for efficient shuffling in MapReduce},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identification of encrypted traffic through attention
mechanism based long short term memory. <em>IEEE Transactions on Big
Data</em>, <em>8</em>(1), 241–252. (<a
href="https://doi.org/10.1109/TBDATA.2019.2940675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Network traffic classification has become an important part of network management, which is beneficial for achieving intelligent network operation and maintenance, enhancing the network quality of service (QoS), and for network security. Given the rapid development of various applications and protocols, more and more encrypted traffic has emerged in networks. Traditional traffic classification methods exhibited the unsatisfied performance since the encrypted traffic is no longer in plain text. In this work, we modeled the time-series network traffic by the recurrent neural network (RNN). Moreover, the attention mechanism was introduced for assisting network traffic classification in the form of the following two models, the attention aided long short term memory (LSTM) as well as the hierarchical attention network (HAN). Finally, relying on the ISCX VPN-NonVPN dataset, extensive experiments were conducted, showing that the proposed methods achieved 91.2 percent in accuracy while the highest accuracy of other methods was 89.8 percent relying on the same dataset.},
  archive  = {J},
  author   = {Haipeng Yao and Chong Liu and Peiying Zhang and Sheng Wu and Chunxiao Jiang and Shui Yu},
  doi      = {10.1109/TBDATA.2019.2940675},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {241-252},
  title    = {Identification of encrypted traffic through attention mechanism based long short term memory},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Forecasting people’s needs in hurricane events from social
network. <em>IEEE Transactions on Big Data</em>, <em>8</em>(1), 229–240.
(<a href="https://doi.org/10.1109/TBDATA.2019.2941887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Social networks can serve as a valuable communication channel for asking for help, offering assistance, and coordinating rescue activities in disaster because it allows users to continuously update critical information in the fast-changing disaster environment. This paper presents a novel sequence to sequence based framework for forecasting people&amp;#x2019;s needs during disasters using social media and weather data. It consists of two Long Short-Term Memory (LSTM) models, one of which encodes input sequences of weather information and the other plays as a conditional decoder that decodes the encoded vector and forecasts the survivors&amp;#x2019; needs. Case studies using data collected during Hurricane Sandy in 2012, Hurricane Harvey and Hurricane Irma in 2017 demonstrate that the proposed approach outperformed the statistical language model n-gram, LSTM generative model, and convolutional neural network (CNN) based model. This research indicates its great promise for enhancing disaster management such as evacuation planning and commodity delivery.},
  archive  = {J},
  author   = {Long Nguyen and Zhou Yang and Jia Li and Zhenhe Pan and Guofeng Cao and Fang Jin},
  doi      = {10.1109/TBDATA.2019.2941887},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {229-240},
  title    = {Forecasting people&amp;#x2019;s needs in hurricane events from social network},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enabling fast exploratory analyses over voluminous
spatiotemporal data using analytical engines. <em>IEEE Transactions on
Big Data</em>, <em>8</em>(1), 213–228. (<a
href="https://doi.org/10.1109/TBDATA.2019.2939834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Fueled by the proliferation of IoT devices and increased adoption of sensing environments the collection of spatiotemporal data has exploded in recent years. Disk based storage systems provide reliable archives but are far too slow for efficient analytics. Furthermore, spatiotemporal datasets quickly exceed the memory capacity of cluster environments. Current solutions focused on in-memory analytics suffer from memory contention and unnecessary network I/O, failing to provide a suitable platform for iterative, exploratory analytics in shared environments. In this work we propose Anamnesis, the first in-memory, sketch aligned, HDFS compliant storage system. Data sketching algorithms reduce dataset sizes by summarizing feature values and inter-feature relationships. Anamnesis leverages data sketches to alleviate memory contention and vastly reduce network I/O during analytics. Upon request, we generate accurate full-resolution datasets with negligible resource and time costs. Datasets are available using a fully HDFS compliant interface allowing Anamnesis to achieve unprecedented compatibility with popular analytics engines. This facilitates adoption into existing workflows by serving as a “drop-in” replacement for canonical HDFS. We evaluate the system using 2 spatiotemporal datasets, a variety of popular analytics engines, and real-world analytical operations.},
  archive  = {J},
  author   = {Daniel Rammer and Thilina Buddhika and Matthew Malensek and Shrideep Pallickara and Sangmi Lee Pallickara},
  doi      = {10.1109/TBDATA.2019.2939834},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {213-228},
  title    = {Enabling fast exploratory analyses over voluminous spatiotemporal data using analytical engines},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BiLSTM-SSVM: Training the BiLSTM with a structured hinge
loss for named-entity recognition. <em>IEEE Transactions on Big
Data</em>, <em>8</em>(1), 203–212. (<a
href="https://doi.org/10.1109/TBDATA.2019.2938163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Building on the achievements of the BiLSTM-CRF in named-entity recognition (NER), this paper introduces the BiLSTM-SSVM, an equivalent neural model where training is performed using a structured hinge loss. The typical loss functions used for evaluating NER are entity-level variants of the &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$F_1$&lt;/tex-math&gt;&lt;/inline-formula&gt; score such as the CoNLL and MUC losses. Unfortunately, the common loss function used for training NER - the cross entropy - is only loosely related to the evaluation losses. For this reason, in this paper we propose a training approach for the BiLSTM-CRF that leverages a hinge loss bounding the CoNLL loss from above. In addition, we present a mixed hinge loss that bounds either the CoNLL loss or the Hamming loss based on the density of entity tokens in each sentence. The experimental results over four benchmark languages (English, German, Spanish and Dutch) show that training with the mixed hinge loss has led to small but consistent improvements over the cross entropy across all languages and four different evaluation measures.},
  archive  = {J},
  author   = {Hanieh Poostchi and Massimo Piccardi},
  doi      = {10.1109/TBDATA.2019.2938163},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {203-212},
  title    = {BiLSTM-SSVM: Training the BiLSTM with a structured hinge loss for named-entity recognition},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast compressive spectral clustering for large-scale sparse
graph. <em>IEEE Transactions on Big Data</em>, <em>8</em>(1), 193–202.
(<a href="https://doi.org/10.1109/TBDATA.2019.2931532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Spectral clustering (SC) is an unsupervised learning method that has been widely used in industrial product analysis. Compressive spectral clustering (CSC) effectively accelerates clustering by leveraging graph filter and random sampling techniques. However, CSC suffers from two major problems. First, the direct use of the dichotomy and eigencount techniques for estimating Laplacian matrix&amp;#x0027;s &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;mml:math&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=&quot;zhang-ieq1-2931532.gif&quot;/&gt;&lt;/inline-formula&gt;th eigenvalue is expensive. Second, the computation of interpolation is time-consuming because it requires to repeat matrix-vector product for every cluster in each iteration. To address these problems, we propose a new method called fast compressive spectral clustering (FCSC). Our method addresses the first problem by assuming that the eigenvalues approximately satisfy local uniform distribution, and addresses the second problem by recalculating the pairwise similarity between nodes with low-dimensional representation to reconstruct denoised laplacian matrix. The time complexity of reconstruction is linear with the number of non-zeros in Laplacian matrix. As experimentally demonstrated on both artificial and real-world datasets, our method significantly reduces the computation time while preserving high clustering accuracy comparable to previous designs, demonstrating the effectiveness of FCSC.},
  archive  = {J},
  author   = {Ting Li and Yiming Zhang and Hao Liu and Guangtao Xue and Ling Liu},
  doi      = {10.1109/TBDATA.2019.2931532},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {193-202},
  title    = {Fast compressive spectral clustering for large-scale sparse graph},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finding and tracking multi-density clusters in online
dynamic data streams. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(1), 178–192. (<a
href="https://doi.org/10.1109/TBDATA.2019.2922969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Change is one of the biggest challenges in dynamic stream mining. From a data-mining perspective, adapting and tracking change is desirable in order to understand how and why change has occurred. Clustering, a form of unsupervised learning, can be used to identify the underlying patterns in a stream. Density-based clustering identifies clusters as areas of high density separated by areas of low density. This paper proposes a Multi-Density Stream Clustering (MDSC) algorithm to address these two problems; the multi-density problem and the problem of discovering and tracking changes in a dynamic stream. MDSC consists of two on-line components; discovered, labelled clusters and an outlier buffer. Incoming points are assigned to a live cluster or passed to the outlier buffer. New clusters are discovered in the buffer using an ant-inspired swarm intelligence approach. The newly discovered cluster is uniquely labelled and added to the set of live clusters. Processed data is subject to an ageing function and will disappear when it is no longer relevant. MDSC is shown to perform favourably to state-of-the-art peer stream-clustering algorithms on a range of real and synthetic data-streams. Experimental results suggest that MDSC can discover qualitatively useful patterns while being scalable and robust to noise.},
  archive  = {J},
  author   = {Conor Fahy and Shengxiang Yang},
  doi      = {10.1109/TBDATA.2019.2922969},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {178-192},
  title    = {Finding and tracking multi-density clusters in online dynamic data streams},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards the inference of travel purpose with heterogeneous
urban data. <em>IEEE Transactions on Big Data</em>, <em>8</em>(1),
166–177. (<a href="https://doi.org/10.1109/TBDATA.2019.2921823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In people’s daily lives, travel takes up an important part, and many trips are generated everyday, such as going to school or shopping. With the widely adoption of GPS-integrated devices, a large amount of trips can be recorded with GPS trajectories. These trajectories are represented by sequences of geo-coordinates and can help us answer simple questions such as “where did you go”. However, there is another important question awaiting to be answered, that is “what did/will you do”, i.e., the trip purpose inference. In practice, people’s trip purposes are very important in understanding travel behaviors and estimating travel demands. Obviously, it is very challenging to infer trip purposes solely based on the trajectories, because the GPS devices are not accurate enough to pinpoint the venues visited. In this paper, we infer individual’s trip purposes by combining the knowledge from heterogeneous data sources including trajectories, POIs and social media data. The proposed Dynamic Bayesian Network model ( DBN ) captures three important factors: the sequential properties of trip activities, the functionality and POI popularity of trip end areas. In addition, we propose an efficient method with local candidate pools to identify POIs from geo-tagged social media messages, and learn the POI popularities from nearby social media data. Moreover, trip data is usually imbalanced across different activities. This data imbalance problem can cause serious challenges because the DBN model could be biased by those “popular” class labels. Considering this challenge, we propose an ensemble DBN method with sampling technique ( eDBN ) which results in more accurate inference. Furthermore, real-world trip data are continuously collected on a daily basis. The batch model would result in unnecessary computation because historical data need to be revisited. We handle this problem by proposing an incremental DBN method ( iDBN ) which is both effective and efficient. Extensive experiments are conducted on real-world data sets with trajectories of 8,361 residents and the 6.9 million geo-tagged tweets in the Bay area. Experimental results demonstrate the advantages of the proposed method on correctly inferring the trip purposes.},
  archive  = {J},
  author   = {Chuishi Meng and Yu Cui and Qing He and Lu Su and Jing Gao},
  doi      = {10.1109/TBDATA.2019.2921823},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {166-177},
  title    = {Towards the inference of travel purpose with heterogeneous urban data},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Survey and analysis of current end-user data analytics tool
support. <em>IEEE Transactions on Big Data</em>, <em>8</em>(1), 152–165.
(<a href="https://doi.org/10.1109/TBDATA.2019.2921774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {There has been a very large growth in interest in big data analytics to discover patterns and insights. A major challenge in this domain is the need to combine domain knowledge – what the data means (semantics) and what it is used for – with advanced data analytics and visualization techniques to mine and communicate important information from the huge volumes of raw data. Many data analytics tools have been developed for both research and practice to assist in specifying, integrating and deploying data analytics applications. However, delivering such big data analytics applications requires a capable team with different skillsets including data scientists, software engineers and domain experts. Such teams and skillsets usually take a long time to build and have high running costs. An alternative is to provide domain experts and data scientists – the end users – with tools they can use to create and deploy complex data analytics application solutions directly with less technical skills required. In this paper we present a survey and analysis of several current research and practice approaches to supporting data analytics for end-users, identifying key strengths, weaknesses and opportunities for future research.},
  archive  = {J},
  author   = {Hourieh Khalajzadeh and Mohamed Abdelrazek and John Grundy and John Hosking and Qiang He},
  doi      = {10.1109/TBDATA.2019.2921774},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {152-165},
  title    = {Survey and analysis of current end-user data analytics tool support},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comprehensive trustworthy data collection approach in
sensor-cloud systems. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(1), 140–151. (<a
href="https://doi.org/10.1109/TBDATA.2018.2811501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Nowadays, sensor-cloud systems have received wide attention from both academia and industry. Sensor-cloud system not only improves performances of wireless sensor networks (WSNs), but also combines different functional WSNs together to provide comprehensive services. However, a variety of malicious attacks threaten the sensor-cloud security, such as integrity, authenticity, availability and so on. Traditional available security mechanisms (e.g., cryptography and authentication) are still vulnerable. Although there are schemes to provide security by trust evaluation, the evaluation considers whether or not a sensor is credible only by checking the communication behaviors. Furthermore, when mobile sensor sinks are employed to collect sensing data, there appears a type of attacks called replicated sink attacks that are often ignored in the previous work. These attacks may bring serious vulnerability to trustworthy data collection in sensor-cloud systems. In this paper, we propose a comprehensive trustworthy data collection (CTDC) approach for sensor-cloud systems. Three kinds of trust, i.e., direct trust, indirect trust, and functional trust are defined to evaluate the trustworthiness of both sensors and mobile sinks. Except for resisting malicious attacks, the performances of sensor-cloud, such as energy, transmission distance and network throughput are also considered. We also conduct extensive simulations to evaluate the efficiency of CTDC. The simulation results show that CTDC correctly identifies malicious nodes and offers an improved performance in the data collection.},
  archive  = {J},
  author   = {Tian Wang and Yang Li and Weiwei Fang and Wenzheng Xu and Junbin Liang and Yewang Chen and Xuxun Liu},
  doi      = {10.1109/TBDATA.2018.2811501},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {140-151},
  title    = {A comprehensive trustworthy data collection approach in sensor-cloud systems},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MRMondrian: Scalable multidimensional anonymisation for big
data privacy preservation. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(1), 125–139. (<a
href="https://doi.org/10.1109/TBDATA.2017.2787661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Scalable data processing platforms built on cloud computing becomes increasingly attractive as infrastructure for supporting big data applications. But privacy concerns are one of the major obstacles to making use of public cloud platforms. Multidimensional anonymisation, a global-recoding generalisation scheme for privacy-preserving data publishing, has been a recent focus due to its capability of balancing data obfuscation and usability. Existing multidimensional anonymisation methods suffer from scalability problems when handling big data due to the impractical serial I/O cost. Given the recursive feature of multidimensional anonymisation, parallelisation is an ideal solution to scalability issues. However, it is still a challenge to use existing distributed and parallel paradigms directly for recursive computation. In this paper, we propose a scalable approach for big data multidimensional anonymisation based on MapReduce, a state-of-the-art data processing paradigm. Our basic idea is to partition a data set recursively into smaller partitions using MapReduce until all partitions can fit in the memory of a computing node. A tree indexing structure is proposed to achieve recursive computation. Moreover, we show the applicability of our approach to differential privacy. Experimental results on real-life data demonstrate that our approach can significantly improve the scalability of multidimensional anonymisation over existing methods.},
  archive  = {J},
  author   = {Xuyun Zhang and Lianyong Qi and Wanchun Dou and Qiang He and Christopher Leckie and Ramamohanarao Kotagiri and Zoran Salcic},
  doi      = {10.1109/TBDATA.2017.2787661},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {125-139},
  title    = {MRMondrian: Scalable multidimensional anonymisation for big data privacy preservation},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trust based incentive scheme to allocate big data tasks with
mobile social cloud. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(1), 113–124. (<a
href="https://doi.org/10.1109/TBDATA.2017.2764925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, mobile social cloud (MSC), formed by mobile users with social ties, has been advocated to allocate tasks of big data applications instead of relying on the conventional cloud systems. However, due to the dynamic topology of networks and social features of users, how to optimally allocate tasks to mobile users based on the trust becomes a new challenge. Therefore, this paper proposes a novel incentive scheme based on the trust of mobile users in the MSC to allocate the tasks of big data. First, a social trust degree is defined according to the social tie among users, the importance of task, and the available resources of networks. With the social trust degree, the task owner can select a group of mobile users as the candidates for task allocation. Second, a reverse auction game model is developed to study the interactions among the task owner and the candidates. With the reverse auction game model, the optimal strategy of task allocation can be obtained with a low cost for the task owner where the selected candidate of mobile users can also obtain the high profit. Finally, simulation experiments are carried out to prove that the proposal can outperform other existing methods with a low delay and a high efficiency to allocate tasks in the MSC.},
  archive  = {J},
  author   = {Qichao Xu and Zhou Su and Shui Yu and Ying Wang},
  doi      = {10.1109/TBDATA.2017.2764925},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {113-124},
  title    = {Trust based incentive scheme to allocate big data tasks with mobile social cloud},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient trustworthiness management for malicious user
detection in big data collection. <em>IEEE Transactions on Big
Data</em>, <em>8</em>(1), 99–112. (<a
href="https://doi.org/10.1109/TBDATA.2017.2761386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Data collection in big data is an effective way to aggregate information that the collector is interested in. However, there is no assurance for the data that the users provide. Since collector does not have the ability to check the authenticity of every piece of information, the trustworthiness of users participated in the collection become important. In this paper, we design an efficient approach to calculate users’ trustworthiness in data collection for big data context. We divide the trustworthiness into familiarity trustworthiness and similarity trustworthiness, and study the influences of user actions on trustworthiness. To prevent malicious users from raising their trustworthiness and providing false information that may mislead final results, we also design a security queue to record users’ historical trust information, so that we can detect malicious users with high accuracy. Simulation results show that our model can sensitively resist the malicious actions of users.},
  archive  = {J},
  author   = {Jiahui Yu and Kun Wang and Peng Li and Rui Xia and Song Guo and Minyi Guo},
  doi      = {10.1109/TBDATA.2017.2761386},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {99-112},
  title    = {Efficient trustworthiness management for malicious user detection in big data collection},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). WukaStore: Scalable, configurable and reliable data storage
on hybrid volunteered cloud and desktop systems. <em>IEEE Transactions
on Big Data</em>, <em>8</em>(1), 85–98. (<a
href="https://doi.org/10.1109/TBDATA.2017.2758791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we propose a hybrid storage framework WukaStore, which offers a scalable, configurable and reliable big data storage service by integrating stable storage (such as Cloud storage or durable storage utilities) and volatile storage (such as idle storage harnessed from desktop PCs over the Internet). By configuring different storage strategies, WukaStore delivers cost-effective storage service to satisfy users&amp;#x2019; requirements. We present trace-driven simulations to study the impact of different strategies and how to ensure high availability and durability in WukaStore. We present the prototype implementation of WukaStore based on BitDew, an open source middleware for Big Data management. We conduct performance evaluations to validate the effectiveness of WukaStore, through the deployment of WukaStore in the French Grid&amp;#x0027;5000 experimental platform. In particular, we also present a case study where WukaStore is deployed in a hybrid environment taking advantage of storage provided by Amazon S3, Dropbox and a local Desktop Grid at the same time. Our evaluation results show that WukaStore provides users with an alternative method to store their big data on hybrid volunteered cloud and desktop systems and to decrease the cost of data storage, while providing great data access performance, scalability as well as reliability.},
  archive  = {J},
  author   = {Bing Tang and Gilles Fedak},
  doi      = {10.1109/TBDATA.2017.2758791},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {85-98},
  title    = {WukaStore: Scalable, configurable and reliable data storage on hybrid volunteered cloud and desktop systems},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Achieving efficient and privacy-preserving cross-domain big
data deduplication in cloud. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(1), 73–84. (<a
href="https://doi.org/10.1109/TBDATA.2017.2721444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Secure data deduplication can significantly reduce the communication and storage overheads in cloud storage services, and has potential applications in our big data-driven society. Existing data deduplication schemes are generally designed to either resist brute-force attacks or ensure the efficiency and data availability, but not both conditions. We are also not aware of any existing scheme that achieves accountability, in the sense of reducing duplicate information disclosure (e.g., to determine whether plaintexts of two encrypted messages are identical). In this paper, we investigate a three-tier cross-domain architecture, and propose an efficient and privacy-preserving big data deduplication in cloud storage (hereafter referred to as EPCDD). EPCDD achieves both privacy-preserving and data availability, and resists brute-force attacks. In addition, we take accountability into consideration to offer better privacy assurances than existing schemes. We then demonstrate that EPCDD outperforms existing competing schemes, in terms of computation, communication and storage overheads. In addition, the time complexity of duplicate search in EPCDD is logarithmic.},
  archive  = {J},
  author   = {Xue Yang and Rongxing Lu and Kim Kwang Raymond Choo and Fan Yin and Xiaohu Tang},
  doi      = {10.1109/TBDATA.2017.2721444},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {73-84},
  title    = {Achieving efficient and privacy-preserving cross-domain big data deduplication in cloud},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time series anomaly detection for trustworthy services in
cloud computing systems. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(1), 60–72. (<a
href="https://doi.org/10.1109/TBDATA.2017.2711039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As a powerful architecture for large-scale computation, cloud computing has revolutionized the way that computing infrastructure is abstracted and utilized. Coupled with the challenges caused by Big Data, the rocketing development of cloud computing boosts the complexity of system management and maintenance, resulting in weakened trustworthiness of cloud services. To cope with this problem, a compelling method, i.e., Support Vector Data Description (SVDD), is investigated in this paper for detecting anomalous performance metrics of cloud services. Although competent in general anomaly detection, SVDD suffers from unsatisfactory false alarm rate and computational complexity in time series anomaly detection, which considerably hinders its practical applications. Therefore, this paper proposes a relaxed form of linear programming SVDD (RLPSVDD) and presents important insights into parameter selection for practical time series anomaly detection in order to monitor the operations of cloud services. Experiments on the Iris dataset and the Yahoo benchmark datasets validate the effectiveness of our approaches. Furthermore, the comparison of RLPSVDD and the methods obtained from Twitter, Numenta, Etsy and Yahoo, shows the overall preference for RLPSVDD in time series anomaly detection.},
  archive  = {J},
  author   = {Chengqiang Huang and Geyong Min and Yulei Wu and Yiming Ying and Ke Pei and Zuochang Xiang},
  doi      = {10.1109/TBDATA.2017.2711039},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {60-72},
  title    = {Time series anomaly detection for trustworthy services in cloud computing systems},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trustworthiness evaluation-based routing protocol for
incompletely predictable vehicular ad hoc networks. <em>IEEE
Transactions on Big Data</em>, <em>8</em>(1), 48–59. (<a
href="https://doi.org/10.1109/TBDATA.2017.2710347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Incompletely predictable vehicular ad hoc networks is a type of networks where vehicles move in a certain range or just in a particular tendency, which is very similar to some circumstances in reality. However, how to route in such type of networks more efficiently according to the node motion characteristics and related historical big data is still an open issue. In this paper, we propose a novel routing protocol named trustworthiness evaluation-based routing protocol (TERP). In our protocol, trustworthiness of each individual is calculated by the cloud depending on the attribute parameters uploaded by the corresponding vehicle. In addition, according to the trustworthiness provided by the cloud, vehicles in the network choose reliable forward nodes and complete the entire route. The analysis shows that our protocol can effectively improve the fairness of the trustworthiness judgement. In the simulation, our protocol has a good performance in terms of the packet delivery ratio, normalized routing overhead and average end-to-end delay.},
  archive  = {J},
  author   = {Jian Shen and Chen Wang and Aniello Castiglione and Dengzhi Liu and Christian Esposito},
  doi      = {10.1109/TBDATA.2017.2710347},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {48-59},
  title    = {Trustworthiness evaluation-based routing protocol for incompletely predictable vehicular ad hoc networks},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward architectural and protocol-level foundation for
end-to-end trustworthiness in cloud/fog computing. <em>IEEE Transactions
on Big Data</em>, <em>8</em>(1), 35–47. (<a
href="https://doi.org/10.1109/TBDATA.2017.2705418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With Cloud/Fog Computing being a paradigm combination of IoT context and Edge Computing extended with Cloud/Fog, business process in it involves dataflows among multi-layers and multi-nodes, possibly provided by multi-organizations. Achieving end-to-end trustworthiness over the whole dataflow in such a Cloud/Fog Computing context is a challenging issue, nonetheless a necessary pre-condition for a successful business process on intra-/inter- organizational level. This paper investigates technical conundrums related to this target and proposes a policy-based approach for trustworthiness governance. An architectural layout is proposed with according modules, by carrying out two methodologies. One resides in tracing data derivation and maintaining security-level over the whole dataflow, handling data aggregation with several protocols. The other is to express data owner trustworthiness requirements with an enhanced attribute-based access control policy model and to evaluate data accessing nodes&amp;#x2019; trustworthiness-related properties. Experiments show that processing time per attribute pair drops as the scales of policies increase, suggesting good scaling property of the system.},
  archive  = {J},
  author   = {Ziyi Su and Fr&amp;#x00E9;d&amp;#x00E9;rique Biennier and Zhihan Lv and Yong Peng and Houbing Song and Jingwei Miao},
  doi      = {10.1109/TBDATA.2017.2705418},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {35-47},
  title    = {Toward architectural and protocol-level foundation for end-to-end trustworthiness in Cloud/Fog computing},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PPHOPCM: Privacy-preserving high-order possibilistic c-means
algorithm for big data clustering with cloud computing. <em>IEEE
Transactions on Big Data</em>, <em>8</em>(1), 25–34. (<a
href="https://doi.org/10.1109/TBDATA.2017.2701816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As one important technique of fuzzy clustering in data mining and pattern recognition, the possibilistic c-means algorithm (PCM) has been widely used in image analysis and knowledge discovery. However, it is difficult for PCM to produce a good result for clustering big data, especially for heterogenous data, since it is initially designed for only small structured dataset. To tackle this problem, the paper proposes a high-order PCM algorithm (HOPCM) for big data clustering by optimizing the objective function in the tensor space. Further, we design a distributed HOPCM method based on MapReduce for very large amounts of heterogeneous data. Finally, we devise a privacy-preserving HOPCM algorithm (PPHOPCM) to protect the private data on cloud by applying the BGV encryption scheme to HOPCM, In PPHOPCM, the functions for updating the membership matrix and clustering centers are approximated as polynomial functions to support the secure computing of the BGV scheme. Experimental results indicate that PPHOPCM can effectively cluster a large number of heterogeneous data using cloud computing without disclosure of private data.},
  archive  = {J},
  author   = {Qingchen Zhang and Laurence T. Yang and Zhikui Chen and Peng Li},
  doi      = {10.1109/TBDATA.2017.2701816},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {25-34},
  title    = {PPHOPCM: Privacy-preserving high-order possibilistic c-means algorithm for big data clustering with cloud computing},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NPP: A new privacy-aware public auditing scheme for cloud
data sharing with group users. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(1), 14–24. (<a
href="https://doi.org/10.1109/TBDATA.2017.2701347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Today, cloud storage becomes one of the critical services, because users can easily modify and share data with others in cloud. However, the integrity of shared cloud data is vulnerable to inevitable hardware faults, software failures or human errors. To ensure the integrity of the shared data, some schemes have been designed to allow public verifiers (i.e., third party auditors) to efficiently audit data integrity without retrieving the entire users’ data from cloud. Unfortunately, public auditing on the integrity of shared data may reveal data owners’ sensitive information to the third party auditor. In this paper, we propose a new privacy-aware public auditing mechanism for shared cloud data by constructing a homomorphic verifiable group signature. Unlike the existing solutions, our scheme requires at least t group managers to recover a trace key cooperatively, which eliminates the abuse of single-authority power and provides non-frameability. Moreover, our scheme ensures that group users can trace data changes through designated binary tree; and can recover the latest correct data block when the current data block is damaged. In addition, the formal security analysis and experimental results indicate that our scheme is provably secure and efficient.},
  archive  = {J},
  author   = {Anmin Fu and Shui Yu and Yuqing Zhang and Huaqun Wang and Chanying Huang},
  doi      = {10.1109/TBDATA.2017.2701347},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {14-24},
  title    = {NPP: A new privacy-aware public auditing scheme for cloud data sharing with group users},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Revocable identity-based access control for big data with
verifiable outsourced computing. <em>IEEE Transactions on Big Data</em>,
<em>8</em>(1), 1–13. (<a
href="https://doi.org/10.1109/TBDATA.2017.2697448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {To be able to leverage big data to achieve enhanced strategic insight, process optimization and make informed decision, we need to be an efficient access control mechanism for ensuring end-to-end security of such information asset. Signcryption is one of several promising techniques to simultaneously achieve big data confidentiality and authenticity. However, signcryption suffers from the limitation of not being able to revoke users from a large-scale system efficiently. We put forward, in this paper, the first identity-based (ID-based) signcryption scheme with efficient revocation as well as the feature to outsource unsigncryption to enable secure big data communications between data collectors and data analytical system(s). Our scheme is designed to achieve end-to-end confidentiality, authentication, non-repudiation, and integrity simultaneously, while providing scalable revocation functionality such that the overhead demanded by the private key generator (PKG) in the key-update phase only increases logarithmically based on the cardiality of users. Although in our scheme the majority of the unsigncryption tasks are outsourced to an untrusted cloud server, this approach does not affect the security of the proposed scheme. We then prove the security of our scheme, as well as demonstrating its utility using simulations.},
  archive  = {J},
  author   = {Hu Xiong and Kim-Kwang Raymond Choo and Athanasios V. Vasilakos},
  doi      = {10.1109/TBDATA.2017.2697448},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {1-13},
  title    = {Revocable identity-based access control for big data with verifiable outsourced computing},
  volume   = {8},
  year     = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
