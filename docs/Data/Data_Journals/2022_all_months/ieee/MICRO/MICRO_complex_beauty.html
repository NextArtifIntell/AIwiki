<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MICRO_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="micro---106">MICRO - 106</h2>
<ul>
<li><details>
<summary>
(2022b). Distributed discretion by the slice. <em>MICRO</em>,
<em>42</em>(6), 142–144. (<a
href="https://doi.org/10.1109/MM.2022.3208464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reports on the success of Domino&#39;s Pizza as a company who has profited from the investment in information technology. Modern information technology has grown from an assortment and assembly of innovative improvements since the commercialization of the Internet in the middle of the 1990s. Domino’s Pizza is among the largest fast-food firms in the world. Domino’s today sells three million pizzas a day in over seventeen thousand stores across ninety countries. Believe it or not, investments in information technology played an essential role in achieving that status. (Truth in advertising: Much of what this column discusses comes from a study written by my colleagues. See Groysberg et al.1 if you want a longer description.) For our purposes, the Domino’s experience has many useful and common features from which to generalize. It is obvious, for example, that Domino’s did not invent the Internet, the smartphone, broadband, nor the World Wide Web, Web2.0, or any of the other advances in information technology infrastructure. As inmany other firms, its efforts took advantage of the opportunity afforded by advanced Internet technologies. Those efforts involved considerable investment and co-invention, and the point of the story is to understand those co-inventions.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2022.3208464},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {142-144},
  shortjournal = {IEEE Micro},
  title        = {Distributed discretion by the slice},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022d). Analysis of historical patenting behavior and patent
characteristics of computer architecture companies—part v: references.
<em>MICRO</em>, <em>42</em>(6), 135–140. (<a
href="https://doi.org/10.1109/MM.2022.3209765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In prior parts of this series, I analyzed the 1) number of all issued patents and issued computer architecture patents; 2) the prosecution time and effective patent term; 3) the number of claims, breakdown of independent and dependent claims, and effect that excess claim fees had on the numbers of total and independent claims; and 4) the type of claims (apparatus, method, or Beauregard), and effect that the Supreme Court’s decision in Alice v. CLS Bank had on the number of independent and dependent method claims, for patents that were issued to 18 leading computer architecture companies and filed between 1996 and 2020. This article examines the 1) number of “backward” citations to previously issued/published U.S. patents and U.S. patent publications, foreign patents, and Other References and 2) the number of “forward” citations to a company’s patent by another U.S. patent or U.S. patent publication.},
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2022.3209765},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {135-140},
  shortjournal = {IEEE Micro},
  title        = {Analysis of historical patenting behavior and patent characteristics of computer architecture Companies—Part v: References},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RadioML meets FINN: Enabling future RF applications with
FPGA streaming architectures. <em>MICRO</em>, <em>42</em>(6), 125–133.
(<a href="https://doi.org/10.1109/MM.2022.3202091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are penetrating into a broad spectrum of applications and replacing manual algorithmic implementations, including the radio frequency communications domain with classical signal processing algorithms. However, the high throughput (gigasamples per second) and low latency requirements of this application domain pose a significant hurdle for adopting computationally demanding DNNs. In this article, we explore highly specialized DNN inference accelerator approaches on field-programmable gate arrays (FPGAs) for RadioML modulation classification. Using an automated end-to-end flow for the generation of the FPGA solution, we can easily explore a spectrum of solutions that optimize for different design targets, including accuracy, power efficiency, resources, throughput, and latency. By leveraging reduced precision arithmetic and customized streaming dataflow, we demonstrate a solution that meets the application requirements and outperforms alternative FPGA efforts by 3.5× in terms of throughput. Against modern embedded graphics processing units (GPUs), we measure $&gt;\mathsf{10}\times$&gt;10× higher throughput and $&gt;\mathsf{100}\times$&gt;100× lower latency under comparable accuracy and power envelopes.},
  archive      = {J_MICRO},
  author       = {Felix Jentzsch and Yaman Umuroglu and Alessandro Pappalardo and Michaela Blott and Marco Platzner},
  doi          = {10.1109/MM.2022.3202091},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {125-133},
  shortjournal = {IEEE Micro},
  title        = {RadioML meets FINN: Enabling future RF applications with FPGA streaming architectures},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Three-dimensional stacked neural network accelerator
architectures for AR/VR applications. <em>MICRO</em>, <em>42</em>(6),
116–124. (<a href="https://doi.org/10.1109/MM.2022.3202254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional integration offers architectural and performance benefits for scaling augmented/virtual reality (AR/VR) models on highly resource-constrained edge devices. Two-dimensional off-chip memory interfaces are too prohibitively energy intensive and bandwidth (BW) limited for AR/VR devices. To solve this, we propose using advanced 3-D stacking technology for high-density vertical integration to local memory and compute, increasing memory capacity within the same footprint at iso-BW with improvements in energy and latency. We evaluate 3-D architectures for a prototype AR/VR accelerator to demonstrate up to 3.9× latency reduction and 1.6× lower energy compared to a 2-D configuration within a smaller/similar footprint. Additionally, we show the feasibility of deploying higher resolution AR/VR models by stacking multiple tiers of memory, providing a pathway to break the footprint constraints of 2-D architectures. The use of high-density 3-D interconnects allows us to demonstrate localized benefits at the accelerator-level compared with standard system-on-chip memory disaggregation techniques/architectures.},
  archive      = {J_MICRO},
  author       = {Lita Yang and Robert M. Radway and Yu-Hsin Chen and Tony F. Wu and Huichu Liu and Elnaz Ansari and Vikas Chandra and Subhasish Mitra and Edith Beigné},
  doi          = {10.1109/MM.2022.3202254},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {116-124},
  shortjournal = {IEEE Micro},
  title        = {Three-dimensional stacked neural network accelerator architectures for AR/VR applications},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient language-guided reinforcement learning for
resource-constrained autonomous systems. <em>MICRO</em>, <em>42</em>(6),
107–114. (<a href="https://doi.org/10.1109/MM.2022.3199686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose an energy-efficient architecture, which is designed to receive both images and text inputs as a step toward designing reinforcement learning agents that can understand human language and act in real-world environments. We evaluate our proposed method on three different software environments and a low power drone named Crazyflie to navigate toward specified goals and avoid obstacles successfully. To find the most efficient language-guided reinforcement learning model, we implemented the model with various configurations of image input sizes and text instruction sizes on the Crazyflie drone GAP8, which consists of eight reduced instruction set computer-V cores. The task completion success rate and onboard power consumption, latency, and memory usage of GAP8 are measured and compared with Jetson TX2 ARM central processing unit and Raspberry Pi 4. The results show that by decreasing 20\% of input image size we achieve up to 78\% energy improvement while achieving an 82\% task completion success rate.},
  archive      = {J_MICRO},
  author       = {Aidin Shiri and Mozhgan Navardi and Tejaswini Manjunath and Nicholas R. Waytowich and Tinoosh Mohsenin},
  doi          = {10.1109/MM.2022.3199686},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {107-114},
  shortjournal = {IEEE Micro},
  title        = {Efficient language-guided reinforcement learning for resource-constrained autonomous systems},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Artificial-intelligence-enhanced ultrasound flow imaging at
the edge. <em>MICRO</em>, <em>42</em>(6), 96–106. (<a
href="https://doi.org/10.1109/MM.2022.3195516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasound flow imaging has long been used for cardiovascular diagnostics. Color Doppler imaging (CDI) is the predominant ultrasound flow imaging mode, but its diagnostic value is hampered by aliasing artifacts that limit the range of detectable blood velocities. Here, we present the first demonstration of how edge artificial intelligence (AI) can enable real-time CDI with aliasing resistance. Specifically, graphical processing unit acceleration and AI-ready edge computing hardware have been leveraged to realize the first end-to-end CDI processing pipeline that involves AI-based aliasing correction. Performance results show that, using our edge AI engine, aliasing-resistant CDI frames with threefold velocity detection range can be generated at a real-time frame rate of 25 frames per second (for raw datasets with 192 channels, 12-bit data resolution, and 25-MHz sampling rate). Overall, edge AI can critically improve the real-time visualization quality of ultrasound flow imaging and, in turn, potentially transform its bedside application value.},
  archive      = {J_MICRO},
  author       = {Hassan Nahas and Sean Huver and Billy Y. S. Yiu and Chris M. Kallweit and Adrian J. Y. Chee and Alfred C. H. Yu},
  doi          = {10.1109/MM.2022.3195516},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {96-106},
  shortjournal = {IEEE Micro},
  title        = {Artificial-intelligence-enhanced ultrasound flow imaging at the edge},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neuromorphic near-sensor computing: From event-based sensing
to edge learning. <em>MICRO</em>, <em>42</em>(6), 88–95. (<a
href="https://doi.org/10.1109/MM.2022.3195634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromorphic near-sensor computing has recently emerged as a low-power and low-memory paradigm for the design of artificial intelligence (AI)-enabled IoT devices working at the extreme edge. Compared to conventional sensing and learning techniques, neuromorphic sampling, and processing reduces data bandwidth requirements, induces large savings on power and area consumption, and enables online learning and adaptation. In this article, we discuss recent studies made in the design of event-based sampling and learning circuits. We show that our event-based sampling methods outperform conventional techniques in terms of power consumption. We also show that our spiking neural network (SNN), learning through spike-timing-dependent plasticity (STDP), outperforms the state-of-the-art SNN-STDP systems in terms of inference accuracy while being orders of magnitude more power efficient than conventional deep-learning systems. We hope that the opportunities discussed in this summary article will inspire future research.},
  archive      = {J_MICRO},
  author       = {Ali Safa and Jonah Van Assche and Mark Daniel Alea and Francky Catthoor and Georges G.E. Gielen},
  doi          = {10.1109/MM.2022.3195634},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {88-95},
  shortjournal = {IEEE Micro},
  title        = {Neuromorphic near-sensor computing: From event-based sensing to edge learning},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ML-HW co-design of noise-robust TinyML models and always-on
analog compute-in-memory edge accelerator. <em>MICRO</em>,
<em>42</em>(6), 76–87. (<a
href="https://doi.org/10.1109/MM.2022.3198321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Always-on TinyML perception tasks in Internet of Things applications require very high energy efficiency. Analog compute-in-memory (CiM) using nonvolatile memory (NVM) promises high energy efficiency and self-contained on-chip model storage. However, analog CiM introduces new practical challenges, including conductance drift, read/write noise, fixed analog-to-digital (ADC) converter gain, etc. These must be addressed to achieve models that can be deployed on analog CiM with acceptable accuracy loss. This article describes AnalogNets: TinyML models for the popular always-on tasks of keyword spotting (KWS) and visual wake word (VWW). The model architectures are specifically designed for analog CiM, and we detail a comprehensive training methodology, to retain accuracy in the face of analog nonidealities, and low-precision data converters at inference time. We also describe AON-CiM, a programmable, minimal-area phase-change memory (PCM) analog CiM accelerator, with a layer-serial approach to remove the cost of complex interconnects associated with a fully pipelined design. We evaluate the AnalogNets on a calibrated simulator, as well as real hardware, and find that accuracy degradation is limited to 0.8\%/1.2\% after 24 h of PCM drift (8 bits) for KWS/VWW. AnalogNets running on the 14-nm AON-CiM accelerator demonstrate 8.55/26.55/56.67 and 4.34/12.64/25.2 TOPS/W for KWS and VWWs with 8-/6-/4-bit activations, respectively.},
  archive      = {J_MICRO},
  author       = {Chuteng Zhou and Fernando García Redondo and Julian Büchel and Irem Boybat and Xavier Timoneda Comas and S. R. Nandakumar and Shidhartha Das and Abu Sebastian and Manuel Le Gallo and Paul N. Whatmough},
  doi          = {10.1109/MM.2022.3198321},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {76-87},
  shortjournal = {IEEE Micro},
  title        = {ML-HW co-design of noise-robust TinyML models and always-on analog compute-in-memory edge accelerator},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AIDA: Associative in-memory deep learning accelerator.
<em>MICRO</em>, <em>42</em>(6), 67–75. (<a
href="https://doi.org/10.1109/MM.2022.3190924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents an associative in-memory deep learning processor (AIDA) for edge devices. An associative processor is a massively parallel non-von Neumann accelerator that uses memory cells for computing; the bulk of data is never transferred outside the memory arrays for external processing. AIDA utilizes a dynamic content addressable memory for both data storage and processing, and benefits from sparsity and limited arithmetic precision, typical in modern deep neural networks. The novel in-data processing implementation designed for the AIDA accelerator achieves a speedup of 270× over an advanced central processing unit at more than three orders-of-magnitude better energy efficiency.},
  archive      = {J_MICRO},
  author       = {Esteban Garzón and Adam Teman and Marco Lanuzza and Leonid Yavits},
  doi          = {10.1109/MM.2022.3190924},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {67-75},
  shortjournal = {IEEE Micro},
  title        = {AIDA: Associative in-memory deep learning accelerator},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fused architecture for dense and sparse matrix processing in
TensorFlow lite. <em>MICRO</em>, <em>42</em>(6), 55–66. (<a
href="https://doi.org/10.1109/MM.2022.3196705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a hardware architecture optimized for sparse and dense matrix processing in TensorFlow Lite and compatible with embedded-heterogeneous devices that integrate central processing unit and field-programmable gate array (FPGA) resources. The fused architecture for dense and sparse matrices design offers multiple configuration options that tradeoff parallelism and complexity, and uses a dataflow model to create four stages that read, compute, scale, and write results. All stages are designed to support TensorFlow Lite operations including asymmetric quantized activations, column-major matrix write, per-filter/per-axis bias values, and current scaling specifications. The configurable accelerator is integrated with the TensorFlow Lite inference engine running on the ARMv8 processor. We compare performance/power/energy with the state-of-the-art RUY software multiplication library showing up to 18× acceleration and 48× in dense and sparse modes, respectively. The sparse mode benefits from structural pruning to fully utilize the digital signal processing blocks present in the FPGA device.},
  archive      = {J_MICRO},
  author       = {Jose Nunez-Yanez},
  doi          = {10.1109/MM.2022.3196705},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {55-66},
  shortjournal = {IEEE Micro},
  title        = {Fused architecture for dense and sparse matrix processing in TensorFlow lite},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hardware/software co-design vision for deep learning at
the edge. <em>MICRO</em>, <em>42</em>(6), 48–54. (<a
href="https://doi.org/10.1109/MM.2022.3195617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing popularity of edgeAI requires novel solutions to support the deployment of compute-intense algorithms in embedded devices. In this article, we advocate for a holistic approach, where application-level transformations are jointly conceived with dedicated hardware platforms. We embody such a stance in a strategy that employs ensemble-based algorithmic transformations to increase robustness and accuracy in convolutional neural networks, enabling the aggressive quantization of weights and activations. Opportunities offered by algorithmic optimizations are then harnessed in domain-specific hardware solutions, such as the use of multiple ultra-low-power processing cores, the provision of shared acceleration resources, the presence of independently power-managed memory banks, and voltage scaling to ultra-low levels, greatly reducing (up to 60\% in our experiments) energy requirements. Furthermore, we show that aggressive quantization schemes can be leveraged to perform efficient computations directly in memory banks, adopting in-memory computing solutions. We showcase that the combination of parallel in-memory execution and aggressive quantization leads to more than 70\% energy and latency gains compared to baseline implementations.},
  archive      = {J_MICRO},
  author       = {Flavio Ponzina and Simone Machetti and Marco Rios and Benoît Walter Denkinger and Alexandre Levisse and Giovanni Ansaloni and Miguel Peón-Quirós and David Atienza},
  doi          = {10.1109/MM.2022.3195617},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {48-54},
  shortjournal = {IEEE Micro},
  title        = {A Hardware/Software co-design vision for deep learning at the edge},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerating silent witness storage. <em>MICRO</em>,
<em>42</em>(6), 39–47. (<a
href="https://doi.org/10.1109/MM.2022.3193048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose hardware acceleration for a new edge computing abstraction called a Silent Witness. This abstraction embodies a severe asymmetry in the ease of write versus read operations. Surveillance data from one or more video cameras are continuously encrypted and recorded, but the decrypting, processing, or transmission of that data only occurs under stringent privacy controls. For the new search workloads of such a system, decode-enabled storage alleviates the scalability bottleneck imposed by frequent decoding of data. Our experiments show throughput improvements up to 3.5× for typical search workloads of a Silent Witness.},
  archive      = {J_MICRO},
  author       = {Mahadev Satyanarayanan and Ziqiang Feng and Shilpa George and Jan Harkes and Roger Iyengar and Haithem Turki and Padmanabhan Pillai},
  doi          = {10.1109/MM.2022.3193048},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {39-47},
  shortjournal = {IEEE Micro},
  title        = {Accelerating silent witness storage},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerating neural network inference with
processing-in-DRAM: From the edge to the cloud. <em>MICRO</em>,
<em>42</em>(6), 25–38. (<a
href="https://doi.org/10.1109/MM.2022.3202350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks (NNs) are growing in importance and complexity. An NN’s performance (and energy efficiency) can be bound either by computation or memory resources. The processing-in-memory (PIM) paradigm, where computation is placed near or within memory arrays, is a viable solution to accelerate memory-bound NNs. However, PIM architectures vary in form, where different PIM approaches lead to different tradeoffs. Our goal is to analyze, discuss, and contrast dynamic random-access memory (DRAM)-based PIM architectures for NN performance and energy efficiency. To do so, we analyze three state-of-the-art PIM architectures: 1) UPMEM, which integrates processors and DRAM arrays into a single 2-D chip, 2) Mensa, a 3-D-stacking-based PIM architecture tailored for edge devices, and 3) SIMDRAM, which uses the analog principles of DRAM to execute bit-serial operations. Our analysis reveals that PIM greatly benefits memory-bound NNs: 1) UPMEM provides 23× the performance of a high-end graphics processing unit (GPU) when the GPU requires memory oversubscription for a general matrix–vector multiplication kernel, 2) Mensa improves energy efficiency and throughput by 3.0× and 3.1× over the baseline Edge tensor processing unit for 24 Google edge NN models, and 3) SIMDRAM outperforms a central processing unit/graphics processing unit by 16.7×/1.4× for three binary NNs. We conclude that the ideal PIM architecture for NN models depends on a model&#39;s distinct attributes, due to the inherent architectural design choices.},
  archive      = {J_MICRO},
  author       = {Geraldo F. Oliveira and Juan Gómez-Luna and Saugata Ghose and Amirali Boroumand and Onur Mutlu},
  doi          = {10.1109/MM.2022.3202350},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {25-38},
  shortjournal = {IEEE Micro},
  title        = {Accelerating neural network inference with processing-in-DRAM: From the edge to the cloud},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Increasing throughput of in-memory DNN accelerators by
flexible layerwise DNN approximation. <em>MICRO</em>, <em>42</em>(6),
17–24. (<a href="https://doi.org/10.1109/MM.2022.3196865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate computing and mixed-signal in-memory accelerators are promising paradigms to significantly reduce computational requirements of deep neural network (DNN) inference without accuracy loss. In this work, we present a novel in-memory design for layerwise approximate computation at different approximation levels. A sensitivity-based high-dimensional search is performed to explore the optimal approximation level for each DNN layer. Our new methodology offers high flexibility and optimal tradeoff between accuracy and throughput, which we demonstrate by an extensive evaluation on various DNN benchmarks for medium- and large-scale image classification with CIFAR10, CIFAR100, and ImageNet. With our novel approach, we reach an average of 5× and up to 8× speedup without accuracy loss.},
  archive      = {J_MICRO},
  author       = {Cecilia De la Parra and Taha Soliman and Andre Guntoro and Akash Kumar and Norbert Wehn},
  doi          = {10.1109/MM.2022.3196865},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {17-24},
  shortjournal = {IEEE Micro},
  title        = {Increasing throughput of in-memory DNN accelerators by flexible layerwise DNN approximation},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). POD-RACING: Bulk-bitwise to floating-point compute in
racetrack memory for machine learning at the edge. <em>MICRO</em>,
<em>42</em>(6), 9–16. (<a
href="https://doi.org/10.1109/MM.2022.3195761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have become a ubiquitous algorithm with growing applications in mobile and edge settings. We describe a compute-in-memory (CIM) technique called POD-RACING using Racetrack memory (RM) to accelerate CNNs for edge systems. Using transverse read, a technique that can determine the number of “1”s in multiple adjacent domains, POD-RACING can efficiently implement multioperand bulk-bitwise and addition computations, and two-operand multiplication. We discuss how POD-RACING can implement both variable precision integer and floating point arithmetic using digital CIM. This allows both CNN inference and on-device training without expensive data movement to the cloud. Based on these functions we demonstrate the implementation of several CNNs with backpropagation using RM CIM and compare these to the state-of-the-art implementations of CNN inference and training. During training, POD-RACING improves efficiency by 2×, energy consumption by $\geq$≥27\%, and increases throughput by $\geq$≥18\% versus a state-of-the-art field-programmable gate array accelerator.},
  archive      = {J_MICRO},
  author       = {Sébastien Ollivier and Xinyi Zhang and Yue Tang and Chayanika Choudhuri and Jingtong Hu and Alex K. Jones},
  doi          = {10.1109/MM.2022.3195761},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {9-16},
  shortjournal = {IEEE Micro},
  title        = {POD-RACING: Bulk-bitwise to floating-point compute in racetrack memory for machine learning at the edge},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on artificial intelligence at the edge.
<em>MICRO</em>, <em>42</em>(6), 6–8. (<a
href="https://doi.org/10.1109/MM.2022.3203489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special section explore cutting edge research on topics that combine artificial intelligence with edge computing, relating to the design, performance, or application of microprocessors and microcomputers.},
  archive      = {J_MICRO},
  author       = {Gabriel Falcao and Joseph R. Cavallaro},
  doi          = {10.1109/MM.2022.3203489},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {6-8},
  shortjournal = {IEEE Micro},
  title        = {Special issue on artificial intelligence at the edge},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Artificial intelligence at the edge: Designs and
architectures for pervasive intelligence. <em>MICRO</em>,
<em>42</em>(6), 4–5. (<a
href="https://doi.org/10.1109/MM.2022.3210497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_MICRO},
  author       = {Lizy Kurian John},
  doi          = {10.1109/MM.2022.3210497},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Artificial intelligence at the edge: Designs and architectures for pervasive intelligence},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Archetypes of risky decisions. <em>MICRO</em>,
<em>42</em>(5), 130–132. (<a
href="https://doi.org/10.1109/MM.2022.3196792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many risky decisions have long time horizons. Consider investing in a business venture that will not pay off for at least half a decade—major construction projects, a new line of product introductions, software upgrades for widely used products, and the list goes on. All of these risky decisions lend themselves to a combination of spread sheets and human assessment. Much of the investing by angel investors and venture capitalist (VCs) fall into this category, and so does plenty of the business development by established technology firms. How do commercial actors make decisions in such risky settings? A new branch of behavioral economics proposes that decision makers use archetypes. An archetype is a recurrent symbol or motif about a type of person or team and associated behavior. An archetype reappears as a (similar) story in a wide set of media, and contains many of the same narrative elements.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2022.3196792},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {130-132},
  shortjournal = {IEEE Micro},
  title        = {Archetypes of risky decisions},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Analysis of historical patenting behavior and patent
characteristics of computer architecture companies—part IV: claims.
<em>MICRO</em>, <em>42</em>(5), 119–127. (<a
href="https://doi.org/10.1109/MM.2022.3195076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzes the historical patenting behavior and patent characteristics of computer architecture companies.},
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2022.3195076},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {119-127},
  shortjournal = {IEEE Micro},
  title        = {Analysis of historical patenting behavior and patent characteristics of computer architecture Companies—Part IV: Claims},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HPVM: Hardware-agnostic programming for heterogeneous
parallel systems. <em>MICRO</em>, <em>42</em>(5), 108–117. (<a
href="https://doi.org/10.1109/MM.2022.3186547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Heterogeneous Parallel Virtual Machine (HPVM), a compiler framework for hardware-agnostic programming on heterogeneous compute platforms. HPVM introduces a hardware-agnostic parallel intermediate representation with constructs for the hierarchical task, data, and pipeline parallelism, including dataflow parallelism, and supports multiple front-end languages. In addition, HPVM provides optimization passes that navigate performance, energy, and accuracy tradeoffs, and includes retargetable back ends for a wide range of diverse hardware targets, including central processing units, graphics processing units, domain-specific accelerators, and field-programmable gate arrays. Across diverse hardware platforms, HPVM optimizations provide significant performance and energy improvements, while preserving object-code portability. With these capabilities, HPVM facilitates developers, domain experts, and hardware vendors in programming modern heterogeneous systems.},
  archive      = {J_MICRO},
  author       = {Adel Ejjeh and Aaron Councilman and Akash Kothari and Maria Kotsifakou and Leon Medvinsky and Abdul Rafae Noor and Hashim Sharif and Yifan Zhao and Sarita Adve and Sasa Misailovic and Vikram Adve},
  doi          = {10.1109/MM.2022.3186547},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {108-117},
  shortjournal = {IEEE Micro},
  title        = {HPVM: Hardware-agnostic programming for heterogeneous parallel systems},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SpecHLS: Speculative accelerator design using high-level
synthesis. <em>MICRO</em>, <em>42</em>(5), 99–107. (<a
href="https://doi.org/10.1109/MM.2022.3188136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Custom hardware accelerators usage is shifting toward new application domains such as graph analytics and unstructured text analysis. These applications expose complex control-flow which is challenging to map to hardware, especially when operating from a C/C++ description using high-level synthesis toolchains. Several approaches relying on speculative execution have been proposed to overcome those limitations, but they often fail to handle the multiple interacting speculations required for realistic use-cases. This article proposes a fully automated hardware synthesis flow based on a source-to-source compiler that identifies and explores intricate speculation configurations to generate speculative hardware accelerators.},
  archive      = {J_MICRO},
  author       = {Jean-Michel Gorius and Simon Rokicki and Steven Derrien},
  doi          = {10.1109/MM.2022.3188136},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {99-107},
  shortjournal = {IEEE Micro},
  title        = {SpecHLS: Speculative accelerator design using high-level synthesis},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Yin-yang: Programming abstractions for cross-domain
multi-acceleration. <em>MICRO</em>, <em>42</em>(5), 89–98. (<a
href="https://doi.org/10.1109/MM.2022.3189416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Field-programmable gate array (FPGA) accelerators offer performance and efficiency gains by narrowing the scope of acceleration to one algorithmic domain. However, real-life applications are often not limited to a single domain, which naturally makes Cross-Domain Multi-Acceleration a crucial next step. The challenge is, existing FPGA accelerators are built upon their specific vertically specialized stacks, which prevents utilizing multiple accelerators from different domains. To that end, we propose a pair of dual abstractions, called Yin-Yang, which work in tandem and enable programmers to develop cross-domain applications using multiple accelerators on a FPGA. The Yin abstraction enables cross-domain algorithmic specification, while the Yang abstraction captures the accelerator capabilities. We also developed a dataflow virtual machine, dubbed Accelerator-Level Virtual Machine (XLVM), which transparently maps domain functions (Yin) to best-fit accelerator capabilities (Yang). With six real-world cross-domain applications, our evaluations show that Yin-Yang unlocks 29.4× speedup, while the best single-domain acceleration achieves 12.0×.},
  archive      = {J_MICRO},
  author       = {Joon Kyung Kim and Byung Hoon Ahn and Sean Kinzer and Soroush Ghodrati and Rohan Mahapatra and Brahmendra Yatham and Shu-Ting Wang and Dohee Kim and Parisa Sarikhani and Babak Mahmoudi and Divya Mahajan and Jongse Park and Hadi Esmaeilzadeh},
  doi          = {10.1109/MM.2022.3189416},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {89-98},
  shortjournal = {IEEE Micro},
  title        = {Yin-yang: Programming abstractions for cross-domain multi-acceleration},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bridging python to silicon: The SODA toolchain.
<em>MICRO</em>, <em>42</em>(5), 78–88. (<a
href="https://doi.org/10.1109/MM.2022.3178580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Systems performing scientific computing, data analysis, and machine learning tasks have a growing demand for application-specific accelerators that can provide high computational performance while meeting strict size and power requirements. However, the algorithms and applications that need to be accelerated are evolving at a rate that is incompatible with manual design processes based on hardware description languages. Agile hardware design tools based on compiler techniques can help by quickly producing an application-specific integrated circuit (ASIC) accelerator starting from a high-level algorithmic description. We present the software-defined accelerator (SODA) synthesizer, a modular and open-source hardware compiler that provides automated end-to-end synthesis from high-level software frameworks to ASIC implementation, relying on multilevel representations to progressively lower and optimize the input code. Our approach does not require the application developer to write any register-transfer level code, and it is able to reach up to 364 giga floating point operations per second (GFLOPS)/W efficiency (32-bit precision) on typical convolutional neural network operators.},
  archive      = {J_MICRO},
  author       = {Nicolas Bohm Agostini and Serena Curzel and Jeff Jun Zhang and Ankur Limaye and Cheng Tan and Vinay Amatya and Marco Minutoli and Vito Giovanni Castellana and Joseph Manzano and David Brooks and Gu-Yeon Wei and Antonino Tumeo},
  doi          = {10.1109/MM.2022.3178580},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {78-88},
  shortjournal = {IEEE Micro},
  title        = {Bridging python to silicon: The SODA toolchain},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synthesizing legacy string code for FPGAs using bounded
automata learning. <em>MICRO</em>, <em>42</em>(5), 70–77. (<a
href="https://doi.org/10.1109/MM.2022.3178037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of hardware accelerators, such as field-programmable gate arrays, into general-purpose computation pipelines continues to expand, but programming models for these devices lag far behind their central processing unit (CPU) counterparts. While high-level synthesis (HLS) can help port some legacy software, many programs perform poorly without manual, architecture-specific optimization. We propose an end-to-end approach combining dynamic and static analyses to learn a model of functional behavior for off-the-shelf legacy code and synthesize a hardware description from this model. Our prototype implementation can correctly learn functionality for string kernels that recognize regular languages and provides a near approximation otherwise. We evaluate our prototype tool on a benchmark suite of real world, legacy string functions mined from GitHub and successfully synthesize—without modification or annotation—over 80\% (72\% exactly and a further 11\% approximately). Traditional HLS, only after extensive modification and custom testbench generation, can synthesize the same number of benchmarks, but with results that have higher hardware requirements and lower maximum clock rates.},
  archive      = {J_MICRO},
  author       = {Kevin Angstadt and Tommy Tracy and Kevin Skadron and Jean-Baptiste Jeannin and Westley Weimer},
  doi          = {10.1109/MM.2022.3178037},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {70-77},
  shortjournal = {IEEE Micro},
  title        = {Synthesizing legacy string code for FPGAs using bounded automata learning},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unifying spatial accelerator compilation with idiomatic and
modular transformations. <em>MICRO</em>, <em>42</em>(5), 59–69. (<a
href="https://doi.org/10.1109/MM.2022.3189976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial accelerators provide high performance, energy efficiency, and flexibility. Recent design frameworks enable these architectures to be quickly designed and customized to a domain. However, constructing a compiler for this immense design space is challenging, first because accelerators express programs with high-level idioms that are difficult to recognize. Second, it is unpredictable whether certain transformations are beneficial or will lead to infeasible hardware mappings. Our work develops a general spatial-accelerator compiler with two key ideas. First, we propose an approach to recognize and represent useful dataflow idioms, along with a novel idiomatic memory representation. Second, we propose the principle of modular compilation, which combines hardware-aware transformation selection and an iterative approach to handle uncertainty. Our compiler achieves 2.3× speedup, and 98.7× area-normalized speedup over high-end server central processing unit (CPU).},
  archive      = {J_MICRO},
  author       = {Jian Weng and Sihao Liu and Dylan Kupsh and Tony Nowatzki},
  doi          = {10.1109/MM.2022.3189976},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {59-69},
  shortjournal = {IEEE Micro},
  title        = {Unifying spatial accelerator compilation with idiomatic and modular transformations},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Compiling for vector extensions with stream-based
specialization. <em>MICRO</em>, <em>42</em>(5), 49–58. (<a
href="https://doi.org/10.1109/MM.2022.3173405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To overcome the current performance wall, data streaming and data-flow computing paradigms have been gradually making their way into the general-purpose domain. However, the proliferation of such paradigms is often hindered by the lack of compilation support, as their execution model is usually incompatible with the internal static single-assignment form used in modern compilers. Accordingly, we propose a new compilation flow that leverages the LLVM infrastructure to automatically extract and encode the memory access pattern and computation data-flow graph with streaming representations. The proposed compilation flow is used to generate code for the recently presented Unlimited Vector Extension, which tackles the shortcomings of vector-length agnostic single-instruction multiple-data extensions by deploying a data streaming paradigm with implicit memory access and loop control. We show that our proposed tool is capable of detecting, representing, and vectorizing a much wider range of loop patterns than existing solutions while providing significant performance gains.},
  archive      = {J_MICRO},
  author       = {Nuno Neves and Joao Mario Domingos and Nuno Roma and Pedro Tomás and Gabriel Falcao},
  doi          = {10.1109/MM.2022.3173405},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {49-58},
  shortjournal = {IEEE Micro},
  title        = {Compiling for vector extensions with stream-based specialization},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performance left on the table: An evaluation of compiler
autovectorization for RISC-v. <em>MICRO</em>, <em>42</em>(5), 41–48. (<a
href="https://doi.org/10.1109/MM.2022.3184867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next-generation length-agnostic vector instruction set architecture (ISA) designs, the RISC-V vector extension, and ARM’s scalable vector extension enable software portability across hardware implementations with different vector engines. While traditional, fixed-length single-instruction–multiple-data ISA instructions, such as Intel AVX and ARM Neon, enjoy mature compiler support for automatic vectorization, compiler support is still emerging for these length-agnostic ISAs. This work studies the compiler shortcomings that constitute the gap in autovectorization capabilities between length-agnostic and fixed-length architectures. We examine LLVM’s support for both the RISC-V vector extension and traditional vector ISAs. We study a set of synthetic scalar loops to compare the breadth of support in the two settings, and we examine a real benchmark suite to compare autovectorized to hand-vectorized RISC-V code. We use both studies to distill a set of recommendations for engineering improvements and future research in compilers and programming models for length-agnostic vector programming.},
  archive      = {J_MICRO},
  author       = {Neil Adit and Adrian Sampson},
  doi          = {10.1109/MM.2022.3184867},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {41-48},
  shortjournal = {IEEE Micro},
  title        = {Performance left on the table: An evaluation of compiler autovectorization for RISC-V},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Compiling for the IBM matrix engine for enterprise
workloads. <em>MICRO</em>, <em>42</em>(5), 34–40. (<a
href="https://doi.org/10.1109/MM.2022.3176529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The matrix-multiply assist (MMA) facility is the latest addition to IBM’s power instruction set architecture and first shipped in the recently introduced POWER10 processor. MMA is designed to accelerate matrix–matrix operations, such as matrix multiplication and convolution, using instructions that compute the outer product of vector-register operands. Outer product computations have been used for decades in linear algebra libraries to deliver high-performance implementations of matrix operations. Such libraries use conventional single-instruction–multiple-data (SIMD) instructions to emulate outer product operations. MMA in POWER10 is the first hardware with direct support for outer product operations released in the market. MMA operates with the widest diversity of data types compared to any accelerator design currently announced. Unleashing the high-performance enabled by MMA requires careful code generation. Two key considerations for optimal MMA code performance are 1) the choice of accumulation layout when maximizing the using the accumulators and 2) the selection of matrix access order. This article shows that over 92\% of peak performance in POWER10 with MMA can be achieved when the code generation makes the right choices.},
  archive      = {J_MICRO},
  author       = {João P. L. de Carvalho and José E. Moreira and José Nelson Amaral},
  doi          = {10.1109/MM.2022.3176529},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {34-40},
  shortjournal = {IEEE Micro},
  title        = {Compiling for the IBM matrix engine for enterprise workloads},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Retargetable optimizing compilers for quantum accelerators
via a multilevel intermediate representation. <em>MICRO</em>,
<em>42</em>(5), 17–33. (<a
href="https://doi.org/10.1109/MM.2022.3179654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a multilevel quantum–classical intermediate representation (IR) that enables an optimizing, retargetable compiler for available quantum languages. Our work builds upon the multilevel intermediate representation (MLIR) framework and leverages its unique progressive lowering capabilities to map quantum languages to the low-level virtual machine (LLVM) machine-level IR. We provide both quantum and classical optimizations via the MLIR pattern rewriting subsystem and standard LLVM optimization passes, and demonstrate the programmability, compilation, and execution of our approach via standard benchmarks and test cases. In comparison to other standalone language and compiler efforts available today, our work results in compile times that are 1,000× faster than standard Pythonic approaches, and 5–10× faster than comparative standalone quantum language compilers. Our compiler provides quantum resource optimizations via standard programming patterns that result in a 10× reduction in entangling operations, a common source of program noise. We see this work as a vehicle for rapid quantum compiler prototyping.},
  archive      = {J_MICRO},
  author       = {Thien Nguyen and Alexander McCaskey},
  doi          = {10.1109/MM.2022.3179654},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {17-33},
  shortjournal = {IEEE Micro},
  title        = {Retargetable optimizing compilers for quantum accelerators via a multilevel intermediate representation},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TinyIREE: An ML execution environment for embedded systems
from compilation to deployment. <em>MICRO</em>, <em>42</em>(5), 9–16.
(<a href="https://doi.org/10.1109/MM.2022.3178068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning model deployment for training and execution has been an important topic for industry and academic research in the last decade. Much of the attention has been focused on developing specific toolchains to support acceleration hardware. In this article, we present Intermediate Representation Execution Environment (IREE), a unified compiler and runtime stack with the explicit goal to scale down machine learning programs to the smallest footprints for mobile and edge devices, while maintaining the ability to scale up to larger deployment targets. IREE adopts a compiler-based approach and optimizes for heterogeneous hardware accelerators through the use of the Multi-Level IR (MLIR) compiler infrastructure, which provides the means to quickly design and implement multilevel compiler intermediate representations (IR). More specifically, this article is focused on TinyIREE, which is a set of deployment options in IREE that accommodate the limited memory and computation resources in embedded systems and bare-metal platforms, while also demonstrating IREE’s intuitive workflow that generates workloads for different ISA extensions and application binary interface (ABIs) through LLVM.},
  archive      = {J_MICRO},
  author       = {Hsin-I Cindy Liu and Marius Brehler and Mahesh Ravishankar and Nicolas Vasilache and Ben Vanik and Stella Laurenzo},
  doi          = {10.1109/MM.2022.3178068},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {9-16},
  shortjournal = {IEEE Micro},
  title        = {TinyIREE: An ML execution environment for embedded systems from compilation to deployment},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on compiling for accelerators. <em>MICRO</em>,
<em>42</em>(5), 6–8. (<a
href="https://doi.org/10.1109/MM.2022.3193765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The articles in this special section focus on compiling for accelerators. These articles are divided into two groups according to how the accelerator is coupled with the microprocessor. The first set of articles describes novel compilation techniques targeting accelerators that are CPU ISA extensions, such as vectorization and matrix multiplication. In the second half, advanced compilation techniques that use dedicated and FPGA-based accelerators are discussed.},
  archive      = {J_MICRO},
  author       = {Guido Araujo and Lucas Wanner},
  doi          = {10.1109/MM.2022.3193765},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {6-8},
  shortjournal = {IEEE Micro},
  title        = {Special issue on compiling for accelerators},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Automatic compilation will be key for success of the
accelerator revolution! <em>MICRO</em>, <em>42</em>(5), 4–5. (<a
href="https://doi.org/10.1109/MM.2022.3196713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware architects have been designing and introducing interesting and feature-rich architectures to handle emerging workloads. When a new architecture arrives, compilation is usually primitive in the initial few years before automatic code generation and optimizations get mature. This has been true with most architectures in the past, including central processing units (CPUs) and SIMD-style CPU extensions; emerging hardware accelerators are no exception.},
  archive      = {J_MICRO},
  author       = {Lizy Kurian John},
  doi          = {10.1109/MM.2022.3196713},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Automatic compilation will be key for success of the accelerator revolution!},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022e). Inflation and technology markets. <em>MICRO</em>,
<em>42</em>(4), 134–136. (<a
href="https://doi.org/10.1109/MM.2022.3181645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The U.S. consumer price index (CPI) rose just over 8\% between May 2021 and April 2022. That grabbed headlines as the highest rate of inflation in four decades. A vast apparatus of government employees collects and collates prices for the CPI. The prices come from 75 U.S. cities, 6,000 urban households, and 22,000 retail establishments. Economic analysts already know announcing one number is misleading, but, frankly, that has not mattered while inflation was 2\%–3\% a year. Today’s column explains the precise sense in which the CPI is misleading, though with two angles in mind. First, what parts of the CPI are relevant to anybody who works at a technology firm, and which parts are irrelevant? Second, what does this trend mean for technology firms in the near term?},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2022.3181645},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {134-136},
  shortjournal = {IEEE Micro},
  title        = {Inflation and technology markets},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Analysis of historical patenting behavior and patent
characteristics of computer architecture companies—part III: claims.
<em>MICRO</em>, <em>42</em>(4), 124–132. (<a
href="https://doi.org/10.1109/MM.2022.3180134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Part I of this series analyzed the number and type of patents that were issued to 18 leading computer architecture companies for patents that were filed between 1996 and 2020. Part II analyzed the prosecution time and effective patent term length for those patents. This article builds on that work by analyzing the number and type of claims. Due to the large amount of claims-related data, Parts III and IV will focus on the claims. Part III is presented here and Part IV will be published in the September-October 2022 issue of IEEE Micro.},
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2022.3180134},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {124-132},
  shortjournal = {IEEE Micro},
  title        = {Analysis of historical patenting behavior and patent characteristics of computer architecture Companies—Part III: Claims},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vector runahead for indirect memory accesses.
<em>MICRO</em>, <em>42</em>(4), 116–123. (<a
href="https://doi.org/10.1109/MM.2022.3163132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vector runahead delivers extremely high memory-level parallelism even for the chains of dependent memory accesses with complex intermediate address computation, which conventional runahead techniques fundamentally cannot handle and, therefore, have ignored. It does this by rearchitecting runahead to use speculative data-level parallelism, rather than work skipping, as its primary form of extracting more memory-level parallelism in runahead mode than a true execution can, which we hope will bring about an entirely new dimension for high-performance processors.},
  archive      = {J_MICRO},
  author       = {Ajeya Naithani and Sam Ainsworth and Timothy M. Jones and Lieven Eeckhout},
  doi          = {10.1109/MM.2022.3163132},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {116-123},
  shortjournal = {IEEE Micro},
  title        = {Vector runahead for indirect memory accesses},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed data persistency. <em>MICRO</em>,
<em>42</em>(4), 107–115. (<a
href="https://doi.org/10.1109/MM.2022.3162183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed applications such as key-value stores and databases provide fault tolerance by replicating records in the memories of different nodes, and using data consistency protocols to ensure consistency across replicas. In this environment, nonvolatile memory (NVM) offers the ability to attain high-performance data durability. However, it is unclear how to tie NVM memory persistency models to the existing data consistency frameworks. In this article, we propose the concept of distributed data persistency (DDP) model, which is the binding of the memory persistency model with the data consistency model in a distributed system. We reason about the interaction between consistency and persistency by using the concepts of visibility point and durability point. We design low-latency distributed protocols for several DDP models, and investigate the tradeoffs between performance, durability, and intuition provided to the programmer.},
  archive      = {J_MICRO},
  author       = {Apostolos Kokolis and Antonis Psistakis and Benjamin Reidys and Jian Huang and Josep Torrellas},
  doi          = {10.1109/MM.2022.3162183},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {107-115},
  shortjournal = {IEEE Micro},
  title        = {Distributed data persistency},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ILLIXR: An open testbed to enable extended reality systems
research. <em>MICRO</em>, <em>42</em>(4), 97–106. (<a
href="https://doi.org/10.1109/MM.2022.3161018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Illinois Extended Reality testbed (ILLIXR), the first fully open-source XR system and research testbed. ILLIXR enables system innovations with end-to-end co-designed hardware, compiler, OS, and algorithms, and driven by end-user perceived Quality-of-Experience (QoE) metrics. Using ILLIXR, we provide the first comprehensive quantitative analysis of performance, power, and QoE for a complete XR system and its individual components. We describe several implications of our results that propel new directions in architecture, systems, and algorithms research for domain-specific systems in general, and XR in particular.},
  archive      = {J_MICRO},
  author       = {Muhammad Huzaifa and Rishi Desai and Samuel Grayson and Xutao Jiang and Ying Jing and Jae Lee and Fang Lu and Yihan Pang and Joseph Ravichandran and Finn Sinclair and Boyuan Tian and Hengzhi Yuan and Jeffrey Zhang and Sarita V. Adve},
  doi          = {10.1109/MM.2022.3161018},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {97-106},
  shortjournal = {IEEE Micro},
  title        = {ILLIXR: An open testbed to enable extended reality systems research},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Systematically understanding graph accelerator dimensions
and the value of hardware flexibility. <em>MICRO</em>, <em>42</em>(4),
87–96. (<a href="https://doi.org/10.1109/MM.2022.3160862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of the importance of graph workloads and the limitations of central processing units/graphics processing units (CPUs/GPUs), many graph-processing accelerators have been proposed. Most prior such accelerators adopt a single fixed algorithm. While helpful for specialization, this leaves performance potential from flexibility on the table and also complicates understanding the relationship between graph types, workloads, algorithms, and specialization. In this work, we explore the value of flexibility in graph-processing accelerators. Our approach is to identify a taxonomy of key algorithm variants, and develop a modular architecture, PolyGraph, which is flexible across them. The key to flexibility is our novel Taskflow execution model, which unifies task and dataflow parallelism. Overall, we find that flexibility is essential; PolyGraph outperforms similarly provisioned GPUs by mean 49.6× (up to 275×), and the best prior accelerator by mean 5.7×.},
  archive      = {J_MICRO},
  author       = {Vidushi Dadu and Sihao Liu and Tony Nowatzki},
  doi          = {10.1109/MM.2022.3160862},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {87-96},
  shortjournal = {IEEE Micro},
  title        = {Systematically understanding graph accelerator dimensions and the value of hardware flexibility},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The laplace microarchitecture for tracking data uncertainty.
<em>MICRO</em>, <em>42</em>(4), 78–86. (<a
href="https://doi.org/10.1109/MM.2022.3166067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents Laplace, a microarchitecture for tracking machine representations of probability distributions paired with architectural state. Laplace uses in-processor distribution representations, which are approximations of probability distributions just as floating-point number representations are approximations of real-valued numbers. The article presents two sets of instruction set architecture (ISA) extensions to 1) provide a mechanism to initialize distributional information in the microarchitecture; and 2) to allow applications to query statistics of the distributional information without exposing the uncertainty representations above the ISA. Unlike existing methods for uncertainty tracking, which require software to be rewritten in a domain-specific language or extensive source-level changes, Laplace achieves all of these benefits while requiring no changes to existing binaries to track uncertainty through them. Compared to repeated Monte Carlo re-executions of applications on a conventional microarchitecture, Laplace achieves the same level of uncertainty tracking accuracy with 2,076× fewer executed instructions on average (up to 21,343× fewer).},
  archive      = {J_MICRO},
  author       = {Vasileios Tsoutsouras and Orestis Kaparounakis and Chatura Samarakoon and Bilgesu Bilgin and James Meech and Jan Heck and Phillip Stanley-Marbell},
  doi          = {10.1109/MM.2022.3166067},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {78-86},
  shortjournal = {IEEE Micro},
  title        = {The laplace microarchitecture for tracking data uncertainty},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Characterizing and mitigating soft errors in GPU DRAM.
<em>MICRO</em>, <em>42</em>(4), 69–77. (<a
href="https://doi.org/10.1109/MM.2022.3163122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While graphics processing units (GPUs) are used in high-reliability systems, wide GPU dynamic random-access memory (DRAM) interfaces make error protection difficult, as wide-device correction through error checking and correcting (ECC) is expensive and impractical. This challenge is compounded by worsening relative rates of multibit DRAM errors and increasing GPU memory capacities. This work uses high-energy neutron beam tests to inform the design and evaluation of GPU DRAM error-protection mechanisms. Based on observed locality in multibit error patterns, we propose several novel ECC schemes to decrease the silent data corruption (SDC) risk by up to five orders of magnitude relative to single-bit-error-correcting and double-bit-error-detecting (SEC-DED) ECC, while also reducing the number of uncorrectable errors by up to 7.87×. We compare novel binary and symbol-based ECC organizations that differ in their design complexity and hardware overheads, ultimately recommending two promising organizations. These schemes replace SEC-DED ECC with no additional redundancy, likely no performance degradation, and modest area and complexity costs.},
  archive      = {J_MICRO},
  author       = {Michael B. Sullivan and Nirmal R. Saxena and Mike O’Connor and Donghyuk Lee and Paul Racunas and Saurabh Hukerikar and Timothy Tsai and Siva Kumar Sastry Hari and Stephen W. Keckler},
  doi          = {10.1109/MM.2022.3163122},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {69-77},
  shortjournal = {IEEE Micro},
  title        = {Characterizing and mitigating soft errors in GPU DRAM},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An architecture to accelerate computation on encrypted data.
<em>MICRO</em>, <em>42</em>(4), 59–68. (<a
href="https://doi.org/10.1109/MM.2022.3170792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully homomorphic encryption (FHE) allows computing on encrypted data, enabling secure offloading of computation to untrusted servers. Though it provides ideal security, FHE is prohibitively expensive when executed in software. These overheads are a major barrier to FHE&#39;s widespread adoption. We present F1, the first FHE accelerator that is capable of executing full FHE programs. F1 builds on an in-depth architectural analysis of the characteristics of FHE computations that reveals acceleration opportunities. F1 is a wide-vector processor with novel functional units deeply specialized to FHE primitives, such as modular arithmetic, number-theoretic transforms, and structured permutations. This organization provides so much compute throughput that data movement becomes the bottleneck. Thus, F1 is primarily designed to minimize data movement. F1 is the first system to accelerate complete FHE programs, and outperforms state-of-the-art software implementations by gmean 5,400x. These speedups counter FHE&#39;s overheads and enable new applications, like real-time private deep learning in the cloud.},
  archive      = {J_MICRO},
  author       = {Axel Feldmann and Nikola Samardzic and Aleksandar Krastev and Srinivas Devadas and Ron Dreslinski and Chris Peikert and Daniel Sanchez},
  doi          = {10.1109/MM.2022.3170792},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {59-68},
  shortjournal = {IEEE Micro},
  title        = {An architecture to accelerate computation on encrypted data},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Maya: Using formal control to obfuscate power side channels.
<em>MICRO</em>, <em>42</em>(4), 48–58. (<a
href="https://doi.org/10.1109/MM.2022.3166886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The security of computers is at risk because of information leaking through their power consumption. Attackers can use advanced signal measurement and analysis to recover sensitive data from this side channel. To address this problem, this article presents Maya, a simple and effective defense against power side channels. The idea is to use formal control to re-shape the power dissipated by a computer in an application-transparent manner—preventing attackers from learning any information about the applications that are running. With formal control, a controller can reliably keep power consumption close to a desired target function even when runtime conditions change unpredictably. By selecting the target function intelligently, the controller can make power to follow any desired shape, appearing to carry activity information which, in reality, is unrelated to the application. Maya can be implemented in privileged software, firmware and hardware. We implement Maya on three machines using only privileged threads against machine learning based attacks, and show its effectiveness and ease of deployment. Maya has already thwarted a newly developed remote power attack.},
  archive      = {J_MICRO},
  author       = {Raghavendra Pradyumna Pothukuchi and Sweta Yamini Pothukuchi and Petros G. Voulgaris and Alexander Schwing and Josep Torrellas},
  doi          = {10.1109/MM.2022.3166886},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {48-58},
  shortjournal = {IEEE Micro},
  title        = {Maya: Using formal control to obfuscate power side channels},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Chasing carbon: The elusive environmental footprint of
computing. <em>MICRO</em>, <em>42</em>(4), 37–47. (<a
href="https://doi.org/10.1109/MM.2022.3163226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given recent algorithm, software, and hardware innovation, computing has enabled a plethora of new applications. As computing becomes increasingly ubiquitous, however, so does its environmental impact. This article brings the issue to the attention of computer-systems researchers. Our analysis, built on industry-reported characterization, quantifies the environmental effects of computing in terms of carbon emissions. Broadly, carbon emissions have two sources: operational energy consumption, and hardware manufacturing and infrastructure. Although carbon emissions from the former are decreasing, thanks to algorithmic, software, and hardware innovations that boost performance and power efficiency, the overall carbon footprint of computer systems continues to grow. This work quantifies the carbon output of computer systems to show that most emissions related to modern mobile and data-center equipment come from hardware manufacturing and infrastructure. We, therefore, outline future directions for minimizing the environmental impact of computing systems.},
  archive      = {J_MICRO},
  author       = {Udit Gupta and Young Geun Kim and Sylvia Lee and Jordan Tse and Hsien-Hsin S. Lee and Gu-Yeon Wei and David Brooks and Carole-Jean Wu},
  doi          = {10.1109/MM.2022.3163226},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {37-47},
  shortjournal = {IEEE Micro},
  title        = {Chasing carbon: The elusive environmental footprint of computing},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Practical and scalable ML-driven cloud performance debugging
with sage. <em>MICRO</em>, <em>42</em>(4), 27–36. (<a
href="https://doi.org/10.1109/MM.2022.3169445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud applications are increasingly shifting from large monolithic services to complex graphs of loosely coupled microservices. Despite their benefits, microservices are prone to cascading performance issues, and can lead to prolonged periods of degraded performance. We present Sage, a machine-learning-driven root cause analysis system for interactive cloud microservices that is both accurate and practical. We show that Sage correctly identifies the root causes of performance issues across a diverse set of microservices and takes action to address them, leading to more predictable, performant, and efficient cloud systems.},
  archive      = {J_MICRO},
  author       = {Yu Gan and Mingyu Liang and Sundar Dev and David Lo and Christina Delimitrou},
  doi          = {10.1109/MM.2022.3169445},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {27-36},
  shortjournal = {IEEE Micro},
  title        = {Practical and scalable ML-driven cloud performance debugging with sage},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Warehouse-scale video acceleration. <em>MICRO</em>,
<em>42</em>(4), 18–26. (<a
href="https://doi.org/10.1109/MM.2022.3163244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video processing is foundational to a spectrum of important workloads: video sharing, video conferencing, photos/video archival, virtual/augmented reality, cloud gaming, and live streaming. The exponential growth in these workloads, coincident with a slowing Moore’s law, poses significant challenges to deliver more computing at higher efficiencies. In this article, we present the design of a new accelerator—the video coding unit—targeted at warehouse-scale (cloud) video transcoding. Our deployment at scale in Google, the first of its kind, has demonstrated significant benefits on several products (YouTube, Meet, Stadia, Photos, etc.) serving billions of users.},
  archive      = {J_MICRO},
  author       = {Parthasarathy Ranganathan and Daniel Stodolsky and Jeff Calow and Jeremy Dorfman and Marisabel Guevara and Clinton Wills Smullen IV and Aki Kuusela},
  doi          = {10.1109/MM.2022.3163244},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {18-26},
  shortjournal = {IEEE Micro},
  title        = {Warehouse-scale video acceleration},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Overclocking in immersion-cooled datacenters.
<em>MICRO</em>, <em>42</em>(4), 10–17. (<a
href="https://doi.org/10.1109/MM.2022.3163107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large cloud providers are starting to leverage liquid cooling for an increasing number of workloads. Liquid cooling enables providers to overclock server components, but they must tradeoff the potential increase in performance with higher power draw and reliability implications. We argue that two-phase immersion cooling is the most promising technology and, in that context, explore overclocking, its uses, and implications.},
  archive      = {J_MICRO},
  author       = {Pulkit A. Misra and Ioannis Manousakis and Esha Choukse and Majid Jalili and Íñigo Goiri and Ashish Raniwala and Brijesh Warrier and Husam Alissa and Bharath Ramakrishnan and Phillip Tuma and Christian Belady and Marcus Fontoura and Ricardo Bianchini},
  doi          = {10.1109/MM.2022.3163107},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {10-17},
  shortjournal = {IEEE Micro},
  title        = {Overclocking in immersion-cooled datacenters},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on top picks from the 2021 computer
architecture conferences. <em>MICRO</em>, <em>42</em>(4), 6–9. (<a
href="https://doi.org/10.1109/MM.2022.3173823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.},
  archive      = {J_MICRO},
  author       = {Sudhanva Gurumurthi and Radu Teodorescu},
  doi          = {10.1109/MM.2022.3173823},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {6-9},
  shortjournal = {IEEE Micro},
  title        = {Special issue on top picks from the 2021 computer architecture conferences},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022f). Top picks from 2021 computer architecture conferences!
<em>MICRO</em>, <em>42</em>(4), 4–5. (<a
href="https://doi.org/10.1109/MM.2022.3178622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I EEE Micro is presenting to you the very familiar IEEE Micro Top Picks issue. For more than a decade, IEEE Micro has had this tradition of evaluating papers from the previous year’s architecture conferences and selecting the papers with the most novelty and potential for long-term impact. IEEE Micro is upholding the tradition this year as well. Any article published in top architecture conferences during 2021 was eligible to compete for the Top Picks honor. In total, 109 submissions were received, from which 12 articles were chosen to represent the cream of the crop of 2021.},
  archive      = {J_MICRO},
  author       = {Lizy Kurian John},
  doi          = {10.1109/MM.2022.3178622},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Top picks from 2021 computer architecture conferences!},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022d). Growth from breadth and depth. <em>MICRO</em>,
<em>42</em>(3), 86–88. (<a
href="https://doi.org/10.1109/MM.2022.3166265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Economic growth arises from users’ ability to do something previously unobtainable, either by replacing a less efficient or less satisfying way of doing things. That happens in one of two ways, either through the creation of new users of new technology or through the creation of new uses for existing users of new technology. We make a distinction between these two types of growth because they largely involve different types of investments and user behavior. The distinction continues to have relevance today, as it helps understand contemporaneous experience with the Internet.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2022.3166265},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {86-88},
  shortjournal = {IEEE Micro},
  title        = {Growth from breadth and depth},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Meet the FaM1ly. <em>MICRO</em>, <em>42</em>(3), 78–84. (<a
href="https://doi.org/10.1109/MM.2022.3169245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Apple has designed a new family of SoCs for use predominantly within its Mac (desktop and laptop) products: M1, M1 Pro, M1 Max, and M1 Ultra. Its design and architecture are wildly different from its competition, and its overall value and performance have caught the attention of the entire market. They have shown the world that, with full control, they can build secure, efficient, and high-performance systems. While ceding this control to Apple has been met with mild distaste, the benefits are well worth the tradeoffs. Taking a step back, this move is yet another instance of vertical integration which continues to make its rounds throughout the industry.},
  archive      = {J_MICRO},
  author       = {Michael Mattioli},
  doi          = {10.1109/MM.2022.3169245},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {78-84},
  shortjournal = {IEEE Micro},
  title        = {Meet the FaM1ly},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022f). Review of patents issued to computer architecture companies
in 2021—part II. <em>MICRO</em>, <em>42</em>(3), 67–77. (<a
href="https://doi.org/10.1109/MM.2022.3163434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last article (Part I), which appeared in the March/April 2022 issue of IEEE Micro, the following was analyzed: 1) the number of all patents and the number of computer architecture patents issued in 2021 and 2) the distribution of those patents across patent classes, for 18 computer architecture companies. The author then presented a notable patent for five of those companies. This article presents a notable patent for the remaining companies. As described in more detail in the previous article, each of the following patents is from the G06F (“Electric digital data processing”) patent class as that patent class had the largest number of patents for all but three companies and because it was the only patent class where all companies had a least one patent. Furthermore, the below patents are from large patent families, which are generally considered to be more valuable.},
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2022.3163434},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {67-77},
  shortjournal = {IEEE Micro},
  title        = {Review of patents issued to computer architecture companies in 2021—Part II},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AI’s 10 to watch award: Call for nominations.
<em>MICRO</em>, <em>42</em>(3), 66. (<a
href="https://doi.org/10.1109/MM.2022.3176768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the guidelines for select CS society award nominations.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2022.3176768},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {66},
  shortjournal = {IEEE Micro},
  title        = {AI&#39;s 10 to watch award: Call for nominations},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Can we trust undervolting in FPGA-based deep learning
designs at harsh conditions? <em>MICRO</em>, <em>42</em>(3), 57–65. (<a
href="https://doi.org/10.1109/MM.2022.3153891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As more neural networks on field-programmable gate arrays (FPGAs) are used in a wider context, the importance of power efficiency increases. However, the focus on power should never compromise application accuracy. One technique to increase power efficiency is reducing the FPGAs’ supply voltage (“undervolting”), which can cause accuracy problems. Therefore, careful design-time considerations are required for correct configuration without hindering the target accuracy. This fact becomes especially important for autonomous systems, edge computing, or data centers. This study reveals the impact of undervolting in harsh environmental conditions on the accuracy and power efficiency of convolutional neural network benchmarks. We perform comprehensive testing in a calibrated infrastructure at controlled temperatures (between –40 $^{\circ }$∘C and 50 $^{\circ }$∘C) and four distinct humidity levels (50\%, 60\%, 70\%, and 80\%) for off-the-shelf FPGAs. We show that the voltage guard-band shift with temperature is linear and propose new reliable undervolting designs providing a 65\% increase in power-efficiency Giga-OPs per second (GOPS/W).},
  archive      = {J_MICRO},
  author       = {Fahrettin Koc and Behzad Salami and Oguz Ergin and Osman Unsal and Adrian Cristal Kestelman},
  doi          = {10.1109/MM.2022.3153891},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {57-65},
  shortjournal = {IEEE Micro},
  title        = {Can we trust undervolting in FPGA-based deep learning designs at harsh conditions?},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reducing the impact of interjob interference in dragonfly
networks using virtual partitions. <em>MICRO</em>, <em>42</em>(3),
50–56. (<a href="https://doi.org/10.1109/MM.2022.3151258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-performance computing systems, the interference among applications that share network resources is one of the major causes of performance degradation and increase variability among different executions of the same application. In this article, we propose a virtual partitioning scheme for dragonfly networks that combines a job-allocation policy with a virtual channel (VC) assignment. Our experiments confirm that our proposal offers a better performance than the established schemes, also reducing variability.},
  archive      = {J_MICRO},
  author       = {German Maglione-Mathey and Jesus Escudero-Sahuquillo and Pedro Javier Garcia and Francisco J. Quiles},
  doi          = {10.1109/MM.2022.3151258},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {50-56},
  shortjournal = {IEEE Micro},
  title        = {Reducing the impact of interjob interference in dragonfly networks using virtual partitions},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interconnects for DNA, quantum, in-memory, and optical
computing: Insights from a panel discussion. <em>MICRO</em>,
<em>42</em>(3), 40–49. (<a
href="https://doi.org/10.1109/MM.2022.3150684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computing world is witnessing a proverbial Cambrian explosion of emerging paradigms propelled by applications, such as artificial intelligence, big data, and cybersecurity. The recent advances in technology to store digital data inside a deoxyribonucleic acid (DNA) strand, manipulate quantum bits (qubits), perform logical operations with photons, and perform computations inside memory systems are ushering in the era of emerging paradigms of DNA computing, quantum computing, optical computing, and in-memory computing. In an orthogonal direction, research on interconnect design using advanced electro-optic, wireless, and microfluidic technologies has shown promising solutions to the architectural limitations of traditional von-Neumann computers. In this article, experts present their comments on the role of interconnects in the emerging computing paradigms, and discuss the potential use of chiplet-based architectures for the heterogeneous integration of such technologies.},
  archive      = {J_MICRO},
  author       = {Amlan Ganguly and Sergi Abadal and Ishan Thakkar and Natalie Enright Jerger and Marc Riedel and Masoud Babaie and Rajeev Balasubramonian and Abu Sebastian and Sudeep Pasricha and Baris Taskin},
  doi          = {10.1109/MM.2022.3150684},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {40-49},
  shortjournal = {IEEE Micro},
  title        = {Interconnects for DNA, quantum, in-memory, and optical computing: Insights from a panel discussion},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerating ML recommendation with over 1,000 RISC-v/tensor
processors on esperanto’s ET-SoC-1 chip. <em>MICRO</em>, <em>42</em>(3),
31–38. (<a href="https://doi.org/10.1109/MM.2022.3140674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) recommendation workloads have demanding performance and memory requirements and, to date, have largely been run on servers with x86 processors. To accelerate these workloads (and others), Esperanto Technologies has implemented over 1,000 low-power RISC-V processors on a single chip along with a distributed on-die memory system. The ET-SoC-1 chip is designed to compute at peak rates between 100 and 200 TOPS and to be able to run ML recommendation workloads while consuming less than 20 W of power. Preliminary data presented at the Hot Chips 33 Conference projected over a hundred times better performance per watt for an Esperanto-based accelerator card versus a standard server platform for the MLPerf Deep Learning Recommendation Model benchmark.},
  archive      = {J_MICRO},
  author       = {David R. Ditzel and the Esperanto team},
  doi          = {10.1109/MM.2022.3140674},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {31-38},
  shortjournal = {IEEE Micro},
  title        = {Accelerating ML recommendation with over 1,000 RISC-V/Tensor processors on esperanto&#39;s ET-SoC-1 chip},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Aquabolt-XL HBM2-PIM, LPDDR5-PIM with in-memory processing,
and AXDIMM with acceleration buffer. <em>MICRO</em>, <em>42</em>(3),
20–30. (<a href="https://doi.org/10.1109/MM.2022.3164651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing-in-memory (PIM) has been proposed to improve the performance of bandwidth-intensive workloads as well as save energy due to reduced compute-memory data movement. To realize PIM, programmable computing units were integrated with memory cores on an HBM2 device to enable parallel processing and minimize data movement. A graphics processing unit (GPU) system equipped with Samsung Aquabolt-XL HBM2-PIM devices improved microkernel general matrix–vector multiplication and speech recognition applications by 8.9× and 3.5×, respectively, and reduced energy consumption by over 60\%. In a Xilinx AlveoU280 system, microkernel GEMV and ADD workload performances improved by 2.8×, and long short-term memory workload improved by 2.54×. Simulations show that a performance gain of over 2.3× may be attained in a system with LP5-PIM for certain transformer-based speech recognition with an energy reduction of 86\%. In addition, AXDIMM, a DIMM-level PIM with acceleration buffers, exhibits an 80\% performance improvement and a 42.6\% energy savings over a regular RDIMM system.},
  archive      = {J_MICRO},
  author       = {Jin Hyun Kim and Shin-Haeng Kang and Sukhan Lee and Hyeonsu Kim and Yuhwan Ro and Seungwon Lee and David Wang and Jihyun Choi and Jinin So and YeonGon Cho and JoonHo Song and Jeonghyeon Cho and Kyomin Sohn and Nam Sung Kim},
  doi          = {10.1109/MM.2022.3164651},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {20-30},
  shortjournal = {IEEE Micro},
  title        = {Aquabolt-XL HBM2-PIM, LPDDR5-PIM with in-memory processing, and AXDIMM with acceleration buffer},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intel alder lake CPU architectures. <em>MICRO</em>,
<em>42</em>(3), 13–19. (<a
href="https://doi.org/10.1109/MM.2022.3164338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alder Lake cores, system-on-a-chip, and hybrid architecture were designed to meet the challenging requirements for various traditional, emerging, and real-world computational tasks at extensive power performance points. Alder Lake introduces a revolutionary hybrid architecture with a whole new performance core and efficient core. The revolutionary thread director built into Alder Lake hardware and firmware guides the operating system in scheduling the right task to the right core type. Alder Lake hybrid architecture with Intel thread director delivers the highest power-performance dynamic range and computational density.},
  archive      = {J_MICRO},
  author       = {Efraim Rotem and Adi Yoaz and Lihu Rappoport and Stephen J. Robinson and Julius Yuli Mandelblat and Arik Gihon and Eliezer Weissmann and Rajshree Chabukswar and Vadim Basin and Russell Fenger and Monica Gupta and Ahmad Yasin},
  doi          = {10.1109/MM.2022.3164338},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {13-19},
  shortjournal = {IEEE Micro},
  title        = {Intel alder lake CPU architectures},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The AMD next-generation “zen 3” core. <em>MICRO</em>,
<em>42</em>(3), 7–12. (<a
href="https://doi.org/10.1109/MM.2022.3152788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {“Zen 3” is a processor core that was designed to deliver another large generational increase in central processing unit (CPU) performance while also supporting new software and security features. The core is leveraged into multiple products spanning markets including mobile, desktop, and server. “Zen 3” also includes an innovative new set of L3 cache solutions including a redesigned eight-core complex and support for stacked AMD 3-D V-cache.},
  archive      = {J_MICRO},
  author       = {Mark Evers and Leslie Barnes and Mike Clark},
  doi          = {10.1109/MM.2022.3152788},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {7-12},
  shortjournal = {IEEE Micro},
  title        = {The AMD next-generation “Zen 3” core},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on hot chips 33. <em>MICRO</em>,
<em>42</em>(3), 6. (<a
href="https://doi.org/10.1109/MM.2022.3165863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The articles in this special section are focus on new chip technologies presented at the Hot Chips 33 Conference held in August 2021. The Hot Chips Conference has served as a leading venue for presenting the technical details of innovative chips designed by academics, startup companies, and established leaders in the field. The conference in 2021 included presentations on a wide range of chips and upcoming chip and packaging technologies. The program committee carefully selected several presentations for which we invited the authors to write an article. Following the customary review process, four articles were accepted to be included in this special issue. These articles describe central processing units (CPUs) that are used in a broad range of products from servers and desktops to mobile solutions, a chip for accelerating AI recommendations, and an HBM2 chip that includes in-memory processing capability. As process technologies continue to provide more transistors per die, each of these chips shows the continuing trend of using those transistors to deliver more functionality per part.},
  archive      = {J_MICRO},
  author       = {Alisa Scherer and Guri Sohi},
  doi          = {10.1109/MM.2022.3165863},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {6},
  shortjournal = {IEEE Micro},
  title        = {Special issue on hot chips 33},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Hot chips 33 and more! <em>MICRO</em>, <em>42</em>(3), 4–5.
(<a href="https://doi.org/10.1109/MM.2022.3166680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This issue features selected articles from the Hot Chips 33 Symposium, held virtually in August 2021. COVID-19 forced Hot Chips to be a virtual event again; however, the chips presented at the event were really interesting and more powerful than ever! Whether it is artificial intelligence (AI) acceleration or sheer increase in traditional compute capability, emerging chip designs are significant enhancements over their predecessors. New chips are announced every month, and AI accelerators are influencing applications from the cloud to the edge.},
  archive      = {J_MICRO},
  author       = {Lizy Kurian John},
  doi          = {10.1109/MM.2022.3166680},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Hot chips 33 and more!},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IEEE COMPUTER SOCIETY JOBS BOARD. <em>MICRO</em>,
<em>42</em>(2), C4. (<a
href="https://doi.org/10.1109/MM.2022.3151210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2022.3151210},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {C4},
  shortjournal = {IEEE Micro},
  title        = {IEEE COMPUTER SOCIETY JOBS BOARD},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). ComputingEdge. <em>MICRO</em>, <em>42</em>(2), C2. (<a
href="https://doi.org/10.1109/MM.2022.3151180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2022.3151180},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {C2},
  shortjournal = {IEEE Micro},
  title        = {ComputingEdge},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022f). Time for a change in u.s. Antitrust for technology?
<em>MICRO</em>, <em>42</em>(2), 86–88. (<a
href="https://doi.org/10.1109/MM.2022.3151204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Antitrust law makes it illegal For dominant firms to use their Success to impede the Competitive markets, but who Defines &quot;impede&quot; and &quot;competitive?&quot; Today’s column steps back from the fray and characterizes the strengths and weaknesses of the present system. The discussion aims at somebody who wants to understand the status quo, and why the present system works sometimes, and why proposals might want to change it. It is possible to illustrate many basic features of the U.S. approach to antitrust by comparing a settled lawsuit against a pending case. The settled case is the historical case against Microsoft. More than two decades ago, Microsoft fought a high-profile federal case over the governance of its PC operating system. It can be compared with the pending case against Alphabet, who owns and operates Google, the dominant search engine. A federal suit seeks to end exclusive deals that Google makes with others.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2022.3151204},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {86-88},
  shortjournal = {IEEE Micro},
  title        = {Time for a change in U.S. antitrust for technology?},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). IEEE computer society. <em>MICRO</em>, <em>42</em>(2), 85.
(<a href="https://doi.org/10.1109/MM.2022.3151206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2022.3151206},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {85},
  shortjournal = {IEEE Micro},
  title        = {IEEE computer society},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). IEEE security &amp; privacy. <em>MICRO</em>,
<em>42</em>(2), 84. (<a
href="https://doi.org/10.1109/MM.2022.3160292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE. IEEE Security &amp; Privacy magazine provides articles with both a practical and research bent by the top thinkers in the field.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2022.3160292},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {84},
  shortjournal = {IEEE Micro},
  title        = {IEEE security &amp; privacy},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022e). Review of patents issued to computer architecture companies
in 2021 [micro law]. <em>MICRO</em>, <em>42</em>(2), 77–84. (<a
href="https://doi.org/10.1109/MM.2022.3150818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This two-part article takes a break from my series on the Analysis of Historical Patenting Behavior and Patent Characteristics of Computer Architecture Companies in order to analyze the patents that were issued to 18 computer architecture companies in 2021. Part III of the series will return in the July/August issue of IEEE Micro. This two-part article first examines the number of total patents and the number of computer architecture patents that were issued to each company in 2021. Second, it compares the distribution of those patents across different patent classes. Third, it highlights one patent from each company that may be particularly notable.},
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2022.3150818},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {77-84},
  shortjournal = {IEEE Micro},
  title        = {Review of patents issued to computer architecture companies in 2021 [Micro law]},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerating deep learning using interconnect-aware UCX
communication for MPI collectives. <em>MICRO</em>, <em>42</em>(2),
68–76. (<a href="https://doi.org/10.1109/MM.2022.3148670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning workloads on modern multi-graphics processing unit (GPU) nodes are highly dependent on intranode interconnects, such as NVLink and PCIe, for high-performance communication. In this article, we take on the challenge to design an interconnect-aware multipath GPU-to-GPU communication using unified communication X (UCX) to utilize all available bandwidth for both NVLink-based systems and those that use a mixture of NVLink and PCIe. Our proposed multipath data transfer mechanism pipelines and stripes the message across multiple intrasocket communication channels and memory regions to achieve 1.84× higher bandwidth for Open message passing interface (MPI) on NVLink-based systems and 1.23× on NVLink and PCIe systems. We then utilize this mechanism to propose a three-stage hierarchical, pipelined MPI_Allreduce design as well as a flat pipelined two-stage algorithm for two different node topologies. For large messages, our proposed algorithms achieve a high speedup when compared to other MPI implementations. We also observe significant speedup for the proposed MPI_Allreduce with Horovod + TensorFlow with a variety of deep learning models.},
  archive      = {J_MICRO},
  author       = {Yiltan Hassan Temuçin and Amir Hossein Sojoodi and Pedram Alizadeh and Benjamin Kitor and Ahmad Afsahi},
  doi          = {10.1109/MM.2022.3148670},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {68-76},
  shortjournal = {IEEE Micro},
  title        = {Accelerating deep learning using interconnect-aware UCX communication for MPI collectives},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). IEEE transactions on big data. <em>MICRO</em>,
<em>42</em>(2), 67. (<a
href="https://doi.org/10.1109/MM.2022.3160261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE. Subscribe and submit. For more information on paper submission, featured articles, calls for papers, and subscription links visit: www.computer.org/tbd .},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2022.3160261},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {67},
  shortjournal = {IEEE Micro},
  title        = {IEEE transactions on big data},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Polarized routing for large interconnection networks.
<em>MICRO</em>, <em>42</em>(2), 61–67. (<a
href="https://doi.org/10.1109/MM.2021.3139392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supercomputers and datacenters comprise hundreds of thousands of servers. Different network topologies have been proposed to attain such a high scalability, from flattened Butterfly and Dragonfly to the most disruptive Jellyfish, which is based on a random graph. The routing problem on such networks remains a challenge that can be tackled either as a topology-aware solution or with an agnostic approach. The case of random networks is a very special one since no a priori topological clues can be exploited. In this article, we introduce the polarized routing algorithm, an adaptive nonminimal hop-by-hop mechanism that can be used in most of topologies, including Jellyfish. Polarized routing follows two design criteria: a source–destination symmetry in the routes and avoiding backtracking. Experimental evaluation proves that polarized routing not only outperforms other routings in random graphs but also attains the best performance provided by ad hoc solutions for specific outstanding low-diameter interconnection networks.},
  archive      = {J_MICRO},
  author       = {Cristóbal Camarero and Carmen Martínez and Ramón Beivide},
  doi          = {10.1109/MM.2021.3139392},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {61-67},
  shortjournal = {IEEE Micro},
  title        = {Polarized routing for large interconnection networks},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimizing distributed DNN training using CPUs and
BlueField-2 DPUs. <em>MICRO</em>, <em>42</em>(2), 53–60. (<a
href="https://doi.org/10.1109/MM.2021.3139027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep learning (DL) training process consists of multiple phases—data augmentation, training, and validation of the trained model. Traditionally, these phases are executed either on the central processing units or graphics processing units in a serial fashion due to lack of additional computing resources to offload independent phases of DL training. Recently, Mellanox/NVIDIA introduced the BlueField-2 data processing units (DPUs), which combine the advanced capabilities of traditional application-specific-integrated-circuit-based network adapters with an array of ARM processors. In this article, we explore how to take advantage of the additional ARM cores on the BlueField-2 DPUs. We propose and evaluate multiple novel designs to efficiently offload the phases of DL training to the DPUs. Our experimental results show that the proposed designs are able to deliver up to 17.5\% improvement in overall DL training time. To the best of our knowledge, this is the first work to explore the use of DPUs to accelerate DL training.},
  archive      = {J_MICRO},
  author       = {Arpan Jain and Nawras Alnaasan and Aamir Shafi and Hari Subramoni and Dhabaleswar K. Panda},
  doi          = {10.1109/MM.2021.3139027},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {53-60},
  shortjournal = {IEEE Micro},
  title        = {Optimizing distributed DNN training using CPUs and BlueField-2 DPUs},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerating allreduce with in-network reduction on intel
PIUMA. <em>MICRO</em>, <em>42</em>(2), 44–52. (<a
href="https://doi.org/10.1109/MM.2021.3139092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Intel Programmable Integrated Unified Memory Architecture (PIUMA) system maps collective operations directly into the network switches and supports pipelined embeddings for high-throughput collective computation. Utilizing these features and PIUMA’s network topology, we develop a methodology to generate extremely low latency embeddings for in-network Allreduce. Our analysis shows that the proposed in-network Allreduce is highly scalable, with less than 1.5-μs single-element latency on 16K nodes. Compared to host-based Allreduce, it exhibits 36× less latency and 3.5× higher throughput. With deep neural network training as an example, we further demonstrate the benefits of our in-network Allreduce on end-user applications.},
  archive      = {J_MICRO},
  author       = {Kartik Lakhotia and Fabrizio Petrini and Rajgopal Kannan and Viktor Prasanna},
  doi          = {10.1109/MM.2021.3139092},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {44-52},
  shortjournal = {IEEE Micro},
  title        = {Accelerating allreduce with in-network reduction on intel PIUMA},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A low-latency and low-power approach for coherency and
memory protocols on PCI express 6.0 PHY at 64.0 GT/s with PAM-4
signaling. <em>MICRO</em>, <em>42</em>(2), 37–43. (<a
href="https://doi.org/10.1109/MM.2021.3137807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PCI Express (PCIe) PHY is used for alternate protocols with memory and coherency semantics such as Compute Express link and Ultra-Path Interconnect due to its low latency and power efficiency. In this article, we propose mechanisms to use PCIe 6.0 PHY at 64.0 GT/s with PAM-4 signaling for coherency and memory protocols in platforms deploying heterogeneous processing elements and memory subsystems with 0 latency impact, high-bandwidth efficiency, and high reliability. We also propose a new L0p mechanism for power savings with low latency.},
  archive      = {J_MICRO},
  author       = {Debendra Das Sharma},
  doi          = {10.1109/MM.2021.3137807},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {37-43},
  shortjournal = {IEEE Micro},
  title        = {A low-latency and low-power approach for coherency and memory protocols on PCI express 6.0 PHY at 64.0 GT/s with PAM-4 signaling},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on hot interconnects. <em>MICRO</em>,
<em>42</em>(2), 35–36. (<a
href="https://doi.org/10.1109/MM.2022.3151214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_MICRO},
  author       = {Sayan Ghosh and Ryan E. Grant and Min Si},
  doi          = {10.1109/MM.2022.3151214},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {35-36},
  shortjournal = {IEEE Micro},
  title        = {Special issue on hot interconnects},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Co-design and system for the supercomputer “fugaku.”
<em>MICRO</em>, <em>42</em>(2), 26–34. (<a
href="https://doi.org/10.1109/MM.2021.3136882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The supercomputer “Fugaku” is an exascale manycore-based parallel system developed as a Japanese national flagship supercomputer in the FLAGSHIP 2020 Project. While “Fugaku” was ranked first for several benchmarks such as TOP500, HPCG, HPL-AI, and Graph500 in 2020, the major design concept is the application-first concept by the co-design for power efficiency and high performance. We have designed an original manycore processor based on Armv8 instruction sets with the scalable vector extension, A64FX processor, with Fujitsu, our industry partner. The system consists of 158,976 nodes with 7.6 million cores in total and a theoretical peak of 537 Peta Floating-point Operations Per Second in double-precision floating points, connected by Tofu-D interconnect. The high performance computing (HPC)-oriented design enables an extremely good performance for memory-intensive workloads thanks to HBM2 memory with breakthrough power efficiency. In this article, we present the pragmatic practice of our co-design effort for “Fugaku” followed by an overview and the performance of “Fugaku.”},
  archive      = {J_MICRO},
  author       = {Mitsuhisa Sato and Yuetsu Kodama and Miwako Tsuji and Tesuya Odajima},
  doi          = {10.1109/MM.2021.3136882},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {26-34},
  shortjournal = {IEEE Micro},
  title        = {Co-design and system for the supercomputer “Fugaku”},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Computing in science &amp; engineering. <em>MICRO</em>,
<em>42</em>(2), 25. (<a
href="https://doi.org/10.1109/MM.2022.3160258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE. The computational and data-centric problems faced by scientists and engineers transcend disciplines. There is a need to share knowledge of algorithms, software, and architectures, and to transmit lessonslearned to a broad scientific audience. Computing in Science &amp; Engineering (CiSE) is a cross-disciplinary, international publication that meets this need by presenting contributions of high interest and educational value from a variety of fields, including physics, biology, chemistry, and astronomy. CiSE emphasizes innovative applications in cutting-edge techniques. CiSE publishes peer-reviewed research articles, as well as departments spanning news and analyses, topical reviews, tutorials, case studies, and more. Read CiSE today! www.computer.org/cise .},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2022.3160258},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {25},
  shortjournal = {IEEE Micro},
  title        = {Computing in science &amp; engineering},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A mobile DNN training processor with automatic bit precision
search and fine-grained sparsity exploitation. <em>MICRO</em>,
<em>42</em>(2), 16–25. (<a
href="https://doi.org/10.1109/MM.2021.3135457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an energy-efficient deep learning processor is proposed for deep neural network (DNN) training in mobile platforms. Conventional mobile DNN training processors suffer from high-bit precision requirement and high ReLU-dependencies. The proposed processor breaks through these fundamental issues by adopting three new features. It first combines the runtime automatic bit precision searching method addition to both conventional dynamic fixed-point representation and stochastic rounding to realize low-precision training. It adopts bit-slice scalable core architecture with the input skipping functionality to exploit bit-slice-level fine-grained sparsity. The iterative channel reordering unit helps the processor to maintain high core utilization by solving the workload unbalancing problem during zero-slice skipping. It finally achieves at least 4.4× higher energy efficiency compared with the conventional DNN training processors.},
  archive      = {J_MICRO},
  author       = {Donghyeon Han and Dongseok Im and Gwangtae Park and Youngwoo Kim and Seokchan Song and Juhyoung Lee and Hoi-Jun Yoo},
  doi          = {10.1109/MM.2021.3135457},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {16-25},
  shortjournal = {IEEE Micro},
  title        = {A mobile DNN training processor with automatic bit precision search and fine-grained sparsity exploitation},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LSFQ: A low-bit full integer quantization for
high-performance FPGA-based CNN acceleration. <em>MICRO</em>,
<em>42</em>(2), 8–15. (<a
href="https://doi.org/10.1109/MM.2021.3134968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effective implementation of quantization depends not only on the specific task but also on the hardware resources. This article presents a hardware-aware customized quantization method for convolutional neural networks. We propose a learnable parameter soft clipping full integer quantization (LSFQ), which includes weight and activation quantization with the learnable clipping parameters. Moreover, the LSFQ accelerator architecture is customized on the field-programmable gate array (FPGA) platform to verify the hardware awareness of our method, in which DSP48E2 is designed to realize the parallel computation of six low-bit integer multiplications. The results showed that the accuracy loss of LSFQ is less than 1\% compared with the full-precision models including VGG7, mobile-net v2 in CIFAR10, and CIFAR100. An LSFQ accelerator was demonstrated at the 57th IEEE/ACM Design Automation Conference System Design Contest (DAC-SDC) and won the championship at the FPGA track.},
  archive      = {J_MICRO},
  author       = {Zhenshan Bao and Guohang Fu and Wenbo Zhang and Kang Zhan and Junnan Guo},
  doi          = {10.1109/MM.2021.3134968},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {8-15},
  shortjournal = {IEEE Micro},
  title        = {LSFQ: A low-bit full integer quantization for high-performance FPGA-based CNN acceleration},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on cool chips. <em>MICRO</em>, <em>42</em>(2),
6–7. (<a href="https://doi.org/10.1109/MM.2022.3149086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special issue of IEEE Micro captures three contributions from among the ten regular and nine invited presentations of Cool Chips 24. The three papers are focused on convolutional neural network (CNN) acceleration, mobile deep neural network (DNN) training, and a supercomputer; were written by authors from China, Korea, and Japan, respectively; and all of them were the emerging topics at Cool Chips 24.},
  archive      = {J_MICRO},
  author       = {Makoto Ikeda and Fumio Arakawa},
  doi          = {10.1109/MM.2022.3149086},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {6-7},
  shortjournal = {IEEE Micro},
  title        = {Special issue on cool chips},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022e). Special issue on cool chips and hot interconnects.
<em>MICRO</em>, <em>42</em>(2), 4–5. (<a
href="https://doi.org/10.1109/MM.2022.3155994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special issue brings to IEEE Micro’s readership the best from two interesting conferences - Cool Chips 2021 and Hot Interconnects 2021. Cool Chips is a conference that focuses on the research on state-of-the-art low-power, high-speed chips and challenges facing researchers to simultaneously achieve low power consumption and high chip performance. The past couple of years have seen a flurry of research in low-power high-speed chips in artificial intelligence (AI), Internet of Things (IoT), consumer electronics, etc., and several of these were presented at the Cool Chips conference. Three selected papers from Cool Chips are presented in this issue. This special issue also presents a collection of articles on interconnect technologies based on the Hot Interconnect Conference of August 2021. Hot Interconnects is a conference that brings to light state of the art in interconnect technology. In modern systems on chip and large-scale computer systems, many processing components are being interconnected and the larger system’s performance and energy consumption significantly depends on the interconnect. Five selected articles from Hot Interconnects are presented in this issue. These are briefly summarized here.},
  archive      = {J_MICRO},
  author       = {Lizy Kurian John},
  doi          = {10.1109/MM.2022.3155994},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Special issue on cool chips and hot interconnects},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IEEE computer society jobs board. <em>MICRO</em>,
<em>42</em>(1), C4. (<a
href="https://doi.org/10.1109/MM.2021.3134739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2021.3134739},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {C4},
  shortjournal = {IEEE Micro},
  title        = {IEEE computer society jobs board},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Get published in the new IEEE open journal of the computer
society. <em>MICRO</em>, <em>42</em>(1), C3. (<a
href="https://doi.org/10.1109/MM.2021.3134737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2021.3134737},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {C3},
  shortjournal = {IEEE Micro},
  title        = {Get published in the new IEEE open journal of the computer society},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). ComputingEdge. <em>MICRO</em>, <em>42</em>(1), C2. (<a
href="https://doi.org/10.1109/MM.2021.3134731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE. ComputingEdge, your one-stop resource for industry hot topics, technical overviews, and in-depth articles. Cutting-edge articles from the IEEE Computer Society&#39;s portfolio of 12 magazines. Unique original content by computing thought leaders, innovators, and experts. Keeps you up to date on what you need to know across the technology spectrum.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2021.3134731},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {C2},
  shortjournal = {IEEE Micro},
  title        = {ComputingEdge},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IEEE annals of the history of computing. <em>MICRO</em>,
<em>42</em>(1), 140. (<a
href="https://doi.org/10.1109/MM.2022.3147392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2022.3147392},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {140},
  shortjournal = {IEEE Micro},
  title        = {IEEE annals of the history of computing},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Google and apple signed a deal. <em>MICRO</em>,
<em>42</em>(1), 138–140. (<a
href="https://doi.org/10.1109/MM.2021.3129395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reports on the deal between and Google and Apple. The pending federal case against Google echoes tropes found in prior antitrust cases. It partially ventures into familiar legal and economic territory. Prosecutors ask for the end of the exclusive deals Google has signedwithmany other market participants. Look closely. The federal case is two cases wrapped into one. One set of exclusive deals, between Google and Apple, fundamentally differ from the others. The deal between two of the richest companies in the world raises its own set of questions. That is so even if all the other exclusive deals disappear. Google pays Apple something like $12 billion a year. The payments are for making the Google search engine the default search engine on the iPhone, as well as making it the default search engine on other Apple products and services—i.e., Siri, Safari, and so on. I think the federal case about this particular deal contains considerable merit.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2021.3129395},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {138-140},
  shortjournal = {IEEE Micro},
  title        = {Google and apple signed a deal},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). IEEE computer society. <em>MICRO</em>, <em>42</em>(1), 137.
(<a href="https://doi.org/10.1109/MM.2022.3142929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2022.3142929},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {137},
  shortjournal = {IEEE Micro},
  title        = {IEEE computer society},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IEEE computer graphics and applications. <em>MICRO</em>,
<em>42</em>(1), 136. (<a
href="https://doi.org/10.1109/MM.2022.3147390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2022.3147390},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {136},
  shortjournal = {IEEE Micro},
  title        = {IEEE computer graphics and applications},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Analysis of historical patenting behavior and patent
characteristics of computer architecture companies—part II: Prosecution
time and effective patent term length. <em>MICRO</em>, <em>42</em>(1),
128–136. (<a href="https://doi.org/10.1109/MM.2021.3135860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents an analysis of historical patenting behavior and patent characteristics of computer architecture companies.},
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2021.3135860},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {128-136},
  shortjournal = {IEEE Micro},
  title        = {Analysis of historical patenting behavior and patent characteristics of computer architecture Companies—Part II: Prosecution time and effective patent term length},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Near-memory processing in action: Accelerating personalized
recommendation with AxDIMM. <em>MICRO</em>, <em>42</em>(1), 116–127. (<a
href="https://doi.org/10.1109/MM.2021.3097700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Near-memory processing (NMP) is a prospective paradigm enabling memory-centric computing. By moving the compute capability next to the main memory (DRAM modules), it can fundamentally address the CPU-memory bandwidth bottleneck and thus effectively improve the performance of memory-constrained workloads. Using the personalized recommendation system as a driving example, we developed a scalable, practical DIMM-based NMP solution tailor-designed for accelerating the inference serving. Our solution is demonstrated on a versatile FPGA-enabled NMP platform called AxDIMM that allows rapid prototyping and evaluation of NMP’s performance potential on real hardware under a realistic system setting using industry-representative recommendation framework. We experimentally validated the performance of a two-ranked AxDIMM prototype, which achieves up to 1.89× speedup in latency and 31.6\% memory energy saving for embedding operations. For end-to-end recommendation inference serving, AxDIMM improves the throughput up to 1.5× and latency-bounded throughput up to 1.77×, respectively.},
  archive      = {J_MICRO},
  author       = {Liu Ke and Xuan Zhang and Jinin So and Jong-Geon Lee and Shin-Haeng Kang and Sukhan Lee and Songyi Han and YeonGon Cho and Jin Hyun Kim and Yongsuk Kwon and KyungSoo Kim and Jin Jung and Ilkwon Yun and Sung Joo Park and Hyunsun Park and Joonho Song and Jeonghyeon Cho and Kyomin Sohn and Nam Sung Kim and Hsien-Hsin S. Lee},
  doi          = {10.1109/MM.2021.3097700},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {116-127},
  shortjournal = {IEEE Micro},
  title        = {Near-memory processing in action: Accelerating personalized recommendation with AxDIMM},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Supporting moderate data dependency, position dependency,
and divergence in PIM-based accelerators. <em>MICRO</em>,
<em>42</em>(1), 108–115. (<a
href="https://doi.org/10.1109/MM.2021.3136189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing in memory (PIM) can alleviate the data movement overhead. However, PIM units built inside memory layers have low frequency, and this requires high parallelism to compensate for the low clock frequency. Single-instruction–multiple-data (SIMD) architectures can provide high parallelism for PIM with low overhead per arithmetic logic unit (ALU) operation. In SIMD, multiple ALUs perform the same instruction, and the processing unit accesses multiple consecutive words at once. Therefore, the control and access overhead is amortized among ALU operations. However, SIMD units cannot fully exploit the available word-level parallelism for 1) operations with data/position dependency or 2) operations with divergence (where different operations are performed on different words). A recent work, Fulcrum, proposes a subarray-level PIM design with high parallelism. This article discusses how Fulcrum alleviates the control and access overhead while exploiting word-level parallelism for operations with data/position dependency and divergence. We evaluate Fulcrum against bank-level SIMD architectures to highlight these benefits.},
  archive      = {J_MICRO},
  author       = {Marzieh Lenjani and Kevin Skadron},
  doi          = {10.1109/MM.2021.3136189},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {108-115},
  shortjournal = {IEEE Micro},
  title        = {Supporting moderate data dependency, position dependency, and divergence in PIM-based accelerators},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ECIM: Exponent computing in memory for an energy-efficient
heterogeneous floating-point DNN training processor. <em>MICRO</em>,
<em>42</em>(1), 99–107. (<a
href="https://doi.org/10.1109/MM.2021.3096236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors propose a heterogeneous floating-point (FP) computing architecture to maximize energy efficiency by separately optimizing exponent processing and mantissa processing. The proposed exponent-computing-in-memory architecture and mantissa-free exponent-computing algorithm reduce the power consumption of both memory and FP MAC while resolving previous FP computing-in-memory processors’ limitations. Also, a bfloat16 DNN training processor with proposed features and sparsity exploitation support is implemented and fabricated in 28-nm CMOS technology. It achieves 13.7-TFLOPS/W energy efficiency while supporting FP operations with CIM architecture.},
  archive      = {J_MICRO},
  author       = {Juhyoung Lee and Jihoon Kim and Wooyoung Jo and Sangyeob Kim and Sangjin Kim and Hoi-Jun Yoo},
  doi          = {10.1109/MM.2021.3096236},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {99-107},
  shortjournal = {IEEE Micro},
  title        = {ECIM: Exponent computing in memory for an energy-efficient heterogeneous floating-point DNN training processor},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Temperature-resilient RRAM-based in-memory computing for DNN
inference. <em>MICRO</em>, <em>42</em>(1), 89–98. (<a
href="https://doi.org/10.1109/MM.2021.3131114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resistive random access memory (RRAM)-based in-memory computing (IMC) has emerged as a promising paradigm for efficient deep neural network (DNN) acceleration. However, the multibit RRAMs often suffer from nonideal characteristics such as drift and retention failure against temperature changes, leading to significant inference accuracy degradation. In this article, we present a new temperature-resilient RRAM-based IMC scheme for reliable DNN inference hardware. From a 90-nm RRAM prototype chip, we first measure the retention characteristics of multilevel HfO$\mathbf {_2}$2 RRAMs at various temperatures up to 120$^{\circ }$∘C, and then rigorously model the temperature-dependent RRAM retention behavior. We propose a novel and efficient DNN training/inference scheme along with the system-level hardware design to resolve the temperature-dependent retention issues with one-time DNN deployment. Employing the proposed scheme on a 256×256 RRAM array with the circuit-level benchmark simulator NeuroSim, we demonstrate robust RRAM IMC-based DNN inference where $&gt;$&gt;30\% CIFAR-10 accuracy and $&gt;$&gt;60\% TinyImageNet accuracy are recovered against temperature variations.},
  archive      = {J_MICRO},
  author       = {Jian Meng and Wonbo Shim and Li Yang and Injune Yeo and Deliang Fan and Shimeng Yu and Jae-sun Seo},
  doi          = {10.1109/MM.2021.3131114},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {89-98},
  shortjournal = {IEEE Micro},
  title        = {Temperature-resilient RRAM-based in-memory computing for DNN inference},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). IEEE security &amp; privacy. <em>MICRO</em>,
<em>42</em>(1), 88. (<a
href="https://doi.org/10.1109/MM.2022.3147388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2022.3147388},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {88},
  shortjournal = {IEEE Micro},
  title        = {IEEE security &amp; privacy},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on in-memory computing. <em>MICRO</em>,
<em>42</em>(1), 87–88. (<a
href="https://doi.org/10.1109/MM.2021.3137536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The articles in this special section focuses on in-memory computing. Computer designers have traditionally separated the role of storage and compute units. Memories and caches stored data. Processors’ logic units computed them. Is this separation necessary? A human brain does not separate the two so distinctly. Why should a processor? In-/near-memory computing paradigm blurs this distinction and imposes the dual responsibility on memory substrates: storing and computing on data. Modern processors and accelerators have over 90\% of their aggregate silicon area dedicated to memory. In-/near-memory processing converts these memory units into powerful allies for massively parallel computing, which can accelerate a plethora of applications including neural networks,},
  archive      = {J_MICRO},
  author       = {Reetuparna Das},
  doi          = {10.1109/MM.2021.3137536},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {87-88},
  shortjournal = {IEEE Micro},
  title        = {Special issue on in-memory computing},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ISOBlue avena: A framework for agricultural edge computing
and data sovereignty. <em>MICRO</em>, <em>42</em>(1), 78–86. (<a
href="https://doi.org/10.1109/MM.2021.3134830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While telematics and edge computing in agriculture have evolved significantly over the past 30 years, the same fundamental problems of vendor lock-in and incompatibility remain. In this article, we introduce Avena, our new open source framework for building an ecosystem around agricultural edge computing that embraces data sovereignty while still encouraging participation from the community’s stakeholders. Built on Linux containers and the technology of modern cloud systems, Avena brings a new way of thinking to agricultural machines.},
  archive      = {J_MICRO},
  author       = {Andrew D. Balmos and Fabio A. Castiblanco and Aaron J. Neustedter and James V. Krogmeier and Dennis R. Buckmaster},
  doi          = {10.1109/MM.2021.3134830},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {78-86},
  shortjournal = {IEEE Micro},
  title        = {ISOBlue avena: A framework for agricultural edge computing and data sovereignty},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Democratizing data-driven agriculture using affordable
hardware. <em>MICRO</em>, <em>42</em>(1), 69–77. (<a
href="https://doi.org/10.1109/MM.2021.3134743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The world needs to sustainably grow more food to feed the growing population of the planet. Data-driven agriculture is a promising technique that can help farmers be more productive, reduce costs, and enable adoption of sustainable agricultural practices. However, the adoption of data-driven agriculture is limited by a lack of affordable technologies, for broadband, sensing, imaging, and insights. In this article, we present an overview of Project FarmBeats, a research project that started in 2014 to increase the adoption of data-driven agricultural practices. We provide an overview of the various components of the FarmBeats architecture, and details of the hardware innovations.},
  archive      = {J_MICRO},
  author       = {Ranveer Chandra and Manohar Swaminathan and Tusher Chakraborty and Jian Ding and Zerina Kapetanovic and Peeyush Kumar and Deepak Vasisht},
  doi          = {10.1109/MM.2021.3134743},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {69-77},
  shortjournal = {IEEE Micro},
  title        = {Democratizing data-driven agriculture using affordable hardware},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Challenges and opportunities for autonomous micro-UAVs in
precision agriculture. <em>MICRO</em>, <em>42</em>(1), 61–68. (<a
href="https://doi.org/10.1109/MM.2021.3134744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile robots such as unmanned ground vehicles (UGVs) and unmanned aerial vehicles (UAVs) are increasingly used for precision agriculture. While UGVs have larger payload capabilities and longer operation time, they are limited to 2-D space. This makes UAVs better suited for tasks that require fast coverage, harsh terrain traversal, and high altitude or multilevel operation. However, it remains a challenging task to develop a reliable yet fully autonomous UAV system that can actively extract actionable information in large-scale cluttered agricultural environments. Such a system will have to estimate its own poses, build a map of the environment, navigate through obstacles, and act to gather information with limited onboard computation and battery life. In this survey, we first review recent advances in UAV hardware and software, ranging from novel platforms and sensors to state-of-the-art autonomous navigation, object detection and segmentation, robot localization, and mapping algorithms related to agriculture. We then provide a list of challenges in each field and potential opportunities for the broader adoption of UAVs in precision agriculture.},
  archive      = {J_MICRO},
  author       = {Xu Liu and Steven W. Chen and Guilherme V. Nardari and Chao Qu and Fernando Cladera and Camillo J. Taylor and Vijay Kumar},
  doi          = {10.1109/MM.2021.3134744},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {61-68},
  shortjournal = {IEEE Micro},
  title        = {Challenges and opportunities for autonomous micro-UAVs in precision agriculture},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). IEEE transactions on big data. <em>MICRO</em>,
<em>42</em>(1), 60. (<a
href="https://doi.org/10.1109/MM.2022.3147365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2022.3147365},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {60},
  shortjournal = {IEEE Micro},
  title        = {IEEE transactions on big data},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluating sensor data quality in internet of things smart
agriculture applications. <em>MICRO</em>, <em>42</em>(1), 51–60. (<a
href="https://doi.org/10.1109/MM.2021.3137401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unprecedented growth of Internet of Things (IoT) underpinned by machine-to-machine communication, analytics, and actuation is spearheading the development of IoT-based smart agriculture applications (IoTSAs). Various factors (e.g., location and weather) impact data availability for such IoT applications, which impacts the decision making and actuation process of IoTSA. This article proposes a conceptual framework and a novel model to compute sensor data quality model that can be used by IoTSA in their decision/actuation process to adapt to uncertainty in IoT sensor data. To demonstrate the efficacy of the proposed data quality metrics and model, we apply them to an IoTSA for monitoring milk condition in dairy farms (an instance of IoTSA). Our real-world experimental evaluations conclude that the proposed model 1) can be employed by IoTSAs to adapt to different factors that may impact the quality of decision making (actuation) and 2) aids in developing data quality-aware IoTSA and beyond.},
  archive      = {J_MICRO},
  author       = {Kaneez Fizza and Prem Prakash Jayaraman and Abhik Banerjee and Dimitrios Georgakopoulos and Rajiv Ranjan},
  doi          = {10.1109/MM.2021.3137401},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {51-60},
  shortjournal = {IEEE Micro},
  title        = {Evaluating sensor data quality in internet of things smart agriculture applications},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MHADBOR: AI-enabled administrative-distance-based
opportunistic load balancing scheme for an agriculture internet of
things network. <em>MICRO</em>, <em>42</em>(1), 41–50. (<a
href="https://doi.org/10.1109/MM.2021.3112264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a supervised machine learning multipath and administrative-distance-based load balancing algorithm for an Agriculture Internet of Things (AG-IoT) network. The proposed algorithm is known as an artificial intelligence or simply AI-enabled multihop and administrative-distance-based opportunistic routing (MHADBOR) algorithm, which processes the collected information from source to the destination by means of multihop count and administrative-distance-based communication infrastructure in the network. Beside that, we used cluster heads (CH), microbase stations ($\boldsymbol {\Re BS}$ℜBS), and macrobase stations ($\boldsymbol {\aleph BS}$ℵBS) in the network with a frequent rate to effectively utilize the administrative distance while managing the deployed network traffic in a congestionless communication environment. In addition, the MHADBOR algorithm empowers the participating devices to practice the administrative distance rather than hop count communication when they are in the vicinity of network special components, e.g., CH and $\boldsymbol {\Re BS}$ℜBS outcome statistics of the MHADBOR algorithm in the simulation environment exhibit an extraordinary improvement in contention, congestion, communication, and computing costs, accompanied by throughput and end-to-end (E2E) delay and packet loss ratio in the deployed AG-IoT network.},
  archive      = {J_MICRO},
  author       = {Muhammad Adil and Muhammad Khurram Khan and Mona Jamjoom and Ahmed Farouk},
  doi          = {10.1109/MM.2021.3112264},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {41-50},
  shortjournal = {IEEE Micro},
  title        = {MHADBOR: AI-enabled administrative-distance-based opportunistic load balancing scheme for an agriculture internet of things network},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PEFL: Deep privacy-encoding-based federated learning
framework for smart agriculture. <em>MICRO</em>, <em>42</em>(1), 33–40.
(<a href="https://doi.org/10.1109/MM.2021.3112476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart agriculture (SA) incorporates low-cost and low-energy-consuming sensors and devices to enhance quantitative and qualitative agricultural production. However, this device uses an open communication channel, i.e., Internet, and generates large amount of data in real time and, thus, has the potential to be misused. As a consequence, the major concern in the implementation of SA is minimizing the risk of security and data privacy violation (e.g., adversaries performing inference attacks). To address these challenges, we propose PEFL, a deep privacy-encoding-based federated learning (FL) framework that adopts a perturbation-based encoding and long short-term memory-autoencoder technique to achieve the target of privacy. Then, an FL-based gated recurrent unit neural network algorithm (FedGRU) is designed using the encoded data for intrusion detection. The experimental results based on the ToN-IoT data set reveal that the PEFL can efficiently identify normal and attack patterns after transformation over other non-FL and FL methods.},
  archive      = {J_MICRO},
  author       = {Prabhat Kumar and Govind P. Gupta and Rakesh Tripathi},
  doi          = {10.1109/MM.2021.3112476},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {33-40},
  shortjournal = {IEEE Micro},
  title        = {PEFL: Deep privacy-encoding-based federated learning framework for smart agriculture},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). UAV–assisted joint wireless power transfer and data
collection mechanism for sustainable precision agriculture in 5G.
<em>MICRO</em>, <em>42</em>(1), 25–32. (<a
href="https://doi.org/10.1109/MM.2021.3122553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The limited battery capacity of sensor nodes (SNs) is one of the challenges for achieving sustainable precision agriculture in 5G. When SNs run out of energy, the data center will not be able to grasp the information of the environment, and the corresponding decisions and actions will not be performed. To solve this problem, this study presents a framework for charging SNs and collecting sensing data using unmanned aerial vehicles. The problems of cluster head (CH) election and path planning are considered at the same time to maximize charging efficiency in a way that the lifetime of SNs can be prolonged. Moreover, ${K}$K-means-based CH and charging position selection algorithm and ACO-based charging path-planning algorithm are proposed to optimize charging planning of the 3-D environment. Simulation results show that the proposed mechanism can minimize the number of CHs and maximize the remaining energy of SNs.},
  archive      = {J_MICRO},
  author       = {Wei-Che Chien and Mohammad Mehedi Hassan and Ahmed Alsanad and Giancarlo Fortino},
  doi          = {10.1109/MM.2021.3122553},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {25-32},
  shortjournal = {IEEE Micro},
  title        = {UAV–Assisted joint wireless power transfer and data collection mechanism for sustainable precision agriculture in 5G},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Computing in science &amp; engineering. <em>MICRO</em>,
<em>42</em>(1), 24. (<a
href="https://doi.org/10.1109/MM.2022.3147343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2022.3147343},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {24},
  shortjournal = {IEEE Micro},
  title        = {Computing in science &amp; engineering},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Artificial intelligence best practices in smart agriculture.
<em>MICRO</em>, <em>42</em>(1), 17–24. (<a
href="https://doi.org/10.1109/MM.2021.3121279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart agriculture, with the aid of artificial intelligence (AI), is playing a pivotal role to ensure agriculture sustainability. AI techniques are employed in soil and irrigation management, weather forecasting, plant growth, disease prediction, and livestock management, which are considered to be significant domains of agriculture. We review recent AI techniques that have been deployed in these domains. We focus on the various AI algorithms used as well as their performance impact. This review not only highlights the effective use of AI at different layers of a smart agriculture architecture but also identifies future research directions in this field. We found that the deep learning algorithms that have been used in recent studies have performed far better than the conventional machine learning algorithms due to recent technological advances that can efficiently process vast amount of data and enable timely intelligent decisions similar to human decisions.},
  archive      = {J_MICRO},
  author       = {Faisal Karim Shaikh and Mohsin Ali Memon and Naeem Ahmed Mahoto and Sherali Zeadally and Jamel Nebhen},
  doi          = {10.1109/MM.2021.3121279},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {17-24},
  shortjournal = {IEEE Micro},
  title        = {Artificial intelligence best practices in smart agriculture},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Soil fertility monitoring with internet of underground
things: A survey. <em>MICRO</em>, <em>42</em>(1), 8–16. (<a
href="https://doi.org/10.1109/MM.2021.3121496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underground soil sensing for fertility monitoring is an emerging paradigm to facilitate quantitative and qualitative improvement in the food production. Moreover, soil fertility monitoring is needed in order to protect the environment from greenhouse gas emissions. Underground soil sensing thus uses numerous types of sensors and devices that can help in the real-time monitoring of soil fertility. These sensory devices incorporate various communication protocols for relaying the sensory data. This provides motivation to present a review of existing underground sensing technologies with Internet of Underground Things (IoUT) that were used in agriculture for monitoring soil fertility. The presented taxonomy covers wide aspects of the state-of-the-art methodologies. Additionally, the challenges presented by IoUT in its design and implementation for monitoring soil fertility are considered and examined.},
  archive      = {J_MICRO},
  author       = {Debjani Ghosh and Akash Anand and Satya Sankalp Gautam and Ankit Vidyarthi},
  doi          = {10.1109/MM.2021.3121496},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {8-16},
  shortjournal = {IEEE Micro},
  title        = {Soil fertility monitoring with internet of underground things: A survey},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022d). Smart agriculture and smart memories. <em>MICRO</em>,
<em>42</em>(1), 4–6. (<a
href="https://doi.org/10.1109/MM.2021.3138625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_MICRO},
  author       = {Lizy Kurian John},
  doi          = {10.1109/MM.2021.3138625},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {4-6},
  shortjournal = {IEEE Micro},
  title        = {Smart agriculture and smart memories},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
